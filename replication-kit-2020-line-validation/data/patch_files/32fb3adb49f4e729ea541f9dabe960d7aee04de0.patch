From 32fb3adb49f4e729ea541f9dabe960d7aee04de0 Mon Sep 17 00:00:00 2001
From: Thomas Vandahl <tv@apache.org>
Date: Wed, 25 Nov 2015 13:10:22 +0000
Subject: [PATCH] JCS-153: Fix file size limitation for Block Disk Cache and
 Indexed Disk Cache

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/jcs/trunk@1716376 13f79535-47bb-0310-9956-ffa450edef68
---
 .../auxiliary/disk/block/BlockDiskCache.java  |   6 +-
 .../disk/block/BlockDiskKeyStore.java         | 249 +++--
 .../disk/indexed/IndexedDiskCache.java        | 804 +++++++-------
 .../jcs/utils/struct/AbstractLRUMap.java      |  15 +-
 .../commons/jcs/utils/struct/LRUMap.java      |  26 +-
 .../block/BlockDiskCacheCountUnitTest.java    |  35 +
 .../block/BlockDiskCacheSizeUnitTest.java     |  35 +
 .../disk/block/BlockDiskCacheUnitTest.java    | 343 ------
 .../block/BlockDiskCacheUnitTestAbstract.java | 399 +++++++
 .../block/BlockDiskCacheUnitTestCount.java    |  14 -
 .../block/BlockDiskCacheUnitTestSize.java     |  14 -
 ....java => IndexDiskCacheCountUnitTest.java} | 201 ++--
 ...e.java => IndexDiskCacheSizeUnitTest.java} | 203 ++--
 .../disk/indexed/IndexDiskCacheUnitTest.java  | 940 -----------------
 .../IndexDiskCacheUnitTestAbstract.java       | 991 ++++++++++++++++++
 src/changes/changes.xml                       |   3 +
 16 files changed, 2295 insertions(+), 1983 deletions(-)
 create mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheCountUnitTest.java
 create mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheSizeUnitTest.java
 delete mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTest.java
 create mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestAbstract.java
 delete mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestCount.java
 delete mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestSize.java
 rename commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/{IndexDiskCacheUnitTestCount.java => IndexDiskCacheCountUnitTest.java} (78%)
 rename commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/{IndexDiskCacheUnitTestSize.java => IndexDiskCacheSizeUnitTest.java} (79%)
 delete mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTest.java
 create mode 100644 commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestAbstract.java

diff --git a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCache.java b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCache.java
index 2836ab664..1a14a68cb 100644
--- a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCache.java
+++ b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCache.java
@@ -125,11 +125,13 @@ public BlockDiskCache( BlockDiskCacheAttributes cacheAttributes, IElementSeriali
             if ( this.blockDiskCacheAttributes.getBlockSizeBytes() > 0 )
             {
                 this.dataFile = new BlockDisk( new File( rootDirectory, fileName + ".data" ),
-                                               this.blockDiskCacheAttributes.getBlockSizeBytes() );
+                                               this.blockDiskCacheAttributes.getBlockSizeBytes(),
+                                               getElementSerializer() );
             }
             else
             {
-                this.dataFile = new BlockDisk( new File( rootDirectory, fileName + ".data" ), getElementSerializer() );
+                this.dataFile = new BlockDisk( new File( rootDirectory, fileName + ".data" ),
+                                               getElementSerializer() );
             }
 
             keyStore = new BlockDiskKeyStore<K>( this.blockDiskCacheAttributes, this );
diff --git a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskKeyStore.java b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskKeyStore.java
index 4a478fb8e..cc3ade1f5 100644
--- a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskKeyStore.java
+++ b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskKeyStore.java
@@ -44,12 +44,13 @@
 /**
  * This is responsible for storing the keys.
  * <p>
+ *
  * @author Aaron Smuts
  */
 public class BlockDiskKeyStore<K>
 {
     /** The logger */
-    private static final Log log = LogFactory.getLog( BlockDiskKeyStore.class );
+    private static final Log log = LogFactory.getLog(BlockDiskKeyStore.class);
 
     /** Attributes governing the behavior of the block disk cache. */
     private final BlockDiskCacheAttributes blockDiskCacheAttributes;
@@ -79,35 +80,36 @@ public class BlockDiskKeyStore<K>
     /**
      * Set the configuration options.
      * <p>
+     *
      * @param cacheAttributes
-     * @param blockDiskCache used for freeing
+     * @param blockDiskCache
+     *            used for freeing
      */
-    public BlockDiskKeyStore( BlockDiskCacheAttributes cacheAttributes,
-            BlockDiskCache<K, ?> blockDiskCache)
+    public BlockDiskKeyStore(BlockDiskCacheAttributes cacheAttributes, BlockDiskCache<K, ?> blockDiskCache)
     {
         this.blockDiskCacheAttributes = cacheAttributes;
         this.logCacheName = "Region [" + this.blockDiskCacheAttributes.getCacheName() + "] ";
         this.fileName = this.blockDiskCacheAttributes.getCacheName();
         this.maxKeySize = cacheAttributes.getMaxKeySize();
         this.blockDiskCache = blockDiskCache;
-        this.diskLimitType  = cacheAttributes.getDiskLimitType();
+        this.diskLimitType = cacheAttributes.getDiskLimitType();
         this.blockSize = cacheAttributes.getBlockSizeBytes();
 
         File rootDirectory = cacheAttributes.getDiskPath();
 
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Cache file root directory [" + rootDirectory + "]" );
+            log.info(logCacheName + "Cache file root directory [" + rootDirectory + "]");
         }
 
-        this.keyFile = new File( rootDirectory, fileName + ".key" );
+        this.keyFile = new File(rootDirectory, fileName + ".key");
 
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Key File [" + this.keyFile.getAbsolutePath() + "]" );
+            log.info(logCacheName + "Key File [" + this.keyFile.getAbsolutePath() + "]");
         }
 
-        if ( keyFile.length() > 0 )
+        if (keyFile.length() > 0)
         {
             loadKeys();
             // TODO verify somehow
@@ -128,27 +130,26 @@ protected void saveKeys()
         {
             ElapsedTimer timer = new ElapsedTimer();
             int numKeys = keyHash.size();
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Saving keys to [" + this.keyFile.getAbsolutePath() + "], key count ["
-                    + numKeys + "]" );
+                log.info(logCacheName + "Saving keys to [" + this.keyFile.getAbsolutePath() + "], key count [" + numKeys + "]");
             }
 
             synchronized (keyFile)
             {
-                FileOutputStream fos = new FileOutputStream( keyFile );
-                BufferedOutputStream bos = new BufferedOutputStream( fos, 65536 );
-                ObjectOutputStream oos = new ObjectOutputStream( bos );
+                FileOutputStream fos = new FileOutputStream(keyFile);
+                BufferedOutputStream bos = new BufferedOutputStream(fos, 65536);
+                ObjectOutputStream oos = new ObjectOutputStream(bos);
                 try
                 {
                     // don't need to synchronize, since the underlying collection makes a copy
                     for (Map.Entry<K, int[]> entry : keyHash.entrySet())
                     {
                         BlockDiskElementDescriptor<K> descriptor = new BlockDiskElementDescriptor<K>();
-                        descriptor.setKey( entry.getKey() );
-                        descriptor.setBlocks( entry.getValue() );
+                        descriptor.setKey(entry.getKey());
+                        descriptor.setBlocks(entry.getValue());
                         // stream these out in the loop.
-                        oos.writeObject( descriptor );
+                        oos.writeUnshared(descriptor);
                     }
                 }
                 finally
@@ -158,15 +159,15 @@ protected void saveKeys()
                 }
             }
 
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Finished saving keys. It took " + timer.getElapsedTimeString() + " to store "
-                    + numKeys + " keys.  Key file length [" + keyFile.length() + "]" );
+                log.info(logCacheName + "Finished saving keys. It took " + timer.getElapsedTimeString() + " to store " + numKeys
+                    + " keys.  Key file length [" + keyFile.length() + "]");
             }
         }
-        catch ( IOException e )
+        catch (IOException e)
         {
-            log.error( logCacheName + "Problem storing keys.", e );
+            log.error(logCacheName + "Problem storing keys.", e);
         }
     }
 
@@ -180,7 +181,6 @@ protected void reset()
             clearMemoryMap();
             saveKeys();
         }
-
     }
 
     /**
@@ -197,16 +197,19 @@ protected void clearMemoryMap()
     private void initKeyMap()
     {
         keyHash = null;
-        if ( maxKeySize >= 0 )
+        if (maxKeySize >= 0)
         {
-            if (this.diskLimitType.equals(DiskLimitType.SIZE)) {
+            if (this.diskLimitType == DiskLimitType.SIZE)
+            {
                 keyHash = new LRUMapSizeLimited(maxKeySize);
-            } else {
-                keyHash = new LRUMapCountLimited( maxKeySize );
             }
-            if ( log.isInfoEnabled() )
+            else
+            {
+                keyHash = new LRUMapCountLimited(maxKeySize);
+            }
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Set maxKeySize to: '" + maxKeySize + "'" );
+                log.info(logCacheName + "Set maxKeySize to: '" + maxKeySize + "'");
             }
         }
         else
@@ -214,9 +217,9 @@ private void initKeyMap()
             // If no max size, use a plain map for memory and processing efficiency.
             keyHash = new HashMap<K, int[]>();
             // keyHash = Collections.synchronizedMap( new HashMap() );
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Set maxKeySize to unlimited'" );
+                log.info(logCacheName + "Set maxKeySize to unlimited'");
             }
         }
     }
@@ -227,9 +230,9 @@ private void initKeyMap()
      */
     protected void loadKeys()
     {
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Loading keys for " + keyFile.toString() );
+            log.info(logCacheName + "Loading keys for " + keyFile.toString());
         }
 
         try
@@ -241,22 +244,23 @@ protected void loadKeys()
 
             synchronized (keyFile)
             {
-                FileInputStream fis = new FileInputStream( keyFile );
-                BufferedInputStream bis = new BufferedInputStream( fis );
-                ObjectInputStream ois = new ObjectInputStreamClassLoaderAware( bis , null);
+                FileInputStream fis = new FileInputStream(keyFile);
+                BufferedInputStream bis = new BufferedInputStream(fis, 65536);
+                ObjectInputStream ois = new ObjectInputStreamClassLoaderAware(bis, null);
                 try
                 {
-                    while ( true )
+                    while (true)
                     {
-                        @SuppressWarnings("unchecked") // Need to cast from Object
+                        @SuppressWarnings("unchecked")
+                        // Need to cast from Object
                         BlockDiskElementDescriptor<K> descriptor = (BlockDiskElementDescriptor<K>) ois.readObject();
-                        if ( descriptor != null )
+                        if (descriptor != null)
                         {
-                            keys.put( descriptor.getKey(), descriptor.getBlocks() );
+                            keys.put(descriptor.getKey(), descriptor.getBlocks());
                         }
                     }
                 }
-                catch ( EOFException eof )
+                catch (EOFException eof)
                 {
                     // nothing
                 }
@@ -266,31 +270,32 @@ protected void loadKeys()
                 }
             }
 
-            if ( !keys.isEmpty() )
+            if (!keys.isEmpty())
             {
-                keyHash.putAll( keys );
+                keyHash.putAll(keys);
 
-                if ( log.isDebugEnabled() )
+                if (log.isDebugEnabled())
                 {
-                    log.debug( logCacheName + "Found " + keys.size() + " in keys file." );
+                    log.debug(logCacheName + "Found " + keys.size() + " in keys file.");
                 }
 
-                if ( log.isInfoEnabled() )
+                if (log.isInfoEnabled())
                 {
-                    log.info( logCacheName + "Loaded keys from [" + fileName + "], key count: " + keyHash.size()
-                        + "; up to " + maxKeySize + " will be available." );
+                    log.info(logCacheName + "Loaded keys from [" + fileName + "], key count: " + keyHash.size() + "; up to "
+                        + maxKeySize + " will be available.");
                 }
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Problem loading keys for file " + fileName, e );
+            log.error(logCacheName + "Problem loading keys for file " + fileName, e);
         }
     }
 
     /**
      * Gets the entry set.
      * <p>
+     *
      * @return entry set.
      */
     public Set<Map.Entry<K, int[]>> entrySet()
@@ -301,6 +306,7 @@ public Set<Map.Entry<K, int[]>> entrySet()
     /**
      * Gets the key set.
      * <p>
+     *
      * @return key set.
      */
     public Set<K> keySet()
@@ -311,6 +317,7 @@ public Set<K> keySet()
     /**
      * Gets the size of the key hash.
      * <p>
+     *
      * @return the number of keys.
      */
     public int size()
@@ -321,91 +328,123 @@ public int size()
     /**
      * gets the object for the key.
      * <p>
+     *
      * @param key
      * @return Object
      */
-    public int[] get( K key )
+    public int[] get(K key)
     {
-        return this.keyHash.get( key );
+        return this.keyHash.get(key);
     }
 
     /**
      * Puts a int[] in the keyStore.
      * <p>
+     *
      * @param key
      * @param value
      */
-    public void put( K key, int[] value )
+    public void put(K key, int[] value)
     {
-        this.keyHash.put( key, value );
+        this.keyHash.put(key, value);
     }
 
     /**
      * Remove by key.
      * <p>
+     *
      * @param key
      * @return BlockDiskElementDescriptor if it was present, else null
      */
-    public int[] remove( K key )
+    public int[] remove(K key)
     {
-        return this.keyHash.remove( key );
+        return this.keyHash.remove(key);
     }
 
-
     /**
      * Class for recycling and lru. This implements the LRU size overflow callback, so we can mark the
      * blocks as free.
      */
-
-    public class LRUMapSizeLimited
-    	extends AbstractLRUMap<K, int[]>
-
+    public class LRUMapSizeLimited extends AbstractLRUMap<K, int[]>
     {
         /**
          * <code>tag</code> tells us which map we are working on.
          */
-        public String tag = "orig-lru-size";
+        public final String tag = "orig-lru-size";
+
         // size of the content in kB
-        private AtomicInteger contentSize = new AtomicInteger();
-        private int maxSize = -1;
+        private AtomicInteger contentSize;
+        private int maxSize;
+
         /**
          * Default
          */
         public LRUMapSizeLimited()
         {
-            super();
+            this(-1);
         }
 
         /**
-         * @param maxKeySize
+         * @param maxSize maximum cache size in kB
          */
-        public LRUMapSizeLimited( int maxKeySize )
+        public LRUMapSizeLimited(int maxSize)
         {
             super();
-            this.maxSize = maxKeySize;
+            this.maxSize = maxSize;
+            this.contentSize = new AtomicInteger(0);
+        }
+
+        // keep the content size in kB, so 2^31 kB is reasonable value
+        private void subLengthFromCacheSize(int[] value)
+        {
+            contentSize.addAndGet(value.length * blockSize / -1024 - 1);
+        }
+
+        // keep the content size in kB, so 2^31 kB is reasonable value
+        private void addLengthToCacheSize(int[] value)
+        {
+            contentSize.addAndGet(value.length * blockSize / 1024 + 1);
         }
 
         @Override
-        public int[] put(K key, int[] value) {
-            try {
-                return super.put(key, value);
-            } finally {
-                // keep the content size in kB, so 2^31 kB is reasonable value
-                contentSize.addAndGet((int) Math.ceil(value.length * blockSize / 1024.0));
+        public int[] put(K key, int[] value)
+        {
+            int[] oldValue = null;
+
+            try
+            {
+                oldValue = super.put(key, value);
             }
+            finally
+            {
+                if (value != null)
+                {
+                    addLengthToCacheSize(value);
+                }
+                if (oldValue != null)
+                {
+                    subLengthFromCacheSize(oldValue);
+                }
+            }
+
+            return oldValue;
         }
 
         @Override
-        public int[] remove(Object key ) {
+        public int[] remove(Object key)
+        {
             int[] value = null;
 
-            try {
+            try
+            {
                 value = super.remove(key);
                 return value;
-            } finally {
-                if (value != null) {
-                    // keep the content size in kB, so 2^31 kB is reasonable value
-                    contentSize.addAndGet((int) ((Math.ceil(value.length * blockSize / 1024.0)) * -1));
+            }
+            finally
+            {
+                if (value != null)
+                {
+                    subLengthFromCacheSize(value);
                 }
             }
         }
@@ -414,57 +453,65 @@ public int[] remove(Object key ) {
          * This is called when the may key size is reached. The least recently used item will be
          * passed here. We will store the position and size of the spot on disk in the recycle bin.
          * <p>
+         *
          * @param key
          * @param value
          */
         @Override
-        protected void processRemovedLRU( K key, int[] value )
+        protected void processRemovedLRU(K key, int[] value)
         {
-            blockDiskCache.freeBlocks( value );
-            if ( log.isDebugEnabled() )
+            blockDiskCache.freeBlocks(value);
+            if (log.isDebugEnabled())
+            {
+                log.debug(logCacheName + "Removing key: [" + key + "] from key store.");
+                log.debug(logCacheName + "Key store size: [" + super.size() + "].");
+            }
+
+            if (value != null)
             {
-                log.debug( logCacheName + "Removing key: [" + key + "] from key store." );
-                log.debug( logCacheName + "Key store size: [" + super.size() + "]." );
+                subLengthFromCacheSize(value);
             }
         }
+
         @Override
-        protected boolean shouldRemove() {
-            return maxSize > 0 && contentSize.intValue() > maxSize && this.size() > 1;
+        protected boolean shouldRemove()
+        {
+            return maxSize > 0 && contentSize.get() > maxSize && this.size() > 1;
         }
     }
+
     /**
      * Class for recycling and lru. This implements the LRU overflow callback, so we can mark the
      * blocks as free.
      */
-    public class LRUMapCountLimited
-    extends LRUMap<K, int[]>
-    // implements Serializable
+    public class LRUMapCountLimited extends LRUMap<K, int[]>
     {
         /**
          * <code>tag</code> tells us which map we are working on.
          */
-        public String tag = "orig-lru-count";
+        public final String tag = "orig-lru-count";
 
-        public LRUMapCountLimited(int maxKeySize) {
+        public LRUMapCountLimited(int maxKeySize)
+        {
             super(maxKeySize);
         }
 
-
         /**
          * This is called when the may key size is reached. The least recently used item will be
          * passed here. We will store the position and size of the spot on disk in the recycle bin.
          * <p>
+         *
          * @param key
          * @param value
          */
         @Override
-        protected void processRemovedLRU( K key, int[] value )
+        protected void processRemovedLRU(K key, int[] value)
         {
-            blockDiskCache.freeBlocks( value );
-            if ( log.isDebugEnabled() )
+            blockDiskCache.freeBlocks(value);
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Removing key: [" + key + "] from key store." );
-                log.debug( logCacheName + "Key store size: [" + super.size() + "]." );
+                log.debug(logCacheName + "Removing key: [" + key + "] from key store.");
+                log.debug(logCacheName + "Key store size: [" + super.size() + "].");
             }
         }
     }
diff --git a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexedDiskCache.java b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexedDiskCache.java
index e7a2f332f..f67ee412d 100644
--- a/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexedDiskCache.java
+++ b/commons-jcs-core/src/main/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexedDiskCache.java
@@ -19,6 +19,24 @@
  * under the License.
  */
 
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.ConcurrentModificationException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
 import org.apache.commons.jcs.auxiliary.AuxiliaryCacheAttributes;
 import org.apache.commons.jcs.auxiliary.disk.AbstractDiskCache;
 import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
@@ -40,34 +58,15 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.ConcurrentModificationException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
 /**
  * Disk cache that uses a RandomAccessFile with keys stored in memory. The maximum number of keys
  * stored in memory is configurable. The disk cache tries to recycle spots on disk to limit file
  * expansion.
  */
-public class IndexedDiskCache<K, V>
-    extends AbstractDiskCache<K, V>
+public class IndexedDiskCache<K, V> extends AbstractDiskCache<K, V>
 {
     /** The logger */
-    private static final Log log = LogFactory.getLog( IndexedDiskCache.class );
+    private static final Log log = LogFactory.getLog(IndexedDiskCache.class);
 
     /** Cache name used in log messages */
     protected final String logCacheName;
@@ -115,8 +114,7 @@ public class IndexedDiskCache<K, V>
     private boolean queueInput = false;
 
     /** list where puts made during optimization are made */
-    private final LinkedList<IndexedDiskElementDescriptor> queuedPutList =
-        new LinkedList<IndexedDiskElementDescriptor>();
+    private final LinkedList<IndexedDiskElementDescriptor> queuedPutList = new LinkedList<IndexedDiskElementDescriptor>();
 
     /** RECYLCE BIN -- array of empty spots */
     private SortedPreferentialArray<IndexedDiskElementDescriptor> recycle;
@@ -147,24 +145,27 @@ public class IndexedDiskCache<K, V>
     /**
      * Constructor for the DiskCache object.
      * <p>
+     *
      * @param cacheAttributes
      */
-    public IndexedDiskCache( IndexedDiskCacheAttributes cacheAttributes )
+    public IndexedDiskCache(IndexedDiskCacheAttributes cacheAttributes)
     {
-        this( cacheAttributes, null );
+        this(cacheAttributes, null);
     }
 
     /**
      * Constructor for the DiskCache object.
      * <p>
+     *
      * @param cattr
-     * @param elementSerializer used if supplied, the super's super will not set a null
+     * @param elementSerializer
+     *            used if supplied, the super's super will not set a null
      */
-    public IndexedDiskCache( IndexedDiskCacheAttributes cattr, IElementSerializer elementSerializer )
+    public IndexedDiskCache(IndexedDiskCacheAttributes cattr, IElementSerializer elementSerializer)
     {
-        super( cattr );
+        super(cattr);
 
-        setElementSerializer( elementSerializer );
+        setElementSerializer(elementSerializer);
 
         this.cattr = cattr;
         this.maxKeySize = cattr.getMaxKeySize();
@@ -177,44 +178,46 @@ public IndexedDiskCache( IndexedDiskCacheAttributes cattr, IElementSerializer el
 
         try
         {
-            initializeFileSystem( cattr );
+            initializeFileSystem(cattr);
 
-            initializeKeysAndData( cattr );
+            initializeKeysAndData(cattr);
 
             initializeRecycleBin();
 
             // Initialization finished successfully, so set alive to true.
             alive = true;
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Indexed Disk Cache is alive." );
+                log.info(logCacheName + "Indexed Disk Cache is alive.");
             }
 
             // TODO: Should we improve detection of whether or not the file should be optimized.
-            if ( isRealTimeOptimizationEnabled && keyHash.size() > 0 )
+            if (isRealTimeOptimizationEnabled && keyHash.size() > 0)
             {
                 // Kick off a real time optimization, in case we didn't do a final optimization.
                 doOptimizeRealTime();
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Failure initializing for fileName: " + fileName + " and directory: "
-                + this.rafDir.getAbsolutePath(), e );
+            log.error(
+                logCacheName + "Failure initializing for fileName: " + fileName + " and directory: "
+                    + this.rafDir.getAbsolutePath(), e);
         }
     }
 
     /**
      * Tries to create the root directory if it does not already exist.
      * <p>
+     *
      * @param cattr
      */
-    private void initializeFileSystem( IndexedDiskCacheAttributes cattr )
+    private void initializeFileSystem(IndexedDiskCacheAttributes cattr)
     {
         this.rafDir = cattr.getDiskPath();
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Cache file root directory: " + rafDir );
+            log.info(logCacheName + "Cache file root directory: " + rafDir);
         }
     }
 
@@ -223,26 +226,27 @@ private void initializeFileSystem( IndexedDiskCacheAttributes cattr )
      * <p>
      * Loads any keys if they are present and ClearDiskOnStartup is false.
      * <p>
+     *
      * @param cattr
      * @throws FileNotFoundException
      * @throws IOException
      * @throws InterruptedException
      */
-    private void initializeKeysAndData( IndexedDiskCacheAttributes cattr )
-        throws FileNotFoundException, IOException, InterruptedException
+    private void initializeKeysAndData(IndexedDiskCacheAttributes cattr) throws FileNotFoundException, IOException,
+        InterruptedException
     {
-        this.dataFile = new IndexedDisk( new File( rafDir, fileName + ".data" ), getElementSerializer() );
-        this.keyFile = new IndexedDisk( new File( rafDir, fileName + ".key" ), getElementSerializer() );
+        this.dataFile = new IndexedDisk(new File(rafDir, fileName + ".data"), getElementSerializer());
+        this.keyFile = new IndexedDisk(new File(rafDir, fileName + ".key"), getElementSerializer());
 
-        if ( cattr.isClearDiskOnStartup() )
+        if (cattr.isClearDiskOnStartup())
         {
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "ClearDiskOnStartup is set to true.  Ingnoring any persisted data." );
+                log.info(logCacheName + "ClearDiskOnStartup is set to true.  Ingnoring any persisted data.");
             }
             initializeEmptyStore();
         }
-        else if ( keyFile.length() > 0 )
+        else if (keyFile.length() > 0)
         {
             // If the key file has contents, try to initialize the keys
             // from it. In no keys are loaded reset the data file.
@@ -259,14 +263,14 @@ else if ( keyFile.length() > 0 )
     /**
      * Initializes an empty disk cache.
      * <p>
+     *
      * @throws IOException
      */
-    private void initializeEmptyStore()
-        throws IOException
+    private void initializeEmptyStore() throws IOException
     {
         initializeKeyMap();
 
-        if ( dataFile.length() > 0 )
+        if (dataFile.length() > 0)
         {
             dataFile.reset();
         }
@@ -276,27 +280,27 @@ private void initializeEmptyStore()
      * Loads any persisted data and checks for consistency. If there is a consistency issue, the
      * files are cleared.
      * <p>
+     *
      * @throws InterruptedException
      * @throws IOException
      */
-    private void initializeStoreFromPersistedData()
-        throws InterruptedException, IOException
+    private void initializeStoreFromPersistedData() throws InterruptedException, IOException
     {
         loadKeys();
 
-        if ( keyHash.size() == 0 )
+        if (keyHash.size() == 0)
         {
             dataFile.reset();
         }
         else
         {
-            boolean isOk = checkKeyDataConsistency( false );
-            if ( !isOk )
+            boolean isOk = checkKeyDataConsistency(false);
+            if (!isOk)
             {
                 keyHash.clear();
                 keyFile.reset();
                 dataFile.reset();
-                log.warn( logCacheName + "Corruption detected.  Reseting data and keys files." );
+                log.warn(logCacheName + "Corruption detected.  Reseting data and keys files.");
             }
             else
             {
@@ -312,14 +316,14 @@ private void initializeStoreFromPersistedData()
      * Loads the keys from the .key file. The keys are stored in a HashMap on disk. This is
      * converted into a LRUMap.
      * <p>
+     *
      * @throws InterruptedException
      */
-    protected void loadKeys()
-        throws InterruptedException
+    protected void loadKeys() throws InterruptedException
     {
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Loading keys for " + keyFile.toString() );
+            log.debug(logCacheName + "Loading keys for " + keyFile.toString());
         }
 
         storageLock.writeLock().lock();
@@ -329,34 +333,33 @@ protected void loadKeys()
             // create a key map to use.
             initializeKeyMap();
 
-            HashMap<K, IndexedDiskElementDescriptor> keys =
-                keyFile.readObject( new IndexedDiskElementDescriptor( 0, (int) keyFile.length()
-                - IndexedDisk.HEADER_SIZE_BYTES ) );
+            HashMap<K, IndexedDiskElementDescriptor> keys = keyFile.readObject(new IndexedDiskElementDescriptor(0, (int) keyFile
+                .length() - IndexedDisk.HEADER_SIZE_BYTES));
 
-            if ( keys != null )
+            if (keys != null)
             {
-                if ( log.isDebugEnabled() )
+                if (log.isDebugEnabled())
                 {
-                    log.debug( logCacheName + "Found " + keys.size() + " in keys file." );
+                    log.debug(logCacheName + "Found " + keys.size() + " in keys file.");
                 }
 
-                keyHash.putAll( keys );
+                keyHash.putAll(keys);
 
-                if ( log.isInfoEnabled() )
+                if (log.isInfoEnabled())
                 {
-                    log.info( logCacheName + "Loaded keys from [" + fileName + "], key count: " + keyHash.size()
-                        + "; up to " + maxKeySize + " will be available." );
+                    log.info(logCacheName + "Loaded keys from [" + fileName + "], key count: " + keyHash.size() + "; up to "
+                        + maxKeySize + " will be available.");
                 }
             }
 
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
-                dump( false );
+                dump(false);
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Problem loading keys for file " + fileName, e );
+            log.error(logCacheName + "Problem loading keys for file " + fileName, e);
         }
         finally
         {
@@ -369,14 +372,16 @@ protected void loadKeys()
      * positions in the keys exceed the file length.
      * <p>
      * The caller should take the appropriate action if the keys and data are not consistent.
-     * @param checkForDedOverlaps if <code>true</code>, do a more thorough check by checking for
+     *
+     * @param checkForDedOverlaps
+     *            if <code>true</code>, do a more thorough check by checking for
      *            data overlap
      * @return <code>true</code> if the test passes
      */
-    private boolean checkKeyDataConsistency( boolean checkForDedOverlaps )
+    private boolean checkKeyDataConsistency(boolean checkForDedOverlaps)
     {
         ElapsedTimer timer = new ElapsedTimer();
-        log.debug( logCacheName + "Performing inital consistency check" );
+        log.debug(logCacheName + "Performing inital consistency check");
 
         boolean isOk = true;
         long fileLength = 0;
@@ -390,29 +395,28 @@ private boolean checkKeyDataConsistency( boolean checkForDedOverlaps )
 
                 isOk = ded.pos + IndexedDisk.HEADER_SIZE_BYTES + ded.len <= fileLength;
 
-                if ( !isOk )
+                if (!isOk)
                 {
-                    log.warn( logCacheName + "The dataFile is corrupted!" + "\n raf.length() = " + fileLength
-                        + "\n ded.pos = " + ded.pos );
+                    log.warn(logCacheName + "The dataFile is corrupted!" + "\n raf.length() = " + fileLength + "\n ded.pos = "
+                        + ded.pos);
                     break;
                 }
             }
 
-            if ( isOk && checkForDedOverlaps )
+            if (isOk && checkForDedOverlaps)
             {
-                isOk = checkForDedOverlaps( createPositionSortedDescriptorList() );
+                isOk = checkForDedOverlaps(createPositionSortedDescriptorList());
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( e );
+            log.error(e);
             isOk = false;
         }
 
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Finished inital consistency check, isOk = " + isOk + " in "
-                + timer.getElapsedTimeString() );
+            log.info(logCacheName + "Finished inital consistency check, isOk = " + isOk + " in " + timer.getElapsedTimeString());
         }
 
         return isOk;
@@ -423,20 +427,21 @@ private boolean checkKeyDataConsistency( boolean checkForDedOverlaps )
      * <p>
      * The total length of an item is IndexedDisk.RECORD_HEADER + ded.len.
      * <p>
+     *
      * @param sortedDescriptors
      * @return false if there are overlaps.
      */
-    protected boolean checkForDedOverlaps( IndexedDiskElementDescriptor[] sortedDescriptors )
+    protected boolean checkForDedOverlaps(IndexedDiskElementDescriptor[] sortedDescriptors)
     {
         long start = System.currentTimeMillis();
         boolean isOk = true;
         long expectedNextPos = 0;
-        for ( int i = 0; i < sortedDescriptors.length; i++ )
+        for (int i = 0; i < sortedDescriptors.length; i++)
         {
             IndexedDiskElementDescriptor ded = sortedDescriptors[i];
-            if ( expectedNextPos > ded.pos )
+            if (expectedNextPos > ded.pos)
             {
-                log.error( logCacheName + "Corrupt file: overlapping deds " + ded );
+                log.error(logCacheName + "Corrupt file: overlapping deds " + ded);
                 isOk = false;
                 break;
             }
@@ -446,9 +451,9 @@ protected boolean checkForDedOverlaps( IndexedDiskElementDescriptor[] sortedDesc
             }
         }
         long end = System.currentTimeMillis();
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Check for DED overlaps took " + ( end - start ) + " ms." );
+            log.debug(logCacheName + "Check for DED overlaps took " + (end - start) + " ms.");
         }
 
         return isOk;
@@ -461,30 +466,29 @@ protected void saveKeys()
     {
         try
         {
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Saving keys to: " + fileName + ", key count: " + keyHash.size() );
+                log.info(logCacheName + "Saving keys to: " + fileName + ", key count: " + keyHash.size());
             }
 
             keyFile.reset();
 
-            HashMap<K, IndexedDiskElementDescriptor> keys =
-                new HashMap<K, IndexedDiskElementDescriptor>();
-            keys.putAll( keyHash );
+            HashMap<K, IndexedDiskElementDescriptor> keys = new HashMap<K, IndexedDiskElementDescriptor>();
+            keys.putAll(keyHash);
 
-            if ( keys.size() > 0 )
+            if (keys.size() > 0)
             {
-                keyFile.writeObject( keys, 0 );
+                keyFile.writeObject(keys, 0);
             }
 
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Finished saving keys." );
+                log.info(logCacheName + "Finished saving keys.");
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Problem storing keys.", e );
+            log.error(logCacheName + "Problem storing keys.", e);
         }
     }
 
@@ -492,20 +496,22 @@ protected void saveKeys()
      * Update the disk cache. Called from the Queue. Makes sure the Item has not been retrieved from
      * purgatory while in queue for disk. Remove items from purgatory when they go to disk.
      * <p>
-     * @param ce The ICacheElement<K, V> to put to disk.
+     *
+     * @param ce
+     *            The ICacheElement<K, V> to put to disk.
      */
     @Override
-    protected void processUpdate( ICacheElement<K, V> ce )
+    protected void processUpdate(ICacheElement<K, V> ce)
     {
-        if ( !alive )
+        if (!alive)
         {
-            log.error( logCacheName + "No longer alive; aborting put of key = " + ce.getKey() );
+            log.error(logCacheName + "No longer alive; aborting put of key = " + ce.getKey());
             return;
         }
 
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Storing element on disk, key: " + ce.getKey() );
+            log.debug(logCacheName + "Storing element on disk, key: " + ce.getKey());
         }
 
         IndexedDiskElementDescriptor ded = null;
@@ -515,17 +521,17 @@ protected void processUpdate( ICacheElement<K, V> ce )
 
         try
         {
-            byte[] data = getElementSerializer().serialize( ce );
+            byte[] data = getElementSerializer().serialize(ce);
 
             // make sure this only locks for one particular cache region
             storageLock.writeLock().lock();
             try
             {
-                old = keyHash.get( ce.getKey() );
+                old = keyHash.get(ce.getKey());
 
                 // Item with the same key already exists in file.
                 // Try to reuse the location if possible.
-                if ( old != null && data.length <= old.len )
+                if (old != null && data.length <= old.len)
                 {
                     // Reuse the old ded. The defrag relies on ded updates by reference, not
                     // replacement.
@@ -535,93 +541,93 @@ protected void processUpdate( ICacheElement<K, V> ce )
                 else
                 {
                     // we need this to compare in the recycle bin
-                    ded = new IndexedDiskElementDescriptor( dataFile.length(), data.length );
+                    ded = new IndexedDiskElementDescriptor(dataFile.length(), data.length);
 
-                    if ( doRecycle )
+                    if (doRecycle)
                     {
-                        IndexedDiskElementDescriptor rep = recycle
-                            .takeNearestLargerOrEqual( ded );
-                        if ( rep != null )
+                        IndexedDiskElementDescriptor rep = recycle.takeNearestLargerOrEqual(ded);
+                        if (rep != null)
                         {
                             ded = rep;
                             ded.len = data.length;
                             recycleCnt++;
-                            this.adjustBytesFree( ded, false );
-                            if ( log.isDebugEnabled() )
+                            this.adjustBytesFree(ded, false);
+                            if (log.isDebugEnabled())
                             {
-                                log.debug( logCacheName + "using recycled ded " + ded.pos + " rep.len = " + rep.len
-                                    + " ded.len = " + ded.len );
+                                log.debug(logCacheName + "using recycled ded " + ded.pos + " rep.len = " + rep.len + " ded.len = "
+                                    + ded.len);
                             }
                         }
                     }
 
                     // Put it in the map
-                    keyHash.put( ce.getKey(), ded );
+                    keyHash.put(ce.getKey(), ded);
 
-                    if ( queueInput )
+                    if (queueInput)
                     {
-                        queuedPutList.add( ded );
-                        if ( log.isDebugEnabled() )
+                        queuedPutList.add(ded);
+                        if (log.isDebugEnabled())
                         {
-                            log.debug( logCacheName + "added to queued put list." + queuedPutList.size() );
+                            log.debug(logCacheName + "added to queued put list." + queuedPutList.size());
                         }
                     }
 
                     // add the old slot to the recycle bin
-                    if ( old != null )
+                    if (old != null)
                     {
-                        addToRecycleBin( old );
+                        addToRecycleBin(old);
                     }
                 }
 
-                dataFile.write( ded, data );
+                dataFile.write(ded, data);
             }
             finally
             {
                 storageLock.writeLock().unlock();
             }
 
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Put to file: " + fileName + ", key: " + ce.getKey() + ", position: "
-                    + ded.pos + ", size: " + ded.len );
+                log.debug(logCacheName + "Put to file: " + fileName + ", key: " + ce.getKey() + ", position: " + ded.pos
+                    + ", size: " + ded.len);
             }
         }
-        catch ( ConcurrentModificationException cme )
+        catch (ConcurrentModificationException cme)
         {
             // do nothing, this means it has gone back to memory mid
             // serialization
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
                 // this shouldn't be possible
-                log.debug( logCacheName + "Caught ConcurrentModificationException." + cme );
+                log.debug(logCacheName + "Caught ConcurrentModificationException." + cme);
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Failure updating element, key: " + ce.getKey() + " old: " + old, e );
+            log.error(logCacheName + "Failure updating element, key: " + ce.getKey() + " old: " + old, e);
         }
     }
 
     /**
      * Gets the key, then goes to disk to get the object.
      * <p>
+     *
      * @param key
      * @return ICacheElement<K, V> or null
      * @see AbstractDiskCache#doGet
      */
     @Override
-    protected ICacheElement<K, V> processGet( K key )
+    protected ICacheElement<K, V> processGet(K key)
     {
-        if ( !alive )
+        if (!alive)
         {
-            log.error( logCacheName + "No longer alive so returning null for key = " + key );
+            log.error(logCacheName + "No longer alive so returning null for key = " + key);
             return null;
         }
 
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Trying to get from disk: " + key );
+            log.debug(logCacheName + "Trying to get from disk: " + key);
         }
 
         ICacheElement<K, V> object = null;
@@ -630,26 +636,26 @@ protected ICacheElement<K, V> processGet( K key )
             storageLock.readLock().lock();
             try
             {
-                object = readElement( key );
+                object = readElement(key);
             }
             finally
             {
                 storageLock.readLock().unlock();
             }
 
-            if ( object != null )
+            if (object != null)
             {
                 hitCount.incrementAndGet();
             }
         }
-        catch ( IOException ioe )
+        catch (IOException ioe)
         {
-            log.error( logCacheName + "Failure getting from disk, key = " + key, ioe );
+            log.error(logCacheName + "Failure getting from disk, key = " + key, ioe);
             reset();
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Failure getting from disk, key = " + key, e );
+            log.error(logCacheName + "Failure getting from disk, key = " + key, e);
         }
         return object;
     }
@@ -657,12 +663,13 @@ protected ICacheElement<K, V> processGet( K key )
     /**
      * Gets matching items from the cache.
      * <p>
+     *
      * @param pattern
      * @return a map of K key to ICacheElement<K, V> element, or an empty map if there is no
      *         data in cache matching keys
      */
     @Override
-    public Map<K, ICacheElement<K, V>> processGetMatching( String pattern )
+    public Map<K, ICacheElement<K, V>> processGetMatching(String pattern)
     {
         Map<K, ICacheElement<K, V>> elements = new HashMap<K, ICacheElement<K, V>>();
         try
@@ -678,20 +685,20 @@ public Map<K, ICacheElement<K, V>> processGetMatching( String pattern )
                 storageLock.readLock().unlock();
             }
 
-            Set<K> matchingKeys = getKeyMatcher().getMatchingKeysFromArray( pattern, keyArray );
+            Set<K> matchingKeys = getKeyMatcher().getMatchingKeysFromArray(pattern, keyArray);
 
             for (K key : matchingKeys)
             {
-                ICacheElement<K, V> element = processGet( key );
-                if ( element != null )
+                ICacheElement<K, V> element = processGet(key);
+                if (element != null)
                 {
-                    elements.put( key, element );
+                    elements.put(key, element);
                 }
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Failure getting matching from disk, pattern = " + pattern, e );
+            log.error(logCacheName + "Failure getting matching from disk, pattern = " + pattern, e);
         }
         return elements;
     }
@@ -699,38 +706,38 @@ public Map<K, ICacheElement<K, V>> processGetMatching( String pattern )
     /**
      * Reads the item from disk.
      * <p>
+     *
      * @param key
      * @return ICacheElement
      * @throws IOException
      */
-    private ICacheElement<K, V> readElement( K key )
-        throws IOException
+    private ICacheElement<K, V> readElement(K key) throws IOException
     {
         ICacheElement<K, V> object = null;
 
-        IndexedDiskElementDescriptor ded = keyHash.get( key );
+        IndexedDiskElementDescriptor ded = keyHash.get(key);
 
-        if ( ded != null )
+        if (ded != null)
         {
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Found on disk, key: " + key );
+                log.debug(logCacheName + "Found on disk, key: " + key);
             }
             try
             {
-                ICacheElement<K, V> readObject = dataFile.readObject( ded );
+                ICacheElement<K, V> readObject = dataFile.readObject(ded);
                 object = readObject;
                 // TODO consider checking key equality and throwing if there is a failure
             }
-            catch ( IOException e )
+            catch (IOException e)
             {
-                log.error( logCacheName + "IO Exception, Problem reading object from file", e );
+                log.error(logCacheName + "IO Exception, Problem reading object from file", e);
                 throw e;
             }
-            catch ( Exception e )
+            catch (Exception e)
             {
-                log.error( logCacheName + "Exception, Problem reading object from file", e );
-                throw new IOException( logCacheName + "Problem reading object from disk. " + e.getMessage() );
+                log.error(logCacheName + "Exception, Problem reading object from file", e);
+                throw new IOException(logCacheName + "Problem reading object from disk. " + e.getMessage());
             }
         }
 
@@ -740,6 +747,7 @@ private ICacheElement<K, V> readElement( K key )
     /**
      * Return the keys in this cache.
      * <p>
+     *
      * @see org.apache.commons.jcs.auxiliary.disk.AbstractDiskCache#getKeySet()
      */
     @Override
@@ -765,19 +773,20 @@ public Set<K> getKeySet() throws IOException
      * Returns true if the removal was successful; or false if there is nothing to remove. Current
      * implementation always result in a disk orphan.
      * <p>
+     *
      * @return true if at least one item was removed.
      * @param key
      */
     @Override
-    protected boolean processRemove( K key )
+    protected boolean processRemove(K key)
     {
-        if ( !alive )
+        if (!alive)
         {
-            log.error( logCacheName + "No longer alive so returning false for key = " + key );
+            log.error(logCacheName + "No longer alive so returning false for key = " + key);
             return false;
         }
 
-        if ( key == null )
+        if (key == null)
         {
             return false;
         }
@@ -788,22 +797,22 @@ protected boolean processRemove( K key )
         {
             storageLock.writeLock().lock();
 
-            if ( key instanceof String && key.toString().endsWith( CacheConstants.NAME_COMPONENT_DELIMITER ) )
+            if (key instanceof String && key.toString().endsWith(CacheConstants.NAME_COMPONENT_DELIMITER))
             {
-                removed = performPartialKeyRemoval( (String) key );
+                removed = performPartialKeyRemoval((String) key);
             }
-            else if ( key instanceof GroupAttrName && ((GroupAttrName<?>)key).attrName == null )
+            else if (key instanceof GroupAttrName && ((GroupAttrName<?>) key).attrName == null)
             {
-                removed = performGroupRemoval( ((GroupAttrName<?>)key).groupId );
+                removed = performGroupRemoval(((GroupAttrName<?>) key).groupId);
             }
             else
             {
-                removed = performSingleKeyRemoval( key );
+                removed = performSingleKeyRemoval(key);
             }
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Problem removing element.", e );
+            log.error(logCacheName + "Problem removing element.", e);
             reset = true;
         }
         finally
@@ -811,14 +820,14 @@ else if ( key instanceof GroupAttrName && ((GroupAttrName<?>)key).attrName == nu
             storageLock.writeLock().unlock();
         }
 
-        if ( reset )
+        if (reset)
         {
             reset();
         }
 
         // this increments the remove count.
         // there is no reason to call this if an item was not removed.
-        if ( removed )
+        if (removed)
         {
             doOptimizeRealTime();
         }
@@ -832,10 +841,11 @@ else if ( key instanceof GroupAttrName && ((GroupAttrName<?>)key).attrName == nu
      * <p>
      * This operates under a lock obtained in doRemove().
      * <p>
+     *
      * @param key
      * @return true if there was a match
      */
-    private boolean performPartialKeyRemoval( String key )
+    private boolean performPartialKeyRemoval(String key)
     {
         boolean removed = false;
 
@@ -844,9 +854,9 @@ private boolean performPartialKeyRemoval( String key )
 
         for (K k : keyHash.keySet())
         {
-            if ( k instanceof String && k.toString().startsWith( key.toString() ) )
+            if (k instanceof String && k.toString().startsWith(key.toString()))
             {
-                itemsToRemove.add( k );
+                itemsToRemove.add(k);
             }
         }
 
@@ -855,7 +865,7 @@ private boolean performPartialKeyRemoval( String key )
         {
             // Don't add to recycle bin here
             // https://issues.apache.org/jira/browse/JCS-67
-            performSingleKeyRemoval( fullKey );
+            performSingleKeyRemoval(fullKey);
             removed = true;
             // TODO this needs to update the remove count separately
         }
@@ -869,10 +879,11 @@ private boolean performPartialKeyRemoval( String key )
      * <p>
      * This operates under a lock obtained in doRemove().
      * <p>
+     *
      * @param key
      * @return true if an element was removed
      */
-    private boolean performGroupRemoval( GroupId key )
+    private boolean performGroupRemoval(GroupId key)
     {
         boolean removed = false;
 
@@ -882,9 +893,9 @@ private boolean performGroupRemoval( GroupId key )
         // remove all keys of the same name hierarchy.
         for (K k : keyHash.keySet())
         {
-            if ( k instanceof GroupAttrName && ( (GroupAttrName<?>) k ).groupId.equals( key ) )
+            if (k instanceof GroupAttrName && ((GroupAttrName<?>) k).groupId.equals(key))
             {
-                itemsToRemove.add( k );
+                itemsToRemove.add(k);
             }
         }
 
@@ -893,7 +904,7 @@ private boolean performGroupRemoval( GroupId key )
         {
             // Don't add to recycle bin here
             // https://issues.apache.org/jira/browse/JCS-67
-            performSingleKeyRemoval( fullKey );
+            performSingleKeyRemoval(fullKey);
             removed = true;
             // TODO this needs to update the remove count separately
         }
@@ -906,20 +917,21 @@ private boolean performGroupRemoval( GroupId key )
      * <p>
      * This operates under a lock obtained in doRemove().
      * <p>
+     *
      * @param key
      * @return true if an item was removed.
      */
-    private boolean performSingleKeyRemoval( K key )
+    private boolean performSingleKeyRemoval(K key)
     {
         boolean removed;
         // remove single item.
-        IndexedDiskElementDescriptor ded = keyHash.remove( key );
+        IndexedDiskElementDescriptor ded = keyHash.remove(key);
         removed = ded != null;
-        addToRecycleBin( ded );
+        addToRecycleBin(ded);
 
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Disk removal: Removed from key hash, key [" + key + "] removed = " + removed );
+            log.debug(logCacheName + "Disk removal: Removed from key hash, key [" + key + "] removed = " + removed);
         }
         return removed;
     }
@@ -930,19 +942,19 @@ private boolean performSingleKeyRemoval( K key )
     @Override
     public void processRemoveAll()
     {
-        ICacheEvent<String> cacheEvent = createICacheEvent( cacheName, "all", ICacheEventLogger.REMOVEALL_EVENT );
+        ICacheEvent<String> cacheEvent = createICacheEvent(cacheName, "all", ICacheEventLogger.REMOVEALL_EVENT);
         try
         {
             reset();
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( logCacheName + "Problem removing all.", e );
+            log.error(logCacheName + "Problem removing all.", e);
             reset();
         }
         finally
         {
-            logICacheEvent( cacheEvent );
+            logICacheEvent(cacheEvent);
         }
     }
 
@@ -953,47 +965,47 @@ public void processRemoveAll()
      */
     private void reset()
     {
-        if ( log.isWarnEnabled() )
+        if (log.isWarnEnabled())
         {
-            log.warn( logCacheName + "Reseting cache" );
+            log.warn(logCacheName + "Reseting cache");
         }
 
         try
         {
             storageLock.writeLock().lock();
 
-            if ( dataFile != null )
+            if (dataFile != null)
             {
                 dataFile.close();
             }
-            File dataFileTemp = new File( rafDir, fileName + ".data" );
+            File dataFileTemp = new File(rafDir, fileName + ".data");
             boolean result = dataFileTemp.delete();
             if (!result && log.isDebugEnabled())
             {
                 log.debug("Could not delete file " + dataFileTemp);
             }
 
-            if ( keyFile != null )
+            if (keyFile != null)
             {
                 keyFile.close();
             }
-            File keyFileTemp = new File( rafDir, fileName + ".key" );
+            File keyFileTemp = new File(rafDir, fileName + ".key");
             result = keyFileTemp.delete();
             if (!result && log.isDebugEnabled())
             {
                 log.debug("Could not delete file " + keyFileTemp);
             }
 
-            dataFile = new IndexedDisk( new File( rafDir, fileName + ".data" ), getElementSerializer() );
-            keyFile = new IndexedDisk( new File( rafDir, fileName + ".key" ), getElementSerializer() );
+            dataFile = new IndexedDisk(new File(rafDir, fileName + ".data"), getElementSerializer());
+            keyFile = new IndexedDisk(new File(rafDir, fileName + ".key"), getElementSerializer());
 
             initializeRecycleBin();
 
             initializeKeyMap();
         }
-        catch ( IOException e )
+        catch (IOException e)
         {
-            log.error( logCacheName + "Failure reseting state", e );
+            log.error(logCacheName + "Failure reseting state", e);
         }
         finally
         {
@@ -1008,10 +1020,10 @@ private void reset()
     private void initializeRecycleBin()
     {
         int recycleBinSize = cattr.getMaxRecycleBinSize() >= 0 ? cattr.getMaxRecycleBinSize() : 0;
-        recycle = new SortedPreferentialArray<IndexedDiskElementDescriptor>( recycleBinSize );
-        if ( log.isDebugEnabled() )
+        recycle = new SortedPreferentialArray<IndexedDiskElementDescriptor>(recycleBinSize);
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "Set recycle max Size to MaxRecycleBinSize: '" + recycleBinSize + "'" );
+            log.debug(logCacheName + "Set recycle max Size to MaxRecycleBinSize: '" + recycleBinSize + "'");
         }
     }
 
@@ -1021,17 +1033,20 @@ private void initializeRecycleBin()
     private void initializeKeyMap()
     {
         keyHash = null;
-        if ( maxKeySize >= 0 )
+        if (maxKeySize >= 0)
         {
-            if (this.diskLimitType.equals(DiskLimitType.COUNT)) {
-                keyHash = new LRUMapCountLimited( maxKeySize );
-            } else {
+            if (this.diskLimitType.equals(DiskLimitType.COUNT))
+            {
+                keyHash = new LRUMapCountLimited(maxKeySize);
+            }
+            else
+            {
                 keyHash = new LRUMapSizeLimited(maxKeySize);
             }
 
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Set maxKeySize to: '" + maxKeySize + "'" );
+                log.info(logCacheName + "Set maxKeySize to: '" + maxKeySize + "'");
             }
         }
         else
@@ -1039,9 +1054,9 @@ private void initializeKeyMap()
             // If no max size, use a plain map for memory and processing efficiency.
             keyHash = new HashMap<K, IndexedDiskElementDescriptor>();
             // keyHash = Collections.synchronizedMap( new HashMap() );
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Set maxKeySize to unlimited'" );
+                log.info(logCacheName + "Set maxKeySize to unlimited'");
             }
         }
     }
@@ -1055,7 +1070,7 @@ private void initializeKeyMap()
     @Override
     public void processDispose()
     {
-        ICacheEvent<String> cacheEvent = createICacheEvent( cacheName, "none", ICacheEventLogger.DISPOSE_EVENT );
+        ICacheEvent<String> cacheEvent = createICacheEvent(cacheName, "none", ICacheEventLogger.DISPOSE_EVENT);
         try
         {
             Runnable disR = new Runnable()
@@ -1066,21 +1081,21 @@ public void run()
                     disposeInternal();
                 }
             };
-            Thread t = new Thread( disR, "IndexedDiskCache-DisposalThread" );
+            Thread t = new Thread(disR, "IndexedDiskCache-DisposalThread");
             t.start();
             // wait up to 60 seconds for dispose and then quit if not done.
             try
             {
-                t.join( 60 * 1000 );
+                t.join(60 * 1000);
             }
-            catch ( InterruptedException ex )
+            catch (InterruptedException ex)
             {
-                log.error( logCacheName + "Interrupted while waiting for disposal thread to finish.", ex );
+                log.error(logCacheName + "Interrupted while waiting for disposal thread to finish.", ex);
             }
         }
         finally
         {
-            logICacheEvent( cacheEvent );
+            logICacheEvent(cacheEvent);
         }
     }
 
@@ -1089,9 +1104,9 @@ public void run()
      */
     protected void disposeInternal()
     {
-        if ( !alive )
+        if (!alive)
         {
-            log.error( logCacheName + "Not alive and dispose was called, filename: " + fileName );
+            log.error(logCacheName + "Not alive and dispose was called, filename: " + fileName);
             return;
         }
 
@@ -1099,23 +1114,23 @@ protected void disposeInternal()
         alive = false;
 
         Thread optimizationThread = currentOptimizationThread;
-        if ( isRealTimeOptimizationEnabled && optimizationThread != null )
+        if (isRealTimeOptimizationEnabled && optimizationThread != null)
         {
             // Join with the current optimization thread.
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "In dispose, optimization already " + "in progress; waiting for completion." );
+                log.debug(logCacheName + "In dispose, optimization already " + "in progress; waiting for completion.");
             }
             try
             {
                 optimizationThread.join();
             }
-            catch ( InterruptedException e )
+            catch (InterruptedException e)
             {
-                log.error( logCacheName + "Unable to join current optimization thread.", e );
+                log.error(logCacheName + "Unable to join current optimization thread.", e);
             }
         }
-        else if ( isShutdownOptimizationEnabled && this.getBytesFree() > 0 )
+        else if (isShutdownOptimizationEnabled && this.getBytesFree() > 0)
         {
             optimizeFile();
         }
@@ -1124,23 +1139,23 @@ else if ( isShutdownOptimizationEnabled && this.getBytesFree() > 0 )
 
         try
         {
-            if ( log.isDebugEnabled() )
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Closing files, base filename: " + fileName );
+                log.debug(logCacheName + "Closing files, base filename: " + fileName);
             }
             dataFile.close();
             dataFile = null;
             keyFile.close();
             keyFile = null;
         }
-        catch ( IOException e )
+        catch (IOException e)
         {
-            log.error( logCacheName + "Failure closing files in dispose, filename: " + fileName, e );
+            log.error(logCacheName + "Failure closing files in dispose, filename: " + fileName, e);
         }
 
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Shutdown complete." );
+            log.info(logCacheName + "Shutdown complete.");
         }
     }
 
@@ -1148,28 +1163,28 @@ else if ( isShutdownOptimizationEnabled && this.getBytesFree() > 0 )
      * Add descriptor to recycle bin if it is not null. Adds the length of the item to the bytes
      * free.
      * <p>
-     * This is called in three places: (1) When an item is removed. All item removals funnel down to
-     * the removeSingleItem method. (2) When an item on disk is updated with a value that will not
-     * fit in the previous slot. (3) When the max key size is reached, the freed slot will be added.
+     * This is called in three places: (1) When an item is removed. All item removals funnel down to the removeSingleItem method.
+     * (2) When an item on disk is updated with a value that will not fit in the previous slot. (3) When the max key size is
+     * reached, the freed slot will be added.
      * <p>
-     * The recylebin is not a set. If a slot it added twice, it will result in the wrong data being
-     * returned.
+     * The recylebin is not a set. If a slot it added twice, it will result in the wrong data being returned.
      * <p>
+     *
      * @param ded
      */
-    protected void addToRecycleBin( IndexedDiskElementDescriptor ded )
+    protected void addToRecycleBin(IndexedDiskElementDescriptor ded)
     {
         // reuse the spot
-        if ( ded != null )
+        if (ded != null)
         {
-            this.adjustBytesFree( ded, true );
+            this.adjustBytesFree(ded, true);
 
-            if ( doRecycle )
+            if (doRecycle)
             {
-                recycle.add( ded );
-                if ( log.isDebugEnabled() )
+                recycle.add(ded);
+                if (log.isDebugEnabled())
                 {
-                    log.debug( logCacheName + "recycled ded" + ded );
+                    log.debug(logCacheName + "recycled ded" + ded);
                 }
 
             }
@@ -1181,25 +1196,25 @@ protected void addToRecycleBin( IndexedDiskElementDescriptor ded )
      */
     protected void doOptimizeRealTime()
     {
-        if ( isRealTimeOptimizationEnabled && !isOptimizing && removeCount++ >= cattr.getOptimizeAtRemoveCount() )
+        if (isRealTimeOptimizationEnabled && !isOptimizing && removeCount++ >= cattr.getOptimizeAtRemoveCount())
         {
             isOptimizing = true;
 
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Optimizing file. removeCount [" + removeCount + "] OptimizeAtRemoveCount ["
-                    + cattr.getOptimizeAtRemoveCount() + "]" );
+                log.info(logCacheName + "Optimizing file. removeCount [" + removeCount + "] OptimizeAtRemoveCount ["
+                    + cattr.getOptimizeAtRemoveCount() + "]");
             }
 
-            if ( currentOptimizationThread == null )
+            if (currentOptimizationThread == null)
             {
                 storageLock.writeLock().lock();
 
                 try
                 {
-                    if ( currentOptimizationThread == null )
+                    if (currentOptimizationThread == null)
                     {
-                        currentOptimizationThread = new Thread( new Runnable()
+                        currentOptimizationThread = new Thread(new Runnable()
                         {
                             @Override
                             public void run()
@@ -1208,7 +1223,7 @@ public void run()
 
                                 currentOptimizationThread = null;
                             }
-                        }, "IndexedDiskCache-OptimizationThread" );
+                        }, "IndexedDiskCache-OptimizationThread");
                     }
                 }
                 finally
@@ -1216,7 +1231,7 @@ public void run()
                     storageLock.writeLock().unlock();
                 }
 
-                if ( currentOptimizationThread != null )
+                if (currentOptimizationThread != null)
                 {
                     currentOptimizationThread.start();
                 }
@@ -1227,26 +1242,30 @@ public void run()
     /**
      * File optimization is handled by this method. It works as follows:
      * <ol>
-     * <li>Shutdown recycling and turn on queuing of puts. </li> <li>Take a snapshot of the current
-     * descriptors. If there are any removes, ignore them, as they will be compacted during the next
-     * optimization.</li> <li>Optimize the snapshot. For each descriptor:
+     * <li>Shutdown recycling and turn on queuing of puts.</li>
+     * <li>Take a snapshot of the current descriptors. If there are any removes, ignore them, as they will be compacted during the
+     * next optimization.</li>
+     * <li>Optimize the snapshot. For each descriptor:
      * <ol>
-     * <li>Obtain the write-lock.</li> <li>Shift the element on the disk, in order to compact out
-     * the free space. </li> <li>Release the write-lock. This allows elements to still be accessible
-     * during optimization.</li>
+     * <li>Obtain the write-lock.</li>
+     * <li>Shift the element on the disk, in order to compact out the free space.</li>
+     * <li>Release the write-lock. This allows elements to still be accessible during optimization.</li>
      * </ol>
-     * </li> <li>Obtain the write-lock.</li> <li>All queued puts are made at the end of the file.
-     * Optimize these under a single write-lock.</li> <li>Truncate the file.</li> <li>Release the
-     * write-lock. </li> <li>Restore system to standard operation.</li>
+     * </li>
+     * <li>Obtain the write-lock.</li>
+     * <li>All queued puts are made at the end of the file. Optimize these under a single write-lock.</li>
+     * <li>Truncate the file.</li>
+     * <li>Release the write-lock.</li>
+     * <li>Restore system to standard operation.</li>
      * </ol>
      */
     protected void optimizeFile()
     {
         ElapsedTimer timer = new ElapsedTimer();
         timesOptimized++;
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Beginning Optimization #" + timesOptimized );
+            log.info(logCacheName + "Beginning Optimization #" + timesOptimized);
         }
 
         // CREATE SNAPSHOT
@@ -1269,7 +1288,7 @@ protected void optimizeFile()
 
         // Defrag the file outside of the write lock. This allows a move to be made,
         // and yet have the element still accessible for reading or writing.
-        long expectedNextPos = defragFile( defragList, 0 );
+        long expectedNextPos = defragFile(defragList, 0);
 
         // ADD THE QUEUED ITEMS to the end and then truncate
         storageLock.writeLock().lock();
@@ -1278,22 +1297,22 @@ protected void optimizeFile()
         {
             try
             {
-                if ( !queuedPutList.isEmpty() )
+                if (!queuedPutList.isEmpty())
                 {
                     // This is perhaps unnecessary, but the list might not be as sorted as we think.
                     defragList = new IndexedDiskElementDescriptor[queuedPutList.size()];
-                    queuedPutList.toArray( defragList );
-                    Arrays.sort( defragList, new PositionComparator() );
+                    queuedPutList.toArray(defragList);
+                    Arrays.sort(defragList, new PositionComparator());
 
                     // pack them at the end
-                    expectedNextPos = defragFile( defragList, expectedNextPos );
+                    expectedNextPos = defragFile(defragList, expectedNextPos);
                 }
                 // TRUNCATE THE FILE
-                dataFile.truncate( expectedNextPos );
+                dataFile.truncate(expectedNextPos);
             }
-            catch ( Exception e )
+            catch (Exception e)
             {
-                log.error( logCacheName + "Error optimizing queued puts.", e );
+                log.error(logCacheName + "Error optimizing queued puts.", e);
             }
 
             // RESTORE NORMAL OPERATION
@@ -1311,10 +1330,9 @@ protected void optimizeFile()
             storageLock.writeLock().unlock();
         }
 
-        if ( log.isInfoEnabled() )
+        if (log.isInfoEnabled())
         {
-            log.info( logCacheName + "Finished #" + timesOptimized + " Optimization took "
-                + timer.getElapsedTimeString() );
+            log.info(logCacheName + "Finished #" + timesOptimized + " Optimization took " + timer.getElapsedTimeString());
         }
     }
 
@@ -1323,11 +1341,14 @@ protected void optimizeFile()
      * forward). If there were no gaps the resulting file would be the same size as the previous
      * file. This must be supplied an ordered defragList.
      * <p>
-     * @param defragList sorted list of descriptors for optimization
-     * @param startingPos the start position in the file
+     *
+     * @param defragList
+     *            sorted list of descriptors for optimization
+     * @param startingPos
+     *            the start position in the file
      * @return this is the potential new file end
      */
-    private long defragFile( IndexedDiskElementDescriptor[] defragList, long startingPos )
+    private long defragFile(IndexedDiskElementDescriptor[] defragList, long startingPos)
     {
         ElapsedTimer timer = new ElapsedTimer();
         long preFileSize = 0;
@@ -1338,14 +1359,14 @@ private long defragFile( IndexedDiskElementDescriptor[] defragList, long startin
             preFileSize = this.dataFile.length();
             // find the first gap in the disk and start defragging.
             expectedNextPos = startingPos;
-            for ( int i = 0; i < defragList.length; i++ )
+            for (int i = 0; i < defragList.length; i++)
             {
                 storageLock.writeLock().lock();
                 try
                 {
-                    if ( expectedNextPos != defragList[i].pos )
+                    if (expectedNextPos != defragList[i].pos)
                     {
-                        dataFile.move( defragList[i], expectedNextPos );
+                        dataFile.move(defragList[i], expectedNextPos);
                     }
                     expectedNextPos = defragList[i].pos + IndexedDisk.HEADER_SIZE_BYTES + defragList[i].len;
                 }
@@ -1360,17 +1381,16 @@ private long defragFile( IndexedDiskElementDescriptor[] defragList, long startin
             // this is the potential new file end
             return expectedNextPos;
         }
-        catch ( IOException e )
+        catch (IOException e)
         {
-            log.error( logCacheName + "Error occurred during defragmentation.", e );
+            log.error(logCacheName + "Error occurred during defragmentation.", e);
         }
         finally
         {
-            if ( log.isInfoEnabled() )
+            if (log.isInfoEnabled())
             {
-                log.info( logCacheName + "Defragmentation took " + timer.getElapsedTimeString()
-                    + ". File Size (before=" + preFileSize + ") (after=" + postFileSize + ") (truncating to "
-                    + expectedNextPos + ")" );
+                log.info(logCacheName + "Defragmentation took " + timer.getElapsedTimeString() + ". File Size (before="
+                    + preFileSize + ") (after=" + postFileSize + ") (truncating to " + expectedNextPos + ")");
             }
         }
 
@@ -1383,19 +1403,20 @@ private long defragFile( IndexedDiskElementDescriptor[] defragList, long startin
      * <p>
      * TODO fix values() method on the LRU map.
      * <p>
+     *
      * @return IndexedDiskElementDescriptor[]
      */
     private IndexedDiskElementDescriptor[] createPositionSortedDescriptorList()
     {
         IndexedDiskElementDescriptor[] defragList = new IndexedDiskElementDescriptor[keyHash.size()];
         Iterator<Map.Entry<K, IndexedDiskElementDescriptor>> iterator = keyHash.entrySet().iterator();
-        for ( int i = 0; iterator.hasNext(); i++ )
+        for (int i = 0; iterator.hasNext(); i++)
         {
             Map.Entry<K, IndexedDiskElementDescriptor> next = iterator.next();
             defragList[i] = next.getValue();
         }
 
-        Arrays.sort( defragList, new PositionComparator() );
+        Arrays.sort(defragList, new PositionComparator());
 
         return defragList;
     }
@@ -1403,6 +1424,7 @@ private IndexedDiskElementDescriptor[] createPositionSortedDescriptorList()
     /**
      * Returns the current cache size.
      * <p>
+     *
      * @return The size value
      */
     @Override
@@ -1414,6 +1436,7 @@ public int getSize()
     /**
      * Returns the size of the recyclebin in number of elements.
      * <p>
+     *
      * @return The number of items in the bin.
      */
     protected int getRecyleBinSize()
@@ -1424,6 +1447,7 @@ protected int getRecyleBinSize()
     /**
      * Returns the number of times we have used spots from the recycle bin.
      * <p>
+     *
      * @return The number of spots used.
      */
     protected int getRecyleCount()
@@ -1435,6 +1459,7 @@ protected int getRecyleCount()
      * Returns the number of bytes that are free. When an item is removed, its length is recorded.
      * When a spot is used form the recycle bin, the length of the item stored is recorded.
      * <p>
+     *
      * @return The number bytes free on the disk file.
      */
     protected synchronized long getBytesFree()
@@ -1453,16 +1478,17 @@ private synchronized void resetBytesFree()
     /**
      * To subtract you can pass in false for add..
      * <p>
+     *
      * @param ded
      * @param add
      */
-    private synchronized void adjustBytesFree( IndexedDiskElementDescriptor ded, boolean add )
+    private synchronized void adjustBytesFree(IndexedDiskElementDescriptor ded, boolean add)
     {
-        if ( ded != null )
+        if (ded != null)
         {
             int amount = ded.len + IndexedDisk.HEADER_SIZE_BYTES;
 
-            if ( add )
+            if (add)
             {
                 this.bytesFree += amount;
             }
@@ -1476,11 +1502,11 @@ private synchronized void adjustBytesFree( IndexedDiskElementDescriptor ded, boo
     /**
      * This is for debugging and testing.
      * <p>
+     *
      * @return the length of the data file.
      * @throws IOException
      */
-    protected long getDataFileSize()
-        throws IOException
+    protected long getDataFileSize() throws IOException
     {
         long size = 0;
 
@@ -1488,7 +1514,7 @@ protected long getDataFileSize()
 
         try
         {
-            if ( dataFile != null )
+            if (dataFile != null)
             {
                 size = dataFile.length();
             }
@@ -1506,27 +1532,29 @@ protected long getDataFileSize()
      */
     public void dump()
     {
-        dump( true );
+        dump(true);
     }
 
     /**
      * For debugging.
      * <p>
-     * @param dumpValues A boolean indicating if values should be dumped.
+     *
+     * @param dumpValues
+     *            A boolean indicating if values should be dumped.
      */
-    public void dump( boolean dumpValues )
+    public void dump(boolean dumpValues)
     {
-        if ( log.isDebugEnabled() )
+        if (log.isDebugEnabled())
         {
-            log.debug( logCacheName + "[dump] Number of keys: " + keyHash.size() );
+            log.debug(logCacheName + "[dump] Number of keys: " + keyHash.size());
 
             for (Map.Entry<K, IndexedDiskElementDescriptor> e : keyHash.entrySet())
             {
                 K key = e.getKey();
                 IndexedDiskElementDescriptor ded = e.getValue();
 
-                log.debug( logCacheName + "[dump] Disk element, key: " + key + ", pos: " + ded.pos + ", ded.len"
-                    + ded.len + ( dumpValues ? ", val: " + get( key ) : "" ) );
+                log.debug(logCacheName + "[dump] Disk element, key: " + key + ", pos: " + ded.pos + ", ded.len" + ded.len
+                    + (dumpValues ? ", val: " + get(key) : ""));
             }
         }
     }
@@ -1543,6 +1571,7 @@ public AuxiliaryCacheAttributes getAuxiliaryCacheAttributes()
     /**
      * Gets basic stats for the disk cache.
      * <p>
+     *
      * @return String
      */
     @Override
@@ -1555,42 +1584,42 @@ public String getStats()
      * Returns info about the disk cache.
      * <p>
      * (non-Javadoc)
+     *
      * @see org.apache.commons.jcs.auxiliary.AuxiliaryCache#getStatistics()
      */
     @Override
     public synchronized IStats getStatistics()
     {
         IStats stats = new Stats();
-        stats.setTypeName( "Indexed Disk Cache" );
+        stats.setTypeName("Indexed Disk Cache");
 
         ArrayList<IStatElement<?>> elems = new ArrayList<IStatElement<?>>();
 
-        elems.add(new StatElement<Boolean>( "Is Alive", Boolean.valueOf(alive) ) );
-        elems.add(new StatElement<Integer>( "Key Map Size",
-                Integer.valueOf(this.keyHash != null ? this.keyHash.size() : -1) ) );
+        elems.add(new StatElement<Boolean>("Is Alive", Boolean.valueOf(alive)));
+        elems.add(new StatElement<Integer>("Key Map Size", Integer.valueOf(this.keyHash != null ? this.keyHash.size() : -1)));
         try
         {
-            elems.add(new StatElement<Long>( "Data File Length",
-                    Long.valueOf(this.dataFile != null ? this.dataFile.length() : -1L) ) );
+            elems
+                .add(new StatElement<Long>("Data File Length", Long.valueOf(this.dataFile != null ? this.dataFile.length() : -1L)));
         }
-        catch ( Exception e )
+        catch (Exception e)
         {
-            log.error( e );
+            log.error(e);
         }
-        elems.add(new StatElement<Integer>( "Max Key Size", this.maxKeySize));
-        elems.add(new StatElement<Integer>( "Hit Count", Integer.valueOf(this.hitCount.get()) ) );
-        elems.add(new StatElement<Long>( "Bytes Free", Long.valueOf(this.bytesFree) ) );
-        elems.add(new StatElement<Integer>( "Optimize Operation Count", Integer.valueOf(this.removeCount) ) );
-        elems.add(new StatElement<Integer>( "Times Optimized", Integer.valueOf(this.timesOptimized) ) );
-        elems.add(new StatElement<Integer>( "Recycle Count", Integer.valueOf(this.recycleCnt) ) );
-        elems.add(new StatElement<Integer>( "Recycle Bin Size", Integer.valueOf(this.recycle.size()) ) );
-        elems.add(new StatElement<Integer>( "Startup Size", Integer.valueOf(this.startupSize) ) );
+        elems.add(new StatElement<Integer>("Max Key Size", this.maxKeySize));
+        elems.add(new StatElement<Integer>("Hit Count", Integer.valueOf(this.hitCount.get())));
+        elems.add(new StatElement<Long>("Bytes Free", Long.valueOf(this.bytesFree)));
+        elems.add(new StatElement<Integer>("Optimize Operation Count", Integer.valueOf(this.removeCount)));
+        elems.add(new StatElement<Integer>("Times Optimized", Integer.valueOf(this.timesOptimized)));
+        elems.add(new StatElement<Integer>("Recycle Count", Integer.valueOf(this.recycleCnt)));
+        elems.add(new StatElement<Integer>("Recycle Bin Size", Integer.valueOf(this.recycle.size())));
+        elems.add(new StatElement<Integer>("Startup Size", Integer.valueOf(this.startupSize)));
 
         // get the stats from the super too
         IStats sStats = super.getStatistics();
         elems.addAll(sStats.getStatElements());
 
-        stats.setStatElements( elems );
+        stats.setStatElements(elems);
 
         return stats;
     }
@@ -1598,6 +1627,7 @@ public synchronized IStats getStatistics()
     /**
      * This is exposed for testing.
      * <p>
+     *
      * @return Returns the timesOptimized.
      */
     protected int getTimesOptimized()
@@ -1608,6 +1638,7 @@ protected int getTimesOptimized()
     /**
      * This is used by the event logging.
      * <p>
+     *
      * @return the location of the disk, either path or ip.
      */
     @Override
@@ -1620,8 +1651,7 @@ protected String getDiskLocation()
      * Compares IndexedDiskElementDescriptor based on their position.
      * <p>
      */
-    protected static final class PositionComparator
-        implements Comparator<IndexedDiskElementDescriptor>, Serializable
+    protected static final class PositionComparator implements Comparator<IndexedDiskElementDescriptor>, Serializable
     {
         /** serialVersionUID */
         private static final long serialVersionUID = -8387365338590814113L;
@@ -1629,19 +1659,20 @@ protected static final class PositionComparator
         /**
          * Compares two descriptors based on position.
          * <p>
+         *
          * @see java.util.Comparator#compare(java.lang.Object, java.lang.Object)
          */
         @Override
-        public int compare( IndexedDiskElementDescriptor o1, IndexedDiskElementDescriptor o2 )
+        public int compare(IndexedDiskElementDescriptor o1, IndexedDiskElementDescriptor o2)
         {
             IndexedDiskElementDescriptor ded1 = o1;
             IndexedDiskElementDescriptor ded2 = o2;
 
-            if ( ded1.pos < ded2.pos )
+            if (ded1.pos < ded2.pos)
             {
                 return -1;
             }
-            else if ( ded1.pos == ded2.pos )
+            else if (ded1.pos == ded2.pos)
             {
                 return 0;
             }
@@ -1656,54 +1687,87 @@ else if ( ded1.pos == ded2.pos )
      * Class for recycling and lru. This implements the LRU overflow callback, so we can add items
      * to the recycle bin. This class counts the size element to decide, when to throw away an element
      */
-    public class LRUMapSizeLimited
-        extends AbstractLRUMap<K, IndexedDiskElementDescriptor>
+    public class LRUMapSizeLimited extends AbstractLRUMap<K, IndexedDiskElementDescriptor>
     {
         /**
          * <code>tag</code> tells us which map we are working on.
          */
         public String tag = "orig";
+
         // size of the content in kB
-        private AtomicInteger contentSize = new AtomicInteger();
-        private int maxSize = -1;
+        private AtomicInteger contentSize;
+        private int maxSize;
+
         /**
          * Default
          */
         public LRUMapSizeLimited()
         {
-            super();
+            this(-1);
         }
 
         /**
          * @param maxKeySize
          */
-        public LRUMapSizeLimited( int maxKeySize )
+        public LRUMapSizeLimited(int maxKeySize)
         {
             super();
             this.maxSize = maxKeySize;
+            this.contentSize = new AtomicInteger(0);
+        }
+
+        // keep the content size in kB, so 2^31 kB is reasonable value
+        private void subLengthFromCacheSize(IndexedDiskElementDescriptor value)
+        {
+            contentSize.addAndGet((value.len + IndexedDisk.HEADER_SIZE_BYTES) / -1024 - 1);
+        }
+
+        // keep the content size in kB, so 2^31 kB is reasonable value
+        private void addLengthToCacheSize(IndexedDiskElementDescriptor value)
+        {
+            contentSize.addAndGet((value.len + IndexedDisk.HEADER_SIZE_BYTES) / 1024 + 1);
         }
 
         @Override
-        public IndexedDiskElementDescriptor put(K key, IndexedDiskElementDescriptor value) {
-            try {
-                return super.put(key, value);
-            } finally {
+        public IndexedDiskElementDescriptor put(K key, IndexedDiskElementDescriptor value)
+        {
+            IndexedDiskElementDescriptor oldValue = null;
+
+            try
+            {
+                oldValue = super.put(key, value);
+            }
+            finally
+            {
                 // keep the content size in kB, so 2^31 kB is reasonable value
-                contentSize.addAndGet((int) Math.ceil((value.len + IndexedDisk.HEADER_SIZE_BYTES) / 1024.0));
+                if (value != null)
+                {
+                    addLengthToCacheSize(value);
+                }
+                if (oldValue != null)
+                {
+                    subLengthFromCacheSize(oldValue);
+                }
             }
+
+            return oldValue;
         }
 
         @Override
-        public IndexedDiskElementDescriptor remove(Object key ) {
+        public IndexedDiskElementDescriptor remove(Object key)
+        {
             IndexedDiskElementDescriptor value = null;
 
-            try {
+            try
+            {
                 value = super.remove(key);
                 return value;
-            } finally {
-                if (value != null) {
-                    // keep the content size in kB, so 2^31 kB is reasonable value
-                    contentSize.addAndGet((int) ((Math.ceil((value.len + IndexedDisk.HEADER_SIZE_BYTES) / 1024.0)) * -1));
+            }
+            finally
+            {
+                if (value != null)
+                {
+                    subLengthFromCacheSize(value);
                 }
             }
         }
@@ -1712,29 +1776,33 @@ public IndexedDiskElementDescriptor remove(Object key ) {
          * This is called when the may key size is reached. The least recently used item will be
          * passed here. We will store the position and size of the spot on disk in the recycle bin.
          * <p>
+         *
          * @param key
          * @param value
          */
         @Override
-        protected void processRemovedLRU(K key, IndexedDiskElementDescriptor value )
+        protected void processRemovedLRU(K key, IndexedDiskElementDescriptor value)
         {
-            if (value != null) {
-                // keep the content size in kB, so 2^31 kB is reasonable value
-                contentSize.addAndGet((int) ((Math.ceil(value.len / 1024.0)) * -1));
+            if (value != null)
+            {
+                subLengthFromCacheSize(value);
             }
-            addToRecycleBin( value );
-            if ( log.isDebugEnabled() )
+
+            addToRecycleBin(value);
+
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Removing key: [" + key + "] from key store." );
-                log.debug( logCacheName + "Key store size: [" + this.size() + "]." );
+                log.debug(logCacheName + "Removing key: [" + key + "] from key store.");
+                log.debug(logCacheName + "Key store size: [" + this.size() + "].");
             }
 
             doOptimizeRealTime();
         }
 
         @Override
-        protected boolean shouldRemove() {
-            return maxSize > 0 && contentSize.intValue() > maxSize && this.size() > 0;
+        protected boolean shouldRemove()
+        {
+            return maxSize > 0 && contentSize.get() > maxSize && this.size() > 0;
         }
     }
 
@@ -1743,28 +1811,30 @@ protected boolean shouldRemove() {
      * to the recycle bin. This class counts the elements to decide, when to throw away an element
      */
 
-    public class LRUMapCountLimited
-    extends LRUMap<K, IndexedDiskElementDescriptor>
+    public class LRUMapCountLimited extends LRUMap<K, IndexedDiskElementDescriptor>
     // implements Serializable
     {
-        public LRUMapCountLimited(int maxKeySize) {
+        public LRUMapCountLimited(int maxKeySize)
+        {
             super(maxKeySize);
         }
+
         /**
          * This is called when the may key size is reached. The least recently used item will be
          * passed here. We will store the position and size of the spot on disk in the recycle bin.
          * <p>
+         *
          * @param key
          * @param value
          */
         @Override
-        protected void processRemovedLRU(K key, IndexedDiskElementDescriptor value )
+        protected void processRemovedLRU(K key, IndexedDiskElementDescriptor value)
         {
-            addToRecycleBin( value );
-            if ( log.isDebugEnabled() )
+            addToRecycleBin(value);
+            if (log.isDebugEnabled())
             {
-                log.debug( logCacheName + "Removing key: [" + key + "] from key store." );
-                log.debug( logCacheName + "Key store size: [" + this.size() + "]." );
+                log.debug(logCacheName + "Removing key: [" + key + "] from key store.");
+                log.debug(logCacheName + "Key store size: [" + this.size() + "].");
             }
 
             doOptimizeRealTime();
diff --git a/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/AbstractLRUMap.java b/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/AbstractLRUMap.java
index 62faf66d9..a8f69b46b 100644
--- a/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/AbstractLRUMap.java
+++ b/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/AbstractLRUMap.java
@@ -88,15 +88,14 @@ public AbstractLRUMap()
     {
         list = new DoubleLinkedList<LRUElementDescriptor<K, V>>();
 
-        // normal hshtable is faster for
+        // normal hashtable is faster for
         // sequential keys.
         map = new ConcurrentHashMap<K, LRUElementDescriptor<K, V>>();
-        // map = new ConcurrentHashMap();
     }
 
 
     /**
-     * This simply returned the number of elements in the map.
+     * This simply returns the number of elements in the map.
      * <p>
      * @see java.util.Map#size()
      */
@@ -229,9 +228,9 @@ public V get( Object key )
     }
 
     /**
-     * This gets an element out of the map without adjusting it's posisiton in the LRU. In other
+     * This gets an element out of the map without adjusting it's position in the LRU. In other
      * words, this does not count as being used. If the element is the last item in the list, it
-     * will still be the last itme in the list.
+     * will still be the last time in the list.
      * <p>
      * @param key
      * @return Object
@@ -325,8 +324,7 @@ public V put(K key, V value)
 
         if (shouldRemove())
         {
-            final boolean debugEnabled = log.isDebugEnabled();
-            if (debugEnabled)
+            if (log.isDebugEnabled())
             {
                 log.debug( "In memory limit reached, removing least recently used." );
             }
@@ -640,9 +638,8 @@ public Set<Map.Entry<K, V>> entrySet()
         lock.lock();
         try
         {
-            // todo, we should return a defensive copy
+            // TODO we should return a defensive copy
             Set<Map.Entry<K, LRUElementDescriptor<K, V>>> entries = map.entrySet();
-
             Set<Map.Entry<K, V>> unWrapped = new HashSet<Map.Entry<K, V>>();
 
             for (Map.Entry<K, LRUElementDescriptor<K, V>> pre : entries) {
diff --git a/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/LRUMap.java b/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/LRUMap.java
index b6cb65bf8..fa100b274 100644
--- a/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/LRUMap.java
+++ b/commons-jcs-core/src/main/java/org/apache/commons/jcs/utils/struct/LRUMap.java
@@ -25,36 +25,42 @@
  *
  * @author Wiktor Niesiobdzki
  *
- * Simple LRUMap implementation that keeps the number of the objects below or equal maxObjects
+ *         Simple LRUMap implementation that keeps the number of the objects below or equal maxObjects
  *
  * @param <K>
  * @param <V>
  */
-public class LRUMap<K, V> extends AbstractLRUMap<K, V> {
+public class LRUMap<K, V> extends AbstractLRUMap<K, V>
+{
 
     /** if the max is less than 0, there is no limit! */
     int maxObjects = -1;
     AtomicInteger counter = new AtomicInteger(0);
 
-    public LRUMap() {
+    public LRUMap()
+    {
         super();
     }
 
     /**
      *
-     * @param maxObjects maximum number to keep in the map
+     * @param maxObjects
+     *            maximum number to keep in the map
      */
-    public LRUMap(int maxObjects) {
+    public LRUMap(int maxObjects)
+    {
         super();
         this.maxObjects = maxObjects;
     }
 
     @Override
-    public boolean shouldRemove() {
-    	return maxObjects > 0 && this.size() > maxObjects; 
+    public boolean shouldRemove()
+    {
+        return maxObjects > 0 && this.size() > maxObjects;
     }
 
-    public Object getMaxCounter() {
-		return maxObjects;
-	}
+    public Object getMaxCounter()
+    {
+        return maxObjects;
+    }
 }
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheCountUnitTest.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheCountUnitTest.java
new file mode 100644
index 000000000..5f7d57ed0
--- /dev/null
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheCountUnitTest.java
@@ -0,0 +1,35 @@
+package org.apache.commons.jcs.auxiliary.disk.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
+
+public class BlockDiskCacheCountUnitTest extends BlockDiskCacheUnitTestAbstract
+{
+
+    @Override
+    public BlockDiskCacheAttributes getCacheAttributes()
+    {
+        BlockDiskCacheAttributes ret = new BlockDiskCacheAttributes();
+        ret.setDiskLimitType(DiskLimitType.COUNT);
+        return ret;
+    }
+
+}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheSizeUnitTest.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheSizeUnitTest.java
new file mode 100644
index 000000000..4a093ff32
--- /dev/null
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheSizeUnitTest.java
@@ -0,0 +1,35 @@
+package org.apache.commons.jcs.auxiliary.disk.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
+
+public class BlockDiskCacheSizeUnitTest extends BlockDiskCacheUnitTestAbstract
+{
+
+    @Override
+    public BlockDiskCacheAttributes getCacheAttributes()
+    {
+        BlockDiskCacheAttributes ret = new BlockDiskCacheAttributes();
+        ret.setDiskLimitType(DiskLimitType.SIZE);
+        return ret;
+    }
+
+}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTest.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTest.java
deleted file mode 100644
index ffbf2ef6e..000000000
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTest.java
+++ /dev/null
@@ -1,343 +0,0 @@
-package org.apache.commons.jcs.auxiliary.disk.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import junit.framework.TestCase;
-import org.apache.commons.jcs.engine.CacheElement;
-import org.apache.commons.jcs.engine.behavior.ICacheElement;
-import org.apache.commons.jcs.utils.serialization.StandardSerializer;
-
-import java.io.File;
-import java.io.Serializable;
-import java.util.Map;
-
-/** Unit tests for the Block Disk Cache */
-public abstract class BlockDiskCacheUnitTest
-    extends TestCase
-{
-    /**
-     * Test the basic get matching.
-     * <p>
-     * @throws Exception
-     */
-	
-	public abstract BlockDiskCacheAttributes getCacheAttributes();
-	
-    public void testPutGetMatching_SmallWait()
-        throws Exception
-    {
-        // SETUP
-        int items = 200;
-
-        String cacheName = "testPutGetMatching_SmallWait";
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>( cattr );
-
-        // DO WORK
-        for ( int i = 0; i <= items; i++ )
-        {
-            diskCache.update( new CacheElement<String, String>( cacheName, i + ":key", cacheName + " data " + i ) );
-        }
-        Thread.sleep( 500 );
-
-        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching( "1.8.+" );
-
-        // VERIFY
-        assertEquals( "Wrong number returned", 10, matchingResults.size() );
-        //System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
-        //System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
-    }
-
-    /**
-     * Test the basic get matching. With no wait this will all come from purgatory.
-     * <p>
-     * @throws Exception
-     */
-    public void testPutGetMatching_NoWait()
-        throws Exception
-    {
-        // SETUP
-        int items = 200;
-
-        String cacheName = "testPutGetMatching_NoWait";
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>( cattr );
-
-        // DO WORK
-        for ( int i = 0; i <= items; i++ )
-        {
-            diskCache.update( new CacheElement<String, String>( cacheName, i + ":key", cacheName + " data " + i ) );
-        }
-
-        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching( "1.8.+" );
-
-        // VERIFY
-        assertEquals( "Wrong number returned", 10, matchingResults.size() );
-        //System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
-        //System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
-    }
-
-    /**
-     * Verify that the block disk cache can handle a big string.
-     * <p>
-     * @throws Exception
-     */
-    public void testChunk_BigString()
-        throws Exception
-    {
-        String string = "This is my big string ABCDEFGH";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( "|" + i + ":" + sb.toString() ); // big string
-        }
-        string = sb.toString();
-
-        StandardSerializer elementSerializer = new StandardSerializer();
-        byte[] data = elementSerializer.serialize( string );
-
-        File file = new File( "target/test-sandbox/BlockDiskCacheUnitTest/testChunk_BigString.data" );
-
-        BlockDisk blockDisk = new BlockDisk( file, 200, elementSerializer );
-
-        int numBlocksNeeded = blockDisk.calculateTheNumberOfBlocksNeeded( data );
-//        System.out.println( numBlocksNeeded );
-
-        // get the individual sub arrays.
-        byte[][] chunks = blockDisk.getBlockChunks( data, numBlocksNeeded );
-
-        byte[] resultData = new byte[0];
-
-        for ( short i = 0; i < chunks.length; i++ )
-        {
-            byte[] chunk = chunks[i];
-            byte[] newTotal = new byte[data.length + chunk.length];
-            // copy data into the new array
-            System.arraycopy( data, 0, newTotal, 0, data.length );
-            // copy the chunk into the new array
-            System.arraycopy( chunk, 0, newTotal, data.length, chunk.length );
-            // swap the new and old.
-            resultData = newTotal;
-        }
-
-        Serializable result = elementSerializer.deSerialize( resultData, null );
-        // System.out.println( result );
-        assertEquals( "wrong string after retrieval", string, result );
-    }
-
-    /**
-     * Verify that the block disk cache can handle a big string.
-     * <p>
-     * @throws Exception
-     */
-    public void testPutGet_BigString()
-        throws Exception
-    {
-        String string = "This is my big string ABCDEFGH";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( " " + i + sb.toString() ); // big string
-        }
-        string = sb.toString();
-
-        String cacheName = "testPutGet_BigString";
-
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setBlockSizeBytes( 200 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, String>( cacheName, "x", string ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, String> afterElement = diskCache.get( "x" );
-        assertNotNull( afterElement );
-        // System.out.println( "afterElement = " + afterElement );
-        String after = afterElement.getVal();
-
-        assertNotNull( after );
-        assertEquals( "wrong string after retrieval", string, after );
-    }
-
-    /**
-     * Verify that the block disk cache can handle utf encoded strings.
-     * <p>
-     * @throws Exception
-     */
-    public void testUTF8String()
-        throws Exception
-    {
-        String string = "Itrntinliztin";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( sb.toString() ); // big string
-        }
-        string = sb.toString();
-
-//        System.out.println( "The string contains " + string.length() + " characters" );
-
-        String cacheName = "testUTF8String";
-
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setBlockSizeBytes( 200 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, String>( cacheName, "x", string ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, String> afterElement = diskCache.get( "x" );
-        assertNotNull( afterElement );
-        // System.out.println( "afterElement = " + afterElement );
-        String after = afterElement.getVal();
-
-        assertNotNull( after );
-        assertEquals( "wrong string after retrieval", string, after );
-    }
-
-    /**
-     * Verify that the block disk cache can handle utf encoded strings.
-     * <p>
-     * @throws Exception
-     */
-    public void testUTF8ByteArray()
-        throws Exception
-    {
-        String string = "Itrntinliztin";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( sb.toString() ); // big string
-        }
-        string = sb.toString();
-        //System.out.println( "The string contains " + string.length() + " characters" );
-        String UTF8 = "UTF-8";
-        byte[] bytes = string.getBytes( UTF8 );
-
-        String cacheName = "testUTF8ByteArray";
-
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setBlockSizeBytes( 200 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, byte[]> diskCache = new BlockDiskCache<String, byte[]>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, byte[]>( cacheName, "x", bytes ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, byte[]> afterElement = diskCache.get( "x" );
-        assertNotNull( afterElement );
-        //System.out.println( "afterElement = " + afterElement );
-        byte[] after = afterElement.getVal();
-
-        assertNotNull( after );
-        assertEquals( "wrong bytes after retrieval", bytes.length, after.length );
-        //assertEquals( "wrong bytes after retrieval", bytes, after );
-        //assertEquals( "wrong bytes after retrieval", string, new String( after, UTF8 ) );
-
-    }
-
-    /**
-     * Verify that the block disk cache can handle utf encoded strings.
-     * <p>
-     * @throws Exception
-     */
-    public void testUTF8StringAndBytes()
-        throws Exception
-    {
-        X before = new X();
-        String string = "Itrntinliztin";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( sb.toString() ); // big string
-        }
-        string = sb.toString();
-        //System.out.println( "The string contains " + string.length() + " characters" );
-        String UTF8 = "UTF-8";
-        before.string = string;
-        before.bytes = string.getBytes( UTF8 );
-
-        String cacheName = "testUTF8StringAndBytes";
-
-        BlockDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setBlockSizeBytes( 500 );
-        cattr.setDiskPath( "target/test-sandbox/BlockDiskCacheUnitTest" );
-        BlockDiskCache<String, X> diskCache = new BlockDiskCache<String, X>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, X>( cacheName, "x", before ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, X> afterElement = diskCache.get( "x" );
-        // System.out.println( "afterElement = " + afterElement );
-        X after = ( afterElement.getVal() );
-
-        assertNotNull( after );
-        assertEquals( "wrong string after retrieval", string, after.string );
-        assertEquals( "wrong bytes after retrieval", string, new String( after.bytes, UTF8 ) );
-
-    }
-
-    /** Holder for a string and byte array. */
-    static class X
-        implements Serializable
-    {
-        /** ignore */
-        private static final long serialVersionUID = 1L;
-
-        /** Test string */
-        String string;
-
-        /*** test byte array. */
-        byte[] bytes;
-    }
-}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestAbstract.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestAbstract.java
new file mode 100644
index 000000000..a3b4bc23f
--- /dev/null
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestAbstract.java
@@ -0,0 +1,399 @@
+package org.apache.commons.jcs.auxiliary.disk.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.File;
+import java.io.Serializable;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.jcs.engine.CacheElement;
+import org.apache.commons.jcs.engine.behavior.ICacheElement;
+import org.apache.commons.jcs.utils.serialization.StandardSerializer;
+
+/** Unit tests for the Block Disk Cache */
+public abstract class BlockDiskCacheUnitTestAbstract extends TestCase
+{
+    /**
+     * Test the basic get matching.
+     * <p>
+     *
+     * @throws Exception
+     */
+
+    public abstract BlockDiskCacheAttributes getCacheAttributes();
+
+    public void testPutGetMatching_SmallWait() throws Exception
+    {
+        // SETUP
+        int items = 200;
+
+        String cacheName = "testPutGetMatching_SmallWait";
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>(cattr);
+
+        // DO WORK
+        for (int i = 0; i <= items; i++)
+        {
+            diskCache.update(new CacheElement<String, String>(cacheName, i + ":key", cacheName + " data " + i));
+        }
+        Thread.sleep(500);
+
+        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching("1.8.+");
+
+        // VERIFY
+        assertEquals("Wrong number returned", 10, matchingResults.size());
+        // System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
+        // System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
+    }
+
+    /**
+     * Test the basic get matching. With no wait this will all come from purgatory.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testPutGetMatching_NoWait() throws Exception
+    {
+        // SETUP
+        int items = 200;
+
+        String cacheName = "testPutGetMatching_NoWait";
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>(cattr);
+
+        // DO WORK
+        for (int i = 0; i <= items; i++)
+        {
+            diskCache.update(new CacheElement<String, String>(cacheName, i + ":key", cacheName + " data " + i));
+        }
+
+        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching("1.8.+");
+
+        // VERIFY
+        assertEquals("Wrong number returned", 10, matchingResults.size());
+        // System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
+        // System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
+    }
+
+    /**
+     * Verify that the block disk cache can handle a big string.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testChunk_BigString() throws Exception
+    {
+        String string = "This is my big string ABCDEFGH";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append("|" + i + ":" + sb.toString()); // big string
+        }
+        string = sb.toString();
+
+        StandardSerializer elementSerializer = new StandardSerializer();
+        byte[] data = elementSerializer.serialize(string);
+
+        File file = new File("target/test-sandbox/BlockDiskCacheUnitTest/testChunk_BigString.data");
+
+        BlockDisk blockDisk = new BlockDisk(file, 200, elementSerializer);
+
+        int numBlocksNeeded = blockDisk.calculateTheNumberOfBlocksNeeded(data);
+        // System.out.println( numBlocksNeeded );
+
+        // get the individual sub arrays.
+        byte[][] chunks = blockDisk.getBlockChunks(data, numBlocksNeeded);
+
+        byte[] resultData = new byte[0];
+
+        for (short i = 0; i < chunks.length; i++)
+        {
+            byte[] chunk = chunks[i];
+            byte[] newTotal = new byte[data.length + chunk.length];
+            // copy data into the new array
+            System.arraycopy(data, 0, newTotal, 0, data.length);
+            // copy the chunk into the new array
+            System.arraycopy(chunk, 0, newTotal, data.length, chunk.length);
+            // swap the new and old.
+            resultData = newTotal;
+        }
+
+        Serializable result = elementSerializer.deSerialize(resultData, null);
+        // System.out.println( result );
+        assertEquals("wrong string after retrieval", string, result);
+    }
+
+    /**
+     * Verify that the block disk cache can handle a big string.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testPutGet_BigString() throws Exception
+    {
+        String string = "This is my big string ABCDEFGH";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(" " + i + sb.toString()); // big string
+        }
+        string = sb.toString();
+
+        String cacheName = "testPutGet_BigString";
+
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setBlockSizeBytes(200);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, String>(cacheName, "x", string));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, String> afterElement = diskCache.get("x");
+        assertNotNull(afterElement);
+        // System.out.println( "afterElement = " + afterElement );
+        String after = afterElement.getVal();
+
+        assertNotNull(after);
+        assertEquals("wrong string after retrieval", string, after);
+    }
+
+    /**
+     * Verify that the block disk cache can handle utf encoded strings.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUTF8String() throws Exception
+    {
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+
+        // System.out.println( "The string contains " + string.length() + " characters" );
+
+        String cacheName = "testUTF8String";
+
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setBlockSizeBytes(200);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, String> diskCache = new BlockDiskCache<String, String>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, String>(cacheName, "x", string));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, String> afterElement = diskCache.get("x");
+        assertNotNull(afterElement);
+        // System.out.println( "afterElement = " + afterElement );
+        String after = afterElement.getVal();
+
+        assertNotNull(after);
+        assertEquals("wrong string after retrieval", string, after);
+    }
+
+    /**
+     * Verify that the block disk cache can handle utf encoded strings.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUTF8ByteArray() throws Exception
+    {
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+        // System.out.println( "The string contains " + string.length() + " characters" );
+        String UTF8 = "UTF-8";
+        byte[] bytes = string.getBytes(UTF8);
+
+        String cacheName = "testUTF8ByteArray";
+
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setBlockSizeBytes(200);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, byte[]> diskCache = new BlockDiskCache<String, byte[]>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, byte[]>(cacheName, "x", bytes));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, byte[]> afterElement = diskCache.get("x");
+        assertNotNull(afterElement);
+        // System.out.println( "afterElement = " + afterElement );
+        byte[] after = afterElement.getVal();
+
+        assertNotNull(after);
+        assertEquals("wrong bytes after retrieval", bytes.length, after.length);
+        // assertEquals( "wrong bytes after retrieval", bytes, after );
+        // assertEquals( "wrong bytes after retrieval", string, new String( after, UTF8 ) );
+
+    }
+
+    /**
+     * Verify that the block disk cache can handle utf encoded strings.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUTF8StringAndBytes() throws Exception
+    {
+        X before = new X();
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+        // System.out.println( "The string contains " + string.length() + " characters" );
+        String UTF8 = "UTF-8";
+        before.string = string;
+        before.bytes = string.getBytes(UTF8);
+
+        String cacheName = "testUTF8StringAndBytes";
+
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setBlockSizeBytes(500);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, X> diskCache = new BlockDiskCache<String, X>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, X>(cacheName, "x", before));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, X> afterElement = diskCache.get("x");
+        // System.out.println( "afterElement = " + afterElement );
+        X after = (afterElement.getVal());
+
+        assertNotNull(after);
+        assertEquals("wrong string after retrieval", string, after.string);
+        assertEquals("wrong bytes after retrieval", string, new String(after.bytes, UTF8));
+
+    }
+
+    public void testLoadFromDisk() throws Exception
+    {
+        for (int i = 0; i < 20; i++)
+        { // usually after 2 time it fails
+            oneLoadFromDisk();
+        }
+    }
+
+    public void oneLoadFromDisk() throws Exception
+    {
+        // initialize object to be stored
+        X before = new X();
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+        String UTF8 = "UTF-8";
+        before.string = string;
+        before.bytes = string.getBytes(UTF8);
+
+        // initialize cache
+        String cacheName = "testLoadFromDisk";
+        BlockDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setBlockSizeBytes(500);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        BlockDiskCache<String, X> diskCache = new BlockDiskCache<String, X>(cattr);
+
+        // DO WORK
+        for (int i = 0; i < 50; i++)
+        {
+            diskCache.update(new CacheElement<String, X>(cacheName, "x" + i, before));
+        }
+        diskCache.dispose();
+
+        // VERIFY
+        diskCache = new BlockDiskCache<String, X>(cattr);
+
+        for (int i = 0; i < 50; i++)
+        {
+            ICacheElement<String, X> afterElement = diskCache.get("x" + i);
+            assertNotNull("Missing element from cache. Cache size: " + diskCache.getSize() + " element: x" + i, afterElement);
+            X after = (afterElement.getVal());
+
+            assertNotNull(after);
+            assertEquals("wrong string after retrieval", string, after.string);
+            assertEquals("wrong bytes after retrieval", string, new String(after.bytes, UTF8));
+        }
+
+        diskCache.dispose();
+    }
+
+    /** Holder for a string and byte array. */
+    static class X implements Serializable
+    {
+        /** ignore */
+        private static final long serialVersionUID = 1L;
+
+        /** Test string */
+        String string;
+
+        /*** test byte array. */
+        byte[] bytes;
+    }
+}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestCount.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestCount.java
deleted file mode 100644
index 2808f00a9..000000000
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestCount.java
+++ /dev/null
@@ -1,14 +0,0 @@
-package org.apache.commons.jcs.auxiliary.disk.block;
-
-import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
-
-public class BlockDiskCacheUnitTestCount extends BlockDiskCacheUnitTest {
-
-	@Override
-	public BlockDiskCacheAttributes getCacheAttributes() {
-		BlockDiskCacheAttributes ret = new BlockDiskCacheAttributes();
-		ret.setDiskLimitType(DiskLimitType.COUNT);
-		return ret;
-	}
-
-}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestSize.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestSize.java
deleted file mode 100644
index 10bb29827..000000000
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/block/BlockDiskCacheUnitTestSize.java
+++ /dev/null
@@ -1,14 +0,0 @@
-package org.apache.commons.jcs.auxiliary.disk.block;
-
-import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
-
-public class BlockDiskCacheUnitTestSize extends BlockDiskCacheUnitTest {
-
-	@Override
-	public BlockDiskCacheAttributes getCacheAttributes() {
-		BlockDiskCacheAttributes ret = new BlockDiskCacheAttributes();
-		ret.setDiskLimitType(DiskLimitType.SIZE);
-		return ret;
-	}
-
-}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestCount.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheCountUnitTest.java
similarity index 78%
rename from commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestCount.java
rename to commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheCountUnitTest.java
index 8d2237feb..14d4d2913 100644
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestCount.java
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheCountUnitTest.java
@@ -1,91 +1,110 @@
-package org.apache.commons.jcs.auxiliary.disk.indexed;
-
-import java.io.IOException;
-
-import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
-import org.apache.commons.jcs.engine.CacheElement;
-import org.apache.commons.jcs.engine.behavior.ICacheElement;
-
-public class IndexDiskCacheUnitTestCount extends IndexDiskCacheUnitTest {
-
-	@Override
-	public IndexedDiskCacheAttributes getCacheAttributes() {
-		IndexedDiskCacheAttributes ret = new IndexedDiskCacheAttributes();
-		ret.setDiskLimitType(DiskLimitType.COUNT);
-		return ret;
-	}
-	  public void testRecycleBin()
-		        throws IOException
-		    {
-		        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-		        cattr.setCacheName( "testRemoveItems" );
-		        cattr.setMaxRecycleBinSize( 2 );
-		        cattr.setOptimizeAtRemoveCount( 7 );
-		        cattr.setMaxKeySize( 5 );
-		        cattr.setMaxPurgatorySize( 0 );
-		        cattr.setDiskPath( "target/test-sandbox/BreakIndexTest" );
-		        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-		        String[] test = { "a", "bb", "ccc", "dddd", "eeeee", "ffffff", "ggggggg", "hhhhhhhhh", "iiiiiiiiii" };
-		        String[] expect = { null, "bb", "ccc", null, null, "ffffff", null, "hhhhhhhhh", "iiiiiiiiii" };
-
-		        //System.out.println( "------------------------- testRecycleBin " );
-
-		        for ( int i = 0; i < 6; i++ )
-		        {
-		            ICacheElement<String, String> element = new CacheElement<String, String>( "testRecycleBin", "key:" + test[i], test[i] );
-		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
-		            disk.processUpdate( element );
-		        }
-
-		        for ( int i = 3; i < 5; i++ )
-		        {
-		            //System.out.println( "About to remove " + "key:" + test[i] + " i = " + i );
-		            disk.remove( "key:" + test[i] );
-		        }
-
-		        // there was a bug where 7 would try to be put in the empty slot left by 4's removal, but it
-		        // will not fit.
-		        for ( int i = 7; i < 9; i++ )
-		        {
-		            ICacheElement<String, String> element = new CacheElement<String, String>( "testRecycleBin", "key:" + test[i], test[i] );
-		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
-		            disk.processUpdate( element );
-		        }
-
-		        try
-		        {
-		            for ( int i = 0; i < 9; i++ )
-		            {
-		                ICacheElement<String, String> element = disk.get( "key:" + test[i] );
-		                if ( element != null )
-		                {
-		                    //System.out.println( "element = " + element.getVal() );
-		                }
-		                else
-		                {
-		                    //System.out.println( "null --" + "key:" + test[i] );
-		                }
-
-		                String expectedValue = expect[i];
-		                if ( expectedValue == null )
-		                {
-		                    assertNull( "Expected a null element", element );
-		                }
-		                else
-		                {
-		                    assertNotNull( "The element for key [" + "key:" + test[i] + "] should not be null. i = " + i,
-		                                   element );
-		                    assertEquals( "Elements contents do not match expected", element.getVal(), expectedValue );
-		                }
-		            }
-		        }
-		        catch ( Exception e )
-		        {
-		            e.printStackTrace();
-		            fail( "Should not get an exception: " + e.toString() );
-		        }
-
-		        disk.removeAll();
-		    }
-}
+package org.apache.commons.jcs.auxiliary.disk.indexed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
+import org.apache.commons.jcs.engine.CacheElement;
+import org.apache.commons.jcs.engine.behavior.ICacheElement;
+
+public class IndexDiskCacheCountUnitTest extends IndexDiskCacheUnitTestAbstract {
+
+	@Override
+	public IndexedDiskCacheAttributes getCacheAttributes() {
+		IndexedDiskCacheAttributes ret = new IndexedDiskCacheAttributes();
+		ret.setDiskLimitType(DiskLimitType.COUNT);
+		return ret;
+	}
+	  public void testRecycleBin()
+		        throws IOException
+		    {
+		        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+		        cattr.setCacheName( "testRemoveItems" );
+		        cattr.setMaxRecycleBinSize( 2 );
+		        cattr.setOptimizeAtRemoveCount( 7 );
+		        cattr.setMaxKeySize( 5 );
+		        cattr.setMaxPurgatorySize( 0 );
+		        cattr.setDiskPath( "target/test-sandbox/BreakIndexTest" );
+		        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
+
+		        String[] test = { "a", "bb", "ccc", "dddd", "eeeee", "ffffff", "ggggggg", "hhhhhhhhh", "iiiiiiiiii" };
+		        String[] expect = { null, "bb", "ccc", null, null, "ffffff", null, "hhhhhhhhh", "iiiiiiiiii" };
+
+		        //System.out.println( "------------------------- testRecycleBin " );
+
+		        for ( int i = 0; i < 6; i++ )
+		        {
+		            ICacheElement<String, String> element = new CacheElement<String, String>( "testRecycleBin", "key:" + test[i], test[i] );
+		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
+		            disk.processUpdate( element );
+		        }
+
+		        for ( int i = 3; i < 5; i++ )
+		        {
+		            //System.out.println( "About to remove " + "key:" + test[i] + " i = " + i );
+		            disk.remove( "key:" + test[i] );
+		        }
+
+		        // there was a bug where 7 would try to be put in the empty slot left by 4's removal, but it
+		        // will not fit.
+		        for ( int i = 7; i < 9; i++ )
+		        {
+		            ICacheElement<String, String> element = new CacheElement<String, String>( "testRecycleBin", "key:" + test[i], test[i] );
+		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
+		            disk.processUpdate( element );
+		        }
+
+		        try
+		        {
+		            for ( int i = 0; i < 9; i++ )
+		            {
+		                ICacheElement<String, String> element = disk.get( "key:" + test[i] );
+		                if ( element != null )
+		                {
+		                    //System.out.println( "element = " + element.getVal() );
+		                }
+		                else
+		                {
+		                    //System.out.println( "null --" + "key:" + test[i] );
+		                }
+
+		                String expectedValue = expect[i];
+		                if ( expectedValue == null )
+		                {
+		                    assertNull( "Expected a null element", element );
+		                }
+		                else
+		                {
+		                    assertNotNull( "The element for key [" + "key:" + test[i] + "] should not be null. i = " + i,
+		                                   element );
+		                    assertEquals( "Elements contents do not match expected", element.getVal(), expectedValue );
+		                }
+		            }
+		        }
+		        catch ( Exception e )
+		        {
+		            e.printStackTrace();
+		            fail( "Should not get an exception: " + e.toString() );
+		        }
+
+		        disk.removeAll();
+		    }
+}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestSize.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheSizeUnitTest.java
similarity index 79%
rename from commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestSize.java
rename to commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheSizeUnitTest.java
index b1fc21ecb..d869d44c9 100644
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestSize.java
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheSizeUnitTest.java
@@ -1,92 +1,111 @@
-package org.apache.commons.jcs.auxiliary.disk.indexed;
-
-import java.io.IOException;
-
-import org.apache.commons.jcs.auxiliary.disk.DiskTestObject;
-import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
-import org.apache.commons.jcs.engine.CacheElement;
-import org.apache.commons.jcs.engine.behavior.ICacheElement;
-
-public class IndexDiskCacheUnitTestSize extends IndexDiskCacheUnitTest {
-
-	@Override
-	public IndexedDiskCacheAttributes getCacheAttributes() {
-		IndexedDiskCacheAttributes ret = new IndexedDiskCacheAttributes();
-		ret.setDiskLimitType(DiskLimitType.SIZE);
-		return ret;
-	}
-	  public void testRecycleBin()
-		        throws IOException
-		    {
-		        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-		        cattr.setCacheName( "testRemoveItems" );
-		        cattr.setMaxRecycleBinSize( 2 );
-		        cattr.setOptimizeAtRemoveCount( 7 );
-		        cattr.setMaxKeySize( 8); // 1kb DiskTestObject takes 1420 bytes, so 5*1420 = 7100, so to keep 5 ojbects, we need max key size of 8
-		        cattr.setMaxPurgatorySize( 0 );
-		        cattr.setDiskPath( "target/test-sandbox/BreakIndexTest" );
-		        IndexedDiskCache<String, DiskTestObject> disk = new IndexedDiskCache<String, DiskTestObject>( cattr );
-
-		        String[] test = { "a", "bb", "ccc", "dddd", "eeeee", "ffffff", "ggggggg", "hhhhhhhhh", "iiiiiiiiii" };
-		        String[] expect = { null, "bb", "ccc", null, null, "ffffff", null, "hhhhhhhhh", "iiiiiiiiii" };
-		        DiskTestObject value = DiskTestObjectUtil.createCacheElementsWithTestObjects( 1, 1, cattr .getCacheName())[0].getVal();
-		        //System.out.println( "------------------------- testRecycleBin " );
-
-		        for ( int i = 0; i < 6; i++ )
-		        {
-		            ICacheElement<String, DiskTestObject> element = new CacheElement<String, DiskTestObject>( "testRecycleBin", "key:" + test[i], value);
-		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
-		            disk.processUpdate( element );
-		        }
-
-		        for ( int i = 3; i < 5; i++ )
-		        {
-		            //System.out.println( "About to remove " + "key:" + test[i] + " i = " + i );
-		            disk.remove( "key:" + test[i] );
-		        }
-
-		        // there was a bug where 7 would try to be put in the empty slot left by 4's removal, but it
-		        // will not fit.
-		        for ( int i = 7; i < 9; i++ )
-		        {
-		            ICacheElement<String, DiskTestObject> element = new CacheElement<String, DiskTestObject>( "testRecycleBin", "key:" + test[i], value);
-		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
-		            disk.processUpdate( element );
-		        }
-
-		        try
-		        {
-		            for ( int i = 0; i < 9; i++ )
-		            {
-		                ICacheElement<String, DiskTestObject> element = disk.get( "key:" + test[i] );
-		                if ( element != null )
-		                {
-		                    //System.out.println( "element = " + element.getVal() );
-		                }
-		                else
-		                {
-		                    //System.out.println( "null --" + "key:" + test[i] );
-		                }
-
-		                String expectedValue = expect[i];
-		                if ( expectedValue == null )
-		                {
-		                    assertNull( "Expected a null element", element );
-		                }
-		                else
-		                {
-		                    assertNotNull( "The element for key [" + "key:" + test[i] + "] should not be null. i = " + i,
-		                                   element );
-		                    assertEquals( "Elements contents do not match expected", element.getVal(), value );
-		                }
-		            }
-		        }
-		        catch ( Exception e )
-		        {
-		            e.printStackTrace();
-		            fail( "Should not get an exception: " + e.toString() );
-		        }
-
-		        disk.removeAll();
-		    }
-}
+package org.apache.commons.jcs.auxiliary.disk.indexed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.commons.jcs.auxiliary.disk.DiskTestObject;
+import org.apache.commons.jcs.auxiliary.disk.behavior.IDiskCacheAttributes.DiskLimitType;
+import org.apache.commons.jcs.engine.CacheElement;
+import org.apache.commons.jcs.engine.behavior.ICacheElement;
+
+public class IndexDiskCacheSizeUnitTest extends IndexDiskCacheUnitTestAbstract {
+
+	@Override
+	public IndexedDiskCacheAttributes getCacheAttributes() {
+		IndexedDiskCacheAttributes ret = new IndexedDiskCacheAttributes();
+		ret.setDiskLimitType(DiskLimitType.SIZE);
+		return ret;
+	}
+	  public void testRecycleBin()
+		        throws IOException
+		    {
+		        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+		        cattr.setCacheName( "testRemoveItems" );
+		        cattr.setMaxRecycleBinSize( 2 );
+		        cattr.setOptimizeAtRemoveCount( 7 );
+		        cattr.setMaxKeySize( 8); // 1kb DiskTestObject takes 1420 bytes, so 5*1420 = 7100, so to keep 5 ojbects, we need max key size of 8
+		        cattr.setMaxPurgatorySize( 0 );
+		        cattr.setDiskPath( "target/test-sandbox/BreakIndexTest" );
+		        IndexedDiskCache<String, DiskTestObject> disk = new IndexedDiskCache<String, DiskTestObject>( cattr );
+
+		        String[] test = { "a", "bb", "ccc", "dddd", "eeeee", "ffffff", "ggggggg", "hhhhhhhhh", "iiiiiiiiii" };
+		        String[] expect = { null, "bb", "ccc", null, null, "ffffff", null, "hhhhhhhhh", "iiiiiiiiii" };
+		        DiskTestObject value = DiskTestObjectUtil.createCacheElementsWithTestObjects( 1, 1, cattr .getCacheName())[0].getVal();
+		        //System.out.println( "------------------------- testRecycleBin " );
+
+		        for ( int i = 0; i < 6; i++ )
+		        {
+		            ICacheElement<String, DiskTestObject> element = new CacheElement<String, DiskTestObject>( "testRecycleBin", "key:" + test[i], value);
+		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
+		            disk.processUpdate( element );
+		        }
+
+		        for ( int i = 3; i < 5; i++ )
+		        {
+		            //System.out.println( "About to remove " + "key:" + test[i] + " i = " + i );
+		            disk.remove( "key:" + test[i] );
+		        }
+
+		        // there was a bug where 7 would try to be put in the empty slot left by 4's removal, but it
+		        // will not fit.
+		        for ( int i = 7; i < 9; i++ )
+		        {
+		            ICacheElement<String, DiskTestObject> element = new CacheElement<String, DiskTestObject>( "testRecycleBin", "key:" + test[i], value);
+		            //System.out.println( "About to add " + "key:" + test[i] + " i = " + i );
+		            disk.processUpdate( element );
+		        }
+
+		        try
+		        {
+		            for ( int i = 0; i < 9; i++ )
+		            {
+		                ICacheElement<String, DiskTestObject> element = disk.get( "key:" + test[i] );
+		                if ( element != null )
+		                {
+		                    //System.out.println( "element = " + element.getVal() );
+		                }
+		                else
+		                {
+		                    //System.out.println( "null --" + "key:" + test[i] );
+		                }
+
+		                String expectedValue = expect[i];
+		                if ( expectedValue == null )
+		                {
+		                    assertNull( "Expected a null element", element );
+		                }
+		                else
+		                {
+		                    assertNotNull( "The element for key [" + "key:" + test[i] + "] should not be null. i = " + i,
+		                                   element );
+		                    assertEquals( "Elements contents do not match expected", element.getVal(), value );
+		                }
+		            }
+		        }
+		        catch ( Exception e )
+		        {
+		            e.printStackTrace();
+		            fail( "Should not get an exception: " + e.toString() );
+		        }
+
+		        disk.removeAll();
+		    }
+}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTest.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTest.java
deleted file mode 100644
index f01f2b330..000000000
--- a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTest.java
+++ /dev/null
@@ -1,940 +0,0 @@
-package org.apache.commons.jcs.auxiliary.disk.indexed;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import junit.framework.TestCase;
-import org.apache.commons.jcs.auxiliary.MockCacheEventLogger;
-import org.apache.commons.jcs.auxiliary.disk.DiskTestObject;
-import org.apache.commons.jcs.engine.CacheElement;
-import org.apache.commons.jcs.engine.ElementAttributes;
-import org.apache.commons.jcs.engine.behavior.ICacheElement;
-import org.apache.commons.jcs.engine.behavior.IElementAttributes;
-import org.apache.commons.jcs.engine.control.group.GroupAttrName;
-import org.apache.commons.jcs.engine.control.group.GroupId;
-import org.apache.commons.jcs.utils.timing.SleepUtil;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-/**
- * Tests for common functionality.
- * <p>
- * @author Aaron Smuts
- */
-public abstract class IndexDiskCacheUnitTest
-    extends TestCase
-{
-	public abstract IndexedDiskCacheAttributes getCacheAttributes();
-    /**
-     * Simply verify that we can put items in the disk cache and retrieve them.
-     * @throws IOException
-     */
-    public void testSimplePutAndGet()
-        throws IOException
-    {
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testSimplePutAndGet" );
-        cattr.setMaxKeySize( 1000 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-        disk.processRemoveAll();
-
-        int cnt = 999;
-        for ( int i = 0; i < cnt; i++ )
-        {
-            IElementAttributes eAttr = new ElementAttributes();
-            eAttr.setIsSpool( true );
-            ICacheElement<String, String> element = new CacheElement<String, String>( "testSimplePutAndGet", "key:" + i, "data:" + i );
-            element.setElementAttributes( eAttr );
-            disk.processUpdate( element );
-        }
-
-        for ( int i = 0; i < cnt; i++ )
-        {
-            ICacheElement<String, String> element = disk.processGet( "key:" + i );
-            assertNotNull( "Should have received an element.", element );
-            assertEquals( "Element is wrong.", "data:" + i, element.getVal() );
-        }
-
-        // Test that getMultiple returns all the expected values
-        Set<String> keys = new HashSet<String>();
-        for ( int i = 0; i < cnt; i++ )
-        {
-            keys.add( "key:" + i );
-        }
-
-        Map<String, ICacheElement<String, String>> elements = disk.getMultiple( keys );
-        for ( int i = 0; i < cnt; i++ )
-        {
-            ICacheElement<String, String> element = elements.get( "key:" + i );
-            assertNotNull( "element " + i + ":key is missing", element );
-            assertEquals( "value key:" + i, "data:" + i, element.getVal() );
-        }
-        //System.out.println( disk.getStats() );
-    }
-
-    /**
-     * Add some items to the disk cache and then remove them one by one.
-     * @throws IOException
-     */
-    public void testRemoveItems()
-        throws IOException
-    {
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRemoveItems" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-        disk.processRemoveAll();
-
-        int cnt = 25;
-        for ( int i = 0; i < cnt; i++ )
-        {
-            IElementAttributes eAttr = new ElementAttributes();
-            eAttr.setIsSpool( true );
-            ICacheElement<String, String> element = new CacheElement<String, String>( "testRemoveItems", "key:" + i, "data:" + i );
-            element.setElementAttributes( eAttr );
-            disk.processUpdate( element );
-        }
-
-        // remove each
-        for ( int i = 0; i < cnt; i++ )
-        {
-            disk.remove( "key:" + i );
-            ICacheElement<String, String> element = disk.processGet( "key:" + i );
-            assertNull( "Should not have received an element.", element );
-        }
-    }
-
-    /**
-     * Verify that we don't override the largest item.
-     * <p>
-     * @throws IOException
-     */
-  
-
-    /**
-     * Verify that the overlap check returns true when there are no overlaps.
-     */
-    public void testCheckForDedOverlaps_noOverlap()
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testCheckForDedOverlaps_noOverlap" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-        int numDescriptors = 5;
-        int pos = 0;
-        IndexedDiskElementDescriptor[] sortedDescriptors = new IndexedDiskElementDescriptor[numDescriptors];
-        for ( int i = 0; i < numDescriptors; i++ )
-        {
-            IndexedDiskElementDescriptor descriptor = new IndexedDiskElementDescriptor( pos, i * 2 );
-            pos = pos + ( i * 2 ) + IndexedDisk.HEADER_SIZE_BYTES;
-            sortedDescriptors[i] = descriptor;
-        }
-
-        // DO WORK
-        boolean result = disk.checkForDedOverlaps( sortedDescriptors );
-
-        // VERIFY
-        assertTrue( "There should be no overlap. it should be ok", result );
-    }
-
-    /**
-     * Verify that the overlap check returns false when there are overlaps.
-     */
-    public void testCheckForDedOverlaps_overlaps()
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testCheckForDedOverlaps_overlaps" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-        int numDescriptors = 5;
-        int pos = 0;
-        IndexedDiskElementDescriptor[] sortedDescriptors = new IndexedDiskElementDescriptor[numDescriptors];
-        for ( int i = 0; i < numDescriptors; i++ )
-        {
-            IndexedDiskElementDescriptor descriptor = new IndexedDiskElementDescriptor( pos, i * 2 );
-            // don't add the header + IndexedDisk.RECORD_HEADER;
-            pos = pos + ( i * 2 );
-            sortedDescriptors[i] = descriptor;
-        }
-
-        // DO WORK
-        boolean result = disk.checkForDedOverlaps( sortedDescriptors );
-
-        // VERIFY
-        assertFalse( "There should be overlaps. it should be not ok", result );
-    }
-
-    /**
-     * Verify that the file size is as expected.
-     * <p>
-     * @throws IOException
-     * @throws InterruptedException
-     */
-    public void testFileSize()
-        throws IOException, InterruptedException
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testFileSize" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>( cattr );
-
-        int numberToInsert = 20;
-        int bytes = 24;
-        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects( numberToInsert, bytes, cattr
-            .getCacheName() );
-
-        for ( int i = 0; i < elements.length; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        Thread.yield();
-        Thread.sleep( 100 );
-        Thread.yield();
-
-        long expectedSize = DiskTestObjectUtil.totalSize( elements, numberToInsert );
-        long resultSize = disk.getDataFileSize();
-
-        //System.out.println( "testFileSize stats " + disk.getStats() );
-
-        assertEquals( "Wrong file size", expectedSize, resultSize );
-    }
-
-    /**
-     * Verify that items are added to the recycle bin on removal.
-     * <p>
-     * @throws IOException
-     * @throws InterruptedException
-     */
-    public void testRecyleBinSize()
-        throws IOException, InterruptedException
-    {
-        // SETUP
-        int numberToInsert = 20;
-
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRecyleBinSize" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        cattr.setMaxRecycleBinSize( numberToInsert );
-        cattr.setOptimizeAtRemoveCount( numberToInsert );
-        cattr.setMaxKeySize( numberToInsert * 2 );
-        cattr.setMaxPurgatorySize( numberToInsert );
-        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>( cattr );
-
-        int bytes = 1;
-        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects( numberToInsert, bytes, cattr
-            .getCacheName() );
-
-        for ( int i = 0; i < elements.length; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        Thread.yield();
-        Thread.sleep( 100 );
-        Thread.yield();
-
-        // remove half
-        int numberToRemove = elements.length / 2;
-        for ( int i = 0; i < numberToRemove; i++ )
-        {
-            disk.processRemove( elements[i].getKey() );
-        }
-
-        // verify that the recycle bin has the correct amount.
-        assertEquals( "The recycle bin should have the number removed.", numberToRemove, disk.getRecyleBinSize() );
-    }
-
-    /**
-     * Verify that items of the same size use recycle bin spots. Setup the recycle bin by removing
-     * some items. Add some of the same size. Verify that the recycle count is the number added.
-     * <p>
-     * @throws IOException
-     * @throws InterruptedException
-     */
-    public void testRecyleBinUsage()
-        throws IOException, InterruptedException
-    {
-        // SETUP
-        int numberToInsert = 20;
-
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRecyleBinUsage" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        cattr.setMaxRecycleBinSize( numberToInsert );
-        cattr.setOptimizeAtRemoveCount( numberToInsert );
-        cattr.setMaxKeySize( numberToInsert * 2 );
-        cattr.setMaxPurgatorySize( numberToInsert );
-        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>( cattr );
-
-        // we will reuse these
-        int bytes = 1;
-        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects( numberToInsert, bytes, cattr
-            .getCacheName() );
-
-        // Add some to the disk
-        for ( int i = 0; i < elements.length; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        Thread.yield();
-        Thread.sleep( 100 );
-        Thread.yield();
-
-        // remove half of those added
-        int numberToRemove = elements.length / 2;
-        for ( int i = 0; i < numberToRemove; i++ )
-        {
-            disk.processRemove( elements[i].getKey() );
-        }
-
-        // verify that the recycle bin has the correct amount.
-        assertEquals( "The recycle bin should have the number removed.", numberToRemove, disk.getRecyleBinSize() );
-
-        // add half as many as we removed. These should all use spots in the recycle bin.
-        int numberToAdd = numberToRemove / 2;
-        for ( int i = 0; i < numberToAdd; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        // verify that we used the correct number of spots
-        assertEquals( "The recycle bin should have the number removed." + disk.getStats(), numberToAdd, disk
-            .getRecyleCount() );
-    }
-
-    /**
-     * Verify that the data size is as expected after a remove and after a put that should use the
-     * spots.
-     * <p>
-     * @throws IOException
-     * @throws InterruptedException
-     */
-    public void testBytesFreeSize()
-        throws IOException, InterruptedException
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testBytesFreeSize" );
-        cattr.setDiskPath( "target/test-sandbox/UnitTest" );
-        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>( cattr );
-
-        int numberToInsert = 20;
-        int bytes = 24;
-        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects( numberToInsert, bytes, cattr
-            .getCacheName() );
-
-        for ( int i = 0; i < elements.length; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        Thread.yield();
-        Thread.sleep( 100 );
-        Thread.yield();
-
-        // remove half of those added
-        int numberToRemove = elements.length / 2;
-        for ( int i = 0; i < numberToRemove; i++ )
-        {
-            disk.processRemove( elements[i].getKey() );
-        }
-
-        long expectedSize = DiskTestObjectUtil.totalSize( elements, numberToRemove );
-        long resultSize = disk.getBytesFree();
-
-        //System.out.println( "testBytesFreeSize stats " + disk.getStats() );
-
-        assertEquals( "Wrong bytes free size" + disk.getStats(), expectedSize, resultSize );
-
-        // add half as many as we removed. These should all use spots in the recycle bin.
-        int numberToAdd = numberToRemove / 2;
-        for ( int i = 0; i < numberToAdd; i++ )
-        {
-            disk.processUpdate( elements[i] );
-        }
-
-        long expectedSize2 = DiskTestObjectUtil.totalSize( elements, numberToAdd );
-        long resultSize2 = disk.getBytesFree();
-        assertEquals( "Wrong bytes free size" + disk.getStats(), expectedSize2, resultSize2 );
-    }
-
-    /**
-     * Add some items to the disk cache and then remove them one by one.
-     * <p>
-     * @throws IOException
-     */
-    public void testRemove_PartialKey()
-        throws IOException
-    {
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRemove_PartialKey" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>( cattr );
-
-        disk.processRemoveAll();
-
-        int cnt = 25;
-        for ( int i = 0; i < cnt; i++ )
-        {
-            IElementAttributes eAttr = new ElementAttributes();
-            eAttr.setIsSpool( true );
-            ICacheElement<String, String> element = new CacheElement<String, String>( "testRemove_PartialKey", i + ":key", "data:" + i );
-            element.setElementAttributes( eAttr );
-            disk.processUpdate( element );
-        }
-
-        // verif each
-        for ( int i = 0; i < cnt; i++ )
-        {
-            ICacheElement<String, String> element = disk.processGet( i + ":key" );
-            assertNotNull( "Shoulds have received an element.", element );
-        }
-
-        // remove each
-        for ( int i = 0; i < cnt; i++ )
-        {
-            disk.remove( i + ":" );
-            ICacheElement<String, String> element = disk.processGet( i + ":key" );
-            assertNull( "Should not have received an element.", element );
-        }
-        // https://issues.apache.org/jira/browse/JCS-67
-        assertEquals( "Recylenbin should not have more elements than we removed. Check for JCS-67", cnt, disk
-            .getRecyleBinSize() );
-    }
-
-    /**
-     * Verify that group members are removed if we call remove with a group.
-     * @throws IOException
-     */
-    public void testRemove_Group()
-        throws IOException
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRemove_Group" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<GroupAttrName<String>, String> disk = new IndexedDiskCache<GroupAttrName<String>, String>( cattr );
-
-        disk.processRemoveAll();
-
-        String cacheName = "testRemove_Group_Region";
-        String groupName = "testRemove_Group";
-
-        int cnt = 25;
-        for ( int i = 0; i < cnt; i++ )
-        {
-            GroupAttrName<String> groupAttrName = getGroupAttrName( cacheName, groupName, i + ":key" );
-            CacheElement<GroupAttrName<String>, String> element = new CacheElement<GroupAttrName<String>, String>( cacheName, groupAttrName, "data:" + i );
-
-            IElementAttributes eAttr = new ElementAttributes();
-            eAttr.setIsSpool( true );
-            element.setElementAttributes( eAttr );
-
-            disk.processUpdate( element );
-        }
-
-        // verify each
-        for ( int i = 0; i < cnt; i++ )
-        {
-            GroupAttrName<String> groupAttrName = getGroupAttrName( cacheName, groupName, i + ":key" );
-            ICacheElement<GroupAttrName<String>, String> element = disk.processGet( groupAttrName );
-            assertNotNull( "Should have received an element.", element );
-        }
-
-        // DO WORK
-        // remove the group
-        disk.remove( getGroupAttrName( cacheName, groupName, null ) );
-
-        for ( int i = 0; i < cnt; i++ )
-        {
-            GroupAttrName<String> groupAttrName = getGroupAttrName( cacheName, groupName, i + ":key" );
-            ICacheElement<GroupAttrName<String>, String> element = disk.processGet( groupAttrName );
-
-            // VERIFY
-            assertNull( "Should not have received an element.", element );
-        }
-
-    }
-
-    /**
-     * Internal method used for group functionality.
-     * <p>
-     * @param cacheName
-     * @param group
-     * @param name
-     * @return GroupAttrName
-     */
-    private GroupAttrName<String> getGroupAttrName( String cacheName, String group, String name )
-    {
-        GroupId gid = new GroupId( cacheName, group );
-        return new GroupAttrName<String>( gid, name );
-    }
-
-    /**
-     * Verify event log calls.
-     * <p>
-     * @throws Exception
-     */
-    public void testUpdate_EventLogging_simple()
-        throws Exception
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testUpdate_EventLogging_simple" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTestCEL" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-        diskCache.processRemoveAll();
-
-        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
-        diskCache.setCacheEventLogger( cacheEventLogger );
-
-        ICacheElement<String, String> item = new CacheElement<String, String>( "region", "key", "value" );
-
-        // DO WORK
-        diskCache.update( item );
-
-        SleepUtil.sleepAtLeast( 200 );
-
-        // VERIFY
-        assertEquals( "Start should have been called.", 1, cacheEventLogger.startICacheEventCalls );
-        assertEquals( "End should have been called.", 1, cacheEventLogger.endICacheEventCalls );
-    }
-
-    /**
-     * Verify event log calls.
-     * <p>
-     * @throws Exception
-     */
-    public void testGet_EventLogging_simple()
-        throws Exception
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testGet_EventLogging_simple" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTestCEL" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-        diskCache.processRemoveAll();
-
-        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
-        diskCache.setCacheEventLogger( cacheEventLogger );
-
-        // DO WORK
-        diskCache.get( "key" );
-
-        // VERIFY
-        assertEquals( "Start should have been called.", 1, cacheEventLogger.startICacheEventCalls );
-        assertEquals( "End should have been called.", 1, cacheEventLogger.endICacheEventCalls );
-    }
-
-    /**
-     * Verify event log calls.
-     * <p>
-     * @throws Exception
-     */
-    public void testGetMultiple_EventLogging_simple()
-        throws Exception
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testGetMultiple_EventLogging_simple" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTestCEL" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-        diskCache.processRemoveAll();
-
-        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
-        diskCache.setCacheEventLogger( cacheEventLogger );
-
-        Set<String> keys = new HashSet<String>();
-        keys.add( "junk" );
-
-        // DO WORK
-        diskCache.getMultiple( keys );
-
-        // VERIFY
-        // 1 for get multiple and 1 for get.
-        assertEquals( "Start should have been called.", 2, cacheEventLogger.startICacheEventCalls );
-        assertEquals( "End should have been called.", 2, cacheEventLogger.endICacheEventCalls );
-    }
-
-    /**
-     * Verify event log calls.
-     * <p>
-     * @throws Exception
-     */
-    public void testRemove_EventLogging_simple()
-        throws Exception
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRemoveAll_EventLogging_simple" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTestCEL" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-        diskCache.processRemoveAll();
-
-        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
-        diskCache.setCacheEventLogger( cacheEventLogger );
-
-        // DO WORK
-        diskCache.remove( "key" );
-
-        // VERIFY
-        assertEquals( "Start should have been called.", 1, cacheEventLogger.startICacheEventCalls );
-        assertEquals( "End should have been called.", 1, cacheEventLogger.endICacheEventCalls );
-    }
-
-    /**
-     * Verify event log calls.
-     * <p>
-     * @throws Exception
-     */
-    public void testRemoveAll_EventLogging_simple()
-        throws Exception
-    {
-        // SETUP
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( "testRemoveAll_EventLogging_simple" );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTestCEL" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-        diskCache.processRemoveAll();
-
-        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
-        diskCache.setCacheEventLogger( cacheEventLogger );
-
-        // DO WORK
-        diskCache.remove( "key" );
-
-        // VERIFY
-        assertEquals( "Start should have been called.", 1, cacheEventLogger.startICacheEventCalls );
-        assertEquals( "End should have been called.", 1, cacheEventLogger.endICacheEventCalls );
-    }
-
-    /**
-     * Test the basic get matching.
-     * <p>
-     * @throws Exception
-     */
-    public void testPutGetMatching_SmallWait()
-        throws Exception
-    {
-        // SETUP
-        int items = 200;
-
-        String cacheName = "testPutGetMatching_SmallWait";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        // DO WORK
-        for ( int i = 0; i <= items; i++ )
-        {
-            diskCache.update( new CacheElement<String, String>( cacheName, i + ":key", cacheName + " data " + i ) );
-        }
-        Thread.sleep( 500 );
-
-        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching( "1.8.+" );
-
-        // VERIFY
-        assertEquals( "Wrong number returned", 10, matchingResults.size() );
-        //System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
-        //System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
-    }
-
-    /**
-     * Test the basic get matching. With no wait this will all come from purgatory.
-     * <p>
-     * @throws Exception
-     */
-    public void testPutGetMatching_NoWait()
-        throws Exception
-    {
-        // SETUP
-        int items = 200;
-
-        String cacheName = "testPutGetMatching_NoWait";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        // DO WORK
-        for ( int i = 0; i <= items; i++ )
-        {
-            diskCache.update( new CacheElement<String, String>( cacheName, i + ":key", cacheName + " data " + i ) );
-        }
-
-        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching( "1.8.+" );
-
-        // VERIFY
-        assertEquals( "Wrong number returned", 10, matchingResults.size() );
-        //System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
-        //System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
-    }
-
-    /**
-     * Verify that the block disk cache can handle utf encoded strings.
-     * <p>
-     * @throws Exception
-     */
-    public void testUTF8String()
-        throws Exception
-    {
-        String string = "Itrntinliztin";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( sb.toString() ); // big string
-        }
-        string = sb.toString();
-
-        //System.out.println( "The string contains " + string.length() + " characters" );
-
-        String cacheName = "testUTF8String";
-
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, String>( cacheName, "x", string ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, String> afterElement = diskCache.get( "x" );
-        assertNotNull( afterElement );
-        // System.out.println( "afterElement = " + afterElement );
-        String after = afterElement.getVal();
-
-        assertNotNull( after );
-        assertEquals( "wrong string after retrieval", string, after );
-    }
-
-    /**
-     * Verify that the block disk cache can handle utf encoded strings.
-     * <p>
-     * @throws Exception
-     */
-    public void testUTF8ByteArray()
-        throws Exception
-    {
-        String string = "Itrntinliztin";
-        StringBuilder sb = new StringBuilder();
-        sb.append( string );
-        for ( int i = 0; i < 4; i++ )
-        {
-            sb.append( sb.toString() ); // big string
-        }
-        string = sb.toString();
-        //System.out.println( "The string contains " + string.length() + " characters" );
-        String UTF8 = "UTF-8";
-        byte[] bytes = string.getBytes( UTF8 );
-
-        String cacheName = "testUTF8ByteArray";
-
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, byte[]> diskCache = new IndexedDiskCache<String, byte[]>( cattr );
-
-        // DO WORK
-        diskCache.update( new CacheElement<String, byte[]>( cacheName, "x", bytes ) );
-
-        // VERIFY
-        assertNotNull( diskCache.get( "x" ) );
-        Thread.sleep( 1000 );
-        ICacheElement<String, byte[]> afterElement = diskCache.get( "x" );
-        assertNotNull( afterElement );
-        // System.out.println( "afterElement = " + afterElement );
-        byte[] after = afterElement.getVal();
-
-        assertNotNull( after );
-        assertEquals( "wrong bytes after retrieval", string, new String( after, UTF8 ) );
-    }
-
-    /**
-     * Verify the item makes it to disk.
-     * <p>
-     * @throws IOException
-     */
-    public void testProcessUpdate_Simple()
-        throws IOException
-    {
-        // SETUP
-        String cacheName = "testProcessUpdate_Simple";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        String key = "myKey";
-        String value = "myValue";
-        ICacheElement<String, String> ce = new CacheElement<String, String>( cacheName, key, value );
-
-        // DO WORK
-        diskCache.processUpdate( ce );
-        ICacheElement<String, String> result = diskCache.processGet( key );
-
-        // VERIFY
-        assertNotNull( "Should have a result", result );
-        long fileSize = diskCache.getDataFileSize();
-        assertTrue( "File should be greater than 0", fileSize > 0 );
-    }
-
-    /**
-     * Verify the item makes it to disk.
-     * <p>
-     * @throws IOException
-     */
-    public void testProcessUpdate_SameKeySameSize()
-        throws IOException
-    {
-        // SETUP
-        String cacheName = "testProcessUpdate_SameKeySameSize";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        String key = "myKey";
-        String value = "myValue";
-        ICacheElement<String, String> ce1 = new CacheElement<String, String>( cacheName, key, value );
-
-        // DO WORK
-        diskCache.processUpdate( ce1 );
-        long fileSize1 = diskCache.getDataFileSize();
-
-        // DO WORK
-        ICacheElement<String, String> ce2 = new CacheElement<String, String>( cacheName, key, value );
-        diskCache.processUpdate( ce2 );
-        ICacheElement<String, String> result = diskCache.processGet( key );
-
-        // VERIFY
-        assertNotNull( "Should have a result", result );
-        long fileSize2 = diskCache.getDataFileSize();
-        assertEquals( "File should be the same", fileSize1, fileSize2 );
-        int binSize = diskCache.getRecyleBinSize();
-        assertEquals( "Should be nothing in the bin.", 0, binSize );
-    }
-
-    /**
-     * Verify the item makes it to disk.
-     * <p>
-     * @throws IOException
-     */
-    public void testProcessUpdate_SameKeySmallerSize()
-        throws IOException
-    {
-        // SETUP
-        String cacheName = "testProcessUpdate_SameKeySmallerSize";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        String key = "myKey";
-        String value = "myValue";
-        String value2 = "myValu";
-        ICacheElement<String, String> ce1 = new CacheElement<String, String>( cacheName, key, value );
-
-        // DO WORK
-        diskCache.processUpdate( ce1 );
-        long fileSize1 = diskCache.getDataFileSize();
-
-        // DO WORK
-        ICacheElement<String, String> ce2 = new CacheElement<String, String>( cacheName, key, value2 );
-        diskCache.processUpdate( ce2 );
-        ICacheElement<String, String> result = diskCache.processGet( key );
-
-        // VERIFY
-        assertNotNull( "Should have a result", result );
-        long fileSize2 = diskCache.getDataFileSize();
-        assertEquals( "File should be the same", fileSize1, fileSize2 );
-        int binSize = diskCache.getRecyleBinSize();
-        assertEquals( "Should be nothing in the bin.", 0, binSize );
-    }
-
-    /**
-     * Verify that the old slot gets in the recycle bin.
-     * <p>
-     * @throws IOException
-     */
-    public void testProcessUpdate_SameKeyBiggerSize()
-        throws IOException
-    {
-        // SETUP
-        String cacheName = "testProcessUpdate_SameKeyBiggerSize";
-        IndexedDiskCacheAttributes cattr = getCacheAttributes();
-        cattr.setCacheName( cacheName );
-        cattr.setMaxKeySize( 100 );
-        cattr.setDiskPath( "target/test-sandbox/IndexDiskCacheUnitTest" );
-        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>( cattr );
-
-        String key = "myKey";
-        String value = "myValue";
-        String value2 = "myValue2";
-        ICacheElement<String, String> ce1 = new CacheElement<String, String>( cacheName, key, value );
-
-        // DO WORK
-        diskCache.processUpdate( ce1 );
-        long fileSize1 = diskCache.getDataFileSize();
-
-        // DO WORK
-        ICacheElement<String, String> ce2 = new CacheElement<String, String>( cacheName, key, value2 );
-        diskCache.processUpdate( ce2 );
-        ICacheElement<String, String> result = diskCache.processGet( key );
-
-        // VERIFY
-        assertNotNull( "Should have a result", result );
-        long fileSize2 = diskCache.getDataFileSize();
-        assertTrue( "File should be greater.", fileSize1 < fileSize2 );
-        int binSize = diskCache.getRecyleBinSize();
-        assertEquals( "Should be one in the bin.", 1, binSize );
-    }
-}
diff --git a/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestAbstract.java b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestAbstract.java
new file mode 100644
index 000000000..a12afcf59
--- /dev/null
+++ b/commons-jcs-core/src/test/java/org/apache/commons/jcs/auxiliary/disk/indexed/IndexDiskCacheUnitTestAbstract.java
@@ -0,0 +1,991 @@
+package org.apache.commons.jcs.auxiliary.disk.indexed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.jcs.auxiliary.MockCacheEventLogger;
+import org.apache.commons.jcs.auxiliary.disk.DiskTestObject;
+import org.apache.commons.jcs.engine.CacheElement;
+import org.apache.commons.jcs.engine.ElementAttributes;
+import org.apache.commons.jcs.engine.behavior.ICacheElement;
+import org.apache.commons.jcs.engine.behavior.IElementAttributes;
+import org.apache.commons.jcs.engine.control.group.GroupAttrName;
+import org.apache.commons.jcs.engine.control.group.GroupId;
+import org.apache.commons.jcs.utils.timing.SleepUtil;
+
+/**
+ * Tests for common functionality.
+ * <p>
+ *
+ * @author Aaron Smuts
+ */
+public abstract class IndexDiskCacheUnitTestAbstract extends TestCase
+{
+    public abstract IndexedDiskCacheAttributes getCacheAttributes();
+
+    /**
+     * Simply verify that we can put items in the disk cache and retrieve them.
+     *
+     * @throws IOException
+     */
+    public void testSimplePutAndGet() throws IOException
+    {
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testSimplePutAndGet");
+        cattr.setMaxKeySize(1000);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>(cattr);
+
+        disk.processRemoveAll();
+
+        int cnt = 999;
+        for (int i = 0; i < cnt; i++)
+        {
+            IElementAttributes eAttr = new ElementAttributes();
+            eAttr.setIsSpool(true);
+            ICacheElement<String, String> element = new CacheElement<String, String>("testSimplePutAndGet", "key:" + i, "data:" + i);
+            element.setElementAttributes(eAttr);
+            disk.processUpdate(element);
+        }
+
+        for (int i = 0; i < cnt; i++)
+        {
+            ICacheElement<String, String> element = disk.processGet("key:" + i);
+            assertNotNull("Should have received an element.", element);
+            assertEquals("Element is wrong.", "data:" + i, element.getVal());
+        }
+
+        // Test that getMultiple returns all the expected values
+        Set<String> keys = new HashSet<String>();
+        for (int i = 0; i < cnt; i++)
+        {
+            keys.add("key:" + i);
+        }
+
+        Map<String, ICacheElement<String, String>> elements = disk.getMultiple(keys);
+        for (int i = 0; i < cnt; i++)
+        {
+            ICacheElement<String, String> element = elements.get("key:" + i);
+            assertNotNull("element " + i + ":key is missing", element);
+            assertEquals("value key:" + i, "data:" + i, element.getVal());
+        }
+        // System.out.println( disk.getStats() );
+    }
+
+    /**
+     * Add some items to the disk cache and then remove them one by one.
+     *
+     * @throws IOException
+     */
+    public void testRemoveItems() throws IOException
+    {
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRemoveItems");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>(cattr);
+
+        disk.processRemoveAll();
+
+        int cnt = 25;
+        for (int i = 0; i < cnt; i++)
+        {
+            IElementAttributes eAttr = new ElementAttributes();
+            eAttr.setIsSpool(true);
+            ICacheElement<String, String> element = new CacheElement<String, String>("testRemoveItems", "key:" + i, "data:" + i);
+            element.setElementAttributes(eAttr);
+            disk.processUpdate(element);
+        }
+
+        // remove each
+        for (int i = 0; i < cnt; i++)
+        {
+            disk.remove("key:" + i);
+            ICacheElement<String, String> element = disk.processGet("key:" + i);
+            assertNull("Should not have received an element.", element);
+        }
+    }
+
+    /**
+     * Verify that we don't override the largest item.
+     * <p>
+     *
+     * @throws IOException
+     */
+
+    /**
+     * Verify that the overlap check returns true when there are no overlaps.
+     */
+    public void testCheckForDedOverlaps_noOverlap()
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testCheckForDedOverlaps_noOverlap");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>(cattr);
+
+        int numDescriptors = 5;
+        int pos = 0;
+        IndexedDiskElementDescriptor[] sortedDescriptors = new IndexedDiskElementDescriptor[numDescriptors];
+        for (int i = 0; i < numDescriptors; i++)
+        {
+            IndexedDiskElementDescriptor descriptor = new IndexedDiskElementDescriptor(pos, i * 2);
+            pos = pos + (i * 2) + IndexedDisk.HEADER_SIZE_BYTES;
+            sortedDescriptors[i] = descriptor;
+        }
+
+        // DO WORK
+        boolean result = disk.checkForDedOverlaps(sortedDescriptors);
+
+        // VERIFY
+        assertTrue("There should be no overlap. it should be ok", result);
+    }
+
+    /**
+     * Verify that the overlap check returns false when there are overlaps.
+     */
+    public void testCheckForDedOverlaps_overlaps()
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testCheckForDedOverlaps_overlaps");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>(cattr);
+
+        int numDescriptors = 5;
+        int pos = 0;
+        IndexedDiskElementDescriptor[] sortedDescriptors = new IndexedDiskElementDescriptor[numDescriptors];
+        for (int i = 0; i < numDescriptors; i++)
+        {
+            IndexedDiskElementDescriptor descriptor = new IndexedDiskElementDescriptor(pos, i * 2);
+            // don't add the header + IndexedDisk.RECORD_HEADER;
+            pos = pos + (i * 2);
+            sortedDescriptors[i] = descriptor;
+        }
+
+        // DO WORK
+        boolean result = disk.checkForDedOverlaps(sortedDescriptors);
+
+        // VERIFY
+        assertFalse("There should be overlaps. it should be not ok", result);
+    }
+
+    /**
+     * Verify that the file size is as expected.
+     * <p>
+     *
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public void testFileSize() throws IOException, InterruptedException
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testFileSize");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>(cattr);
+
+        int numberToInsert = 20;
+        int bytes = 24;
+        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects(numberToInsert,
+            bytes, cattr.getCacheName());
+
+        for (int i = 0; i < elements.length; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        Thread.yield();
+        Thread.sleep(100);
+        Thread.yield();
+
+        long expectedSize = DiskTestObjectUtil.totalSize(elements, numberToInsert);
+        long resultSize = disk.getDataFileSize();
+
+        // System.out.println( "testFileSize stats " + disk.getStats() );
+
+        assertEquals("Wrong file size", expectedSize, resultSize);
+    }
+
+    /**
+     * Verify that items are added to the recycle bin on removal.
+     * <p>
+     *
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public void testRecyleBinSize() throws IOException, InterruptedException
+    {
+        // SETUP
+        int numberToInsert = 20;
+
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRecyleBinSize");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        cattr.setMaxRecycleBinSize(numberToInsert);
+        cattr.setOptimizeAtRemoveCount(numberToInsert);
+        cattr.setMaxKeySize(numberToInsert * 2);
+        cattr.setMaxPurgatorySize(numberToInsert);
+        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>(cattr);
+
+        int bytes = 1;
+        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects(numberToInsert,
+            bytes, cattr.getCacheName());
+
+        for (int i = 0; i < elements.length; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        Thread.yield();
+        Thread.sleep(100);
+        Thread.yield();
+
+        // remove half
+        int numberToRemove = elements.length / 2;
+        for (int i = 0; i < numberToRemove; i++)
+        {
+            disk.processRemove(elements[i].getKey());
+        }
+
+        // verify that the recycle bin has the correct amount.
+        assertEquals("The recycle bin should have the number removed.", numberToRemove, disk.getRecyleBinSize());
+    }
+
+    /**
+     * Verify that items of the same size use recycle bin spots. Setup the recycle bin by removing
+     * some items. Add some of the same size. Verify that the recycle count is the number added.
+     * <p>
+     *
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public void testRecyleBinUsage() throws IOException, InterruptedException
+    {
+        // SETUP
+        int numberToInsert = 20;
+
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRecyleBinUsage");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        cattr.setMaxRecycleBinSize(numberToInsert);
+        cattr.setOptimizeAtRemoveCount(numberToInsert);
+        cattr.setMaxKeySize(numberToInsert * 2);
+        cattr.setMaxPurgatorySize(numberToInsert);
+        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>(cattr);
+
+        // we will reuse these
+        int bytes = 1;
+        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects(numberToInsert,
+            bytes, cattr.getCacheName());
+
+        // Add some to the disk
+        for (int i = 0; i < elements.length; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        Thread.yield();
+        Thread.sleep(100);
+        Thread.yield();
+
+        // remove half of those added
+        int numberToRemove = elements.length / 2;
+        for (int i = 0; i < numberToRemove; i++)
+        {
+            disk.processRemove(elements[i].getKey());
+        }
+
+        // verify that the recycle bin has the correct amount.
+        assertEquals("The recycle bin should have the number removed.", numberToRemove, disk.getRecyleBinSize());
+
+        // add half as many as we removed. These should all use spots in the recycle bin.
+        int numberToAdd = numberToRemove / 2;
+        for (int i = 0; i < numberToAdd; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        // verify that we used the correct number of spots
+        assertEquals("The recycle bin should have the number removed." + disk.getStats(), numberToAdd, disk.getRecyleCount());
+    }
+
+    /**
+     * Verify that the data size is as expected after a remove and after a put that should use the
+     * spots.
+     * <p>
+     *
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public void testBytesFreeSize() throws IOException, InterruptedException
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testBytesFreeSize");
+        cattr.setDiskPath("target/test-sandbox/UnitTest");
+        IndexedDiskCache<Integer, DiskTestObject> disk = new IndexedDiskCache<Integer, DiskTestObject>(cattr);
+
+        int numberToInsert = 20;
+        int bytes = 24;
+        ICacheElement<Integer, DiskTestObject>[] elements = DiskTestObjectUtil.createCacheElementsWithTestObjects(numberToInsert,
+            bytes, cattr.getCacheName());
+
+        for (int i = 0; i < elements.length; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        Thread.yield();
+        Thread.sleep(100);
+        Thread.yield();
+
+        // remove half of those added
+        int numberToRemove = elements.length / 2;
+        for (int i = 0; i < numberToRemove; i++)
+        {
+            disk.processRemove(elements[i].getKey());
+        }
+
+        long expectedSize = DiskTestObjectUtil.totalSize(elements, numberToRemove);
+        long resultSize = disk.getBytesFree();
+
+        // System.out.println( "testBytesFreeSize stats " + disk.getStats() );
+
+        assertEquals("Wrong bytes free size" + disk.getStats(), expectedSize, resultSize);
+
+        // add half as many as we removed. These should all use spots in the recycle bin.
+        int numberToAdd = numberToRemove / 2;
+        for (int i = 0; i < numberToAdd; i++)
+        {
+            disk.processUpdate(elements[i]);
+        }
+
+        long expectedSize2 = DiskTestObjectUtil.totalSize(elements, numberToAdd);
+        long resultSize2 = disk.getBytesFree();
+        assertEquals("Wrong bytes free size" + disk.getStats(), expectedSize2, resultSize2);
+    }
+
+    /**
+     * Add some items to the disk cache and then remove them one by one.
+     * <p>
+     *
+     * @throws IOException
+     */
+    public void testRemove_PartialKey() throws IOException
+    {
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRemove_PartialKey");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> disk = new IndexedDiskCache<String, String>(cattr);
+
+        disk.processRemoveAll();
+
+        int cnt = 25;
+        for (int i = 0; i < cnt; i++)
+        {
+            IElementAttributes eAttr = new ElementAttributes();
+            eAttr.setIsSpool(true);
+            ICacheElement<String, String> element = new CacheElement<String, String>("testRemove_PartialKey", i + ":key", "data:"
+                + i);
+            element.setElementAttributes(eAttr);
+            disk.processUpdate(element);
+        }
+
+        // verif each
+        for (int i = 0; i < cnt; i++)
+        {
+            ICacheElement<String, String> element = disk.processGet(i + ":key");
+            assertNotNull("Shoulds have received an element.", element);
+        }
+
+        // remove each
+        for (int i = 0; i < cnt; i++)
+        {
+            disk.remove(i + ":");
+            ICacheElement<String, String> element = disk.processGet(i + ":key");
+            assertNull("Should not have received an element.", element);
+        }
+        // https://issues.apache.org/jira/browse/JCS-67
+        assertEquals("Recylenbin should not have more elements than we removed. Check for JCS-67", cnt, disk.getRecyleBinSize());
+    }
+
+    /**
+     * Verify that group members are removed if we call remove with a group.
+     *
+     * @throws IOException
+     */
+    public void testRemove_Group() throws IOException
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRemove_Group");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<GroupAttrName<String>, String> disk = new IndexedDiskCache<GroupAttrName<String>, String>(cattr);
+
+        disk.processRemoveAll();
+
+        String cacheName = "testRemove_Group_Region";
+        String groupName = "testRemove_Group";
+
+        int cnt = 25;
+        for (int i = 0; i < cnt; i++)
+        {
+            GroupAttrName<String> groupAttrName = getGroupAttrName(cacheName, groupName, i + ":key");
+            CacheElement<GroupAttrName<String>, String> element = new CacheElement<GroupAttrName<String>, String>(cacheName,
+                groupAttrName, "data:" + i);
+
+            IElementAttributes eAttr = new ElementAttributes();
+            eAttr.setIsSpool(true);
+            element.setElementAttributes(eAttr);
+
+            disk.processUpdate(element);
+        }
+
+        // verify each
+        for (int i = 0; i < cnt; i++)
+        {
+            GroupAttrName<String> groupAttrName = getGroupAttrName(cacheName, groupName, i + ":key");
+            ICacheElement<GroupAttrName<String>, String> element = disk.processGet(groupAttrName);
+            assertNotNull("Should have received an element.", element);
+        }
+
+        // DO WORK
+        // remove the group
+        disk.remove(getGroupAttrName(cacheName, groupName, null));
+
+        for (int i = 0; i < cnt; i++)
+        {
+            GroupAttrName<String> groupAttrName = getGroupAttrName(cacheName, groupName, i + ":key");
+            ICacheElement<GroupAttrName<String>, String> element = disk.processGet(groupAttrName);
+
+            // VERIFY
+            assertNull("Should not have received an element.", element);
+        }
+
+    }
+
+    /**
+     * Internal method used for group functionality.
+     * <p>
+     *
+     * @param cacheName
+     * @param group
+     * @param name
+     * @return GroupAttrName
+     */
+    private GroupAttrName<String> getGroupAttrName(String cacheName, String group, String name)
+    {
+        GroupId gid = new GroupId(cacheName, group);
+        return new GroupAttrName<String>(gid, name);
+    }
+
+    /**
+     * Verify event log calls.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUpdate_EventLogging_simple() throws Exception
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testUpdate_EventLogging_simple");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTestCEL");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+        diskCache.processRemoveAll();
+
+        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
+        diskCache.setCacheEventLogger(cacheEventLogger);
+
+        ICacheElement<String, String> item = new CacheElement<String, String>("region", "key", "value");
+
+        // DO WORK
+        diskCache.update(item);
+
+        SleepUtil.sleepAtLeast(200);
+
+        // VERIFY
+        assertEquals("Start should have been called.", 1, cacheEventLogger.startICacheEventCalls);
+        assertEquals("End should have been called.", 1, cacheEventLogger.endICacheEventCalls);
+    }
+
+    /**
+     * Verify event log calls.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testGet_EventLogging_simple() throws Exception
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testGet_EventLogging_simple");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTestCEL");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+        diskCache.processRemoveAll();
+
+        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
+        diskCache.setCacheEventLogger(cacheEventLogger);
+
+        // DO WORK
+        diskCache.get("key");
+
+        // VERIFY
+        assertEquals("Start should have been called.", 1, cacheEventLogger.startICacheEventCalls);
+        assertEquals("End should have been called.", 1, cacheEventLogger.endICacheEventCalls);
+    }
+
+    /**
+     * Verify event log calls.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testGetMultiple_EventLogging_simple() throws Exception
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testGetMultiple_EventLogging_simple");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTestCEL");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+        diskCache.processRemoveAll();
+
+        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
+        diskCache.setCacheEventLogger(cacheEventLogger);
+
+        Set<String> keys = new HashSet<String>();
+        keys.add("junk");
+
+        // DO WORK
+        diskCache.getMultiple(keys);
+
+        // VERIFY
+        // 1 for get multiple and 1 for get.
+        assertEquals("Start should have been called.", 2, cacheEventLogger.startICacheEventCalls);
+        assertEquals("End should have been called.", 2, cacheEventLogger.endICacheEventCalls);
+    }
+
+    /**
+     * Verify event log calls.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testRemove_EventLogging_simple() throws Exception
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRemoveAll_EventLogging_simple");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTestCEL");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+        diskCache.processRemoveAll();
+
+        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
+        diskCache.setCacheEventLogger(cacheEventLogger);
+
+        // DO WORK
+        diskCache.remove("key");
+
+        // VERIFY
+        assertEquals("Start should have been called.", 1, cacheEventLogger.startICacheEventCalls);
+        assertEquals("End should have been called.", 1, cacheEventLogger.endICacheEventCalls);
+    }
+
+    /**
+     * Verify event log calls.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testRemoveAll_EventLogging_simple() throws Exception
+    {
+        // SETUP
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName("testRemoveAll_EventLogging_simple");
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTestCEL");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+        diskCache.processRemoveAll();
+
+        MockCacheEventLogger cacheEventLogger = new MockCacheEventLogger();
+        diskCache.setCacheEventLogger(cacheEventLogger);
+
+        // DO WORK
+        diskCache.remove("key");
+
+        // VERIFY
+        assertEquals("Start should have been called.", 1, cacheEventLogger.startICacheEventCalls);
+        assertEquals("End should have been called.", 1, cacheEventLogger.endICacheEventCalls);
+    }
+
+    /**
+     * Test the basic get matching.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testPutGetMatching_SmallWait() throws Exception
+    {
+        // SETUP
+        int items = 200;
+
+        String cacheName = "testPutGetMatching_SmallWait";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        // DO WORK
+        for (int i = 0; i <= items; i++)
+        {
+            diskCache.update(new CacheElement<String, String>(cacheName, i + ":key", cacheName + " data " + i));
+        }
+        Thread.sleep(500);
+
+        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching("1.8.+");
+
+        // VERIFY
+        assertEquals("Wrong number returned", 10, matchingResults.size());
+        // System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
+        // System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
+    }
+
+    /**
+     * Test the basic get matching. With no wait this will all come from purgatory.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testPutGetMatching_NoWait() throws Exception
+    {
+        // SETUP
+        int items = 200;
+
+        String cacheName = "testPutGetMatching_NoWait";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        // DO WORK
+        for (int i = 0; i <= items; i++)
+        {
+            diskCache.update(new CacheElement<String, String>(cacheName, i + ":key", cacheName + " data " + i));
+        }
+
+        Map<String, ICacheElement<String, String>> matchingResults = diskCache.getMatching("1.8.+");
+
+        // VERIFY
+        assertEquals("Wrong number returned", 10, matchingResults.size());
+        // System.out.println( "matchingResults.keySet() " + matchingResults.keySet() );
+        // System.out.println( "\nAFTER TEST \n" + diskCache.getStats() );
+    }
+
+    /**
+     * Verify that the block disk cache can handle utf encoded strings.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUTF8String() throws Exception
+    {
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+
+        // System.out.println( "The string contains " + string.length() + " characters" );
+
+        String cacheName = "testUTF8String";
+
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, String>(cacheName, "x", string));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, String> afterElement = diskCache.get("x");
+        assertNotNull(afterElement);
+        // System.out.println( "afterElement = " + afterElement );
+        String after = afterElement.getVal();
+
+        assertNotNull(after);
+        assertEquals("wrong string after retrieval", string, after);
+    }
+
+    /**
+     * Verify that the block disk cache can handle utf encoded strings.
+     * <p>
+     *
+     * @throws Exception
+     */
+    public void testUTF8ByteArray() throws Exception
+    {
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+        // System.out.println( "The string contains " + string.length() + " characters" );
+        String UTF8 = "UTF-8";
+        byte[] bytes = string.getBytes(UTF8);
+
+        String cacheName = "testUTF8ByteArray";
+
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, byte[]> diskCache = new IndexedDiskCache<String, byte[]>(cattr);
+
+        // DO WORK
+        diskCache.update(new CacheElement<String, byte[]>(cacheName, "x", bytes));
+
+        // VERIFY
+        assertNotNull(diskCache.get("x"));
+        Thread.sleep(1000);
+        ICacheElement<String, byte[]> afterElement = diskCache.get("x");
+        assertNotNull(afterElement);
+        // System.out.println( "afterElement = " + afterElement );
+        byte[] after = afterElement.getVal();
+
+        assertNotNull(after);
+        assertEquals("wrong bytes after retrieval", string, new String(after, UTF8));
+    }
+
+    /**
+     * Verify the item makes it to disk.
+     * <p>
+     *
+     * @throws IOException
+     */
+    public void testProcessUpdate_Simple() throws IOException
+    {
+        // SETUP
+        String cacheName = "testProcessUpdate_Simple";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        String key = "myKey";
+        String value = "myValue";
+        ICacheElement<String, String> ce = new CacheElement<String, String>(cacheName, key, value);
+
+        // DO WORK
+        diskCache.processUpdate(ce);
+        ICacheElement<String, String> result = diskCache.processGet(key);
+
+        // VERIFY
+        assertNotNull("Should have a result", result);
+        long fileSize = diskCache.getDataFileSize();
+        assertTrue("File should be greater than 0", fileSize > 0);
+    }
+
+    /**
+     * Verify the item makes it to disk.
+     * <p>
+     *
+     * @throws IOException
+     */
+    public void testProcessUpdate_SameKeySameSize() throws IOException
+    {
+        // SETUP
+        String cacheName = "testProcessUpdate_SameKeySameSize";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        String key = "myKey";
+        String value = "myValue";
+        ICacheElement<String, String> ce1 = new CacheElement<String, String>(cacheName, key, value);
+
+        // DO WORK
+        diskCache.processUpdate(ce1);
+        long fileSize1 = diskCache.getDataFileSize();
+
+        // DO WORK
+        ICacheElement<String, String> ce2 = new CacheElement<String, String>(cacheName, key, value);
+        diskCache.processUpdate(ce2);
+        ICacheElement<String, String> result = diskCache.processGet(key);
+
+        // VERIFY
+        assertNotNull("Should have a result", result);
+        long fileSize2 = diskCache.getDataFileSize();
+        assertEquals("File should be the same", fileSize1, fileSize2);
+        int binSize = diskCache.getRecyleBinSize();
+        assertEquals("Should be nothing in the bin.", 0, binSize);
+    }
+
+    /**
+     * Verify the item makes it to disk.
+     * <p>
+     *
+     * @throws IOException
+     */
+    public void testProcessUpdate_SameKeySmallerSize() throws IOException
+    {
+        // SETUP
+        String cacheName = "testProcessUpdate_SameKeySmallerSize";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        String key = "myKey";
+        String value = "myValue";
+        String value2 = "myValu";
+        ICacheElement<String, String> ce1 = new CacheElement<String, String>(cacheName, key, value);
+
+        // DO WORK
+        diskCache.processUpdate(ce1);
+        long fileSize1 = diskCache.getDataFileSize();
+
+        // DO WORK
+        ICacheElement<String, String> ce2 = new CacheElement<String, String>(cacheName, key, value2);
+        diskCache.processUpdate(ce2);
+        ICacheElement<String, String> result = diskCache.processGet(key);
+
+        // VERIFY
+        assertNotNull("Should have a result", result);
+        long fileSize2 = diskCache.getDataFileSize();
+        assertEquals("File should be the same", fileSize1, fileSize2);
+        int binSize = diskCache.getRecyleBinSize();
+        assertEquals("Should be nothing in the bin.", 0, binSize);
+    }
+
+    /**
+     * Verify that the old slot gets in the recycle bin.
+     * <p>
+     *
+     * @throws IOException
+     */
+    public void testProcessUpdate_SameKeyBiggerSize() throws IOException
+    {
+        // SETUP
+        String cacheName = "testProcessUpdate_SameKeyBiggerSize";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/IndexDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        String key = "myKey";
+        String value = "myValue";
+        String value2 = "myValue2";
+        ICacheElement<String, String> ce1 = new CacheElement<String, String>(cacheName, key, value);
+
+        // DO WORK
+        diskCache.processUpdate(ce1);
+        long fileSize1 = diskCache.getDataFileSize();
+
+        // DO WORK
+        ICacheElement<String, String> ce2 = new CacheElement<String, String>(cacheName, key, value2);
+        diskCache.processUpdate(ce2);
+        ICacheElement<String, String> result = diskCache.processGet(key);
+
+        // VERIFY
+        assertNotNull("Should have a result", result);
+        long fileSize2 = diskCache.getDataFileSize();
+        assertTrue("File should be greater.", fileSize1 < fileSize2);
+        int binSize = diskCache.getRecyleBinSize();
+        assertEquals("Should be one in the bin.", 1, binSize);
+    }
+
+    public void testLoadFromDisk() throws Exception
+    {
+        for (int i = 0; i < 15; i++)
+        { // usually after 2 time it fails
+            oneLoadFromDisk();
+        }
+    }
+
+    public void oneLoadFromDisk() throws Exception
+    {
+        // initialize object to be stored
+        String string = "Itrntinliztin";
+        StringBuilder sb = new StringBuilder();
+        sb.append(string);
+        for (int i = 0; i < 4; i++)
+        {
+            sb.append(sb.toString()); // big string
+        }
+        string = sb.toString();
+
+        // initialize cache
+        String cacheName = "testLoadFromDisk";
+        IndexedDiskCacheAttributes cattr = getCacheAttributes();
+        cattr.setCacheName(cacheName);
+        cattr.setMaxKeySize(100);
+        cattr.setDiskPath("target/test-sandbox/BlockDiskCacheUnitTest");
+        IndexedDiskCache<String, String> diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        // DO WORK
+        for (int i = 0; i < 50; i++)
+        {
+            diskCache.update(new CacheElement<String, String>(cacheName, "x" + i, string));
+        }
+        // Thread.sleep(1000);
+        // VERIFY
+        diskCache.dispose();
+        // Thread.sleep(1000);
+
+        diskCache = new IndexedDiskCache<String, String>(cattr);
+
+        for (int i = 0; i < 50; i++)
+        {
+            ICacheElement<String, String> afterElement = diskCache.get("x" + i);
+            assertNotNull("Missing element from cache. Cache size: " + diskCache.getSize() + " element: x" + i, afterElement);
+            assertEquals("wrong string after retrieval", string, afterElement.getVal());
+        }
+    }
+}
diff --git a/src/changes/changes.xml b/src/changes/changes.xml
index f3b3d741f..ae0c358e2 100644
--- a/src/changes/changes.xml
+++ b/src/changes/changes.xml
@@ -20,6 +20,9 @@
 	</properties>
 	<body>
         <release version="2.0" date="unreleased" description="JDK 1.6 based major release">
+            <action issue="JCS-153" dev="tv" type="fix" due-to="Wiktor Niesiobedzki">
+                Fix file size limitation for Block Disk Cache and Indexed Disk Cache
+            </action>
             <action issue="JCS-149" dev="tv" type="fix" due-to="Youngho Cho">
                 When reading keys from disk, a StreamCorruptedException happens when a custom serializer is applied
             </action>
