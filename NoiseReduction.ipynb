{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from openai import OpenAI\n",
    "import configparser\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import io\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "random_seed = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 5\n",
    "df = pd.read_csv(\"./data/GoldSet.csv\")\n",
    "df[\"Diff\"] = df[\"Diff\"].apply(lambda x: x.replace(\"\\\\ No newline at end of file\",\"\").strip())\n",
    "\n",
    "buggy_df = df[df.Decision == 'Buggy'][[\"Diff\", \"Decision\", \"CommitMessage\"]]\n",
    "rows = np.random.choice(buggy_df.index.values,sample_count)\n",
    "buggy_df = buggy_df.loc[rows]\n",
    "\n",
    "notbuggy_df = df[df.Decision == 'NotBuggy'][[\"Diff\", \"Decision\", \"CommitMessage\"]]\n",
    "rows = np.random.choice(notbuggy_df.index.values,sample_count)\n",
    "notbuggy_df = notbuggy_df.loc[rows]\n",
    "\n",
    "few_shot_data = [*buggy_df.values.tolist() , *notbuggy_df.values.tolist()]\n",
    "json.dump(few_shot_data,open(\"./data/FewShots.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Untangler:\n",
    "    def __init__(self, model_name):\n",
    "        self.__setup()\n",
    "\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('.config')\n",
    "        GEMINI_API_KEY = config[\"API_KEYS\"][\"GEMINI_API_KEY\"]\n",
    "        OPENAI_API_KEY = config[\"API_KEYS\"][\"OPENAI_API_KEY\"]\n",
    "        DEEPSEEK_API_KEY = config[\"API_KEYS\"][\"DEEPSEEK_API_KEY\"]\n",
    "\n",
    "        self.model_name = model_name\n",
    "        if self.model_name == \"gemini\":\n",
    "            self.__client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        elif self.model_name in [\"openai\", \"deepseek\"]:\n",
    "                if self.model_name == \"openai\":\n",
    "                    self.__client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "        else:\n",
    "             raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    def __setup(self):\n",
    "        self.__gemini_model_name = \"gemini-2.0-flash\"\n",
    "        self.__openai_model_name = \"gpt-4o-mini\"\n",
    "        self.__deepseek_model_name = \"\"\n",
    "\n",
    "        self.__batch_file_name = \"request.jsonl\"\n",
    "\n",
    "        self.__initial_prompt = \"You are a Git commit review assistant. Given a Java source code diff and its commit message, analyze both to determine if the changes align with the described bug fix. Assess the relevance between the commit message and code modifications, identifying patterns such as error-handling updates, logical corrections, exception-handling improvements, and other indicators of bug-related changes. Use the provided examples as reference points to enhance accuracy in detecting bug-related changes.\"\n",
    "        \n",
    "        self.__few_shot_data = json.load(open(\"./data/FewShots.json\",\"r\"))\n",
    "        random.Random(random_seed).shuffle(self.__few_shot_data)\n",
    "    \n",
    "    def __prepare_prompt_for_openai(self, commitMessage, diff):\n",
    "        messages = [{\"role\": \"developer\", \"content\": self.__initial_prompt}]\n",
    "        \n",
    "        for item in self.__few_shot_data:\n",
    "            item[0] = item[0].strip()\n",
    "            item[2] = item[2].replace(\"\\n\",\" \").strip()\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Commit Messaage: {item[2]}\\nGit Diff:\\n{item[0]}\"})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": item[1]})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Commit Messaage: {commitMessage}\\nGit Diff:\\n{diff}\"})\n",
    "        return messages\n",
    "    \n",
    "    def prepare_batch_prompt_for_openai(self, model, messages):\n",
    "        if os.path.exists(self.__batch_file_name):\n",
    "            os.remove(self.__batch_file_name)\n",
    "\n",
    "        jsonl_file = open(self.__batch_file_name, \"a\")\n",
    "\n",
    "        for i, message in enumerate(messages):\n",
    "            request = {\n",
    "                \"custom_id\": f\"request-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": message\n",
    "                }\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(request)+\"\\n\")\n",
    "        jsonl_file.close()\n",
    "    \n",
    "    def __prepare_prompt_for_gemini(self, commitMessage, diff):\n",
    "        few_shots = \"\\n\"\n",
    "        for item in self.__few_shot_data:\n",
    "            item[0] = item[0].strip()\n",
    "            item[2] = item[2].replace(\"\\n\",\" \").strip()\n",
    "            few_shots = few_shots + f\"\\nCommit Messaage: {item[2]}\\nGit Diff:\\n{item[0]}\\nAnswer: {item[1]}\\n\"\n",
    "\n",
    "        question = f\"\\nCommit Messaage: {commitMessage}\\nGit Diff:\\n{diff}\\nAnswer:\"\n",
    "\n",
    "        return few_shots + question\n",
    "\n",
    "    def prepare_prompt(self, commitMessage, diff):\n",
    "        if self.model_name == \"gemini\":\n",
    "             self.prompt = self.__prepare_prompt_for_gemini(commitMessage, diff)\n",
    "        if self.model_name == \"openai\":\n",
    "            self.prompt = self.__prepare_prompt_for_openai(commitMessage, diff)\n",
    "        else:\n",
    "             raise NotImplementedError()\n",
    "    \n",
    "    def print_prompt(self):\n",
    "        prompt = self.prompt\n",
    "        if prompt == \"\":\n",
    "             print(\"Printing last prompt...\")\n",
    "             prompt = self.last_prompt\n",
    "\n",
    "        print(\"Prompt for:\", self.model_name)\n",
    "        print()\n",
    "        print(prompt)\n",
    "    \n",
    "    def get_prompt(self):\n",
    "        prompt = self.prompt\n",
    "        if prompt == \"\":\n",
    "            prompt = self.last_prompt\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def detect(self):\n",
    "        prediction = \"\"\n",
    "        if self.prompt == \"\":\n",
    "            raise ValueError(\"Provide a new diff using prepare_prompt()\")\n",
    "        else:\n",
    "            if self.model_name == \"gemini\":\n",
    "                response = self.__client.models.generate_content(\n",
    "                    model=self.__gemini_model_name,\n",
    "                    config=genai.types.GenerateContentConfig(\n",
    "                        system_instruction=self.__initial_prompt,\n",
    "                        temperature=0.3,\n",
    "                        max_output_tokens=3\n",
    "                    ),\n",
    "                    contents=self.prompt\n",
    "                )\n",
    "                prediction = response.text\n",
    "\n",
    "            elif self.model_name == \"openai\":\n",
    "                completion = self.__client.chat.completions.create(\n",
    "                    model=self.__openai_model_name,\n",
    "                    messages=self.prompt,\n",
    "                    temperature=0.3\n",
    "                )\n",
    "                prediction = completion.choices[0].message.content\n",
    "                \n",
    "            elif self.model_name == \"deepseek\":\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            self.last_prompt = self.prompt\n",
    "            self.prompt = \"\"\n",
    "\n",
    "            return prediction.strip()\n",
    "        \n",
    "    def batch_detect(self, df):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for index, row in df.iterrows():\n",
    "            y_true.append(row[\"Decision\"])\n",
    "\n",
    "        if self.model_name == \"gemini\":\n",
    "            for index, row in tqdm(df.iterrows()):\n",
    "                error = True\n",
    "                self.prepare_prompt(row[\"CommitMessage\"], row[\"Diff\"])\n",
    "                while error:\n",
    "                    try:\n",
    "                        pred = self.predict()\n",
    "                        error = False\n",
    "                    except genai.errors.ClientError:\n",
    "                        error = True\n",
    "                        time.sleep(60)\n",
    "                y_pred.append(pred)\n",
    "                y_true.append(row[\"Decision\"])\n",
    "                time.sleep(3)\n",
    "                \n",
    "        elif self.model_name == \"openai\":\n",
    "            \n",
    "            messages = []\n",
    "            for index, row in tqdm(df.iterrows()):\n",
    "                messages.append(self.__prepare_prompt_for_openai(row[\"CommitMessage\"], row[\"Diff\"]))\n",
    "\n",
    "            self.prepare_batch_prompt_for_openai(self.__openai_model_name, messages)\n",
    "\n",
    "            batch_input_file = self.__client.files.create(\n",
    "                file=open(self.__batch_file_name, \"rb\"),\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "\n",
    "            batch_input_file_id = batch_input_file.id\n",
    "            self.__current_openai_batch = self.__client.batches.create(\n",
    "                input_file_id=batch_input_file_id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\",\n",
    "            )\n",
    "\n",
    "            self.__current_openai_batch, y_pred = self.get_batch_result()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def get_batch_result(self, batch_id = None):\n",
    "        if batch_id == None:\n",
    "            batch_id = self.__current_openai_batch.id\n",
    "\n",
    "        batch = self.__client.batches.retrieve(batch_id)\n",
    "\n",
    "        with tqdm(total=batch.request_counts.total) as pbar:\n",
    "            pbar.update(batch.request_counts.completed)\n",
    "            while batch.status != \"completed\":\n",
    "                time.sleep(5)\n",
    "                batch = self.__client.batches.retrieve(batch_id)\n",
    "                pbar.update(max(0, batch.request_counts.completed - pbar.n))\n",
    "\n",
    "        y_pred = []\n",
    "        if batch.status == \"completed\":\n",
    "            file_response = self.__client.files.content(batch.output_file_id)\n",
    "            jsonObj = pd.read_json(io.StringIO(file_response.text), lines=True)\n",
    "            for r in jsonObj[\"response\"].to_list():\n",
    "                y_pred.append(r[\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "        return batch, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/GoldSet.csv\")\n",
    "df[\"Diff\"] = df[\"Diff\"].apply(lambda x: x.replace(\"\\\\ No newline at end of file\",\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangler = Untangler(\"gemini\")\n",
    "y_true, y_pred = untangler.batch_detect(df)\n",
    "accuracy_score(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Buggy       0.72      0.95      0.82       144\n",
      "    NotBuggy       0.91      0.56      0.69       122\n",
      "\n",
      "    accuracy                           0.77       266\n",
      "   macro avg       0.81      0.75      0.75       266\n",
      "weighted avg       0.80      0.77      0.76       266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangler = Untangler(\"openai\")\n",
    "untangler.get_batch_result(\"batch_67ba5b015398819094fe8cc1b5928ae6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7819548872180451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Buggy       0.73      0.94      0.82       144\n",
      "    NotBuggy       0.89      0.60      0.72       122\n",
      "\n",
      "    accuracy                           0.78       266\n",
      "   macro avg       0.81      0.77      0.77       266\n",
      "weighted avg       0.81      0.78      0.77       266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "untangler = Untangler(\"openai\")\n",
    "\n",
    "y_true = []\n",
    "for index, row in df.iterrows():\n",
    "    y_true.append(row[\"Decision\"])\n",
    "batch, y_pred = untangler.get_batch_result(\"batch_67ba558986f08190bb2c61a32d87a2e3\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
