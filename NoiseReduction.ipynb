{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from modules.Untangler import Untangler\n",
    "\n",
    "random_seed = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 5\n",
    "df = pd.read_csv(\"./data/GoldSet.csv\")\n",
    "df[\"Diff\"] = df[\"Diff\"].apply(lambda x: x.replace(\"\\\\ No newline at end of file\",\"\").strip())\n",
    "\n",
    "buggy_df = df[df.Decision == 'Buggy'][[\"CommitMessage\", \"Diff\", \"Decision\"]]\n",
    "rows = np.random.choice(buggy_df.index.values,sample_count)\n",
    "buggy_df = buggy_df.loc[rows]\n",
    "\n",
    "notbuggy_df = df[df.Decision == 'NotBuggy'][[\"CommitMessage\", \"Diff\", \"Decision\"]]\n",
    "rows = np.random.choice(notbuggy_df.index.values,sample_count)\n",
    "notbuggy_df = notbuggy_df.loc[rows]\n",
    "\n",
    "few_shot_data = [*buggy_df.values.tolist() , *notbuggy_df.values.tolist()]\n",
    "json.dump(few_shot_data,open(\"./data/FewShots.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/GoldSet.csv\")\n",
    "#df[\"Diff\"] = df[\"Diff\"].apply(lambda x: x.replace(\"\\\\ No newline at end of file\",\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai\n",
      "Prompt for: openai\n",
      "\n",
      "[{'role': 'developer', 'content': 'You are a Git commit review assistant. Given a Java source code diff and its commit message, analyze both to determine if the changes align with the described bug fix. Assess the relevance between the commit message and code modifications, identifying patterns such as error-handling updates, logical corrections, exception-handling improvements, and other indicators of bug-related changes. Use the provided examples as reference points to enhance accuracy in detecting bug-related changes.'}, {'role': 'user', 'content': 'Commit Messaage: Fix two bugs in MinMaxPriorityQueue (introduced in []). First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.\\n\\nReported externally at https://github.com/google/guava/issues/2658\\n\\n-------------\\nCreated by MOE: https://github.com/google/moe\\nMOE_MIGRATED_REVID=140382230\\nGit Diff:\\n@@ -1,16 +1,16 @@\\n-    int getCorrectLastElement(E actualLastElement) {\\n+    int swapWithConceptuallyLastElement(E actualLastElement) {\\n       int parentIndex = getParentIndex(size);\\n       if (parentIndex != 0) {\\n         int grandparentIndex = getParentIndex(parentIndex);\\n         int uncleIndex = getRightChildIndex(grandparentIndex);\\n         if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {\\n           E uncleElement = elementData(uncleIndex);\\n           if (ordering.compare(uncleElement, actualLastElement) < 0) {\\n             queue[uncleIndex] = actualLastElement;\\n             queue[size] = uncleElement;\\n             return uncleIndex;\\n           }\\n         }\\n       }\\n       return size;\\n     }\\n\\\\ No newline at end of file'}, {'role': 'assistant', 'content': 'This diff appears to rename the method getCorrectLastElement to swapWithConceptuallyLastElement without making any functional changes to the logic. Since the reported bugs involve incorrect element removal and duplicate iteration during removal, this change does not directly address those issues. It is likely a refactor or clarification rather than a bug fix. The answer is NotBuggy.'}, {'role': 'user', 'content': 'Commit Messaage: Fix Quality flaws\\n\\nGit Diff:\\n@@ -1,7 +1,9 @@\\n   static void writeResponse(BulkChangeResult result, Response response) {\\n-    JsonWriter json = response.newJsonWriter().beginObject();\\n-    json.prop(\"succeeded\", result.countSucceeded());\\n-    json.prop(\"failed\", result.countFailed());\\n-    WebServiceEngine.writeErrors(json, result.getErrors());\\n-    json.endObject().close();\\n+    try (JsonWriter json = response.newJsonWriter()) {\\n+      json.beginObject();\\n+      json.prop(\"succeeded\", result.countSucceeded());\\n+      json.prop(\"failed\", result.countFailed());\\n+      WebServiceEngine.writeErrors(json, result.getErrors());\\n+      json.endObject();\\n+    }\\n   }\\n\\\\ No newline at end of file\\n'}]\n"
     ]
    }
   ],
   "source": [
    "untangler = Untangler(\"openai\", True)\n",
    "untangler.prepare_prompt(df[\"CommitMessage\"].iloc[250], df[\"Diff\"].iloc[250])\n",
    "untangler.print_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NotBuggy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Decision\"].iloc[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The commit message \"Fix Quality flaws\" suggests that the changes made are intended to improve the quality of the code rather than specifically addressing a bug. The code diff shows a modification in how the `JsonWriter` is handled, changing it from a manual instantiation and closure to a try-with-resources statement. \\n\\nThis change improves resource management by ensuring that the `JsonWriter` is properly closed after use, which can prevent potential resource leaks. While this is a quality improvement and may indirectly prevent issues related to resource handling, it does not directly address any specific bugs or errors in functionality as described in the commit message.\\n\\nOverall, the changes align with improving code quality but do not indicate a direct bug fix. Therefore, the assessment is NotBuggy.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untangler.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangler = Untangler(\"gemini\")\n",
    "y_true, y_pred = untangler.batch_detect(df)\n",
    "accuracy_score(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Buggy       0.72      0.95      0.82       144\n",
      "    NotBuggy       0.91      0.56      0.69       122\n",
      "\n",
      "    accuracy                           0.77       266\n",
      "   macro avg       0.81      0.75      0.75       266\n",
      "weighted avg       0.80      0.77      0.76       266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangler = Untangler(\"openai\")\n",
    "untangler.get_batch_result(\"batch_67ba5b015398819094fe8cc1b5928ae6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7819548872180451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Buggy       0.73      0.94      0.82       144\n",
      "    NotBuggy       0.89      0.60      0.72       122\n",
      "\n",
      "    accuracy                           0.78       266\n",
      "   macro avg       0.81      0.77      0.77       266\n",
      "weighted avg       0.81      0.78      0.77       266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "untangler = Untangler(\"openai\")\n",
    "\n",
    "y_true = []\n",
    "for index, row in df.iterrows():\n",
    "    y_true.append(row[\"Decision\"])\n",
    "batch, y_pred = untangler.get_batch_result(\"batch_67ba558986f08190bb2c61a32d87a2e3\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
