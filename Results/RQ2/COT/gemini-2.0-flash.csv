Project,File,CommitHash,Diff,Decision,CommitMessage,Detection,Explanation
spring-boot,9232.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,7 +1,7 @@
 	public static String template(TemplateEngine engine, String name, Map<String, ?> model)
 			throws IOException, CompilationFailedException, ClassNotFoundException {
-		Writable writable = getTemplate(name).make(model);
+		Writable writable = getTemplate(engine, name).make(model);
 		StringWriter result = new StringWriter();
 		writable.writeTo(result);
 		return result.toString();
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"The commit message indicates a bug fix related to the `template()` method in GroovyTemplate. It states that the `engine` argument was being ignored in the 3-argument version of the `template()` method.

The provided diff shows a change in the `template()` method where `getTemplate(name)` is replaced with `getTemplate(engine, name)`. This change directly addresses the issue described in the commit message, ensuring that the `engine` argument is now being used when retrieving the template. This correction aligns perfectly with the stated bug fix.

**Buggy**"
spring-boot,9233.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,17 +1,16 @@
-	private static Template getTemplate(String name) throws CompilationFailedException,
+	private static Template getTemplate(TemplateEngine engine, String name) throws CompilationFailedException,
 			ClassNotFoundException, IOException {
-		GStringTemplateEngine engine = new GStringTemplateEngine();
 
 		File file = new File(""templates"", name);
 		if (file.exists()) {
 			return engine.createTemplate(file);
 		}
 
 		ClassLoader classLoader = GroovyTemplate.class.getClassLoader();
 		URL resource = classLoader.getResource(""templates/"" + name);
 		if (resource != null) {
 			return engine.createTemplate(resource);
 		}
 
 		return engine.createTemplate(name);
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix bug in GroovyTemplate convenience"" clearly states that the commit is intended to fix a bug. Specifically, it mentions that the `engine` argument was being ignored in the 3-argument version of the `template()` method.

2.  **Code Diff:**
    *   The original code snippet initializes a new `GStringTemplateEngine` *inside* the `getTemplate` method. This means that any `TemplateEngine` passed as an argument to the `template()` method (which presumably calls `getTemplate`) would be ignored, as the method would always use a *new* engine.
    *   The modified code snippet removes the initialization of `GStringTemplateEngine` from within the `getTemplate` method. This suggests that the `TemplateEngine` is now expected to be passed *into* the `getTemplate` method, which aligns with the commit message's claim that the `engine` argument was being ignored. The method signature also changed to accept a TemplateEngine.

3.  **Alignment:** The code changes directly address the issue described in the commit message. By removing the local `GStringTemplateEngine` initialization, the code now correctly uses the `engine` argument passed to the method.

**Conclusion:**

The commit message and code diff are consistent and indicate a bug fix. The original code was ignoring the provided `TemplateEngine`, and the modified code now correctly utilizes it.

**Buggy**"
spring-boot,510.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,3 +1,3 @@
-		public CouchbaseEnvironment couchbaseEnvironment() throws Exception {
-			return createEnvironment(this.properties);
+		public DefaultCouchbaseEnvironment couchbaseEnvironment() throws Exception {
+			return initializeEnvironmentBuilder(this.properties).build();
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",Buggy,"Okay, let's analyze this commit.

**Commit Message Analysis:**

The commit message indicates two primary changes:

1.  **Customizable Socket Connect Timeout:** The main reason for the commit is to allow users to configure the socket connect timeout for Couchbase connections. The default timeout was too short, causing failures on Windows builds. The commit introduces a property to adjust this timeout.
2.  **Extensible `CouchbaseConfiguration`:** The commit also addresses the issue that `CouchbaseConfiguration` was not easily extensible, even though it was public. The commit aims to fix this and provides a test case to demonstrate the extensibility.

**Diff Analysis:**

The diff shows a change in the return type of the `couchbaseEnvironment()` method from `CouchbaseEnvironment` to `DefaultCouchbaseEnvironment`. It also replaces `createEnvironment(this.properties)` with `initializeEnvironmentBuilder(this.properties).build()`.

**Reasoning:**

*   The change in return type from `CouchbaseEnvironment` to `DefaultCouchbaseEnvironment` suggests that the implementation is now using a concrete class instead of an interface, possibly to expose more configuration options or to facilitate the customization of the environment.
*   The replacement of `createEnvironment` with `initializeEnvironmentBuilder(...).build()` strongly suggests that the commit is introducing a builder pattern for configuring the Couchbase environment. This is a common pattern for making configuration more flexible and extensible. The builder pattern allows users to customize various aspects of the environment, including the socket connect timeout.
*   The commit message explicitly mentions fixing a problem where `CouchbaseConfiguration` was not easily extensible. The introduction of the builder pattern aligns with this goal, as it allows users to customize the environment configuration by modifying the builder.
*   The commit message also mentions that the Windows build was failing due to a short socket connection timeout. This indicates that the original timeout value was a bug.

**Conclusion:**

The changes in the diff, combined with the commit message, strongly suggest that this commit is addressing a bug related to the default socket connection timeout and the lack of extensibility in `CouchbaseConfiguration`. The introduction of the builder pattern and the change in return type support this conclusion.

**Buggy**
"
spring-boot,516.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,22 +1,23 @@
-		protected CouchbaseEnvironment createEnvironment(CouchbaseProperties properties) {
+		protected DefaultCouchbaseEnvironment.Builder initializeEnvironmentBuilder(CouchbaseProperties properties) {
 			CouchbaseProperties.Endpoints endpoints = properties.getEnv().getEndpoints();
 			CouchbaseProperties.Timeouts timeouts = properties.getEnv().getTimeouts();
 			DefaultCouchbaseEnvironment.Builder builder = DefaultCouchbaseEnvironment
 					.builder().connectTimeout(timeouts.getConnect())
 					.kvEndpoints(endpoints.getKeyValue())
 					.kvTimeout(timeouts.getKeyValue())
 					.queryEndpoints(endpoints.getQuery())
 					.queryTimeout(timeouts.getQuery()).viewEndpoints(endpoints.getView())
+					.socketConnectTimeout(timeouts.getSocketConnect())
 					.viewTimeout(timeouts.getView());
 			CouchbaseProperties.Ssl ssl = properties.getEnv().getSsl();
 			if (ssl.getEnabled()) {
 				builder.sslEnabled(true);
 				if (ssl.getKeyStore() != null) {
 					builder.sslKeystoreFile(ssl.getKeyStore());
 				}
 				if (ssl.getKeyStorePassword() != null) {
 					builder.sslKeystorePassword(ssl.getKeyStorePassword());
 				}
 			}
-			return builder.build();
+			return builder;
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",Buggy,"The commit message indicates that the Windows build is failing because the Couchbase server is not handling socket connections within the default timeout of one second. The commit introduces a property to customize the socket connect timeout and sets it to 10 seconds in the sample. The diff shows the addition of `socketConnectTimeout(timeouts.getSocketConnect())` to the `DefaultCouchbaseEnvironment.Builder`. This change directly addresses the reported issue of the connection timeout being too short, which caused the build to fail. The commit message also mentions that `CouchbaseConfiguration` was not extensible, and this commit fixes that problem. However, the diff only shows changes related to the socket connect timeout.

The addition of `socketConnectTimeout` and the explanation in the commit message strongly suggest that the original timeout value was a bug, as it was causing build failures.

**Buggy**"
spring-boot,6313.json,1a2186e6efed5c53d7c50548de7b30b9a6dcd7f7,"@@ -1,6 +1,6 @@
 		private boolean startsWithArgumentClassName(String message) {
-			Predicate<Object> startsWith = (argument) -> argument != null
-					&& message.startsWith(argument.getClass().getName());
+			Predicate<Object> startsWith = (argument) -> startsWithArgumentClassName(
+					message, argument);
 			return startsWith.test(this.argument)
 					|| Stream.of(this.additionalArguments).anyMatch(startsWith);
 		}",Buggy,"Attempt to fix lambda error detection on JDK 9

Update `LambdaSafe` to also detect `ClassCastException` messages that
start with ""module/name"".

See gh-11584",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Attempt to fix lambda error detection on JDK 9"" strongly suggests a bug fix. It explicitly mentions an attempt to resolve an error related to lambda expressions specifically on JDK 9. The message also indicates that the fix involves detecting `ClassCastException` messages that start with ""module/name"". This implies that the existing error detection mechanism was failing to properly identify `ClassCastException` under certain conditions (likely related to modules in JDK 9). The reference to gh-11584 further supports the presence of a reported issue.

**Code Diff Analysis:**

The code diff modifies the `startsWithArgumentClassName` method. The original code used a lambda expression `startsWith` that directly checked if the message starts with the class name of the `argument`. The modified code replaces this lambda with a call to a method named `startsWithArgumentClassName` and passes both the message and the argument to it.

This change suggests that the original implementation was not robust enough to handle all cases of `ClassCastException` messages, particularly those related to modules in JDK 9. The introduction of a separate `startsWithArgumentClassName` method likely allows for more complex or specific logic to handle these cases.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly states an attempt to fix an error, which is a strong indicator of a bug fix.
2.  **Error Handling:** The commit message mentions `ClassCastException`, which is a type of error.
3.  **Conditional Logic:** The code change suggests that the original logic was insufficient and needed to be updated to handle specific cases related to JDK 9 and modules. The introduction of a new method implies a more tailored approach to error detection.
4.  **Contextual Information:** The reference to gh-11584 likely provides more context about the specific bug being addressed.

**Conclusion:**

Based on the commit message and code diff, the changes indicate a bug fix. The commit message explicitly mentions an error, and the code modifications suggest an attempt to improve error detection and handling.

**Buggy**
"
spring-boot,2051.json,4b4dc28a869e2f8b988f6ac6ea8a31c274477da5,"@@ -1,9 +1,10 @@
 	private void logError(ServerRequest request, ServerResponse response, Throwable throwable) {
 		if (logger.isDebugEnabled()) {
 			logger.debug(request.exchange().getLogPrefix() + formatError(throwable, request));
 		}
-		if (response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
+		if (HttpStatus.resolve(response.rawStatusCode()) != null
+				&& response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
 			logger.error(request.exchange().getLogPrefix() + ""500 Server Error for "" + formatRequest(request),
 					throwable);
 		}
 	}",Buggy,"Support non-standard error codes with AbstractErrorWebExceptionHandler

Fixes gh-16691",Buggy,"The commit message indicates a fix for issue gh-16691 and mentions supporting non-standard error codes within `AbstractErrorWebExceptionHandler`. The code diff modifies the `logError` method. Specifically, the condition `response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)` is extended to `HttpStatus.resolve(response.rawStatusCode()) != null && response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)`.

The original code only logged an error if the status code was exactly `HttpStatus.INTERNAL_SERVER_ERROR`. The modified code first checks if the raw status code is a valid HTTP status code using `HttpStatus.resolve(response.rawStatusCode()) != null` before comparing it to `HttpStatus.INTERNAL_SERVER_ERROR`. This change suggests that the original code was not handling non-standard or custom error codes correctly, leading to errors not being logged appropriately. The fix ensures that even if a non-standard error code is used that resolves to a valid HTTP status, the error will be logged, addressing the issue described in the commit message. This indicates a bug fix related to error handling.

**Buggy**"
guava,10860.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,13 +1,15 @@
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message ""Fix bug in Optional#presentInstances"" suggests a bug fix related to the `presentInstances` method of the `Optional` class. The diff shows a change within the `iterator()` method, specifically the addition of `private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());`. This line initializes the iterator and also adds a null check using `checkNotNull`.

The original code was likely missing an explicit initialization of the `iterator` field, potentially leading to a `NullPointerException` if `optionals` was null or if the iterator was not properly initialized before being used in the `computeNext()` method. The added `checkNotNull` enforces that the iterator is not null, preventing a potential runtime error. This indicates a bug fix related to handling null iterators.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
guava,10861.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,19 +1,20 @@
-  public static <T> Iterable<T> presentInstances(Iterable<Optional<T>> optionals) {
+  public static <T> Iterable<T> presentInstances(final Iterable<Optional<T>> optionals) {
     checkNotNull(optionals);
-    final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
     return new Iterable<T>() {
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };
     };
   }",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message indicates a bug fix in `Optional#presentInstances`. The diff moves the initialization of the `iterator` from outside the anonymous `Iterator` class to inside it, making it a field of the anonymous class. This ensures that a new iterator is created each time `iterator()` is called on the `Iterable` returned by `presentInstances`. Without this change, multiple calls to `iterator()` would share the same underlying iterator, potentially leading to unexpected behavior if the first iterator is exhausted. This change directly addresses a bug related to iterator management.

**Buggy**"
guava,14951.json,803f44ac9c6a3457388394e952132fe18449cb9f,"@@ -1,4 +1,6 @@
     @Override public void clear() {
-      linkedEntries.removeAll(createEntries(delegate()));
+      for (V value : delegate) {
+        linkedEntries.remove(createEntry(value));
+      }
       delegate.clear();
     }",Buggy,"Fix performance problem in LinkedHashMultimap.removeAll(key), as documented in http://code.google.com/p/guava-libraries/issues/detail?id=371&start=100

R=kak
DELTA=3  (2 added, 0 deleted, 1 changed)


Revision created by MOE tool push_codebase.
MOE_MIGRATION=177089


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@93 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a performance fix in `LinkedHashMultimap.removeAll(key)`. The provided diff shows a change in the `clear()` method.

The original implementation used `linkedEntries.removeAll(createEntries(delegate()))`. The modified implementation iterates through the `delegate` collection and removes each entry individually using `linkedEntries.remove(createEntry(value))`.

The change suggests that the original `removeAll` operation on `linkedEntries` was inefficient, possibly due to the way `createEntries` was implemented or the internal workings of `removeAll`. The new implementation likely improves performance by iterating and removing elements one by one.

The commit message and the code change are related, and the change addresses a performance issue, which can be considered a bug fix.

**Buggy**"
guava,9470.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,3 +1,5 @@
   public boolean hasEdgeConnecting(N nodeU, N nodeV) {
-    return !edgesConnecting(nodeU, nodeV).isEmpty();
+    checkNotNull(nodeU);
+    checkNotNull(nodeV);
+    return nodes().contains(nodeU) && successors(nodeU).contains(nodeV);
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states: ""fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph."" This clearly indicates a bug fix related to the `hasEdgeConnecting()` method in the `AbstractNetwork` class. The method was throwing an exception when either of the input nodes (`nodeU`, `nodeV`) was not present in the graph.

**Diff Analysis:**

The diff shows the following changes to the `hasEdgeConnecting()` method:

-   **Original Code:**

    ```java
    public boolean hasEdgeConnecting(N nodeU, N nodeV) {
        return !edgesConnecting(nodeU, nodeV).isEmpty();
    }
    ```

-   **Modified Code:**

    ```java
    public boolean hasEdgeConnecting(N nodeU, N nodeV) {
        checkNotNull(nodeU);
        checkNotNull(nodeV);
        return nodes().contains(nodeU) && successors(nodeU).contains(nodeV);
    }
    ```

The changes introduce the following:

1.  `checkNotNull(nodeU);` and `checkNotNull(nodeV);`: These lines add null checks for the input nodes `nodeU` and `nodeV`. This is a defensive programming practice to prevent `NullPointerException`s.  However, the original bug report indicates the issue was that the method threw an exception if the nodes were *not in the graph*, not if they were null.
2.  `return nodes().contains(nodeU) && successors(nodeU).contains(nodeV);`: This line replaces the original logic. The original logic checked if there were any edges connecting the two nodes by calling `edgesConnecting(nodeU, nodeV)` and checking if the result was empty. The new logic first checks if `nodeU` is in the graph using `nodes().contains(nodeU)` and then checks if `nodeV` is a successor of `nodeU` using `successors(nodeU).contains(nodeV)`.

**Reasoning:**

The original code likely threw an exception because `edgesConnecting(nodeU, nodeV)` probably tried to access the nodes in the graph without first verifying their existence. If either `nodeU` or `nodeV` was not in the graph, this could lead to a `NoSuchElementException` or similar exception within the `edgesConnecting` method.

The modified code addresses this by explicitly checking if `nodeU` exists in the graph using `nodes().contains(nodeU)` before proceeding. This prevents the potential exception that could occur when trying to access a non-existent node. The change in logic from `edgesConnecting` to `successors` also indicates a change in how the existence of an edge is determined, likely to avoid the exception.

The commit message aligns perfectly with the code changes. The code was modified to prevent an exception when either endpoint was not in the graph.

**Conclusion:**

**Buggy**
"
guava,9471.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,7 +1,7 @@
   public boolean hasEdgeConnecting(EndpointPair<N> endpoints) {
     checkNotNull(endpoints);
     if (!isOrderingCompatible(endpoints)) {
       return false;
     }
-    return !edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty();
+    return hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV());
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"The commit message indicates a bug fix in `AbstractNetwork.hasEdgeConnecting()` that caused it to throw an exception when either endpoint was not in the graph. The provided diff shows a change in the implementation of `hasEdgeConnecting(EndpointPair<N> endpoints)`.

The original implementation used `!edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty()` to check for an edge. This could potentially throw an exception if `endpoints.nodeU()` or `endpoints.nodeV()` are not present in the graph, as `edgesConnecting` might not handle such cases gracefully.

The corrected implementation uses `hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV())`, which presumably includes a check to ensure that both endpoints are present in the graph before proceeding. This aligns with the commit message's description of the bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**"
guava,18902.json,314727b81a4f45210ab53ebd46b695150ea36f8c,"@@ -1,14 +1,11 @@
   public static long copy(Readable from, Appendable to) throws IOException {
     CharBuffer buf = CharBuffer.allocate(BUF_SIZE);
     long total = 0;
-    while (true) {
-      int r = from.read(buf);
-      if (r == -1) {
-        break;
-      }
+    while (from.read(buf) != -1) {
       buf.flip();
-      to.append(buf, 0, r);
-      total += r;
+      to.append(buf);
+      total += buf.remaining();
+      buf.clear();
     }
     return total;
   }",Buggy,"Fix a bug with CharStreams.copy not clearing its buffer after reading.

Guava issue 1061: http://code.google.com/p/guava-libraries/issues/detail?id=1061
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=31793143",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix a bug with CharStreams.copy not clearing its buffer after reading"" clearly states that the commit is intended to fix a bug in the `CharStreams.copy` method. The bug is specifically related to the buffer not being cleared after reading, which could lead to incorrect data being copied in subsequent iterations.

**Code Diff Analysis:**

The code diff shows the changes made to the `CharStreams.copy` method. Let's break down the changes:

1.  **`while (true)` loop replaced with `while (from.read(buf) != -1)`:** This change simplifies the loop condition. The original loop used `break` when `from.read(buf)` returned -1 (end of stream). The new loop directly incorporates this condition.

2.  **`int r = from.read(buf);` removed and `buf.flip();` moved inside the loop:** The `read` call is now part of the loop condition. `buf.flip()` is still called to prepare the buffer for reading.

3.  **`to.append(buf, 0, r);` changed to `to.append(buf);`:** This is a significant change. The original code was appending only `r` characters from the buffer to the `Appendable`. The new code appends the entire buffer.

4.  **`total += r;` changed to `total += buf.remaining();`:** The original code was adding the number of characters read (`r`) to the total. The new code adds the number of characters remaining in the buffer after flipping (`buf.remaining()`), which should be the same as the number of characters read.

5.  **`buf.clear();` added:** This is the most important change related to the commit message. The `buf.clear()` call resets the buffer's position, limit, and mark, effectively clearing it for the next read operation.

**Reasoning:**

The commit message states that the bug was related to the buffer not being cleared after reading. The code diff confirms this by adding the `buf.clear()` call within the loop. Without clearing the buffer, the next `read` operation would append new data to the existing data in the buffer, potentially leading to incorrect data being copied. The other changes in the diff are likely related to optimizing the code and ensuring that the correct number of characters is appended and counted. The change from `to.append(buf, 0, r)` to `to.append(buf)` along with the addition of `buf.clear()` strongly suggests that the previous implementation was not correctly handling the buffer, leading to a bug.

**Conclusion:**

The code changes align with the commit message, and the addition of `buf.clear()` clearly indicates a fix for a bug related to buffer handling.

**Buggy**
"
guava,12172.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,3 +1,3 @@
     @Override public int size() {
-      return safeIntFactorial(inputList.size());
+      return IntMath.factorial(inputList.size());
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"The commit message indicates a bug fix related to size calculation in `Collections2`. The original code used `safeIntFactorial`, which was replaced with `IntMath.factorial`. The commit message also mentions that the previous test was flawed and didn't catch the bug because it incorrectly assumed `C(34,14) > MAX_VALUE`. This suggests that the `safeIntFactorial` function was either incorrect or inefficient, and `IntMath.factorial` provides a more accurate or efficient solution. The change directly addresses the size calculation, aligning with the commit message's description of fixing a size calculation bug.

**Buggy**"
guava,12171.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,32 +1,25 @@
-    static <E> int calculateSize(List<E> sortedInputList,
-        Comparator<? super E> comparator) {
-      try {
-        long permutations = 1;
-        int n = 1;
-        int r = 1;
-        for (; n < sortedInputList.size(); n++, r++) {
-          int comparison = comparator.compare(sortedInputList.get(n - 1),
-              sortedInputList.get(n));
-          // The list is sorted, this is an invariant.
-          checkState(comparison <= 0);
-          if (comparison < 0) {
-            // We move to the next non-repeated element.
-            permutations *= binomialCoefficient(n, r);
-            r = 0;
-
-            // Return early if we have more than MAX_VALUE permutations.
-            if (!isPositiveInt(permutations)) {
-              return Integer.MAX_VALUE;
-            }
+    private static <E> int calculateSize(
+        List<E> sortedInputList, Comparator<? super E> comparator) {
+      long permutations = 1;
+      int n = 1;
+      int r = 1;
+      while (n < sortedInputList.size()) {
+        int comparison = comparator.compare(
+            sortedInputList.get(n - 1), sortedInputList.get(n));
+        if (comparison < 0) {
+          // We move to the next non-repeated element.
+          permutations *= binomial(n, r);
+          r = 0;
+          if (!isPositiveInt(permutations)) {
+            return Integer.MAX_VALUE;
           }
         }
-        permutations *= binomialCoefficient(n, r);
-        if (!isPositiveInt(permutations)) {
-          return Integer.MAX_VALUE;
-        }
-        return (int) permutations;
-      } catch (IllegalArgumentException e) {
-        // Overflow. Fall back to max size.
+        n++;
+        r++;
+      }
+      permutations *= binomial(n, r);
+      if (!isPositiveInt(permutations)) {
         return Integer.MAX_VALUE;
       }
+      return (int) permutations;
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"The commit message indicates a bug fix related to size calculation in `Collections2`. The code diff shows a significant reduction in code size, achieved by adopting `common.math` library functions, specifically `binomial` instead of `binomialCoefficient`. The message also mentions a bad test that failed to detect the bug. The change in the `calculateSize` method, along with the commit message's explicit mention of a bug fix, strongly suggests that this commit addresses a bug.

**Buggy**"
guava,1857.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,9 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     valueType = (Class<V>) stream.readObject();
-    setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        WellBehavedMap.wrap(new EnumMap<V, K>(valueType)));
+    setDelegates(new EnumMap<K, V>(keyType), new EnumMap<V, K>(valueType));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",Buggy,"The commit message indicates the removal of `WellBehavedMap` due to a bug fix in Java 7 that made the workaround it provided unnecessary. The commit message also suggests that the bug was likely never present in Android or GWT. The diff shows that the code using `WellBehavedMap.wrap` is being changed to directly use `new EnumMap<K, V>(keyType)` and `new EnumMap<V, K>(valueType)`. This change aligns with the commit message's intention to remove the `WellBehavedMap` wrapper. The removal of the wrapper implies the original code was working around a bug.

**Buggy**"
guava,13304.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,8 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
+        new EnumMap<K, V>(keyType), new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",Buggy,"The commit message indicates the removal of `WellBehavedMap` due to a bug it was designed to work around being fixed in Java 7 and not present in Android or older GWT versions. The diff shows the removal of `WellBehavedMap.wrap()` when creating the `EnumMap` during deserialization. This aligns with the commit message's intention to simplify `EnumMap` usage by removing the unnecessary workaround. The commit message explicitly mentions a bug fix in Java 7, and the code change removes a workaround related to that bug.

**Buggy**"
guava,22245.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,7 +1,7 @@
   public static RegularImmutableAsList<Object> instantiate(SerializationStreamReader reader)
       throws SerializationException {
-    @SuppressWarnings(""unchecked"") // serialization is necessarily type unsafe
-    ImmutableCollection<Object> delegateCollection = (ImmutableCollection) reader.readObject();
-    ImmutableList<?> delegateList = (ImmutableList<?>) reader.readObject();
-    return new RegularImmutableAsList<Object>(delegateCollection, delegateList);
+    ArrayList<Object> elements = new ArrayList<Object>();
+    Collection_CustomFieldSerializerBase.deserialize(reader, elements);
+    ImmutableList<Object> delegate = ImmutableList.copyOf(elements);
+    return new RegularImmutableAsList<Object>(delegate, delegate);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states that it ""fixes serialization errors when serializing a RegularImmutableAsList."" It further explains the specific scenario where the issue occurs: when an `ImmutableList` is included in GWT's serialization policy but `ImmutableSet` is not, and a list created by `ImmutableSet.asList` needs to be serialized. The ""Fixes #2614"" also strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff shows a change in the `instantiate` method of `RegularImmutableAsList`.  The original code reads two objects from the `SerializationStreamReader`: `delegateCollection` and `delegateList`. The new code reads into an `ArrayList<Object>` using `Collection_CustomFieldSerializerBase.deserialize`, then creates an `ImmutableList` from the `ArrayList` using `ImmutableList.copyOf(elements)`. Finally, it constructs a `RegularImmutableAsList` with the created `ImmutableList` as both the delegate collection and list.

**Reasoning:**

The original code's approach of reading two separate objects (`delegateCollection` and `delegateList`) likely caused issues when the underlying data structure was an `ImmutableSet` (which is not serializable in the specified GWT configuration). The modified code addresses this by deserializing the elements into a standard `ArrayList` and then creating an `ImmutableList` from it. This ensures that the serialization process works correctly even when `ImmutableSet` is not directly serializable. The change to use `ImmutableList.copyOf` and then pass the same list as both arguments to the constructor also suggests an attempt to ensure consistency after deserialization.

The commit message and the code diff are highly relevant. The code changes directly address the serialization issue described in the commit message. The change from reading two separate objects to deserializing into an `ArrayList` and then creating an `ImmutableList` is a clear indication of a fix for a serialization bug.

**Conclusion:**

**Buggy**
"
guava,22246.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,5 +1,4 @@
   public static void serialize(SerializationStreamWriter writer, RegularImmutableAsList<?> instance)
       throws SerializationException {
-    writer.writeObject(instance.delegateCollection());
-    writer.writeObject(instance.delegateList());
+    Collection_CustomFieldSerializerBase.serialize(writer, instance);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"The commit message indicates a fix for serialization errors specifically related to `RegularImmutableAsList` when used with GWT's serialization policy. The issue arises when an `ImmutableList` is included in the GWT serialization policy, but `ImmutableSet` is not, and a list created by `ImmutableSet.asList` needs to be serialized.

The code diff shows that the original serialization logic, which involved writing both `delegateCollection()` and `delegateList()`, has been replaced with a call to `Collection_CustomFieldSerializerBase.serialize(writer, instance)`. This suggests a change in how the `RegularImmutableAsList` instance is serialized, likely to address the serialization issue described in the commit message. The change replaces a specific serialization strategy with a more general one, which is a common approach to fixing serialization issues.

Based on the commit message and the code diff, the changes indicate a bug fix related to serialization.

**Buggy**"
guava,11944.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,20 +1,26 @@
   private Type resolveTypeArgsForSubclass(Class<?> subclass) {
-    if (runtimeType instanceof Class) {
+    // If both runtimeType and subclass are not parameterized, return subclass
+    // If runtimeType is not parameterized but subclass is, process subclass as a parameterized type
+    // If runtimeType is a raw type (i.e. is a parameterized type specified as a Class<?>), we
+    // return subclass as a raw type
+    if (runtimeType instanceof Class
+        && ((subclass.getTypeParameters().length == 0)
+            || (getRawType().getTypeParameters().length != 0))) {
       // no resolution needed
       return subclass;
     }
     // class Base<A, B> {}
     // class Sub<X, Y> extends Base<X, Y> {}
     // Base<String, Integer>.subtype(Sub.class):
 
     // Sub<X, Y>.getSupertype(Base.class) => Base<X, Y>
     // => X=String, Y=Integer
     // => Sub<X, Y>=Sub<String, Integer>
     TypeToken<?> genericSubtype = toGenericType(subclass);
     @SuppressWarnings({""rawtypes"", ""unchecked""}) // subclass isn't <? extends T>
     Type supertypeWithArgsFromSubtype =
         genericSubtype.getSupertype((Class) getRawType()).runtimeType;
     return new TypeResolver()
         .where(supertypeWithArgsFromSubtype, runtimeType)
         .resolveType(genericSubtype.runtimeType);
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message describes a bug where `SomeClass<?>.getSubType(SubClass<?, OtherT>)` incorrectly returns `SubClass` as a raw type when it should return `SubClass<OtherT>`. The code diff addresses this by adding a condition to check if `runtimeType` is a `Class` and if `subclass` has type parameters while `getRawType()` also has type parameters. This condition ensures that if `runtimeType` is not parameterized but `subclass` is, the code processes `subclass` as a parameterized type, fixing the raw type issue. The commit message also mentions fixing the generation of proper owners for static classes, although this is not directly visible in the provided diff.

The changes in the code diff directly relate to the bug described in the commit message, indicating a bug fix.

**Buggy**
"
guava,11937.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,24 +1,26 @@
   static <T> TypeToken<? extends T> toGenericType(Class<T> cls) {
     if (cls.isArray()) {
       Type arrayOfGenericType =
           Types.newArrayType(
               // If we are passed with int[].class, don't turn it to GenericArrayType
               toGenericType(cls.getComponentType()).runtimeType);
       @SuppressWarnings(""unchecked"") // array is covariant
       TypeToken<? extends T> result = (TypeToken<? extends T>) of(arrayOfGenericType);
       return result;
     }
     TypeVariable<Class<T>>[] typeParams = cls.getTypeParameters();
     Type ownerType =
-        cls.isMemberClass() ? toGenericType(cls.getEnclosingClass()).runtimeType : null;
+        cls.isMemberClass() && !Modifier.isStatic(cls.getModifiers())
+            ? toGenericType(cls.getEnclosingClass()).runtimeType
+            : null;
 
-    if ((typeParams.length > 0) || (ownerType != cls.getEnclosingClass())) {
+    if ((typeParams.length > 0) || ((ownerType != null) && ownerType != cls.getEnclosingClass())) {
       @SuppressWarnings(""unchecked"") // Like, it's Iterable<T> for Iterable.class
       TypeToken<? extends T> type =
           (TypeToken<? extends T>)
               of(Types.newParameterizedTypeWithOwner(ownerType, cls, typeParams));
       return type;
     } else {
       return of(cls);
     }
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message describes a bug where `SomeClass<?>.getSubType(SubClass<?, OtherT>)` returns `SubClass<?, OtherT>` but `NotGeneric.getSubtype(SubClass<OtherT>)` returns `SubClass` as a raw type. The commit message also mentions a related bug where proper owners were not being generated for static classes.

The code diff modifies the `toGenericType` method to address these issues. Specifically, it changes the logic for determining the `ownerType` of a class. The original code only checked if the class was a member class to determine the owner type. The updated code adds an additional check to ensure that the member class is not static. If the class is a static member class, the owner type should be null.

The diff also updates the condition for creating a parameterized type. The original code checked if the class had type parameters or if the owner type was different from the enclosing class. The updated code checks if the class has type parameters or if the owner type is not null and different from the enclosing class.

These changes appear to be aimed at fixing the bugs described in the commit message. The changes to the owner type logic specifically address the issue of incorrect owner types for static classes. The changes to the parameterized type condition likely address the issue of raw types being returned in certain cases.

Therefore, the changes indicate a bug fix.
**Buggy**"
guava,15642.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,31 +1,30 @@
     int crossOverUp(int index, E x) {
       if (index == 0) {
         queue[0] = x;
         return 0;
       }
       int parentIndex = getParentIndex(index);
       E parentElement = elementData(parentIndex);
       if (parentIndex != 0) {
-        // This is a guard for the case of the childless uncle. No checks are
-        // performed for childlessness (even if we could check it), but since
-        // it is the minimum sibling that is moved from ""max"" to ""min"" half
-        // of the heap, and only if x is larger, and this is at the bottom
-        // edge of the heap, the heap invariant is still preserved.
+        // This is a guard for the case of the childless uncle.
+        // Since the end of the array is actually the middle of the heap,
+        // a smaller childless uncle can become a child of x when we
+        // bubble up alternate levels, violating the invariant.
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
-        if (uncleIndex != parentIndex) {
+        if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, parentElement) < 0) {
             parentIndex = uncleIndex;
             parentElement = uncleElement;
           }
         }
       }
       if (ordering.compare(parentElement, x) < 0) {
         queue[index] = parentElement;
         queue[parentIndex] = x;
         return parentIndex;
       }
       queue[index] = x;
       return index;
     }",Buggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message ""Fix a second case of 'childless uncle' bug causing heap corruption"" indicates that the code change is intended to resolve a bug related to a specific scenario in a heap data structure, where a ""childless uncle"" node can lead to heap corruption.

The code diff modifies the `crossOverUp` method, specifically addressing the ""childless uncle"" case. The original code had a comment mentioning the ""childless uncle"" but didn't seem to fully address the issue, relying on the assumption that the heap invariant would still be preserved. The modified code includes a check `getLeftChildIndex(uncleIndex) >= size` to explicitly verify if the uncle has children. If the uncle is childless and smaller than the parent, the code now updates the parent index to the uncle index, effectively preventing the potential heap corruption. The comment explains the issue more clearly.

The changes directly address the bug described in the commit message by adding a condition to check for childlessness of the uncle. This prevents a smaller childless uncle from becoming a child of x, which could violate the heap invariant.

Therefore, the changes indicate a bug fix.
**Buggy**
"
guava,15629.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,32 +1,28 @@
   @VisibleForTesting MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
+    E actualLastElement = elementData(size);
+    int lastElementAt = heapForIndex(size)
+        .getCorrectLastElement(actualLastElement);
     E toTrickle = elementData(size);
     queue[size] = null;
-    Heap heap = heapForIndex(index);
-    // We consider elementData(index) a ""hole"", and we want to fill it
-    // with the last element of the heap, toTrickle.
-    // Since the last element of the heap is from the bottom level, we
-    // optimistically fill index position with elements from lower levels,
-    // moving the hole down. In most cases this reduces the number of
-    // comparisons with toTrickle, but in some cases we will need to bubble it
-    // all the way up again.
-    int vacated = heap.fillHoleAt(index);
-    // Try to see if toTrickle can be bubbled up min levels.
-    int bubbledTo = heap.bubbleUpAlternatingLevels(vacated, toTrickle);
-    if (bubbledTo == vacated) {
-      // Could not bubble toTrickle up min levels, try moving
-      // it from min level to max level (or max to min level) and bubble up
-      // there.
-      return heap.tryCrossOverAndBubbleUp(index, vacated, toTrickle);
-    } else {
-      return (bubbledTo < index)
-          ? new MoveDesc<E>(toTrickle, elementData(index))
-          : null;
+    MoveDesc<E> changes = fillHole(index, toTrickle);
+    if (lastElementAt < index) {
+      // Last element is moved to before index, swapped with trickled element.
+      if (changes == null) {
+        // The trickled element is still after index.
+        return new MoveDesc<E>(actualLastElement, toTrickle);
+      } else {
+        // The trickled element is back before index, but the replaced element
+        // has now been moved after index.
+        return new MoveDesc<E>(actualLastElement, changes.replaced);
+      }
     }
+    // Trickled element was after index to begin with, no adjustment needed.
+    return changes;
   }",NotBuggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a fix for a ""childless uncle"" bug that leads to heap corruption. The code diff modifies the `removeAt` method, specifically addressing how elements are rearranged within the heap after removing an element. The changes include:

1.  **Introducing `actualLastElement` and `lastElementAt`**: These variables capture the last element and its correct position within the heap.
2.  **Replacing the original heap manipulation logic**: The original code involving `fillHoleAt`, `bubbleUpAlternatingLevels`, and `tryCrossOverAndBubbleUp` is replaced with a call to a new method `fillHole`.
3.  **Adjusting MoveDesc based on `lastElementAt`**: The code now checks if the last element's correct position is before the removed element's index. If so, it adjusts the `MoveDesc` to reflect the element swaps that occurred during the heap rearrangement.

The introduction of `actualLastElement` and `lastElementAt` suggests a more precise handling of the last element during the removal process. The conditional logic based on `lastElementAt` indicates an attempt to correct a specific scenario where the last element's movement affects the overall heap structure, potentially leading to the ""childless uncle"" issue and heap corruption. The `fillHole` method likely encapsulates the core logic for filling the hole created by the removal, and the subsequent adjustments to `MoveDesc` are crucial for maintaining heap integrity.

Given the commit message explicitly mentioning a bug fix related to heap corruption and the code changes focusing on refining heap rearrangement logic, this commit is indeed a bug fix.

**Buggy**"
guava,12247.json,0007cb257b593400097f37cf5fa9c8d5fb598c62,"@@ -1,7 +1,9 @@
     SetBuilderImpl<E> review() {
       int targetTableSize = chooseTableSize(distinct);
       if (targetTableSize * 2 < hashTable.length) {
         hashTable = rebuildHashTable(targetTableSize, dedupedElements, distinct);
+        maxRunBeforeFallback = maxRunBeforeFallback(targetTableSize);
+        expandTableThreshold = (int) (DESIRED_LOAD_FACTOR * targetTableSize);
       }
       return hashFloodingDetected(hashTable) ? new JdkBackedSetBuilderImpl<E>(this) : this;
     }",Buggy,"Fix #3570 by resetting expandTableThreshold and
maxRunBeforeFallback after resizing the hashTable.

Fixes #3571

[]

RELNOTES=Fixed a bug in `ImmutableSet.Builder` that could lead to infinite loops when building multiple sets from the same builder.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=264648412",Buggy,"The commit message indicates a fix for a potential infinite loop in `ImmutableSet.Builder` when building multiple sets from the same builder. The code diff shows that `expandTableThreshold` and `maxRunBeforeFallback` are being reset after resizing the `hashTable`. This suggests that the resizing operation was not correctly updating these thresholds, potentially leading to a state where the builder would continuously try to resize the table, resulting in an infinite loop. The resetting of these values after resizing the hash table strongly suggests a bug fix.

**Buggy**
"
guava,11603.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,7 +1,6 @@
   public Stopwatch start() {
-    checkState(!isRunning,
-        ""This stopwatch is already running; it cannot be started more than once."");
+    checkState(!isRunning, ""This stopwatch is already running."");
     isRunning = true;
     startTick = ticker.read();
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",Buggy,"The commit message indicates documentation fixes and internal cleanups. The diff removes ""it cannot be started more than once"" from the Stopwatch exception message. This change aligns with the commit message's description of removing potentially misleading information. This change is related to improving the clarity of the exception message, which can be considered a bug fix in terms of improving user experience and reducing confusion.

**Buggy**"
guava,11604.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,8 +1,7 @@
   public Stopwatch stop() {
     long tick = ticker.read();
-    checkState(isRunning,
-        ""This stopwatch is already stopped; it cannot be stopped more than once."");
+    checkState(isRunning, ""This stopwatch is already stopped."");
     isRunning = false;
     elapsedNanos += tick - startTick;
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",Buggy,"The commit message indicates ""documentation fixes and internal cleanups."" One of the changes mentioned is ""Remove potentially misleading 'it cannot be started/stopped more than once' from Stopwatch exception messages."" The provided code diff shows the removal of the phrase ""it cannot be stopped more than once"" from the `checkState` message in the `Stopwatch.stop()` method. This change directly corresponds to the commit message's description. This removal suggests that the original message was misleading, implying a potential bug or incorrect behavior that is now being corrected.

**Buggy**
"
guava,81.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",Buggy,"The commit message indicates a change to replace the terms ""whitelist"" and ""blacklist"" with less problematic terms in non-public identifiers and documentation.

The code diff shows a change in a comment: ""whitelisted"" is replaced with ""allowed"". This aligns with the commit message's intention to replace problematic terms. The change appears to be a direct substitution of a term, and doesn't introduce any new logic or error handling. It's a simple find and replace.

**Buggy**
"
guava,10879.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static @Nullable Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",NotBuggy,"The commit message indicates a change to replace the terms ""whitelist"" and ""blacklist"" with less problematic terms. The diff shows that ""whitelisted"" is replaced with ""allowed"" in a comment. This aligns with the commit message and suggests a terminology update rather than a bug fix.

**NotBuggy**"
guava,21458.json,eb3a9f447715b05c18179bf6313dfd28851bb56e,"@@ -1,18 +1,29 @@
   public void addListener(Runnable listener, Executor executor) {
     checkNotNull(listener, ""Runnable was null."");
     checkNotNull(executor, ""Executor was null."");
-    Listener oldHead = listeners;
-    if (oldHead != Listener.TOMBSTONE) {
-      Listener newNode = new Listener(listener, executor);
-      do {
-        newNode.next = oldHead;
-        if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
-          return;
-        }
-        oldHead = listeners; // re-read
-      } while (oldHead != Listener.TOMBSTONE);
+    // Checking isDone and listeners != TOMBSTONE may seem redundant, but our contract for
+    // addListener says that listeners execute 'immediate' if the future isDone(). However, our
+    // protocol for completing a future is to assign the value field (which sets isDone to true) and
+    // then to release waiters, followed by executing afterDone(), followed by releasing listeners.
+    // That means that it is possible to observe that the future isDone and that your listeners
+    // don't execute 'immediately'.  By checking isDone here we avoid that.
+    // A corollary to all that is that we don't need to check isDone inside the loop because if we
+    // get into the loop we know that we weren't done when we entered and therefore we aren't under
+    // an obligation to execute 'immediately'.
+    if (!isDone()) {
+      Listener oldHead = listeners;
+      if (oldHead != Listener.TOMBSTONE) {
+        Listener newNode = new Listener(listener, executor);
+        do {
+          newNode.next = oldHead;
+          if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
+            return;
+          }
+          oldHead = listeners; // re-read
+        } while (oldHead != Listener.TOMBSTONE);
+      }
     }
     // If we get here then the Listener TOMBSTONE was set, which means the future is done, call
     // the listener.
     executeListener(listener, executor);
   }",Buggy,"Fix a bug in AbstractFuture.addListener where we failed to follow the contract precisely.

ListenableFuture.addListener says that listeners should execute 'immediately' when the future is done.  However, because the old version of addListener only checked the listeners field, it is possible to observe that the future is done and that your listener would not execute immediately.  This cl fixes that by adding an explicit check for isDone.

RELNOTES=Close a short race where you could observe that an AbstractFuture was done, but that listeners wouldn't execute immediately.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=212281545",Buggy,"The commit message indicates a fix for a race condition in `AbstractFuture.addListener`. The original implementation only checked the `listeners` field, potentially leading to a scenario where a future was observed as done, but the listener wouldn't execute immediately, violating the `ListenableFuture.addListener` contract.

The code diff introduces a check for `isDone()` before attempting to add the listener to the list. This ensures that if the future is already done, the listener is executed immediately, adhering to the contract. The added comment explains the reasoning behind this check, detailing the potential race condition and how the fix addresses it.

The changes directly relate to the bug described in the commit message, indicating a logical correction to ensure the correct behavior of `addListener`.

**Buggy**"
guava,22829.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,4 +1,4 @@
     public void invalidate(Object key) {
-      key = checkNotNull(key);
+      checkNotNull(key);
       localCache.remove(key);
     }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",NotBuggy,"The commit message indicates a fix for Error Prone errors in GWT code. The diff shows a change in the `invalidate` method where `key = checkNotNull(key)` is replaced with `checkNotNull(key)`.

The original code assigned the result of `checkNotNull(key)` back to the `key` variable. This is unnecessary because `checkNotNull` throws an exception if the key is null, and otherwise returns the key unchanged. The modified code simply calls `checkNotNull(key)` without assigning the result, which achieves the same effect of validating the key is not null, but avoids the unnecessary assignment.

This change likely addresses an Error Prone warning related to the unused assignment. While it doesn't directly fix a functional bug, it improves code quality by removing redundant code identified by a static analysis tool. It's a cleanup rather than a bug fix.

**NotBuggy**"
guava,22803.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,19 +1,19 @@
   public V get(Object key) {
-    key = checkNotNull(key);
+    checkNotNull(key);
     Timestamped<V> value = cachingHashMap.get(key);
 
     if (value == null) {
       statsCounter.recordMisses(1);
       return null;
     } else if (!isExpired(value)) {
       statsCounter.recordHits(1);
       value.updateTimestamp();
       return value.getValue();
     } else {
       statsCounter.recordEviction();
       statsCounter.recordMisses(1);
       alertListenerIfPresent(key, value.getValue(), RemovalCause.EXPIRED);
       cachingHashMap.remove(key);
       return null;
     }
   }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",NotBuggy,"The commit message indicates a fix for Error Prone errors in GWT code. The diff shows a change in the `get` method where `key = checkNotNull(key)` is replaced with `checkNotNull(key)`.

Here's a step-by-step analysis:

1. **Error Prone and `checkNotNull`:** Error Prone is a static analysis tool that can detect common programming errors in Java code.  `checkNotNull` is a common utility method (often from Guava) used to ensure that a value is not null, throwing a `NullPointerException` if it is.

2. **Original Code:** `key = checkNotNull(key);` This line assigns the result of `checkNotNull(key)` back to the `key` variable.  If `key` is not null, the assignment is redundant. If `key` *is* null, `checkNotNull` throws a `NullPointerException` *before* the assignment, so `key` remains null at the point of the exception.

3. **Modified Code:** `checkNotNull(key);` This line simply calls `checkNotNull(key)` without assigning the result.  If `key` is not null, nothing happens. If `key` *is* null, `checkNotNull` throws a `NullPointerException`.

4. **Bug Fix?** The original code, while perhaps unconventional, doesn't introduce a functional bug. The modified code is functionally equivalent. The change likely addresses an Error Prone warning about the redundant assignment or a stylistic issue. It's not directly fixing a bug that would cause incorrect behavior. It's more about code quality and adherence to best practices.

Therefore, the change is not a bug fix in the sense of correcting incorrect behavior. It's addressing a code quality issue flagged by Error Prone.

**NotBuggy**"
guava,21488.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,12 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
       V value = getDone(this);
-      builder.append(""SUCCESS, result=["").append(value).append(""]"");
+      builder.append(""SUCCESS, result=["").append(userObjectToString(value)).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED""); // shouldn't be reachable
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for potential stack overflow errors in `AbstractFuture.toString()`, specifically when dealing with deeply nested or recursive data structures. The message suggests the fix is partial, aiming to address simple cases without introducing complex solutions like thread locals or call stack analysis.

The code diff replaces `builder.append(value)` with `builder.append(userObjectToString(value))`. This change suggests that the original code was directly appending the `value` object to the string builder, which could lead to a stack overflow if the `value`'s `toString()` method recursively called `AbstractFuture.toString()`. The `userObjectToString()` method likely provides a safer way to represent the object as a string, possibly by limiting the recursion depth or providing a more concise representation.

The change addresses a potential bug where deeply nested objects could cause a stack overflow during the `toString()` call. The commit message explicitly mentions this issue and the limitations of the fix.

**Buggy**"
guava,10179.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,11 +1,11 @@
   protected String pendingToString() {
     Object localValue = value;
     if (localValue instanceof SetFuture) {
-      return ""setFuture=["" + ((SetFuture) localValue).future + ""]"";
+      return ""setFuture=["" + userObjectToString(((SetFuture) localValue).future) + ""]"";
     } else if (this instanceof ScheduledFuture) {
       return ""remaining delay=[""
           + ((ScheduledFuture) this).getDelay(TimeUnit.MILLISECONDS)
           + "" ms]"";
     }
     return null;
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message indicates a fix for potential stack overflow errors in `AbstractFuture.toString()`, specifically when dealing with `SetFuture` instances. The provided diff shows that the `pendingToString()` method, which is used in the `toString()` implementation, is modified to use `userObjectToString()` when handling `SetFuture`. This change aims to prevent infinite recursion and potential stack overflow errors that could occur if the `future` field of the `SetFuture` instance refers back to the `AbstractFuture` itself, directly or indirectly. The commit message explicitly mentions that this is a partial fix for specific cases that can lead to stack overflow.

Based on the analysis, the changes in the code are directly related to addressing a potential bug (stack overflow).

**Buggy**"
guava,10881.json,c95fc106ff08d0652b5d661cab9a73f38697eccc,"@@ -1,3 +1,12 @@
   private static Method getSizeMethod() {
-    return getJlaMethod(""getStackTraceDepth"", Throwable.class);
+    try {
+      Method getStackTraceDepth = getJlaMethod(""getStackTraceDepth"", Throwable.class);
+      if (getStackTraceDepth == null) {
+        return null;
+      }
+      getStackTraceDepth.invoke(getJLA(), new Throwable());
+      return getStackTraceDepth;
+    } catch (UnsupportedOperationException | IllegalAccessException | InvocationTargetException e) {
+      return null;
+    }
   }",Buggy,"Throwables#lazyStackTrace unsupported on IBM JDK 6

Fix addresses a problem occurred on IBM JDK 6. To ensure
that we can call underlying method, Throwables#getSizeMethod,
to get  stack size, first a smoke test  with dummy exception.
Once method executed with success, it is safe to return it for
further usage.

Fixes #2947

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=169910803",Buggy,"Okay, let's analyze the provided commit message and Java code diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:**
    *   The commit message explicitly states that it addresses a problem occurring on IBM JDK 6 related to `Throwables#lazyStackTrace`.
    *   It describes a workaround involving a ""smoke test"" with a dummy exception to ensure the underlying `Throwables#getSizeMethod` can be called successfully.
    *   It mentions that if the method executes successfully during the smoke test, it's considered safe to return it for further usage.
    *   The commit message also includes ""Fixes #2947"", which strongly suggests that this commit is intended to resolve a reported issue.

2.  **Code Diff:**
    *   The code diff modifies the `getSizeMethod()` method.
    *   It introduces a `try-catch` block to handle `UnsupportedOperationException`, `IllegalAccessException`, and `InvocationTargetException`.
    *   Inside the `try` block, it attempts to invoke the `getStackTraceDepth` method with a new `Throwable` instance (the ""smoke test"").
    *   If the invocation is successful, the `getStackTraceDepth` method is returned.
    *   If any of the exceptions are caught, the method returns `null`.

3.  **Relevance and Bug Fix Indication:**
    *   The commit message and code diff are highly relevant to each other. The commit message describes the problem and the intended solution, and the code diff implements that solution.
    *   The introduction of the `try-catch` block and the handling of specific exceptions (`UnsupportedOperationException`, `IllegalAccessException`, `InvocationTargetException`) strongly suggest that the original code was failing under certain circumstances (specifically on IBM JDK 6), which is a clear indicator of a bug.
    *   The ""smoke test"" approach is a common technique to detect and handle runtime incompatibilities or unexpected behavior.
    *   The fact that the code now returns `null` instead of potentially throwing an exception indicates a defensive programming approach to avoid crashing the application.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix related to compatibility issues on IBM JDK 6. The code was modified to handle exceptions and ensure the method's availability, which is a typical bug-fixing scenario.

**Buggy**
"
guava,11748.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,7 +1,7 @@
-    static Object forLookup(Type t) {
+    static TypeVariableKey forLookup(Type t) {
       if (t instanceof TypeVariable) {
         return new TypeVariableKey((TypeVariable<?>) t);
       } else {
         return null;
       }
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",NotBuggy,"The commit message indicates bug fixes related to incorrect usage of `contains(Object)` and `get(Object)` methods with potentially wrong types. It also mentions tightening generics for clarity.

The provided diff focuses on the `TypeResolver` class, specifically the `forLookup` method. The change involves modifying the return type of the `forLookup` method from `Object` to `TypeVariableKey`.

Reasoning:

1.  **Bug Fix Indication:** The commit message explicitly states ""Fix calls... that pass a value of apparently the wrong type."" While this specific diff doesn't directly address the `contains` or `get` method calls, the overall commit aims to correct type-related issues.
2.  **Type Tightening:** The change from returning `Object` to `TypeVariableKey` suggests a refinement in type safety. This aligns with the commit message's mention of ""tightening up some generics... to make the type, which was correct after all, look more correct.""
3.  **Potential for Error Reduction:** By specifying a more precise return type, the code becomes less prone to errors caused by unexpected type casting or incorrect assumptions about the returned object.

Conclusion:

The diff, in conjunction with the commit message, suggests an effort to improve type safety and reduce potential errors. Although the diff itself doesn't directly fix a bug, it contributes to the overall goal of correcting type-related issues as described in the commit message.

**NotBuggy**
"
guava,11957.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,24 +1,24 @@
     private int collectTypes(K type, Map<? super K, Integer> map) {
-      Integer existing = map.get(this);
+      Integer existing = map.get(type);
       if (existing != null) {
         // short circuit: if set contains type it already contains its supertypes
         return existing;
       }
       int aboveMe = getRawType(type).isInterface()
           ? 1 // interfaces should be listed before Object
           : 0;
       for (K interfaceType : getInterfaces(type)) {
         aboveMe = Math.max(aboveMe, collectTypes(interfaceType, map));
       }
       K superclass = getSuperclass(type);
       if (superclass != null) {
         aboveMe = Math.max(aboveMe, collectTypes(superclass, map));
       }
       /*
        * TODO(benyu): should we include Object for interface?
        * Also, CharSequence[] and Object[] for String[]?
        *
        */
       map.put(type, aboveMe + 1);
       return aboveMe + 1;
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",Buggy,"The commit message indicates bug fixes related to incorrect usage of `contains(Object)` and `get(Object)` methods, where the wrong type of argument was being passed. The provided code diff shows a change in the `collectTypes` method within the `TypeToken` class. Specifically, the line `Integer existing = map.get(this);` is changed to `Integer existing = map.get(type);`.

Reasoning:

1.  **Bug Fix Indication:** The commit message explicitly mentions fixing calls to `get(Object)` with incorrect types. The diff directly reflects this by changing the argument passed to the `map.get()` method from `this` (which likely refers to the `TypeCollector` instance, as indicated in the commit message) to `type` (which is of type `K`, the correct key type for the map).
2.  **Type Correction:** The original code `map.get(this)` suggests an attempt to retrieve a value from the map using the `TypeCollector` instance as the key, which is likely incorrect. The corrected code `map.get(type)` uses the type `K` as the key, aligning with the intended logic of the method.
3.  **Commit Message Alignment:** The commit message states ""Call map.get(K) instead of map.get(TypeCollector)."" This perfectly matches the code change observed in the diff.

Conclusion:

**Buggy**
"
guava,10943.json,3fbaf56f390db1a7c442ceaeab2c3a8bc1c8fd79,"@@ -1,29 +1,43 @@
     @Override protected String computeNext() {
       while (offset != -1) {
         int start = offset;
         int end;
 
         int separatorPosition = separatorStart(offset);
         if (separatorPosition == -1) {
           end = toSplit.length();
           offset = -1;
         } else {
           end = separatorPosition;
           offset = separatorEnd(separatorPosition);
         }
 
         while (start < end && trimmer.matches(toSplit.charAt(start))) {
           start++;
         }
         while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
           end--;
         }
 
         if (omitEmptyStrings && start == end) {
           continue;
         }
 
+        if (limit == 1) {
+          // The limit has been reached, return the rest of the string as the
+          // final item.  This is tested after empty string removal so that
+          // empty strings do not count towards the limit.
+          end = toSplit.length();
+          offset = -1;
+          // Since we may have changed the end, we need to trim it again.
+          while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
+            end--;
+          }
+        } else {
+          limit--;
+        }
+
         return toSplit.subSequence(start, end).toString();
       }
       return endOfData();
     }",Buggy,"Fixed bug/532.
Add warnings to README.

These should be the last changes for Guava release 08.


Revision created by MOE tool push_codebase.
MOE_MIGRATION=


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@160 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message ""Fixed bug/532. Add warnings to README."" suggests that the primary purpose of this commit is to address a bug, specifically bug number 532. The secondary part of the message mentions adding warnings to the README, which seems less relevant to the code diff provided.

The code diff modifies the `computeNext()` method, which is part of a splitting iterator. The core logic involves finding separators, trimming whitespace, and handling empty strings. The added code block specifically addresses the `limit` parameter. When the `limit` is 1, the code now consumes the rest of the string and returns it as the final item. It also readjusts the `end` pointer and trims it again to remove any trailing whitespace.

The original code likely had an issue where the `limit` was not being handled correctly, potentially leading to incorrect splitting behavior or unexpected results when a limit was specified. The added code block fixes this by ensuring that when the limit is reached, the remaining part of the string is returned as a single, final item. This aligns with the commit message indicating a bug fix.

Therefore, the changes in the code diff indicate a bug fix related to the handling of the `limit` parameter in the splitting logic.

**Buggy**"
guava,15629.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,27 +1,34 @@
   MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
     E actualLastElement = elementData(size);
-    int lastElementAt = heapForIndex(size).getCorrectLastElement(actualLastElement);
+    int lastElementAt = heapForIndex(size).swapWithConceptuallyLastElement(actualLastElement);
+    if (lastElementAt == index) {
+      // 'actualLastElement' is now at 'lastElementAt', and the element that was at 'lastElementAt'
+      // is now at the end of queue. If that's the element we wanted to remove in the first place,
+      // don't try to (incorrectly) trickle it. Instead, just delete it and we're done.
+      queue[size] = null;
+      return null;
+    }
     E toTrickle = elementData(size);
     queue[size] = null;
     MoveDesc<E> changes = fillHole(index, toTrickle);
     if (lastElementAt < index) {
       // Last element is moved to before index, swapped with trickled element.
       if (changes == null) {
         // The trickled element is still after index.
         return new MoveDesc<E>(actualLastElement, toTrickle);
       } else {
         // The trickled element is back before index, but the replaced element
         // has now been moved after index.
         return new MoveDesc<E>(actualLastElement, changes.replaced);
       }
     }
     // Trickled element was after index to begin with, no adjustment needed.
     return changes;
   }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message indicates two bug fixes in `MinMaxPriorityQueue`. The first bug is in `removeAt(int)` and the second is related to iteration over elements when elements are removed during iteration.

The provided diff focuses on the `removeAt(int)` method. The original code had a potential issue where the wrong element could be removed. The modified code introduces a check to handle the scenario where the last element, after being swapped, ends up at the index that was initially targeted for removal. In this case, the code now correctly avoids attempting to trickle the element, which was the source of the bug.

The changes in `removeAt(int)` method directly address the bug described in the commit message.

**Buggy**"
guava,15643.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,16 +1,16 @@
-    int getCorrectLastElement(E actualLastElement) {
+    int swapWithConceptuallyLastElement(E actualLastElement) {
       int parentIndex = getParentIndex(size);
       if (parentIndex != 0) {
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
         if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, actualLastElement) < 0) {
             queue[uncleIndex] = actualLastElement;
             queue[size] = uncleElement;
             return uncleIndex;
           }
         }
       }
       return size;
     }",NotBuggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states that it ""Fix[es] two bugs in MinMaxPriorityQueue."" It details the nature of these bugs:

1.  A bug in `removeAt(int)` causing incorrect element removal.
2.  A bug during iteration that leads to elements being iterated over multiple times after removals.

The commit message also links to an external report, further suggesting a bug fix.

**Diff Analysis:**

The provided diff snippet shows a change to a method named `getCorrectLastElement`, which has been renamed to `swapWithConceptuallyLastElement`. The logic within this method appears to be related to finding the correct position for the last element after a removal operation, potentially within the heap structure of the priority queue. The code involves comparing elements and swapping them within the `queue` array.

**Reasoning:**

The commit message clearly indicates a bug fix, and the diff snippet shows a change in a method that is likely involved in the `removeAt(int)` operation or the rebalancing of the priority queue after a removal. The logic within the method suggests that it's trying to maintain the heap property of the priority queue after an element has been removed. The renaming of the method also suggests a change in its purpose or functionality, which could be related to fixing the bug in `removeAt(int)`.

Given the explicit statement in the commit message and the nature of the code changes, it's highly probable that this commit addresses a bug.

**Conclusion:**

**Buggy**
"
guava,15653.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,21 +1,23 @@
     public void remove() {
       checkRemove(canRemove);
       checkModCount();
       canRemove = false;
       expectedModCount++;
       if (cursor < size()) {
         MoveDesc<E> moved = removeAt(cursor);
         if (moved != null) {
           if (forgetMeNot == null) {
             forgetMeNot = new ArrayDeque<E>();
             skipMe = new ArrayList<E>(3);
           }
-          forgetMeNot.add(moved.toTrickle);
+          if (!containsExact(skipMe, moved.toTrickle)) {
+            forgetMeNot.add(moved.toTrickle);
+          }
           skipMe.add(moved.replaced);
         }
         cursor--;
       } else { // we must have set lastFromForgetMeNot in next()
         checkState(removeExact(lastFromForgetMeNot));
         lastFromForgetMeNot = null;
       }
     }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states that it ""Fix[es] two bugs in MinMaxPriorityQueue"". It even provides details about the bugs:

1.  A bug in `removeAt(int)` that causes the wrong element to be removed.
2.  A bug that causes elements to be iterated over multiple times if elements are removed during iteration.

The commit message also references an external report, which further suggests a bug fix.

**Diff Analysis:**

The code diff is relatively small, focusing on the `remove()` method within an iterator. The key change is the addition of a check `if (!containsExact(skipMe, moved.toTrickle)) { ... }` before adding `moved.toTrickle` to the `forgetMeNot` queue.

**Reasoning:**

1.  **Alignment with Commit Message:** The change directly relates to the second bug mentioned in the commit message (iterating over elements multiple times). The `forgetMeNot` queue seems to be used to track elements that need to be revisited during iteration after a removal. By adding the `containsExact` check, the code prevents adding the same element to `forgetMeNot` multiple times, which could lead to the same element being iterated over more than once.

2.  **Bug Fix Indication:** The added `containsExact` check is a defensive measure to prevent duplicate entries in `forgetMeNot`. This strongly suggests that the original code was indeed flawed and could lead to incorrect iteration behavior when elements were removed during iteration. The fix addresses the issue of iterating over certain elements more than once.

3.  **Error Handling/Logical Correction:** The change represents a logical correction to the iteration logic. It's not strictly error handling, but it prevents a situation where the iterator behaves incorrectly due to duplicate entries in `forgetMeNot`.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix. The added `containsExact` check addresses the issue of iterating over elements multiple times when elements are removed during iteration.

**Buggy**
"
guava,11915.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,19 +1,20 @@
   final TypeToken<T> rejectTypeVariables() {
     new TypeVisitor() {
       @Override void visitTypeVariable(TypeVariable<?> type) {
         throw new IllegalArgumentException(
             runtimeType + ""contains a type variable and is not safe for the operation"");
       }
       @Override void visitWildcardType(WildcardType type) {
         visit(type.getLowerBounds());
         visit(type.getUpperBounds());
       }
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }
       @Override void visitGenericArrayType(GenericArrayType type) {
         visit(type.getGenericComponentType());
       }
     }.visit(runtimeType);
     return this;
   }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The commit message indicates a bug fix in the `rejectTypeVariable()` method. The bug was related to failing to account for the owner type's type variables when processing parameterized types. The provided code diff shows that the `visitParameterizedType` method in the `TypeVisitor` class has been updated to include a call to `visit(type.getOwnerType())`. This change directly addresses the bug described in the commit message by ensuring that the owner type is also visited and checked for type variables. Therefore, the changes indicate a bug fix.
**Buggy**"
guava,11913.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,3 +1,4 @@
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The commit message indicates a bug fix in the `rejectTypeVariable()` method, specifically related to handling owner type's type variables in parameterized types. The provided code example demonstrates a scenario where the bug occurs. The diff shows that the `visitParameterizedType` method in the `TypeVisitor` class is modified to include `visit(type.getOwnerType());`. This change ensures that the owner type is also visited during the type variable rejection process, which directly addresses the bug described in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
guava,17172.json,0330f712f56cc53633874e95bdc1203cf881eb85,"@@ -1,10 +1,10 @@
     public V get(@Nullable Object key) {
       if (key instanceof Range) {
         Range<?> range = (Range<?>) key;
         RangeMapEntry<K, V> rangeMapEntry = entriesByLowerBound.get(range.lowerBound);
-        if (rangeMapEntry.getKey().equals(range)) {
+        if (rangeMapEntry != null && rangeMapEntry.getKey().equals(range)) {
           return rangeMapEntry.getValue();
         }
       }
       return null;
     }",Buggy,"Fix bug in TreeRangeMap.asMapOfRanges().get(absentRange)
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=38651843",Buggy,"The commit message indicates a bug fix in `TreeRangeMap.asMapOfRanges().get(absentRange)`. The provided diff modifies the `get` method within an anonymous class, likely the one returned by `asMapOfRanges()`.

The original code directly accessed `rangeMapEntry` without checking if it was null. This could lead to a `NullPointerException` if `entriesByLowerBound.get(range.lowerBound)` returned null, which would happen when querying with an absent range.

The modified code adds a null check (`rangeMapEntry != null`) before accessing `rangeMapEntry.getKey()`. This prevents the `NullPointerException` and aligns with the commit message's intention to fix a bug related to absent ranges.

Therefore, the changes indicate a bug fix.

**Buggy**"
guava,21153.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void startTask() {
-      lock.lock();
-      try {
-        if (isShutdown()) {
+      synchronized (lock) {
+        if (shutdown) {
           throw new RejectedExecutionException(""Executor already shutdown"");
         }
         runningTasks++;
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message indicates a fix for a signaling bug in `newDirectExecutorService()` and a switch to implicit monitors. The provided code diff shows a change in the `startTask()` method where explicit `lock.lock()` and `lock.unlock()` calls are replaced with a `synchronized (lock)` block. This change directly relates to the commit message's mention of switching to implicit monitors. The original code used explicit locking, which could potentially lead to issues if the lock was not released correctly. The synchronized block ensures that the lock is always released, even if an exception occurs. Furthermore, the commit message describes a scenario where `awaitTermination` might not wake up under certain conditions, suggesting a bug related to signaling or locking. The switch to implicit monitors likely addresses this bug by ensuring proper synchronization and signaling. Therefore, the changes indicate a bug fix.

**Buggy**"
guava,21154.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void endTask() {
-      lock.lock();
-      try {
-        runningTasks--;
-        if (isTerminated()) {
-          termination.signalAll();
+      synchronized (lock) {
+        int numRunning = --runningTasks;
+        if (numRunning == 0) {
+          lock.notifyAll();
         }
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message indicates a bug fix related to signaling in `newDirectExecutorService()` where a thread waiting for termination might not be woken up after shutdown. The code diff shows a change in the `endTask()` method.

Here's a step-by-step analysis:

1.  **Commit Message Analysis:** The commit message explicitly states ""Fix a signaling bug"". It describes a scenario where `awaitTermination` might not be woken up after `shutdown`. This clearly indicates a bug fix. The message also mentions switching to implicit monitors, which could be related to the bug fix or a performance optimization.

2.  **Code Diff Analysis:**
    *   The original code uses `lock.lock()` and `lock.unlock()` with a `try-finally` block to ensure the lock is always released. It decrements `runningTasks` and signals all waiting threads if the executor is terminated.
    *   The modified code uses a `synchronized` block on `lock`. It decrements `runningTasks`, stores the result in `numRunning`, and notifies all waiting threads if `numRunning` becomes 0.
    *   The change from `termination.signalAll()` to `lock.notifyAll()` suggests that the original signaling mechanism was not working correctly in the described scenario. The condition `isTerminated()` in the original code might not have been sufficient to guarantee proper signaling. The new condition `numRunning == 0` seems more directly related to the bug described in the commit message, as it ensures that waiting threads are notified only when all tasks have finished.
    *   The switch to `synchronized` might be related to the biased locking mentioned in the commit message.

3.  **Relevance and Bug Fix Indication:** The code change directly addresses the bug described in the commit message. The original signaling mechanism was flawed, and the modified code provides a more reliable way to notify waiting threads when all tasks have completed. The change from explicit locking to implicit monitors using `synchronized` is likely an optimization to take advantage of biased locking. The core of the change is the signaling correction.

**Conclusion:** The code diff and commit message are strongly related, and the changes clearly indicate a bug fix.

**Buggy**"
guava,11437.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,25 +1,21 @@
   public String collapseFrom(CharSequence sequence, char replacement) {
-    int first = indexIn(sequence);
-    if (first == -1) {
-      return sequence.toString();
-    }
-
-    // TODO(kevinb): see if this implementation can be made faster
-    StringBuilder builder = new StringBuilder(sequence.length())
-        .append(sequence.subSequence(0, first))
-        .append(replacement);
-    boolean in = true;
-    for (int i = first + 1; i < sequence.length(); i++) {
+    // This implementation avoids unnecessary allocation.
+    int len = sequence.length();
+    for (int i = 0; i < len; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        if (!in) {
-          builder.append(replacement);
-          in = true;
+        if (c == replacement
+            && (i == len - 1 || !matches(sequence.charAt(i + 1)))) {
+          // a no-op replacement
+          i++;
+        } else {
+          StringBuilder builder = new StringBuilder(len)
+              .append(sequence.subSequence(0, i))
+              .append(replacement);
+          return finishCollapseFrom(sequence, i + 1, len, replacement, builder, true);
         }
-      } else {
-        builder.append(c);
-        in = false;
       }
     }
-    return builder.toString();
+    // no replacement needed
+    return sequence.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",Buggy,"The commit message indicates optimization and removal of workarounds, suggesting potential bug fixes or performance improvements. The code diff shows changes in `collapseFrom` method to avoid unnecessary allocation. The new implementation checks for the need for replacement before allocating a `StringBuilder`. This optimization suggests a potential bug fix related to performance or memory usage.

Therefore, the changes indicate a bug fix.

**Buggy**"
guava,11403.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,21 +1,17 @@
-  public String trimAndCollapseFrom(CharSequence sequence, char replacement) {
-    int first = negate().indexIn(sequence);
-    if (first == -1) {
-      return """"; // everything matches. nothing's left.
-    }
-    StringBuilder builder = new StringBuilder(sequence.length());
-    boolean inMatchingGroup = false;
-    for (int i = first; i < sequence.length(); i++) {
+  private String finishCollapseFrom(
+      CharSequence sequence, int start, int end, char replacement,
+      StringBuilder builder, boolean inMatchingGroup) {
+    for (int i = start; i < end; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        inMatchingGroup = true;
-      } else {
-        if (inMatchingGroup) {
+        if (!inMatchingGroup) {
           builder.append(replacement);
-          inMatchingGroup = false;
+          inMatchingGroup = true;
         }
+      } else {
         builder.append(c);
+        inMatchingGroup = false;
       }
     }
     return builder.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",Buggy,"Okay, let's analyze this Git diff and commit message.

**Commit Message Analysis:**

The commit message indicates several changes:

1.  **Optimization:** `Optimize collapseFrom and trimAndCollapseFrom so that they only allocate if necessary.` This suggests performance improvements by avoiding unnecessary memory allocation.
2.  **Testing:** `Add a few tests.` This implies the addition of new test cases, likely to verify the correctness of the optimized code.
3.  **GWT Workaround Removal:** `Remove the GWT workarounds for GWT bug 4491 since it appears to be fixed.` This indicates the removal of code that was specifically added to address a known bug in the Google Web Toolkit (GWT).

**Code Diff Analysis:**

The code diff shows a refactoring of the `trimAndCollapseFrom` method (though the full method isn't shown, only a helper method `finishCollapseFrom`). The code iterates through a character sequence and replaces consecutive matching characters with a single replacement character. The key change appears to be the introduction of a `StringBuilder` only when needed, which aligns with the optimization goal stated in the commit message.

**Reasoning:**

The commit message explicitly mentions optimization and the removal of a workaround for a specific bug. The code diff, while not showing the entire method, does suggest an optimization by potentially avoiding `StringBuilder` allocation. The removal of the GWT workaround strongly suggests that a bug was previously present and has now been resolved (or worked around).

**Conclusion:**

**Buggy**
"
guava,22907.json,9bf42862f877a0ed8234bd484aa0089a3773ccd4,"@@ -1,11 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
-      builder.append(""SUCCESS, result=["").append(getDone(this)).append(""]"");
+      V value = getDone(this);
+      builder.append(""SUCCESS, result=["").append(value).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED"");
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"ImmediateFuture.toString() implementation to provide the returned value or exception.
Provides for all ImmediateFutures implementation the same syntax that AbstractFuture uses.

It's especially convenient during debugging of mocked asynchronous gRPC services.

Fixes bug in AbstractFuture#toString in GWT that caused wrong representation:
""Class$S413@205[status=SUCCESS, result=[FAILURE, cause=[Class$S6]]"" due to ongoing
builder being already called when getDone() throws exception.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165925433",Buggy,"The commit message indicates that the `toString()` method of `ImmediateFuture` implementations is being updated to provide more informative output, including the returned value or exception. It also mentions a bug fix in `AbstractFuture#toString` related to incorrect representation in GWT due to an exception being thrown during the builder call.

The code diff shows a modification within the `addDoneString` method. It specifically addresses exception handling during the retrieval of the result using `getDone(this)`. The original code directly appended the result of `getDone(this)` to the builder. The modified code now stores the result of `getDone(this)` in a variable `value` before appending it to the builder. This change is likely intended to address the bug mentioned in the commit message, where an exception during `getDone()` would lead to an incomplete or incorrect representation in the `toString()` output. The added `V value = getDone(this);` line allows the code to catch the exception before the builder is used.

The changes align with the commit message, and the exception handling improvements suggest a bug fix.

**Buggy**
"
pmd,8491.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,17 +1,17 @@
     public Object visit(ASTMethodDeclarator node, Object data) {
         Scope scope = node.getScope();
         Map params = scope.getVariableDeclarations(true);
         for (Iterator i = params.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration)i.next();
             List usages = (List)params.get(decl);
             for (Iterator j = usages.iterator();j.hasNext();) {
                 NameOccurrence occ = (NameOccurrence)j.next();
-                if (occ.isOnLeftHandSide()) {
+                if (occ.isOnLeftHandSide() && (occ.getNameForWhichThisIsAQualifier() == null)) {
                     RuleContext ctx = (RuleContext)data;
                     String msg = MessageFormat.format(getMessage(), new Object[] {decl.getImage()});
                     ctx.getReport().addRuleViolation(createRuleViolation(ctx, decl.getLine(), msg));
                 }
             }
         }
         return super.visit(node, data);
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""Several bug fixes"" indicates that the commit aims to resolve existing issues in the code.

The code diff modifies a condition within a nested loop. Specifically, the change adds `&& (occ.getNameForWhichThisIsAQualifier() == null)` to the `if` statement:

```java
if (occ.isOnLeftHandSide() && (occ.getNameForWhichThisIsAQualifier() == null)) {
    // ...
}
```

This addition suggests a bug fix. The original condition `occ.isOnLeftHandSide()` was likely too broad, causing false positives. The added condition refines the check to only consider occurrences on the left-hand side that are not qualifiers. This implies that the original code was incorrectly flagging certain variable usages as violations. The added condition filters out those incorrect flags.

Therefore, the code change addresses a bug where the rule was incorrectly identifying violations.

**Buggy**"
pmd,8745.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,20 +1,20 @@
     public Object visit(ASTMethodDeclaration node, Object data) {
         if (node.isAbstract()) {
             return data;
         }
 
         List returnNodes = new ArrayList();
-        node.findChildrenOfType(ASTReturnStatement.class, returnNodes);
+        node.findChildrenOfType(ASTReturnStatement.class, returnNodes, false);
         if (returnNodes.size() > 1) {
             RuleContext ctx = (RuleContext)data;
             for (Iterator i = returnNodes.iterator(); i.hasNext();) {
                 SimpleNode problem = (SimpleNode)i.next();
                 // skip the last one, it's OK
                 if (!i.hasNext()) {
                     continue;
                 }
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, problem.getBeginLine()));
             }
         }
         return data;
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""Several bug fixes"" suggests that the changes in the diff are intended to fix bugs.

The diff modifies the `visit` method of a class, specifically changing how `ASTReturnStatement` nodes are found within a method declaration. The original code used `node.findChildrenOfType(ASTReturnStatement.class, returnNodes)` to find all return statements. The modified code uses `node.findChildrenOfType(ASTReturnStatement.class, returnNodes, false)`. The addition of the `false` argument likely changes the search behavior, potentially preventing the method from traversing into nested scopes (e.g., inner classes or anonymous classes).

Multiple return statements within a method can sometimes indicate complex logic or potential code smells. The original code might have incorrectly identified return statements in nested scopes, leading to false positives. The change with the `false` argument suggests an attempt to correct this behavior, implying a bug fix.

Therefore, the changes align with the commit message indicating bug fixes.

**Buggy**"
pmd,1151.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,15 +1,18 @@
-    private void findChildrenOfType(Node node, Class targetType, List results) {
+    private void findChildrenOfType(Node node, Class targetType, List results, boolean descendIntoNestedClasses) {
         if (node.getClass().equals(targetType)) {
             results.add(node);
         }
+        if (node.getClass().equals(ASTClassBody.class) && !descendIntoNestedClasses) {
+            return;
+        }
         for (int i=0; i<node.jjtGetNumChildren(); i++) {
-            Node child = (Node)node.jjtGetChild(i);
+            Node child = node.jjtGetChild(i);
             if (child.jjtGetNumChildren()>0) {
-                findChildrenOfType(child, targetType, results);
+                findChildrenOfType(child, targetType, results, descendIntoNestedClasses);
             } else {
                 if (child.getClass().equals(targetType)) {
                     results.add(child);
                 }
             }
         }
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates several bug fixes. The code changes involve the `findChildrenOfType` method. A new parameter `descendIntoNestedClasses` is added to control whether the search should descend into nested classes. The code now checks if the current node is an `ASTClassBody` and if `descendIntoNestedClasses` is false, it returns, preventing further recursion into the nested class. This change suggests a bug fix related to unwanted traversal of nested classes during the search for nodes of a specific type.

**Buggy**"
pmd,7973.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,10 +1,4 @@
     public boolean isOnRightHandSide() {
         SimpleNode node = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
-        if (node instanceof ASTExpression) {
-            SimpleNode parent = (SimpleNode) node.jjtGetParent();
-            if (node.jjtGetNumChildren() == 3) {
-                return true;
-            }
-        }
-        return false;
+        return node instanceof ASTExpression && node.jjtGetNumChildren() == 3;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix for a false positive reported by the `ImmutableFieldRule` for static fields. The diff shows a change in the `isOnRightHandSide()` method. The original code had nested `if` statements to check if the node is an `ASTExpression` and if it has 3 children. The modified code simplifies this logic into a single `return` statement using a boolean `&&` operator. This change directly relates to the bug fix described in the commit message because it corrects the logic that determines if a field is on the right-hand side of an expression, which is crucial for the `ImmutableFieldRule` to function correctly and avoid false positives. The simplification suggests a more accurate and concise way of determining the condition, implying the previous logic was flawed and leading to the reported bug.

**Buggy**"
pmd,8886.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,18 +1,18 @@
     public Object visit(ASTUnmodifiedClassDeclaration node, Object data) {
         Map vars = node.getScope().getVariableDeclarations();
         for (Iterator i = vars.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration) i.next();
-            if (!decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
+            if (decl.getAccessNodeParent().isStatic() || !decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
                 continue;
             }
+
             int result = initializedInConstructor((List)vars.get(decl));
             if (result == MUTABLE) {
             	continue;
             }
-            if ((result == IMMUTABLE) ||
-                ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
+            if (result == IMMUTABLE || ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
                 ((RuleContext) data).getReport().addRuleViolation(createRuleViolation((RuleContext) data, decl.getLine(), MessageFormat.format(getMessage(), new Object[]{decl.getImage()})));
             }
         }
         return super.visit(node, data);
     }",Buggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix for a false positive in the `ImmutableFieldRule` and a version number update. The code diff modifies the condition within the `visit` method of the `ASTUnmodifiedClassDeclaration` node. Specifically, it adds `decl.getAccessNodeParent().isStatic()` to the condition that skips the immutability check. This change suggests that the rule was incorrectly flagging static fields as mutable, which aligns with the bug fix described in the commit message. The addition of the static check prevents the rule from analyzing static fields, thus resolving the false positive.

Therefore, the changes indicate a bug fix.

**Buggy**
"
pmd,8888.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,31 +1,31 @@
     private int initializedInConstructor(List usages) {
         int rc = MUTABLE, initCount = 0;
         boolean setInConstructor = false;
 		boolean foundUsage = false;
 
         for (Iterator j = usages.iterator(); j.hasNext();) {
         	foundUsage = true;
-        	NameOccurrence occurance = (NameOccurrence)j.next();
-            if (occurance.isOnLeftHandSide()) {
-            	SimpleNode node = occurance.getLocation();
+        	NameOccurrence occ = (NameOccurrence)j.next();
+            if (occ.isOnLeftHandSide()) {
+            	SimpleNode node = occ.getLocation();
             	if (node.getFirstParentOfType(ASTConstructorDeclaration.class) != null) {
             		setInConstructor = true;
             		initCount++;
  				}
  				else {
  					if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
  						initCount++;
  					}
  				}
  			}
         }
         if (!foundUsage || (initCount == 0)) {
         	rc = CHECKDECL;
         }
         else {
         	if ((initCount == 1) && setInConstructor) {
         		rc = IMMUTABLE;
         	}
         }
         return rc;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the `ImmutableFieldRule` in PMD, specifically addressing false positives for static fields. The provided code diff focuses on the `initializedInConstructor` method. Let's analyze the changes:

- **Variable Naming:** `occurance` changed to `occ`. This is a minor refactoring and doesn't directly indicate a bug fix.
- **Logic:** The core logic within the `for` loop and the subsequent `if` conditions remains the same. There are no apparent changes to how the method determines if a field is initialized in the constructor.

Based on the diff, it's difficult to directly correlate the code changes with the bug fix described in the commit message (""ImmutableFieldRule no longer reports false positives for static fields""). The changes are minimal and don't seem to address the specific issue of static fields. It's possible that the actual fix for the static field issue lies elsewhere in the codebase, and this diff represents a related but separate improvement or refactoring.

Therefore, based on the provided diff alone, it's not possible to confirm that this code change directly addresses the bug described in the commit message.

**Buggy**
"
pmd,8249.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,4 +1,4 @@
 		public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
 			parent.addType(Type.getType(desc));
-			return annotationVisitor;
+			return parent.annotationVisitor;
 		}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the `acceptType` functionality, limiting its operation to fields and locals. The provided code snippet shows a change in the `visitAnnotation` method, specifically in how the `annotationVisitor` is returned.

The original code returned a local `annotationVisitor`, while the corrected code returns `parent.annotationVisitor`. This suggests that the original implementation might have been creating a new, isolated `annotationVisitor` instead of using the intended one associated with the `parent` object. This would lead to incorrect or incomplete annotation processing, which aligns with the bug fix description.

The change ensures that the correct `annotationVisitor` is used, likely fixing a bug where annotations were not being processed correctly when applied to elements other than fields and locals.

**Buggy**"
pmd,8247.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,5 +1,5 @@
 	private void extractSignature(String sig) {
 		if (sig != null) {
-			new SignatureReader(sig).acceptType(sigVisitor);
+			new SignatureReader(sig).accept(sigVisitor);
 		}
 	}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""bug fix: acceptType only works for fields and locals"" suggests that the original code `acceptType` was incorrectly used and has been replaced with `accept`. The diff shows that `acceptType` is indeed replaced with `accept`. This indicates a bug fix where the wrong method was called, and the correct method `accept` is now being used.

**Buggy**"
pmd,6069.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,8 +1,8 @@
     public void jjtClose() {
-        if ((children == null) || (children.length == 0)) {
+        if (beginLine == -1 && (children == null || children.length == 0)) {
             beginLine = parser.token.beginLine;
             beginColumn = parser.token.beginColumn;
         }
         endLine = parser.token.endLine;
         endColumn = parser.token.endColumn;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 583047, specifically addressing incorrect column numbers on ASTName nodes. The provided code diff modifies the `jjtClose` method, adding a condition `beginLine == -1` to the existing check for null or empty children. This suggests that the original logic was flawed, potentially leading to incorrect column assignments under certain circumstances. The added condition likely addresses a specific scenario where `beginLine` was not properly initialized, causing the subsequent column assignment to be incorrect. This aligns with the commit message's claim of fixing incorrect column numbers, indicating a bug fix.

**Buggy**"
pmd,6068.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,6 +1,6 @@
     public void jjtOpen() {
-        if (parser.token.next != null) {
+        if (beginLine == -1 && parser.token.next != null) {
             beginLine = parser.token.next.beginLine;
             beginColumn = parser.token.next.beginColumn;
         }
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 583047, specifically addressing incorrect column numbers on ASTName nodes. The provided code diff modifies the `jjtOpen` method. It adds a condition `beginLine == -1` to the existing `if` statement. This suggests that the original code was unconditionally assigning line and column numbers, potentially leading to incorrect values in certain scenarios. The added condition likely prevents this incorrect assignment, thus fixing the bug described in the commit message.

Reasoning:

1.  **Commit Message Analysis:** The commit message explicitly states a bug fix related to incorrect column numbers in ASTName nodes.
2.  **Code Diff Analysis:** The code modification introduces a conditional check (`beginLine == -1`) before assigning line and column numbers. This implies that the original code was assigning these values without proper validation, potentially leading to incorrect assignments and thus the bug.
3.  **Relevance:** The code change directly addresses the issue described in the commit message by preventing potentially incorrect column assignments.

Conclusion: **Buggy**
"
pmd,1134.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,11 +1,3 @@
     public int getBeginLine() {
-        if (beginLine != -1) {
-            return beginLine;
-        } else {
-            if ((children != null) && (children.length > 0)) {
-                return ((SimpleNode) children[0]).getBeginLine();
-            } else {
-                throw new RuntimeException(""Unable to determine begining line of Node."");
-            }
-        }
+        return beginLine;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 583047, specifically addressing incorrect column information on ASTName nodes. The provided code diff removes a block of code that attempts to determine the begin line of a node based on its children if the `beginLine` field is not already set. The original code also included a `RuntimeException` if the begin line could not be determined. The removal of this logic suggests that the original approach was flawed or unnecessary, and the fix likely involves ensuring that the `beginLine` field is correctly populated elsewhere, thus resolving the incorrect column information issue. The removal of the exception handling also indicates that the original code was trying to handle a situation that should not occur after the bug fix.

**Buggy**"
pmd,2689.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,7 +1,7 @@
     public int getLineCount(Mark mark, Match match) {
         TokenEntry endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount());
         if (endTok.equals(TokenEntry.EOF)) {
             endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount() - 1);
         }
-        return endTok.getBeginLine() - mark.getBeginLine();
+        return endTok.getBeginLine() - mark.getBeginLine() - 1;
     }",Buggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in the ""source code slice"" logic. The diff shows a change in the `getLineCount` method, specifically modifying the return value by subtracting 1. This adjustment suggests a correction to the line count calculation, which aligns with the commit message's claim of fixing a bug in the source code slice logic.

**Buggy**"
pmd,2580.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,32 +1,32 @@
     public void findMatches(int min) {
        /*
          Assign sort codes to all the pooled code. This should speed
          up sorting them.
        */
         int count = 1;
         for (Iterator iter = pool.keySet().iterator(); iter.hasNext();) {
            TokenEntry token = (TokenEntry)iter.next();
            token.setSortCode(count++);
         }
 
         MarkComparator mc = new MarkComparator(cpdListener, code);
         Collections.sort(marks, mc);
 
         MatchCollector coll = new MatchCollector(marks, code, mc);
         matches = coll.collect(min);
         Collections.sort(matches);
 
         for (Iterator i = matches(); i.hasNext();) {
             Match match = (Match)i.next();
             for (Iterator occurrences = match.iterator(); occurrences.hasNext();) {
                 Mark mark = (Mark)occurrences.next();
-                SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                 match.setLineCount(tokens.getLineCount(mark, match));
                 if (!occurrences.hasNext()) {
                     int start = mark.getBeginLine();
-                    int end = mark.getBeginLine()-1 + tokens.getLineCount(mark, match);
+                    int end = start + match.getLineCount() - 1;
+                    SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                     match.setSourceCodeSlice(sourceCode.getSlice(start, end));
                 }
             }
         }
     }",NotBuggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in the 'source code slice' logic.

The code changes involve modifying the calculation of the `end` variable and retrieving the `SourceCode` object.

Specifically, the original code had:
```java
int end = mark.getBeginLine()-1 + tokens.getLineCount(mark, match);
SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
```

The modified code has:
```java
int start = mark.getBeginLine();
int end = start + match.getLineCount() - 1;
SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
```
The `end` line calculation is changed to `int end = start + match.getLineCount() - 1;` and the `SourceCode` retrieval is moved after the `end` line calculation.

The original calculation of `end` was incorrect. It subtracted 1 from the begin line and added the line count. The new calculation correctly adds the line count to the begin line and subtracts 1. This indicates a bug fix in how the end line was calculated for the source code slice.

Therefore, the changes indicate a bug fix.

**Buggy**"
pmd,961.json,1ee8cc4e8daf001172f2a3aa1a8c2a89706e988f,"@@ -1,32 +1,32 @@
     public static MetricKey<ASTAnyTypeDeclaration> of(final Metric<ASTAnyTypeDeclaration> metric, final String name) {
         return new MetricKey<ASTAnyTypeDeclaration>() {
             @Override
             public String name() {
                 return name;
             }
 
 
             @Override
             public Metric<ASTAnyTypeDeclaration> getCalculator() {
                 return metric;
             }
 
 
             @Override
             public boolean supports(ASTAnyTypeDeclaration node) {
                 return metric.supports(node);
             }
 
 
             @Override
             public boolean equals(Object obj) {
                 return obj == this;
             }
 
 
             @Override
             public int hashCode() {
-                return metric.hashCode() * 31 + name.hashCode();
+                return (metric != null ? metric.hashCode() * 31 : 0) + (name != null ? name.hashCode() : 0);
             }
         };
     }",Buggy,Fix bug in key hashcode with null names or metric,Buggy,"The commit message indicates a fix for a bug related to the key's hashcode calculation when the name or metric is null.

The code diff modifies the `hashCode()` method.  The original implementation directly called `name.hashCode()` without checking if `name` was null, which would cause a `NullPointerException`. Similarly, `metric.hashCode()` could throw a `NullPointerException` if metric was null. The updated implementation adds null checks for both `metric` and `name` before calling `hashCode()`, providing a default value of 0 if either is null. This prevents the `NullPointerException` and correctly handles null values in the hashcode calculation.

The changes directly address the bug described in the commit message.

**Buggy**"
pmd,5595.json,64ca9b6f01f9b946a456f6de05eada2374906d6a,"@@ -1,3 +1,6 @@
     public EcmascriptNode getFinallyBlock() {
-	return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
+        if (!isFinally()) {
+            return null;
+        }
+        return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
     }",Buggy,"pmd: fix #1141 ECMAScript: getFinallyBlock() is buggy.
fix #1142 ECMAScript: getCatchClause() is buggy.",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""pmd: fix #1141 ECMAScript: getFinallyBlock() is buggy. fix #1142 ECMAScript: getCatchClause() is buggy."" clearly states that the commit aims to fix bugs in the `getFinallyBlock()` and `getCatchClause()` methods within the ECMAScript component of PMD. The use of ""fix"" and the explicit mention of bug numbers (#1141 and #1142) strongly suggest a bug fix.

**Diff Analysis:**

The diff shows a modification to the `getFinallyBlock()` method.  The original code simply returned the last child node. The modified code now includes a check `if (!isFinally())`. If the condition is true, it returns `null`. This indicates that the original code could potentially return an incorrect node (or throw an exception) if the block wasn't actually a finally block. The added `isFinally()` check prevents this, which aligns with the commit message's claim of a bug fix.

**Reasoning:**

1.  **Explicit Bug Fix:** The commit message explicitly states that it fixes bugs.
2.  **Code Correction:** The diff shows a conditional check being added to `getFinallyBlock()`. This suggests that the original code had a flaw that could lead to incorrect behavior. The added check corrects this flaw.
3.  **Alignment:** The commit message and the code changes are directly related. The commit message mentions a bug in `getFinallyBlock()`, and the diff shows a modification to that very method.

**Conclusion:**

The commit message and the code changes strongly indicate a bug fix. The original code had a potential flaw, and the changes address it.

**Buggy**"
pmd,376.json,a3d5e7e1921e2c1c10f6a1e60fdfc19983732097,"@@ -1,3 +1,3 @@
     public void setClasspathRef(Reference r) {
-        createClasspath().setRefid(r);
+        createLongClasspath().setRefid(r);
     }",Buggy,"Fixed nested classpath bug


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1901 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""Fixed nested classpath bug"" suggests a bug fix related to how the classpath is handled, specifically when it involves nested classpath configurations.

The code diff shows a change from `createClasspath().setRefid(r)` to `createLongClasspath().setRefid(r)`. This indicates a modification in how the classpath is created or handled. The change from `createClasspath` to `createLongClasspath` implies that the original method might have had limitations in handling long or nested classpaths, which could have led to a bug. The new method likely addresses this limitation.

Based on the commit message and the code change, it's highly probable that this commit fixes a bug related to nested or long classpaths.

**Buggy**"
pmd,7138.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,7 +1,11 @@
-    private String getPackageName(String importName) {
+    public String getPackageName() {
+        String importName = getImportedName();
+        if (isImportOnDemand) {
+            return importName;
+        }
         if (importName.indexOf('.') == -1) {
             return """";
         }
         int lastDot = importName.lastIndexOf('.');
         return importName.substring(0, lastDot);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in"" clearly indicates a bug fix.  The bug involves `ImportFromSamePackage` and specifically addresses a scenario where a class uses an on-demand import (e.g., `import mypackage.*;`) for its own package.

**Diff Analysis:**

The diff shows a change to the `getPackageName` method.  Here's a breakdown:

1.  **Visibility Change:** The method's visibility changed from `private` to `public`. This might be related to making the method accessible for testing or use in other parts of the class.

2.  **Name Change:** The method's name changed from `getPackageName(String importName)` to `getPackageName()`. This suggests that the import name is now obtained internally rather than being passed as an argument.

3.  **Logic Change:** The code now includes a check for `isImportOnDemand`. If the import is on-demand, the entire `importName` is returned. This is the core of the bug fix.

**Reasoning:**

The commit message and the diff strongly align. The original code likely failed to correctly identify the package name when an on-demand import was used for the same package. The updated code now handles this case by returning the full import name when `isImportOnDemand` is true. This addresses the specific bug described in the commit message. The change from `private` to `public` and the name change are likely refactoring steps to support the core logic change.

**Conclusion:**

**Buggy**
"
pmd,8736.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,19 +1,19 @@
     public Object visit(ASTImportDeclaration node, Object data) {
-        ImportWrapper wrapper = new ImportWrapper(node.getImportedNameNode().getImage(), node.getImportedNameNode().getImage(), node.getImportedNameNode());
+        ImportWrapper wrapper = new ImportWrapper(node.getImportedName(), node.getImportedName(), node.getImportedNameNode());
 
         // blahhhh... this really wants to be ASTImportDeclaration to be polymorphic...
         if (node.isImportOnDemand()) {
             if (importOnDemandImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 importOnDemandImports.add(wrapper);
             }
         } else {
             if (singleTypeImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 singleTypeImports.add(wrapper);
             }
         }
         return data;
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to handling on-demand imports from the same package. The code diff modifies how the `ImportWrapper` is constructed, specifically changing `node.getImportedNameNode().getImage()` to `node.getImportedName()`. This suggests that the previous method of retrieving the imported name was incorrect, leading to a failure in detecting on-demand imports from the same package. The change likely fixes the incorrect name retrieval, thus resolving the bug described in the commit message.

**Buggy**"
pmd,8913.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,6 +1,6 @@
     public Object visit(ASTImportDeclaration node, Object o) {
-        if (node.getImportedNameNode().getImage().indexOf(""junit"") != -1) {
+        if (node.getImportedName().indexOf(""junit"") != -1) {
             junitImported = true;
         }
         return super.visit(node, o);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix in `ImportFromSamePackage` related to handling on-demand imports from the same package. The provided diff modifies the `visit(ASTImportDeclaration node, Object o)` method. Specifically, it changes `node.getImportedNameNode().getImage()` to `node.getImportedName()`. While this change itself doesn't directly reveal the bug fix described in the commit message, it suggests an attempt to correctly retrieve the imported name. It's plausible that `getImportedNameNode().getImage()` was not correctly capturing the imported name in the case of on-demand imports, and `getImportedName()` provides a more accurate representation. Therefore, the change aligns with the commit message's intention to fix a bug related to import handling.

**Buggy**"
pmd,4678.json,fb25329e0d9ec5d632667df393f7c1b25a698e68,"@@ -1,42 +1,42 @@
     public Object visit(ASTUserClass node, Object data) {
 
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
-        // baz = String.escapeSignleQuotes(...);
-        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
-        for (ASTAssignmentExpression a : assignmentCalls) {
-            findSanitizedVariables(a);
-            findSelectContainingVariables(a);
-        }
-
         final List<ASTFieldDeclaration> fieldExpr = node.findDescendantsOfType(ASTFieldDeclaration.class);
         for (ASTFieldDeclaration a : fieldExpr) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
         // String foo = String.escapeSignleQuotes(...);
         final List<ASTVariableDeclaration> variableDecl = node.findDescendantsOfType(ASTVariableDeclaration.class);
         for (ASTVariableDeclaration a : variableDecl) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
+        // baz = String.escapeSignleQuotes(...);
+        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
+        for (ASTAssignmentExpression a : assignmentCalls) {
+            findSanitizedVariables(a);
+            findSelectContainingVariables(a);
+        }
+
         // Database.query(...) check
         final List<ASTMethodCallExpression> potentialDbQueryCalls = node
                 .findDescendantsOfType(ASTMethodCallExpression.class);
 
         for (ASTMethodCallExpression m : potentialDbQueryCalls) {
             if (!Helper.isTestMethodOrClass(m) && Helper.isMethodName(m, DATABASE, QUERY)) {
                 reportStrings(m, data);
                 reportVariables(m, data);
             }
         }
-        
+
         safeVariables.clear();
         selectContainingVariables.clear();
 
         return data;
     }",NotBuggy,Bug fix to improve detection of concatenated vars,Buggy,"The commit message indicates an improvement in detecting concatenated variables. The diff shows that the code block for processing `ASTAssignmentExpression` has been moved from the beginning of the `visit` method to after the `ASTVariableDeclaration` block.

The original order was:
1. Process `ASTAssignmentExpression`
2. Process `ASTFieldDeclaration`
3. Process `ASTVariableDeclaration`
4. Process `ASTMethodCallExpression`

The modified order is:
1. Process `ASTFieldDeclaration`
2. Process `ASTVariableDeclaration`
3. Process `ASTAssignmentExpression`
4. Process `ASTMethodCallExpression`

The change in order suggests that the processing of `ASTAssignmentExpression` might depend on information gathered during the processing of `ASTFieldDeclaration` or `ASTVariableDeclaration`. This dependency could be related to identifying variables that are later used in assignments, potentially involving concatenation. The movement of the code block implies a logical correction to ensure that all relevant variables are identified before processing assignments, which could lead to improved detection of concatenated variables.

Therefore, the change is likely a bug fix.

**Buggy**"
pmd,7859.json,15ff7ca9447619240ec96fdb32f7974e1ecf5730,"@@ -1,34 +1,43 @@
-    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, final int parameterCount) {
+    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, String... parameterTypes) {
         ASTMethodDeclaration methodDeclaration = new ASTMethodDeclaration(JavaParserTreeConstants.JJTMETHODDECLARATION);
         methodDeclaration.setPublic(true);
         methodDeclaration.setScope(this);
 
         ASTMethodDeclarator methodDeclarator = new ASTMethodDeclarator(JavaParserTreeConstants.JJTMETHODDECLARATOR);
         methodDeclarator.setImage(methodName);
         methodDeclarator.setScope(this);
 
         ASTFormalParameters formalParameters = new ASTFormalParameters(JavaParserTreeConstants.JJTFORMALPARAMETERS);
         formalParameters.setScope(this);
 
         methodDeclaration.jjtAddChild(methodDeclarator, 0);
         methodDeclarator.jjtSetParent(methodDeclaration);
         methodDeclarator.jjtAddChild(formalParameters, 0);
         formalParameters.jjtSetParent(methodDeclarator);
 
+        int parameterCount = parameterTypes.length;
         for (int i = 0; i < parameterCount; i++) {
             ASTFormalParameter formalParameter = new ASTFormalParameter(JavaParserTreeConstants.JJTFORMALPARAMETER);
             formalParameters.jjtAddChild(formalParameter, i);
             formalParameter.jjtSetParent(formalParameters);
 
             ASTType type = new ASTType(JavaParserTreeConstants.JJTTYPE);
             formalParameter.jjtAddChild(type, 0);
             type.jjtSetParent(formalParameter);
+            ASTReferenceType referenceType = new ASTReferenceType(JavaParserTreeConstants.JJTREFERENCETYPE);
+            type.jjtAddChild(referenceType, 0);
+            referenceType.jjtSetParent(type);
+            ASTClassOrInterfaceType classOrInterfaceType = new ASTClassOrInterfaceType(JavaParserTreeConstants.JJTCLASSORINTERFACETYPE);
+            classOrInterfaceType.setImage(parameterTypes[i]);
+            referenceType.jjtAddChild(classOrInterfaceType, 0);
+            classOrInterfaceType.jjtSetParent(referenceType);
+
             ASTVariableDeclaratorId variableDeclaratorId = new ASTVariableDeclaratorId(JavaParserTreeConstants.JJTVARIABLEDECLARATORID);
             variableDeclaratorId.setImage(""arg"" + i);
             formalParameter.jjtAddChild(variableDeclaratorId, 1);
             variableDeclaratorId.jjtSetParent(formalParameter);
         }
 
         MethodNameDeclaration mnd = new MethodNameDeclaration(methodDeclarator);
         return mnd;
     }",Buggy,Fixes #1490 [java] PMD Error while processing - NullPointerException,Buggy,"The commit message indicates a fix for a `NullPointerException` that occurred during PMD processing. The code diff modifies the `createBuiltInMethodDeclaration` method.

Here's a breakdown of the changes:

1.  **Parameter Type Change:** The method signature of `createBuiltInMethodDeclaration` is changed from accepting an `int parameterCount` to accepting a variable number of `String` arguments (`String... parameterTypes`). This suggests the original method might have been inflexible in handling different parameter types, potentially leading to errors when the expected number or type of parameters didn't match.

2.  **Parameter Handling Logic:** The code now iterates through the `parameterTypes` array to create formal parameters for the method declaration.  It creates AST nodes for `ASTType`, `ASTReferenceType`, and `ASTClassOrInterfaceType` using the provided `parameterTypes`. This is a significant change from the original method, which presumably didn't handle parameter types correctly or at all.

The changes strongly suggest a bug fix related to how method parameters were being handled. The original code likely had a flaw in how it dealt with method parameters, leading to a `NullPointerException` in certain scenarios. The modified code addresses this by explicitly handling parameter types, making the method more robust and preventing the reported exception.

Therefore, the changes indicate a bug fix.

**Buggy**"
pmd,10015.json,8694a26cafe730add04695ec1917cd51bd4bbc4e,"@@ -1,4 +1,3 @@
     public TokenManager createTokenManager(Reader source) {
-	return new PLSQLTokenManager(source);
-
+        return new PLSQLTokenManager(IOUtil.skipBOM(source));
     }",Buggy,Fix #1167 Error while processing PLSQL file with BOM,Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""Fix #1167 Error while processing PLSQL file with BOM"" clearly indicates a bug fix. The bug is related to handling PLSQL files that contain a Byte Order Mark (BOM). The commit aims to resolve an error encountered during the processing of such files.

**Git Diff Analysis:**

The diff shows a modification in the `createTokenManager` method.  Specifically, the original code directly passed the `Reader` to the `PLSQLTokenManager` constructor. The modified code now wraps the `Reader` with `IOUtil.skipBOM(source)` before passing it to the `PLSQLTokenManager`.

`IOUtil.skipBOM(source)` likely handles the removal or skipping of the BOM from the input stream. This aligns perfectly with the commit message, as it addresses the issue of processing PLSQL files with a BOM.  The original code likely failed when encountering the BOM at the beginning of the file, leading to parsing errors or incorrect tokenization.

**Reasoning:**

The commit message explicitly states a bug fix related to BOM handling in PLSQL files. The code modification introduces a mechanism (`IOUtil.skipBOM`) to handle BOMs before processing the file content. This strongly suggests that the original code had a bug where it couldn't correctly process files with BOMs, and the change fixes this issue.

**Conclusion:**

**Buggy**
"
pmd,3080.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,12 +1,12 @@
     public void publish(LogRecord logRecord) {
-        System.out.println(logRecord.getMessage());
+        System.out.println(FORMATTER.format(logRecord));
         if (logRecord.getThrown() != null) {
             // Use the same channel, to make sure that the stacktrace comes
             // after the message on the console (using printStackTrace
             // directly messes things up)
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             System.out.println(stringWriter.toString());
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for formatting problems in loggers. The diff shows that the `logRecord.getMessage()` is now formatted using `FORMATTER.format(logRecord)` before being printed to the console. This change suggests that the original message was not being formatted correctly, which is a bug.

**Buggy**"
pmd,3084.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,25 +1,25 @@
     public void publish(LogRecord logRecord) {
         //Map the log levels from java.util.logging to Ant
         int antLevel;
         Level level = logRecord.getLevel();
         if (level == Level.FINEST)
             antLevel = Project.MSG_DEBUG;   //Shown when -debug is supplied to Ant
         else if (level == Level.FINE || level == Level.FINER || level == Level.CONFIG)
             antLevel = Project.MSG_VERBOSE; //Shown when -verbose is supplied to Ant
         else if (level == Level.INFO)
             antLevel = Project.MSG_INFO;    //Always shown
         else if (level == Level.WARNING)
             antLevel = Project.MSG_WARN;    //Always shown
         else if (level == Level.SEVERE)
             antLevel = Project.MSG_ERR;     //Always shown
         else
             throw new IllegalStateException(""Unknown logging level"");   //shouldn't get ALL or NONE
         
-        antTask.log(logRecord.getMessage(), antLevel);
+        antTask.log(FORMATTER.format(logRecord), antLevel);
         if (logRecord.getThrown() != null) {
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             antTask.log(stringWriter.toString(), antLevel);
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for formatting problems in loggers. The diff shows a change in the `publish` method of a class, where the log message is now formatted using `FORMATTER.format(logRecord)` instead of directly using `logRecord.getMessage()`. This change suggests that the previous formatting of log messages was incorrect or insufficient, and the new formatting addresses this issue. This can be considered a bug fix.

**Buggy**"
pmd,7444.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
-    public boolean isArrayDeference() {
-        return isArrayDeference;
+    public boolean isArrayDereference() {
+        return isArrayDereference;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the `SimplifyConditional` rule in PMD. The bug (1242544) caused the rule to incorrectly flag null checks that precede an `instanceof` check involving an array dereference.

The code diff shows a simple change: renaming the method `isArrayDeference` to `isArrayDereference`. This change, by itself, doesn't seem directly related to the bug described in the commit message. It's more likely a typo fix or a general code cleanup. However, the commit message explicitly mentions fixing a bug where the rule incorrectly flags null checks before `instanceof` checks on array dereferences.

While the code change itself doesn't scream ""bug fix"", the commit message provides the context. The renaming might be part of a larger fix that involved other code changes not included in this diff. The renaming could have been necessary to clarify the intent of the method, which was crucial for correctly implementing the fix for the bug.

Therefore, given the commit message's explicit mention of a bug fix and the context it provides, it's reasonable to assume that this change, even though small, is related to addressing the bug.

**Buggy**"
pmd,7443.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
     public void setIsArrayDereference() {
-        isArrayDeference = true;
+        isArrayDereference = true;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to the `SimplifyConditional` rule in PMD. Specifically, it addresses an issue where null checks preceding an `instanceof` check involving an array dereference were incorrectly flagged.

The provided code diff is very small and seems incomplete. It only shows a single method `setIsArrayDereference` with a minor change: `isArrayDeference = true;`.

Given the limited context, it's difficult to definitively confirm if this specific change directly fixes the bug described in the commit message. However, the method name `setIsArrayDereference` strongly suggests that this change is related to the bug fix. The bug involves array dereferences within `instanceof` checks, and this method likely plays a role in identifying or handling such cases.

Therefore, based on the commit message and the available code snippet, it's reasonable to conclude that the change is part of a bug fix.

**Buggy**"
pmd,9037.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,43 +1,43 @@
     public Object visit(ASTStatementExpression node, Object data) {
         if (node.jjtGetNumChildren() != 3
                 || !(node.jjtGetChild(0) instanceof ASTPrimaryExpression)
                 || !(node.jjtGetChild(1) instanceof ASTAssignmentOperator)
                 || (((ASTAssignmentOperator) (node.jjtGetChild(1))).isCompound())
                 || !(node.jjtGetChild(2) instanceof ASTExpression)
                 || node.jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
                 || node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
         ) {
             return super.visit(node, data);
         }
 
         SimpleNode lhs = (SimpleNode) node.jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(lhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         SimpleNode rhs = (SimpleNode) node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(rhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         if (!lhs.getImage().equals(rhs.getImage())) {
             return super.visit(node, data);
         }
 
         if (lhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = lhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         if (rhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = rhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         addViolation(data, node);
         return super.visit(node, data);
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for a bug in the `SimplifyConditional` rule, specifically related to null checks preceding `instanceof` checks involving array dereferences. The change modifies the conditions under which the rule flags code. The code checks for array dereferences in both the left-hand side (LHS) and right-hand side (RHS) of an assignment. If an array dereference is detected, the rule is skipped. This aligns with the commit message's description of preventing the rule from flagging null checks that precede `instanceof` checks with array dereferences.

**Buggy**"
pmd,8010.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,11 +1,19 @@
     public JavaTypeDefinition getGenericType(final String parameterName) {
-        final TypeVariable<?>[] typeParameters = clazz.getTypeParameters();
-        for (int i = 0; i < typeParameters.length; i++) {
-            if (typeParameters[i].getName().equals(parameterName)) {
-                return getGenericType(i);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            final TypeVariable<?>[] typeParameters = currTypeDef.clazz.getTypeParameters();
+            for (int i = 0; i < typeParameters.length; i++) {
+                if (typeParameters[i].getName().equals(parameterName)) {
+                    return currTypeDef.getGenericType(i);
+                }
             }
         }
 
-        throw new IllegalArgumentException(""No generic parameter by name "" + parameterName
-                                                   + "" on class "" + clazz.getSimpleName());
+        // throw because we could not find parameterName
+        StringBuilder builder = new StringBuilder(""No generic parameter by name "").append(parameterName);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            builder.append(""\n on class "");
+            builder.append(clazz.getSimpleName());
+        }
+
+        throw new IllegalArgumentException(builder.toString());
     }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Java, typedef: fix a bug with nested classes if the outer class is generic"" clearly states that the change is intended to fix a bug related to how generic type parameters are resolved in nested classes. The bug specifically occurs when the outer class is generic.

**Diff Analysis:**

The code modification focuses on the `getGenericType` method. Here's a breakdown of the changes:

1.  **Outer Loop for Enclosing Classes:** The original code only considered the type parameters of the immediate class (`clazz`). The modified code introduces a `for` loop that iterates through the enclosing classes (`currTypeDef.enclosingClass`) to search for the generic parameter. This suggests that the original implementation failed to look up generic parameters in outer classes when dealing with nested classes.

2.  **Contextual `getGenericType(i)` Call:** The call to `getGenericType(i)` is now made on `currTypeDef` instead of `this`. This ensures that the generic type is resolved within the correct class context.

3.  **Improved Error Message:** The exception message is enhanced to include the class hierarchy in which the generic parameter was not found. This helps in debugging and understanding the context of the error.

**Reasoning:**

The changes strongly suggest a bug fix. The original code only considered the type parameters of the immediate class, leading to incorrect resolution of generic types when nested classes were involved and the outer class was generic. The added loop addresses this issue by traversing the enclosing classes to find the generic parameter. The improved error message also indicates that the original error reporting was insufficient, which is a common symptom of a bug. The changes align perfectly with the commit message.

**Conclusion:**

**Buggy**
"
pmd,8270.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,3 +1,3 @@
         public void visitInnerClassType(String name) {
-            parent.parseClassName(name);
+            // parent.parseClassName(name);
         }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"The commit message indicates a bug fix related to nested classes within generic outer classes in Java's typedef functionality. The diff shows a commented-out line `parent.parseClassName(name)` within the `visitInnerClassType` method. This suggests that the original code was incorrectly parsing the class name of inner classes when the outer class was generic. Commenting out this line likely prevents the incorrect parsing, thus fixing the bug. The change aligns with the commit message, indicating a bug fix.

**Buggy**
"
pmd,7974.json,0b90fd01542a5a765cbb72e0b0c90185168d2331,"@@ -1,31 +1,32 @@
     public boolean isOnLeftHandSide() {
         // I detest this method with every atom of my being
         Node primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(
-                    ""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
-                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
+                    ""Found a NameOccurrence ("" + location + "") that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
+                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent()
+                            + "" (location line "" + location.getBeginLine() + "" col "" + location.getBeginColumn() + "")"");
         }
 
         if (isStandAlonePostfix(primaryExpression)) {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         return !isCompoundAssignment(primaryExpression);
     }",Buggy,"[java] Fix processing error with Cast + MethodReference

Occurred in https://github.com/spring-projects/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/support/DefaultListableBeanFactory.java#L746",Buggy,"The commit message indicates a fix related to processing errors involving ""Cast + MethodReference"". The provided code diff modifies the `isOnLeftHandSide()` method.

The original code threw a `RuntimeException` with a message that lacked specific information about the `NameOccurrence` and its location. The modified code enhances the exception message by including the `location` itself, along with its line and column numbers. This makes debugging easier when such an exception occurs.

The change improves error reporting, which is a common characteristic of bug fixes. The original exception message was not informative enough to pinpoint the exact location of the error. The updated message provides more context, aiding in the diagnosis and resolution of the underlying issue.

Therefore, the changes indicate a bug fix.
**Buggy**
"
pmd,7846.json,9a940e0cc57577c730a54adf2582612581cb05af,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""Variable "" + node.getImage() + "":"" + node.getBeginLine();
+        return ""Variable symbol "" + node.getImage() + "" line "" + node.getBeginLine();
     }",Buggy,"fixed bug 660069; thx to mcclain looney for the bug report


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1323 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix (bug 660069) reported by mcclain looney. The code change modifies the `toString()` method of a class, likely for debugging or logging purposes. The change adds ""symbol"" and ""line"" to the string representation, making it more descriptive. This change improves readability and helps in debugging, aligning with the bug fix mentioned in the commit message.

**Buggy**"
pmd,2946.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,4 +1,3 @@
         public String toString() {
-            SimpleNode n = (SimpleNode) node.jjtGetChild(1);
-            return n.getImage();
+            return node.getMethodName();
         }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1235300, specifically addressing an issue where `NullAssignment` was incorrectly flagging assignments to final fields. The provided code diff modifies the `toString()` method. The original implementation extracted the image from a child node, while the modified version directly retrieves the method name from the node. This change suggests a correction in how the node's information is accessed or represented, which could be related to the bug fix described in the commit message. It's plausible that the original code was incorrectly identifying assignments to final fields due to incorrect node information retrieval.

**Buggy**"
pmd,9191.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,15 +1,20 @@
     public Object visit(ASTNullLiteral node, Object data) {
-        if (lookUp(node) instanceof ASTStatementExpression) {
-            Node n = lookUp(node);
+        if (get5thParent(node) instanceof ASTStatementExpression) {
+            ASTStatementExpression n = (ASTStatementExpression)get5thParent(node);
+
+            if (isAssignmentToFinalField(n)) {
+                return data;
+            }
+
             if (n.jjtGetNumChildren() > 2 && n.jjtGetChild(1) instanceof ASTAssignmentOperator) {
                 RuleContext ctx = (RuleContext) data;
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, node));
             }
-        } else if (lookUp2(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp2(node), data, node);
-        } else if (lookUp(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp(node), data, node);
+        } else if (get4thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get4thParent(node), data, node);
+        } else if (get5thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get5thParent(node), data, node);
         }
 
         return data;
     }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields"" clearly indicates a bug fix. The bug was that the `NullAssignment` rule was incorrectly flagging assignments to final fields as violations.

**Code Diff Analysis:**

The code diff modifies the `visit(ASTNullLiteral node, Object data)` method. Let's break down the changes:

1.  **`lookUp` replaced with `get5thParent` and `get4thParent`:** The original code used `lookUp` and `lookUp2` methods to find parent nodes. These have been replaced with `get5thParent` and `get4thParent`. This suggests a change in how the parent nodes are determined, likely to correct the logic for identifying the correct parent expression.

2.  **`isAssignmentToFinalField` check:** A new check `isAssignmentToFinalField(n)` has been introduced. This check is performed when the 5th parent is an `ASTStatementExpression`. If the assignment is to a final field, the method returns early, preventing the rule violation from being reported. This directly addresses the bug described in the commit message.

3.  **Casting to `ASTStatementExpression`:** The code now explicitly casts the parent node to `ASTStatementExpression`. This indicates a more precise type handling, which is often associated with bug fixes.

**Relevance and Bug Fix Identification:**

The code changes directly implement the fix described in the commit message. The `isAssignmentToFinalField` check prevents the `NullAssignment` rule from flagging assignments to final fields. The changes in parent node lookup also likely contribute to the fix by ensuring the correct expression is analyzed.

**Conclusion:**

The commit message and code diff are highly relevant. The changes clearly address the bug of incorrectly flagging assignments to final fields.

**Buggy**
"
pmd,7976.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,3 +1,3 @@
-    private boolean postFixWithExceptions(SimpleNode primaryExpression) {
+    private boolean isStandAlonePostfix(SimpleNode primaryExpression) {
         return primaryExpression instanceof ASTPostfixExpression && primaryExpression.jjtGetParent() instanceof ASTStatementExpression && thirdChildHasDottedName(primaryExpression);
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Fixed a predecrement problem"" clearly indicates a bug fix related to a predecrement operation. However, the diff doesn't show any changes directly related to predecrement or decrement operations. It's renaming a method.

2.  **Code Diff:** The code diff shows a method rename from `postFixWithExceptions` to `isStandAlonePostfix`. The logic within the method appears to remain the same. Method renaming itself isn't usually a bug fix, but it can be part of a larger refactoring effort to address a bug. However, in this case, the renaming doesn't seem to be directly related to the ""predecrement problem"" mentioned in the commit message.

3.  **Relevance:** The commit message mentions a ""predecrement problem,"" but the code diff only shows a method rename. There is no evident connection between the two. It's possible that the actual fix for the predecrement problem was done in a different commit or file, and this method rename is an unrelated change included in the same commit. It's also possible that the commit message is misleading or incomplete.

**Conclusion:**

Based on the available information, the code diff does not directly address the ""predecrement problem"" mentioned in the commit message. The method rename, by itself, doesn't suggest a bug fix. Therefore, I'll conclude that the provided diff doesn't indicate a bug fix.

**NotBuggy**"
pmd,7974.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,32 +1,32 @@
     public boolean isOnLeftHandSide() {
         SimpleNode primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = "" + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
         }
 
-        if (postFixWithExceptions(primaryExpression))  {
+        if (isStandAlonePostfix(primaryExpression))  {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         if (isCompoundAssignment(primaryExpression)) {
             return false;
         }
 
         return true;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""Fixed a predecrement problem"" suggests a bug fix related to a predecrement operation. The code diff modifies the `isOnLeftHandSide` method, specifically renaming `postFixWithExceptions` to `isStandAlonePostfix`. While the diff doesn't explicitly show a predecrement operation being fixed, the change in method name suggests a potential refinement in how postfix expressions (which can include predecrement) are handled. The original method name `postFixWithExceptions` is vague, while `isStandAlonePostfix` is more descriptive and suggests a more precise check for standalone postfix expressions. This could be related to fixing a bug where a predecrement operation was incorrectly identified as being on the left-hand side of an assignment.

**Buggy**
"
pmd,8888.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,30 +1,30 @@
     private int initializedInConstructor(List usages, Set allConstructors) {
         int rc = MUTABLE, methodInitCount = 0;
         boolean foundUsage = false;
         Set consSet = new HashSet();
-        
+
         for (Iterator j = usages.iterator(); j.hasNext();) {
             foundUsage = true;
             NameOccurrence occ = (NameOccurrence)j.next();
-            if (occ.isOnLeftHandSide()) {
+            if (occ.isOnLeftHandSide() || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreDecrementExpression || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreIncrementExpression) {
                 SimpleNode node = occ.getLocation();
                 SimpleNode constructor = (SimpleNode)node.getFirstParentOfType(ASTConstructorDeclaration.class);
                 if (constructor != null) {
                     consSet.add(constructor);
                 } else {
                     if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
                         methodInitCount++;
                     }
                 }
             }
         }
         if (!foundUsage || ((methodInitCount == 0) && consSet.isEmpty())) {
             rc = CHECKDECL;
         } else {
             allConstructors.removeAll(consSet);
             if (allConstructors.isEmpty() && (methodInitCount == 0)) {
                 rc = IMMUTABLE;
             }
         }
         return rc;
     }",Buggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message ""Fixed a predecrement problem"" suggests a bug fix related to predecrement operations.

The code diff modifies the condition `occ.isOnLeftHandSide()` to include checks for `ASTPreDecrementExpression` and `ASTPreIncrementExpression`. This change suggests that the original condition was not correctly identifying variable usages within predecrement expressions, leading to incorrect analysis. The added checks ensure that occurrences within predecrement and preincrement expressions are also considered as modifications.

The modification directly addresses the issue described in the commit message, indicating a bug fix.

**Buggy**"
pmd,8892.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,6 +1,5 @@
     private Set findAllConstructors(ASTUnmodifiedClassDeclaration node) {
-        List results = node.findChildrenOfType(ASTConstructorDeclaration.class);
-        HashSet set = new HashSet();
-        set.addAll(results);
+        Set set = new HashSet();
+        set.addAll(node.findChildrenOfType(ASTConstructorDeclaration.class));
         return set;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message ""Fixed a predecrement problem"" suggests a bug fix related to a predecrement operation. However, the provided code diff doesn't show any predecrement operations or any arithmetic operations at all. The change involves simplifying the code by directly using a `Set` instead of creating a `List` and then adding all elements to the `Set`. This change is more of a code cleanup or refactoring than a bug fix related to a predecrement operation. Therefore, the commit message and the code change do not align.

**NotBuggy**"
pmd,6536.json,1942e94cec6b6a30532716d14da5c7cd0cd8f075,"@@ -1,15 +1,15 @@
     private boolean isUnbalanced(String image, String pattern) {
         int occurance = 0;
-        int index = image.lastIndexOf(""="");
+        int index = image.indexOf(""="");
         index = image.indexOf(pattern, index + 1);
         while (index >= 0) {
             occurance++;
             index = image.indexOf(pattern, index + 1);
         }
 
         if ((occurance % 2) != 0) {
             return true;
         }
 
         return false;
     }",Buggy,Bug fix,Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message is simply ""Bug fix,"" which is not very descriptive but suggests that the change intends to correct a bug.

2.  **Code Diff Analysis:**
    *   The code modifies the `isUnbalanced` method.
    *   The core change is from `image.lastIndexOf(""="")` to `image.indexOf(""="")`. This is a significant change in logic. `lastIndexOf` finds the last occurrence of ""="", while `indexOf` finds the first occurrence.
    *   The original code was likely intended to find occurrences of `pattern` after the *last* ""="". The corrected code now finds occurrences of `pattern` after the *first* ""="".
    *   The rest of the code related to counting occurrences and checking for an odd number of occurrences remains the same.

3.  **Bug Fix Indication:**
    *   The change from `lastIndexOf` to `indexOf` strongly suggests a correction of a flawed algorithm. The original algorithm was likely not behaving as intended, and this change fixes that.
    *   The method name `isUnbalanced` implies that it's checking for some kind of imbalance based on the occurrences of a pattern. The change in how the starting index is determined would directly affect the result of this check.

**Conclusion:**

The code change, combined with the commit message ""Bug fix,"" strongly indicates that the change corrects a bug in the logic of the `isUnbalanced` method. The use of `lastIndexOf` was likely incorrect, and `indexOf` provides the intended behavior.

**Buggy**"
pmd,358.json,6156ba5de5f004b96bceb0cc8e44191c6876c7c9,"@@ -1,23 +1,23 @@
     private void validate() throws BuildException {
         if (formatters.isEmpty() && !printToConsole) {
             throw new BuildException(""No formatter specified; and printToConsole was false"");
         }
 
         for (Iterator i = formatters.iterator(); i.hasNext();) {
             Formatter f = (Formatter) i.next();
-            if (f.isToFileNull()) {
+            if (f.isNoOutputSupplied()) {
                 throw new BuildException(""Formatter toFile attribute is required"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', or '1.5'"");
         }
     }",Buggy,"Applied a patch from Wouter Zelle to clean up the Ant Formatter class, fix a TextRenderer bug, and make toConsole cleaner.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4292 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a cleanup of the Ant Formatter class and a fix for a TextRenderer bug. The provided diff modifies the `validate()` method in what appears to be the Ant Formatter class. Specifically, it changes the condition `f.isToFileNull()` to `f.isNoOutputSupplied()`. This suggests a bug fix related to how the formatter handles output destinations. The original condition might have been incorrectly checking for null file outputs, while the corrected condition `f.isNoOutputSupplied()` likely provides a more accurate check for whether any output destination is specified for the formatter. This change directly relates to the ""clean up the Ant Formatter class"" part of the commit message and could be considered a bug fix if the previous condition was faulty.

**Buggy**
"
pmd,412.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,12 +1,10 @@
     private void logRulesUsed(RuleSets rules) {
         log(""Using these rulesets: "" + ruleSetFiles, Project.MSG_VERBOSE);
 
         RuleSet[] ruleSets = rules.getAllRuleSets();
-        for (int j = 0; j < ruleSets.length; j++) {
-            RuleSet ruleSet = ruleSets[j];
-
+        for (RuleSet ruleSet : ruleSets) {
             for (Rule rule: ruleSet.getRules()) {
                 log(""Using rule "" + rule.getName(), Project.MSG_VERBOSE);
             }
         }
     }",NotBuggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a bug fix related to the `<ruleset>` path in an Ant task. The path should be relative to the Ant basedir. The provided code diff modifies the `logRulesUsed` method. This method logs the rulesets and rules being used. The change replaces a traditional `for` loop with an enhanced `for` loop (for-each loop) to iterate through the `ruleSets` array. This change itself doesn't seem directly related to the bug described in the commit message (relative path resolution). It's a minor code style improvement but not a bug fix. Therefore, the code change doesn't directly address the bug mentioned in the commit message.

**NotBuggy**"
pmd,358.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,19 +1,31 @@
     private void validate() throws BuildException {
         // TODO - check for empty Formatters List here?
         for (Formatter f: formatters) {
             if (f.isNoOutputSupplied()) {
                 throw new BuildException(""toFile or toConsole needs to be specified in Formatter"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
+        // convert relative paths and substitute env variables/properties
+        final StringBuffer sb = new StringBuffer();
+        for(String s: ruleSetFiles.split("","")) {
+            Path p = new Path(getProject());
+            p.setPath(getProject().replaceProperties(s));
+            if (sb.length() > 0) {
+                sb.append(',');
+            }
+            sb.append(p);
+        }
+        ruleSetFiles = sb.toString();
+
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"") && !targetJDK.equals(""1.6"") && !targetJDK.equals(""1.7"") && !targetJDK.equals(""jsp"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', '1.5', '1.6', '1.7' or 'jsp'"");
         }
     }",Buggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix for bug 1943204, specifically addressing an issue where the `<ruleset>` path in the Ant task should be relative to the Ant basedir.

The code diff modifies the `validate()` method. It introduces a loop that iterates through the `ruleSetFiles` string (which is assumed to be comma-separated), converts each path to be relative to the Ant project's base directory, and then reconstructs the `ruleSetFiles` string with the updated paths. This is done using `Path` and `getProject().replaceProperties(s)`.

The code change directly addresses the issue described in the commit message. The original code likely treated the `ruleSetFiles` paths as absolute or relative to some other location, causing problems when the Ant task was executed from different directories. The new code ensures that the paths are always interpreted relative to the Ant project's base directory, resolving the bug.

Therefore, the changes indicate a bug fix.
**Buggy**
"
pmd,6753.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,8 +1,10 @@
     public Object visit(ASTPrimaryExpression node, Object data) {
-        if (isForeignAttributeOrMethod(node) && (isAttributeAccess(node)
-            || isMethodCall(node) && isForeignGetterSetterCall(node))) {
-
-            ((MutableInt) data).increment();
+        if (isForeignAttributeOrMethod(node)) {
+            if (isAttributeAccess(node)) {
+                ((MutableInt) data).increment();
+            } else {
+                ((MutableInt) data).add(countForeignGetterSetterCalls(node));
+            }
         }
         return super.visit(node, data);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message ""[java] ATFD calculation problem"" suggests that there was an issue in calculating the ATFD (Access To Foreign Data) metric. The message also mentions that method call chains are now considered, implying that the previous calculation was incomplete or incorrect. The ""Fixes #1910"" part further confirms that this commit addresses a reported bug or issue.

**2. Analyzing the Code Diff:**

The code diff is within the `visit` method, which likely traverses an Abstract Syntax Tree (AST) to analyze Java code. The core change lies in how foreign attribute and method accesses are counted.

*   **Original Code:** The original code only incremented the counter (`((MutableInt) data).increment();`) if it was a foreign attribute or method and also either an attribute access or a foreign getter/setter call. This implies that method call chains were not being fully considered.
*   **Modified Code:** The modified code changes the logic to specifically handle attribute accesses and other method calls separately. If it's an attribute access, it increments the counter by one. If it's another method call, it calls `countForeignGetterSetterCalls(node)` and adds the result to the counter. This indicates that the `countForeignGetterSetterCalls` method likely handles the logic for counting method call chains, which was previously missing.

**3. Reasoning:**

The commit message and code diff strongly suggest a bug fix. The original code was not correctly calculating the ATFD metric because it wasn't considering method call chains. The modified code addresses this by explicitly counting foreign getter/setter calls within method call chains using the `countForeignGetterSetterCalls` method. This aligns with the commit message's claim of fixing the ATFD calculation problem and considering method call chains. The fact that the issue is tracked (""Fixes #1910"") further supports the conclusion that this is a bug fix.

**Conclusion:**

**Buggy**
"
pmd,6754.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,6 +1,4 @@
     private boolean isForeignGetterSetterCall(ASTPrimaryExpression node) {
-
         String methodOrAttributeName = getMethodOrAttributeName(node);
-
-        return methodOrAttributeName != null && StringUtils.startsWithAny(methodOrAttributeName, ""get"", ""is"", ""set"");
+        return isForeignGetterSetterCall(methodOrAttributeName);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message ""[java] ATFD calculation problem"" clearly states that there was an issue with the calculation of ATFD (Access To Foreign Data). The message also mentions that method call chains are now considered, suggesting that the previous calculation was incomplete or incorrect because it didn't account for chained method calls. The ""Fixes #1910"" part further confirms that this commit addresses a reported bug.

**2. Analyzing the Code Diff:**

The code diff shows a change in the `isForeignGetterSetterCall` method. The original implementation directly checked if the method or attribute name starts with ""get"", ""is"", or ""set"". The modified version delegates the check to another `isForeignGetterSetterCall` method, which likely handles the method name differently.

**3. Reasoning and Conclusion:**

The original code likely failed to identify foreign getter/setter calls when they were part of a method call chain (e.g., `object.getSomething().getValue()`). The change suggests that the new `isForeignGetterSetterCall` method (the one being called) now correctly identifies such calls, thus fixing the ATFD calculation problem. The commit message explicitly states that method call chains are now considered, which aligns perfectly with the code change. This indicates that the previous implementation had a bug in how it calculated ATFD.

**Buggy**
"
hbase,27505.json,d589b7238257c78dc8616e47ab5bb3e2309a5237,"@@ -1,3 +1,3 @@
-    public Throwable getCause() {
+    public synchronized Throwable getCause() {
       return cause;
     }",Buggy,"HBASE-19847 Fix findbugs and error-prone warnings in hbase-thrift (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>",Buggy,"The commit message indicates a fix for findbugs and error-prone warnings in the hbase-thrift module. The provided code diff shows a change in the `getCause()` method of a class, adding the `synchronized` keyword. This change likely addresses a potential thread-safety issue identified by findbugs or error-prone, where multiple threads could access and modify the `cause` field concurrently, leading to data corruption or unexpected behavior. This is a bug fix.

**Buggy**"
hbase,7228.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,8 +1,8 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (now < nextRefillTime) {
       return 0;
     }
     nextRefillTime = now + super.getTimeUnitInMillis();
     return limit;
   }",NotBuggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""HBASE-13888 Fix refill bug from HBASE-13686"" strongly suggests that this commit is intended to address a bug introduced in a previous commit (HBASE-13686). The phrase ""Fix refill bug"" is a clear indicator.

2.  **Code Diff:** The code diff shows a change in the `refill` method signature. The original method `refill(long limit, long available)` takes two arguments, while the modified method `refill(long limit)` only takes one argument. The `available` parameter has been removed. Also, the `available` parameter is not used in the original code.

3.  **Reasoning:**
    *   The commit message explicitly states that this commit fixes a bug.
    *   The change in the method signature suggests that the original method signature might have been incorrect or unnecessary, potentially leading to incorrect behavior. Removing the `available` parameter suggests it was redundant or causing issues.
    *   The removal of the `available` parameter without any usage indicates a correction of an oversight or a potential source of error.

**Conclusion:**

Based on the commit message and the code diff, it is highly likely that this commit addresses a bug related to the `refill` method. The change in the method signature and the removal of an unused parameter point towards a correction of an error or an improvement in the code's correctness.

**Buggy**
"
hbase,7525.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,16 +1,16 @@
   public synchronized boolean canExecute(final long amount) {
-    long refillAmount = refill(limit, avail);
+    long refillAmount = refill(limit);
     if (refillAmount == 0 && avail < amount) {
       return false;
     }
     // check for positive overflow
     if (avail <= Long.MAX_VALUE - refillAmount) {
       avail = Math.max(0, Math.min(avail + refillAmount, limit));
     } else {
       avail = Math.max(0, limit);
     }
     if (avail >= amount) {
       return true;
     }
     return false;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang)"" explicitly states that this commit is a bug fix related to a previous commit (HBASE-13686) and involves a ""refill bug."" This strongly suggests that the changes are intended to correct incorrect behavior.

**Code Diff Analysis:**

The code diff shows a change in the `canExecute` method. Specifically, the `refill` method call is modified.

-   **Original Code:** `long refillAmount = refill(limit, avail);`
-   **Modified Code:** `long refillAmount = refill(limit);`

The `refill` method call previously took two arguments (`limit` and `avail`), but now it only takes one (`limit`). This indicates a change in the method signature or how the refill amount is calculated. The removal of `avail` as an argument suggests that the previous implementation might have been using the available amount incorrectly in the refill calculation, potentially leading to a bug.

**Reasoning:**

1.  **Explicit Bug Fix:** The commit message explicitly mentions a bug fix related to a ""refill bug.""
2.  **Method Signature Change:** The `refill` method call's argument list has been reduced, suggesting a change in the refill logic.
3.  **Potential Incorrect Usage:** The removal of the `avail` argument from the `refill` method call indicates that the previous implementation might have been using the available amount incorrectly in the refill calculation.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix. The modification to the `refill` method call strongly suggests that the previous implementation had a flaw in its refill logic.

**Buggy**"
hbase,7389.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,15 +1,15 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (nextRefillTime == -1) {
       // Till now no resource has been consumed.
       nextRefillTime = EnvironmentEdgeManager.currentTime();
       return limit;
     }
 
     long delta = (limit * (now - nextRefillTime)) / super.getTimeUnitInMillis();
     if (delta > 0) {
       this.nextRefillTime = now;
-      return Math.min(limit, available + delta);
+      return Math.min(limit, delta);
     }
     return 0;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang)"" explicitly states that this commit is a bug fix related to a previous commit (HBASE-13686) and involves a ""refill bug."" This strongly suggests that the code changes are intended to correct faulty behavior.

**Code Diff Analysis:**

The code diff modifies the `refill` method. The key changes are:

1.  **Parameter List:** The `refill` method's signature has changed from `refill(long limit, long available)` to `refill(long limit)`. The `available` parameter has been removed.
2.  **Return Value Calculation:** The return value calculation has changed from `Math.min(limit, available + delta)` to `Math.min(limit, delta)`. The `available` variable is no longer added to `delta`.

**Reasoning:**

The removal of the `available` parameter and its subsequent removal from the return value calculation strongly suggests a correction of a logical error. The original code was likely incorrectly adding the `available` amount to the `delta` when refilling, potentially leading to an overestimation of the available resource. The commit message confirms this interpretation. The removal of the `available` parameter suggests that it was either unused or, more likely, used incorrectly. The change in the return statement confirms that the logic for refilling was flawed and is now corrected.

**Conclusion:**

The commit message explicitly indicates a bug fix, and the code diff demonstrates a change in the refill logic, which likely corrects an error in resource calculation. Therefore, the changes indicate a bug fix.

**Buggy**"
hbase,1189.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,5 +1,4 @@
   public static void main(String[] args) throws Exception {
-    if (conf == null) conf = HBaseConfiguration.create();
-    int ret = ToolRunner.run(conf, new ReplicationSyncUp(), args);
+    int ret = ToolRunner.run(HBaseConfiguration.create(), new ReplicationSyncUp(), args);
     System.exit(ret);
   }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,"The commit message ""HBASE-20083 Fix findbugs error for ReplicationSyncUp"" indicates that the purpose of the commit is to fix a FindBugs error in the `ReplicationSyncUp` class.

The provided code diff shows a change in the `main` method of what appears to be the same `ReplicationSyncUp` class. Specifically, the lines:

```java
if (conf == null) conf = HBaseConfiguration.create();
int ret = ToolRunner.run(conf, new ReplicationSyncUp(), args);
```

are replaced with:

```java
int ret = ToolRunner.run(HBaseConfiguration.create(), new ReplicationSyncUp(), args);
```

The original code checks if `conf` is null and initializes it if it is. The modified code directly initializes `conf` using `HBaseConfiguration.create()` within the `ToolRunner.run()` method call. This change eliminates the potential for `conf` to be null when passed to `ToolRunner.run()`.

This change likely addresses a potential null pointer exception or an uninitialized variable issue, which FindBugs could have flagged. The commit message accurately reflects the code change, indicating a fix for a FindBugs error.

Therefore, the changes indicate a bug fix.

**Buggy**"
hbase,1193.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,3 +1,3 @@
     public Configuration getConfiguration() {
-      return conf;
+      return getConf();
     }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-20083 Fix findbugs error for ReplicationSyncUp"" clearly states that the purpose of the commit is to address a FindBugs error in the `ReplicationSyncUp` class. FindBugs (and similar static analysis tools) identify potential bugs or code quality issues. Therefore, the commit message strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `getConfiguration()` method:

`-      return conf;`
`+      return getConf();`

This change replaces direct access to a field named `conf` with a call to a method `getConf()`. This pattern is very common when addressing potential issues related to visibility, thread safety, or lazy initialization.  It's highly likely that FindBugs flagged the direct field access as a potential issue (e.g., if `conf` wasn't properly initialized or if concurrent access could lead to problems).  The change to use `getConf()` is a standard way to encapsulate the field and ensure proper access.

**Reasoning:**

1.  **Explicit Bug Fix:** The commit message explicitly mentions fixing a FindBugs error, which implies a bug fix.
2.  **Code Change Pattern:** The change from direct field access to using a getter method is a common pattern for addressing potential issues identified by static analysis tools like FindBugs. This suggests that the original code had a potential flaw that the getter addresses.

**Conclusion:**

Based on the commit message and the code change, it's highly probable that this commit addresses a bug or potential issue identified by FindBugs. Therefore, the change indicates a bug fix.

**Buggy**
"
hbase,5737.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,7 +1,10 @@
   private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {
     // If meta, move it last otherwise other unassigns fail because meta is not
     // online for them to update state in. This is dodgy. Needs to be made more
     // robust. See TODO below.
-    if (hri.isMetaRegion()) regions.addLast(hri);
-    else regions.addFirst(hri);
+    if (hri.isMetaRegion()) {
+      regions.addLast(hri);
+    } else {
+      regions.addFirst(hri);
+    }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors and enabling Checkstyle to fail on violations. The provided code diff shows a minor change in the `addRegion` method, specifically adding curly braces to the `if` and `else` blocks. This change improves code readability and adheres to Checkstyle guidelines, but it doesn't directly address a bug or logical error in the code. The change is purely stylistic to satisfy Checkstyle requirements. Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
hbase,5705.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,8 +1,11 @@
   static Set<Address> getOnlineServers(final MasterServices master) {
     Set<Address> onlineServers = new HashSet<Address>();
-    if (master == null) return onlineServers;
+    if (master == null) {
+      return onlineServers;
+    }
+
     for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {
       onlineServers.add(server.getAddress());
     }
     return onlineServers;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations"" clearly states that the purpose of the commit is to fix Checkstyle errors and enforce Checkstyle rules. This suggests a focus on code style and quality rather than functional correctness.

2.  **Git Diff:** The diff shows a minor change in the `getOnlineServers` method. Specifically, it adds curly braces `{}` around the `return` statement within the `if (master == null)` condition.

3.  **Relevance:** The code change directly addresses the commit message. Adding curly braces is a common Checkstyle requirement for single-line `if` statements. This change doesn't alter the logic or functionality of the code; it simply improves code style to comply with Checkstyle rules.

4.  **Bug Fix Indication:** This change does not indicate a bug fix. It's purely a code style improvement. There's no error handling update, logical correction, or exception handling improvement. The code's behavior remains the same.

**Conclusion:**

**NotBuggy**
"
hbase,5782.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,4 +1,7 @@
   public boolean isOnline() {
-    if (this.rsGroupInfoManager == null) return false;
+    if (this.rsGroupInfoManager == null) {
+      return false;
+    }
+
     return this.rsGroupInfoManager.isOnline();
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors and enabling Checkstyle to fail on violations. The provided code diff shows a minor formatting change, specifically adding curly braces to an `if` statement. This change aligns with the commit message's focus on code style improvements. There's no indication of a bug fix in the logic or error handling.

**NotBuggy**"
hbase,5736.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,16 +1,19 @@
   private List<RegionInfo> getRegions(final Address server) {
     LinkedList<RegionInfo> regions = new LinkedList<>();
     for (Map.Entry<RegionInfo, ServerName> el :
         master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {
-      if (el.getValue() == null) continue;
+      if (el.getValue() == null) {
+        continue;
+      }
+
       if (el.getValue().getAddress().equals(server)) {
         addRegion(regions, el.getKey());
       }
     }
     for (RegionStateNode state : master.getAssignmentManager().getRegionsInTransition()) {
       if (state.getRegionLocation().getAddress().equals(server)) {
         addRegion(regions, state.getRegionInfo());
       }
     }
     return regions;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors and enabling Checkstyle to fail on violations. The provided diff doesn't show any code changes related to bug fixes or logic improvements. The changes are purely formatting-related, adding braces to a single-line `if` statement, which aligns with Checkstyle requirements. Therefore, this commit is not related to a bug fix.

**NotBuggy**"
hbase,5743.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,58 +1,61 @@
   public boolean balanceRSGroup(String groupName) throws IOException {
     ServerManager serverManager = master.getServerManager();
     AssignmentManager assignmentManager = master.getAssignmentManager();
     LoadBalancer balancer = master.getLoadBalancer();
 
     synchronized (balancer) {
       // If balance not true, don't run balancer.
-      if (!((HMaster) master).isBalancerOn()) return false;
+      if (!((HMaster) master).isBalancerOn()) {
+        return false;
+      }
+
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().preBalanceRSGroup(groupName);
       }
       if (getRSGroupInfo(groupName) == null) {
         throw new ConstraintException(""RSGroup does not exist: ""+groupName);
       }
       // Only allow one balance run at at time.
       Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);
       if (groupRIT.size() > 0) {
         LOG.debug(""Not running balancer because "" + groupRIT.size() + "" region(s) in transition: "" +
           StringUtils.abbreviate(
               master.getAssignmentManager().getRegionStates().getRegionsInTransition().toString(),
               256));
         return false;
       }
       if (serverManager.areDeadServersInProgress()) {
         LOG.debug(""Not running balancer because processing dead regionserver(s): "" +
             serverManager.getDeadServers());
         return false;
       }
 
       //We balance per group instead of per table
       List<RegionPlan> plans = new ArrayList<>();
       for(Map.Entry<TableName, Map<ServerName, List<RegionInfo>>> tableMap:
           getRSGroupAssignmentsByTable(groupName).entrySet()) {
         LOG.info(""Creating partial plan for table "" + tableMap.getKey() + "": ""
             + tableMap.getValue());
         List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());
         LOG.info(""Partial plan for table "" + tableMap.getKey() + "": "" + partialPlans);
         if (partialPlans != null) {
           plans.addAll(partialPlans);
         }
       }
       long startTime = System.currentTimeMillis();
       boolean balancerRan = !plans.isEmpty();
       if (balancerRan) {
         LOG.info(""RSGroup balance "" + groupName + "" starting with plan count: "" + plans.size());
         for (RegionPlan plan: plans) {
           LOG.info(""balance "" + plan);
           assignmentManager.moveAsync(plan);
         }
         LOG.info(""RSGroup balance "" + groupName + "" completed after "" +
             (System.currentTimeMillis()-startTime) + "" seconds"");
       }
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().postBalanceRSGroup(groupName, balancerRan);
       }
       return balancerRan;
     }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates a fix for Checkstyle errors and enabling Checkstyle to fail on violations. The diff shows minor code formatting changes, specifically adding curly braces to a single-line `if` statement. This change improves code readability and maintainability, aligning with Checkstyle rules. There are no error handling updates, logical corrections or exception handling improvements.

**NotBuggy**"
hbase,3328.json,d272ac908ceb4696e05431066ae02d953fa6fc9d,"@@ -1,30 +1,30 @@
   protected void chore() {
-    if (!connected) {
+    if (!isConnected()) {
       return;
     }
 
     List<ServerName> sns = generateDeadServersListToSend();
     if (sns.isEmpty()) {
       // Nothing to send. Done.
       return;
     }
 
     final long curTime = EnvironmentEdgeManager.currentTime();
     if (lastMessageTime > curTime - messagePeriod) {
       // We already sent something less than 10 second ago. Done.
       return;
     }
 
     // Ok, we're going to send something then.
     lastMessageTime = curTime;
 
     // We're reusing an existing protobuf message, but we don't send everything.
     // This could be extended in the future, for example if we want to send stuff like the
     //  hbase:meta server name.
     publisher.publish(ClusterMetricsBuilder.newBuilder()
       .setHBaseVersion(VersionInfo.getVersion())
       .setClusterId(master.getMasterFileSystem().getClusterId().toString())
       .setMasterName(master.getServerName())
       .setDeadServerNames(sns)
       .build());
   }",NotBuggy,"HBASE-20069 fix existing findbugs errors in hbase-server; ADDENDUM Address review
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Commit Message Analysis:**

The commit message ""HBASE-20069 fix existing findbugs errors in hbase-server; ADDENDUM Address review"" indicates that the primary goal is to resolve FindBugs errors within the `hbase-server` module. The ""ADDENDUM Address review"" part suggests that the changes also incorporate feedback from a code review.

**Code Diff Analysis:**

The code diff shows a change in the `chore()` method. Specifically, `!connected` is changed to `!isConnected()`.

*   **`!connected` vs. `!isConnected()`:** This change suggests that the original code directly accessed a boolean field named `connected`. The updated code uses a method `isConnected()`. This is a common pattern for encapsulation and potentially thread-safe access to the connection status.

**Reasoning:**

1.  **FindBugs and Encapsulation:** FindBugs often flags direct access to mutable fields as potential bugs, especially in multi-threaded environments. Replacing direct field access with a getter method (`isConnected()`) is a standard way to address this type of FindBugs warning.

2.  **Bug Fix Implication:** While not a direct bug fix in terms of incorrect logic, the change improves code quality and reduces the risk of future bugs related to inconsistent or unsynchronized access to the connection status. It addresses a potential concurrency issue.

**Conclusion:**

The change addresses a potential bug related to direct field access and improves code maintainability. Therefore, I will classify this as a bug fix.

**Buggy**"
hbase,36406.json,aeffca497bf36ea12f89a5f92d2f918b010741fc,"@@ -1,6 +1,6 @@
   public void writeTo(Object object, Class<?> type, Type genericType,
       Annotation[] annotations, MediaType mediaType,
       MultivaluedMap<String, Object> httpHeaders, OutputStream outStream)
       throws IOException, WebApplicationException {
-    outStream.write(object.toString().getBytes());
+    outStream.write(Bytes.toBytes(object.toString()));
   }",Buggy,"HBASE-19846 Fix findbugs and error-prone warnings in hbase-rest (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""HBASE-19846 Fix findbugs and error-prone warnings in hbase-rest (branch-2)"" indicates that the primary goal of this commit is to address static analysis warnings (FindBugs and Error-Prone). This suggests a focus on code quality and potential bug prevention rather than a direct bug fix.

2.  **Code Diff:** The code diff shows a change in the `writeTo` method of a class related to REST functionality. Specifically, it replaces `object.toString().getBytes()` with `Bytes.toBytes(object.toString())`.

3.  **Reasoning:**
    *   The original code `object.toString().getBytes()` uses the platform's default charset to convert the string to bytes. This can lead to inconsistencies and potential data corruption if the client and server have different default charsets.
    *   The corrected code `Bytes.toBytes(object.toString())` likely uses a specific, well-defined charset (most likely UTF-8) to ensure consistent byte representation across different environments. This is a common practice in systems like HBase where data integrity is crucial.
    *   The change addresses a potential character encoding issue, which can be considered a bug. Although it might not be a critical bug causing immediate failures, it's a potential source of data corruption or misinterpretation, especially in distributed systems. The use of `Bytes.toBytes` is indicative of a fix related to byte representation, which is a common source of bugs.

**Conclusion:**

The change addresses a potential bug related to character encoding. While the commit message focuses on static analysis warnings, the code modification itself indicates a fix for a potential data corruption issue.

**Buggy**"
hbase,34490.json,2413a10e6718afaf74185078f0320f2e9e47d273,"@@ -1,47 +1,48 @@
   public List<InputSplit> getSplits(JobContext context) throws IOException {
     boolean closeOnFinish = false;
 
     // Just in case a subclass is relying on JobConfigurable magic.
     if (table == null) {
       initialize(context);
       closeOnFinish = true;
     }
 
     // null check in case our child overrides getTable to not throw.
     try {
       if (getTable() == null) {
         // initialize() must not have been implemented in the subclass.
         throw new IOException(INITIALIZATION_ERROR);
       }
     } catch (IllegalStateException exception) {
       throw new IOException(INITIALIZATION_ERROR, exception);
     }
 
     try {
       List<InputSplit> splits = oneInputSplitPerRegion();
 
       // set same number of mappers for each region
       if (context.getConfiguration().get(NUM_MAPPERS_PER_REGION) != null) {
         int nSplitsPerRegion = context.getConfiguration().getInt(NUM_MAPPERS_PER_REGION, 1);
         List<InputSplit> res = new ArrayList<>();
         for (int i = 0; i < splits.size(); i++) {
           List<InputSplit> tmp = createNInputSplitsUniform(splits.get(i), nSplitsPerRegion);
           res.addAll(tmp);
         }
         return res;
       }
 
       //The default value of ""hbase.mapreduce.input.autobalance"" is false.
-      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false) {
-        long maxAveRegionSize = context.getConfiguration().getInt(MAX_AVERAGE_REGION_SIZE, 8*1073741824);
+      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false)) {
+        long maxAveRegionSize = context.getConfiguration()
+            .getLong(MAX_AVERAGE_REGION_SIZE, 8L*1073741824); //8GB
         return calculateAutoBalancedSplits(splits, maxAveRegionSize);
       }
 
       // return one mapper per region
       return splits;
     } finally {
       if (closeOnFinish) {
         closeTable();
       }
     }
   }",NotBuggy,"HBASE-19195 error-prone fixes for client, mr, and server
",Buggy,"The commit message indicates that the changes are related to ""error-prone fixes"". Let's analyze the diff to see if it aligns with this message.

The code changes involve a few modifications:

1.  **Exception Handling:** The code now catches `IllegalStateException` when calling `getTable()` and wraps it in an `IOException` with a specific error message. This suggests a potential issue where `getTable()` might throw an `IllegalStateException` under certain circumstances, and the code is now handling it more gracefully.
2.  **Configuration Property:** The condition `context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false` has been simplified to `context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false)`. While seemingly trivial, this change could be related to an error-prone finding that flagged the redundant comparison.
3.  **Configuration Property (Long):** The code now uses `getLong` instead of `getInt` for the configuration property `MAX_AVERAGE_REGION_SIZE` and also appends `L` to the default value. This indicates a potential issue where the value might exceed the range of an integer, leading to incorrect behavior.

Based on these observations, the changes appear to be addressing potential errors and improving the robustness of the code. The exception handling, the simplification of the boolean condition, and the use of `getLong` for a potentially large value all suggest that the changes are related to fixing or preventing bugs.

Therefore, the changes indicate a bug fix.

**Buggy**"
hbase,5669.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,7 +1,6 @@
   public static void resetCounters() throws Exception {
     Class<?> cl = (new SplitLogCounters()).getClass();
-    Field[] flds = cl.getDeclaredFields();
-    for (Field fld : flds) {
+    for (Field fld : cl.getDeclaredFields()) {
       ((AtomicLong)fld.get(null)).set(0);
     }
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an effort to analyze and fix FindBugs reports. The provided diff shows a minor change in the `resetCounters` method. Specifically, the code iterates through the declared fields of the `SplitLogCounters` class and resets `AtomicLong` fields to zero. The change involves simplifying the loop by directly iterating over `cl.getDeclaredFields()` instead of first assigning it to a local variable. This change doesn't seem to directly address a bug, but rather improves code readability or style, potentially as a result of FindBugs analysis. Therefore, it's unlikely to be a bug fix.

**NotBuggy**"
hbase,10177.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,6 +1,5 @@
   public void start() {
     worker = new Thread(null, this, ""SplitLogWorker-"" + serverName);
     exitWorker = false;
     worker.start();
-    return;
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an effort to analyze and fix FindBugs reports, and to add invalid bugs to an exclusion filter. The provided diff shows a single change: the removal of a `return` statement at the end of the `start()` method.

Reasoning:

1.  **Relevance to Commit Message:** The removal of the `return` statement doesn't directly relate to analyzing FindBugs reports or adding exclusions. It's a very minor code cleanup.
2.  **Bug Fix Indicators:** The removal of a `return` statement at the end of a `void` method is often a stylistic change or a removal of redundant code. It doesn't inherently suggest a bug fix. The `return` statement in a `void` method serves no purpose if it's the last statement in the method.

Conclusion: **NotBuggy**"
hbase,28329.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,8 +1,11 @@
-  public Get setTimeStamp(long timestamp) {
+  public Get setTimeStamp(long timestamp)
+  throws IOException {
     try {
       tr = new TimeRange(timestamp, timestamp+1);
     } catch(IOException e) {
-      // Will never happen
+      // This should never happen, unless integer overflow or something extremely wrong...
+      LOG.error(""TimeRange failed, likely caused by integer overflow. "", e);
+      throw e;
     }
     return this;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)"" explicitly states that the commit addresses potential bugs within exception handlers. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `setTimeStamp` method in the `Get` class.  Specifically, the `catch` block for the `IOException` that might be thrown by the `TimeRange` constructor has been updated.

Original code:

```java
catch(IOException e) {
  // Will never happen
}
```

Modified code:

```java
catch(IOException e) {
  // This should never happen, unless integer overflow or something extremely wrong...
  LOG.error(""TimeRange failed, likely caused by integer overflow. "", e);
  throw e;
}
```

The original code had an empty `catch` block with a comment indicating that the exception ""will never happen."" This is generally bad practice because exceptions *can* happen, and ignoring them can lead to unexpected behavior or silent failures.

The modified code improves the exception handling by:

1.  Logging an error message using `LOG.error` to provide information about the potential cause of the exception (integer overflow).
2.  Re-throwing the exception using `throw e;`. This allows the calling code to handle the exception appropriately, preventing the program from continuing in a potentially inconsistent state.

**Reasoning:**

The commit message and the code changes are highly aligned. The commit message indicates a fix to exception handlers, and the diff shows a change to an exception handler that was previously inadequate. The original code's assumption that the exception would ""never happen"" was a potential bug, as it could mask errors. The updated code addresses this by logging the error and re-throwing the exception, making the code more robust and easier to debug. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
hbase,40756.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,25 +1,26 @@
   public int getNumberOfRunningProcess(){
     if (!isUnix()){
       return 0;
     }
 
     BufferedReader input = null;
     try {
       int count = 0;
       Process p = Runtime.getRuntime().exec(""ps -e"");
       input = new BufferedReader(new InputStreamReader(p.getInputStream()));
       while (input.readLine() != null) {
         count++;
       }
       return count - 1; //  -1 because there is a headline
     } catch (IOException e) {
       return -1;
     }  finally {
       if (input != null){
         try {
           input.close();
-        } catch (IOException ignored) {
+        } catch (IOException e) {
+          LOG.warn(""Not able to close the InputStream"", e);
         }
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""HBASE-10452 Fix potential bugs in exception handlers"" suggests that the commit aims to fix bugs related to exception handling.

The provided code diff shows a change in the `finally` block of the `getNumberOfRunningProcess` method. Specifically, the `IOException` that might occur during `input.close()` is now caught, and a warning message is logged using `LOG.warn()`. Previously, this exception was silently ignored.

This change aligns with the commit message because it addresses a potential bug in the exception handling logic. If `input.close()` fails, the original code would silently ignore the exception, potentially leading to resource leaks or other unexpected behavior. The updated code logs a warning message, providing visibility into the error and allowing for debugging.

Therefore, the changes indicate a bug fix.

**Buggy**"
hbase,40081.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,10 +1,15 @@
   public static boolean isShowConfInServlet() {
     boolean isShowConf = false;
     try {
       if (Class.forName(""org.apache.hadoop.conf.ConfServlet"") != null) {
         isShowConf = true;
       }
-    } catch (Exception e) {
+    } catch (LinkageError e) {
+       // should we handle it more aggressively in addition to log the error?
+       LOG.warn(""Error thrown: "", e);
+    } catch (ClassNotFoundException ce) {
+      LOG.debug(""ClassNotFound: ConfServlet"");
+      // ignore
     }
     return isShowConf;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)"" explicitly states that the purpose of the commit is to fix potential bugs in exception handlers. This strongly suggests that the changes are related to bug fixes.

**Git Diff Analysis:**

The diff focuses on the `isShowConfInServlet()` method.  The original code had a single `catch` block for `Exception`. The modified code replaces the general `Exception` catch with two specific catch blocks:

1.  `LinkageError`: A `LinkageError` is caught, logged with a warning, and the execution continues. The comment ""should we handle it more aggressively in addition to log the error?"" indicates that the original exception handling might have been insufficient.
2.  `ClassNotFoundException`: A `ClassNotFoundException` is caught, logged at the DEBUG level, and ignored. This suggests that the absence of the `ConfServlet` class is a non-fatal condition that should not prevent the method from completing.

**Reasoning:**

The commit message and the code changes are highly aligned. The original code's generic `catch (Exception e)` block could have masked specific exceptions that should have been handled differently. The changes introduce more specific exception handling, addressing potential issues that could have arisen from the original, overly broad exception handling. The handling of `LinkageError` and `ClassNotFoundException` suggests that the original code might have been masking or mishandling these specific error conditions, which is a sign of a bug fix.

**Conclusion:**

**Buggy**
"
hbase,10426.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,30 +1,34 @@
   protected void configureForRegion(HRegion region) {
     super.configureForRegion(region);
     if (region != null) {
       prefixLength = 0;
 
       // read the prefix length from the table descriptor
       String prefixLengthString = region.getTableDesc().getValue(
           PREFIX_LENGTH_KEY);
       if (prefixLengthString == null) {
         //read the deprecated value
         prefixLengthString = region.getTableDesc().getValue(PREFIX_LENGTH_KEY_DEPRECATED);
         if (prefixLengthString == null) {
           LOG.error(PREFIX_LENGTH_KEY + "" not specified for table ""
               + region.getTableDesc().getTableName()
               + "". Using default RegionSplitPolicy"");
           return;
         }
       }
       try {
         prefixLength = Integer.parseInt(prefixLengthString);
       } catch (NumberFormatException nfe) {
-        // ignore
+        /* Differentiate NumberFormatException from an invalid value range reported below. */
+        LOG.error(""Number format exception when parsing "" + PREFIX_LENGTH_KEY + "" for table ""
+            + region.getTableDesc().getTableName() + "":""
+            + prefixLengthString + "". "" + nfe);
+        return;
       }
       if (prefixLength <= 0) {
         LOG.error(""Invalid value for "" + PREFIX_LENGTH_KEY + "" for table ""
             + region.getTableDesc().getTableName() + "":""
             + prefixLengthString + "". Using default RegionSplitPolicy"");
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for potential bugs in exception handlers. The code diff shows a change in the `configureForRegion` method where a `NumberFormatException` is caught when parsing the `PREFIX_LENGTH_KEY`. The original code ignored the exception, while the modified code logs the exception with more details and then returns. This change addresses a potential bug where an invalid prefix length could be silently ignored, leading to unexpected behavior. The added logging and early return provide better error handling and prevent the program from continuing with an invalid configuration.

**Buggy**"
ant,5703.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,3 +1,8 @@
     public String getJavacExecutable() {
+        if (forkedExecutable == null && isForkedJavac()) {
+            forkedExecutable = getSystemJavac();
+        } else if (forkedExecutable != null && !isForkedJavac()) {
+            forkedExecutable = null;
+        }
         return forkedExecutable;
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states:

*   ""Add testcases for latest `<javac>` changes"" - This suggests new functionality or updates to the `<javac>` task.
*   ""fix some problem with some rare combination of the fork attribute and build.compiler settings"" - This clearly indicates a bug fix related to the `fork` attribute and `build.compiler` settings in the `<javac>` task. The use of the word ""fix"" is a strong indicator.

**Git Diff Analysis:**

The diff shows changes within the `getJavacExecutable()` method. Let's break it down:

```java
     public String getJavacExecutable() {
+        if (forkedExecutable == null && isForkedJavac()) {
+            forkedExecutable = getSystemJavac();
+        } else if (forkedExecutable != null && !isForkedJavac()) {
+            forkedExecutable = null;
+        }
         return forkedExecutable;
     }
```

*   The code checks if `forkedExecutable` is null and `isForkedJavac()` returns true. If so, it assigns the result of `getSystemJavac()` to `forkedExecutable`.
*   The code checks if `forkedExecutable` is not null and `isForkedJavac()` returns false. If so, it assigns null to `forkedExecutable`.

**Reasoning:**

The diff modifies the logic for determining the `javac` executable when the `fork` attribute is used. The code ensures that `forkedExecutable` is correctly set or reset based on the `isForkedJavac()` condition. This aligns with the commit message's claim of fixing a problem related to the `fork` attribute. The added `if` and `else if` conditions suggest that the previous logic was incomplete or incorrect, leading to a bug. The code now handles cases where the `fork` attribute and `build.compiler` settings interact in a way that previously caused issues.

**Conclusion:**

The commit message explicitly mentions a bug fix, and the code changes directly relate to the `fork` attribute of the `<javac>` task. The code changes introduce logic to handle different states of `forkedExecutable` based on the `isForkedJavac()` condition, which indicates a correction of previous incorrect behavior.

**Buggy**
"
ant,5699.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,15 +1,16 @@
     public void setFork(String f) {
         if (f.equalsIgnoreCase(""on"")
             || f.equalsIgnoreCase(""true"")
             || f.equalsIgnoreCase(""yes"")) {
             fork = ""true"";
             forkedExecutable = getSystemJavac();
         } else if (f.equalsIgnoreCase(""off"")
                    || f.equalsIgnoreCase(""false"")
                    || f.equalsIgnoreCase(""no"")) {
             fork = ""false"";
+            forkedExecutable = null;
         } else {
             fork = ""true"";
             forkedExecutable = f;
         }
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates two main changes: adding test cases and fixing a problem related to the `fork` attribute and `build.compiler` settings. The provided diff focuses on the `setFork` method.

The code changes within the `setFork` method involve setting the `forkedExecutable` based on the input string `f`. Specifically, when `f` is ""off"", ""false"", or ""no"", `forkedExecutable` is explicitly set to `null`. This suggests a fix related to how the `fork` attribute interacts with the execution environment. The original code might have had an issue where `forkedExecutable` was not properly reset when forking was turned off, potentially leading to unexpected behavior or errors.

The explicit setting of `forkedExecutable` to `null` when `fork` is set to ""false"" suggests a correction to a previous oversight or bug. This ensures that when forking is disabled, the associated executable is also cleared, preventing potential conflicts or incorrect behavior.

Therefore, the code change in the diff aligns with the commit message's claim of fixing a problem related to the `fork` attribute.

**Buggy**
"
ant,9617.json,4e2a4f9c9507b19b248548ec6333698af3c1a4b8,"@@ -1,35 +1,38 @@
     public String substitute(String input, String argument, int options)
         throws BuildException {
         // translate \1 to $1 so that the Perl5Substitution will work
         StringBuffer subst = new StringBuffer();
         for (int i = 0; i < argument.length(); i++) {
             char c = argument.charAt(i);
-            if (c == '\\') {
+            if (c == '$') {
+                subst.append('\\');
+                subst.append('$');
+            } else if (c == '\\') {
                 if (++i < argument.length()) {
                     c = argument.charAt(i);
                     int value = Character.digit(c, 10);
                     if (value > -1) {
                         subst.append(""$"").append(value);
                     } else {
                         subst.append(c);
                     }
                 } else {
                     // XXX - should throw an exception instead?
                     subst.append('\\');
                 }
             } else {
                 subst.append(c);
             }
         }
         
 
         // Do the substitution
         Substitution s = 
             new Perl5Substitution(subst.toString(), 
                                   Perl5Substitution.INTERPOLATE_ALL);
         return Util.substitute(matcher,
                                getCompiledPattern(options),
                                s,
                                input,
                                getSubsOptions(options));
     }",Buggy,"fix for bug 20306 - regex handling of $ in replace string


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274633 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix for bug 20306 - regex handling of $ in replace string"" suggests a bug fix related to how the regular expression replacement handles the '$' character.

The code diff modifies the `substitute` method. The original code handled backslashes (`\`) for escaping characters. The modified code now handles dollar signs (`$`) by escaping them with another dollar sign (`$$`). This change directly addresses the bug described in the commit message, as it ensures that dollar signs are treated literally in the replacement string rather than being interpreted as backreferences. The original code had a comment about potentially throwing an exception, but this wasn't changed in the diff.

Given the commit message and the code changes, it's clear that this commit is a bug fix.

**Buggy**"
ant,3632.json,94182a1dfa6d104e3b48d12a651dffd53f089e85,"@@ -1,21 +1,24 @@
     protected void scanDir(File srcDir, File dest, JspMangler mangler, String files[]) {
 
         long now = (new Date()).getTime();
 
         for (int i = 0; i < files.length; i++) {
             String filename = files[i];
             File srcFile = new File(srcDir, filename);
             File javaFile = mapToJavaFile(mangler, srcFile, srcDir, dest);
+            if(javaFile==null) {
+                continue;
+            }
 
             if (srcFile.lastModified() > now) {
                 log(""Warning: file modified in the future: "" + filename,
                         Project.MSG_WARN);
             }
             boolean shouldCompile = false;
             shouldCompile = isCompileNeeded(srcFile, javaFile);
             if (shouldCompile) {
                compileList.addElement(srcFile.getAbsolutePath());
                javaFiles.addElement(javaFile);
             }
         }
     }",Buggy,"this patch does not fix the failing of jspc test, it fixes two other bugs in bugzilla -NPE on inclusion of non jsp file, and webapp attr broken.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@273214 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states that the patch addresses two bugs:

1.  **NPE on inclusion of non-JSP file:** This suggests that the code was previously throwing a NullPointerException when attempting to include a file that wasn't a JSP.
2.  **Webapp attr broken:** This indicates that a web application attribute was not functioning correctly.

The commit message also clarifies that this patch *does not* fix a failing `jspc` test.

**Code Diff Analysis:**

The code diff shows a modification within the `scanDir` method. A null check `if(javaFile==null)` has been added, and if `javaFile` is null, the loop continues using `continue`. `javaFile` is the result of `mapToJavaFile`.

**Reasoning:**

The addition of the null check `if(javaFile==null)` strongly suggests a bug fix.  If `mapToJavaFile` can return `null` under certain conditions (likely when a non-JSP file is encountered), and the code didn't previously handle this, it could indeed lead to a NullPointerException when `javaFile.getAbsolutePath()` is called later in the `compileList.addElement(srcFile.getAbsolutePath());` line. The `continue` statement prevents the NPE by skipping the rest of the loop iteration.

The commit message mentions an NPE on inclusion of a non-JSP file. The code change directly addresses this potential NPE.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix related to a potential NullPointerException. Therefore, the answer is:

**Buggy**
"
ant,4122.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,28 +1,28 @@
     private Source getSource(InputStream is, Resource resource)
         throws ParserConfigurationException, SAXException {
         // todo: is this comment still relevant ??
         // FIXME: need to use a SAXSource as the source for the transform
         // so we can plug in our own entity resolver
         Source src = null;
         if (entityResolver != null) {
             if (getFactory().getFeature(SAXSource.FEATURE)) {
                 SAXParserFactory spFactory = SAXParserFactory.newInstance();
                 spFactory.setNamespaceAware(true);
                 XMLReader reader = spFactory.newSAXParser().getXMLReader();
                 reader.setEntityResolver(entityResolver);
                 src = new SAXSource(reader, new InputSource(is));
             } else {
                 throw new IllegalStateException(""xcatalog specified, but ""
                     + ""parser doesn't support SAX"");
             }
         } else {
             // WARN: Don't use the StreamSource(File) ctor. It won't work with
             // xalan prior to 2.2 because of systemid bugs.
             src = new StreamSource(is);
         }
         // The line below is a hack: the system id must an URI, but it is not
         // cleat to get the URI of an resource, so just set the name of the
         // resource as a system id
-        src.setSystemId(resource.getName());
+        src.setSystemId(resourceToURI(resource));
         return src;
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix for SystemId of stylesheet"" and ""Bugzilla 39407"" clearly indicate a bug fix related to how the SystemId of a stylesheet is handled.

The code diff shows a change in the `getSource` method where `src.setSystemId(resource.getName());` is replaced with `src.setSystemId(resourceToURI(resource));`. The original code was setting the system ID to the resource name, which is described as a ""hack"" and potentially problematic because the system ID should be a URI. The new code uses `resourceToURI(resource)` to obtain a URI representation of the resource, which is a more correct and robust approach. This change directly addresses the issue described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**
"
ant,4118.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,5 +1,6 @@
     public void setStylesheet(File stylesheet) throws Exception {
         FileResource fr = new FileResource();
         fr.setProject(project);
         fr.setFile(stylesheet);
+        setStylesheet(fr);
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix for SystemId of stylesheet"" and the associated Bugzilla number suggest a bug fix related to how the stylesheet's SystemId is handled. The code change introduces a call to `setStylesheet(fr)` after setting the file for the FileResource. This indicates that the previous implementation might have been missing a step to properly set the SystemId, which is now being addressed. This is a strong indicator of a bug fix.

**Buggy**"
ant,9255.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,4 +1,12 @@
     private void resetBufferInfo() {    
         Thread current = Thread.currentThread();
-        buffers.remove(current);
+        BufferInfo bufferInfo = (BufferInfo)buffers.get(current);
+        try {
+            bufferInfo.buffer.close();
+        }
+        catch (IOException e) {
+            // Shouldn't happen
+        }
+        bufferInfo.buffer = new ByteArrayOutputStream();
+        bufferInfo.skip = false;
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed problem which gave (on Windows) output such as..."" clearly indicates that the commit is intended to resolve a bug. The description of the erroneous output pattern suggests an issue with how output is being buffered or handled, particularly on Windows. The empty line inserted between the real output lines is the core problem.

**Git Diff Analysis:**

The diff shows a modification to the `resetBufferInfo()` method.  Here's a breakdown:

1.  **Original Code:** `buffers.remove(current);` - This line simply removes the `BufferInfo` associated with the current thread from the `buffers` map.

2.  **Modified Code:**
    *   `BufferInfo bufferInfo = (BufferInfo)buffers.get(current);` - Retrieves the `BufferInfo` object.
    *   `try { bufferInfo.buffer.close(); } catch (IOException e) { // Shouldn't happen }` - This is the crucial part. It attempts to close the `ByteArrayOutputStream` (`buffer`) associated with the thread's `BufferInfo`. The `try-catch` block suggests that an `IOException` might occur during the close operation, although the comment indicates it's unexpected.
    *   `bufferInfo.buffer = new ByteArrayOutputStream();` - Creates a new `ByteArrayOutputStream` and assigns it to the `buffer` field of the `BufferInfo` object.
    *   `bufferInfo.skip = false;` - Resets the `skip` flag.

**Reasoning:**

The original code simply removed the `BufferInfo` object. The modified code now attempts to close the `ByteArrayOutputStream` before creating a new one. This is highly suggestive of a bug fix related to resource management.

The problem described in the commit message (extra empty lines in the output) could be caused by the `ByteArrayOutputStream` not being properly closed and flushed before being reused. If the stream isn't closed, some data might remain in the buffer, leading to unexpected output when the buffer is reused. Closing the stream ensures that all data is flushed, potentially resolving the issue of the extra empty lines.

The `try-catch` block, even with the ""Shouldn't happen"" comment, indicates awareness of a potential `IOException` during the close operation. This further strengthens the argument that the change is related to fixing a bug, possibly related to resource handling or stream corruption.

**Conclusion:**

The commit message explicitly states that it fixes a problem. The code changes involve closing a `ByteArrayOutputStream` which is related to buffering output, and the `try-catch` block indicates an attempt to handle a potential `IOException`. These factors strongly suggest that the commit is a bug fix.

**Buggy**
"
ant,9260.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,3 +1,4 @@
     public void close() throws IOException {
         flush();
+        removeBuffer();
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an issue on Windows where extra empty lines were being inserted into the output. The code change adds `removeBuffer()` to the `close()` method. This suggests that the buffer might not have been properly cleared or reset, leading to the extra lines. The `removeBuffer()` call likely addresses this by ensuring the buffer is cleared, thus fixing the bug described in the commit message.

Reasoning:
1. **Commit Message:** The commit message clearly states a bug fix related to extra empty lines in the output on Windows.
2. **Code Change:** The addition of `removeBuffer()` in the `close()` method implies a buffer management issue.
3. **Relevance:** The code change directly relates to the bug described in the commit message. Clearing the buffer in the `close()` method would prevent the extra empty lines from appearing in the output.

Conclusion: **Buggy**
"
ant,7847.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,10 +1,10 @@
     public void setClassname(String classname) {
         if (executableType == ExecutableType.MODULE) {
             javaCommand.setExecutable(createModuleClassPair(
                     parseModuleFromModuleClassPair(javaCommand.getExecutable()),
-                    classname));
+                    classname), false);
         } else {
             javaCommand.setExecutable(classname);
             executableType = ExecutableType.CLASS;
         }
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""avoid slash translations for module names"" suggests that the change is intended to prevent incorrect handling of module names, specifically related to slash characters. The phrase ""should fix Windows test errors introduced with #15"" strongly implies that a bug was introduced in a previous commit (#15) that caused test failures on Windows, and this commit aims to resolve that issue.

**Git Diff Analysis:**

The diff shows a modification within the `setClassname` method.  Specifically, when the `executableType` is `MODULE`, the `javaCommand.setExecutable` method is now called with an additional `false` argument. Looking at the surrounding code, it seems that the `createModuleClassPair` and `parseModuleFromModuleClassPair` methods are involved in handling module names. The addition of the `false` argument to `javaCommand.setExecutable` likely disables some kind of slash translation or escaping that was previously being applied.

**Reasoning:**

The commit message explicitly states that the change is intended to fix a bug introduced in a previous commit that caused Windows test failures. The code modification involves altering how module names are handled, likely to avoid incorrect slash translations. This aligns with the commit message's intent to address a bug related to module name handling on Windows. The addition of the `false` argument suggests a deliberate change to disable a specific behavior that was causing problems.

**Conclusion:**

**Buggy**
"
ant,7722.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,7 +1,3 @@
     public void setExecutable(String executable) {
-        if (executable == null || executable.length() == 0) {
-            return;
-        }
-        this.executable = executable.replace('/', File.separatorChar)
-            .replace('\\', File.separatorChar);
+        setExecutable(executable, true);
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"The commit message ""avoid slash translations for module names"" suggests that the change aims to prevent the translation of forward slashes (/) and backslashes (\) in module names, likely to resolve issues on Windows where path separators are different. The message also indicates that this change should fix Windows test errors introduced by a previous commit (#15).

The code diff shows that the original `setExecutable` method replaced both forward and backward slashes with the platform's file separator character. The modified code removes this replacement logic. This change directly aligns with the commit message's intention to avoid slash translations. The commit message also mentions fixing Windows test errors, which strongly suggests that the original slash translation was causing problems on Windows, thus indicating a bug fix.

Reasoning:
1. **Commit Message and Code Alignment:** The code change directly implements the intent described in the commit message, which is to avoid slash translations.
2. **Bug Fix Indication:** The commit message explicitly states that the change is intended to fix Windows test errors introduced by a previous commit. This strongly suggests that the original slash translation logic was causing a bug on Windows.
3. **Error Handling/Logical Correction:** The removal of the slash replacement logic can be seen as a logical correction, as it prevents unintended modifications of the executable path, which was causing errors on Windows.

Conclusion: **Buggy**
"
ant,7516.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFilelist(FileList fl) throws BuildException {
+        if (fl.getProject() == null) {
+            fl.setProject(getProject());
+        }
         add(fl);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse's ecj compiler due to a missing Project attribute in a FileSet. The root cause is identified in the `addExtDirs` method of `org.eclipse.jdt.core.JDTCompiler`. The provided diff shows a modification to the `addFilelist` method where a check is added to ensure that the FileList has a Project attribute set. If it doesn't, the project is set using `fl.setProject(getProject());`. This aligns perfectly with the commit message's description of the problem and its solution. The code change directly addresses the reported NPE by ensuring the FileList has a project associated with it before being added to a Path.

**Buggy**
"
ant,7517.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addDirset(DirSet dset) throws BuildException {
+        if (dset.getProject() == null) {
+            dset.setProject(getProject());
+        }
         add(dset);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse ECJ 3.1.x. The root cause is identified as being in the `org.eclipse.jdt.core.JDTCompiler` method `addExtDirs`, where a `FileSet` was created without the `Project` attribute set, and then added to a `Path`.

The provided code diff shows a modification to the `addDirset` method. Specifically, it adds a check to see if the `DirSet`'s project is null. If it is, the project is set using `dset.setProject(getProject())`.

This change directly addresses the issue described in the commit message. By ensuring that the `DirSet` has a `Project` attribute set, the code prevents the NullPointerException that was occurring when the `FileSet` (or `DirSet`) was later used in a context where the `Project` attribute was expected.

Therefore, the code change aligns with the commit message and clearly indicates a bug fix.

**Buggy**"
ant,7515.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFileset(FileSet fs) throws BuildException {
+        if (fs.getProject() == null) {
+            fs.setProject(getProject());
+        }
         add(fs);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when compiling with Eclipse ECJ 3.1.x due to a missing Project attribute in a FileSet. The root cause is identified in the `addExtDirs` method of `org.eclipse.jdt.core.JDTCompiler`. The provided diff shows a modification to the `addFileset` method, where it now explicitly sets the Project attribute of the FileSet if it's null. This aligns perfectly with the commit message's description of the problem and its solution. The code change directly addresses the reported NPE by ensuring the FileSet has a Project attribute before being added to a Path.

Reasoning:
1. **Problem Identification:** The commit message clearly states the problem: an NPE during compilation with Eclipse ECJ 3.1.x.
2. **Root Cause Analysis:** The commit message identifies the root cause as a missing Project attribute in a FileSet within the `addExtDirs` method.
3. **Code Change Alignment:** The diff shows that the `addFileset` method is modified to set the Project attribute of the FileSet if it's null. This directly addresses the root cause identified in the commit message.
4. **Bug Fix Indication:** The code change is a direct fix for the reported NPE, indicating a bug fix.

Conclusion:
**Buggy**"
ant,787.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,29 +1,22 @@
     public final int read() throws IOException {
         if (!getInitialized()) {
             initialize();
             setInitialized(true);
         }
 
         int ch = -1;
         if (unicodeBuf.length() == 0) {
             ch = in.read();
             if (ch != -1) {
                 char achar = (char) ch;
                 if (achar >= '\u0080') {
-                    unicodeBuf = new StringBuffer(""u0000"");
-                    String s = Integer.toHexString(ch);
-                    //replace the last 0s by the chars contained in s
-                    for (int i = 0; i < s.length(); i++) {
-                        unicodeBuf.setCharAt(unicodeBuf.length()
-                                             - s.length() + i,
-                                             s.charAt(i));
-                    }
+                    unicodeBuf = UnicodeUtil.EscapeUnicode(achar);
                     ch = '\\';
                 }
             }
         } else {
             ch = (int) unicodeBuf.charAt(0);
             unicodeBuf.deleteCharAt(0);
         }
         return ch;
     }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 50515, which involves incorrect Unicode escapes in the `propertyfile` task. The diff shows a modification in the `read()` method.

The original code attempted to convert characters with a value greater than or equal to `\u0080` into Unicode escape sequences. The code constructed a `StringBuffer` ""u0000"" and then replaced the last characters with the hexadecimal representation of the character.

The modified code replaces this logic with a call to `UnicodeUtil.EscapeUnicode(achar)`. This suggests that the original implementation was either incorrect or inefficient in handling Unicode escapes. The use of `UnicodeUtil.EscapeUnicode` implies a more robust and correct way to handle Unicode conversion.

The change directly addresses the issue described in the commit message (incorrect Unicode escapes). The original code's logic for creating Unicode escapes was replaced by a utility function, indicating a bug fix.

**Buggy**"
ant,9748.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,7 +1,3 @@
     private String escapeUnicode(char ch) {
-        StringBuffer buffy = new StringBuffer(""\\u"");
-        String hex = Integer.toHexString((int)ch);
-        buffy.append(""0000"".substring(4-hex.length()));
-        buffy.append(hex);
-        return buffy.toString();
-    }

+        return ""\\"" + UnicodeUtil.EscapeUnicode(ch);
+        }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 50515, specifically addressing incorrect Unicode escapes in the `propertyfile` task. The diff shows a change in the `escapeUnicode` method. The original implementation used a `StringBuffer` to construct the Unicode escape sequence, while the modified version uses `UnicodeUtil.EscapeUnicode(ch)`. This change suggests a correction to the way Unicode characters are escaped, aligning with the bug fix mentioned in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
ant,4816.json,29fface4fb93fb33b33c86124a168c04779271c0,"@@ -1,24 +1,24 @@
     public boolean execute() throws BuildException {
         Rmic owner = getRmic();
         Commandline cmd = setupRmicCommand();
         Project project = owner.getProject();
         //rely on RMIC being on the path
-        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(SunRmic.RMIC_EXECUTABLE));
+        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(getExecutableName()));
 
         //set up the args
         String[] args = cmd.getCommandline();
 
         try {
             Execute exe = new Execute(new LogStreamHandler(owner,
                     Project.MSG_INFO,
                     Project.MSG_WARN));
             exe.setAntRun(project);
             exe.setWorkingDirectory(project.getBaseDir());
             exe.setCommandline(args);
             exe.execute();
             return !exe.isFailure();
         } catch (IOException exception) {
-            throw new BuildException(""Error running "" + SunRmic.RMIC_EXECUTABLE
+            throw new BuildException(""Error running "" + getExecutableName()
                     + "" -maybe it is not on the path"", exception);
         }
     }",Buggy,"bug ID#38732 , rmic task doesn't work with -Xnew and JDK 6.0

Fixed by writing a new adapter, xnew, that extends the forking adapter and sets the -Xnew argument. Tests supplied, though the old test, the one that would fail on java1.6, is still there.

Also made the name matching code of rmic locale-independent.

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@434029 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `rmic` task not working with `-Xnew` and JDK 6.0. It also mentions making the name matching code of `rmic` locale-independent.

The code diff shows a change in how the executable name is retrieved. Instead of directly using `SunRmic.RMIC_EXECUTABLE`, it now uses `getExecutableName()`. The exception message also uses `getExecutableName()` instead of `SunRmic.RMIC_EXECUTABLE`. This change suggests an attempt to generalize or make the code more flexible in handling different executable names, which could be related to the bug fix for JDK 6.0. The commit message also mentions a new adapter, `xnew`, which is not visible in the diff.

The change in executable retrieval and exception message, combined with the commit message's description of a bug fix and locale-independent name matching, indicates that the code modification is indeed a bug fix.

**Buggy**"
ant,10020.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,3 @@
     public boolean tryHardToDelete(File f) {
-        if (!f.delete()) {
-            if (ON_WINDOWS) {
-                System.gc();
-            }
-            try {
-                Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
-            } catch (InterruptedException ex) {
-                // Ignore Exception
-            }
-            return f.delete();
-        }
-        return true;
+        return tryHardToDelete(f, ON_WINDOWS);
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to file deletion failures, potentially due to NFS issues, and aims to extend a workaround (running GC before retrying deletion) to non-Windows systems.

The code diff shows that the original `tryHardToDelete` method only attempted to run `System.gc()` on Windows systems before retrying the deletion. The modified code removes the Windows-specific condition and introduces a new method `tryHardToDelete(File f, boolean onWindows)` (not shown in the diff but implied), suggesting the original logic was flawed by being OS-specific. This change aligns with the commit message's intent to apply the GC workaround more broadly.

Therefore, the changes indicate a bug fix.

**Buggy**"
ant,10021.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
-    public boolean tryHardToDelete(File f) {
+    public boolean tryHardToDelete(File f, boolean runGC) {
         if (!f.delete()) {
-            if (ON_WINDOWS) {
+            if (runGC) {
                 System.gc();
             }
             try {
                 Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
             } catch (InterruptedException ex) {
                 // Ignore Exception
             }
             return f.delete();
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786"" strongly suggests a bug fix. Here's why:

*   **""Might fix the NFS problem""**: This explicitly indicates an attempt to resolve a known issue (likely related to file deletion on NFS filesystems).
*   **""retrying a failed build""**:  This implies that the original file deletion process was failing, leading to build failures. The change aims to improve the reliability of file deletion.
*   **""run the GC before retrying""**: Running the garbage collector (GC) is often a workaround for file locking or resource contention issues, which are common causes of file deletion failures.

**Diff Analysis:**

The diff shows a modification to the `tryHardToDelete` method.

*   **`tryHardToDelete(File f)` changed to `tryHardToDelete(File f, boolean runGC)`**: A new `runGC` parameter has been added, allowing the caller to control whether `System.gc()` is called.
*   **`if (ON_WINDOWS)` changed to `if (runGC)`**: The condition for running `System.gc()` has been changed from checking the operating system to using the new `runGC` parameter.

**Reasoning:**

The commit message and diff are highly correlated. The commit message describes adding an option to run the GC before retrying a delete, and the diff shows that the `tryHardToDelete` method now accepts a boolean parameter to control GC execution. The original code only ran the GC on Windows. The change allows the GC to be run on other operating systems as well, which is consistent with the commit message's intention to address an NFS-related problem. The fact that the commit message mentions a potential fix for a specific problem (NFS issue) and the code change introduces a mechanism (GC before retry) to address file deletion failures strongly suggests that this commit is intended to fix a bug.

**Conclusion:**

**Buggy**
"
ant,5445.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
     private boolean delete(File f) {
-        if (!FILE_UTILS.tryHardToDelete(f)) {
+        if (!FILE_UTILS.tryHardToDelete(f, performGc)) {
             if (deleteOnExit) {
                 int level = quiet ? Project.MSG_VERBOSE : Project.MSG_INFO;
                 log(""Failed to delete "" + f + "", calling deleteOnExit.""
                     + "" This attempts to delete the file when the Ant jvm""
                     + "" has exited and might not succeed."", level);
                 f.deleteOnExit();
                 return true;
             }
             return false;
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a potential fix for an NFS-related problem by adding an option to run the garbage collector (GC) before retrying a failed build on non-Windows OSes during file deletion. This suggests the original file deletion mechanism might have been failing due to resource contention or file locking issues, which the GC could alleviate.

The code diff shows a modification to the `delete` method. Specifically, the call to `FILE_UTILS.tryHardToDelete(f)` is changed to `FILE_UTILS.tryHardToDelete(f, performGc)`. This change introduces the `performGc` flag, which presumably controls whether the GC is run during the ""hard delete"" attempt.

The connection between the commit message and the code change is clear: the `performGc` flag, as suggested by the commit message, is now being used during file deletion. The commit message suggests that this change is intended to address a specific problem (NFS issues), implying that the previous file deletion mechanism was not robust enough. The modification of calling `tryHardToDelete` with the `performGc` parameter indicates an attempt to fix a bug or deficiency in the original code.

**Buggy**
"
ant,7946.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,9 +1,11 @@
     public boolean isSelected(File basedir, String filename, File file) {
 
         // throw BuildException on error
         validate();
 
         if (file.isDirectory()) {
             return type.equals(FileType.DIR);
-        } else return type.equals(FileType.FILE);
+        } else {
+            return type.equals(FileType.FILE);
+        }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a focus on fixing checkstyle issues, primarily through code reformatting and identifying problem spots using IDEA. The provided diff shows a minor change in the `isSelected` method: adding braces to the `else` statement. This change improves code readability and potentially avoids subtle bugs related to statement scope in the future. While the change itself doesn't scream ""bug fix,"" it aligns with the commit message's intention of addressing code style issues. The addition of braces around the `else` statement's return statement does not indicate a bug fix.

**NotBuggy**"
ant,8194.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,11 +1,10 @@
     public void verifySettings() {
         if (dateTime == null && millis < 0) {
             setError(""You must provide a datetime or the number of ""
-                + ""milliseconds."");
-        }
-        else if (millis < 0) {
+                    + ""milliseconds."");
+        } else if (millis < 0) {
             setError(""Date of "" + dateTime
-                + "" results in negative milliseconds""
-                + "" value relative to epoch (January 1, 1970, 00:00:00 GMT)."");
+                    + "" results in negative milliseconds value""
+                    + "" relative to epoch (January 1, 1970, 00:00:00 GMT)."");
         }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a fix for checkstyle issues, primarily focusing on code formatting and identifying potential problem areas. The diff shows changes in spacing and line breaks within the `verifySettings` method. Specifically, the `else if` block's formatting is adjusted, and string concatenation is modified for better readability.

The changes do not introduce new functionality or fix any logical errors. They primarily address code style and readability, aligning with the commit message's intention to resolve checkstyle issues. There are no error-handling updates, logical corrections, or exception-handling improvements.

**NotBuggy**"
ant,4305.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,87 +1,89 @@
         private URLConnection openConnection(final URL aSource) throws IOException {
 
             // set up the URL connection
             final URLConnection connection = aSource.openConnection();
             // modify the headers
             // NB: things like user authentication could go in here too.
             if (hasTimestamp) {
                 connection.setIfModifiedSince(timestamp);
             }
             // Set the user agent
             connection.addRequestProperty(""User-Agent"", this.userAgent);
 
             // prepare Java 1.1 style credentials
             if (uname != null || pword != null) {
                 final String up = uname + "":"" + pword;
                 String encoding;
                 // we do not use the sun impl for portability,
                 // and always use our own implementation for consistent
                 // testing
                 final Base64Converter encoder = new Base64Converter();
                 encoding = encoder.encode(up.getBytes());
                 connection.setRequestProperty(""Authorization"", ""Basic ""
                         + encoding);
             }
 
-            connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            if (tryGzipEncoding) {
+                connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            }
 
             if (connection instanceof HttpURLConnection) {
                 ((HttpURLConnection) connection)
                         .setInstanceFollowRedirects(false);
                 ((HttpURLConnection) connection)
                         .setUseCaches(httpUseCaches);
             }
             // connect to the remote site (may take some time)
             try {
                 connection.connect();
             } catch (final NullPointerException e) {
                 //bad URLs can trigger NPEs in some JVMs
                 throw new BuildException(""Failed to parse "" + source.toString(), e);
             }
 
             // First check on a 301 / 302 (moved) response (HTTP only)
             if (connection instanceof HttpURLConnection) {
                 final HttpURLConnection httpConnection = (HttpURLConnection) connection;
                 final int responseCode = httpConnection.getResponseCode();
                 if (isMoved(responseCode)) {
                     final String newLocation = httpConnection.getHeaderField(""Location"");
                     final String message = aSource
                             + (responseCode == HttpURLConnection.HTTP_MOVED_PERM ? "" permanently""
                                     : """") + "" moved to "" + newLocation;
                     log(message, logLevel);
                     final URL newURL = new URL(aSource, newLocation);
                     if (!redirectionAllowed(aSource, newURL)) {
                         return null;
                     }
                     return openConnection(newURL);
                 }
                 // next test for a 304 result (HTTP only)
                 final long lastModified = httpConnection.getLastModified();
                 if (responseCode == HttpURLConnection.HTTP_NOT_MODIFIED
                         || (lastModified != 0 && hasTimestamp && timestamp >= lastModified)) {
                     // not modified so no file download. just return
                     // instead and trace out something so the user
                     // doesn't think that the download happened when it
                     // didn't
                     log(""Not modified - so not downloaded"", logLevel);
                     return null;
                 }
                 // test for 401 result (HTTP only)
                 if (responseCode == HttpURLConnection.HTTP_UNAUTHORIZED) {
                     final String message = ""HTTP Authorization failure"";
                     if (ignoreErrors) {
                         log(message, logLevel);
                         return null;
                     } else {
                         throw new BuildException(message);
                     }
                 }
             }
 
             //REVISIT: at this point even non HTTP connections may
             //support the if-modified-since behaviour -we just check
             //the date of the content and skip the write if it is not
             //newer. Some protocols (FTP) don't include dates, of
             //course.
             return connection;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The commit message states that transparent gzip encoding is only enabled when explicitly specified, and it fixes bugzilla issue 57048. The diff shows that the line `connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);` has been moved inside an `if (tryGzipEncoding)` block. This means that the ""Accept-Encoding"" header, which tells the server that the client can accept gzipped content, is only set if `tryGzipEncoding` is true. This aligns with the commit message, as it ensures that gzip encoding is only requested when explicitly specified. This change likely fixes a bug where gzip encoding was being requested even when it wasn't desired or supported, potentially causing issues with certain servers or content.

**Buggy**
"
ant,4307.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,51 +1,52 @@
         private boolean downloadFile()
                 throws FileNotFoundException, IOException {
             for (int i = 0; i < numberRetries; i++) {
                 // this three attempt trick is to get round quirks in different
                 // Java implementations. Some of them take a few goes to bind
                 // properly; we ignore the first couple of such failures.
                 try {
                     is = connection.getInputStream();
                     break;
                 } catch (final IOException ex) {
                     log(""Error opening connection "" + ex, logLevel);
                 }
             }
             if (is == null) {
                 log(""Can't get "" + source + "" to "" + dest, logLevel);
                 if (ignoreErrors) {
                     return false;
                 }
                 throw new BuildException(""Can't get "" + source + "" to "" + dest,
                         getLocation());
             }
 
-            if (GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
+            if (tryGzipEncoding
+                && GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
                 is = new GZIPInputStream(is);
             }
 
             os = new FileOutputStream(dest);
             progress.beginDownload();
             boolean finished = false;
             try {
                 final byte[] buffer = new byte[BIG_BUFFER_SIZE];
                 int length;
                 while (!isInterrupted() && (length = is.read(buffer)) >= 0) {
                     os.write(buffer, 0, length);
                     progress.onTick();
                 }
                 finished = !isInterrupted();
             } finally {
                 FileUtils.close(os);
                 FileUtils.close(is);
 
                 // we have started to (over)write dest, but failed.
                 // Try to delete the garbage we'd otherwise leave
                 // behind.
                 if (!finished) {
                     dest.delete();
                 }
             }
             progress.endDownload();
             return true;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The commit message indicates a fix related to enabling transparent gzip encoding only when explicitly specified, addressing bugzilla issue 57048. The code diff modifies the condition under which the `GZIPInputStream` is used. Previously, it was based solely on the `Content-Encoding` header being ""gzip"". Now, it also requires the `tryGzipEncoding` flag to be true. This suggests that the previous implementation might have been automatically applying gzip decompression even when it wasn't intended, potentially leading to errors or unexpected behavior. The change ensures that gzip decompression is only applied when explicitly requested, which aligns with the commit message's intention to fix a bug related to uncontrolled gzip encoding.

**Buggy**"
ant,658.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,45 +1,46 @@
     private final void tsort(String root, Hashtable targets,
                              Hashtable state, Stack visiting,
                              Vector ret)
         throws BuildException {
         state.put(root, VISITING);
         visiting.push(root);
 
         Target target = (Target)(targets.get(root));
 
         // Make sure we exist
         if (target == null) {
             StringBuffer sb = new StringBuffer(""Target `"");
             sb.append(root);
             sb.append(""' does not exist in this project. "");
             visiting.pop();
             if (!visiting.empty()) {
                 String parent = (String)visiting.peek();
                 sb.append(""It is used from target `"");
                 sb.append(parent);
                 sb.append(""'."");
             }
 
             throw new BuildException(new String(sb));
         }
 
-        for (Enumeration en=target.getDependencies(); en.hasMoreElements();) {
+        for (Enumeration en = target.getDependencies(); en.hasMoreElements();) {
             String cur = (String) en.nextElement();
-            String m=(String)state.get(cur);
+            String m = (String)state.get(cur);
             if (m == null) {
                 // Not been visited
                 tsort(cur, targets, state, visiting, ret);
             }
             else if (m == VISITING) {
                 // Currently visiting this node, so have a cycle
                 throw makeCircularException(cur, visiting);
             }
         }
 
         String p = (String) visiting.pop();
         if (root != p) {
-            throw new RuntimeException(""Unexpected internal error: expected to pop ""+root+"" but got ""+p);
+            throw new RuntimeException(""Unexpected internal error: expected to ""
+                + ""pop "" + root + "" but got "" + p);
         }
         state.put(root, VISITED);
         ret.addElement(target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates fixes for errors found by checkstyle, which usually involves code formatting, javadoc, and other style-related issues.

The diff shows the following changes:

1.  **Formatting:** Changed `for (Enumeration en=target.getDependencies(); en.hasMoreElements();)` to `for (Enumeration en = target.getDependencies(); en.hasMoreElements();)`. This is a minor formatting change, adding a space after `en=`.
2.  **String concatenation:** Changed `throw new RuntimeException(""Unexpected internal error: expected to pop ""+root+"" but got ""+p);` to `throw new RuntimeException(""Unexpected internal error: expected to "" + ""pop "" + root + "" but got "" + p);`. This change improves readability by breaking the long string literal.
3.  **Error Message:** The error message is slightly improved by adding a space after ""to"".

These changes are consistent with the commit message, indicating fixes for style issues and minor error message improvements. These changes are likely related to addressing checkstyle warnings and do not indicate a bug fix in the logic of the code.

**NotBuggy**"
ant,624.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,7 +1,7 @@
-    public void addTarget(Target target) {
+    public void addTarget(Target target) throws BuildException {
         String name = target.getName();
         if (targets.get(name) != null) {
-            throw new BuildException(""Duplicate target: `""+name+""'"");
+            throw new BuildException(""Duplicate target: `"" + name + ""'"");
         }
         addOrReplaceTarget(name, target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix up errors in Project format and javadoc picked up with checkstyle"" indicates that the commit aims to correct formatting issues, javadoc problems, or other style-related errors identified by checkstyle.

The diff shows a change in the `addTarget` method. Specifically, it adds `throws BuildException` to the method signature. This implies that the method can now throw a `BuildException`, which wasn't explicitly declared before. The exception is thrown when a duplicate target name is detected.

The change addresses a potential issue where a duplicate target could cause unexpected behavior or errors during the build process. By explicitly declaring that the method throws a `BuildException`, the code becomes more robust and provides better error handling. This aligns with the commit message's intention to fix errors.

**Buggy**
"
ant,1464.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,15 +1,25 @@
     private void processComment( final String line )
     {
         final String lineSeparator = System.getProperty( ""line.separator"" );
-        if( line.startsWith( ""======"" ) || line.startsWith( ""------"" ) )
+        if( line.startsWith( ""======"" ) )
+        {
+            //We have ended changelog for that particular file
+            //so we can save it
+            final int end = m_comment.length() - lineSeparator.length(); //was -1
+            m_comment = m_comment.substring( 0, end );
+            m_comment = ""<![CDATA["" + m_comment + ""]]>"";
+            saveEntry();
+            m_status = GET_FILE;
+        }
+        else if( line.startsWith( ""------"" ) )
         {
             final int end = m_comment.length() - lineSeparator.length(); //was -1
             m_comment = m_comment.substring( 0, end );
             m_comment = ""<![CDATA["" + m_comment + ""]]>"";
             m_status = GET_PREVIOUS_REV;
         }
         else
         {
             m_comment += line + lineSeparator;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to retrieving the first change of a single file log when there are no other logs in between. The diff shows changes in the `processComment` method, specifically how it handles lines starting with ""======"" and ""------"".

The original code treated both ""======"" and ""------"" similarly, truncating the comment and updating the status. The modified code now differentiates between them. ""======"" now triggers saving the entry and setting the status to `GET_FILE`, while ""------"" only sets the status to `GET_PREVIOUS_REV`.

This change suggests that the original code incorrectly handled the end of a changelog for a single file, potentially causing it to only retrieve the first change. The modification addresses this by correctly identifying the end of the changelog with ""======"" and saving the entry.

Therefore, the changes indicate a bug fix.

**Buggy**
"
ant,1463.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,24 +1,25 @@
     public void stdout( final String line )
     {
         switch( m_status )
         {
             case GET_FILE:
                 processFile( line );
                 break;
             case GET_REVISION:
                 processRevision( line );
                 //Was a fall through ....
-                //break;
+                break;
+
             case GET_DATE:
                 processDate( line );
                 break;
 
             case GET_COMMENT:
                 processComment( line );
                 break;
 
             case GET_PREVIOUS_REV:
                 processGetPreviousRevision( line );
                 break;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to retrieving the first change of a single file log. The code diff removes a commented-out `break` statement in the `stdout` method's `switch` statement, specifically after the `processRevision(line)` call within the `GET_REVISION` case. The original code had a fall-through from the `GET_REVISION` case to the `GET_DATE` case. This fall-through would cause incorrect processing when a single file log was processed, as the date would be processed immediately after the revision, skipping the other cases. Removing the fall-through ensures that each case is handled correctly, thus fixing the bug described in the commit message.

**Buggy**"
ant,1468.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,36 +1,13 @@
     private void processGetPreviousRevision( final String line )
     {
-        final String entryKey = m_date + m_author + m_comment;
-        if( line.startsWith( ""revision"" ) )
+        if( !line.startsWith( ""revision"" ) )
         {
-            m_previousRevision = line.substring( 9 );
-            m_status = GET_FILE;
+            throw new IllegalStateException( ""Unexpected line from CVS: "" + line );
+        }
+        m_previousRevision = line.substring( 9 );
 
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision, m_previousRevision );
-        }
-        else if( line.startsWith( ""======"" ) )
-        {
-            m_status = GET_FILE;
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision );
-        }
+        saveEntry();
+
+        m_revision = m_previousRevision;
+        m_status = GET_COMMENT;
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to retrieving the first change of a single file log. The diff shows changes in the `processGetPreviousRevision` method.

The original code had two code paths, one for `revision` lines and another for lines starting with `======`. Both code paths contained nearly identical logic for creating or retrieving a `CVSEntry` and adding a file to it. The new code removes the `======` code path and throws an `IllegalStateException` if the line doesn't start with ""revision"". It then extracts the revision, calls a `saveEntry()` method (not shown in the diff), updates `m_revision`, and sets the status to `GET_COMMENT`.

The removal of the `======` code path and the introduction of the `IllegalStateException` suggest that the original code was incorrectly handling certain CVS output formats, leading to the bug described in the commit message. The modification ensures that only ""revision"" lines are processed, and an exception is thrown if the format is unexpected. This indicates a correction of a logical error. The commit message and the code changes are aligned.

**Buggy**"
ant,5442.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,116 +1,115 @@
     public void execute() throws BuildException {
         if (usedMatchingTask) {
             log(""DEPRECATED - Use of the implicit FileSet is deprecated.  ""
-                + ""Use a nested fileset element instead."");
+                + ""Use a nested fileset element instead."", quiet ? Project.MSG_VERBOSE : verbosity);
         }
 
         if (file == null && dir == null && filesets.size() == 0 && rcs == null) {
             throw new BuildException(""At least one of the file or dir ""
                                      + ""attributes, or a nested resource collection, ""
                                      + ""must be set."");
         }
 
         if (quiet && failonerror) {
             throw new BuildException(""quiet and failonerror cannot both be ""
                                      + ""set to true"", getLocation());
         }
 
         // delete the single file
         if (file != null) {
             if (file.exists()) {
                 if (file.isDirectory()) {
                     log(""Directory "" + file.getAbsolutePath()
                         + "" cannot be removed using the file attribute.  ""
-                        + ""Use dir instead."");
+                        + ""Use dir instead."", quiet ? Project.MSG_VERBOSE : verbosity);
                 } else {
                     log(""Deleting: "" + file.getAbsolutePath());
 
                     if (!delete(file)) {
                         handle(""Unable to delete file "" + file.getAbsolutePath());
                     }
                 }
             } else {
                 log(""Could not find file "" + file.getAbsolutePath()
-                    + "" to delete."",
-                    Project.MSG_VERBOSE);
+                    + "" to delete."", quiet ? Project.MSG_VERBOSE : verbosity);
             }
         }
 
         // delete the directory
         if (dir != null && dir.exists() && dir.isDirectory()
             && !usedMatchingTask) {
             /*
                If verbosity is MSG_VERBOSE, that mean we are doing
                regular logging (backwards as that sounds).  In that
                case, we want to print one message about deleting the
                top of the directory tree.  Otherwise, the removeDir
                method will handle messages for _all_ directories.
              */
             if (verbosity == Project.MSG_VERBOSE) {
                 log(""Deleting directory "" + dir.getAbsolutePath());
             }
             removeDir(dir);
         }
         Resources resourcesToDelete = new Resources();
         resourcesToDelete.setProject(getProject());
         Resources filesetDirs = new Resources();
         filesetDirs.setProject(getProject());
 
-        for (int i = 0; i < filesets.size(); i++) {
+        for (int i = 0, size = filesets.size(); i < size; i++) {
             FileSet fs = (FileSet) filesets.get(i);
             if (fs.getProject() == null) {
                 log(""Deleting fileset with no project specified;""
                     + "" assuming executing project"", Project.MSG_VERBOSE);
                 fs = (FileSet) fs.clone();
                 fs.setProject(getProject());
             }
             resourcesToDelete.add(fs);
             if (includeEmpty && fs.getDir().isDirectory()) {
               filesetDirs.add(new ReverseDirs(fs.getDir(),
                   fs.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         if (usedMatchingTask && dir != null && dir.isDirectory()) {
             //add the files from the default fileset:
             FileSet implicit = getImplicitFileSet();
             resourcesToDelete.add(implicit);
             if (includeEmpty) {
               filesetDirs.add(new ReverseDirs(dir,
                   implicit.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         resourcesToDelete.add(filesetDirs);
         if (rcs != null) {
             // sort first to files, then dirs
             Restrict exists = new Restrict();
             exists.add(EXISTS);
             exists.add(rcs);
             Sort s = new Sort();
             s.add(REVERSE_FILESYSTEM);
             s.add(exists);
             resourcesToDelete.add(s);
         }
         try {
             if (resourcesToDelete.isFilesystemOnly()) {
                 for (Iterator iter = resourcesToDelete.iterator(); iter.hasNext();) {
                     FileResource r = (FileResource) iter.next();
                     // nonexistent resources could only occur if we already
                     // deleted something from a fileset:
                     if (!r.isExists()) {
                         continue;
                     }
                     if (!(r.isDirectory()) || r.getFile().list().length == 0) {
                         log(""Deleting "" + r, verbosity);
                         if (!delete(r.getFile()) && failonerror) {
                             handle(""Unable to delete ""
                                 + (r.isDirectory() ? ""directory "" : ""file "") + r);
                         }
                     }
                 }
             } else {
                  handle(getTaskName() + "" handles only filesystem resources"");
             }
         } catch (Exception e) {
             handle(e);
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix bug#32738, implement quiet check for every message logged"" indicates a bug fix related to logging. The diff shows that several `log` statements have been modified to include a ternary operator `quiet ? Project.MSG_VERBOSE : verbosity`. This operator checks the `quiet` flag and adjusts the log level accordingly. This change ensures that logging verbosity is controlled by the `quiet` flag, which aligns with the commit message's intention to implement a quiet check for every message logged.

**Buggy**
"
ant,5446.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,22 +1,22 @@
     protected void removeDir(File d) {
         String[] list = d.list();
         if (list == null) {
             list = new String[0];
         }
         for (int i = 0; i < list.length; i++) {
             String s = list[i];
             File f = new File(d, s);
             if (f.isDirectory()) {
                 removeDir(f);
             } else {
-                log(""Deleting "" + f.getAbsolutePath(), verbosity);
+                log(""Deleting "" + f.getAbsolutePath(), quiet ? Project.MSG_VERBOSE : verbosity);
                 if (!delete(f)) {
                     handle(""Unable to delete file "" + f.getAbsolutePath());
                 }
             }
         }
         log(""Deleting directory "" + d.getAbsolutePath(), verbosity);
         if (!delete(d)) {
             handle(""Unable to delete directory "" + dir.getAbsolutePath());
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix bug#32738, implement quiet check for every message logged"" indicates a bug fix and the implementation of a quiet check for log messages.

The diff shows a change in the `removeDir` method. Specifically, the line `log(""Deleting "" + f.getAbsolutePath(), verbosity);` is changed to `log(""Deleting "" + f.getAbsolutePath(), quiet ? Project.MSG_VERBOSE : verbosity);`. This change introduces a conditional check using the `quiet` variable. If `quiet` is true, the log message is logged at `Project.MSG_VERBOSE` level; otherwise, it's logged at the original `verbosity` level. This aligns perfectly with the commit message's intention to implement a quiet check for every message logged. The original code always logged at the specified verbosity, potentially flooding the output. The modified code allows suppressing some of the log messages based on the `quiet` flag.

The commit message mentions bug#32738, which further suggests that this change is a bug fix.

Therefore, the changes indicate a bug fix.
**Buggy**"
ant,5444.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,7 +1,7 @@
     private void handle(Exception e) {
         if (failonerror) {
             throw (e instanceof BuildException)
                 ? (BuildException) e : new BuildException(e);
         }
-        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : Project.MSG_WARN);
+        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : verbosity);
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""fix bug#32738, implement quiet check for every message logged"" explicitly states that the commit is intended to fix a bug (bug #32738) and implement a ""quiet check"" for every logged message. This strongly suggests that the changes are related to correcting incorrect behavior or adding a feature to control logging verbosity.

**Git Diff Analysis:**

The diff shows a modification within the `handle(Exception e)` method. Specifically, the line:

```java
log(e.getMessage(), quiet ? Project.MSG_VERBOSE : Project.MSG_WARN);
```

is changed to:

```java
log(e.getMessage(), quiet ? Project.MSG_VERBOSE : verbosity);
```

This change replaces the hardcoded `Project.MSG_WARN` with a variable named `verbosity`. The original code would always log exceptions at the `WARN` level unless the `quiet` flag was set, in which case it would log at the `VERBOSE` level. The modified code now uses the `verbosity` variable, which presumably allows for more flexible control over the logging level.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly mentions fixing a bug.
2.  **Logging Control:** The code change directly affects how exceptions are logged. The original code had a fixed logging level (`WARN`) that was overridden by the `quiet` flag. The modified code introduces a `verbosity` variable, allowing for more granular control over the logging level.
3.  **Potential Bug:** The original code's behavior of always logging exceptions at the `WARN` level (unless `quiet` was enabled) might have been considered a bug. Perhaps users wanted to log exceptions at a different level (e.g., `ERROR` or `INFO`). The change addresses this by introducing the `verbosity` variable.
4.  **Alignment:** The commit message and the code change are aligned. The commit message mentions implementing a ""quiet check"" and the code change modifies the logging behavior based on the `quiet` flag and the `verbosity` variable.

**Conclusion:**

Based on the commit message and the code changes, it's highly likely that this commit addresses a bug related to logging verbosity. The original code's fixed logging level was probably considered incorrect or inflexible, and the change introduces a `verbosity` variable to provide more control.

**Buggy**
"
ant,1828.json,85f6ea3795fa8d4393723ec229e9168f8b824f28,"@@ -1,11 +1,22 @@
     public void execute() throws BuildException {
+        ScriptRunner runner = new ScriptRunner();
+        if (language != null) {
+            runner.setLanguage(language);
+        }
+        if (src != null) {
+            runner.setSrc(src);
+        }
+        if (text != null) {
+            runner.addText(text);
+        }
+        
         runner.addBeans(getProject().getProperties());
         runner.addBeans(getProject().getUserProperties());
         runner.addBeans(getProject().getTargets());
         runner.addBeans(getProject().getReferences());
 
         runner.addBean(""project"", getProject());
         runner.addBean(""self"", this);
 
         runner.executeScript(""<ANT>"");
     }",Buggy,"Fix for script memory retaining problem.
PR: 25394
Obtained from: Jose Alberto Fernandez /  Antoine Levy-Lambert


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275836 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix for script memory retaining problem"" suggests a bug fix related to how scripts retain memory.

The diff shows changes within the `execute()` method. The code initializes a `ScriptRunner` and sets its properties like language, source, and text from the Ant task's attributes. It then adds various beans (project properties, user properties, targets, references, ""project"", and ""self"") to the `ScriptRunner`. Finally, it executes the script.

The original code was likely retaining memory because the `ScriptRunner` instance was not properly disposed of or cleared after execution. The provided diff doesn't show any explicit disposal or clearing of the `ScriptRunner` instance. However, the creation of a *new* `ScriptRunner` instance *within* the `execute()` method, instead of reusing a potentially long-lived instance, could address the memory retention issue. By creating a new `ScriptRunner` each time the task is executed, any memory held by the previous runner becomes eligible for garbage collection.

Therefore, the changes, while not explicitly showing memory management code, implicitly address the problem described in the commit message by ensuring a fresh `ScriptRunner` instance is used for each execution.

**Buggy**
"
lucene-solr,14918.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,20 +1,3 @@
   public static SolrDocument convertLuceneDocToSolrDoc(Document doc, final IndexSchema schema) {
-    SolrDocument out = new SolrDocument();
-    for (IndexableField f : doc.getFields()) {
-      // Make sure multivalued fields are represented as lists
-      Object existing = out.get(f.name());
-      if (existing == null) {
-        SchemaField sf = schema.getFieldOrNull(f.name());
-        if (sf != null && sf.multiValued()) {
-          List<Object> vals = new ArrayList<>();
-          vals.add(f);
-          out.setField(f.name(), vals);
-        } else {
-          out.setField(f.name(), f);
-        }
-      } else {
-        out.addField(f.name(), f);
-      }
-    }
-    return out;
+    return convertLuceneDocToSolrDoc(doc,schema, new SolrReturnFields());
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"Okay, let's analyze the provided commit message and Git diff.

**Commit Message Analysis:**

The commit message indicates several things:

1.  **Optimization:** The primary goal is to optimize `DocStreamer` by making it respect `ReturnFields`. This suggests a performance improvement by reducing unnecessary field processing.
2.  **Bug Fixes:** It explicitly mentions fixing three bugs: SOLR-11891, SOLR-12107, and SOLR-12108.
    *   SOLR-12107: A bug in the `[child]` transformer, potentially occurring when the document cache is disabled.
    *   SOLR-12108: A bug in the `[raw]` and `[xml]` transformers related to fallback behavior when an incompatible `wt` (writer type) is specified, leading to data loss if the document cache is disabled.
    *   SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument

**Git Diff Analysis:**

The provided diff shows a change in the `convertLuceneDocToSolrDoc` method.  The original implementation iterated through all fields in the Lucene `Document` and added them to the `SolrDocument`. The modified version calls an overloaded version of the same method passing a new `SolrReturnFields()` object.

**Reasoning:**

The commit message clearly states that this commit includes bug fixes. The diff itself, while small, supports the idea of a bug fix related to how fields are handled when converting a Lucene `Document` to a `SolrDocument`. The change in `convertLuceneDocToSolrDoc` suggests that the original method was not respecting the `ReturnFields` parameter, which is now being addressed by passing a `SolrReturnFields` object to the overloaded method. This aligns with the description of SOLR-11891. The other bug fixes are not directly visible in the provided diff, but the commit message states that they are included in this commit.

**Conclusion:**

**Buggy**
"
lucene-solr,14569.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,30 +1,31 @@
   public DocTransformer create(String display, SolrParams params, SolrQueryRequest req) {
     String field = params.get(""f"");
     if(Strings.isNullOrEmpty(field)) {
       field = display;
     }
     // When a 'wt' is specified in the transformer, only apply it to the same wt
     boolean apply = true;
     if(applyToWT!=null) {
       String qwt = req.getParams().get(CommonParams.WT);
       if(qwt==null) {
         QueryResponseWriter qw = req.getCore().getQueryResponseWriter(req);
         QueryResponseWriter dw = req.getCore().getQueryResponseWriter(applyToWT);
         if(qw!=dw) {
           apply = false;
         }
       }
       else {
         apply = applyToWT.equals(qwt);
       }
     }
 
     if(apply) {
       return new RawTransformer( field, display );
     }
     
-    if(field.equals(display)) {
-      return null; // nothing
+    if (field.equals(display)) {
+      // we have to ensure the field is returned
+      return new NoopFieldTransformer(field);
     }
     return new RenameFieldTransformer( field, display, false );
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"The commit message indicates a bug fix related to the `[raw]` and `[xml]` transformers. Specifically, it mentions that the fallback behavior of these transformers was broken when an incompatible `wt` (writer type) was specified and the document cache was not used, leading to the loss of the field value (SOLR-12108).

The code diff shows a change in the `create` method of a `DocTransformer` class. The original code had a condition `if(field.equals(display)) { return null; }`. The modified code replaces this with `if (field.equals(display)) { return new NoopFieldTransformer(field); }`. This change ensures that the field is returned even when `field` and `display` are equal. This aligns with the bug fix described in the commit message, as it prevents the field value from being lost in certain scenarios.

Therefore, the changes in the code diff indicate a bug fix.

**Buggy**
"
lucene-solr,17639.json,9548481c8c301740067229d09af5db0f06dccb94,"@@ -1,32 +1,35 @@
-  private Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
+  protected Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
     Map<String, Object> obj = new HashMap<>();
     List<String> errors = new ArrayList<>();
     obj.put(""host"", zkHostPort);
     List<String> lines = getZkRawResponse(zkHostPort, ""ruok"");
+    validateZkRawResponse(lines, zkHostPort,""ruok"");
     boolean ok = ""imok"".equals(lines.get(0));
     obj.put(""ok"", ok);
     lines = getZkRawResponse(zkHostPort, ""mntr"");
+    validateZkRawResponse(lines, zkHostPort,""mntr"");
     for (String line : lines) {
       String[] parts = line.split(""\t"");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else {
-        String err = String.format(""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     lines = getZkRawResponse(zkHostPort, ""conf"");
+    validateZkRawResponse(lines, zkHostPort,""conf"");
     for (String line : lines) {
       String[] parts = line.split(""="");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else if (!line.startsWith(""membership:"")) {
-        String err = String.format(""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     obj.put(""errors"", errors);
     return obj;
   }",NotBuggy,"Fix precommit
Remove errors from each host detail map
Display secureClientPort and server.1, server.2, server.3...
Added test for various failure responses and expected result from multiple nodes
",Buggy,"The commit message indicates several changes: fixing precommit issues, removing errors from each host detail map, displaying `secureClientPort` and server details, and adding a test for failure responses.

The code diff shows the addition of `validateZkRawResponse` calls after each `getZkRawResponse` call. It also includes changes to the error message formatting using `Locale.ENGLISH`. The error messages are added to the `errors` list, which is then added to the `obj` map.

The addition of `validateZkRawResponse` suggests a fix to handle potentially invalid or unexpected responses from Zookeeper. The error handling is improved by logging warnings and adding errors to a list. The `Locale.ENGLISH` change suggests a fix related to localization or formatting issues.

Based on the changes, it appears that the commit addresses potential issues with Zookeeper response handling and error reporting. Therefore, the changes indicate a bug fix.

**Buggy**"
lucene-solr,38367.json,0a70e721ce98b3c4ae10aadf9edcb312d4f57da4,"@@ -1,92 +1,103 @@
     private void initIter(Shape filter) {
       cellNumber = -1;
-      if (filter instanceof LevelledValue && ((LevelledValue)filter).getLevel() == 0)
+      if (filter instanceof LevelledValue && ((LevelledValue) filter).getLevel() == 0)
         filter = null;//world means everything -- no filter
       iterFilter = filter;
 
-      NRCell parent = getLVAtLevel(getLevel()-1);
+      NRCell parent = getLVAtLevel(getLevel() - 1);
 
       // Initialize iter* members.
 
       //no filter means all subcells
       if (filter == null) {
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
         return;
       }
 
       final LevelledValue minLV;
       final LevelledValue maxLV;
+      final int lastLevelInCommon;//between minLV & maxLV
       if (filter instanceof NRShape) {
         NRShape nrShape = (NRShape) iterFilter;
         minLV = nrShape.getMinLV();
         maxLV = nrShape.getMaxLV();
+        lastLevelInCommon = nrShape.getLastLevelInCommon();
       } else {
-        minLV = (LevelledValue)iterFilter;
+        minLV = (LevelledValue) iterFilter;
         maxLV = minLV;
+        lastLevelInCommon = minLV.getLevel();
       }
 
-      //fast path check when using same filter
-      if (iterFilter == parent.iterFilter) {
+      //fast path optimization that is usually true, but never first level
+      if (iterFilter == parent.iterFilter &&
+          (getLevel() <= lastLevelInCommon || parent.iterFirstCellNumber != parent.iterLastCellNumber)) {
+        //TODO benchmark if this optimization pays off. We avoid two comparePrefixLV calls.
         if (parent.iterFirstIsIntersects && parent.cellNumber == parent.iterFirstCellNumber
             && minLV.getLevel() >= getLevel()) {
           iterFirstCellNumber = minLV.getValAtLevel(getLevel());
           iterFirstIsIntersects = (minLV.getLevel() > getLevel());
         } else {
           iterFirstCellNumber = 0;
           iterFirstIsIntersects = false;
         }
         if (parent.iterLastIsIntersects && parent.cellNumber == parent.iterLastCellNumber
             && maxLV.getLevel() >= getLevel()) {
           iterLastCellNumber = maxLV.getValAtLevel(getLevel());
           iterLastIsIntersects = (maxLV.getLevel() > getLevel());
         } else {
           iterLastCellNumber = getNumSubCells(parent) - 1;
           iterLastIsIntersects = false;
         }
         if (iterFirstCellNumber == iterLastCellNumber) {
           if (iterLastIsIntersects)
             iterFirstIsIntersects = true;
           else if (iterFirstIsIntersects)
             iterLastIsIntersects = true;
         }
         return;
       }
 
-      //uncommon to get here, except for level 1 which always happens
+      //not common to get here, except for level 1 which always happens
 
       int startCmp = comparePrefixLV(minLV, parent);
       if (startCmp > 0) {//start comes after this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       int endCmp = comparePrefixLV(maxLV, parent);//compare to end cell
       if (endCmp < 0) {//end comes before this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       if (startCmp < 0 || minLV.getLevel() < getLevel()) {
         //start comes before...
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
       } else {
         iterFirstCellNumber = minLV.getValAtLevel(getLevel());
         iterFirstIsIntersects = (minLV.getLevel() > getLevel());
       }
       if (endCmp > 0 || maxLV.getLevel() < getLevel()) {
         //end comes after...
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
       } else {
         iterLastCellNumber = maxLV.getValAtLevel(getLevel());
         iterLastIsIntersects = (maxLV.getLevel() > getLevel());
       }
+      if (iterFirstCellNumber == iterLastCellNumber) {
+        if (iterLastIsIntersects)
+          iterFirstIsIntersects = true;
+        else if (iterFirstIsIntersects)
+          iterLastIsIntersects = true;
+      }
     }",Buggy,"LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602857 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required."" indicates a bug fix in the `initIter` method of the `NumberRangePrefixTree` class. The message also states that re-indexing is required, which is a strong indicator that the fix involves a change in how data is indexed or queried, likely due to a previous indexing error.

The code diff focuses on the `initIter` method, which appears to be responsible for initializing an iterator over cells within a spatial index, potentially for filtering or searching. The changes include:

1.  **Addition of `lastLevelInCommon`:** A new variable `lastLevelInCommon` is introduced and used when the filter is an `NRShape`. This variable seems to represent the last level shared between the minimum and maximum LevelledValues.
2.  **Modified Fast Path Optimization:** The condition for the fast path optimization is changed. It now includes a check `getLevel() <= lastLevelInCommon || parent.iterFirstCellNumber != parent.iterLastCellNumber`. This suggests that the previous fast path logic was flawed and didn't correctly handle cases where the level was beyond `lastLevelInCommon` or when the parent cell represented a range of subcells.
3.  **Duplicated Logic:** The code block that handles the case where `iterFirstCellNumber == iterLastCellNumber` is duplicated. This block sets `iterFirstIsIntersects` or `iterLastIsIntersects` to true based on the other's value. This duplication suggests that the logic was either missing from the original code or was incorrectly placed.

The changes to the fast path optimization, especially the addition of `lastLevelInCommon` and the level check, strongly suggest a bug fix. The original optimization likely led to incorrect iterator initialization under certain conditions, potentially causing incorrect search results or exceptions. The duplicated logic further supports the idea of a correction to the iterator initialization process.

**Buggy**
"
lucene-solr,11525.json,1b3b9294cc99985db88c2ef9074f05e802c6b55b,"@@ -1,8 +1,8 @@
   public void collect(int doc) throws IOException {
     delegate.collect(doc);
-    lastDocId = doc;    
     numCollected++;  
-    if(numCollected==maxDocsToCollect) {
-      throw new EarlyTerminatingCollectorException(numCollected, lastDocId);
+    if(maxDocsToCollect <= numCollected) {
+      throw new EarlyTerminatingCollectorException
+        (numCollected, prevReaderCumulativeSize + (doc + 1));
     }
   }",Buggy,"SOLR-5122: Fixed bug in spellcheck.collateMaxCollectDocs.  Eliminates risk of divide by zero, and makes estimated hit counts meaningful in non-optimized indexes.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1514402 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-5122: Fixed bug in spellcheck.collateMaxCollectDocs. Eliminates risk of divide by zero, and makes estimated hit counts meaningful in non-optimized indexes."" explicitly states that a bug has been fixed. The message mentions a ""divide by zero"" risk and improvements to hit count estimation, suggesting numerical or logical errors in the original code.

**Git Diff Analysis:**

The diff shows a modification within the `collect` method of a collector class. Let's break down the changes:

-   **Original Code:**

    ```java
    if(numCollected==maxDocsToCollect) {
      throw new EarlyTerminatingCollectorException(numCollected, lastDocId);
    }
    ```

-   **Modified Code:**

    ```java
    if(maxDocsToCollect <= numCollected) {
      throw new EarlyTerminatingCollectorException
        (numCollected, prevReaderCumulativeSize + (doc + 1));
    }
    ```

Key observations:

1.  **Condition Change:** The condition `numCollected==maxDocsToCollect` has been changed to `maxDocsToCollect <= numCollected`. This change is subtle but significant. The original code only triggered the exception when `numCollected` was *exactly* equal to `maxDocsToCollect`. The modified code triggers the exception when `numCollected` is greater than or equal to `maxDocsToCollect`. This suggests a potential off-by-one error in the original implementation. If `numCollected` ever exceeded `maxDocsToCollect` (due to some unforeseen circumstance), the original code would *not* have thrown the exception, potentially leading to incorrect behavior.

2.  **`EarlyTerminatingCollectorException` Argument:** The second argument to the `EarlyTerminatingCollectorException` constructor has changed from `lastDocId` to `prevReaderCumulativeSize + (doc + 1)`. The commit message mentioned ""makes estimated hit counts meaningful in non-optimized indexes"". This change in the exception's argument strongly suggests that the `lastDocId` was not providing a meaningful hit count estimate in all cases (specifically, non-optimized indexes), and the new expression is intended to correct this.

**Reasoning:**

The commit message explicitly states a bug fix related to ""divide by zero"" and ""meaningful hit counts"". While the diff doesn't directly show a division operation, the change in the exception's argument, coupled with the commit message, strongly suggests that the original `lastDocId` was leading to inaccurate or potentially zero hit count estimates in some scenarios. The change to `prevReaderCumulativeSize + (doc + 1)` likely provides a more robust and accurate estimate, especially in non-optimized indexes. The condition change from `==` to `<=` also indicates a correction to ensure the early termination logic works as intended, even if `numCollected` slightly exceeds `maxDocsToCollect`.

**Conclusion:**

The commit message and the code modifications align to indicate a bug fix related to hit count estimation and potential off-by-one errors. Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,14422.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,4 +1,4 @@
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used"" clearly states that the change is intended to fix an issue related to error logs when the `QuerySenderListener` is in use. This strongly suggests a bug fix. The problem likely involves incorrect or missing `SolrRequestInfo` in the context of the `QuerySenderListener`, leading to errors in logging.

**Git Diff Analysis:**

The diff shows a single line change within the `store` method:

`- SolrRequestInfo me = threadLocal.get();`
`+ SolrRequestInfo me = SolrRequestInfo.getRequestInfo();`

The change replaces `threadLocal.get()` with `SolrRequestInfo.getRequestInfo()`. This indicates that the original code was retrieving the `SolrRequestInfo` from a thread-local variable, while the corrected code retrieves it using a static method `getRequestInfo()`.

**Reasoning:**

1.  **Error Log Issue:** The commit message mentions fixing error logs. This implies that the original code was not correctly providing the necessary context (`SolrRequestInfo`) for logging, especially when `QuerySenderListener` was involved.
2.  **ThreadLocal vs. Static Method:** The change from `threadLocal.get()` to `SolrRequestInfo.getRequestInfo()` suggests that the thread-local variable was not reliably providing the correct `SolrRequestInfo` in all cases, particularly within the context of the `QuerySenderListener`. The `getRequestInfo()` method likely provides a more reliable or context-aware way to access the `SolrRequestInfo`.
3.  **Contextual Relevance:** The `QuerySenderListener` likely operates in a different thread or context than where the `SolrRequestInfo` was initially stored in the thread-local. This would explain why the thread-local approach was failing and causing logging errors. The `getRequestInfo()` method might be retrieving the information from a request-scoped context or a more globally accessible location.

**Conclusion:**

Based on the commit message and the code change, it's highly likely that this commit fixes a bug related to how `SolrRequestInfo` is accessed, leading to incorrect error logs when `QuerySenderListener` is used. The change addresses an issue where the thread-local variable was not providing the correct context, and the `getRequestInfo()` method provides a more reliable solution.

**Buggy**
"
lucene-solr,14423.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,7 +1,7 @@
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used"" explicitly states that the change is intended to fix an issue related to error logs when the `QuerySenderListener` is in use. This strongly suggests a bug fix. The message implies that the error logs were incorrect or missing in certain scenarios involving `QuerySenderListener`.

**Git Diff Analysis:**

The diff shows a change in the `set` method of an anonymous class. The original code:

```java
threadLocal.set(me);
```

is replaced with:

```java
SolrRequestInfo.setRequestInfo(me);
```

This change suggests that the way `SolrRequestInfo` is being stored or accessed within the thread context is being modified. The original code used `threadLocal.set(me)` which is likely a thread-local variable. The new code uses `SolrRequestInfo.setRequestInfo(me)`, which implies a static method call to set the request info.

**Reasoning:**

The commit message indicates a fix for error logs related to `QuerySenderListener`. The code change involves modifying how `SolrRequestInfo` is stored/accessed in the thread context. It's plausible that the original thread-local approach was not correctly propagating or clearing the `SolrRequestInfo` in scenarios involving `QuerySenderListener`, leading to incorrect or missing information in the error logs. The change to `SolrRequestInfo.setRequestInfo(me)` likely addresses this issue by providing a more reliable way to manage the request info, ensuring that the correct information is available when error logs are generated.

**Conclusion:**

The commit message explicitly states a bug fix, and the code change aligns with the description by modifying how request information is managed in a thread context, which could directly impact error logging. Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,14425.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,23 +1,23 @@
   public static ExecutorUtil.InheritableThreadLocalProvider getInheritableThreadLocalProvider() {
     return new ExecutorUtil.InheritableThreadLocalProvider() {
       @Override
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }
 
       @Override
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }
 
       @Override
       public void clean(AtomicReference ctx) {
-        threadLocal.remove();
+        SolrRequestInfo.clearRequestInfo();
       }
     };
   }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used"" explicitly states that the change is a fix for error logs related to `SolrRequestInfo` when `QuerySenderListener` is in use. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff focuses on the `ExecutorUtil.InheritableThreadLocalProvider` used to manage `SolrRequestInfo` within threads. The key changes are:

1.  **`threadLocal.get()` replaced with `SolrRequestInfo.getRequestInfo()`:** This suggests that the original code was directly accessing a thread-local variable (`threadLocal`), while the corrected code uses a static method `SolrRequestInfo.getRequestInfo()`. This could be to ensure that the correct `SolrRequestInfo` is retrieved, especially in scenarios where the thread context might be complex or when `QuerySenderListener` is involved.
2.  **`threadLocal.set(me)` replaced with `SolrRequestInfo.setRequestInfo(me)`:** Similar to the above, this change suggests a shift from direct thread-local manipulation to using a dedicated method for setting the `SolrRequestInfo`.
3.  **`threadLocal.remove()` replaced with `SolrRequestInfo.clearRequestInfo()`:** This change suggests a shift from directly removing the thread local to using a dedicated method for clearing the `SolrRequestInfo`.

These changes indicate that the original code might have had issues with how `SolrRequestInfo` was being managed in the thread context, potentially leading to incorrect or missing information in error logs when `QuerySenderListener` was used. The new code uses static methods to manage the `SolrRequestInfo`, which likely centralizes and improves the handling of this information.

**Reasoning:**

The commit message and the code changes are highly correlated. The commit message explicitly mentions fixing error logs related to `SolrRequestInfo` and `QuerySenderListener`. The code changes modify how `SolrRequestInfo` is accessed, set, and cleared within threads, which directly impacts the context available for logging. The changes from direct thread local access to using static methods strongly suggests an attempt to fix a bug related to thread context management.

**Conclusion:**

**Buggy**
"
ant,3769.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setServerLanguageCodeConfig(LanguageCode serverLanguageCode) {
-        if (serverLanguageCode != null && !serverLanguageCode.equals("""")) {
+        if (serverLanguageCode != null && !"""".equals(serverLanguageCode.getValue())) {
             this.serverLanguageCodeConfig = serverLanguageCode;
             configurationHasBeenSet();
         }
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object)"" suggests that the commit addresses at least two distinct issues:

1.  **Thread Safety:** The mention of ""Formatter class is not thread safe"" implies a fix related to concurrent access or modification of a `Formatter` object, which could lead to data corruption or unexpected behavior.  We don't see any code related to thread safety in the diff, so we will ignore this.
2.  **Incorrect `equals` Usage:** The phrase ""equals from string to another object"" indicates a potential bug where the `equals()` method was being used to compare a string with an object of a different type, which would always return `false` or potentially throw an exception.

**Diff Analysis:**

The provided diff shows a change in the `setServerLanguageCodeConfig` method:

```diff
-        if (serverLanguageCode != null && !serverLanguageCode.equals("""")) {
+        if (serverLanguageCode != null && !"""".equals(serverLanguageCode.getValue())) {
```

The original code used `serverLanguageCode.equals("""")` to check if the `serverLanguageCode` object was equivalent to an empty string. This is problematic because `serverLanguageCode` is an object of type `LanguageCode`, not a String.  The corrected code uses `"""".equals(serverLanguageCode.getValue())` which is safer and more correct. It retrieves the string value from the `LanguageCode` object using `getValue()` and then compares that string with an empty string.

**Relevance and Bug Fix Assessment:**

The diff directly addresses the second issue mentioned in the commit message (""equals from string to another object""). The original code was attempting to compare a `LanguageCode` object with a string, which is semantically incorrect and would likely lead to unexpected behavior. The corrected code properly extracts the string value from the `LanguageCode` object and compares it with the empty string. This is a clear indication of a bug fix.

**Conclusion:**

The code change aligns with the commit message and corrects a logical error in the `equals` comparison. Therefore, the changes indicate a bug fix.

**Buggy**
"
ant,3780.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setTimestampGranularity(Granularity timestampGranularity) {
-        if (null == timestampGranularity || """".equals(timestampGranularity)) {
+        if (null == timestampGranularity || """".equals(timestampGranularity.getValue())) {
             return;
         }
         this.timestampGranularity = timestampGranularity;
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object)."" suggests that the commit addresses at least two distinct issues:

1.  **Thread Safety:** A formatter class (likely within the codebase) was found to be not thread-safe. This implies a potential concurrency bug where multiple threads accessing the formatter could lead to data corruption or unexpected behavior.
2.  **Incorrect `equals` Usage:** The message mentions an `equals` comparison between a string and another object. This strongly suggests a bug where an equality check was performed between incompatible types, likely always returning `false` or potentially throwing an exception.

**Diff Analysis:**

The provided diff shows a change within the `setTimestampGranularity` method. Specifically, the condition `"""".equals(timestampGranularity)` has been changed to `"""".equals(timestampGranularity.getValue())`.

**Reasoning:**

The original code was attempting to compare an empty string (`""""`) directly with the `timestampGranularity` object. This is problematic because `timestampGranularity` is an object of type `Granularity` (as indicated by the method signature), not a string. The `equals` method in Java is type-sensitive. Comparing a string with an object of a different type will almost always return `false` (unless the object's `equals` method is explicitly designed to handle such comparisons, which is unlikely and generally bad practice).

The corrected code `"""".equals(timestampGranularity.getValue())` suggests that the `Granularity` class has a `getValue()` method that returns a string representation of the granularity. This allows for a meaningful comparison with the empty string.

**Alignment and Bug Fix Indication:**

The diff directly addresses the second issue mentioned in the commit message (""equals from string to another object""). The original code had a type mismatch in the `equals` comparison, which would have led to incorrect behavior (likely preventing the timestamp granularity from being properly reset or initialized under certain conditions). The corrected code fixes this type mismatch, ensuring that the comparison is performed between two strings.

While the diff doesn't directly show changes related to thread safety, the commit message indicates that thread safety was also addressed in the commit. It's possible that other parts of the commit (not shown in the provided diff) contain the thread safety fixes.

**Conclusion:**

The provided diff clearly demonstrates a fix for a bug related to an incorrect `equals` comparison. The commit message accurately describes this fix. Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,43949.json,b0db06bad568b7eedf528379a2fe5ac935992d56,"@@ -1,43 +1,45 @@
       public Scorer scorer(LeafReaderContext context) throws IOException {
         SortedNumericDocValues values = getValues(context.reader(), field);
         if (values == null) {
           return null;
         }
-        final NumericDocValues singleton = DocValues.unwrapSingleton(values);
+        final NumericDocValues singleton = null; // TODO: LUCENE-7649, re-consider optimization that broke SOLR-10013
+        // final NumericDocValues singleton = DocValues.unwrapSingleton(values);
         final TwoPhaseIterator iterator;
         if (singleton != null) {
+          assert false : ""imposible code -- or: someone re-enabled singleton optinization w/o reading the whole method"";
           iterator = new TwoPhaseIterator(singleton) {
             @Override
             public boolean matches() throws IOException {
               final long value = singleton.longValue();
               return value >= lowerValue && value <= upperValue;
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         } else {
           iterator = new TwoPhaseIterator(values) {
             @Override
             public boolean matches() throws IOException {
               for (int i = 0, count = values.docValueCount(); i < count; ++i) {
                 final long value = values.nextValue();
                 if (value < lowerValue) {
                   continue;
                 }
                 // Values are sorted, so the first value that is >= lowerValue is our best candidate
                 return value <= upperValue;
               }
               return false; // all values were < lowerValue
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         }
         return new ConstantScoreScorer(this, score(), iterator);
       }",Buggy,"SOLR-10013: Fix DV range query bug introduced by LUCENE-7643 by disabling and optimization (LUCENE-7649 to track re-enabling or removing completely)
",Buggy,"The commit message indicates a fix for a bug (SOLR-10013) introduced by a previous change (LUCENE-7643). The bug is related to range queries on DocValues. The message also mentions disabling an optimization as a workaround.

The diff shows that a section of code that attempts to optimize the query execution path when the SortedNumericDocValues are single-valued (i.e., a singleton) has been disabled. Specifically, the line `final NumericDocValues singleton = DocValues.unwrapSingleton(values);` has been commented out and replaced with `final NumericDocValues singleton = null;`. An assertion is added to indicate that the singleton optimization should not be re-enabled without careful consideration. The code then proceeds to use the non-optimized path for all cases.

The commit message and the code changes align perfectly. The code disables an optimization, which directly addresses the bug mentioned in the commit message. The disabling of the optimization suggests that the optimization was causing incorrect results in certain range queries, indicating a bug.

**Buggy**"
lucene-solr,20864.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,3 +1,3 @@
-  public static ConfigSolr fromSolrHome(String solrHome) {
-    return fromFile(new File(solrHome, SOLR_XML_FILE));
+  public static ConfigSolr fromSolrHome(SolrResourceLoader loader, String solrHome) {
+    return fromFile(loader, new File(solrHome, SOLR_XML_FILE));
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a performance improvement by avoiding the creation of multiple `SolrResourceLoader` instances for the same Solr home. It also mentions a fix for a resource leak that caused test failures on Windows.

The code diff shows a change in the `fromSolrHome` method signature. It now accepts a `SolrResourceLoader` instance as an argument, instead of creating a new one internally. This change aligns with the commit message's goal of reusing `SolrResourceLoader` instances. The commit message mentions that the previous implementation was not correctly closing the loader, which caused test failures on Windows. This suggests that the change in the method signature is part of a bug fix related to resource management.

**Buggy**
"
lucene-solr,20441.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,5 +1,6 @@
   public static CoreContainer createAndLoad(String solrHome, File configFile) {
-    CoreContainer cc = new CoreContainer(new SolrResourceLoader(solrHome), ConfigSolr.fromFile(configFile));
+    SolrResourceLoader loader = new SolrResourceLoader(solrHome);
+    CoreContainer cc = new CoreContainer(loader, ConfigSolr.fromFile(loader, configFile));
     cc.load();
     return cc;
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a performance improvement by avoiding the creation of multiple `SolrResourceLoader` instances for the same Solr home. It also mentions a fix for a resource leak that caused test failures on Windows.

The code diff shows that a `SolrResourceLoader` is now created once and passed to both the `CoreContainer` constructor and the `ConfigSolr.fromFile` method. This aligns with the commit message's intention to reuse the same loader instance. The original code created a new `SolrResourceLoader` directly in the `CoreContainer` constructor.

The commit message also mentions that the loader was not correctly closed, causing test failures on Windows. While the diff doesn't explicitly show a `close()` call, the change in how the `SolrResourceLoader` is managed suggests that it's now handled in a way that ensures proper resource management and closure, indirectly addressing the Windows test failure issue.

The changes indicate a bug fix because the original code had a resource leak issue that was causing test failures. The updated code fixes this issue by ensuring that only one `SolrResourceLoader` is created and properly managed.

**Buggy**
"
lucene-solr,48003.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,48 +1,48 @@
   public static void compress(byte[] bytes, int off, int len, DataOutput out, HashTable ht) throws IOException {
 
     final int base = off;
     final int end = off + len;
 
     int anchor = off++;
 
     if (len > LAST_LITERALS + MIN_MATCH) {
 
       final int limit = end - LAST_LITERALS;
       final int matchLimit = limit - MIN_MATCH;
       ht.reset(len);
       final int hashLog = ht.hashLog;
       final PackedInts.Mutable hashTable = ht.hashTable;
 
       main:
-      while (off < limit) {
+      while (off <= limit) {
         // find a match
         int ref;
         while (true) {
           if (off >= matchLimit) {
             break main;
           }
           final int v = readInt(bytes, off);
           final int h = hash(v, hashLog);
           ref = base + (int) hashTable.get(h);
           assert PackedInts.bitsRequired(off - base) <= hashTable.getBitsPerValue();
           hashTable.set(h, off - base);
           if (off - ref < MAX_DISTANCE && readInt(bytes, ref) == v) {
             break;
           }
           ++off;
         }
 
         // compute match length
         final int matchLen = MIN_MATCH + commonBytes(bytes, ref + MIN_MATCH, off + MIN_MATCH, limit);
 
         encodeSequence(bytes, anchor, ref, off, matchLen, out);
         off += matchLen;
         anchor = off;
       }
     }
 
     // last literals
     final int literalLen = end - anchor;
     assert literalLen >= LAST_LITERALS || literalLen == len;
     encodeLastLiterals(bytes, anchor, end - anchor, out);
   }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to compression, specifically with highly compressible inputs when using `LZ4.compressHC`.

The code diff focuses on the `compress` method. The key change is in the `while` loop condition:

- **Original:** `while (off < limit)`
- **Modified:** `while (off <= limit)`

This change from `<` to `<=` suggests a potential off-by-one error in the original code. The loop might have been terminating prematurely, especially with highly compressible data where the `off` variable could reach `limit` quickly. By including `limit` in the loop, the code ensures that all possible matches are considered, potentially fixing a bug where the compression wasn't optimal or was failing in certain cases.

The change directly addresses a potential logical error in the compression algorithm, indicating a bug fix.

**Buggy**
"
lucene-solr,48001.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,10 +1,10 @@
     private void addHash(byte[] bytes, int off) {
       final int v = readInt(bytes, off);
       final int h = hashHC(v);
       int delta = off - hashTable[h];
       if (delta >= MAX_DISTANCE) {
         delta = MAX_DISTANCE - 1;
       }
       chainTable[off & MASK] = (short) delta;
-      hashTable[h] = off - base;
+      hashTable[h] = off;
     }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to compression, specifically with highly compressible inputs when using `LZ4.compressHC`. The code diff shows a change in the `addHash` method, where the `hashTable[h]` is updated.

Before: `hashTable[h] = off - base;`
After: `hashTable[h] = off;`

The change involves removing the subtraction of `base` from `off` when updating `hashTable[h]`. This suggests that the original calculation was incorrect, potentially leading to incorrect hash table entries and thus affecting the compression process, especially for highly compressible data where repeated patterns are crucial. The original code was likely causing an out-of-bounds access or incorrect distance calculation due to the subtraction of `base`. Removing the subtraction likely fixes the hash table indexing, resolving the compression bug described in the commit message.

**Buggy**"
lucene-solr,41767.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public int docID() {
-            return doc;
-          }

+    public int docID() { return doc; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)"" indicates refactoring and bug fixes. The code change involves modifying the formatting of the `docID()` method. While this could be part of a larger refactoring effort, the commit message explicitly mentions bug fixes. Without more context, it's difficult to definitively say if this specific change is a bug fix. However, given the commit message's emphasis on bug fixes, it's plausible that this change addresses a minor bug related to code style or consistency.

**Buggy**
"
lucene-solr,41766.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public float score() {
-            return score;
-          }

+    public float score() { return score; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)"" indicates that the commit includes bug fixes.

The code diff shows a minor change in the `score()` method, specifically the removal of curly braces and whitespace adjustments, which can be considered refactoring. While this change itself doesn't scream ""bug fix,"" the commit message explicitly mentions bug fixes. Therefore, it's reasonable to assume that this change is part of a larger effort to address bugs.

**Buggy**
"
lucene-solr,41782.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,52 +1,60 @@
   public void collect(int doc) throws IOException {
 
     if (curDocs == null) {
       // Cache was too large
-      if (curScores != null) {
-        score = scorer.score();
+      if (cacheScores) {
+        cachedScorer.score = scorer.score();
       }
-      this.doc = doc;
+      cachedScorer.doc = doc;
       other.collect(doc);
       return;
     }
 
+    // Allocate a bigger array or abort caching
     if (upto == curDocs.length) {
       base += upto;
-      final int nextLength;
-      // Max out at 512K arrays:
-      if (curDocs.length < 524288) {
-        nextLength = 8*curDocs.length;
-      } else {
-        nextLength = curDocs.length;
+      
+      // Compute next array length - don't allocate too big arrays
+      int nextLength = 8*curDocs.length;
+      if (nextLength > MAX_ARRAY_SIZE) {
+        nextLength = MAX_ARRAY_SIZE;
       }
 
       if (base + nextLength > maxDocsToCache) {
-        // Too many docs to collect -- clear cache
-        curDocs = null;
-        if (curScores != null) {
-          score = scorer.score();
+        // try to allocate a smaller array
+        nextLength = maxDocsToCache - base;
+        if (nextLength <= 0) {
+          // Too many docs to collect -- clear cache
+          curDocs = null;
+          curScores = null;
+          cachedSegs.clear();
+          cachedDocs.clear();
+          cachedScores.clear();
+          if (cacheScores) {
+            cachedScorer.score = scorer.score();
+          }
+          cachedScorer.doc = doc;
+          other.collect(doc);
+          return;
         }
-        this.doc = doc;
-        other.collect(doc);
-        cachedDocs.clear();
-        cachedScores.clear();
-        return;
       }
+      
       curDocs = new int[nextLength];
       cachedDocs.add(curDocs);
-      if (curScores != null) {
+      if (cacheScores) {
         curScores = new float[nextLength];
         cachedScores.add(curScores);
       }
       upto = 0;
     }
+    
     curDocs[upto] = doc;
     // TODO: maybe specialize private subclass so we don't
     // null check per collect...
-    if (curScores != null) {
-      score = curScores[upto] = scorer.score();
+    if (cacheScores) {
+      cachedScorer.score = curScores[upto] = scorer.score();
     }
     upto++;
-    this.doc = doc;
+    cachedScorer.doc = doc;
     other.collect(doc);
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates bug fixes.

The code changes include:

1.  **Error Handling:** The code now checks if `nextLength` is less than or equal to 0 after attempting to allocate a smaller array. If it is, the cache is cleared, and the document is collected by `other.collect(doc)`. This suggests a previous scenario where the code might have failed to handle cases where the remaining space was insufficient.
2.  **Array Size Limit:** The code now limits the maximum array size to `MAX_ARRAY_SIZE`. This implies a previous issue where the array could grow too large, potentially leading to memory issues or crashes.
3.  **Cache Clearing:** The code now clears `cachedSegs` along with `cachedDocs` and `cachedScores` when the cache is cleared due to insufficient space. This suggests a potential issue where `cachedSegs` was not being properly cleared, leading to inconsistencies or memory leaks.

These changes indicate that the commit addresses potential bugs related to array size limits, error handling when allocating arrays, and cache management.

**Buggy**"
lucene-solr,41774.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,40 +1,42 @@
   public void replay(Collector other) throws IOException {
     if (!isCached()) {
       throw new IllegalStateException(""cannot replay: cache was cleared because too much RAM was required"");
     }
+    
+    if (!other.acceptsDocsOutOfOrder() && this.other.acceptsDocsOutOfOrder()) {
+      throw new IllegalArgumentException(
+          ""cannot replay: given collector does not support ""
+              + ""out-of-order collection, while the wrapped collector does. ""
+              + ""Therefore cached documents may be out-of-order."");
+    }
+
     //System.out.println(""CC: replay totHits="" + (upto + base));
     if (lastReaderContext != null) {
       cachedSegs.add(new SegStart(lastReaderContext, base+upto));
       lastReaderContext = null;
     }
-    final int uptoSav = upto;
-    final int baseSav = base;
-    try {
-      upto = 0;
-      base = 0;
-      int chunkUpto = 0;
-      other.setScorer(cachedScorer);
-      curDocs = EMPTY_INT_ARRAY;
-      for(SegStart seg : cachedSegs) {
-        other.setNextReader(seg.readerContext);
-        while(base+upto < seg.end) {
-          if (upto == curDocs.length) {
-            base += curDocs.length;
-            curDocs = cachedDocs.get(chunkUpto);
-            if (curScores != null) {
-              curScores = cachedScores.get(chunkUpto);
-            }
-            chunkUpto++;
-            upto = 0;
+    
+    int curupto = 0;
+    int curbase = 0;
+    int chunkUpto = 0;
+    other.setScorer(cachedScorer);
+    curDocs = EMPTY_INT_ARRAY;
+    for(SegStart seg : cachedSegs) {
+      other.setNextReader(seg.readerContext);
+      while(curbase+curupto < seg.end) {
+        if (curupto == curDocs.length) {
+          curbase += curDocs.length;
+          curDocs = cachedDocs.get(chunkUpto);
+          if (cacheScores) {
+            curScores = cachedScores.get(chunkUpto);
           }
-          if (curScores != null) {
-            score = curScores[upto];
-          }
-          other.collect(curDocs[upto++]);
+          chunkUpto++;
+          curupto = 0;
         }
+        if (cacheScores) {
+          cachedScorer.score = curScores[curupto];
+        }
+        other.collect(curDocs[curupto++]);
       }
-    } finally {
-      upto = uptoSav;
-      base = baseSav;
     }
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)"" explicitly mentions ""bug fixes"".

The diff introduces a check for `acceptsDocsOutOfOrder` which suggests a potential issue where documents were being collected out of order, leading to incorrect results. This check prevents the replay if the provided collector doesn't support out-of-order collection while the cached data might be out of order.

The diff also modifies the loop within the `replay` method. The original code saved and restored the `upto` and `base` variables using a `try...finally` block. The modified code removes this `try...finally` block and introduces `curupto` and `curbase` variables. This change suggests a correction to the loop logic, potentially fixing an issue where the loop wasn't iterating correctly or was causing incorrect offsets. The introduction of `cachedScorer.score = curScores[curupto];` also indicates a fix related to score handling during replay.

Given the explicit mention of ""bug fixes"" in the commit message and the code modifications that address potential issues with out-of-order collection and loop logic, it's highly likely that this commit includes bug fixes.

**Buggy**
"
lucene-solr,47123.json,52d1ab79192a331e1f3fc5f5a202d6a4c5a633b2,"@@ -1,23 +1,27 @@
-  ByteBuffer[] map(FileChannel fc, long offset, long length) throws IOException {
+  final ByteBuffer[] map(String resourceDescription, FileChannel fc, long offset, long length) throws IOException {
     if ((length >>> chunkSizePower) >= Integer.MAX_VALUE)
-      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + fc.toString());
+      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + resourceDescription);
     
     final long chunkSize = 1L << chunkSizePower;
     
     // we always allocate one more buffer, the last one may be a 0 byte one
     final int nrBuffers = (int) (length >>> chunkSizePower) + 1;
     
     ByteBuffer buffers[] = new ByteBuffer[nrBuffers];
     
     long bufferStart = 0L;
     for (int bufNr = 0; bufNr < nrBuffers; bufNr++) { 
       int bufSize = (int) ( (length > (bufferStart + chunkSize))
           ? chunkSize
               : (length - bufferStart)
           );
-      buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      try {
+        buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      } catch (IOException ioe) {
+        throw convertMapFailedIOException(ioe, resourceDescription, bufSize);
+      }
       bufferStart += bufSize;
     }
     
     return buffers;
   }",NotBuggy,"LUCENE-5673: MMapDirectory: Work around a ""bug"" in the JDK that throws a confusing OutOfMemoryError wrapped inside IOException if the FileChannel  mapping failed because of lack of virtual address space. The IOException is rethrown with more useful information about the problem, omitting the incorrect OutOfMemoryError

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1595213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a specific issue in `MMapDirectory` related to handling `OutOfMemoryError` during file channel mapping. The error is misleading because it's caused by a lack of virtual address space, not necessarily a lack of memory. The commit aims to provide a more informative error message.

The code diff shows the introduction of a `try-catch` block around the `fc.map` call. If an `IOException` occurs during the mapping, it's caught, and a new `IOException` is thrown using the `convertMapFailedIOException` method. This new exception includes more context about the failure, specifically omitting the misleading `OutOfMemoryError`. The `resourceDescription` is also used in the exception message.

The changes directly address the issue described in the commit message. The `try-catch` block and the `convertMapFailedIOException` method are introduced to handle the specific `IOException` that wraps the misleading `OutOfMemoryError`. This confirms that the commit is indeed a bug fix.

**Buggy**
"
lucene-solr,3974.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,31 +1,32 @@
-    private MapWriter serializeToV2Format(SolrParams params, String[] list) {
+    private MapWriter serializeToV2Format(SolrParams paramsV1, String[] list) {
       return ew -> ew.put(meta.commandName, (MapWriter) ew1 -> {
-        Iterator<String> iter = params.getParameterNamesIterator();
+        Iterator<String> iter = paramsV1.getParameterNamesIterator();
         Map<String, Map<String, String>> subProperties = null;
         while (iter.hasNext()) {
           String key = iter.next();
           if (CoreAdminParams.ACTION.equals(key)) continue;
           Object substitute = meta.getReverseParamSubstitute(key);
           int idx = template.variables.indexOf(substitute);
           if (idx > -1) {
-            String val = params.get(String.valueOf(substitute));
+            String val = paramsV1.get(key);
             if (val == null) throw new RuntimeException(""null value is not valid for "" + key);
             list[idx] = val;
             continue;
           }
           if (substitute instanceof Pair) {//this is a nested object
+            @SuppressWarnings(""unchecked"")
             Pair<String, String> p = (Pair<String, String>) substitute;
             if (subProperties == null) subProperties = new HashMap<>();
-            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), params.get(key));
+            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), paramsV1.get(key));
           } else {
-            Object val = params.get(key);
+            Object val = paramsV1.get(key);
             ew1.put(substitute.toString(), val);
           }
         }
         if (subProperties != null) {
           for (Map.Entry<String, Map<String, String>> e : subProperties.entrySet()) {
             ew1.put(e.getKey(), e.getValue());
           }
         }
       });
     }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"The commit message ""SOLR-12061: Fix substitution bug in API V1 to V2 migration"" indicates a bug fix related to parameter substitution during the migration from API V1 to V2.

The code diff shows changes within the `serializeToV2Format` method. The key changes involve how parameters are retrieved and processed.

1.  **Parameter Renaming:** The `params` parameter is renamed to `paramsV1`, suggesting that this method is specifically dealing with V1 parameters.
2.  **Corrected Parameter Retrieval:** Inside the `if (idx > -1)` block, the code now uses `paramsV1.get(key)` instead of `params.get(String.valueOf(substitute))`. This is a crucial change. The original code was attempting to retrieve the value using the *substituted* value as the key, which is incorrect. The corrected code uses the original key (`key`) to retrieve the parameter value from the V1 parameter set.
3.  **Corrected Nested Object Handling:** Similarly, in the `subProperties.computeIfAbsent` call, `paramsV1.get(key)` is now used instead of `params.get(key)`.
4.  **General Parameter Retrieval:** The code now uses `paramsV1.get(key)` instead of `params.get(key)` in the `else` block.

These changes clearly indicate a bug fix. The original code was incorrectly retrieving parameter values using the substituted values as keys, leading to incorrect or missing data during the API migration. The corrected code ensures that the original parameter keys are used to retrieve the correct values from the V1 parameter set.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**
"
lucene-solr,3973.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,12 +1,12 @@
-    public V2Request.Builder convert(SolrParams params) {
+    public V2Request.Builder convert(SolrParams paramsV1) {
       String[] list = new String[template.variables.size()];
-      MapWriter data = serializeToV2Format(params, list);
+      MapWriter data = serializeToV2Format(paramsV1, list);
       Map o = data.toMap(new LinkedHashMap<>());
       return new V2Request.Builder(template.apply(s -> {
         int idx = template.variables.indexOf(s);
         return list[idx];
       }))
           .withMethod(meta.getHttpMethod())
           .withPayload(o);
 
     }",NotBuggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-12061: Fix substitution bug in API V1 to V2 migration"" explicitly states that the commit addresses a bug related to substitution during the migration from API version 1 to version 2.  The term ""fix"" strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the method `convert`. The parameter name `params` is changed to `paramsV1`. Also, `params` is passed to `serializeToV2Format` method.

Given the commit message indicates a bug fix related to substitution in API V1 to V2 migration, and the code changes involve parameter renaming and passing the parameter to a method, it's highly likely that the original code had an issue with how parameters were being handled or substituted during the migration process. The renaming suggests an attempt to clarify the origin of the parameters (V1).

**Conclusion:**

Based on the commit message and the code changes, it's clear that this commit addresses a bug.

**Buggy**
"
lucene-solr,3975.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,11 +1,11 @@
-  public static V2Request.Builder convert(CollectionAdminRequest request) {
+  public static V2Request.Builder convert(CollectionAdminRequest<?> request) {
     ActionInfo info = mapping.get(request.action);
     if (info == null) throw new RuntimeException(""Unsupported action :"" + request.action);
 
     if (info.meta.getHttpMethod() == SolrRequest.METHOD.POST) {
       if (info.path == null) info.setPath();
       return info.convert(request.getParams());
     }
 
     return null;
   }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-12061: Fix substitution bug in API V1 to V2 migration"" explicitly states that the commit addresses a bug related to substitution during the migration from API V1 to API V2. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `convert` method signature:

`- public static V2Request.Builder convert(CollectionAdminRequest request)`
`+ public static V2Request.Builder convert(CollectionAdminRequest<?> request)`

The change involves modifying the parameter type from `CollectionAdminRequest` to `CollectionAdminRequest<?>`. The `<?>` indicates a wildcard generic type, meaning the method can now accept `CollectionAdminRequest` objects with any type parameter.

The rest of the method remains the same. The code retrieves action information from a mapping, checks the HTTP method, and then calls `info.convert` with the request parameters.

**Reasoning:**

The change in the method signature from `CollectionAdminRequest` to `CollectionAdminRequest<?>` suggests that the original code was too restrictive in the types of `CollectionAdminRequest` it could handle. This could have led to errors or unexpected behavior during the API V1 to V2 migration if the `convert` method received a `CollectionAdminRequest` with a type parameter that didn't match the expected type. By using a wildcard generic type, the method becomes more flexible and can handle a wider range of `CollectionAdminRequest` objects, which aligns with the commit message's claim of fixing a substitution bug.

**Conclusion:**

The commit message explicitly mentions a bug fix, and the code change relaxes the type constraint on the `convert` method's parameter, which is consistent with fixing a substitution issue during API migration. Therefore, the changes indicate a bug fix.

**Buggy**"
lucene-solr,30452.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,13 +1,10 @@
   public void setRangeValues(InetAddress min, InetAddress max) {
-    if (StringHelper.compare(BYTES, min.getAddress(), 0, max.getAddress(), 0) > 0) {
-      throw new IllegalArgumentException(""min value cannot be greater than max value for range field (name="" + name + "")"");
-    }
     final byte[] bytes;
     if (fieldsData == null) {
       bytes = new byte[BYTES*2];
       fieldsData = new BytesRef(bytes);
     } else {
       bytes = ((BytesRef)fieldsData).bytes;
     }
     encode(min, max, bytes);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests."" explicitly states that the commit addresses a bug related to the verification of minimum and maximum values in `InetAddressRange`, specifically focusing on the correct comparison between IPv4 and IPv6 addresses. The message also mentions updating tests, which is a good practice when fixing bugs.

**Git Diff Analysis:**

The diff shows a modification in the `setRangeValues` method. The original code included a comparison using `StringHelper.compare` to ensure that the minimum value is not greater than the maximum value. If the condition was true, an `IllegalArgumentException` was thrown.

The change removes this comparison entirely.

**Reasoning:**

The commit message indicates a bug in the min/max verification logic. The removal of the comparison suggests that the original comparison logic was flawed, possibly leading to incorrect rejections of valid ranges or acceptance of invalid ranges, especially when comparing IPv4 and IPv6 addresses. By removing the flawed check, the code now relies on the `encode` method to handle the range validation, or potentially defers the validation to a later stage or a different part of the code. The updated tests (mentioned in the commit message but not shown in the diff) would presumably verify the correct behavior of the `InetAddressRange` class with the corrected logic.

**Conclusion:**

Based on the commit message and the code modification (removal of the comparison logic), it's highly likely that this commit addresses a bug in the `InetAddressRange` class related to min/max value verification.

**Buggy**
"
lucene-solr,30453.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,4 +1,11 @@
   private static void encode(final InetAddress min, final InetAddress max, final byte[] bytes) {
-    System.arraycopy(InetAddressPoint.encode(min), 0, bytes, 0, BYTES);
-    System.arraycopy(InetAddressPoint.encode(max), 0, bytes, BYTES, BYTES);
+    // encode min and max value (consistent w/ InetAddressPoint encoding)
+    final byte[] minEncoded = InetAddressPoint.encode(min);
+    final byte[] maxEncoded = InetAddressPoint.encode(max);
+    // ensure min is lt max
+    if (StringHelper.compare(BYTES, minEncoded, 0, maxEncoded, 0) > 0) {
+      throw new IllegalArgumentException(""min value cannot be greater than max value for InetAddressRange field"");
+    }
+    System.arraycopy(minEncoded, 0, bytes, 0, BYTES);
+    System.arraycopy(maxEncoded, 0, bytes, BYTES, BYTES);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"The commit message indicates a bug fix related to the verification of minimum and maximum values in `InetAddressRange`, specifically addressing the comparison between IPv4 and IPv6 addresses. The code diff introduces a check to ensure that the minimum value is less than or equal to the maximum value before encoding them. If the minimum value is greater than the maximum value, an `IllegalArgumentException` is thrown. This addition directly addresses the bug described in the commit message by preventing incorrect range encoding. The added check `StringHelper.compare(BYTES, minEncoded, 0, maxEncoded, 0) > 0` validates the order of min and max values, which is crucial for correct range queries.

**Buggy**
"
lucene-solr,26824.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,14 +1,17 @@
   public void inform(ResourceLoader loader) {
     String stopTagFiles = args.get(""tags"");
     enablePositionIncrements = getBoolean(""enablePositionIncrements"", false);
+    stopTags = null;
     try {
       CharArraySet cas = getWordSet(loader, stopTagFiles, false);
-      stopTags = new HashSet<String>();
-      for (Object element : cas) {
-        char chars[] = (char[]) element;
-        stopTags.add(new String(chars));
+      if (cas != null) {
+        stopTags = new HashSet<String>();
+        for (Object element : cas) {
+          char chars[] = (char[]) element;
+          stopTags.add(new String(chars));
+        }
       }
     } catch (IOException e) {
       throw new InitializationException(""IOException thrown while loading tags"", e);
     }
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-2510: fix more factory arg bugs found by TestFactories"" suggests that the commit fixes bugs related to factory arguments, likely discovered through testing.

The code diff modifies the `inform` method. The key changes are:

1.  `stopTags = null;` is added before the try block. This initializes `stopTags` to null, ensuring a default value if an exception occurs or if `cas` is null.
2.  A null check `if (cas != null)` is added before processing the `cas` variable. This prevents a `NullPointerException` if `getWordSet` returns null.

The addition of the null check and initialization of `stopTags` to null strongly suggests a bug fix. Without these changes, the code could throw a `NullPointerException` if `getWordSet` returns null or if an exception is thrown before `stopTags` is initialized. The commit message and the code changes are aligned, indicating a bug fix related to factory arguments.

**Buggy**
"
lucene-solr,26825.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,3 +1,4 @@
   public TokenStream create(TokenStream stream) {
-    return new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
+    // if stoptags is null, it means the file is empty
+    return stopTags == null ? stream : new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-2510: fix more factory arg bugs found by TestFactories"" indicates that the commit aims to fix bugs related to factory arguments, which were discovered during testing.

The code diff introduces a null check for `stopTags` before creating a `JapanesePartOfSpeechStopFilter`. If `stopTags` is null, the original stream is returned, otherwise, the filter is created with the provided `stopTags`. This change suggests that the factory might have been producing incorrect results or throwing errors when `stopTags` was null, implying a bug fix. The null check prevents the `JapanesePartOfSpeechStopFilter` from being instantiated with a null `stopTags` argument, which could lead to a NullPointerException or other unexpected behavior.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
lucene-solr,46955.json,3b35de6599b12e08e5edd7549cd64c947cdb5a15,"@@ -1,12 +1,15 @@
                     public Number next() {
                       if (++curDoc >= maxDoc) {
                         throw new NoSuchElementException(""no more documents to return values for"");
                       }
                       Long updatedValue = updates.get(curDoc);
                       if (updatedValue == null) {
-                        updatedValue = Long.valueOf(currentValues.get(curDoc));
+                        // only read the current value if the document had a value before
+                        if (currentValues != null && docsWithField.get(curDoc)) {
+                          updatedValue = currentValues.get(curDoc);
+                        }
                       } else if (updatedValue == NumericUpdate.MISSING) {
                         updatedValue = null;
                       }
                       return updatedValue;
                     }",Buggy,"LUCENE-5189: fix updates-order and docsWithField bugs

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1528837 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5189: fix updates-order and docsWithField bugs"" indicates that the commit aims to fix bugs related to update ordering and the `docsWithField` functionality.

The code diff modifies the `next()` method within a class related to numeric updates. Specifically, it changes how the `updatedValue` is determined when it's not directly found in the `updates` map.

The original code unconditionally retrieved the current value using `currentValues.get(curDoc)` if `updatedValue` was null. The modified code adds a condition: it now only retrieves the current value if `currentValues` is not null and `docsWithField.get(curDoc)` returns true. This suggests that the original code might have been incorrectly retrieving values for documents that didn't previously have a value, potentially leading to incorrect results or unexpected behavior. The added condition `docsWithField.get(curDoc)` seems to address a bug where the code was not correctly checking if a document had a field before attempting to retrieve its value.

The commit message explicitly mentions a ""docsWithField bug,"" and the code change directly relates to how the `docsWithField` information is used to determine whether to retrieve the current value. This strongly suggests that the code change is indeed a bug fix.

**Buggy**"
lucene-solr,48196.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,28 +1,27 @@
   public static IntsRef getSingleton(Automaton a) {
     if (a.isDeterministic() == false) {
       throw new IllegalArgumentException(""input automaton must be deterministic"");
     }
     IntsRefBuilder builder = new IntsRefBuilder();
     HashSet<Integer> visited = new HashSet<>();
     int s = 0;
-    boolean done;
     Transition t = new Transition();
     while (true) {
       visited.add(s);
       if (a.isAccept(s) == false) {
         if (a.getNumTransitions(s) == 1) {
           a.getTransition(s, 0, t);
           if (t.min == t.max && !visited.contains(t.dest)) {
             builder.append(t.min);
             s = t.dest;
             continue;
           }
         }
       } else if (a.getNumTransitions(s) == 0) {
         return builder.get();
       }
 
       // Automaton accepts more than one string:
       return null;
     }
   }",NotBuggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-6365: fix buggy Operations.topoSort; add test"" indicates a bug fix in `Operations.topoSort` and the addition of a test case. The provided code diff is for `Automaton.getSingleton`, which seems unrelated to `Operations.topoSort`. However, the commit message also mentions ""fix buggy"" which implies a bug fix. The code changes involve checking if the automaton is deterministic, using a `HashSet` to track visited states, and traversing the automaton's transitions. The logic seems to be related to finding a single path or string accepted by the automaton. The original code had a `boolean done` variable which is removed, and the loop continues until a specific condition is met or the automaton accepts more than one string. The changes suggest a correction in the automaton traversal logic, potentially fixing a bug where the method would not correctly identify a singleton string or would enter an infinite loop.

**Buggy**"
lucene-solr,48202.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,26 +1,25 @@
   public static int[] topoSortStates(Automaton a) {
+    if (a.getNumStates() == 0) {
+      return new int[0];
+    }
     int numStates = a.getNumStates();
     int[] states = new int[numStates];
     final BitSet visited = new BitSet(numStates);
-    final LinkedList<Integer> worklist = new LinkedList<>();
-    worklist.add(0);
-    visited.set(0);
-    int upto = 0;
-    states[upto] = 0;
-    upto++;
-    Transition t = new Transition();
-    while (worklist.size() > 0) {
-      int s = worklist.removeFirst();
-      int count = a.initTransition(s, t);
-      for (int i=0;i<count;i++) {
-        a.getNextTransition(t);
-        if (!visited.get(t.dest)) {
-          visited.set(t.dest);
-          worklist.add(t.dest);
-          states[upto++] = t.dest;
-        }
-      }
+    int upto = topoSortStatesRecurse(a, visited, states, 0, 0);
+
+    if (upto < states.length) {
+      // There were dead states
+      int[] newStates = new int[upto];
+      System.arraycopy(states, 0, newStates, 0, upto);
+      states = newStates;
+    }
+
+    // Reverse the order:
+    for(int i=0;i<states.length/2;i++) {
+      int s = states[i];
+      states[i] = states[states.length-1-i];
+      states[states.length-1-i] = s;
     }
 
     return states;
   }",Buggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-6365: fix buggy Operations.topoSort; add test"" indicates a bug fix in the `topoSortStates` method of the `Operations` class. The diff shows significant changes to the implementation of the `topoSortStates` method. The original implementation used a breadth-first search (BFS) approach with a worklist. The updated implementation uses a recursive depth-first search (DFS) approach. Additionally, the updated code handles cases with dead states and reverses the order of the sorted states. The commit message explicitly states that the original implementation was buggy, and the code modifications reflect a substantial change in the algorithm used, suggesting a correction of a flawed logic.

Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,49563.json,e48f99c36cb1a5a2cca505266f886fa52fd2be5f,"@@ -1,8 +1,8 @@
   public static ReaderIterator getReaderIterator(DataInput in, int mem) throws IOException {
-    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_START);
+    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT);
     final int bitsPerValue = in.readVInt();
     assert bitsPerValue > 0 && bitsPerValue <= 64: ""bitsPerValue="" + bitsPerValue;
     final int valueCount = in.readVInt();
     final Format format = Format.byId(in.readVInt());
     return getReaderIteratorNoHeader(in, format, version, valueCount, bitsPerValue, mem);
   }",Buggy,"Fix error-prone header check.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1359861 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix error-prone header check"" suggests that the commit addresses an issue in the header check mechanism.

The diff shows a change in the `CodecUtil.checkHeader` method call. Specifically, the upper bound of the acceptable version is changed from `VERSION_START` to `VERSION_CURRENT`. This indicates that the previous implementation was likely too strict, only accepting a specific version (`VERSION_START`) instead of a range of versions (from `VERSION_START` to `VERSION_CURRENT`). This could have led to errors when reading data written with newer versions of the codec, thus indicating a bug fix.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
lucene-solr,13424.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,37 +1,38 @@
   public SlotAcc createSlotAcc(FacetContext fcontext, int numDocs, int numSlots) throws IOException {
     ValueSource vs = getArg();
 
     SchemaField sf = null;
 
     if (vs instanceof FieldNameValueSource) {
       String field = ((FieldNameValueSource)vs).getFieldName();
       sf = fcontext.qcontext.searcher().getSchema().getField(field);
 
       if (sf.multiValued() || sf.getType().multiValuedFieldCache()) {
         vs = null;
         throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ""min/max aggregations can't be used on multi-valued field "" + field);
       } else {
         vs = sf.getType().getValueSource(sf, null);
       }
     }
 
     if (vs instanceof StrFieldSource) {
       return new SingleValuedOrdAcc(fcontext, sf, numSlots);
     }
 
     // Since functions don't currently have types, we rely on the type of the field
     if (sf != null && sf.getType().getNumberType() != null) {
       switch (sf.getType().getNumberType()) {
         case FLOAT:
         case DOUBLE:
           return new DFuncAcc(vs, fcontext, numSlots);
         case INTEGER:
         case LONG:
-        case DATE:
           return new LFuncAcc(vs, fcontext, numSlots);
+        case DATE:
+          return new DateFuncAcc(vs, fcontext, numSlots);
       }
     }
 
     // numeric functions
     return new DFuncAcc(vs, fcontext, numSlots);
   }",NotBuggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"The commit message ""SOLR-11316: date support for min/max, fix missing bug for int/long fields"" indicates two things: adding date support for min/max aggregations and fixing a bug related to int/long fields.

The code diff shows a change in the `createSlotAcc` method. Specifically, within the `if (sf != null && sf.getType().getNumberType() != null)` block, the `switch` statement now includes a case for `DATE`. In the original code, `INTEGER`, `LONG`, and `DATE` all resulted in the creation of an `LFuncAcc`. Now, `DATE` results in the creation of a `DateFuncAcc`. This change directly addresses the ""date support for min/max"" part of the commit message.

The original code treated DATE fields the same as INTEGER and LONG fields, which could lead to incorrect results or errors when performing min/max aggregations on date fields. The introduction of `DateFuncAcc` suggests a specialized accumulator for date values, implying a bug fix related to how date fields were previously handled.

The commit message also mentions a ""missing bug for int/long fields"". While the diff doesn't directly show changes to how INTEGER or LONG fields are handled, the original code's grouping of INTEGER, LONG, and DATE into the same `LFuncAcc` might have been the source of the bug. By separating DATE into its own `DateFuncAcc`, the potential bug related to int/long fields might have been indirectly addressed.

Therefore, the code changes align with the commit message, and the introduction of `DateFuncAcc` to handle date fields separately indicates a bug fix.

**Buggy**
"
lucene-solr,13439.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,8 +1,3 @@
     public Object getValue(int slot) {
-      long val = result[slot];
-      if (val == 0 && exists.get(slot)) {
-        return null;
-      } else {
-        return val;
-      }
+      return result[slot] == MISSING ? null : new Date(result[slot]);
     }",Buggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-11316: date support for min/max, fix missing bug for int/long fields"" suggests two primary goals:

1.  **Date Support for min/max:** This implies adding or improving functionality to handle date values when calculating minimum or maximum values.
2.  **Fix missing bug for int/long fields:** This explicitly states the correction of a bug related to integer or long fields. The phrase ""missing bug"" is a bit unusual, but it likely means a bug that was previously overlooked or not properly addressed.

**Code Diff Analysis:**

The code diff shows a change within a `getValue(int slot)` method. Let's break down the changes:

*   **Original Code:**
    ```java
    public Object getValue(int slot) {
        long val = result[slot];
        if (val == 0 && exists.get(slot)) {
            return null;
        } else {
            return val;
        }
    }
    ```
    This code retrieves a `long` value from a `result` array at the given `slot`. It checks if the value is 0 and if an `exists` flag is true for that slot. If both are true, it returns `null`; otherwise, it returns the `long` value.

*   **Modified Code:**
    ```java
    public Object getValue(int slot) {
        return result[slot] == MISSING ? null : new Date(result[slot]);
    }
    ```
    This code now checks if the value in the `result` array at the given `slot` is equal to a constant `MISSING`. If it is, it returns `null`; otherwise, it creates a new `Date` object using the `long` value from the `result` array and returns the `Date` object.

**Reasoning:**

1.  **Date Support:** The change from returning a `long` to returning a `Date` object strongly suggests the addition of date support. The `long` value is now being interpreted as milliseconds since the epoch, which is the standard way to represent dates in Java.

2.  **Bug Fix:** The original code had a specific condition for returning `null`: `val == 0 && exists.get(slot)`. This likely intended to handle cases where a value was considered ""missing"" or ""non-existent."" However, the modified code replaces this with a simpler check against a `MISSING` constant. This suggests that the original logic was either incorrect or incomplete, leading to a bug where valid `0` values were incorrectly treated as missing, or that the `exists` flag was not correctly representing the presence of a value. The introduction of the `MISSING` constant and the direct `Date` conversion implies a more robust and correct handling of missing or invalid date values. The commit message explicitly mentions a bug fix for int/long fields, and this change aligns with that. The original code's logic of checking for `0` and `exists.get(slot)` seems like a potential source of error, especially if `0` is a valid value in some cases. Replacing it with `MISSING` makes the code more robust.

**Conclusion:**

The code diff clearly shows a change related to date handling and the simplification of logic for determining missing values. This, combined with the commit message explicitly mentioning a bug fix for int/long fields, strongly indicates that the changes are indeed a bug fix.

**Buggy**
"
lucene-solr,17606.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,54 +1,54 @@
   public void checkSystemCollection() {
     if (cloudManager != null) {
       try {
         if (cloudManager.isClosed() || Thread.interrupted()) {
           factory.setPersistent(false);
           return;
         }
         ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();
         DocCollection systemColl = clusterState.getCollectionOrNull(CollectionAdminParams.SYSTEM_COLL);
         if (systemColl == null) {
           if (logMissingCollection) {
-            log.warn(""Missing "" + CollectionAdminParams.SYSTEM_COLL + "", keeping metrics history in memory"");
+            log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
             logMissingCollection = false;
           }
           factory.setPersistent(false);
           return;
         } else {
           boolean ready = false;
           for (Replica r : systemColl.getReplicas()) {
             if (r.isActive(clusterState.getLiveNodes())) {
               ready = true;
               break;
             }
           }
           if (!ready) {
-            log.debug(CollectionAdminParams.SYSTEM_COLL + "" not ready yet, keeping metrics history in memory"");
+            log.debug(CollectionAdminParams.SYSTEM_COLL + ""collection not ready yet, keeping metrics history in memory"");
             factory.setPersistent(false);
             return;
           }
         }
       } catch (Exception e) {
         if (logMissingCollection) {
           log.warn(""Error getting cluster state, keeping metrics history in memory"", e);
         }
         logMissingCollection = false;
         factory.setPersistent(false);
         return;
       }
       logMissingCollection = true;
       factory.setPersistent(true);
     } else {
       try {
         solrClient.query(CollectionAdminParams.SYSTEM_COLL, new SolrQuery(CommonParams.Q, ""*:*"", CommonParams.ROWS, ""0""));
         factory.setPersistent(true);
         logMissingCollection = true;
       } catch (Exception e) {
         if (logMissingCollection) {
-          log.warn(""Error querying .system collection, keeping metrics history in memory"", e);
+          log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
         }
         logMissingCollection = false;
         factory.setPersistent(false);
       }
     }
   }",NotBuggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",Buggy,"The commit message indicates two changes: reducing logging and fixing an index size conversion bug.

The provided diff focuses on the first part of the commit message: reducing logging. The changes replace `log.warn` with `log.info` in two places when the system collection is missing or when there's an error querying it. This change directly relates to reducing the severity of log messages in specific scenarios, aligning with the ""reduce logging"" part of the commit message. The diff doesn't show any changes related to index size conversion. However, the commit message mentions it, implying that part of the fix is in another diff not provided.

The change from `log.warn` to `log.info` suggests that the original logging level was deemed too high for the given situations (missing system collection or query errors). This implies that the previous logging behavior was considered a bug or at least undesirable.

Therefore, the changes indicate a bug fix related to excessive logging.

**Buggy**"
lucene-solr,17615.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,149 +1,145 @@
   private void collectGlobalMetrics() {
     if (!amIOverseerLeader()) {
       return;
     }
     Set<String> nodes = new HashSet<>(cloudManager.getClusterStateProvider().getLiveNodes());
     NodeStateProvider nodeStateProvider = cloudManager.getNodeStateProvider();
     Set<String> collTags = new HashSet<>();
     collTags.addAll(counters.get(Group.core.toString()));
     collTags.addAll(gauges.get(Group.core.toString()));
 
     Set<String> nodeTags = new HashSet<>();
     String nodePrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.node) + "":"";
     counters.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     gauges.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     String jvmPrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.jvm) + "":"";
     counters.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
     gauges.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
 
     // per-registry totals
     // XXX at the moment the type of metrics that we collect allows
     // adding all partial values. At some point it may be necessary to implement
     // other aggregation functions.
     // group : registry : name : value
     Map<Group, Map<String, Map<String, Number>>> totals = new HashMap<>();
 
     // collect and aggregate per-collection totals
     for (String node : nodes) {
       if (cloudManager.isClosed() || Thread.interrupted()) {
         return;
       }
       // add core-level stats
       Map<String, Map<String, List<ReplicaInfo>>> infos = nodeStateProvider.getReplicaInfo(node, collTags);
       infos.forEach((coll, shards) -> {
         shards.forEach((sh, replicas) -> {
           String registry = SolrMetricManager.getRegistryName(Group.collection, coll);
           Map<String, Number> perReg = totals
               .computeIfAbsent(Group.collection, g -> new HashMap<>())
               .computeIfAbsent(registry, r -> new HashMap<>());
           replicas.forEach(ri -> {
             collTags.forEach(tag -> {
               double value = ((Number)ri.getVariable(tag, 0.0)).doubleValue();
-              // TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion
-              if (tag.contains(Suggestion.coreidxsize)) {
-                value = value * 1024.0 * 1024.0 * 1024.0;
-              }
               DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(tag, t -> new DoubleAdder());
               adder.add(value);
             });
           });
         });
       });
       // add node-level stats
       Map<String, Object> nodeValues = nodeStateProvider.getNodeValues(node, nodeTags);
       for (Group g : Arrays.asList(Group.node, Group.jvm)) {
         String registry = SolrMetricManager.getRegistryName(g);
         Map<String, Number> perReg = totals
             .computeIfAbsent(g, gr -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Set<String> names = new HashSet<>();
         names.addAll(counters.get(g.toString()));
         names.addAll(gauges.get(g.toString()));
         names.forEach(name -> {
           String tag = ""metrics:"" + registry + "":"" + name;
           double value = ((Number)nodeValues.getOrDefault(tag, 0.0)).doubleValue();
           DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(name, t -> new DoubleAdder());
           adder.add(value);
         });
       }
     }
 
     // add numNodes
     String nodeReg = SolrMetricManager.getRegistryName(Group.node);
     Map<String, Number> perNodeReg = totals
         .computeIfAbsent(Group.node, gr -> new HashMap<>())
         .computeIfAbsent(nodeReg, r -> new HashMap<>());
     perNodeReg.put(NUM_NODES_KEY, nodes.size());
 
     // add some global collection-level stats
     try {
       ClusterState state = cloudManager.getClusterStateProvider().getClusterState();
       state.forEachCollection(coll -> {
         String registry = SolrMetricManager.getRegistryName(Group.collection, coll.getName());
         Map<String, Number> perReg = totals
             .computeIfAbsent(Group.collection, g -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Collection<Slice> slices = coll.getActiveSlices();
         perReg.put(NUM_SHARDS_KEY, slices.size());
         DoubleAdder numActiveReplicas = new DoubleAdder();
         slices.forEach(s -> {
           s.forEach(r -> {
             if (r.isActive(state.getLiveNodes())) {
               numActiveReplicas.add(1.0);
             }
           });
         });
         perReg.put(NUM_REPLICAS_KEY, numActiveReplicas);
       });
     } catch (IOException e) {
       log.warn(""Exception getting cluster state"", e);
     }
 
     // now update the db-s
     totals.forEach((group, perGroup) -> {
       perGroup.forEach((reg, perReg) -> {
         RrdDb db = getOrCreateDb(reg, group);
         if (db == null) {
           return;
         }
         try {
           // set the timestamp
           Sample s = db.createSample(TimeUnit.SECONDS.convert(timeSource.getEpochTimeNs(), TimeUnit.NANOSECONDS));
           AtomicBoolean dirty = new AtomicBoolean(false);
           List<Group> groups = new ArrayList<>();
           groups.add(group);
           if (group == Group.collection) {
             groups.add(Group.core);
           }
           for (Group g : groups) {
             counters.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
             gauges.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
           }
           if (dirty.get()) {
             s.update();
           }
         } catch (Exception e) {
         }
       });
     });
   }",Buggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",Buggy,"The commit message ""SOLR-11779: Reduce logging, fix index size conversion bug."" indicates two changes: reducing logging and fixing an index size conversion bug.

The provided diff removes the following code block:

```java
// TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion
-              if (tag.contains(Suggestion.coreidxsize)) {
-                value = value * 1024.0 * 1024.0 * 1024.0;
-              }
```

This code block appears to be a temporary fix or workaround for an incorrect index size conversion. The comment ""TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion"" suggests that the proper fix should be in `Suggestion.Condition.DISK_IDX`, and this block is a temporary workaround. Removing this block indicates that the proper conversion is now handled elsewhere, thus fixing the index size conversion bug.

The diff doesn't show any changes related to reducing logging. However, the removal of the index size conversion workaround directly addresses the ""fix index size conversion bug"" part of the commit message.

Therefore, the provided diff indicates a bug fix.

**Buggy**"
lucene-solr,50428.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,22 +1,23 @@
   private GeoPoint[] findAdjoiningPoints(final Plane plane, final GeoPoint pointOnPlane, final Plane envelopePlane) {
     // Compute a normalized perpendicular vector
     final Vector perpendicular = new Vector(plane, pointOnPlane);
     double distanceFactor = 0.0;
     for (int i = 0; i < MAX_ITERATIONS; i++) {
       distanceFactor += DELTA_DISTANCE;
       // Compute two new points along this vector from the original
       final GeoPoint pointA = planetModel.createSurfacePoint(pointOnPlane.x + perpendicular.x * distanceFactor,
         pointOnPlane.y + perpendicular.y * distanceFactor,
         pointOnPlane.z + perpendicular.z * distanceFactor);
       final GeoPoint pointB = planetModel.createSurfacePoint(pointOnPlane.x - perpendicular.x * distanceFactor,
         pointOnPlane.y - perpendicular.y * distanceFactor,
         pointOnPlane.z - perpendicular.z * distanceFactor);
       if (Math.abs(envelopePlane.evaluate(pointA)) > OFF_PLANE_AMOUNT && Math.abs(envelopePlane.evaluate(pointB)) > OFF_PLANE_AMOUNT) {
         //System.out.println(""Distance: ""+computeSquaredDistance(rval[0], pointOnPlane)+"" and ""+computeSquaredDistance(rval[1], pointOnPlane));
         return new GeoPoint[]{pointA, pointB};
       }
       // Loop back around and use a bigger delta
     }
     // Had to abort, so return null.
+    //System.out.println(""     Adjoining points not found.  Are planes parallel?  edge = ""+plane+""; envelope = ""+envelopePlane+""; perpendicular = ""+perpendicular);
     return null;
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix related to how travel planes close to the edge of the world are handled. The disallowed window size is also increased.

The code diff adds a debug `println` statement when adjoining points are not found. This suggests that the original code might have failed to find adjoining points in certain scenarios, leading to incorrect behavior. The debug statement helps diagnose cases where planes might be parallel, which could be a cause of the failure. The original code returned `null` when it couldn't find the points.

Based on the commit message and the code diff, the changes address a bug where travel planes too close to the edge of the world were not being handled correctly. The increased disallowed window size and the debug statement further support this conclusion.

**Buggy**
"
lucene-solr,50391.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,46 +1,47 @@
     public boolean apply(final GeoPoint testPoint, final boolean testPointInSet,
       final double x, final double y, final double z) {
       // First, try with two individual legs.  If that doesn't work, try the DualCrossingIterator.
       try {
         // First, we'll determine if the intersection point is in set or not
         //System.out.println("" Finding whether ""+intersectionPoint+"" is in-set, based on travel from ""+testPoint+"" along ""+firstLegPlane+"" (value=""+firstLegValue+"")"");
         final CountingEdgeIterator testPointEdgeIterator = createLinearCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           intersectionPoint.x, intersectionPoint.y, intersectionPoint.z);
         // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
         firstLegTree.traverse(testPointEdgeIterator, firstLegValue);
         final boolean intersectionPointOnEdge = testPointEdgeIterator.isOnEdge();
         // If the intersection point is on the edge, we cannot use this combination of legs, since it's not logically possible to compute in-set or out-of-set
         // with such a starting point.
         if (intersectionPointOnEdge) {
           throw new IllegalArgumentException(""Intersection point landed on an edge -- illegal path"");
         }
         final boolean intersectionPointInSet = intersectionPointOnEdge || (((testPointEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
         
         //System.out.println(""  Intersection point in-set? ""+intersectionPointInSet+"" On edge? ""+intersectionPointOnEdge);
 
         // Now do the final leg
         //System.out.println("" Finding whether [""+x+"",""+y+"",""+z+""] is in-set, based on travel from ""+intersectionPoint+"" along ""+secondLegPlane+"" (value=""+secondLegValue+"")"");
         final CountingEdgeIterator travelEdgeIterator = createLinearCrossingEdgeIterator(intersectionPoint,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z);
         // Traverse our way from the test point to the check point.
         secondLegTree.traverse(travelEdgeIterator, secondLegValue);
         final boolean rval = travelEdgeIterator.isOnEdge() || (((travelEdgeIterator.getCrossingCount() & 1) == 0)?intersectionPointInSet:!intersectionPointInSet);
         
         //System.out.println("" Check point in set? ""+rval);
         return rval;
       } catch (IllegalArgumentException e) {
         // Intersection point apparently was on edge, so try another strategy
+        //System.out.println("" Trying dual crossing edge iterator"");
         final CountingEdgeIterator edgeIterator = new DualCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z, intersectionPoint);
         firstLegTree.traverse(edgeIterator, firstLegValue);
         if (edgeIterator.isOnEdge()) {
           return true;
         }
         secondLegTree.traverse(edgeIterator, secondLegValue);
         return edgeIterator.isOnEdge() || (((edgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
       }
     }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The code diff shows a change in the `apply` method of a class related to geospatial calculations. The original code had a `try-catch` block that caught `IllegalArgumentException`. The catch block now includes a commented-out `System.out.println` statement. The `IllegalArgumentException` is thrown when the intersection point lands on an edge, indicating an illegal path. The catch block then attempts an alternative strategy using `DualCrossingEdgeIterator`. The commit message ""LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude"" suggests that the original implementation had issues with handling travel planes near the edge of the world, and the changes aim to fix these problems. The `IllegalArgumentException` and the alternative strategy using `DualCrossingEdgeIterator` indicate that the original code had a bug related to edge cases.

**Buggy**
"
lucene-solr,50389.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,17 +1,19 @@
   private CountingEdgeIterator createLinearCrossingEdgeIterator(final GeoPoint testPoint,
     final Plane plane, final Plane abovePlane, final Plane belowPlane,
     final double thePointX, final double thePointY, final double thePointZ) {
     // If thePoint and testPoint are parallel, we won't be able to determine sidedness of the bounding planes.  So detect that case, and build the iterator differently if we find it.
     // This didn't work; not sure why not:
     //if (testPoint.isParallel(thePointX, thePointY, thePointZ)) {
     //  return new FullLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //}
     //return new SectorLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //
     try {
+      //System.out.println("" creating sector linear crossing edge iterator"");
       return new SectorLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     } catch (IllegalArgumentException e) {
       // Assume we failed because we could not construct bounding planes, so do it another way.
+      //System.out.println("" create full linear crossing edge iterator"");
       return new FullLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     }
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix related to how travel planes close to the edge of the world are handled. The code diff shows a change in the `createLinearCrossingEdgeIterator` method. Specifically, it attempts to create a `SectorLinearCrossingEdgeIterator` and, if an `IllegalArgumentException` occurs, it falls back to creating a `FullLinearCrossingEdgeIterator`. The `IllegalArgumentException` suggests that the initial attempt to create the `SectorLinearCrossingEdgeIterator` can fail under certain conditions, and the fallback mechanism is intended to handle those cases. The added comments also suggest debugging efforts related to understanding when each iterator is created. This aligns with the commit message's intention to fix problems related to travel planes near the edge of the world. The exception handling and fallback mechanism strongly suggest a bug fix.

**Buggy**
"
lucene-solr,50383.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,242 +1,242 @@
   private boolean isInSet(final double x, final double y, final double z,
     final GeoPoint testPoint,
     final boolean testPointInSet,
     final Plane testPointFixedXPlane, final Plane testPointFixedXAbovePlane, final Plane testPointFixedXBelowPlane,
     final Plane testPointFixedYPlane, final Plane testPointFixedYAbovePlane, final Plane testPointFixedYBelowPlane,
     final Plane testPointFixedZPlane, final Plane testPointFixedZAbovePlane, final Plane testPointFixedZBelowPlane) {
 
     //System.out.println(""\nIsInSet called for [""+x+"",""+y+"",""+z+""], testPoint=""+testPoint+""; is in set? ""+testPointInSet);
     // If we're right on top of the point, we know the answer.
     if (testPoint.isNumericallyIdentical(x, y, z)) {
       return testPointInSet;
     }
     
     // If we're right on top of any of the test planes, we navigate solely on that plane.
     if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && testPointFixedYPlane.evaluateIsZero(x, y, z)) {
       // Use the XZ plane exclusively.
       //System.out.println("" Using XZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the y tree because that's fixed.
       yTree.traverse(crossingEdgeIterator, testPoint.y);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && testPointFixedXPlane.evaluateIsZero(x, y, z)) {
       // Use the YZ plane exclusively.
       //System.out.println("" Using YZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the x tree because that's fixed.
       xTree.traverse(crossingEdgeIterator, testPoint.x);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && testPointFixedZPlane.evaluateIsZero(x, y, z)) {
       //System.out.println("" Using XY plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
       zTree.traverse(crossingEdgeIterator, testPoint.z);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else {
       //System.out.println("" Using two planes"");
       // This is the expensive part!!
       // Changing the code below has an enormous impact on the queries per second we see with the benchmark.
       
       // We need to use two planes to get there.  We don't know which two planes will do it but we can figure it out.
       final Plane travelPlaneFixedX = new Plane(1.0, 0.0, 0.0, -x);
       final Plane travelPlaneFixedY = new Plane(0.0, 1.0, 0.0, -y);
       final Plane travelPlaneFixedZ = new Plane(0.0, 0.0, 1.0, -z);
 
       Plane fixedYAbovePlane = new Plane(travelPlaneFixedY, true);
-      if (fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedYAbovePlane = null;
       }
       
       Plane fixedYBelowPlane = new Plane(travelPlaneFixedY, false);
-      if (fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedYBelowPlane = null;
       }
       
       Plane fixedXAbovePlane = new Plane(travelPlaneFixedX, true);
-      if (fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedXAbovePlane = null;
       }
       
       Plane fixedXBelowPlane = new Plane(travelPlaneFixedX, false);
-      if (fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedXBelowPlane = null;
       }
       
       Plane fixedZAbovePlane = new Plane(travelPlaneFixedZ, true);
-      if (fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedZAbovePlane = null;
       }
       
       Plane fixedZBelowPlane = new Plane(travelPlaneFixedZ, false);
-      if (fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedZBelowPlane = null;
       }
 
       // Find the intersection points for each one of these and the complementary test point planes.
 
       final List<TraversalStrategy> traversalStrategies = new ArrayList<>(12);
       
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsY = travelPlaneFixedX.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : XIntersectionsY) {
             // Travel would be in YZ plane (fixed x) then in XZ (fixed y)
             // We compute distance we need to travel as a placeholder for the number of intersections we might encounter.
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, x,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               yTree, xTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsZ = travelPlaneFixedX.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : XIntersectionsZ) {
             // Travel would be in YZ plane (fixed x) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, x,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               zTree, xTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsX = travelPlaneFixedY.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : YIntersectionsX) {
             // Travel would be in XZ plane (fixed y) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, y,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               xTree, yTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsZ = travelPlaneFixedY.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : YIntersectionsZ) {
             // Travel would be in XZ plane (fixed y) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, y,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               zTree, yTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsX = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : ZIntersectionsX) {
             // Travel would be in XY plane (fixed z) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, z,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               xTree, zTree, p));
           }
         }
       }
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsY = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : ZIntersectionsY) {
             // Travel would be in XY plane (fixed z) then in XZ (fixed y)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, z,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               yTree, zTree, p));
           }
         }
       }
 
       Collections.sort(traversalStrategies);
       
       if (traversalStrategies.size() == 0) {
         throw new IllegalArgumentException(""No dual-plane travel strategies were found"");
       }
 
       // Loop through travel strategies, in order, until we find one that works.
       for (final TraversalStrategy ts : traversalStrategies) {
         try {
           return ts.apply(testPoint, testPointInSet, x, y, z);
         } catch (IllegalArgumentException e) {
           // Continue
         }
       }
       
       throw new IllegalArgumentException(""Exhausted all traversal strategies"");
     }
   }",Buggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message indicates a fix related to how travel planes close to the edge of the world are handled. The message also mentions increasing the size of the disallowed window.

The code diff modifies the conditions under which `fixedXAbovePlane`, `fixedXBelowPlane`, `fixedYAbovePlane`, `fixedYBelowPlane`, `fixedZAbovePlane`, and `fixedZBelowPlane` are set to null. The original code had:
```java
if (fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYAbovePlane.D > NEAR_EDGE_CUTOFF)
```
The modified code has:
```java
if (-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYAbovePlane.D > NEAR_EDGE_CUTOFF)
```
The change involves negating `fixedYAbovePlane.D` in the first part of the condition and adding `fixedYAbovePlane.D` in the second part. This pattern is consistently applied to all six planes. This suggests a correction in how the distance from the plane to the edge of the world is calculated. The commit message also mentions increasing the size of the disallowed window, which isn't directly visible in the diff, but could be related to the value of `NEAR_EDGE_CUTOFF` which is not shown in the diff.

Based on the commit message and the code changes, it's clear that the commit addresses a bug related to the handling of travel planes near the edge of the world.

**Buggy**
"
lucene-solr,28702.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,132 +1,134 @@
   public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermNonLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
         ste.termExists = subCode == 0;
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       final int code = suffixesReader.readVInt();
       suffix = code >>> 1;
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: "" + ((code&1)==1 ? ""sub-block"" : ""term"") + "" "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       ste.termExists = (code & 1) == 0;
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
+      // Must save ord before we skip over a sub-block in case we push, below:
+      long prevTermOrd = termOrd;
       if (ste.termExists) {
         state.termBlockOrd++;
         termOrd++;
         subCode = 0;
       } else {
         subCode = suffixesReader.readVLong();
         termOrd += suffixesReader.readVLong();
         lastSubFP = fp - subCode;
       }
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
               //termExists = true;
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
           if (!exactOnly && !ste.termExists) {
             // We are on a sub-block, and caller wants
             // us to position to the next term after
             // the target, so we must recurse into the
             // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
+            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, prevTermOrd);
             ste.currentFrame.loadBlock();
             while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
+              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, prevTermOrd);
               ste.currentFrame.loadBlock();
             }
           }
                 
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",Buggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5819: fix ord bug; add test case; remove dead code"" indicates a bug fix related to ordinals (""ord bug""). The diff shows a change in the `scanToTermNonLeaf` method within a class related to term enumeration. Specifically, the line `ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);` is modified to `ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, prevTermOrd);`. This suggests that the term ordinal being passed to `pushFrame` was incorrect, and the change replaces it with `prevTermOrd`. The comment ""// Must save ord before we skip over a sub-block in case we push, below:"" further supports this interpretation. The addition of a test case mentioned in the commit message is not visible in this diff, but the code change clearly addresses a bug related to term ordinals.

**Buggy**
"
lucene-solr,42320.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,123 +1,110 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + brToString(target) + "" term="" + brToString(term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5819: fix ord bug; add test case; remove dead code"" indicates a bug fix related to term ordinals. The diff shows the removal of code within the `if (!exactOnly && !ste.termExists)` block. This code appears to be related to recursively descending into sub-blocks when an exact match is not required and the current entry is a sub-block. Removing this code suggests a potential issue with how sub-blocks were handled in non-exact seek operations, which aligns with the ""fix ord bug"" part of the commit message. The removal of dead code further supports the idea of a bug fix.

**Buggy**
"
lucene-solr,28701.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,124 +1,111 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
       termOrd++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //    BytesRef suffixBytesRef = new BytesRef();
       //    suffixBytesRef.bytes = suffixBytes;
       //    suffixBytesRef.offset = suffixesReader.getPosition();
       //    suffixBytesRef.length = suffix;
       //    System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + OrdsSegmentTermsEnum.brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to term ordinals (""fix ord bug""), the addition of a test case, and the removal of dead code.

The diff shows the removal of code within the `if (!exactOnly && !ste.termExists)` block. This block appears to have been intended to handle the case where an inexact seek lands on a sub-block, and the caller wants to position to the next term after the target. The code recursively pushed frames and loaded blocks. Its removal suggests that this logic was either incorrect or unnecessary, indicating a bug fix. The removal of dead code, as stated in the commit message, further supports this.

The change also includes `termOrd++` inside the `while (true)` loop. This incrementing of `termOrd` is strongly related to fixing an ordinal bug.

Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,20313.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,11 +1,6 @@
-  private IndexSchema initSchema(SolrConfig config, IndexSchema schema) {
+  private void initSchema(SolrConfig config, IndexSchema schema) {
     if (schema == null) {
       schema = IndexSchemaFactory.buildIndexSchema(IndexSchema.DEFAULT_SCHEMA_FILE, config);
     }
-    final SimilarityFactory similarityFactory = schema.getSimilarityFactory();
-    if (similarityFactory instanceof SolrCoreAware) {
-      // Similarity needs SolrCore before inform() is called on all registered SolrCoreAware listeners below
-      ((SolrCoreAware) similarityFactory).inform(this);
-    }
-    return schema;
+    setLatestSchema(schema);
   }",Buggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features"" clearly states that a bug related to `SimilarityFactory` initialization was fixed. The bug specifically affected `SolrCoreAware` factories when used with managed schema features. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `initSchema` method.  The original code snippet included the following:

```java
final SimilarityFactory similarityFactory = schema.getSimilarityFactory();
if (similarityFactory instanceof SolrCoreAware) {
  // Similarity needs SolrCore before inform() is called on all registered SolrCoreAware listeners below
  ((SolrCoreAware) similarityFactory).inform(this);
}
return schema;
```

This code was attempting to initialize `SolrCoreAware` `SimilarityFactory` instances by calling their `inform()` method with the current `SolrCore` instance. The diff removes this code block entirely and replaces `return schema;` with `setLatestSchema(schema);`.

**Reasoning:**

The commit message and the code diff are highly related. The commit message explicitly mentions a bug in `SimilarityFactory` initialization, particularly for `SolrCoreAware` factories. The diff shows the removal of code that was specifically trying to initialize `SolrCoreAware` `SimilarityFactory` instances. The removal of this code suggests that the original approach was flawed and causing the bug described in the commit message. The fact that the code was removed rather than modified further suggests that the original approach was fundamentally incorrect. The new approach seems to be setting the schema as the latest schema.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix related to the initialization of `SolrCoreAware` `SimilarityFactory` instances.

**Buggy**
"
lucene-solr,12746.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,4 +1,6 @@
   public Similarity getSimilarity() {
-    assert core != null : ""inform must be called first"";
+    if (null == core) {
+      throw new IllegalStateException(""SchemaSimilarityFactory can not be used until SolrCoreAware.inform has been called"");
+    }
     return similarity;
   }",NotBuggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features"" explicitly states that a bug has been fixed. The bug is related to the initialization of `SimilarityFactory` and its interaction with `SolrCoreAware` factories, specifically in the context of managed schema features. This suggests that the previous implementation had an issue that prevented these factories from working correctly.

**Git Diff Analysis:**

The diff shows a change within the `getSimilarity()` method.  The original code had an `assert core != null` statement. This assertion would cause the program to halt if `core` was null. The modified code replaces the assertion with an `if (null == core)` check that throws an `IllegalStateException` if `core` is null.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly mentions a bug fix.
2.  **Error Handling Improvement:** The change from `assert` to `if` and throwing `IllegalStateException` indicates an improvement in error handling. Assertions are typically disabled in production environments, meaning the original code would potentially fail silently if `core` was null. The new code throws an exception, providing a clearer indication of the problem.
3.  **Initialization Issue:** The commit message mentions a problem with `SimilarityFactory` initialization. The code change directly addresses a scenario where the `core` variable, presumably initialized during the factory's initialization process, is null when `getSimilarity()` is called. This aligns with the described bug.
4. **SolrCoreAware:** The commit message mentions SolrCoreAware factories. SolrCoreAware is an interface that allows components to be initialized with a SolrCore instance. The `core` variable in the diff likely represents the SolrCore instance. The change ensures that the SolrCore instance is available before the similarity is accessed, which is crucial for SolrCoreAware factories to function correctly.

**Conclusion:**

The commit message and the code changes are consistent with each other and indicate a bug fix related to the initialization of `SimilarityFactory` and its interaction with `SolrCoreAware` components. The change improves error handling and ensures that the `core` variable is properly initialized before being used.

**Buggy**
"
lucene-solr,43373.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,18 +1,16 @@
   private static final boolean isIntersectingPolygon(final Node start, final double x0, final double y0,
                                                      final double x1, final double y1) {
     Node node = start;
     Node nextNode;
     do {
       nextNode = node.next;
-      if(node.getX() != x0 && node.getY() != y0 && nextNode.getX() != x0
-          && nextNode.getY() != y0 && node.getX() != x1 && node.getY() != y1
-          && nextNode.getX() != x1 && nextNode.getY() != y1) {
+      if(isVertexEquals(node, x0, y0) == false && isVertexEquals(node, x1, y1) == false) {
         if (linesIntersect(node.getX(), node.getY(), nextNode.getX(), nextNode.getY(), x0, y0, x1, y1)) {
           return true;
         }
       }
       node = nextNode;
     } while (node != start);
 
     return false;
   }",Buggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message ""LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections"" suggests a bug fix related to polygon intersection checks.

The code diff modifies the `isIntersectingPolygon` method. The original code had a complex `if` condition:

```java
if(node.getX() != x0 && node.getY() != y0 && nextNode.getX() != x0
          && nextNode.getY() != y0 && node.getX() != x1 && node.getY() != y1
          && nextNode.getX() != x1 && nextNode.getY() != y1)
```

This condition appears to be intended to skip checking the intersection of the given line segment (x0, y0) to (x1, y1) with polygon edges that share a vertex with the line segment. However, the original condition uses strict inequality (`!=`). This could lead to skipping edges that should be checked for intersection, especially if floating-point numbers are involved and vertices are very close but not exactly equal.

The modified code replaces this complex condition with:

```java
if(isVertexEquals(node, x0, y0) == false && isVertexEquals(node, x1, y1) == false)
```

This new condition uses a helper function `isVertexEquals` (not shown in the diff, but presumably compares vertices for equality with some tolerance). This change suggests an attempt to fix the original logic by correctly identifying and skipping only the edges that share a vertex with the line segment being checked. The original condition was too strict and could skip edges that should have been included in the intersection check, which aligns with the commit message indicating a bug fix where polygon edges were skipped.

Therefore, the changes in the code diff align with the commit message and indicate a bug fix.

**Buggy**"
lucene-solr,43381.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
   private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+    return isVertexEquals(a, b.getX(), b.getY());
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message indicates a bug fix related to polygon edge intersection checks. The code change replaces a direct comparison of `Node` objects' x and y coordinates with a call to `isVertexEquals(a, b.getX(), b.getY())`. This suggests that the original comparison might have been flawed, potentially leading to incorrect intersection detection and skipped edges. The change aims to correct this flawed logic.

**Buggy**
"
lucene-solr,43382.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
-  private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+  private static final boolean isVertexEquals(final Node a, final double x, final  double y) {
+    return a.getX() == x && a.getY() == y;
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections"" explicitly states that the commit addresses a bug. The bug involves polygon edge skipping during intersection checks. This suggests a problem in the logic related to geometric calculations.

**Diff Analysis:**

The diff shows a change in the `isVertexEquals` method.

*   **Original Code:**
    ```java
    private static final boolean isVertexEquals(final Node a, final Node b) {
        return a.getX() == b.getX() && a.getY() == b.getY();
    }
    ```
*   **Modified Code:**
    ```java
    private static final boolean isVertexEquals(final Node a, final double x, final  double y) {
        return a.getX() == x && a.getY() == y;
    }
    ```

The change involves modifying the method signature of `isVertexEquals`. Instead of comparing two `Node` objects, it now compares a `Node` object with two `double` values `x` and `y`. This suggests that the original implementation might have been inefficient or incorrect when comparing a vertex with coordinates derived from calculations (which are often doubles). The original method compared two `Node` objects, which might have led to precision issues or incorrect comparisons if the coordinates being compared were not represented as `Node` objects. The new method directly compares the `Node`'s x and y coordinates with double values, potentially addressing a precision or type-related bug.

**Reasoning:**

The commit message clearly indicates a bug fix related to polygon intersection checks. The code change modifies a method used for comparing vertices, suggesting that the original implementation had a flaw that could lead to incorrect intersection results. The change in parameter types from `Node` to `double` for x and y coordinates strongly hints at a fix for potential precision or type-related issues in vertex comparison, which would directly relate to the bug described in the commit message.

**Conclusion:**

**Buggy**
"
lucene-solr,38533.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,15 +1,16 @@
-  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int heatLen,
+  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int numCells,
                                         double cellMin, double cellMax,
                                         int[] out) {
+    assert heatMin < heatMax && cellMin < cellMax;
     //precondition: we know there's an intersection
     if (heatMin >= cellMin) {
       out[0] = 0;
     } else {
       out[0] = (int) Math.round((cellMin - heatMin) / heatCellLen);
     }
     if (heatMax <= cellMax) {
-      out[1] = heatLen - 1;
+      out[1] = numCells - 1;
     } else {
       out[1] = (int) Math.round((cellMax - heatMin) / heatCellLen) - 1;
     }
   }",NotBuggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes"" explicitly states that the commit fixes a bug in the `HeatmapFacetCounter` related to spatial data, specifically when dealing with datelines and large, non-point shapes. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff modifies the `intersectInterval` method. Let's break down the changes:

1.  **Parameter Name Change:** `heatLen` is renamed to `numCells`. This change by itself is not indicative of a bug fix, but rather a clarification of the parameter's purpose.

2.  **Assertion:** An assertion `assert heatMin < heatMax && cellMin < cellMax;` is added. Assertions are often added to validate assumptions and catch potential errors during development or testing. The addition of this assertion suggests that there might have been cases where these conditions were not met, potentially leading to incorrect calculations.

3.  **`out[1]` Calculation:** The calculation of `out[1]` is modified from `heatLen - 1` to `numCells - 1`. Given the renaming of `heatLen` to `numCells`, this change ensures that the upper bound of the interval is correctly calculated based on the number of cells. This is a crucial change, especially considering the context of the commit message mentioning ""dateline and large non-point shapes."" It's plausible that the original calculation was incorrect when dealing with shapes that cross the dateline or are significantly larger than a single cell, leading to an out-of-bounds access or incorrect facet counts.

**Reasoning:**

The commit message clearly states a bug fix. The code changes, particularly the modification to the `out[1]` calculation and the added assertion, support this claim. The change in the `out[1]` calculation suggests that the original code might have been incorrectly calculating the upper bound of the interval, potentially leading to incorrect facet counts or other issues when dealing with datelines or large shapes. The added assertion further reinforces the idea that there were potential issues with the input parameters.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix. The modification to the `out[1]` calculation and the addition of the assertion strongly suggest that the original code had a flaw that was addressed by this commit.

**Buggy**
"
lucene-solr,38532.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,152 +1,153 @@
   public static Heatmap calcFacets(PrefixTreeStrategy strategy, IndexReaderContext context, Bits topAcceptDocs,
                                    Shape inputShape, final int facetLevel, int maxCells) throws IOException {
     if (maxCells > (MAX_ROWS_OR_COLUMNS * MAX_ROWS_OR_COLUMNS)) {
       throw new IllegalArgumentException(""maxCells ("" + maxCells + "") should be <= "" + MAX_ROWS_OR_COLUMNS);
     }
     if (inputShape == null) {
       inputShape = strategy.getSpatialContext().getWorldBounds();
     }
     final Rectangle inputRect = inputShape.getBoundingBox();
     //First get the rect of the cell at the bottom-left at depth facetLevel
     final SpatialPrefixTree grid = strategy.getGrid();
     final SpatialContext ctx = grid.getSpatialContext();
     final Point cornerPt = ctx.makePoint(inputRect.getMinX(), inputRect.getMinY());
     final CellIterator cellIterator = grid.getTreeCellIterator(cornerPt, facetLevel);
     Cell cornerCell = null;
     while (cellIterator.hasNext()) {
       cornerCell = cellIterator.next();
     }
     assert cornerCell != null && cornerCell.getLevel() == facetLevel : ""Cell not at target level: "" + cornerCell;
     final Rectangle cornerRect = (Rectangle) cornerCell.getShape();
     assert cornerRect.hasArea();
     //Now calculate the number of columns and rows necessary to cover the inputRect
     double heatMinX = cornerRect.getMinX();//note: we might change this below...
     final double cellWidth = cornerRect.getWidth();
     final Rectangle worldRect = ctx.getWorldBounds();
     final int columns = calcRowsOrCols(cellWidth, heatMinX, inputRect.getWidth(), inputRect.getMinX(), worldRect.getWidth());
     final double heatMinY = cornerRect.getMinY();
     final double cellHeight = cornerRect.getHeight();
     final int rows = calcRowsOrCols(cellHeight, heatMinY, inputRect.getHeight(), inputRect.getMinY(), worldRect.getHeight());
     assert rows > 0 && columns > 0;
     if (columns > MAX_ROWS_OR_COLUMNS || rows > MAX_ROWS_OR_COLUMNS || columns * rows > maxCells) {
       throw new IllegalArgumentException(
           ""Too many cells ("" + columns + "" x "" + rows + "") for level "" + facetLevel + "" shape "" + inputRect);
     }
 
     //Create resulting heatmap bounding rectangle & Heatmap object.
     final double halfCellWidth = cellWidth / 2.0;
     // if X world-wraps, use world bounds' range
     if (columns * cellWidth + halfCellWidth > worldRect.getWidth()) {
       heatMinX = worldRect.getMinX();
     }
     double heatMaxX = heatMinX + columns * cellWidth;
     if (Math.abs(heatMaxX - worldRect.getMaxX()) < halfCellWidth) {//numeric conditioning issue
       heatMaxX = worldRect.getMaxX();
     } else if (heatMaxX > worldRect.getMaxX()) {//wraps dateline (won't happen if !geo)
       heatMaxX = heatMaxX - worldRect.getMaxX() +  worldRect.getMinX();
     }
     final double halfCellHeight = cellHeight / 2.0;
     double heatMaxY = heatMinY + rows * cellHeight;
     if (Math.abs(heatMaxY - worldRect.getMaxY()) < halfCellHeight) {//numeric conditioning issue
       heatMaxY = worldRect.getMaxY();
     }
 
     final Heatmap heatmap = new Heatmap(columns, rows, ctx.makeRectangle(heatMinX, heatMaxX, heatMinY, heatMaxY));
 
     //All ancestor cell counts (of facetLevel) will be captured during facet visiting and applied later. If the data is
     // just points then there won't be any ancestors.
     //Facet count of ancestors covering all of the heatmap:
     int[] allCellsAncestorCount = new int[1]; // single-element array so it can be accumulated in the inner class
     //All other ancestors:
     Map<Rectangle,Integer> ancestors = new HashMap<>();
 
     //Now lets count some facets!
     PrefixTreeFacetCounter.compute(strategy, context, topAcceptDocs, inputShape, facetLevel,
         new PrefixTreeFacetCounter.FacetVisitor() {
       @Override
       public void visit(Cell cell, int count) {
         final double heatMinX = heatmap.region.getMinX();
         final Rectangle rect = (Rectangle) cell.getShape();
         if (cell.getLevel() == facetLevel) {//heatmap level; count it directly
           //convert to col & row
           int column;
           if (rect.getMinX() >= heatMinX) {
             column = (int) Math.round((rect.getMinX() - heatMinX) / cellWidth);
           } else { // due to dateline wrap
             column = (int) Math.round((rect.getMinX() + 360 - heatMinX) / cellWidth);
           }
           int row = (int) Math.round((rect.getMinY() - heatMinY) / cellHeight);
           //note: unfortunately, it's possible for us to visit adjacent cells to the heatmap (if the SpatialPrefixTree
           // allows adjacent cells to overlap on the seam), so we need to skip them
           if (column < 0 || column >= heatmap.columns || row < 0 || row >= heatmap.rows) {
             return;
           }
           // increment
           heatmap.counts[column * heatmap.rows + row] += count;
 
         } else if (rect.relate(heatmap.region) == SpatialRelation.CONTAINS) {//containing ancestor
           allCellsAncestorCount[0] += count;
 
         } else { // ancestor
           // note: not particularly efficient (possible put twice, and Integer wrapper); oh well
           Integer existingCount = ancestors.put(rect, count);
           if (existingCount != null) {
             ancestors.put(rect, count + existingCount);
           }
         }
       }
     });
 
     //Update the heatmap counts with ancestor counts
 
     // Apply allCellsAncestorCount
     if (allCellsAncestorCount[0] > 0) {
       for (int i = 0; i < heatmap.counts.length; i++) {
         heatmap.counts[i] += allCellsAncestorCount[0];
       }
     }
 
     // Apply ancestors
     //  note: This approach isn't optimized for a ton of ancestor cells. We'll potentially increment the same cells
     //    multiple times in separate passes if any ancestors overlap. IF this poses a problem, we could optimize it
     //    with additional complication by keeping track of intervals in a sorted tree structure (possible TreeMap/Set)
     //    and iterate them cleverly such that we just make one pass at this stage.
 
     int[] pair = new int[2];//output of intersectInterval
     for (Map.Entry<Rectangle, Integer> entry : ancestors.entrySet()) {
-      Rectangle rect = entry.getKey();
+      Rectangle rect = entry.getKey(); // from a cell (thus doesn't cross DL)
       final int count = entry.getValue();
+
       //note: we approach this in a way that eliminates int overflow/underflow (think huge cell, tiny heatmap)
       intersectInterval(heatMinY, heatMaxY, cellHeight, rows, rect.getMinY(), rect.getMaxY(), pair);
       final int startRow = pair[0];
       final int endRow = pair[1];
 
       if (!heatmap.region.getCrossesDateLine()) {
         intersectInterval(heatMinX, heatMaxX, cellWidth, columns, rect.getMinX(), rect.getMaxX(), pair);
         final int startCol = pair[0];
         final int endCol = pair[1];
         incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
 
       } else {
+        // note: the cell rect might intersect 2 disjoint parts of the heatmap, so we do the left & right separately
+        final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth);
+        final int rightColumns = heatmap.columns - leftColumns;
         //left half of dateline:
-        if (rect.getMaxX() >= heatMinX) {
-          final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth) + 1;
+        if (rect.getMaxX() > heatMinX) {
           intersectInterval(heatMinX, 180, cellWidth, leftColumns, rect.getMinX(), rect.getMaxX(), pair);
           final int startCol = pair[0];
           final int endCol = pair[1];
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
         //right half of dateline
-        if (rect.getMinY() <= heatMaxX) {
-          final int rightColumns = (int) Math.round(heatMaxX / cellWidth) + 1;
-          intersectInterval(0, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
-          final int startCol = pair[0];
-          final int endCol = pair[1];
+        if (rect.getMinX() < heatMaxX) {
+          intersectInterval(-180, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
+          final int startCol = pair[0] + leftColumns;
+          final int endCol = pair[1] + leftColumns;
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
       }
-
     }
 
     return heatmap;
   }",Buggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"The commit message indicates a bug fix related to the `HeatmapFacetCounter` in the context of spatial data, specifically addressing issues with dateline handling and large non-point shapes.

The code diff focuses on the `calcFacets` method within the `HeatmapFacetCounter`. The changes primarily involve adjustments to how the code handles rectangles that cross the dateline, especially when calculating column indices and intersecting intervals. There are modifications in the dateline wrapping logic, including adjustments to the `heatMaxX` calculation and the conditional logic for processing the left and right halves of the dateline. The original code had issues with calculating the number of columns and intersecting intervals when the input shape crossed the dateline, which could lead to incorrect facet counts. The updated code addresses these issues by correctly handling the disjoint parts of the heatmap and adjusting the column indices accordingly.

These changes directly address the issues mentioned in the commit message, indicating that the commit is indeed a bug fix.

**Buggy**"
lucene-solr,36488.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,42 +1,34 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     DoubleRange[] ranges = (DoubleRange[]) this.ranges;
 
     LongRange[] longRanges = new LongRange[ranges.length];
     for(int i=0;i<ranges.length;i++) {
       DoubleRange range = ranges[i];
       longRanges[i] =  new LongRange(range.label,
                                      NumericUtils.doubleToSortableLong(range.minIncl), true,
                                      NumericUtils.doubleToSortableLong(range.maxIncl), true);
     }
 
     LongRangeCounter counter = new LongRangeCounter(longRanges);
 
-    // Compute min & max over all ranges:
-    double minIncl = Double.POSITIVE_INFINITY;
-    double maxIncl = Double.NEGATIVE_INFINITY;
-    for(DoubleRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
         } else {
           missingCount++;
         }
         doc++;
       }
     }
 
     missingCount += counter.fillCounts(counts);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix to a test bug and removal of dead code.

The diff removes the computation of `minIncl` and `maxIncl` which are not used later in the code. This aligns with the ""remove dead code"" part of the commit message. The removal of dead code can be considered a bug fix in the sense that it improves code maintainability and reduces potential for confusion.

Therefore, the commit message and the code changes are related and the changes indicate a bug fix.

**Buggy**
"
lucene-solr,36489.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,39 +1,31 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     LongRange[] ranges = (LongRange[]) this.ranges;
 
-    // Compute min & max over all ranges:
-    long minIncl = Long.MAX_VALUE;
-    long maxIncl = Long.MIN_VALUE;
-    for(LongRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     LongRangeCounter counter = new LongRangeCounter(ranges);
 
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(fv.longVal(doc));
         } else {
           missingCount++;
         }
 
         doc++;
       }
     }
     
     int x = counter.fillCounts(counts);
 
     missingCount += x;
 
     //System.out.println(""totCount "" + totCount + "" missingCount "" + counter.missingCount);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a test bug and removal of dead code.

The diff removes the computation of `minIncl` and `maxIncl` over all ranges. This calculation was unused, indicating dead code removal. The rest of the code remains the same, suggesting that the core logic wasn't changed. The removal of dead code can be related to fixing a test bug if the dead code was interfering with the test or causing incorrect test results.

Therefore, the changes align with the commit message and suggest a bug fix.

**Buggy**
"
lucene-solr,46441.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,45 +1,51 @@
   private DocMap[] buildDocMaps(List<CodecReader> readers, Sort indexSort) throws IOException {
 
     int numReaders = readers.size();
 
     if (indexSort == null) {
       // no index sort ... we only must map around deletions, and rebase to the merged segment's docID space
 
       int totalDocs = 0;
       DocMap[] docMaps = new DocMap[numReaders];
 
       // Remap docIDs around deletions:
       for (int i = 0; i < numReaders; i++) {
         LeafReader reader = readers.get(i);
         Bits liveDocs = reader.getLiveDocs();
 
         final PackedLongValues delDocMap;
         if (liveDocs != null) {
           delDocMap = removeDeletes(reader.maxDoc(), liveDocs);
         } else {
           delDocMap = null;
         }
 
         final int docBase = totalDocs;
         docMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             if (liveDocs == null) {
               return docBase + docID;
             } else if (liveDocs.get(docID)) {
               return docBase + (int) delDocMap.get(docID);
             } else {
               return -1;
             }
           }
         };
         totalDocs += reader.numDocs();
       }
 
       return docMaps;
 
     } else {
       // do a merge sort of the incoming leaves:
-      return MultiSorter.sort(indexSort, readers);
+      long t0 = System.nanoTime();
+      DocMap[] result = MultiSorter.sort(indexSort, readers);
+      long t1 = System.nanoTime();
+      if (infoStream.isEnabled(""SM"")) {
+        infoStream.message(""SM"", String.format(Locale.ROOT, ""%.2f msec to build merge sorted DocMaps"", (t1-t0)/1000000.0));
+      }
+      return result;
     }
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",Buggy,"The commit message ""LUCENE-6766: more IW.infoStream logging around sorting; fix test bug"" indicates two changes: adding more logging and fixing a test bug.

The diff adds logging around the `MultiSorter.sort` method call. Specifically, it measures the time taken to build the merge sorted DocMaps and logs this information to the infoStream if it is enabled with the ""SM"" tag. This aligns with the ""more IW.infoStream logging around sorting"" part of the commit message.

The diff doesn't directly show the test bug fix, but the commit message mentions it.

The addition of logging, while not directly fixing a bug in the core logic, aids in debugging and performance analysis, which can indirectly help identify and fix bugs. The commit message explicitly mentions a bug fix.

Therefore, the changes indicate a bug fix, even if not directly visible in the provided diff snippet.

**Buggy**
"
lucene-solr,46444.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,61 +1,65 @@
   private List<CodecReader> maybeSortReaders(List<CodecReader> originalReaders, SegmentInfo segmentInfo) throws IOException {
 
     // Default to identity:
     for(int i=0;i<originalReaders.size();i++) {
       leafDocMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             return docID;
           }
         };
     }
 
     Sort indexSort = segmentInfo.getIndexSort();
     if (indexSort == null) {
       return originalReaders;
     }
 
     // If an incoming reader is not sorted, because it was flushed by IW, we sort it here:
     final Sorter sorter = new Sorter(indexSort);
     List<CodecReader> readers = new ArrayList<>(originalReaders.size());
 
     for (CodecReader leaf : originalReaders) {
       Sort segmentSort = leaf.getIndexSort();
 
       if (segmentSort == null) {
         // TODO: fix IW to also sort when flushing?  It's somewhat tricky because of stored fields and term vectors, which write ""live""
         // to their index files on each indexed document:
 
         // This segment was written by flush, so documents are not yet sorted, so we sort them now:
+        long t0 = System.nanoTime();
         Sorter.DocMap sortDocMap = sorter.sort(leaf);
+        long t1 = System.nanoTime();
+        double msec = (t1-t0)/1000000.0;
+        
         if (sortDocMap != null) {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted; wrapping for sort "" + indexSort + "" now"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted; wrapping for sort %s now (%.2f msec to sort)"", leaf, indexSort, msec));
           }
           leaf = SlowCodecReaderWrapper.wrap(SortingLeafReader.wrap(new MergeReaderWrapper(leaf), sortDocMap));
           leafDocMaps[readers.size()] = new DocMap() {
               @Override
               public int get(int docID) {
                 return sortDocMap.oldToNew(docID);
               }
             };
         } else {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted, but is already accidentally in sort "" + indexSort + "" order"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted, but is already accidentally in sort %s order (%.2f msec to sort)"", leaf, indexSort, msec));
           }
         }
 
       } else {
         if (segmentSort.equals(indexSort) == false) {
           throw new IllegalArgumentException(""index sort mismatch: merged segment has sort="" + indexSort + "" but to-be-merged segment has sort="" + segmentSort);
         }
         if (infoStream.isEnabled(""SM"")) {
           infoStream.message(""SM"", ""segment "" + leaf + "" already sorted"");
         }
       }
 
       readers.add(leaf);
     }
 
     return readers;
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",Buggy,"The commit message ""LUCENE-6766: more IW.infoStream logging around sorting; fix test bug"" indicates two main changes: increased logging and a bug fix in a test.

The diff shows changes in the `maybeSortReaders` method. The changes include:
1. Adding timing information around the `sorter.sort(leaf)` call using `System.nanoTime()` to measure the sorting time in milliseconds.
2. Incorporating the measured sorting time into the `infoStream.message` calls, providing more detailed logging information about the sorting process.
3. The logging messages now include the sorting time in milliseconds, formatted to two decimal places.

The addition of timing information and its inclusion in the logging messages suggests a debugging or performance analysis effort, which could be related to identifying and fixing a bug or performance issue. The commit message explicitly mentions a bug fix. The added logging helps in diagnosing the performance of the sorting process, which could be crucial in identifying the root cause of the bug.

Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,44598.json,a2a9f2a6f9cba3b27b248102d88431b8b234530e,"@@ -1,32 +1,34 @@
     public SeekStatus seekCeil(BytesRef text) {
 
       // TODO: we could instead keep the BytesRefHash
       // intact so this is a hash lookup
 
       // binary search:
       int lo = 0;
       int hi = numTerms - 1;
       while (hi >= lo) {
         int mid = (lo + hi) >>> 1;
         int textStart = postingsArray.textStarts[sortedTermIDs[mid]];
         terms.bytePool.setBytesRef(scratch, textStart);
         int cmp = scratch.compareTo(text);
         if (cmp < 0) {
           lo = mid + 1;
         } else if (cmp > 0) {
           hi = mid - 1;
         } else {
           // found:
           ord = mid;
           return SeekStatus.FOUND;
         }
       }
 
       // not found:
       ord = lo + 1;
-      if (ord == numTerms) {
+      if (ord >= numTerms) {
         return SeekStatus.END;
       } else {
+        int textStart = postingsArray.textStarts[sortedTermIDs[ord]];
+        terms.bytePool.setBytesRef(scratch, textStart);
         return SeekStatus.NOT_FOUND;
       }
     }",Buggy,"test seekCeil in pull postings API; fix trunk-only bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1611305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `seekCeil` method in the pull postings API. The diff shows a change in the `seekCeil` method. Specifically, the condition `ord == numTerms` is changed to `ord >= numTerms`. Additionally, when `ord` is not `numTerms`, the code now retrieves the term at the `ord` index and sets the `scratch` variable before returning `SeekStatus.NOT_FOUND`.

The original code had a potential bug: if `lo` ended up being equal to `numTerms` after the binary search, the code would return `SeekStatus.END`. However, if `lo` was greater than `numTerms`, it would not be handled correctly, potentially leading to an out-of-bounds access or incorrect behavior. The change to `ord >= numTerms` fixes this.

Also, the added lines after the `else` statement in the `if (ord >= numTerms)` block are important. Without these lines, the `scratch` variable would not be set to the correct term when `SeekStatus.NOT_FOUND` is returned. This could lead to incorrect behavior in subsequent operations that rely on the `scratch` variable.

Therefore, the changes in the diff indicate a bug fix.

**Buggy**
"
jna,165.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,14 +1,15 @@
         private static long getVisualID(GraphicsConfiguration config) {
             // Use reflection to call
             // X11GraphicsConfig.getVisual
             try {
                 Object o = config.getClass()
                     .getMethod(""getVisual"", (Class[])null)
                     .invoke(config, (Object[])null);
                 return ((Number)o).longValue();
             }
             catch (Exception e) {
+                // FIXME properly handle this error
                 e.printStackTrace();
                 return -1;
             }
         }",NotBuggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",Buggy,"The commit message indicates a fix for mappings to work properly on 32-bit Windows, addressing memory faults. The code diff shows a change in the `getVisualID` method where an exception handler is modified. The original exception handler simply printed the stack trace and returned -1. The modified code adds a comment `// FIXME properly handle this error` and keeps the original behavior. Although the comment suggests further work is needed, the original issue was likely a crash due to an unhandled exception, which the original code masked by returning -1. The commit message indicates a fix for memory faults, which could be caused by incorrect mappings due to the exception not being handled correctly. Therefore, the changes indicate a bug fix.

**Buggy**"
jna,146.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,17 +1,18 @@
-				public boolean callback(final HWND hwnd, final Pointer arg1) {
-					try {
-						final boolean visible = !onlyVisibleWindows
-								|| User32.INSTANCE.IsWindowVisible(hwnd);
-						if (visible) {
-							final String title = getWindowTitle(hwnd);
-							final String filePath = getProcessFilePath(hwnd);
-							final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
-							result.add(new DesktopWindow(hwnd, title, filePath,
-									locAndSize));
-						}
-					} catch (final Exception e) {
-						e.printStackTrace();
-					}
-
-					return true;
-				}

+                public boolean callback(final HWND hwnd, final Pointer arg1) {
+                    try {
+                        final boolean visible = !onlyVisibleWindows
+                            || User32.INSTANCE.IsWindowVisible(hwnd);
+                        if (visible) {
+                            final String title = getWindowTitle(hwnd);
+                            final String filePath = getProcessFilePath(hwnd);
+                            final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
+                            result.add(new DesktopWindow(hwnd, title, filePath,
+                                                         locAndSize));
+                        }
+                    } catch (final Exception e) {
+                        // FIXME properly handle whatever error is raised
+                        e.printStackTrace();
+                    }
+                    
+                    return true;
+                }",Buggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",Buggy,"The commit message indicates a fix for memory faults specifically on 32-bit Windows systems related to mappings. The code diff shows a change within a callback function where a `try-catch` block is present. Inside the `try` block, window properties like title, file path, and location/size are retrieved. The `catch` block simply prints the stack trace of any caught exception. The modification adds a comment `// FIXME properly handle whatever error is raised` inside the catch block.

Reasoning:
1. **Commit Message Relevance:** The commit message mentions fixing memory faults, suggesting a bug fix.
2. **Code Change Analysis:** The code change focuses on the exception handling within the callback function. The addition of the ""FIXME"" comment suggests that the original error handling was insufficient or incorrect, potentially leading to the memory faults mentioned in the commit message. The original code only printed the stack trace, which might not have been sufficient to prevent the program from crashing or exhibiting unexpected behavior, especially if the exception was related to memory access.
3. **Bug Fix Indication:** The combination of the commit message and the code change strongly suggests a bug fix. The original code's inadequate exception handling likely contributed to the memory faults on 32-bit Windows. The ""FIXME"" comment highlights the need for better error handling, implying that the previous handling was a source of the problem.

Conclusion:
**Buggy**
"
jna,3926.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,86 +1,84 @@
     public Object invoke(Class returnType, Object[] inArgs, Map options) {
         // Clone the argument array to obtain a scratch space for modified
         // types/values
         Object[] args = { };
         if (inArgs != null) {
             if (inArgs.length > MAX_NARGS) {
                 throw new UnsupportedOperationException(""Maximum argument count is "" + MAX_NARGS);
             }
             args = new Object[inArgs.length];
             System.arraycopy(inArgs, 0, args, 0, args.length);
         }
 
         TypeMapper mapper = 
             (TypeMapper)options.get(Library.OPTION_TYPE_MAPPER);
         Method invokingMethod = (Method)options.get(OPTION_INVOKING_METHOD);
         for (int i=0; i < args.length; i++) {
             args[i] = convertArgument(args, i, invokingMethod, mapper);
         }
         
         Class nativeType = returnType;
         FromNativeConverter resultConverter = null;
         if (NativeMapped.class.isAssignableFrom(returnType)) {
             NativeMappedConverter tc = new NativeMappedConverter(returnType);
             resultConverter = tc;
             nativeType = tc.nativeType();
         }
         else if (mapper != null) {
             resultConverter = mapper.getFromNativeConverter(returnType);
             if (resultConverter != null) {
                 nativeType = resultConverter.nativeType();
             }
         }
 
         Object result = invoke(args, nativeType);
 
         // Convert the result to a custom value/type if appropriate
         if (resultConverter != null) {
             FromNativeContext context;
             
             if (invokingMethod != null) {
                 context = new MethodResultContext(returnType, this, inArgs, invokingMethod);
             } else {
                 context = new FunctionResultContext(returnType, this, inArgs);
             }
             result = resultConverter.fromNative(result, context);
         }
 
         // Sync all memory which might have been modified by the native call
         if (inArgs != null) {
             for (int i=0; i < inArgs.length; i++) {
                 Object arg = inArgs[i];
                 if (arg == null)
                     continue;
                 if (arg instanceof Structure) {
                     if (!(arg instanceof Structure.ByValue)) {
                         ((Structure)arg).read();
                     }
                 }
                 else if (args[i] instanceof StringArray) {
-                    // Read back arrays of String, just in case they
-                    // were modified
                     ((StringArray)args[i]).read();
                 }
                 else if (args[i] instanceof PointerArray) {
                     PointerArray array = (PointerArray)args[i];
                     array.read();
                     if (Structure.ByReference[].class.isAssignableFrom(arg.getClass())) {
                         Class type = arg.getClass().getComponentType();
                         Structure[] ss = (Structure[])arg;
                         for (int si=0;si < ss.length;si++) {
                             Pointer p = array.getPointer(Pointer.SIZE * si);
                             ss[si] = Structure.updateStructureByReference(type, ss[si], p);
                         }
                     }
                 }
                 else if (Structure[].class.isAssignableFrom(arg.getClass())) {
                     Structure[] ss = (Structure[])arg;
                     for (int si=0;si < ss.length;si++) {
                         ss[si].read();
                     }
                 }
             }
         }
                         
         return result;
     }",NotBuggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to stdcall function mapping when using struct by value. The code diff focuses on synchronizing memory that might have been modified by the native call, specifically within the loop that iterates through the input arguments (`inArgs`). The key changes are within the `if` block that checks the type of each argument.

The diff removes the comment `// Read back arrays of String, just in case they were modified` from the `StringArray` block. This suggests that the code was indeed reading back the `StringArray` to account for potential modifications made by the native call.

The diff also includes logic to update `Structure.ByReference` arrays after the native call. This is done by iterating through the array and calling `Structure.updateStructureByReference` for each element. This update is based on the pointer obtained from the `PointerArray`. This suggests that the original code might not have correctly handled updates to structures passed by reference within arrays.

The commit message and the code diff are aligned. The changes address potential issues with how structures, especially those passed by value or by reference within arrays, are handled after a native call. The code ensures that any modifications made by the native call are reflected in the Java objects.

**Buggy**
"
jna,3554.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,23 +1,30 @@
-    private int getNativeSize(Class type, Object value) {
-        if (Structure.class.isAssignableFrom(type)) {
-            if (ByReference.class.isAssignableFrom(type)) {
-                return Pointer.SIZE;
-            }
-            else {
-                if (value == null)
-                    value = newInstance(type);
-                Structure s = (Structure)value;
-                return s.size();
-            }
-        }
+    private static int getNativeSize(Class type, Object value) {
         if (type.isArray()) {
             int len = Array.getLength(value);
             if (len > 0) {
                 Object o = Array.get(value, 0);
                 return len * getNativeSize(type.getComponentType(), o);
             }
             // Don't process zero-length arrays
-            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + this);
+            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + type);
         }
-        return getNativeSize(type);
+        // May provide this in future; problematic on read, since we can't
+        // auto-create a java.nio.Buffer w/o knowing its size
+        if (Buffer.class.isAssignableFrom(type)) {
+            throw new IllegalArgumentException(""the type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
+        if (Structure.class.isAssignableFrom(type)
+            && !Structure.ByReference.class.isAssignableFrom(type)) {
+            if (value == null)
+                value = newInstance(type);
+            return ((Structure)value).size();
+        }
+        try {
+            return Native.getNativeSize(type);
+        }
+        catch(IllegalArgumentException e) {
+            throw new IllegalArgumentException(""The type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
     }",Buggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to structure mapping when using struct by value in `stdcall` functions. The code diff modifies the `getNativeSize` method, which is responsible for determining the size of native types.

Here's a breakdown of the changes and their implications:

1.  **Static method:** The method `getNativeSize` is changed to `private static`.
2.  **Buffer check:** A check is added to throw an `IllegalArgumentException` if the type is a `java.nio.Buffer`. This suggests that `Buffer` types were previously mishandled, potentially leading to errors.
3.  **Structure size calculation:** The logic for calculating the size of `Structure` types is moved and slightly modified. It now explicitly checks for `Structure.ByReference` and only calculates the size if it's not a reference.
4.  **Exception handling:** A `try-catch` block is added to handle `IllegalArgumentException` when calling `Native.getNativeSize(type)`. This indicates that certain types were not properly handled by `Native.getNativeSize`, and the exception is now caught and re-thrown with a more informative message.
5.  **Array exception message:** The exception message for zero-length arrays is updated to include the type.

The changes address potential issues with:

*   Incorrect size calculation for structures passed by value.
*   Unsupported `java.nio.Buffer` types.
*   Unhandled types in `Native.getNativeSize`.

The addition of exception handling and specific type checks strongly suggests that the original code had bugs related to type handling and size calculation. The commit message aligns with the code changes, indicating a bug fix related to structure mapping.

**Buggy**"
jna,3652.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,9 +1,10 @@
     Object getField(StructField structField) {
         try {
             return structField.field.get(this);
         }
         catch (Exception e) {
             throw new Error(""Exception reading field '""
-                            + structField.name + ""' in "" + getClass(), e);
+                            + structField.name + ""' in "" + getClass() 
+                            + "": "" + e);
         }
     }",NotBuggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fix bug in nested struct array read/write"" clearly states that the commit is intended to resolve a bug related to reading and writing nested struct arrays. This is a strong indicator that the changes are bug-related.

**2. Code Diff Analysis:**

The code diff shows a modification within the `getField` method of a class (likely related to structures, given the context of `StructField`). The change involves the `catch` block of a `try-catch` statement.

*   **Original Code:** The original `catch` block throws a new `Error` with a message including the field name, class, and the original exception `e`.
*   **Modified Code:** The modified `catch` block throws a new `Error` with a message including the field name, class, and the original exception `e`, but the exception is appended using string concatenation.

**3. Reasoning:**

The change in the `catch` block suggests an attempt to provide more detailed error information when an exception occurs while reading a field. The original code already included the exception, but the modification might be aimed at making the error message more readable or providing more context about the specific exception that occurred. This is a common practice when debugging and fixing bugs, especially when dealing with complex data structures like nested struct arrays. The original code might have been masking the underlying exception, making it difficult to diagnose the root cause of the problem. By including the exception message directly in the `Error` message, the developer can gain better insight into the error.

Given the commit message explicitly mentioning a bug fix and the code change focusing on improving error reporting within a field access method (likely related to struct array access), it's highly probable that this commit addresses a bug.

**Conclusion:**

**Buggy**
"
jna,4096.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,137 +1,147 @@
     void writeField(StructField structField) {
         // Get the offset of the field
         int offset = structField.offset;
 
         // Get the value from the field
         Object value = getField(structField);
         
         // Determine the type of the field
         Class nativeType = structField.type;
         ToNativeConverter converter = structField.writeConverter;
         if (converter != null) {
             value = converter.toNative(value, 
                     new StructureWriteContext(this, structField.field));
             // Assume any null values are pointers
             nativeType = value != null ? value.getClass() : Pointer.class;
         }
 
         // Java strings get converted to C strings, where a Pointer is used
         if (String.class == nativeType
             || WString.class == nativeType) {
 
             // Allocate a new string in memory
             boolean wide = nativeType == WString.class;
             if (value != null) {
                 NativeString nativeString = new NativeString(value.toString(), wide);
                 // Keep track of allocated C strings to avoid 
                 // premature garbage collection of the memory.
                 nativeStrings.put(structField.name, nativeString);
                 value = nativeString.getPointer();
             }
             else {
                 value = null;
             }
         }
 
         // Set the value at the offset according to its type
         if (nativeType == boolean.class || nativeType == Boolean.class) {
             memory.setInt(offset, Boolean.TRUE.equals(value) ? -1 : 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             memory.setByte(offset, ((Byte)value).byteValue());
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             memory.setShort(offset, ((Short)value).shortValue());
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             memory.setChar(offset, ((Character)value).charValue());
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             memory.setInt(offset, ((Integer)value).intValue());
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             memory.setLong(offset, ((Long)value).longValue());
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             memory.setFloat(offset, ((Float)value).floatValue());
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             memory.setDouble(offset, ((Double)value).doubleValue());
         }
         else if (nativeType == Pointer.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == String.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == WString.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             if (cls == byte.class) {
                 byte[] buf = (byte[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == short.class) {
                 short[] buf = (short[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == char.class) {
                 char[] buf = (char[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == int.class) {
                 int[] buf = (int[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == long.class) {
                 long[] buf = (long[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == float.class) {
                 float[] buf = (float[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == double.class) {
                 double[] buf = (double[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 Pointer[] buf = (Pointer[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sbuf = (Structure[])value;
-                Pointer[] buf = new Pointer[sbuf.length];
-                for (int i=0;i < sbuf.length;i++) {
-                    buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] buf = new Pointer[sbuf.length];
+                    for (int i=0;i < sbuf.length;i++) {
+                        buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                    }
+                    memory.write(offset, buf, 0, buf.length);
                 }
-                memory.write(offset, buf, 0, buf.length);
+                else {
+                    for (int i=0;i < sbuf.length;i++) {
+                        if (sbuf[i] == null) {
+                            sbuf[i] = newInstance(cls);
+                            sbuf[i].useMemory(memory, offset + i * sbuf[i].size());
+                        }
+                        sbuf[i].write();
+                    }
+                }
             }
             else {
                 throw new IllegalArgumentException(""Inline array of ""
                                                    + cls + "" not supported"");
             }
         }
         else if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)value;
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 memory.setPointer(offset, s == null ? null : s.getPointer());
             }
             else {
                 s.useMemory(memory, offset);
                 s.write();
             }
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             memory.setPointer(offset, CallbackReference.getFunctionPointer((Callback)value));
         }
         else {
         	String msg = ""Structure field \"""" + structField.name
         	    + ""\"" was declared as "" + nativeType 
         	    + "", which is not supported within a Structure"";
             throw new IllegalArgumentException(msg);
         }
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to reading/writing nested struct arrays. The diff modifies the `writeField` method in the `Structure` class, specifically addressing how arrays of `Structure` objects are handled.

Here's a breakdown of the changes:

1.  **Conditional Handling of `ByReference`:** The original code assumed that if the array element type was a `Structure` and a `ByReference`, it would always be handled as an array of pointers. The modified code introduces a conditional check:

    *   If the array element type is a `Structure` *and* a `ByReference`, it's handled as an array of pointers (as before).
    *   Otherwise (if it's a `Structure` but *not* a `ByReference`), it iterates through the array, creates a new instance if an element is null, associates the structure with a memory region, and calls `write()` on each element.

2.  **Handling Non-`ByReference` Structures:** The new `else` block within the `Structure` array handling is significant. It suggests that the original code was not correctly handling cases where a structure array was embedded directly within another structure (i.e., not using pointers). The new code initializes each structure in the array, assigns memory to it using `useMemory`, and then writes the structure's data to that memory.

The changes strongly suggest that the original code had a bug where it couldn't correctly write arrays of nested structures when those structures were not passed by reference. The new code fixes this by explicitly handling the memory layout and writing the data for each structure in the array.

Therefore, the changes indicate a bug fix.

**Buggy**"
jna,4092.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,136 +1,146 @@
     Object readField(StructField structField) {
         
         // Get the offset of the field
         int offset = structField.offset;
 
         // Determine the type of the field
         Class nativeType = structField.type;
         FromNativeConverter readConverter = structField.readConverter;
         if (readConverter != null) {
             nativeType = readConverter.nativeType();
         }
 
         // Get the value at the offset according to its type
         Object result = null;
         if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)getField(structField);
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 s = updateStructureByReference(nativeType, s, memory.getPointer(offset));
             }
             else {
                 s.useMemory(memory, offset);
                 s.read();
             }
             result = s;
         }
         else if (nativeType == boolean.class || nativeType == Boolean.class) {
             result = Boolean.valueOf(memory.getInt(offset) != 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             result = new Byte(memory.getByte(offset));
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             result = new Short(memory.getShort(offset));
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             result = new Character(memory.getChar(offset));
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             result = new Integer(memory.getInt(offset));
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             result = new Long(memory.getLong(offset));
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             result=new Float(memory.getFloat(offset));
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             result = new Double(memory.getDouble(offset));
         }
         else if (nativeType == Pointer.class) {
             result = memory.getPointer(offset);
         }
         else if (nativeType == String.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? p.getString(0) : null;
         }
         else if (nativeType == WString.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? new WString(p.getString(0, true)) : null;
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             // Overwrite the Java memory if the native pointer is a different
             // function pointer.
             Pointer fp = memory.getPointer(offset);
             if (fp == null) {
                 result = null;
             }
             else {
                 Callback cb = (Callback)getField(structField);
                 Pointer oldfp = CallbackReference.getFunctionPointer(cb);
                 if (!fp.equals(oldfp)) {
                     cb = CallbackReference.getCallback(nativeType, fp);
                 }
                 result = cb;
             }
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             int length = 0;
             Object o = getField(structField);
             if (o == null) {
                 throw new IllegalStateException(""Array field in Structure not initialized"");
             }
             length = Array.getLength(o);
             result = o;
 
             if (cls == byte.class) {
                 memory.read(offset, (byte[])result, 0, length);
             }
             else if (cls == short.class) {
                 memory.read(offset, (short[])result, 0, length);
             }
             else if (cls == char.class) {
                 memory.read(offset, (char[])result, 0, length);
             }
             else if (cls == int.class) {
                 memory.read(offset, (int[])result, 0, length);
             }
             else if (cls == long.class) {
                 memory.read(offset, (long[])result, 0, length);
             }
             else if (cls == float.class) {
                 memory.read(offset, (float[])result, 0, length);
             }
             else if (cls == double.class) {
                 memory.read(offset, (double[])result, 0, length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 memory.read(offset, (Pointer[])result, 0, length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sarray = (Structure[])result;
-                Pointer[] parray = memory.getPointerArray(offset, sarray.length);
-                for (int i=0;i < sarray.length;i++) {
-                    sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] parray = memory.getPointerArray(offset, sarray.length);
+                    for (int i=0;i < sarray.length;i++) {
+                        sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                    }
+                }
+                else {
+                    for (int i=0;i < sarray.length;i++) {
+                        if (sarray[i] == null) {
+                            sarray[i] = newInstance(cls);
+                            sarray[i].useMemory(memory, offset + i * sarray[i].size());
+                        }
+                        sarray[i].read();
+                    }
                 }
             }
             else {
                 throw new IllegalArgumentException(""Array of ""
                                                    + cls + "" not supported"");
             }
         }
         else {
             throw new IllegalArgumentException(""Unsupported field type \""""
                                                + nativeType + ""\"""");
         }
 
         if (readConverter != null) {
             result = readConverter.fromNative(result, structField.context);
         }
 
         // Update the value on the field
         setField(structField, result);
         return result;
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates a bug fix related to reading/writing nested struct arrays. The code changes modify the logic for handling arrays of `Structure` within the `readField` method. Specifically, the code now differentiates between `Structure` arrays that are `ByReference` and those that are not. For non-`ByReference` `Structure` arrays, it initializes each element of the array if it's null, assigns memory to it, and then reads the data. This suggests a fix for a scenario where nested structures within arrays were not being properly initialized or read, which aligns with the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
jna,2043.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,3 +1,3 @@
     public static boolean FAILED(int hr) {
-        return (hr != S_OK);
+        return hr < 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions"" strongly suggests a bug fix. It explicitly states that the definitions of `SUCCEEDED` and `FAILED` were incorrect and are being corrected based on MSDN documentation. The phrase ""instead of trying to fix the bug where it's visible"" implies that a previous attempt to address the underlying issue was not the correct approach, and the root cause was an incorrect definition of these helper functions.

**Code Diff Analysis:**

The code diff shows a change in the `FAILED` function:

-   **Original:** `return (hr != S_OK);`
-   **Modified:** `return hr < 0;`

This change directly alters the logic of determining whether an `hr` (presumably an HRESULT, a common return type in Windows programming) indicates failure. The original logic checks if `hr` is not equal to `S_OK` (success). The modified logic checks if `hr` is less than 0.

**Relevance and Bug Fix Assessment:**

The commit message and code diff are highly relevant. The commit message states that the definitions of `SUCCEEDED` and `FAILED` were incorrect, and the code diff directly changes the definition of `FAILED`. The reference to MSDN suggests that the corrected definition (`hr < 0`) is the standard and correct way to determine failure for HRESULTs. The original definition could have led to incorrect error handling and unexpected behavior. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
jna,2041.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,6 +1,3 @@
     public static boolean SUCCEEDED(int hr) {
-        if (hr == S_OK)
-            return true;
-        else
-            return false;
+        return hr >= 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions"" strongly suggests a bug fix. It explicitly states that the `SUCCEEDED` and `FAILED` methods were not working correctly and that the fix is based on the definitions provided in MSDN (Microsoft Developer Network). The phrase ""instead of trying to fix the bug where it's visible"" implies a previous attempt to fix the bug was unsuccessful or misdirected.

**Code Diff Analysis:**

The code diff shows a change in the `SUCCEEDED` method. The original implementation was:

```java
public static boolean SUCCEEDED(int hr) {
    if (hr == S_OK)
        return true;
    else
        return false;
}
```

This implementation only considered `S_OK` (presumably a constant representing a success code) as a success. The new implementation is:

```java
public static boolean SUCCEEDED(int hr) {
    return hr >= 0;
}
```

This new implementation checks if the return code `hr` is greater than or equal to 0. This aligns with the standard definition of `SUCCEEDED` in Windows programming, where any non-negative value indicates success.

**Reasoning:**

The original code was clearly flawed because it only considered one specific value (`S_OK`) as a success. The commit message explicitly states that the fix is based on the correct definition of `SUCCEEDED` and `FAILED` (implicitly, since `SUCCEEDED` is being fixed). The code change aligns with this by implementing the standard definition of `SUCCEEDED` where any non-negative value indicates success. This indicates a correction of a logical error in the original code.

**Conclusion:**

Buggy
"
jna,4111.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,21 +1,21 @@
     private void validateField(String name, Class type) {
         if (typeMapper != null) {
             ToNativeConverter toNative = typeMapper.getToNativeConverter(type);
             if (toNative != null) {
                 validateField(name, toNative.nativeType());
                 return;
             }
         }
         if (type.isArray()) {
             validateField(name, type.getComponentType());
         }
         else {
             try {
                 getNativeSize(type);
             }
             catch(IllegalArgumentException e) {
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
         }
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The commit message indicates that the change is intended to fix a broken test by ensuring that the exception message includes all faulty field information.

The code diff modifies the `validateField` method to improve the exception message when an `IllegalArgumentException` is caught. Specifically, the original exception message only included the class, field name, and type. The modified code now appends the original exception's message (`e.getMessage()`) to the new exception message. This ensures that all available information about the faulty field is included in the exception message, which aligns with the commit message's intention to provide more detailed error information. This change directly addresses a bug where the exception message was insufficient, leading to a broken test.

**Buggy**"
jna,4113.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,147 +1,147 @@
     private LayoutInfo deriveLayout(boolean force, boolean avoidFFIType) {
         int calculatedSize = 0;
         List fields = getFields(force);
         if (fields == null) {
             return null;
         }
 
         LayoutInfo info = new LayoutInfo();
         info.alignType = this.alignType;
         info.typeMapper = this.typeMapper;
 
         boolean firstField = true;
         for (Iterator i=fields.iterator();i.hasNext();firstField=false) {
             Field field = (Field)i.next();
             int modifiers = field.getModifiers();
 
             Class type = field.getType();
             if (type.isArray()) {
                 info.variable = true;
             }
             StructField structField = new StructField();
             structField.isVolatile = Modifier.isVolatile(modifiers);
             structField.isReadOnly = Modifier.isFinal(modifiers);
             if (structField.isReadOnly) {
                 if (!Platform.RO_FIELDS) {
                     throw new IllegalArgumentException(""This VM does not support read-only fields (field '""
                                                        + field.getName() + ""' within "" + getClass() + "")"");
                 }
                 // In J2SE VMs, this allows overriding the value of final
                 // fields
                 field.setAccessible(true);
             }
             structField.field = field;
             structField.name = field.getName();
             structField.type = type;
 
             // Check for illegal field types
             if (Callback.class.isAssignableFrom(type) && !type.isInterface()) {
                 throw new IllegalArgumentException(""Structure Callback field '""
                                                    + field.getName()
                                                    + ""' must be an interface"");
             }
             if (type.isArray()
                 && Structure.class.equals(type.getComponentType())) {
                 String msg = ""Nested Structure arrays must use a ""
                     + ""derived Structure type so that the size of ""
                     + ""the elements can be determined"";
                 throw new IllegalArgumentException(msg);
             }
 
             int fieldAlignment = 1;
             if (!Modifier.isPublic(field.getModifiers())) {
                 continue;
             }
 
             Object value = getFieldValue(structField.field);
             if (value == null && type.isArray()) {
                 if (force) {
                     throw new IllegalStateException(""Array fields must be initialized"");
                 }
                 // can't calculate size yet, defer until later
                 return null;
             }
             Class nativeType = type;
             if (NativeMapped.class.isAssignableFrom(type)) {
                 NativeMappedConverter tc = NativeMappedConverter.getInstance(type);
                 nativeType = tc.nativeType();
                 structField.writeConverter = tc;
                 structField.readConverter = tc;
                 structField.context = new StructureReadContext(this, field);
             }
             else if (typeMapper != null) {
                 ToNativeConverter writeConverter = typeMapper.getToNativeConverter(type);
                 FromNativeConverter readConverter = typeMapper.getFromNativeConverter(type);
                 if (writeConverter != null && readConverter != null) {
                     value = writeConverter.toNative(value,
                                                     new StructureWriteContext(this, structField.field));
                     nativeType = value != null ? value.getClass() : Pointer.class;
                     structField.writeConverter = writeConverter;
                     structField.readConverter = readConverter;
                     structField.context = new StructureReadContext(this, field);
                 }
                 else if (writeConverter != null || readConverter != null) {
                     String msg = ""Structures require bidirectional type conversion for "" + type;
                     throw new IllegalArgumentException(msg);
                 }
             }
 
             if (value == null) {
                 value = initializeField(structField.field, type);
             }
 
             try {
                 structField.size = getNativeSize(nativeType, value);
                 fieldAlignment = getNativeAlignment(nativeType, value, firstField);
             }
             catch(IllegalArgumentException e) {
                 // Might simply not yet have a type mapper set yet
                 if (!force && typeMapper == null) {
                     return null;
                 }
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
 
             // Align fields as appropriate
             if (fieldAlignment == 0) {
                 throw new Error(""Field alignment is zero for field '"" + structField.name + ""' within "" + getClass());
             }
             info.alignment = Math.max(info.alignment, fieldAlignment);
             if ((calculatedSize % fieldAlignment) != 0) {
                 calculatedSize += fieldAlignment - (calculatedSize % fieldAlignment);
             }
             if (this instanceof Union) {
                 structField.offset = 0;
                 calculatedSize = Math.max(calculatedSize, structField.size);
             }
             else {
                 structField.offset = calculatedSize;
                 calculatedSize += structField.size;
             }
 
             // Save the field in our list
             info.fields.put(structField.name, structField);
 
             if (info.typeInfoField == null
                 || info.typeInfoField.size < structField.size
                 || (info.typeInfoField.size == structField.size
                     && Structure.class.isAssignableFrom(structField.type))) {
                 info.typeInfoField = structField;
             }
         }
 
         if (calculatedSize > 0) {
             int size = addPadding(calculatedSize, info.alignment);
             // Update native FFI type information, if needed
             if (this instanceof ByValue && !avoidFFIType) {
                 getTypeInfo();
             }
             info.size = size;
             return info;
         }
 
         throw new IllegalArgumentException(""Structure "" + getClass()
                                            + "" has unknown or zero size (ensure ""
                                            + ""all fields are public)"");
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The commit message ""Ensure exception message includes all faulty field information (fixes broken test)"" indicates that the commit aims to improve the clarity of exception messages, specifically by including more detailed information about the faulty field. The parenthetical remark ""(fixes broken test)"" strongly suggests that a test was failing due to the lack of sufficient information in the exception message, and this commit is intended to resolve that issue.

The diff modifies the catch block for `IllegalArgumentException` within the `deriveLayout` method. The original code threw an `IllegalArgumentException` with a message indicating an invalid structure field, but it lacked specific details about the cause of the exception. The modified code appends the exception's message (`e.getMessage()`) to the original message, providing more context about why the field is considered invalid. This change directly addresses the issue described in the commit message by including more faulty field information in the exception message.

This change is likely a bug fix because the original exception message was insufficient for debugging purposes, leading to a failing test. The improved message provides more clarity, making it easier to identify and resolve the underlying issue.

**Buggy**"
spring-framework,25502.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,9 +1,7 @@
 	public static void sortBySpecificityAndQuality(List<MediaType> mediaTypes) {
 		Assert.notNull(mediaTypes, ""'mediaTypes' must not be null"");
 		if (mediaTypes.size() > 1) {
-			Comparator<?>[] comparators = new Comparator[2];
-			comparators[0] = MediaType.SPECIFICITY_COMPARATOR;
-			comparators[1] = MediaType.QUALITY_VALUE_COMPARATOR;
-			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(comparators));
+			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(
+					MediaType.SPECIFICITY_COMPARATOR, MediaType.QUALITY_VALUE_COMPARATOR));
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Refactor and polish various Comparator impls"" suggests a focus on code improvement rather than bug fixing. The specific points mentioned are:

*   **Refactor CompoundComparator constructor to use varargs:** This is a code restructuring change.
*   **Refactor MediaType to consume new varargs constructor:** This is related to the previous point and also a restructuring change.
*   **Add notNull assertions where appropriate:** This adds defensive programming, which *could* be related to preventing a bug, but it's primarily about robustness.
*   **Add generic typing where appropriate:** This improves type safety and reduces warnings, but isn't directly bug-related.
*   **Suppress generics warnings elsewhere:** This is about cleaning up the code and doesn't indicate a bug fix.
*   **Fix whitespace errors:** This is a cosmetic change.

**Git Diff Analysis:**

The diff shows a change in `sortBySpecificityAndQuality` method.

*   The code is refactored to use the varargs constructor of `CompoundComparator`.
*   The explicit creation of the `Comparator` array is removed, making the code more concise.

The `Assert.notNull` check was already present, so the diff doesn't add any new null checks in this specific snippet.

**Reasoning:**

The commit message and the diff align. The changes are primarily refactoring to use varargs and improve code readability. While the commit message mentions adding `notNull` assertions, the diff provided doesn't show the addition of any new assertions. The changes are focused on improving the code structure and style rather than addressing a specific bug.

**Conclusion:**

NotBuggy
"
spring-framework,15028.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof NullSafeComparator)) {
 			return false;
 		}
-		NullSafeComparator other = (NullSafeComparator) obj;
+		NullSafeComparator<T> other = (NullSafeComparator<T>) obj;
 		return (this.nonNullComparator.equals(other.nonNullComparator) && this.nullsLow == other.nullsLow);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates a refactoring and polishing of Comparator implementations, including constructor changes, null assertions, generic typing, and whitespace fixes. The provided code diff modifies the `equals` method of `NullSafeComparator` to add generic typing. This change aligns with the commit message's intention to add generic typing where appropriate. There is no indication of a bug fix in this specific diff.

**NotBuggy**"
spring-framework,15035.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof InvertibleComparator)) {
 			return false;
 		}
-		InvertibleComparator other = (InvertibleComparator) obj;
+		InvertibleComparator<T> other = (InvertibleComparator<T>) obj;
 		return (this.comparator.equals(other.comparator) && this.ascending == other.ascending);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",Buggy,"The commit message indicates refactoring, polishing, and fixing whitespace errors in various Comparator implementations. The diff shows a change in the `equals` method of `InvertibleComparator`. Specifically, it adds a generic type `T` to the cast of `obj` to `InvertibleComparator`. This change suggests an improvement in type safety and potentially fixes a bug related to incorrect type handling. The addition of generic typing can be considered a bug fix if the previous code could lead to runtime errors due to type mismatches.

**Buggy**"
spring-framework,15051.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof CompoundComparator)) {
 			return false;
 		}
-		CompoundComparator other = (CompoundComparator) obj;
+		CompoundComparator<T> other = (CompoundComparator<T>) obj;
 		return this.comparators.equals(other.comparators);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",Buggy,"The commit message indicates a refactoring and polishing effort across various Comparator implementations, including changes to the `CompoundComparator`. The provided diff focuses on the `equals` method of `CompoundComparator`.

Here's a breakdown of the analysis:

1.  **Commit Message Alignment:** The commit message mentions refactoring the `CompoundComparator`. The diff shows a change within the `CompoundComparator` class, specifically in the `equals` method. This aligns with the commit message.
2.  **Bug Fix Indicators:**
    *   The change involves adding a generic type `T` to the `CompoundComparator` cast: `CompoundComparator other = (CompoundComparator) obj;` becomes `CompoundComparator<T> other = (CompoundComparator<T>) obj;`. This suggests an improvement in type safety and potentially resolves a type-related issue. While not a direct bug fix, it addresses a potential source of errors.
    *   The commit message mentions ""Add generic typing where appropriate"" which this change fulfills.
3.  **Error Handling/Logic Corrections:** The change doesn't directly involve error handling or logical corrections in the traditional sense. It's more about improving type safety.
4.  **Exception Handling:** No changes related to exception handling are present in the diff.
5.  **Other Bug-Related Changes:** The change addresses a potential type safety issue, which can be considered a bug fix in a broader sense.

Given the addition of generic typing, which enhances type safety and aligns with the commit message's intention to ""polish"" the code, it suggests a proactive measure to prevent potential type-related errors.

**Buggy**"
spring-framework,15045.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,5 +1,5 @@
 	public void invertOrder() {
-		for (InvertibleComparator comparator : this.comparators) {
+		for (InvertibleComparator<T> comparator : this.comparators) {
 			comparator.invertOrder();
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates a refactoring and polishing effort across various Comparator implementations. The provided diff snippet shows a change in the `invertOrder` method, specifically adding a generic type `T` to the `InvertibleComparator` within the loop. This change aligns with the commit message's intention to add generic typing where appropriate. The other changes mentioned in the commit message, such as using varargs, adding notNull assertions, suppressing generics warnings, and fixing whitespace errors, are not directly visible in this snippet. However, the provided diff is consistent with the overall theme of refactoring and improving the Comparator implementations. There is no indication of a bug fix in the traditional sense (i.e., correcting incorrect behavior). Instead, the changes seem to focus on improving code quality, readability, and type safety.

**NotBuggy**"
spring-framework,16701.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,8 @@
 	public ClientResponse build() {
+
 		ClientHttpResponse httpResponse =
 				new BuiltClientHttpResponse(this.statusCode, this.headers, this.cookies, this.body);
 
-		// When building ClientResponse manually, the ClientRequest.logPrefix() has to be passed,
-		// e.g. via ClientResponse.Builder, but this (builder) is not used currently.
-		return new DefaultClientResponse(httpResponse, this.strategies, """", """", () -> this.request);
+		return new DefaultClientResponse(
+				httpResponse, this.strategies, """", """", () -> this.request);
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a change to the `ClientResponse` class, specifically adding a `mutate()` method and deprecating the `from()` method. The reason for this change is that `from()` has a flaw of ignoring the body, and this cannot be fixed due to backward compatibility concerns. The commit message also suggests that `mutate()` is a better fit for filter chains and is more efficient.

The diff shows a minor change in the `build()` method of a builder class, likely associated with `ClientResponse`. The change removes a comment related to `ClientRequest.logPrefix()` and simplifies the return statement. This change doesn't seem directly related to the addition of `mutate()` or deprecation of `from()`. However, it could be a minor cleanup or adjustment made in conjunction with the other changes. The diff itself doesn't indicate a bug fix. However, the commit message explicitly mentions a flaw in the `from()` method, which can be considered a bug. The change aims to address this flaw by introducing `mutate()` as a replacement.

Therefore, the changes indicate a bug fix.
**Buggy**
"
spring-framework,16453.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,7 @@
 	public static ExchangeFilterFunction limitResponseSize(long maxByteCount) {
 		return (request, next) ->
-				next.exchange(request).map(response -> {
-					Flux<DataBuffer> body = response.body(BodyExtractors.toDataBuffers());
-					body = DataBufferUtils.takeUntilByteCount(body, maxByteCount);
-					return ClientResponse.from(response).body(body).build();
-				});
+				next.exchange(request).map(response ->
+						response.mutate()
+								.body(body -> DataBufferUtils.takeUntilByteCount(body, maxByteCount))
+								.build());
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a change to address a flaw in the `from()` method of `ClientResponse`. The message explains that `from()` ignores the body, and this cannot be fixed due to potential conflicts with existing applications. The commit introduces `mutate()` as a replacement, which is more efficient and consistent. The diff shows the replacement of `ClientResponse.from(response).body(body).build()` with `response.mutate().body(body -> DataBufferUtils.takeUntilByteCount(body, maxByteCount)).build()`. This change directly addresses the issue described in the commit message by replacing the flawed `from()` method with the new `mutate()` method. The commit message also mentions gh-24680, which likely contains more details about the bug.

**Buggy**"
spring-framework,16698.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,11 +1,10 @@
 	public ClientResponse.Builder body(String body) {
 		Assert.notNull(body, ""Body must not be null"");
 		releaseBody();
-		DataBufferFactory dataBufferFactory = new DefaultDataBufferFactory();
 		this.body = Flux.just(body).
 				map(s -> {
 					byte[] bytes = body.getBytes(StandardCharsets.UTF_8);
-					return dataBufferFactory.wrap(bytes);
+					return new DefaultDataBufferFactory().wrap(bytes);
 				});
 		return this;
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message indicates a change to the `ClientResponse` class, specifically addressing a flaw in the `from()` method and introducing a `mutate()` method as a replacement. The flaw in `from()` is related to how it handles the body, potentially ignoring it or causing issues when the body is set multiple times. The commit message also mentions consistency with server-side options and efficiency improvements.

The provided code diff shows a modification to the `body(String body)` method within the `ClientResponse.Builder` class. The change involves replacing a local variable `DataBufferFactory dataBufferFactory` with a direct instantiation of `new DefaultDataBufferFactory()` within the lambda expression.

Reasoning:

1.  **Bug Fix Indication:** The commit message explicitly mentions a flaw in the existing `from()` method. Although the provided diff doesn't directly modify `from()`, it modifies `body(String body)` which is used by the `from()` method. The change in `body(String body)` might be related to addressing the flaw mentioned in the commit message, even if indirectly.
2.  **Error Handling/Logical Correction:** The original code had a local variable `DataBufferFactory dataBufferFactory` which was used to create a `DataBuffer`. The modified code creates a new `DefaultDataBufferFactory` instance every time the lambda is executed. This change might be related to ensuring that a new `DataBufferFactory` is used for each body, potentially addressing issues with shared state or resource management. The commit message mentions the old way of doing things had a flaw.
3.  **Relevance to Commit Message:** The change in the `body(String body)` method, while seemingly minor, could be related to the issues described in the commit message regarding body handling in `from()`. The commit message indicates that the old way of doing things had a flaw.

Conclusion:

**Buggy**
"
spring-framework,16585.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,3 +1,3 @@
 	static Builder from(ClientResponse other) {
-		return new DefaultClientResponseBuilder(other);
+		return new DefaultClientResponseBuilder(other, false);
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly mentions a flaw in the `from()` method: it ignores the body of the `ClientResponse`.
    *   It states that this flaw cannot be fixed directly due to potential compatibility issues (applications already setting the body).
    *   It introduces a new `mutate()` method as a replacement, which is deemed a better fit and more efficient.
    *   The commit message refers to gh-24680, which likely contains more context about the issue.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `from()` method. Specifically, a boolean `false` is added as the second argument to the `DefaultClientResponseBuilder` constructor.
    *   The commit message suggests that `from()` is flawed because it ignores the body. The added `false` argument likely modifies the behavior of the `DefaultClientResponseBuilder` in a way that mitigates the issue, or at least prepares the ground for the new `mutate()` method to handle it correctly.

3.  **Connecting the Dots:**
    *   The commit message describes a bug (ignoring the body).
    *   The code change modifies the `from()` method, which is identified as having the bug.
    *   The introduction of `mutate()` as a replacement further supports the idea that the original `from()` method had a design flaw or bug.
    *   The change to `from()` is likely a partial fix or workaround related to the body-handling issue.

**Conclusion:**

The commit message explicitly states a flaw in the `from()` method, and the code change modifies this method. This strongly suggests that the commit addresses a bug.

**Buggy**"
sonarqube,9310.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,14 +1,13 @@
   private java.util.Optional<CeTask> submit(CeTaskSubmit submission, EnumSet<SubmitOption> submitOptions) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     try (DbSession dbSession = dbClient.openSession(false)) {
       if (submitOptions.contains(UNIQUE_QUEUE_PER_COMPONENT)
         && submission.getComponentUuid() != null
         && dbClient.ceQueueDao().countByStatusAndComponentUuid(dbSession, PENDING, submission.getComponentUuid()) > 0) {
         return java.util.Optional.empty();
       }
       CeQueueDto dto = addToQueueInDb(dbSession, submission);
       CeTask task = loadTask(dbSession, dto);
       dbSession.commit();
       return java.util.Optional.of(task);
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws mainly by removing unused code"" suggests that the primary goal of this commit is to improve code quality by removing unnecessary or redundant code. This could indirectly address bugs if the unused code was contributing to confusion or potential errors, but it doesn't explicitly state a bug fix.

**Git Diff Analysis:**

The diff shows the removal of the line `checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");`. `checkState` usually throws an exception if the condition is not met. Removing this check means that the system will now proceed even if `submitPaused.get()` is true.

**Reasoning:**

The removal of `checkState` suggests a change in the intended behavior of the `submit` method. The original code enforced a condition (`submitPaused` being false) before proceeding. Removing this check implies that the system might have been incorrectly preventing task submissions, or that the condition was deemed unnecessary. This could be a bug fix if the original check was too strict or was causing issues in certain scenarios. The commit message mentions removing unused code, but this line is not unused, it is actively preventing the code from running.

**Conclusion:**

**Buggy**
"
sonarqube,9312.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,16 +1,15 @@
   public List<CeTask> massSubmit(Collection<CeTaskSubmit> submissions, SubmitOption... options) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     if (submissions.isEmpty()) {
       return Collections.emptyList();
     }
 
     try (DbSession dbSession = dbClient.openSession(true)) {
       List<CeQueueDto> ceQueueDtos = submissions.stream()
         .filter(filterBySubmitOptions(options, submissions, dbSession))
         .map(submission -> addToQueueInDb(dbSession, submission))
         .collect(Collectors.toList());
       List<CeTask> tasks = loadTasks(dbSession, ceQueueDtos);
       dbSession.commit();
       return tasks;
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws mainly by removing unused code"" suggests that the primary goal of the commit is to improve code quality by removing unnecessary or redundant code. This could indirectly address bugs if the unused code was contributing to confusion or potential errors, but it doesn't explicitly state a bug fix.

**Git Diff Analysis:**

The diff shows the removal of the line `checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");` from the `massSubmit` method.

**Reasoning:**

1.  **Unused Code:** The commit message mentions removing unused code. Removing a `checkState` call could be considered removing unused code if it was determined that this check was no longer necessary or was redundant with another check.
2.  **Bug Fix Potential:** While the commit message focuses on code quality, removing a check like `checkState` could be related to a bug fix. For example, if the check was incorrectly preventing submissions when they should have been allowed, removing it would fix that bug. However, without more context, it's difficult to say for sure. The check might have been removed because it was redundant or because the logic around submission pausing was changed elsewhere.
3.  **Error Handling:** The removed line appears to be a form of error handling or pre-condition check. Removing it could be due to a change in how submissions are handled, or because the check was deemed unnecessary.

**Conclusion:**

Given that the commit message mentions ""Fix Quality flaws"" and the diff shows the removal of a check, it's possible that the removed check was causing unintended behavior, thus fixing a bug. While the commit message doesn't explicitly state a bug fix, the nature of the change suggests that it could be related to correcting a flaw in the system's logic.

**Buggy**
"
sonarqube,8268.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // do nothing
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message ""Fix some quality flaws"" is vague but suggests improvements to the code. The diff shows a change in the `stop()` method, where an empty method body is replaced with a comment ""// do nothing"". This change doesn't seem to address any specific bug or error. It's more likely a code cleanup or a placeholder indicating that the method is intentionally empty.

**NotBuggy**"
sonarqube,1454.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
-  public String metadata() {
-    return metadata;
+  public String internalKey() {
+    return internalKey;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague but suggests improvements or corrections were made. The code change replaces a method named `metadata()` with `internalKey()`. This could be a renaming to better reflect the method's purpose, or it could be correcting a previous misnaming. Without more context, it's difficult to definitively say if this is a bug fix. However, the change directly alters the API, suggesting a potential correction of a flaw.

**Buggy**
"
sonarqube,1485.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,4 +1,4 @@
-  public NewRule setMetadata(@Nullable String metadata) {
-    this.metadata = metadata;
+  public NewRule setInternalKey(@Nullable String s) {
+    this.internalKey = s;
     return this;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague but suggests an improvement or correction. The code diff shows a renaming of a method from `setMetadata` to `setInternalKey` and a corresponding change of the field from `metadata` to `internalKey`. This looks like a refactoring or renaming to improve code clarity or correctness, which can be considered a fix to a quality flaw.

Reasoning:
1. **Commit Message Ambiguity:** The commit message is not specific, but the term ""quality flaws"" implies some kind of deficiency being addressed.
2. **Code Change Analysis:** The code diff shows a method and field rename. This suggests a change in the internal representation or API of the class.
3. **Potential Bug Fix:** Renaming a method and field suggests a potential improvement in code clarity, maintainability, or correctness. This could be in response to a flaw.

Conclusion: **Buggy**
"
sonarqube,2297.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,14 +1,15 @@
   boolean isDescendant(DbSession dbSession, QualityProfileDto childProfile, @Nullable QualityProfileDto parentProfile) {
     QualityProfileDto currentParent = parentProfile;
     while (currentParent != null) {
       if (childProfile.getName().equals(currentParent.getName())) {
         return true;
       }
-      if (currentParent.getParentKee() != null) {
-        currentParent = db.qualityProfileDao().getByKey(dbSession, currentParent.getParentKee());
+      String parentKey = currentParent.getParentKee();
+      if (parentKey != null) {
+        currentParent = db.qualityProfileDao().getByKey(dbSession, parentKey);
       } else {
         currentParent = null;
       }
     }
     return false;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix some quality flaws"" is quite vague. It suggests that the commit addresses issues related to code quality, but it doesn't provide specific details about the nature of the flaws or the area of the code affected. This lack of specificity makes it harder to directly correlate the message with the code changes.

**Code Diff Analysis:**

The code diff focuses on the `isDescendant` method, which appears to be part of a quality profile management system. The core change lies in how the parent key is retrieved and used to fetch the parent profile.

*   **Original Code:**
    ```java
    if (currentParent.getParentKee() != null) {
        currentParent = db.qualityProfileDao().getByKey(dbSession, currentParent.getParentKee());
    }
    ```

*   **Modified Code:**
    ```java
    String parentKey = currentParent.getParentKee();
    if (parentKey != null) {
        currentParent = db.qualityProfileDao().getByKey(dbSession, parentKey);
    }
    ```

The change introduces a local variable `parentKey` to store the result of `currentParent.getParentKee()`. While seemingly minor, this change can address a potential issue:

1.  **NullPointerException Avoidance:** If `currentParent.getParentKee()` could potentially return null, the original code would directly pass this null value to `db.qualityProfileDao().getByKey()`. Depending on the implementation of `getByKey()`, this could lead to a `NullPointerException`. The modified code assigns the result to `parentKey` first, and then checks if `parentKey` is null before calling `getByKey()`. This prevents the potential `NullPointerException`.

**Reasoning:**

The introduction of the `parentKey` variable and the explicit null check suggest a defensive programming approach to prevent a potential `NullPointerException`. This aligns with the idea of fixing a ""quality flaw"" by improving the robustness of the code. The change handles a potential error condition (null parent key) more gracefully, which is a common characteristic of bug fixes.

**Conclusion:**

**Buggy**
"
sonarqube,6627.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,30 +1,34 @@
   private ActiveRuleDto doUpdate(ActiveRuleChange change, RuleActivatorContext context, DbSession dbSession) {
     ActiveRuleDto activeRule;
     ActiveRuleDao dao = db.activeRuleDao();
     activeRule = context.activeRule();
-    activeRule.setSeverity(change.getSeverity());
-    if (change.getInheritance() != null) {
-      activeRule.setInheritance(change.getInheritance().name());
+    String severity = change.getSeverity();
+    if (severity != null) {
+      activeRule.setSeverity(severity);
+    }
+    ActiveRule.Inheritance inheritance = change.getInheritance();
+    if (inheritance != null) {
+      activeRule.setInheritance(inheritance.name());
     }
     dao.update(dbSession, activeRule);
 
     for (Map.Entry<String, String> param : change.getParameters().entrySet()) {
       ActiveRuleParamDto activeRuleParamDto = context.activeRuleParamsAsMap().get(param.getKey());
       if (activeRuleParamDto == null) {
         // did not exist
         if (param.getValue() != null) {
           activeRuleParamDto = ActiveRuleParamDto.createFor(context.ruleParamsByKeys().get(param.getKey()));
           activeRuleParamDto.setValue(param.getValue());
           dao.addParam(dbSession, activeRule, activeRuleParamDto);
         }
       } else {
         if (param.getValue() != null) {
           activeRuleParamDto.setValue(param.getValue());
           dao.updateParam(dbSession, activeRule, activeRuleParamDto);
         } else {
           dao.deleteParam(dbSession, activeRule, activeRuleParamDto);
         }
       }
     }
     return activeRule;
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague. Let's analyze the diff.

The code modifies the `doUpdate` method. The changes involve updating the `ActiveRuleDto` based on the `ActiveRuleChange` object. Specifically, it updates the severity and inheritance properties of the `activeRule` object.

The original code directly sets the severity and inheritance. The modified code adds null checks before setting these properties. This suggests that the previous code might have thrown a `NullPointerException` if `change.getSeverity()` or `change.getInheritance()` returned null. The change also ensures that the `Inheritance` enum is properly handled.

The addition of null checks and proper enum handling indicates a bug fix related to potential null pointer exceptions or incorrect enum assignments.

Therefore, the changes indicate a bug fix.

**Buggy**"
sonarqube,2295.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,27 +1,27 @@
   void setParent(DbSession dbSession, String profileKey, @Nullable String parentKey) {
     QualityProfileDto profile = db.qualityProfileDao().getNonNullByKey(dbSession, profileKey);
     if (parentKey == null) {
       // unset if parent is defined, else nothing to do
       removeParent(dbSession, profile);
 
-    } else if (profile.getParentKee() == null || !profile.getParentKee().equals(parentKey)) {
+    } else if (profile.getParentKee() == null || !parentKey.equals(profile.getParentKee())) {
       QualityProfileDto parentProfile = db.qualityProfileDao().getNonNullByKey(dbSession, parentKey);
       if (isDescendant(dbSession, profile, parentProfile)) {
         throw new BadRequestException(String.format(""Descendant profile '%s' can not be selected as parent of '%s'"", parentKey, profileKey));
       }
       removeParent(dbSession, profile);
 
       // set new parent
       profile.setParentKee(parentKey);
       db.qualityProfileDao().update(dbSession, profile);
       for (ActiveRuleDto parentActiveRule : db.activeRuleDao().findByProfileKey(dbSession, parentKey)) {
         try {
           RuleActivation activation = new RuleActivation(parentActiveRule.getKey().ruleKey());
           activate(dbSession, activation, profileKey);
         } catch (BadRequestException e) {
           // for example because rule status is REMOVED
           // TODO return errors
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is quite vague. Let's analyze the code diff to understand the changes.

The core change lies in the condition `profile.getParentKee() == null || !profile.getParentKee().equals(parentKey)` being modified to `profile.getParentKee() == null || !parentKey.equals(profile.getParentKee())`.  The order of comparison in the `equals()` method has been swapped.

Initially, the code was checking if the parent key of the profile was null or if it was *not* equal to the `parentKey` argument. The original code `!profile.getParentKee().equals(parentKey)` could potentially throw a `NullPointerException` if `profile.getParentKee()` was null, even though the first part of the condition `profile.getParentKee() == null` should have prevented this.

The corrected code `!parentKey.equals(profile.getParentKee())` avoids this potential `NullPointerException` because it's calling `equals()` on `parentKey` which is guaranteed to be non-null due to the earlier `parentKey == null` check. If `profile.getParentKee()` is null, the `equals()` method will return false, and the condition will evaluate correctly.

This change addresses a potential `NullPointerException`, which is a bug.

**Buggy**"
sonarqube,6621.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,57 +1,58 @@
   private List<ActiveRuleChange> doActivate(DbSession dbSession, RuleActivation activation, RuleActivatorContext context) {
     context.verifyForActivation();
     List<ActiveRuleChange> changes = Lists.newArrayList();
     ActiveRuleChange change;
     boolean stopPropagation = false;
 
-    if (context.activeRule() == null) {
+    ActiveRuleDto activeRule = context.activeRule();
+    if (activeRule == null) {
       if (activation.isReset()) {
         // ignore reset when rule is not activated
         return changes;
       }
       // new activation
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.ACTIVATED, context.activeRuleKey());
       applySeverityAndParamToChange(activation, context, change);
       if (activation.isCascade() || context.isSameAsParent(change)) {
         change.setInheritance(ActiveRule.Inheritance.INHERITED);
       }
     } else {
       // already activated
-      if (activation.isCascade() && context.activeRule().doesOverride()) {
+      if (activation.isCascade() && activeRule.doesOverride()) {
         // propagating to descendants, but child profile already overrides rule -> stop propagation
         return changes;
       }
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.UPDATED, context.activeRuleKey());
-      if (activation.isCascade() && context.activeRule().getInheritance() == null) {
+      if (activation.isCascade() && activeRule.getInheritance() == null) {
         // activate on child, then on parent -> mark child as overriding parent
         change.setInheritance(ActiveRule.Inheritance.OVERRIDES);
         change.setSeverity(context.currentSeverity());
         change.setParameters(context.activeRuleParamsAsStringMap());
         stopPropagation = true;
       } else {
         applySeverityAndParamToChange(activation, context, change);
         if (!activation.isCascade() && context.parentActiveRule() != null) {
           // override rule which is already declared on parents
           change.setInheritance(context.isSameAsParent(change) ? ActiveRule.Inheritance.INHERITED : ActiveRule.Inheritance.OVERRIDES);
         }
       }
       if (context.isSame(change)) {
         change = null;
       }
     }
 
     if (change != null) {
       changes.add(change);
       persist(change, context, dbSession);
     }
 
     if (!stopPropagation) {
       changes.addAll(cascadeActivation(dbSession, activation, context.profile().getKey()));
     }
 
     if (!changes.isEmpty()) {
       updateProfileDate(dbSession, context);
       previewCache.reportGlobalModification(dbSession);
     }
     return changes;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message ""Fix some quality flaws"" is vague, but the diff provides more context.

The code changes primarily involve:
1. Introducing a local variable `activeRule` to store `context.activeRule()` to avoid calling it multiple times.
2. Using the local variable `activeRule` instead of calling `context.activeRule()` repeatedly.

The changes suggest an improvement in code readability and potentially performance by avoiding redundant method calls. There's no explicit error handling, logical correction, or exception handling improvement. However, reducing redundant calls and improving readability can be considered a quality improvement, potentially preventing future bugs.

Therefore, the changes align with the commit message, and although not a direct bug fix, the changes improve code quality.

**NotBuggy**"
sonarqube,11064.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,8 +1,8 @@
   private static String groupSearchToSql(@Nullable String query) {
     if (query == null) {
       return null;
     }
 
     String upperCasedNameQuery = StringUtils.upperCase(query, Locale.ENGLISH);
-    return DaoDatabaseUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
+    return DaoUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws mainly by removing unused code"" suggests that the primary goal of the commit is to improve code quality, potentially by addressing issues identified through static analysis or code review. The mention of ""removing unused code"" directly indicates a cleanup operation.

**Git Diff Analysis:**

The diff shows a single line change within the `groupSearchToSql` method. The change replaces `DaoDatabaseUtils.buildLikeValue` with `DaoUtils.buildLikeValue`.

**Reasoning:**

1.  **Potential Bug Fix:** The change from `DaoDatabaseUtils` to `DaoUtils` could be a bug fix if `DaoDatabaseUtils` was deprecated, incorrect, or causing issues. Without further context about the two classes, it's difficult to definitively say if it's a bug fix. However, the commit message mentions ""Quality flaws,"" which could include using deprecated or incorrect classes.
2.  **Code Quality Improvement:** The change could also be a simple refactoring to use a more appropriate or preferred utility class. This aligns with the ""Quality flaws"" part of the commit message.
3.  **Unused Code:** The commit message mentions removing unused code, but the diff doesn't directly show the removal of unused code. This part of the commit message doesn't seem to align with the actual code change in the diff.

**Conclusion:**

Given the commit message mentioning ""Quality flaws"" and the change of a utility class, it's plausible that this commit addresses a bug or a code quality issue. It's possible that `DaoDatabaseUtils` was causing an issue, or was simply the wrong class to use in this context. Therefore, I will classify this as a bug fix.

**Buggy**"
sonarqube,12391.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,13 +1,13 @@
   private Optional<CeQueueDto> tryToPeek(DbSession session, EligibleTaskDto eligible, String workerUuid) {
     long now = system2.now();
     int touchedRows = mapper(session).updateIf(eligible.getUuid(),
       new UpdateIf.NewProperties(IN_PROGRESS, workerUuid, eligible.getExecutionCount() + 1, now, now),
       new UpdateIf.OldProperties(PENDING, eligible.getExecutionCount()));
     if (touchedRows != 1) {
       return Optional.empty();
     }
 
     CeQueueDto result = mapper(session).selectByUuid(eligible.getUuid());
     session.commit();
-    return Optional.of(result);
+    return Optional.ofNullable(result);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws mainly by removing unused code"" suggests that the primary goal of the commit is to improve code quality, potentially by removing dead code or addressing other code quality issues. The phrase ""Fix Quality flaws"" is vague but hints at addressing existing problems.

**Git Diff Analysis:**

The diff shows a modification within the `tryToPeek` method. Specifically, the return statement has changed from `return Optional.of(result);` to `return Optional.ofNullable(result);`.

*   **`Optional.of(result)`:** This method throws a `NullPointerException` if `result` is null.
*   **`Optional.ofNullable(result)`:** This method returns an empty `Optional` if `result` is null, which is the safer and more appropriate behavior when `result` might be null.

**Reasoning:**

The change from `Optional.of(result)` to `Optional.ofNullable(result)` strongly suggests a bug fix. The original code would throw a `NullPointerException` if `mapper(session).selectByUuid(eligible.getUuid())` returned `null`, which is a potential runtime error. The updated code handles the `null` case gracefully by returning an empty `Optional`, preventing the exception. This aligns with the commit message's intention to ""Fix Quality flaws"". The original code had a flaw that could lead to a crash, and the change addresses that flaw.

**Conclusion:**

**Buggy**
"
sonarqube,14783.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,18 +1,18 @@
-  private static Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
-    if (!measureOptional.isPresent()) {
-      return Optional.absent();
-    }
-    Measure measure = measureOptional.get();
-    switch (measure.getValueType()) {
-      case DOUBLE:
-        return Optional.of(measure.getDoubleValue());
-      case LONG:
-        return Optional.of((double) measure.getLongValue());
-      case INT:
-        return Optional.of((double) measure.getIntValue());
-      case NO_VALUE:
+    private Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
+      if (!measureOptional.isPresent()) {
         return Optional.absent();
-      default:
-        throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
-    }
-  }

+      }
+      Measure measure = measureOptional.get();
+      switch (measure.getValueType()) {
+        case DOUBLE:
+          return Optional.of(measure.getDoubleValue());
+        case LONG:
+          return Optional.of((double) measure.getLongValue());
+        case INT:
+          return Optional.of((double) measure.getIntValue());
+        case NO_VALUE:
+          return Optional.absent();
+        default:
+          throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message ""Fix quality flaws"" is quite vague. Let's analyze the code diff.

The diff involves a method `getDoubleValue` that takes an `Optional<Measure>` as input and returns an `Optional<Double>`. The code extracts a `Measure` from the optional, and based on the `ValueType` of the measure, it converts the value to a double.

The only change in the code is the removal of the `static` keyword from the method signature. Removing the `static` keyword changes the method from a class-level method to an instance-level method. This change could be related to how the class is being used or how the method is being called from other parts of the code.

Given the vague commit message and the nature of the code change (removing `static`), it's difficult to determine if this is a bug fix. It could be a refactoring or a change in the class's design. However, removing static could be fixing a bug related to shared state or incorrect usage of the method.

Without more context, it's hard to say definitively. However, the change itself doesn't scream ""bug fix"". It's more likely a refactoring or design change.

**NotBuggy**"
sonarqube,14622.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,8 +1,8 @@
-  private static String convertType(Constants.ComponentLinkType reportType) {
-    String type = typesConverter.get(reportType);
-    if (type != null) {
-      return type;
-    } else {
-      throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
-    }
-  }

+    private String convertType(Constants.ComponentLinkType reportType) {
+      String type = typesConverter.get(reportType);
+      if (type != null) {
+        return type;
+      } else {
+        throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message ""Fix quality flaws"" is quite vague. Let's analyze the code diff.

The code diff shows a change in the `convertType` method. Specifically, the `static` keyword has been removed from the method signature. This change suggests that the method is no longer intended to be a static utility method, implying a potential change in the class's design or how this method is used within the class.

While the commit message mentions ""quality flaws,"" the removal of the `static` keyword doesn't inherently indicate a bug fix. It could be a refactoring or a change in the intended usage of the method. Without more context, it's difficult to determine if this change directly addresses a bug. It's possible that the previous static implementation had issues, but the diff alone doesn't provide enough information to confirm this.

Therefore, based on the provided information, it's not clear if this change is a bug fix.

**NotBuggy**"
sonarqube,14062.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,6 +1,6 @@
-  private static String computeRevision(@Nullable Changeset latestChange) {
-    if (latestChange == null) {
-      return null;
-    }
-    return latestChange.getRevision();
-  }

+    private String computeRevision(@Nullable Changeset latestChange) {
+      if (latestChange == null) {
+        return null;
+      }
+      return latestChange.getRevision();
+    }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" suggests improvements to the code's quality, which could include addressing potential bugs or vulnerabilities, enhancing readability, or improving maintainability.

The code diff changes `private static String computeRevision(@Nullable Changeset latestChange)` to `private String computeRevision(@Nullable Changeset latestChange)`. This change involves removing the `static` keyword from the method signature.

Reasoning:
1. **Static Modifier Removal:** Removing the `static` modifier suggests a change in how the method interacts with the class. A static method belongs to the class itself, while a non-static method belongs to an instance of the class. Removing `static` implies that the method now relies on instance-specific data or behavior.
2. **Potential Bug Fix:** The removal of the `static` keyword could be related to fixing a bug if the original static method was incorrectly accessing or modifying shared state, leading to unexpected behavior or data corruption. It's possible that the method's logic was intended to be instance-specific, and the `static` modifier was a mistake.
3. **Quality Improvement:** Removing the `static` keyword can also be considered a quality improvement if it aligns the method's behavior with its intended purpose and reduces the risk of unintended side effects.

Conclusion:
Based on the removal of the `static` keyword, it's plausible that this change addresses a bug related to incorrect method scope or shared state access. Therefore, the commit likely indicates a bug fix.

**Buggy**"
sonarqube,13389.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,16 +1,16 @@
-  private static DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
-    DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
-    if (sourceRange.hasStartLine()) {
-      targetRange.setStartLine(sourceRange.getStartLine());
-    }
-    if (sourceRange.hasStartOffset()) {
-      targetRange.setStartOffset(sourceRange.getStartOffset());
-    }
-    if (sourceRange.hasEndLine()) {
-      targetRange.setEndLine(sourceRange.getEndLine());
-    }
-    if (sourceRange.hasEndOffset()) {
-      targetRange.setEndOffset(sourceRange.getEndOffset());
-    }
-    return targetRange;
-  }

+    private DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
+      DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
+      if (sourceRange.hasStartLine()) {
+        targetRange.setStartLine(sourceRange.getStartLine());
+      }
+      if (sourceRange.hasStartOffset()) {
+        targetRange.setStartOffset(sourceRange.getStartOffset());
+      }
+      if (sourceRange.hasEndLine()) {
+        targetRange.setEndLine(sourceRange.getEndLine());
+      }
+      if (sourceRange.hasEndOffset()) {
+        targetRange.setEndOffset(sourceRange.getEndOffset());
+      }
+      return targetRange;
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message ""Fix quality flaws"" suggests improvements to the code's quality. The diff shows a change in the method signature of `convertTextRange` from `private static` to `private`. This change reduces the scope of the method, which can be considered a quality improvement by reducing potential external dependencies and making the code easier to maintain. The change doesn't seem to be directly related to fixing a bug, but rather improving the code's structure and maintainability.

**NotBuggy**"
sonarqube,1136.json,371ab065edbef2e92b502878eec9a73d310af54b,"@@ -1,3 +1,3 @@
     public Reader getReader() {
-      return new StringReader(source_code);
+      return new StringReader(sourceCode);
     }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message ""Fix some quality flaws"" is vague but suggests improvements to the code. The diff shows a change from `source_code` to `sourceCode`. This looks like a simple variable name change to adhere to Java naming conventions (camelCase). This is a code quality improvement but not necessarily a bug fix.

**NotBuggy**"
sonarqube,18140.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,7 +1,7 @@
   public String toString() {
     return Objects.toStringHelper(DebtRemediationFunction.class)
       .add(""type"", type)
-      .add(""coefficient"", factor)
+      .add(""coefficient"", coefficient)
       .add(""offset"", offset)
       .toString();
   }",NotBuggy,"Fix quality flaws
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix quality flaws"" is quite generic. It suggests that the commit addresses some issues related to code quality, but it doesn't provide specific details about the nature of the flaws or the type of fix applied.

**Git Diff Analysis:**

The diff shows a single line change within the `toString()` method of the `DebtRemediationFunction` class. The change replaces `factor` with `coefficient` in the `.add(""coefficient"", ...)` part of the `toString()` method.

**Reasoning:**

1.  **Potential Bug:** The change from `factor` to `coefficient` strongly suggests a bug fix. It's likely that the original code was incorrectly using the variable `factor` when it should have been using `coefficient`. This would have resulted in the wrong value being displayed in the `toString()` representation of the object, which is a bug.
2.  **Relevance:** The commit message ""Fix quality flaws"" is relevant to the code change. Using the wrong variable name is definitely a code quality issue.
3.  **Type of Bug:** This is likely a simple variable naming error, which can lead to incorrect program behavior or debugging difficulties.

**Conclusion:**

Based on the analysis of the commit message and the code diff, the change appears to be a bug fix. The commit message is vague, but the code change indicates a correction of a variable naming error.

**Buggy**"
sonarqube,18137.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,21 +1,21 @@
   private void validate() {
     switch (type) {
       case LINEAR:
-        if (this.factor == null || this.offset != null) {
+        if (this.coefficient == null || this.offset != null) {
           throw new IllegalArgumentException(String.format(""Only coefficient must be set on %s"", this));
         }
         break;
       case LINEAR_OFFSET:
-        if (this.factor == null || this.offset == null) {
+        if (this.coefficient == null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Both coefficient and offset are required on %s"", this));
         }
         break;
       case CONSTANT_ISSUE:
-        if (this.factor != null || this.offset == null) {
+        if (this.coefficient != null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Only offset must be set on %s"", this));
         }
         break;
       default:
         throw new IllegalArgumentException(String.format(""Unknown type on %s"", this));
     }
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message.

The commit message ""Fix quality flaws"" is quite generic. It doesn't give specific details about the nature of the flaws being addressed.

The diff shows changes within the `validate()` method.  The core change is renaming `factor` to `coefficient` in the conditional checks. The logic of the checks themselves remains the same. The `validate` method checks if the correct fields are set based on the `type` of the object. The checks ensure that for `LINEAR` type, only `coefficient` is set, for `LINEAR_OFFSET` both `coefficient` and `offset` are set, and for `CONSTANT_ISSUE` only `offset` is set.

Given that the only change is renaming the field `factor` to `coefficient`, it is likely a refactoring or code cleanup to improve code readability or consistency. It doesn't seem to be directly fixing a bug in the sense of correcting incorrect behavior. The validation logic remains the same. The commit message ""Fix quality flaws"" aligns with this change, as renaming a variable to a more descriptive name improves code quality.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
sonarqube,18133.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,3 +1,3 @@
   public String coefficient() {
-    return factor;
+    return coefficient;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" is quite generic. Let's analyze the code diff.

The code diff shows a change from `return factor;` to `return coefficient;` in the `coefficient()` method. This indicates that the method was likely returning the wrong value (factor instead of coefficient). This is a clear bug fix. The commit message, although vague, does align with the code change.

**Buggy**
"
sonarqube,18138.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,16 +1,14 @@
   public boolean equals(Object o) {
+    if (!(o instanceof DefaultDebtRemediationFunction)) {
+      return false;
+    }
     if (this == o) {
       return true;
     }
-    if (o == null || getClass() != o.getClass()) {
-      return false;
-    }
-    DefaultDebtRemediationFunction that = (DefaultDebtRemediationFunction) o;
-    if (factor != null ? !factor.equals(that.factor) : that.factor != null) {
-      return false;
-    }
-    if (offset != null ? !offset.equals(that.offset) : that.offset != null) {
-      return false;
-    }
-    return type == that.type;
+    DefaultDebtRemediationFunction other = (DefaultDebtRemediationFunction) o;
+    return new EqualsBuilder()
+      .append(coefficient, other.coefficient())
+      .append(offset, other.offset())
+      .append(type, other.type())
+      .isEquals();
   }",NotBuggy,"Fix quality flaws
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix quality flaws"" is quite generic. It suggests that the commit addresses issues related to code quality, which could encompass a wide range of improvements, including bug fixes, code style enhancements, or improved maintainability. The vagueness of the message makes it difficult to directly correlate it with a specific bug fix.

**Code Diff Analysis:**

The code diff modifies the `equals` method of the `DefaultDebtRemediationFunction` class. The original implementation had a few potential issues:

1.  **Null Check Inefficiency:** The original code checks `o == null` which is redundant because the `instanceof` check in the proposed change already handles nulls.
2.  **Class Check:** The original code uses `getClass() != o.getClass()` which can cause issues with subclasses. The `instanceof` check is more robust.
3.  **Manual Field Comparison:** The original code manually compares the `factor`, `offset`, and `type` fields. This is verbose and prone to errors if new fields are added to the class.
4.  **Use of `factor` instead of `coefficient`:** The original code uses `factor` which doesn't exist in the new code. The new code uses `coefficient` instead.

The new implementation addresses these issues by:

1.  Using `instanceof` for type checking, which is generally preferred for `equals` methods to handle subclass comparisons correctly.
2.  Using `EqualsBuilder` from Apache Commons Lang, which simplifies the field comparison process and reduces the risk of errors.
3.  Using the correct field name `coefficient` instead of `factor`.

**Reasoning:**

The changes in the `equals` method suggest a bug fix or at least an improvement in the correctness of the equality check. The original implementation could have led to incorrect results in certain scenarios, especially when dealing with subclasses or when comparing objects with null fields. The use of `EqualsBuilder` and the `instanceof` check makes the `equals` method more robust and reliable. The change of `factor` to `coefficient` strongly suggests a bug fix.

**Conclusion:**

**Buggy**
"
sonarqube,18139.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,6 +1,6 @@
   public int hashCode() {
     int result = type.hashCode();
-    result = 31 * result + (factor != null ? factor.hashCode() : 0);
+    result = 31 * result + (coefficient != null ? coefficient.hashCode() : 0);
     result = 31 * result + (offset != null ? offset.hashCode() : 0);
     return result;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" is quite generic. Let's analyze the diff.

The diff shows a change in the `hashCode()` method. Specifically, `factor` is changed to `coefficient`. This suggests that the field name was incorrect or misleading, and it has been corrected to a more appropriate name. This is likely a bug fix, as an incorrect field name could lead to confusion and potentially incorrect calculations or logic elsewhere in the code. The `hashCode` method is used to generate a hash code for an object, and if the field used to calculate the hash code was incorrect, it could lead to incorrect hash codes and problems with data structures like hash maps and hash sets.

Therefore, the change indicates a bug fix.

**Buggy**"
sonarqube,10480.json,d20f21e4cd409de904ef4a0ae5926b61706b402e,"@@ -1,4 +1,4 @@
-  public QualityProfileDto setParent(String parent) {
+  public QualityProfileDto setParent(@Nullable String parent) {
     this.parent = parent;
     return this;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" is quite generic. Let's analyze the code diff.

The code diff changes the `setParent` method in `QualityProfileDto` to accept a `@Nullable String` argument. This means the `parent` field can now be explicitly set to `null`. This change likely addresses a potential NullPointerException or improves the handling of cases where a parent is not defined. This adjustment directly relates to code quality by preventing potential null-related issues.

Given the addition of `@Nullable`, the change addresses a potential bug or improves the robustness of the code against null values.

**Buggy**"
sonarqube,19951.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,12 +1,12 @@
   private void autodetection() {
-    for (ScmProvider provider : providerPerKey.values()) {
-      if (provider.supports(projectReactor.getRoot().getBaseDir())) {
+    for (ScmProvider installedProvider : providerPerKey.values()) {
+      if (installedProvider.supports(projectReactor.getRoot().getBaseDir())) {
         if (this.provider == null) {
-          this.provider = provider;
+          this.provider = installedProvider;
         } else {
-          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + provider.key()
+          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + installedProvider.key()
             + "" claim to support this project. Please use "" + CoreProperties.SCM_PROVIDER_KEY + "" to define SCM of your project."");
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message ""Fix some quality flaws"" is vague. Let's analyze the code diff to understand the changes.

The code iterates through `providerPerKey.values()`, assigning each to `provider`. Inside the loop, `provider` is used to check support and potentially assign it to `this.provider`. If `this.provider` is already set, an `IllegalStateException` is thrown.

The change replaces the loop variable `provider` with `installedProvider`. This change improves readability by clarifying the variable's purpose within the loop. It doesn't seem to be directly related to fixing a bug, but rather improves the code's clarity and maintainability. The exception message is also updated to use the new variable name.

Given the variable renaming and the vague commit message, it's unlikely this commit fixes a bug.

**NotBuggy**"
sonarqube,19956.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // Nothing to do
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

The commit message ""Fix some quality flaws"" is quite vague but suggests improvements to the code's quality. The diff shows a change within the `stop()` method. Initially, the method was empty. The change adds a comment ""// Nothing to do"" inside the method.

This change doesn't seem to address any specific bug. It's more of a clarification or a placeholder. It's possible the developer added this comment to indicate that the method is intentionally empty and to prevent future developers from thinking it's incomplete or needs implementation. This is a common practice to improve code readability and maintainability, which aligns with the ""quality flaws"" mentioned in the commit message. There's no error handling, logical correction, or exception handling involved. It's simply adding a comment.

**Conclusion:**

**NotBuggy**
"
sonarqube,4891.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,28 +1,30 @@
   public void handle(Request request, Response response) throws Exception {
     int pageSize = request.mandatoryParamAsInt(Param.PAGE_SIZE);
     int page = request.mandatoryParamAsInt(Param.PAGE);
     String queryString = request.param(Param.TEXT_QUERY);
     String selected = request.mandatoryParam(Param.SELECTED);
 
     try (DbSession dbSession = dbClient.openSession(false)) {
       GroupId group = support.findGroup(dbSession, request);
       userSession.checkPermission(OrganizationPermission.ADMINISTER, group.getOrganizationUuid());
 
       UserMembershipQuery query = UserMembershipQuery.builder()
         .groupId(group.getId())
         .organizationUuid(group.getOrganizationUuid())
         .memberSearch(queryString)
         .membership(getMembership(selected))
         .pageIndex(page)
         .pageSize(pageSize)
         .build();
       int total = dbClient.groupMembershipDao().countMembers(dbSession, query);
       Paging paging = forPageIndex(page).withPageSize(pageSize).andTotal(total);
       List<UserMembershipDto> users = dbClient.groupMembershipDao().selectMembers(dbSession, query, paging.offset(), paging.pageSize());
 
-      JsonWriter json = response.newJsonWriter().beginObject();
-      writeMembers(json, users);
-      writePaging(json, paging);
-      json.endObject().close();
+      try (JsonWriter json = response.newJsonWriter()) {
+        json.beginObject();
+        writeMembers(json, users);
+        writePaging(json, paging);
+        json.endObject();
+      }
     }
   }",NotBuggy,"Fix Quality flaws
",Buggy,"The commit message ""Fix Quality flaws"" suggests an improvement in the code's quality, which could include fixing potential bugs or improving code maintainability.

The diff shows a change in how the `JsonWriter` is handled. The original code created a `JsonWriter` without a try-with-resources block, and then explicitly closed it. The updated code uses a try-with-resources block to ensure the `JsonWriter` is closed properly, even if exceptions occur. This is a good practice because it prevents resource leaks and improves the robustness of the code.

The change addresses a potential resource leak, which can be considered a bug. Therefore, the commit fixes a bug related to resource management.

**Buggy**"
sonarqube,2559.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,7 +1,9 @@
   static void writeResponse(BulkChangeResult result, Response response) {
-    JsonWriter json = response.newJsonWriter().beginObject();
-    json.prop(""succeeded"", result.countSucceeded());
-    json.prop(""failed"", result.countFailed());
-    WebServiceEngine.writeErrors(json, result.getErrors());
-    json.endObject().close();
+    try (JsonWriter json = response.newJsonWriter()) {
+      json.beginObject();
+      json.prop(""succeeded"", result.countSucceeded());
+      json.prop(""failed"", result.countFailed());
+      WebServiceEngine.writeErrors(json, result.getErrors());
+      json.endObject();
+    }
   }",NotBuggy,"Fix Quality flaws
",Buggy,"The commit message ""Fix Quality flaws"" suggests improvements to the code's quality, which could include bug fixes, but isn't explicit.

The code diff introduces a `try-with-resources` block for the `JsonWriter`. This ensures that the `JsonWriter` is properly closed after use, even if exceptions occur. This addresses a potential resource leak, which is a quality flaw. The original code might have left the `JsonWriter` open in case of an exception, potentially leading to resource exhaustion.

The change improves resource management and exception handling, indicating a fix for a potential resource leak.

**Buggy**"
sonarqube,18845.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,3 +1,3 @@
-  public void resolveOrder() throws CyclicDependenciesException {
+  public void resolveOrder() {
     resolveOrder(toString());
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague but suggests improvements to the code. The diff removes the `throws CyclicDependenciesException` from the method signature. This indicates a change in how the method handles cyclic dependencies, possibly by handling the exception internally or preventing it from occurring in the first place. This change is likely a bug fix or an improvement to the code's robustness.

Reasoning:

1.  **Commit Message:** The commit message indicates a fix for ""quality flaws,"" which can include bug fixes, improvements to error handling, or code cleanup.
2.  **Code Diff:** The removal of `throws CyclicDependenciesException` suggests that the method's error handling or dependency resolution logic has been modified. This could mean the exception is now handled internally, or the conditions that caused the exception have been addressed.
3.  **Bug Fix Indication:** The change in exception handling suggests a potential bug fix or improvement in the code's robustness. The original code might have been throwing an exception in cases where it could be avoided or handled more gracefully.

Conclusion: **Buggy**"
sonarqube,18846.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,19 +1,19 @@
-  private int resolveOrder(String path) throws CyclicDependenciesException {
+  private int resolveOrder(String path) {
     seen = true;
     try {
       int highOrder = -1;
       for (Node dep : dependencies) {
         if (dep.seen) {
           throw new CyclicDependenciesException(path + "" -> "" + dep.toString());
         }
         highOrder = Math.max(highOrder, dep.resolveOrder(path + "" -> "" + dep.toString()));
 
       }
 
       order = highOrder + 1;
       return order;
 
     } finally {
       seen = false;
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague, but the code diff provides more context.

The key change is the removal of `throws CyclicDependenciesException` from the `resolveOrder` method signature. This suggests that the method is no longer directly throwing this exception. Looking at the code, the `try...finally` block remains, and the `CyclicDependenciesException` is still thrown within the `try` block. This implies that the exception is now being caught and handled (or re-thrown as a different exception) within the method itself, rather than being propagated up the call stack.

The removal of the `throws` declaration indicates a change in how the method interacts with the exception, likely to improve error handling or prevent unexpected program termination. This kind of change often addresses a bug or potential issue related to exception propagation.

Therefore, the changes indicate a bug fix related to exception handling.

**Buggy**"
sonarqube,8517.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,35 +1,33 @@
   public static Collection<String> listResources(ClassLoader classLoader, String rootPath, Predicate<String> predicate) {
     try {
       Collection<String> paths = Lists.newArrayList();
       rootPath = StringUtils.removeStart(rootPath, ""/"");
 
       URL root = classLoader.getResource(rootPath);
       if (root != null) {
-        if (!""jar"".equals(root.getProtocol())) {
-          throw new IllegalStateException(""Unsupported protocol: "" + root.getProtocol());
-        }
+        checkJarFile(root);
 
         // Path of the root directory
         // Examples :
         // org/sonar/sqale/index.txt  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale/  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale  -> rootDirectory is org/sonar/sqale
         String rootDirectory = rootPath;
         if (StringUtils.substringAfterLast(rootPath, ""/"").indexOf('.') >= 0) {
           rootDirectory = StringUtils.substringBeforeLast(rootPath, ""/"");
         }
         String jarPath = root.getPath().substring(5, root.getPath().indexOf(""!"")); //strip out only the JAR file
         JarFile jar = new JarFile(URLDecoder.decode(jarPath, CharEncoding.UTF_8));
         Enumeration<JarEntry> entries = jar.entries();
         while (entries.hasMoreElements()) {
           String name = entries.nextElement().getName();
           if (name.startsWith(rootDirectory) && predicate.apply(name)) {
             paths.add(name);
           }
         }
       }
       return paths;
     } catch (Exception e) {
       throw Throwables.propagate(e);
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message ""Fix some quality flaws"" is vague. Let's analyze the code diff.

The original code had a check for the protocol of the URL being ""jar"". If it wasn't, it threw an `IllegalStateException`. This check has been replaced with a call to `checkJarFile(root)`. We don't have the implementation of `checkJarFile`, but the name suggests it's doing a similar check, possibly with more robust error handling or logging.

The change suggests a potential bug fix related to how the code handles non-jar URLs or perhaps a more general improvement in handling different types of URLs. It's likely that the original code was too strict or didn't handle certain edge cases correctly.

Therefore, the changes indicate a bug fix.

**Buggy**"
sonarqube,7268.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,14 +1,14 @@
   private UserDto synchronize(String userLogin, UserDetails details, HttpServletRequest request) {
     String name = details.getName();
     UserIdentity.Builder userIdentityBuilder = UserIdentity.builder()
       .setLogin(userLogin)
-      .setName(isNullOrEmpty(name) ? userLogin : name)
+      .setName(isEmpty(name) ? userLogin : name)
       .setEmail(trimToNull(details.getEmail()))
       .setProviderLogin(userLogin);
     if (externalGroupsProvider != null) {
       ExternalGroupsProvider.Context context = new ExternalGroupsProvider.Context(userLogin, request);
       Collection<String> groups = externalGroupsProvider.doGetGroups(context);
       userIdentityBuilder.setGroups(new HashSet<>(groups));
     }
     return userIdentityAuthenticator.authenticate(userIdentityBuilder.build(), new ExternalIdentityProvider());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" suggests improvements to the code's quality, specifically addressing:

1.  Missing tests for Protobuf.
2.  Removal of incorrect imports of `org.elasticsearch.common.Strings`.

The second point directly indicates a bug fix related to incorrect dependencies.

**Git Diff Analysis:**

The diff shows a change in the `synchronize` method. The line:

`.setName(isNullOrEmpty(name) ? userLogin : name)`

is changed to:

`.setName(isEmpty(name) ? userLogin : name)`

This change suggests that the original code used `isNullOrEmpty`, which might not have been the correct method for checking if a name is empty. The change to `isEmpty` suggests a correction to the logic, which could be considered a bug fix. It's likely that `isNullOrEmpty` was behaving unexpectedly, leading to incorrect name handling.

**Reasoning:**

The commit message explicitly mentions removing incorrect imports, which is a bug fix. The code change replaces `isNullOrEmpty` with `isEmpty`. While the exact impact depends on the implementation of these methods, it strongly suggests a correction to the logic for handling empty names. This correction addresses a potential flaw in the code's behavior, which can be considered a bug fix.

**Conclusion:**

**Buggy**
"
sonarqube,7206.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,12 +1,12 @@
   private static Optional<String> getTokenFromCookie(HttpServletRequest request) {
     Optional<Cookie> jwtCookie = findCookie(JWT_COOKIE, request);
     if (!jwtCookie.isPresent()) {
       return Optional.empty();
     }
     Cookie cookie = jwtCookie.get();
     String token = cookie.getValue();
-    if (isNullOrEmpty(token)) {
+    if (isEmpty(token)) {
       return Optional.empty();
     }
     return Optional.of(token);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" indicates improvements to the codebase's quality. Specifically, it mentions:

1.  Missing tests for Protobuf (not present in the diff).
2.  Removal of incorrect imports of `org.elasticsearch.common.Strings`.

**Diff Analysis:**

The diff shows a change in the `getTokenFromCookie` method. The condition `isNullOrEmpty(token)` is replaced with `isEmpty(token)`.

**Reasoning:**

1.  **`isNullOrEmpty` vs. `isEmpty`:** The change from `isNullOrEmpty(token)` to `isEmpty(token)` suggests a correction in how the token's validity is checked. It's possible that `isNullOrEmpty` was incorrectly implemented or had unintended side effects, leading to incorrect behavior.  The `isEmpty` method is likely the correct way to check for an empty string. This is a logical correction.

2.  **Relevance to Commit Message:** The commit message mentions ""Quality flaws."" While the diff doesn't directly relate to Protobuf tests, the change from `isNullOrEmpty` to `isEmpty` can be considered a quality improvement because it fixes a potential logical flaw in the code. It's also possible that `org.elasticsearch.common.Strings` contained the `isNullOrEmpty` method, and its removal necessitated this change.

3.  **Bug Fix Indication:** The change suggests that the previous implementation (`isNullOrEmpty`) might have caused issues, such as not correctly identifying empty tokens, which could lead to unexpected behavior or errors. This points towards a bug fix.

**Conclusion:**

The change from `isNullOrEmpty` to `isEmpty` indicates a correction in the token validation logic, which can be considered a bug fix. The change aligns with the commit message's intention to fix quality flaws.

**Buggy**"
sonarqube,3674.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,7 +1,7 @@
   private List<Setting> loadDefaultSettings(Set<String> keys) {
     return propertyDefinitions.getAll().stream()
       .filter(definition -> keys.contains(definition.key()))
-      .filter(defaultProperty -> !isNullOrEmpty(defaultProperty.defaultValue()))
+      .filter(defaultProperty -> !isEmpty(defaultProperty.defaultValue()))
       .map(Setting::createForDefinition)
       .collect(Collectors.toList());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" is quite broad but specifies two main points:

1.  ""missing tests for Protobuf"": This implies the addition of tests, which could be related to fixing a bug or preventing future ones. However, the provided diff doesn't show any test-related changes.
2.  ""remove incorrect imports of org.elasticsearch.common.Strings"": This suggests a cleanup of unnecessary or incorrect imports, which is a code quality improvement.

**Git Diff Analysis:**

The diff shows a change in the `loadDefaultSettings` method. Specifically, `isNullOrEmpty` is replaced with `isEmpty`. This suggests that `isNullOrEmpty` was either deprecated, incorrect, or not behaving as intended. The change likely aims to correctly check if a default property's value is empty.

**Reasoning:**

The change from `isNullOrEmpty` to `isEmpty` strongly suggests a bug fix. The original code might have been incorrectly determining whether a default value was present, potentially leading to incorrect settings being loaded. The commit message mentions ""Quality flaws,"" and this change directly addresses a potential flaw in the logic related to default settings. The removal of incorrect imports, while a quality improvement, doesn't directly point to a bug fix in the same way. The core logic change related to checking for empty default values is the primary indicator.

**Conclusion:**

**Buggy**
"
sonarqube,4743.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,8 +1,8 @@
   private UserDto authenticate(HttpServletRequest request) {
     String login = request.getParameter(""login"");
     String password = request.getParameter(""password"");
-    if (isNullOrEmpty(login) || isNullOrEmpty(password)) {
+    if (isEmpty(login) || isEmpty(password)) {
       throw new UnauthorizedException();
     }
     return credentialsAuthenticator.authenticate(login, password, request);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" suggests improvements to the code's quality. The message specifies two changes:

1.  ""missing tests for Protobuf"": This indicates the addition of tests, which improves code coverage and reliability.
2.  ""remove incorrect imports of org.elasticsearch.common.Strings"": This suggests the removal of an unnecessary or incorrect import statement, cleaning up the codebase.

**Git Diff Analysis:**

The diff shows a change within the `authenticate` method:

*   `if (isNullOrEmpty(login) || isNullOrEmpty(password))` is changed to `if (isEmpty(login) || isEmpty(password))`

This change suggests that the method `isNullOrEmpty` was replaced with `isEmpty`. This could be due to a few reasons:

1.  `isNullOrEmpty` was deprecated or removed, and `isEmpty` is the new recommended way.
2.  `isNullOrEmpty` had a bug or unexpected behavior that `isEmpty` fixes.
3.  `isEmpty` is a more appropriate or efficient method for checking if a string is null or empty in this context.

Given the context of ""Quality flaws"", it's likely that `isNullOrEmpty` was either deprecated or potentially buggy, and `isEmpty` provides a more reliable or correct implementation.

**Reasoning:**

The commit message indicates a focus on code quality. The diff shows a change from `isNullOrEmpty` to `isEmpty`. While it's a small change, it's likely related to fixing a potential issue with how null or empty strings were being handled, or using a deprecated method. The removal of incorrect imports also contributes to code quality.

**Conclusion:**

The change from `isNullOrEmpty` to `isEmpty` suggests a correction or improvement in how the code handles null or empty strings, which can be considered a bug fix or at least a bug prevention measure.

**Buggy**"
sonarqube,20010.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,9 +1,9 @@
   private static void cleanTempFolders(Path path) throws IOException {
-    if (Files.exists(path)) {
+    if (path.toFile().exists()) {
       try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, new CleanFilter())) {
         for (Path p : stream) {
           deleteQuietly(p.toFile());
         }
       }
     }
   }",NotBuggy,"Fix Quality flaws
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" is quite generic. It suggests that the commit addresses some issues related to code quality, but it doesn't provide specific details about the nature of the flaws or how they are being fixed. This lack of specificity makes it harder to directly correlate the message with the code changes.

**Git Diff Analysis:**

The diff shows a change within the `cleanTempFolders` method. Specifically, the condition `Files.exists(path)` is replaced with `path.toFile().exists()`.

*   **`Files.exists(path)`:** This uses the NIO.2 API to check if a path exists.
*   **`path.toFile().exists()`:** This converts the `Path` object to a `File` object and then uses the older `File.exists()` method.

**Reasoning:**

The change from `Files.exists(path)` to `path.toFile().exists()` suggests a potential issue with how the existence of the path was being checked previously. The `Files.exists()` method might have been behaving unexpectedly in certain situations, leading to incorrect behavior in the `cleanTempFolders` method. The older `File.exists()` method might be more reliable in the specific environment where this code is running, or it might be addressing a subtle difference in how symbolic links or other special file types are handled.

Given the change involves checking for the existence of a file or directory, and the commit message mentions fixing quality flaws, it's plausible that the original code had a bug where it was failing to correctly identify the existence of the temporary folders, leading to errors or unexpected behavior during cleanup.

**Conclusion:**

Based on the code change and the commit message, it's likely that this commit addresses a bug related to file existence checking. Therefore, I conclude:

**Buggy**"
sonarqube,20011.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,24 +1,24 @@
     public boolean accept(Path path) throws IOException {
-      if (!Files.isDirectory(path)) {
+      if (!path.toFile().exists()) {
         return false;
       }
 
       if (!path.getFileName().toString().startsWith(TMP_NAME_PREFIX)) {
         return false;
       }
 
       long threshold = System.currentTimeMillis() - CLEAN_MAX_AGE;
 
       // we could also check the timestamp in the name, instead
       BasicFileAttributes attrs;
 
       try {
         attrs = Files.readAttributes(path, BasicFileAttributes.class);
       } catch (IOException ioe) {
         LOG.error(String.format(""Couldn't read file attributes for %s : "", path), ioe);
         return false;
       }
 
       long creationTime = attrs.creationTime().toMillis();
       return creationTime < threshold;
     }",NotBuggy,"Fix Quality flaws
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fix Quality flaws"" is quite vague. It doesn't specify what kind of quality flaws are being addressed. This makes it harder to directly correlate with the code changes. However, the term ""flaws"" suggests there might be some existing issues in the code.

**2. Code Diff Analysis:**

*   **Change 1:** `if (!Files.isDirectory(path))` changed to `if (!path.toFile().exists())`

    *   The original code checked if the path is a directory. The modified code checks if the file exists. This is a significant change in logic. The original check would return `false` if the path was not a directory, even if a regular file existed at that path. The new check returns `false` if *anything* doesn't exist at that path. This could be fixing a bug where the code was incorrectly rejecting valid files because they weren't directories.
*   **Change 2:** `IOException` handling in `Files.readAttributes`:

    *   The code already had a `try-catch` block around `Files.readAttributes`. The catch block logs an error and returns `false`. This indicates that the original code already anticipated potential `IOExceptions` during attribute reading.

**3. Reasoning:**

The change from `Files.isDirectory(path)` to `path.toFile().exists()` is the most telling. The original code was likely too restrictive, only accepting directories. The new code is more general, accepting any existing file (or directory). This suggests a bug where valid files were being incorrectly rejected. The `IOException` handling was already in place, so it's not a new bug fix in itself, but reinforces the idea that the code is dealing with potential file system issues.

**Conclusion:**

**Buggy**
"
sonarqube,8900.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,7 +1,7 @@
-  public static void main(String[] args) throws Exception {
+  public static void main(String[] args) {
     ProcessEntryPoint entryPoint = ProcessEntryPoint.createForArguments(args);
     Props props = entryPoint.getProps();
     new WebServerProcessLogging().configure(props);
     WebServer server = new WebServer(props);
     entryPoint.launch(server);
   }",NotBuggy,"Fix Quality flaws
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Quality flaws"" is quite vague. It doesn't specify what kind of quality flaws were addressed. This makes it difficult to directly correlate it with specific code changes. However, the word ""fix"" suggests that the commit aims to correct something that was not working as intended or was considered substandard.

**Code Diff Analysis:**

The code diff shows a change in the `main` method signature. Specifically, the `throws Exception` clause has been removed.

**Reasoning:**

1.  **Exception Handling:** The removal of `throws Exception` suggests a change in how exceptions are handled in the `main` method. Previously, any exception thrown within the `main` method would be propagated up to the JVM, potentially leading to unhandled exceptions and program termination. Removing this clause implies that the code now handles exceptions internally, preventing them from bubbling up. This is a common practice to improve the robustness and stability of an application.

2.  **Quality Improvement:** Removing `throws Exception` can be considered a quality improvement. It makes the code more robust by explicitly handling exceptions instead of relying on the default JVM behavior. This can prevent unexpected crashes and provide more informative error messages to the user or administrator.

3.  **Bug Fix Indication:** The change strongly suggests a bug fix. Unhandled exceptions are often the cause of application crashes and unexpected behavior. By removing the `throws Exception` clause and presumably handling exceptions internally, the code is now more resilient to errors, which can be considered a bug fix.

**Conclusion:**

Based on the analysis of the commit message and code diff, the changes indicate a bug fix related to exception handling. Therefore, the answer is:

**Buggy**
"
sonarqube,4469.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,15 +1,15 @@
   public void handle(Request wsRequest, Response wsResponse) throws Exception {
     try (DbSession dbSession = dbClient.openSession(false)) {
       WsTemplateRef templateRef = WsTemplateRef.fromRequest(wsRequest);
       PermissionTemplateDto template = support.findTemplate(dbSession, templateRef);
       checkGlobalAdmin(userSession, template.getOrganizationUuid());
 
       PermissionQuery query = buildPermissionQuery(wsRequest);
-      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getId());
+      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getOrganizationUuid(), template.getId());
       Paging paging = Paging.forPageIndex(wsRequest.mandatoryParamAsInt(PAGE)).withPageSize(wsRequest.mandatoryParamAsInt(PAGE_SIZE)).andTotal(total);
       List<GroupDto> groups = findGroups(dbSession, query, template);
       List<PermissionTemplateGroupDto> groupPermissions = findGroupPermissions(dbSession, groups, template);
       WsPermissions.WsGroupsResponse groupsResponse = buildResponse(groups, groupPermissions, paging);
       writeProtobuf(groupsResponse, wsRequest, wsResponse);
     }
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-8682 fix error when filtering on name and a given group name exists in multiple organizations"" clearly states that the commit addresses an error scenario. The error occurs when filtering by name, and the same group name exists in multiple organizations. This suggests a bug related to how the filtering logic handles duplicate group names across different organizations.

**Git Diff Analysis:**

The diff shows a change in the `handle` method. Specifically, the `dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate` method call has been modified.

*   **Original Code:**
    ```java
    int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getId());
    ```
*   **Modified Code:**
    ```java
    int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getOrganizationUuid(), template.getId());
    ```

The change involves adding `template.getOrganizationUuid()` as an argument to the `countGroupNamesByQueryAndTemplate` method. This suggests that the original query for counting group names was not considering the organization's UUID, which would lead to an incorrect count when the same group name exists in multiple organizations. By adding the organization UUID, the query is now more specific and should return the correct count for the given organization and template.

**Reasoning:**

The commit message and the code diff align perfectly. The commit message describes an error related to filtering group names across multiple organizations. The code change modifies the query to include the organization UUID, which would resolve the issue of incorrectly counting group names when they exist in multiple organizations. This indicates that the original code had a bug where it wasn't properly scoping the group name count to the correct organization.

**Conclusion:**

**Buggy**
"
sonarqube,12672.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).selectGroupNamesByQueryAndTemplate(query, templateId, new RowBounds(query.getPageOffset(), query.getPageSize()));
+  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).selectGroupNamesByQueryAndTemplate(organizationUuid, templateId, query, new RowBounds(query.getPageOffset(), query.getPageSize()));
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error that occurs when filtering on a group name that exists in multiple organizations. The diff shows a change in the `selectGroupNamesByQueryAndTemplate` method signature. Specifically, an `organizationUuid` parameter has been added. This suggests that the original method was not correctly handling cases where the same group name existed in multiple organizations, and the added parameter is used to differentiate between them. This is a clear indication of a bug fix.

**Buggy**"
sonarqube,4472.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,8 +1,8 @@
   private List<GroupDto> findGroups(DbSession dbSession, PermissionQuery dbQuery, PermissionTemplateDto template) {
-    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getId());
+    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getOrganizationUuid(), template.getId());
     List<GroupDto> groups = dbClient.groupDao().selectByNames(dbSession, template.getOrganizationUuid(), orderedNames);
     if (orderedNames.contains(DefaultGroups.ANYONE)) {
       groups.add(0, new GroupDto().setId(0L).setName(DefaultGroups.ANYONE));
     }
     return Ordering.explicit(orderedNames).onResultOf(GroupDto::getName).immutableSortedCopy(groups);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-8682 fix error when filtering on name and a given group name exists in multiple organizations"" suggests that there was an error in the filtering logic when searching for groups by name, specifically when the same group name exists across multiple organizations. This strongly implies a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `findGroups` method.  The key change is in the line:

`-   List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getId());`
`+   List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getOrganizationUuid(), template.getId());`

The change involves adding `template.getOrganizationUuid()` as a parameter to the `selectGroupNamesByQueryAndTemplate` method call.

**Reasoning:**

The original code was likely only using the `template.getId()` (presumably the template's ID) to retrieve group names. This would lead to an issue if the same group name existed in multiple organizations because the query wouldn't be specific to the organization. By adding `template.getOrganizationUuid()`, the query now includes the organization's UUID, ensuring that only group names within the correct organization are retrieved. This aligns perfectly with the commit message's description of the bug. The fix ensures that the group name filtering is organization-specific, resolving the error when a group name exists in multiple organizations.

**Conclusion:**

The code change directly addresses the issue described in the commit message. The addition of the organization UUID to the query fixes the bug where group names were not being filtered correctly when they existed in multiple organizations.

**Buggy**
"
sonarqube,12673.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).countGroupNamesByQueryAndTemplate(query, templateId);
+  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).countGroupNamesByQueryAndTemplate(organizationUuid, query, templateId);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-8682 fix error when filtering on name and a given group name exists in multiple organizations"" clearly states that the commit addresses an error that occurs when filtering group names, specifically when the same group name exists in multiple organizations. This suggests a bug fix related to how group names are handled in the context of multiple organizations.

**Git Diff Analysis:**

The diff shows a change in the `countGroupNamesByQueryAndTemplate` method signature.

*   **Original:** `public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId)`
*   **Modified:** `public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId)`

The key change is the addition of the `String organizationUuid` parameter. This strongly suggests that the original implementation was not considering the organization when counting group names, which could lead to incorrect counts if the same group name existed in multiple organizations. The addition of `organizationUuid` allows the method to scope the count to a specific organization, resolving the issue described in the commit message. The order of parameters was also changed.

**Reasoning:**

The commit message explicitly mentions an error related to filtering group names when they exist in multiple organizations. The code change introduces an `organizationUuid` parameter to the `countGroupNamesByQueryAndTemplate` method, which directly addresses the issue described in the commit message by allowing the method to filter group names based on the organization. This indicates that the original implementation had a bug where it was not considering the organization when counting group names, leading to incorrect results.

**Conclusion:**

**Buggy**
"
voldemort,9608.json,4a463214895e56db53db78064bde3ebd10b6e949,"@@ -1,8 +1,8 @@
-    public void recordOpTimeUs(SocketDestination dest, long opTimeUs) {
+    public void recordOpTimeNs(SocketDestination dest, long opTimeNs) {
         if (dest != null) {
-            getOrCreateNodeStats(dest).recordOpTimeUs(null, opTimeUs);
-            recordOpTimeUs(null, opTimeUs);
+            getOrCreateNodeStats(dest).recordOpTimeNs(null, opTimeNs);
+            recordOpTimeNs(null, opTimeNs);
         } else {
-            this.opTimeRequestCounter.addRequest(opTimeUs * Time.NS_PER_US);
+            this.opTimeRequestCounter.addRequest(opTimeNs);
         }
     }",Buggy,"Fix time convertion (Nano to micro) bug

Make the method accept nanoseconds to avoid unnecessary
convertion.
",Buggy,"The commit message indicates a bug fix related to time conversion, specifically from nanoseconds to microseconds. The code diff shows that the method `recordOpTimeUs` has been renamed to `recordOpTimeNs`, and the parameter type remains `long`. The core change is that the method now accepts nanoseconds directly, avoiding an unnecessary conversion. The line `this.opTimeRequestCounter.addRequest(opTimeUs * Time.NS_PER_US);` has been changed to `this.opTimeRequestCounter.addRequest(opTimeNs);`. This confirms that the previous code was performing an unnecessary conversion from microseconds to nanoseconds before adding the request. This change aligns with the commit message, indicating a bug fix where an incorrect time conversion was being performed.

**Buggy**"
voldemort,8659.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,15 +1,15 @@
         void streamingPut(ByteArray key, Versioned<byte[]> value) {
-            if(overwrite == false) {
-                dstStreamingClient.streamingPut(key, value);
-            } else {
+            if(overwrite) {
                 VectorClock denseClock = VectorClockUtils.makeClockWithCurrentTime(dstServerIds);
                 Versioned<byte[]> updatedValue = new Versioned<byte[]>(value.getValue(), denseClock);
                 dstStreamingClient.streamingPut(key, updatedValue);
+            } else {
+                dstStreamingClient.streamingPut(key, value);
             }
 
             entriesForkLifted++;
             if(entriesForkLifted % progressOps == 0) {
                 logger.info(workName + "" fork lifted "" + entriesForkLifted
                             + "" entries successfully"");
             }
         }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Incorporate code review comments\n\nFix error message\nChange the order of if/else"" suggests two primary changes:

1.  **Incorporating code review comments:** This is a general statement and doesn't directly imply a bug fix, but it suggests improvements based on feedback.
2.  **Fix error message:** This explicitly indicates a bug fix related to an error message.
3.  **Change the order of if/else:** This could be a bug fix if the original order was logically incorrect, leading to unexpected behavior.

**Git Diff Analysis:**

The diff shows a change in the order of an `if/else` block within the `streamingPut` method. The original code had `if(overwrite == false)` and the new code has `if(overwrite)`. The logic within the `if` and `else` blocks has been swapped.

**Reasoning:**

The commit message explicitly states that the order of the `if/else` statement was changed. This change, combined with the implication that the original order was incorrect (hence the need to ""change"" it), suggests a logical correction. The original code likely had a condition that was evaluated incorrectly, leading to the wrong branch being executed. By changing the order and the condition, the code now behaves as intended. Also, the commit message mentions fixing an error message, which is a strong indicator of a bug fix.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix. The `if/else` order change suggests a logical correction, and the error message fix further supports this conclusion.

**Buggy**"
voldemort,8669.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,82 +1,82 @@
     public static void main(String[] args) throws Exception {
         OptionParser parser = null;
         OptionSet options = null;
         try {
             parser = getParser();
             options = parser.parse(args);
         } catch(Exception oe) {
             logger.error(""Exception processing command line options"", oe);
             parser.printHelpOn(System.out);
             return;
         }
 
         /* validate options */
         if(options.has(""help"")) {
             parser.printHelpOn(System.out);
             return;
         }
 
         if(!options.has(""src-url"") || !options.has(""dst-url"")) {
             logger.error(""Both 'src-url' and 'dst-url' options are mandatory"");
             parser.printHelpOn(System.out);
             return;
         }
 
         String srcBootstrapUrl = (String) options.valueOf(""src-url"");
         String dstBootstrapUrl = (String) options.valueOf(""dst-url"");
         int maxPutsPerSecond = DEFAULT_MAX_PUTS_PER_SEC;
         if(options.has(""max-puts-per-second""))
             maxPutsPerSecond = (Integer) options.valueOf(""max-puts-per-second"");
         List<String> storesList = null;
         if(options.has(""stores"")) {
             storesList = new ArrayList<String>((List<String>) options.valuesOf(""stores""));
         }
         List<Integer> partitions = null;
         if(options.has(""partitions"")) {
             partitions = (List<Integer>) options.valuesOf(""partitions"");
         }
 
         int partitionParallelism = DEFAULT_PARTITION_PARALLELISM;
         if(options.has(""parallelism"")) {
             partitionParallelism = (Integer) options.valueOf(""parallelism"");
         }
         int progressOps = DEFAULT_PROGRESS_PERIOD_OPS;
         if(options.has(""progress-period-ops"")) {
             progressOps = (Integer) options.valueOf(""progress-period-ops"");
         }
 
         ForkLiftTaskMode mode;
         mode = ForkLiftTaskMode.primary_resolution;
         if(options.has(""mode"")) {
             mode = Utils.getEnumFromString(ForkLiftTaskMode.class, (String) options.valueOf(""mode""));
             if(mode == null)
                 mode = ForkLiftTaskMode.primary_resolution;
 
         }
 
         Boolean overwrite = false;
-        if(options.has(""overwrite"")) {
-            if(options.hasArgument(""overwrite"")) {
-                overwrite = (Boolean) options.valueOf(""overwrite"");
+        if(options.has(OVERWRITE_OPTION)) {
+            if(options.hasArgument(OVERWRITE_OPTION)) {
+                overwrite = (Boolean) options.valueOf(OVERWRITE_OPTION);
             } else {
                 overwrite = true;
             }
         }
 
         if(overwrite) {
             logger.warn(OVERWRITE_WARNING_MESSAGE);
         }
 
         ClusterForkLiftTool forkLiftTool = new ClusterForkLiftTool(srcBootstrapUrl,
                                                                    dstBootstrapUrl,
                                                                    overwrite,
                                                                    maxPutsPerSecond,
                                                                    partitionParallelism,
                                                                    progressOps,
                                                                    storesList,
                                                                    partitions,
                                                                    mode);
         forkLiftTool.run();
         // TODO cleanly shut down the hanging threadpool
         System.exit(0);
     }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",Buggy,"The commit message indicates several changes: incorporating code review comments, fixing an error message, and changing the order of if/else statements.

The diff shows a change in the `if(options.has(""overwrite""))` block. Specifically, `""overwrite""` is replaced by `OVERWRITE_OPTION`. This suggests a refactoring where the string literal `""overwrite""` is replaced by a constant `OVERWRITE_OPTION`. This change is likely related to code review comments and improves code maintainability.

The change in the `if` condition from `options.has(""overwrite"")` to `options.has(OVERWRITE_OPTION)` doesn't inherently indicate a bug fix. However, the commit message explicitly mentions fixing an error message and changing the order of if/else statements, which are common indicators of bug fixes. While the provided diff doesn't directly show the error message fix or the if/else reordering, the overall context suggests that the commit addresses some underlying issue.

Therefore, based on the commit message and the nature of the code change, it's likely that this commit includes a bug fix.

**Buggy**"
voldemort,9827.json,8fe6ef89e6f219a246413f918167b573f69e070c,"@@ -1,18 +1,20 @@
     public boolean delete(ByteArray key, Version version) throws VoldemortException {
         boolean deleteSuccessful = false;
         StoreUtils.assertValidKey(key);
         String keyString = new String(key.get());
         String initialValue = this.metadataMap.get(keyString);
         if(initialValue != null) {
             String removedValue = this.metadataMap.remove(keyString);
             if(removedValue != null) {
                 deleteSuccessful = (initialValue.equals(removedValue));
             }
         }
         if(deleteSuccessful) {
             this.flushData();
             // Reset the vector clock and persist it.
+            // FIXME this also needs to be done per entry, as opposed to
+            // versioning the file.
             writeVersion(new VectorClock());
         }
         return deleteSuccessful;
     }",NotBuggy,"Workaround SystemStore bug and fix quota store inserts
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Workaround SystemStore bug and fix quota store inserts"" suggests that the commit addresses a bug in the `SystemStore` and fixes issues related to quota store inserts. The term ""workaround"" implies a temporary solution to a problem, further hinting at a bug.

**2. Code Diff Analysis:**

The code diff shows a modification within the `delete` method. Specifically, a comment `// FIXME this also needs to be done per entry, as opposed to versioning the file.` has been added.

**3. Reasoning:**

*   The commit message explicitly mentions a bug.
*   The added comment `// FIXME` indicates an area of code that requires further attention and is potentially a temporary or incomplete solution. The comment suggests that the current versioning strategy for the file is not ideal and should be done per entry. This implies that the existing implementation might be incorrect or have limitations, which aligns with the idea of a bug fix or workaround.
*   The fact that the commit message mentions ""workaround"" and the code contains a ""FIXME"" comment strongly suggests that the change is related to addressing a bug or deficiency in the existing code.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix or a workaround for a bug.

**Buggy**"
voldemort,9441.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,35 +1,36 @@
     public void run() {
         Node proxyNode = metadata.getCluster().getNodeById(destinationNode);
         long startNs = System.nanoTime();
         try {
             // TODO there are no retries now if the node we want to write to is
             // unavailable
             redirectingStore.checkNodeAvailable(proxyNode);
             Store<ByteArray, byte[], byte[]> socketStore = redirectingStore.getRedirectingSocketStore(redirectingStore.getName(),
                                                                                                       destinationNode);
 
             socketStore.put(key, value, transforms);
             redirectingStore.recordSuccess(proxyNode, startNs);
+            redirectingStore.reportProxyPutSuccess();
             if(logger.isTraceEnabled()) {
                 logger.trace(""Proxy write for store "" + redirectingStore.getName() + "" key ""
-                             + ByteUtils.toBinaryString(key.get()) + "" to destinationNode:""
+                             + ByteUtils.toHexString(key.get()) + "" to destinationNode:""
                              + destinationNode);
             }
         } catch(UnreachableStoreException e) {
             redirectingStore.recordException(proxyNode, startNs, e);
             logFailedProxyPutIfNeeded(e);
         } catch(ObsoleteVersionException ove) {
             /*
              * Proxy puts can get an OVE if somehow there are two stealers for
              * the same proxy node and the other stealer's proxy put already got
              * tothe proxy node.. This will not result from online put winning,
              * since we don't issue proxy puts if the proxy node is still a
              * replica
              */
             logFailedProxyPutIfNeeded(ove);
         } catch(Exception e) {
             // Just log the key.. Not sure having values in the log is a good
             // idea.
             logFailedProxyPutIfNeeded(e);
         }
     }",NotBuggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message indicates several changes, including a bug fix related to proxy put stats. The code diff shows the addition of `redirectingStore.reportProxyPutSuccess();` and changes to logging. The addition of `reportProxyPutSuccess` strongly suggests a fix related to the reporting or tracking of successful proxy puts, aligning with the ""Bug fix in proxy put stats"" part of the commit message. The logging change is minor.

**Buggy**
"
voldemort,7092.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,122 +1,150 @@
     public void rebalanceStateChange(Cluster cluster,
                                      List<RebalancePartitionsInfo> rebalancePartitionsInfo,
                                      boolean swapRO,
                                      boolean changeClusterMetadata,
                                      boolean changeRebalanceState,
                                      boolean rollback) {
         Cluster currentCluster = metadataStore.getCluster();
 
         logger.info(""Server doing rebalance state change with options [ cluster metadata change - ""
                     + changeClusterMetadata + "" ], [ changing rebalancing state - ""
                     + changeRebalanceState + "" ], [ changing swapping RO - "" + swapRO
                     + "" ], [ rollback - "" + rollback + "" ]"");
 
         // Variables to track what has completed
         List<RebalancePartitionsInfo> completedRebalancePartitionsInfo = Lists.newArrayList();
         List<String> swappedStoreNames = Lists.newArrayList();
         boolean completedClusterChange = false;
         boolean completedRebalanceSourceClusterChange = false;
         Cluster previousRebalancingSourceCluster = null;
 
         try {
-            // CHANGE CLUSTER METADATA
-            if(changeClusterMetadata) {
-                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
-                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
-                completedClusterChange = true;
-            }
 
-            // SWAP RO DATA FOR ALL STORES
-            if(swapRO) {
-                swapROStores(swappedStoreNames, false);
-            }
-
+            /*
+             * Do the rebalancing state changes. It is important that this
+             * happens before the actual cluster metadata is changed. Here's
+             * what could happen otherwise. When a batch completes with
+             * {current_cluster c2, rebalancing_source_cluster c1} and the next
+             * rebalancing state changes it to {current_cluster c3,
+             * rebalancing_source_cluster c2} is set for the next batch, then
+             * there could be a window during which the state is
+             * {current_cluster c3, rebalancing_source_cluster c1}. On the other
+             * hand, when we update the rebalancing source cluster first, there
+             * is a window where the state is {current_cluster c2,
+             * rebalancing_source_cluster c2}, which still fine, because of the
+             * following. Successful completion of a batch means the cluster is
+             * finalized, so its okay to stop proxying based on {current_cluster
+             * c2, rebalancing_source_cluster c1}. And since the cluster
+             * metadata has not yet been updated to c3, the writes will happen
+             * based on c2.
+             * 
+             * Even if some clients have already seen the {current_cluster c3,
+             * rebalancing_source_cluster c2} state from other servers, the
+             * operation will be rejected with InvalidMetadataException since
+             * this server itself is not aware of C3
+             */
             // CHANGE REBALANCING STATE
             if(changeRebalanceState) {
                 try {
                     previousRebalancingSourceCluster = metadataStore.getRebalancingSourceCluster();
                     if(!rollback) {
                         // Save up the current cluster for Redirecting store
+                        logger.info(""Setting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to "" + currentCluster);
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, currentCluster);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.addRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     } else {
                         // Reset the rebalancing source cluster back to null
+                        logger.info(""Resetting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to null"");
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, null);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.deleteRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     }
                 } catch(Exception e) {
                     throw new VoldemortException(e);
                 }
             }
+
+            // CHANGE CLUSTER METADATA
+            if(changeClusterMetadata) {
+                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
+                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
+                completedClusterChange = true;
+            }
+
+            // SWAP RO DATA FOR ALL STORES
+            if(swapRO) {
+                swapROStores(swappedStoreNames, false);
+            }
         } catch(VoldemortException e) {
 
             logger.error(""Got exception while changing state, now rolling back changes"", e);
 
             // ROLLBACK CLUSTER CHANGE
             if(completedClusterChange) {
                 try {
                     logger.info(""Rolling back cluster.xml to "" + currentCluster);
                     changeCluster(MetadataStore.CLUSTER_KEY, currentCluster);
                 } catch(Exception exception) {
                     logger.error(""Error while rolling back cluster metadata to "" + currentCluster,
                                  exception);
                 }
             }
 
             // SWAP RO DATA FOR ALL COMPLETED STORES
             if(swappedStoreNames.size() > 0) {
                 try {
                     swapROStores(swappedStoreNames, true);
                 } catch(Exception exception) {
                     logger.error(""Error while swapping back to old state "", exception);
                 }
             }
 
             // CHANGE BACK ALL REBALANCING STATES FOR COMPLETED ONES
             if(completedRebalancePartitionsInfo.size() > 0) {
                 if(!rollback) {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.deleteRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while deleting back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 } else {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.addRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while adding back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 }
 
             }
 
             // Revert changes to REBALANCING_SOURCE_CLUSTER_XML
             if(completedRebalanceSourceClusterChange) {
                 logger.info(""Reverting the REBALANCING_SOURCE_CLUSTER_XML back to ""
                             + previousRebalancingSourceCluster);
                 changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML,
                               previousRebalancingSourceCluster);
             }
 
             throw e;
         }
 
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message mentions ""Bug fix in proxy put stats"" and ""Changing order of state change updates for correctness"". The code diff reorders the state change updates, specifically moving the rebalancing state change logic before the cluster metadata change. The commit message explains the reasoning behind this change, highlighting a potential race condition if the cluster metadata is updated before the rebalancing state. This reordering aims to ensure data consistency during rebalancing.

Therefore, the changes indicate a bug fix related to the order of operations during rebalancing.

**Buggy**"
voldemort,9466.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,4 +1,3 @@
     protected void recordSuccess(Node node, long startNs) {
-        proxyPutStats.reportProxyPutCompletion();
         failureDetector.recordSuccess(node, (System.nanoTime() - startNs) / Time.NS_PER_MS);
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Breakdown:**

*   ""1. Enabling proxy puts by default"": This suggests a feature change or configuration update.
*   ""2. Bug fix in proxy put stats"": This explicitly states a bug fix related to proxy put statistics.
*   ""3. Changing order of state change updates for correctness"": This indicates a logical correction to ensure the correct order of operations.
*   ""4. Setting proxy put tests to do one batch rebalancing"": This points to a change in test configuration, likely to improve test reliability or coverage.

**Git Diff Analysis:**

The diff shows the removal of the line `proxyPutStats.reportProxyPutCompletion();` within the `recordSuccess` method. This method seems to be related to recording the success of a proxy put operation.

**Reasoning:**

The commit message explicitly mentions ""Bug fix in proxy put stats."" The diff shows the removal of a line that directly interacts with `proxyPutStats`. The removal of `proxyPutStats.reportProxyPutCompletion();` from the `recordSuccess` method suggests that this line was either incorrectly placed, causing incorrect statistics, or was causing some other issue related to the proxy put statistics. This aligns perfectly with the commit message's claim of a bug fix in proxy put stats.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix.

**Buggy**"
voldemort,8840.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getCheckedInResourcesCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.queue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.queue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case."" suggests two things:
    *   Code cleanup/refactoring in the `get stats methods` of the `(Queued)KeyedResourcePool` class.
    *   A bug fix in a test case.

2.  **Git Diff:**
    *   The diff focuses on the `getCheckedInResourcesCount(K key)` method.
    *   The original code had an `if` condition to check if `resourcePoolMap` contains the key, and if not, it would return 0. Then, it would try to get the resource pool and return the queue size.
    *   The modified code first checks if the `resourcePoolMap` contains the key. If it does, it attempts to retrieve the resource pool and return the queue size within a `try-catch` block. If an `IllegalArgumentException` occurs (likely due to an invalid key), it logs a debug message. If the key is not present in the map, it returns 0.

3.  **Reasoning:**
    *   The original code had a potential issue: if `resourcePoolMap` did not contain the key, it would return 0. However, it would still attempt to retrieve the resource pool, which could lead to an exception if the key was truly invalid.
    *   The modified code improves error handling by wrapping the resource pool retrieval in a `try-catch` block. This allows the code to gracefully handle invalid keys and log a debug message.
    *   The change also simplifies the logic by returning 0 directly if the key is not present, avoiding the unnecessary `try-catch` block in that case.
    *   The commit message mentions cleaning up the `get stats methods` and fixing an error in a test case. The code changes align with the cleanup aspect. The `IllegalArgumentException` handling suggests a potential bug where an invalid key could cause issues.

**Conclusion:**

The code changes improve error handling and simplify the logic of the `getCheckedInResourcesCount` method. The commit message explicitly mentions a bug fix in a test case, and the code changes address a potential issue with invalid keys. Therefore, the changes indicate a bug fix.

**Buggy**"
voldemort,8883.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,14 +1,12 @@
     public int getRegisteredResourceRequestCount(K key) {
-        int rc = 0;
-        if(!requestQueueMap.containsKey(key)) {
-            return rc;
+        if(requestQueueMap.containsKey(key)) {
+            try {
+                Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
+                // FYI: .size() is not constant time in the next call. ;)
+                return requestQueue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
-            // FYI: .size() is not constant time in the next call. ;)
-            rc = requestQueue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case."" suggests two primary changes:

1.  **Cleanup of `get stats` methods:** This implies refactoring or improvement of the code related to retrieving statistics, potentially addressing inefficiencies or inconsistencies.
2.  **Fix error in test case:** This explicitly states that a test case was incorrect and has been fixed. This is a strong indicator of a bug fix.

**Git Diff Analysis:**

The diff focuses on the `getRegisteredResourceRequestCount(K key)` method. Let's break down the changes:

*   **Original Code:**
    *   Initializes `rc` to 0.
    *   Checks if `requestQueueMap` contains the key. If not, returns `rc` (which is 0).
    *   If the key exists, it tries to get the request queue using `getRequestQueueForExistingKey(key)`.
    *   If `getRequestQueueForExistingKey` throws an `IllegalArgumentException`, it logs a debug message.
    *   Finally, returns `rc` (which is the size of the queue if no exception occurred, or 0 if an exception occurred or the key didn't exist).

*   **Modified Code:**
    *   Directly checks if `requestQueueMap` contains the key.
    *   If the key exists, it tries to get the request queue using `getRequestQueueForExistingKey(key)`.
    *   If `getRequestQueueForExistingKey` throws an `IllegalArgumentException`, it logs a debug message.
    *   Returns 0 if the key doesn't exist or an exception occurs.

**Reasoning:**

1.  **Error Handling:** The code now returns 0 directly if the key is not present in the map. Previously, it initialized `rc` to 0 and then returned it. This is a minor cleanup.
2.  **Exception Handling:** The exception handling remains the same. If `getRequestQueueForExistingKey` throws an `IllegalArgumentException`, it logs a debug message and returns 0.
3.  **Potential Bug Fix:** The original code had a redundant initialization of `rc` to 0. The modified code removes this redundancy, which can be considered a cleanup. More importantly, the commit message explicitly mentions fixing an error in a test case. While the diff doesn't directly show the test case change, the cleanup in the `getRegisteredResourceRequestCount` method might be related to making the test case pass. The `IllegalArgumentException` handling suggests that invalid keys might have been causing issues, and the test case fix might involve providing valid keys or handling the exception appropriately.

**Conclusion:**

Based on the commit message and the code changes, the changes indicate a bug fix. The commit message explicitly states that an error in a test case was fixed, and the code cleanup might be related to resolving the issue that caused the test to fail.

**Buggy**
"
voldemort,8838.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getTotalResourceCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.size.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getTotalResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.size.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getTotalResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states ""Fix error in test case"" and ""Clean up get stats methods"". This strongly suggests that the commit addresses a bug or an incorrect behavior, and also includes some code cleanup.

2.  **Code Diff Analysis:**
    *   The `getTotalResourceCount` method is modified. The original code initialized `rc` to 0 and returned it immediately if the `resourcePoolMap` did not contain the key. The modified code checks for the key's existence and directly returns 0 if it's not present. The try-catch block remains, handling `IllegalArgumentException`.
    *   The change improves the code's readability and potentially its efficiency by avoiding unnecessary operations when the key is not present. The original code was not necessarily incorrect, but the new version is more concise.
    *   The try-catch block suggests that an invalid key could be passed to the `getResourcePoolForExistingKey` method, leading to an `IllegalArgumentException`. The logging of this exception indicates a defensive programming approach, which is often associated with bug fixes or preventing potential issues.

3.  **Alignment and Bug Fix Indication:**
    *   The code changes align with the ""Clean up get stats methods"" part of the commit message.
    *   The ""Fix error in test case"" part of the commit message, combined with the defensive programming aspect of the try-catch block and the overall code cleanup, strongly suggests that the changes are related to fixing a bug or preventing a potential issue that was exposed by the test case. The original code might have been returning incorrect values under certain circumstances, or the test case might have been revealing a potential for `IllegalArgumentException` to be thrown.

**Conclusion:**

**Buggy**
"
voldemort,8842.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getBlockingGetsCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.blockingGets.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.blockingGets.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

The commit message ""Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case."" suggests two things:

*   **Cleanup:** Refactoring or improving the structure of the `get stats` methods.
*   **Bug Fix:** An error was present in a test case and has been resolved.

The presence of ""Fix error in test case"" strongly suggests a bug fix.

**2. Code Diff Analysis:**

The code diff modifies the `getBlockingGetsCount(K key)` method. Let's break down the changes:

*   **Original Code:** The original code first checks if the `resourcePoolMap` contains the given `key`. If not, it returns 0. If the key exists, it retrieves the resource pool and attempts to get the `blockingGets` count. It includes a try-catch block to handle `IllegalArgumentException`.
*   **Modified Code:** The modified code inverts the initial check. It checks if the `resourcePoolMap` contains the given `key`. If it does, it proceeds to retrieve the resource pool and get the `blockingGets` count within a try-catch block. If the key doesn't exist, it directly returns 0.

**3. Reasoning:**

The code change improves the readability and efficiency of the method. The original code had an unnecessary initialization of `rc` to 0 and then returned it if the key was not found. The modified code directly returns 0 if the key is not found, which is cleaner. The try-catch block remains, suggesting that the `IllegalArgumentException` is still a potential issue when retrieving the resource pool for a given key.

The commit message mentions cleaning up the `get stats` methods. This diff aligns with that part of the message. The commit message also mentions fixing an error in a test case. While we don't see the test case code, the changes in `getBlockingGetsCount` could be related to making the method behave as expected in the test case, thus fixing a bug.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix related to the `getBlockingGetsCount` method and potentially a test case.

**Buggy**"
voldemort,532.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,14 +1,18 @@
     public RebalancePlan getPlan(Cluster finalCluster,
                                  List<StoreDefinition> finalStoreDefs,
                                  int batchSize) {
         RebalanceUtils.validateClusterStores(finalCluster, finalStoreDefs);
         RebalanceUtils.validateCurrentFinalCluster(currentCluster, finalCluster);
 
+        // TODO: (currentCluster vs interimCluster) Add more validation before
+        // constructing plan? Given that currentCluster was polled from prod
+        // cluster, should confirm that it is an ""interim cluster"" i.e., has
+        // same (superset?) of nodes as are in finalCluster.
         String outputDir = null;
         return new RebalancePlan(currentCluster,
                                  currentStoreDefs,
                                  finalCluster,
                                  finalStoreDefs,
                                  batchSize,
                                  outputDir);
     }",NotBuggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message Breakdown:**
    *   ""Fix some bugs I introduced"": This clearly indicates the commit aims to correct previously introduced errors.
    *   ""Fixed overflow introduced in AdminClient.waitForCompletion"": This explicitly states a fix for an overflow issue, a type of bug.
    *   ""Verify cluster store definition in StoreRoutingPlan"": This suggests a validation improvement, potentially addressing incorrect store routing, which can be considered a bug fix.
    *   ""Added TODOs"": This indicates areas needing further attention but doesn't directly relate to bug fixes in this specific commit.
    *   ""Minor tweaks to tests based on above changes"": Test tweaks often accompany bug fixes to ensure the fix works as expected and prevent regressions.

2.  **Git Diff Analysis:**
    *   The diff adds a `TODO` comment related to `currentCluster` vs `interimCluster`. This aligns with the commit message's mention of `TODOs` about these clusters.
    *   The `TODO` suggests a need for validation, indicating a potential area where the system might be behaving incorrectly if the clusters are not properly validated.

3.  **Relevance and Bug Fix Indicators:**
    *   The commit message explicitly mentions fixing an overflow bug.
    *   The validation of the cluster store definition also points towards a potential bug fix, as incorrect store definitions can lead to data corruption or routing failures.
    *   The test tweaks further support the idea that this commit includes bug fixes.
    *   The `TODO` added in the diff, while not a direct fix, highlights an area needing further attention to prevent potential issues, further supporting the idea that the commit addresses existing or potential bugs.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate bug fixes. The commit message explicitly states the intention to fix bugs, and the changes include fixing an overflow issue and adding validation for cluster store definitions. Therefore, the answer is:

**Buggy**
"
voldemort,887.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,3 +1,3 @@
         public String waitForCompletion(int nodeId, int requestId) {
-            return waitForCompletion(nodeId, requestId, Long.MAX_VALUE, TimeUnit.SECONDS, null);
+            return waitForCompletion(nodeId, requestId, 0, TimeUnit.SECONDS, null);
         }",Buggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",Buggy,"The commit message indicates bug fixes and the addition of TODOs. The diff shows a change in the `waitForCompletion` method, specifically modifying the timeout duration from `Long.MAX_VALUE` to `0` seconds. This change directly addresses an overflow issue mentioned in the commit message (""Fixed overflow introduced in AdminClient.waitForCompletion""). Since the code change is directly related to fixing an overflow, it indicates a bug fix.

**Buggy**"
voldemort,54.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,39 +1,42 @@
     public void reduce(BytesWritable key,
                        Iterator<BytesWritable> values,
                        OutputCollector<Text, Text> output,
                        Reporter reporter) throws IOException {
         BytesWritable writable = values.next();
         byte[] valueBytes = writable.get();
 
         if(this.nodeId == -1)
             this.nodeId = ByteUtils.readInt(valueBytes, 0);
         if(this.chunkId == -1)
             this.chunkId = ReadOnlyUtils.chunk(key.get(), this.numChunks);
 
         // Write key and position
         this.indexFileStream.write(key.get(), 0, key.getSize());
-        this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
         this.indexFileStream.writeInt(this.position);
-        this.checkSumDigestIndex.update(this.position);
+        if(this.checkSumDigestIndex != null) {
+            this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
+            this.checkSumDigestIndex.update(this.position);
+        }
 
         // Write length and value
         int valueLength = writable.getSize() - 4;
         this.valueFileStream.writeInt(valueLength);
-        this.checkSumDigestValue.update(valueLength);
         this.valueFileStream.write(valueBytes, 4, valueLength);
-        this.checkSumDigestValue.update(valueBytes, 4, valueLength);
-
+        if(this.checkSumDigestValue != null) {
+            this.checkSumDigestValue.update(valueLength);
+            this.checkSumDigestValue.update(valueBytes, 4, valueLength);
+        }
         this.position += 4 + valueLength;
         if(this.position < 0)
             throw new VoldemortException(""Chunk overflow exception: chunk "" + chunkId
                                          + "" has exceeded "" + Integer.MAX_VALUE + "" bytes."");
 
         // if we have multiple values for this md5 that is a collision, throw an
         // exception--either the data itself has duplicates, there are trillions
         // of keys, or someone is attempting something malicious
         if(values.hasNext())
             throw new VoldemortException(""Duplicate keys detected for md5 sum ""
                                          + ByteUtils.toHexString(ByteUtils.copy(key.get(),
                                                                                 0,
                                                                                 key.getSize())));
     }",Buggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when the Checksum is None. The code diff introduces null checks for `this.checkSumDigestIndex` and `this.checkSumDigestValue` before calling their `update` methods. This prevents the NPE from occurring when these checksum objects are not initialized (i.e., when Checksum is None). The changes directly address the issue described in the commit message.

Reasoning:

1.  **Commit Message:** The commit message explicitly states ""Fixed minor bug which returns NPE when Checksum is None."" This clearly indicates a bug fix related to a potential NullPointerException.
2.  **Code Diff:** The code diff adds `if(this.checkSumDigestIndex != null)` and `if(this.checkSumDigestValue != null)` checks before calling `this.checkSumDigestIndex.update(...)` and `this.checkSumDigestValue.update(...)` respectively. This suggests that `checkSumDigestIndex` and `checkSumDigestValue` can be null in some cases, and the code was previously attempting to dereference them without checking for null, leading to an NPE.
3.  **Alignment:** The code changes directly align with the commit message. The null checks prevent the NPE that occurs when the checksum is not configured (None).

Conclusion: **Buggy**
"
voldemort,10530.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,10 +1,12 @@
     public static String toString(CheckSumType type) {
         if(type == CheckSumType.ADLER32) {
             return ""adler32"";
         } else if(type == CheckSumType.CRC32) {
             return ""crc32"";
         } else if(type == CheckSumType.MD5) {
             return ""md5"";
+        } else if(type == CheckSumType.NONE) {
+            return ""none"";
         }
-        return null;
+        return ""none"";
     }",NotBuggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed minor bug which returns NPE when Checksum is None"" clearly states that the commit addresses a bug. Specifically, it mentions a `NullPointerException` (NPE) occurring when the `Checksum` type is `NONE`. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `toString(CheckSumType type)` method.  Let's break it down:

*   **Original Code:**

    ```java
    public static String toString(CheckSumType type) {
        if(type == CheckSumType.ADLER32) {
            return ""adler32"";
        } else if(type == CheckSumType.CRC32) {
            return ""crc32"";
        } else if(type == CheckSumType.MD5) {
            return ""md5"";
        }
        return null;
    }
    ```

*   **Modified Code:**

    ```java
    public static String toString(CheckSumType type) {
        if(type == CheckSumType.ADLER32) {
            return ""adler32"";
        } else if(type == CheckSumType.CRC32) {
            return ""crc32"";
        } else if(type == CheckSumType.MD5) {
            return ""md5"";
        } else if(type == CheckSumType.NONE) {
            return ""none"";
        }
        return ""none"";
    }
    ```

The changes introduce a specific check for `CheckSumType.NONE`.  If the type is `NONE`, the method now returns the string ""none"".  Critically, the original code returned `null` if none of the `if` conditions were met. This `null` return is very likely the cause of the `NullPointerException` mentioned in the commit message. The modified code also changes the default return value from `null` to `""none""`.

**Relevance and Bug Fix Confirmation:**

The code change directly addresses the issue described in the commit message. The original code's potential to return `null` when the `CheckSumType` was not one of the explicitly handled types (ADLER32, CRC32, MD5) would indeed lead to an NPE if the calling code didn't anticipate a `null` return. The added `if` condition and the change of the default return value to `""none""` prevent this.

**Conclusion:**

The commit message and the code diff are highly correlated. The changes clearly fix a bug that could cause a `NullPointerException`.

**Buggy**
"
weka,27677.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,27 +1,26 @@
   private double instanceTransformationProbability(Instance first, 
 						   Instance second) {
     String debug = ""(KStar.instanceTransformationProbability) "";
     double transProb = 1.0;
     int numMissAttr = 0;
     for (int i = 0; i < m_NumAttributes; i++) {
       if (i == m_Train.classIndex()) {
 	continue; // ignore class attribute
       }
       if (first.isMissing(i)) { // test instance attribute value is missing
 	numMissAttr++;
 	continue;
       }
       transProb *= attrTransProb(first, second, i);
       // normilize for missing values
       if (numMissAttr != m_NumAttributes) {
-	// I don't know where this comes from!!!
 	transProb = Math.pow(transProb, (double)m_NumAttributes / 
 			     (m_NumAttributes - numMissAttr));
       }
       else { // weird case!
 	transProb = 0.0;
       }
     }
     // normilize for the train dataset
      return transProb / m_NumInstances;
   }",NotBuggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a fix related to the initialization of the `-E` option during incremental training. However, the provided code diff doesn't seem to be directly related to option initialization or incremental training setup. The code calculates the transformation probability between two instances, normalizing for missing attribute values. There's no clear indication of a bug fix related to the `-E` option or initialization within this code snippet. The removal of the comment `// I don't know where this comes from!!!` doesn't indicate a bug fix, but rather code cleanup.

Therefore, based on the provided information, it's difficult to confirm that the code changes align with the commit message's stated bug fix.

**NotBuggy**
"
weka,27694.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,3 +1,4 @@
   private void update_m_Attributes() {
     m_NumInstances = m_Train.numInstances();
+    m_InitFlag = ON;
   }",Buggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the initialization of the `-E` option during incremental training. The provided code diff shows a modification within the `update_m_Attributes()` method, where `m_InitFlag` is set to `ON`. This suggests that the initialization process was incomplete or incorrect before, and this change aims to rectify it. The modification directly relates to initialization, aligning with the commit message's description of fixing an initialization bug.

**Buggy**"
weka,31023.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,13 +1,14 @@
   public void setSource(InputStream input) throws IOException {
     BufferedReader	reader;
     String		line;
     
     m_structure    = null;
     m_sourceFile   = null;
     m_File         = null;
 
     m_StreamBuffer = new StringBuffer();
     reader         = new BufferedReader(new InputStreamReader(input));
-    while ((line = reader.readLine()) != null)
+    while ((line = reader.readLine()) != null) {
       m_StreamBuffer.append(line + ""\n"");
+    }
   }",NotBuggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates two main changes: 1) moving code related to gzip compression handling from `ArffLoader` to `AbstractFileLoader` (not shown in the diff), and 2) fixing a bug in `CSVLoader` that caused the source file to be loaded twice into the `StringBuffer`.

The provided diff shows a modification to the `setSource` method. The change involves adding curly braces to the `while` loop:

```diff
-    while ((line = reader.readLine()) != null)
+    while ((line = reader.readLine()) != null) {
       m_StreamBuffer.append(line + ""\n"");
+    }
```

While this change itself doesn't scream ""bug fix"", the commit message explicitly mentions a bug fix in `CSVLoader` related to loading the source file twice.  The provided diff is *likely* related to that fix. Without the curly braces, if `m_StreamBuffer.append(line + ""\n"");` threw an exception, the loop might not terminate correctly, potentially leading to the file being read multiple times. Adding the curly braces ensures that the `while` loop condition is checked after each iteration, even if an exception occurs within the loop body. This would prevent the double-loading issue described in the commit message.

Therefore, the diff, in conjunction with the commit message, suggests a bug fix.

**Buggy**
"
weka,30923.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,18 +1,22 @@
   public void setSource(File file) throws IOException {
     m_structure = null;
+    
     setRetrieval(NONE);
 
-    if (file == null) {
+    if (file == null)
       throw new IOException(""Source file object is null!"");
-    }
 
     try {
-      setSource(new FileInputStream(file));
+      if (file.getName().endsWith(getFileExtension() + FILE_EXTENSION_COMPRESSED)) {
+	setSource(new GZIPInputStream(new FileInputStream(file)));
+      } else {
+	setSource(new FileInputStream(file));
+      }
     }
     catch (FileNotFoundException ex) {
       throw new IOException(""File not found"");
     }
-
+    
     m_sourceFile = file;
     m_File       = file.getAbsolutePath();
-  }

+    }",Buggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates two main changes: code movement for gzip compression handling and a bug fix in CSVLoader.

The provided diff focuses on the `setSource(File file)` method. It adds gzip compression handling based on the file extension. This aligns with the commit message's description of enabling other loaders to handle gzip compression. The original code directly created a FileInputStream. The modified code checks if the filename ends with "".gz"" (getFileExtension() + FILE_EXTENSION_COMPRESSED) and if so, wraps the FileInputStream in a GZIPInputStream. This change allows the loader to handle compressed files, which was likely a missing feature or a bug.

The changes in the `setSource` method are related to adding functionality and fixing a potential issue where compressed files couldn't be loaded. While not directly fixing a bug in the traditional sense, it addresses a missing feature that could be considered a bug. The commit message explicitly mentions a bug fix in CSVLoader, which isn't reflected in this diff. However, the gzip handling can be considered a fix for a missing feature.

Therefore, the changes indicate a bug fix or a missing feature being addressed.

**Buggy**"
weka,11692.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,53 +1,52 @@
 	public void drawGraph() {
 		// build the panel
 		try {
 		  remove(canvas3D);
-		  System.out.println(""remove ok"");	
 		
 		} catch (Exception e) {
 		}
 //		if (!m_canvasCreated) {
 		  canvas3D = new Canvas3D(SimpleUniverse.getPreferredConfiguration());
 //		}
 
 		int scrHeight = (new Double(this.getSize().getHeight())).intValue();
 		int scrWidth = (new Double(this.getSize().getWidth())).intValue();
 		
 		canvas3D.setSize(scrWidth - 120, scrHeight - 50);
 		
 //		if (!m_canvasCreated) {
 		add(canvas3D, java.awt.BorderLayout.CENTER);
 		m_canvasCreated = true;
 //		}
 		
 		freeResources();				
 		
 		// build the visualisation
                 m_scene = createSceneGraph();
 
                 // compile the scene
                 m_scene.compile();
 
 	        // build the universe
 		m_simpleU = new SimpleUniverse(canvas3D);
 
 
 		// add the behaviors to the ViewingPlatform
 		ViewingPlatform viewingPlatform = m_simpleU.getViewingPlatform();
 
 		viewingPlatform.setNominalViewingTransform();
 
 		// add orbit behavior to ViewingPlatform
 		orbit =
 			new OrbitBehavior(
 				canvas3D,
 				OrbitBehavior.REVERSE_ALL | OrbitBehavior.STOP_ZOOM);
 		BoundingSphere bounds =
 			new BoundingSphere(new Point3d(0.0, 0.0, 0.0), 100.0);
 		orbit.setSchedulingBounds(bounds);
 		viewingPlatform.setViewPlatformBehavior(orbit);
 		
 
 		m_simpleU.addBranchGraph(m_scene);
 
 	}",NotBuggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a refresh bug when switching between tabs, specifically from the 3D viewer back to the rules selection tab. The diff shows a change in the `drawGraph` method, where the `canvas3D` component is removed and re-added. The try-catch block surrounding the `remove` call suggests an attempt to handle a potential exception during the removal process, which could be related to the refresh bug. The comment ""remove ok"" was also removed. This suggests that the original code might have been failing to properly remove the canvas, leading to issues when switching tabs and redrawing the 3D view. The changes made are directly related to fixing the refresh bug described in the commit message.

**Buggy**"
weka,11730.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,23 +1,37 @@
 	private void panelChanged() {
 		int numTab = viewerTabbedPanel.getSelectedIndex();
 		visu1Rules = selectionPanel.getSelectedRules(1);
 /*		visu2Rules = selectionPanel.getSelectedRules(2);
 		visu3Rules = selectionPanel.getSelectedRules(3); */
 
 		switch (numTab) {
+		case 0:
+			visu1PanelHolder.removeAll();
+			visu1PanelHolder.revalidate();
+			visu1PanelHolder.repaint();
+			break;
 			case 1 :
+				visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
+				revalidate();
+				repaint();
 				String[] selectedCriteria =
 					selectionPanel.getSelectedCriteria();
 				visu1Panel3D.setData(visu1Rules, selectedCriteria);
 //				visu1Panel.setSelectedRules(visu2Rules);
 				break;
 			case 2 :
 				visu2Panel.setData(visu2Rules, criteres);
 				visu2Panel.setSelectedRules(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 			case 3 :
 				visu3Panel.setData(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 		}
 
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a refresh bug when switching between tabs in the viewer, specifically between the rules selection tab and the 3D viewer tab. The code diff shows modifications within the `panelChanged()` method, which seems to be responsible for updating the view based on the selected tab.

Here's a breakdown of the changes and their relevance to the bug fix:

*   **Case 0 (Rules Selection Tab):** The code now includes `visu1PanelHolder.removeAll()`, `revalidate()`, and `repaint()` when the first tab (likely the rules selection tab) is selected. This suggests that the previous implementation might not have properly cleared the contents of the `visu1PanelHolder` when switching back to this tab, leading to visual artifacts or incorrect display.
*   **Case 1 (3D Viewer Tab):** The code adds the 3D panel and calls `revalidate()` and `repaint()`.
*   **Cases 2 and 3:** Similar to Case 0, `visu1PanelHolder.removeAll()`, `revalidate()`, and `repaint()` are added when tabs 2 and 3 are selected. This indicates that the contents of `visu1PanelHolder` might have been incorrectly persisting when switching away from the 3D viewer tab, causing the refresh issue.

The addition of `removeAll()`, `revalidate()`, and `repaint()` calls strongly suggests an attempt to fix a visual refresh or rendering problem. The commit message explicitly mentions a refresh bug, and these code changes directly address that concern by ensuring the panel is cleared and redrawn when switching tabs.

Therefore, the changes in the code are directly related to fixing the bug described in the commit message.

**Buggy**
"
weka,11722.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,114 +1,115 @@
 	private void initComponents() {
 	    setLayout(new java.awt.BorderLayout());
 		viewerTabbedPanel = new javax.swing.JTabbedPane();
 		selectionPanel = new selection.SelectionPanel();
 		visu1Panel3D = new Panel3D();
+		visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
 		/*visu1Panel = new RulesSelectionPanel(visu1Panel3D);
 		visu1Panel.addActionListener(this); */
 /*		visu2PanelLine = new PanelLine();
 		visu2Panel = new RulesSelectionPanel(visu2PanelLine);
 		visu2Panel.setSingleSelection();
 		visu2Panel.addActionListener(this);
 		visu2Panel.setColored();
 		visu3Panel = new visu3.PanelDDecker(); */
 		// viewerBar = new javax.swing.JMenuBar();
 /*		fileMenu = new javax.swing.JMenu();
 		openItem = new javax.swing.JMenuItem();
 		printItem = new javax.swing.JMenuItem();
 		saveItem = new javax.swing.JMenuItem();
 		quitItem = new javax.swing.JMenuItem();
 		helpMenu = new javax.swing.JMenu();
 		aboutItem = new javax.swing.JMenuItem();
 		contentsItem = new javax.swing.JMenuItem(); */
 
 		selectionPanel.addMultipleListSelectionListener(this);
 
 //		setTitle(""Association Rules Viewer"");
 /*		addWindowListener(new java.awt.event.WindowAdapter() {
 			public void windowClosing(java.awt.event.WindowEvent evt) {
 				exitForm(evt);
 			}
 		}); */
 
 		viewerTabbedPanel.addChangeListener(new ChangeListener() {
 			public void stateChanged(ChangeEvent e) {
 				panelChanged();
 			}
 
 		});
 
 		viewerTabbedPanel.addTab(""Selection"", selectionPanel);
-		viewerTabbedPanel.addTab(""3D Representation"", visu1Panel3D);
+		viewerTabbedPanel.addTab(""3D Representation"", visu1PanelHolder);
 		/*viewerTabbedPanel.addTab(""N Dimensional Line"", visu2Panel);
 		viewerTabbedPanel.addTab(""Double Decker Plot"", visu3Panel); */
 
 		viewerTabbedPanel.setEnabledAt(1, false);
 		// viewerTabbedPanel.setEnabledAt(2, false);
 //		viewerTabbedPanel.setEnabledAt(3, false);
 
 		add(viewerTabbedPanel, java.awt.BorderLayout.CENTER);
 
 /*		fileMenu.setText(""File"");
 		openItem.setText(""Open"");
 		openItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				openPerformed(evt);
 			}
 		});
 
 		fileMenu.add(openItem);
 
 		printItem.setText(""Print"");
 		printItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				printPerformed(evt);
 			}
 		});
 
 		fileMenu.add(printItem);
 
 		saveItem.setText(""Save as"");
 		saveItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				savePerformed(evt);
 			}
 		});
 
 		fileMenu.add(saveItem);
 
 		quitItem.setText(""Quit"");
 		quitItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				quitPerformed(evt);
 			}
 		});
 
 		fileMenu.add(quitItem);
 
 		viewerBar.add(fileMenu);
 
 		helpMenu.setText(""Help"");
 		aboutItem.setText(""About"");
 		aboutItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showAbout(evt);
 			}
 		});
 
 		helpMenu.add(aboutItem);
 
 		contentsItem.setText(""Contents"");
 		contentsItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showHelp(evt);
 			}
 		});
 
 		helpMenu.add(contentsItem);
 
 		viewerBar.add(helpMenu);
 
 		setJMenuBar(viewerBar);
 
 		pack(); */
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a refresh bug that prevented switching back to the rules selection tab from the 3D viewer tab. The code diff shows that `visu1Panel3D` is now added to `visu1PanelHolder` which is then added to the tabbed panel. This suggests that the original implementation might have had issues with how the 3D panel was being managed within the tabbed panel, potentially causing the refresh or switching problem. The change replaces adding `visu1Panel3D` directly to the tabbed panel with adding `visu1PanelHolder` which contains `visu1Panel3D`. This could be a fix for layout or refresh issues.

**Buggy**
"
weka,28529.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,11 +1,12 @@
     public static void main(String[] args) {
         try {
+	    System.err.println(""okidoki"");
             BIFReader br = new BIFReader();
             br.processFile(args[0]);
 	    System.out.println(br.toString());
         
         }
         catch (Throwable t) {
             t.printStackTrace();
         }
     } // main",NotBuggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix spotted by Gladys Castillo Jordan. The diff shows the addition of `System.err.println(""okidoki"");` within the `try` block of the `main` method. While the commit message suggests a bug fix, the added line appears to be a debugging statement. Debugging statements are often added during the process of identifying and fixing bugs. Therefore, the presence of this debugging statement, coupled with the commit message, suggests that the commit is related to a bug fix.

**Buggy**"
weka,28685.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,10 +1,10 @@
     public Enumeration listOptions() {
         Vector newVector = new Vector(4);
 
-        newVector.addElement(new Option(""\tUse ADTree data structure\n"", ""D"", 0, ""-D""));
+        newVector.addElement(new Option(""\tDo not use ADTree data structure\n"", ""D"", 0, ""-D""));
         newVector.addElement(new Option(""\tBIF file to compare with\n"", ""B"", 1, ""-B <BIF file>""));
         newVector.addElement(new Option(""\tSearch algorithm\n"", ""Q"", 1, ""-Q weka.classifiers.bayes.net.search.SearchAlgorithm""));
         newVector.addElement(new Option(""\tEstimator algorithm\n"", ""E"", 1, ""-E weka.classifiers.bayes.net.estimate.SimpleEstimator""));
 
         return newVector.elements();
     } // listOptions",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Bug fix spotted by Gladys Castillo Jordan"" indicates a bug fix.
The code diff changes the description of the ""-D"" option from ""Use ADTree data structure"" to ""Do not use ADTree data structure"". This change corrects the description to accurately reflect the option's function, indicating a bug fix in the option description.

**Buggy**"
weka,28381.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,40 +1,41 @@
 	public double kFoldCV(BayesNet bayesNet, int nNrOfFolds) throws Exception {
 		m_BayesNet = bayesNet;
 		double fAccuracy = 0.0;
 		double fWeight = 0.0;
 		Instances instances = bayesNet.m_Instances;
 		// estimate CPTs based on complete data set
 		bayesNet.estimateCPTs();
 		int nFoldStart = 0;
 		int nFoldEnd = instances.numInstances() / nNrOfFolds;
 		int iFold = 1;
 		while (nFoldStart < instances.numInstances()) {
 			// remove influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 			
 			// measure accuracy on fold iFold
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				fAccuracy += accuracyIncrease(instance);
+				instance.setWeight(-instance.weight());
 				fWeight += instance.weight();
 			}
 
 			// restore influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 
 			// go to next fold
 			nFoldStart = nFoldEnd;
 			iFold++;
 			nFoldEnd = iFold * instances.numInstances() / nNrOfFolds;
 		}
 		return fAccuracy / fWeight;
 	} // kFoldCV",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix identified by Gladys Castillo Jordan. The code diff shows a modification within the `kFoldCV` method of the `BayesNet` class. Specifically, the line `instance.setWeight(-instance.weight());` has been added within the loop that measures accuracy on fold `iFold`. This line appears to correct an issue where the weight of the instance was not being properly reset after being temporarily negated. The added line ensures the instance's weight is restored to its original value before being used in the accuracy calculation. This suggests a bug where the accuracy calculation was being affected by incorrect instance weights, leading to inaccurate results. Thus, the changes indicate a bug fix.

**Buggy**"
weka,25948.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,61 +1,57 @@
   private double findSplitNominalNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal;
     double[][] counts = new double[theInstances.attribute(index).numValues() 
 				  + 1][theInstances.numClasses()];
     double[] sumCounts = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
     int numMissing = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (inst.isMissing(index)) {
 	numMissing++;
 	counts[theInstances.attribute(index).numValues()]
 	  [(int)inst.classValue()] += inst.weight();
       } else {
 	counts[(int)inst.value(index)][(int)inst.classValue()] += inst
 	  .weight();
       }
     }
 
     // Compute sum of counts
-
     for (int i = 0; i < theInstances.attribute(index).numValues() + 1; i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	sumCounts[j] += counts[i][j];
       }
     }
     
     // Make split counts for each possible split and evaluate
-
     System.arraycopy(counts[theInstances.attribute(index).numValues()], 0,
 		     m_Distribution[2], 0, theInstances.numClasses());
     for (int i = 0; i < theInstances.attribute(index).numValues(); i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	m_Distribution[0][j] = counts[i][j];
 	m_Distribution[1][j] = sumCounts[j] - counts[i][j];
       }
       currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
       if (Utils.sm(currVal, bestVal)) {
 	bestVal = currVal;
 	m_SplitPoint = (double)i;
 	for (int j = 0; j < 3; j++) {
 	  System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			   theInstances.numClasses());
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sumCounts, 0, bestDist[2], 0, 
 		       theInstances.numClasses());
     }
    
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The provided diff doesn't contain any arithmetic operations like division. The code mainly deals with counting instances and calculating entropy. Therefore, it's unlikely that this diff directly addresses a division by zero bug.

**NotBuggy**"
weka,25951.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,54 +1,50 @@
   private double findSplitNumericNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sum = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][(int)inst.classValue()] += inst.weight();
       } else {
 	m_Distribution[2][(int)inst.classValue()] += inst.weight();
 	numMissing++;
       }
     }
     System.arraycopy(m_Distribution[1], 0, sum, 0, theInstances.numClasses());
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][(int)inst.classValue()] += inst.weight();
       m_Distribution[1][(int)inst.classValue()] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     theInstances.numClasses());
 	  }
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sum, 0, bestDist[2], 0, theInstances.numClasses());
     }
  
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The diff doesn't show any changes related to division. The commit message mentions ""Fixed division by zero bugs for numeric prediction,"" but the provided code snippet doesn't contain any division operations that could potentially lead to a division by zero error. The division operation `currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;` is dividing by 2.0, which is a constant and cannot be zero.

Therefore, the code diff doesn't align with the commit message.

**NotBuggy**"
weka,25940.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,63 +1,58 @@
   public void buildClassifier(Instances instances) throws Exception {
     
     double bestVal = Double.MAX_VALUE, currVal;
     double bestPoint = -Double.MAX_VALUE, sum;
     int bestAtt = -1, numClasses;
 
     if (instances.checkForStringAttributes()) {
       throw new Exception(""Can't handle string attributes!"");
     }
 
     double[][] bestDist = new double[3][instances.numClasses()];
 
     theInstances = new Instances(instances);
     theInstances.deleteWithMissingClass();
     if (theInstances.classAttribute().isNominal()) {
       numClasses = theInstances.numClasses();
     } else {
       numClasses = 1;
     }
 
     // For each attribute
-    
     for (int i = 0; i < theInstances.numAttributes(); i++) {
       if (i != theInstances.classIndex()) {
 	
 	// Reserve space for distribution.
-	
 	m_Distribution = new double[3][numClasses];
 
 	// Compute value of criterion for best split on attribute
-	
 	if (theInstances.attribute(i).isNominal()) {
 	  currVal = findSplitNominal(i);
 	} else {
 	  currVal = findSplitNumeric(i);
 	}
 	if (Utils.sm(currVal, bestVal)) {
 	  bestVal = currVal;
 	  bestAtt = i;
 	  bestPoint = m_SplitPoint;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     numClasses);
 	  }
 	}
       }
     }
     
     // Set attribute, split point and distribution.
-    
     m_AttIndex = bestAtt;
     m_SplitPoint = bestPoint;
     m_Distribution = bestDist;
     if (theInstances.classAttribute().isNominal()) {
       for (int i = 0; i < m_Distribution.length; i++) {
 	Utils.normalize(m_Distribution[i]);
       }
     }
     
     // Save memory
-    
     theInstances = new Instances(theInstances, 0);
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Fixed division by zero bugs for numeric prediction"" indicates a bug fix related to division by zero errors in the numeric prediction functionality.

The diff doesn't show any explicit changes that directly address division by zero. There are no conditional statements added to check for zero divisors, nor are there any modifications to the code that would prevent a division by zero error. The changes mainly involve refactoring and code cleanup, such as removing unnecessary comments and standardizing the code format.

Therefore, based on the provided diff, there is no evidence of a bug fix related to division by zero.

**NotBuggy**"
weka,25952.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,64 +1,60 @@
   private double findSplitNumericNumeric(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sumsSquares = new double[3], sumOfWeights = new double[3];
     double[][] bestDist = new double[3][1];
-    double meanNoMissing;
+    double totalSum = 0, totalSumOfWeights = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][0] += inst.classValue() * inst.weight();
 	sumsSquares[1] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[1] += inst.weight();
       } else {
 	m_Distribution[2][0] += inst.classValue() * inst.weight();
 	sumsSquares[2] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[2] += inst.weight();
 	numMissing++;
       }
+      totalSumOfWeights += inst.weight();
+      totalSum += inst.classValue() * inst.weight();
     }
-    meanNoMissing = m_Distribution[1][0] / sumOfWeights[1];
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][0] += inst.classValue() * inst.weight();
       sumsSquares[0] += inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[0] += inst.weight();
       m_Distribution[1][0] -= inst.classValue() * inst.weight();
       sumsSquares[1] -= inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[1] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = variance(m_Distribution, sumsSquares, sumOfWeights);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
-	    bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    if (!Utils.eq(sumOfWeights[j], 0)) {
+	      bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    } else {
+	      bestDist[j][0] = totalSum / totalSumOfWeights;
+	    }
 	  }
 	}
       }
     }
 
-    // No missing values in training data
-    
-    if (numMissing == 0) {
-      bestDist[2][0] = meanNoMissing;
-    }
-
     m_Distribution = bestDist;
     return bestVal;
   }",Buggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for division by zero errors in numeric prediction. The code diff modifies the `findSplitNumericNumeric` method. Specifically, it adds a check `!Utils.eq(sumOfWeights[j], 0)` before dividing `m_Distribution[j][0]` by `sumOfWeights[j]` to calculate `bestDist[j][0]`. If `sumOfWeights[j]` is zero, it assigns `totalSum / totalSumOfWeights` to `bestDist[j][0]`. This prevents division by zero when `sumOfWeights[j]` is zero. The original code would have resulted in a division by zero error in such cases. Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
weka,20096.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,38 +1,40 @@
   public void performRequest(String request) {
     if (request.compareTo(""Show chart"") == 0) {
       try {
-	// popup visualize panel
-	if (!m_framePoppedUp) {
-	  m_framePoppedUp = true;
+        // popup visualize panel
+        if (!m_framePoppedUp) {
+          m_framePoppedUp = true;
 
-	  final javax.swing.JFrame jf = 
-	    new javax.swing.JFrame(""Model Performance Chart"");
-	  jf.setSize(800,600);
-	  jf.getContentPane().setLayout(new BorderLayout());
-	  jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
-	  jf.addWindowListener(new java.awt.event.WindowAdapter() {
-	      public void windowClosing(java.awt.event.WindowEvent e) {
-		jf.dispose();
-		m_framePoppedUp = false;
-	      }
-	    });
-	  jf.setVisible(true);
-	  m_popupFrame = jf;
-	} else {
-	  m_popupFrame.toFront();
-	}
+          final javax.swing.JFrame jf = new javax.swing.JFrame(
+              ""Model Performance Chart"");
+          jf.setSize(800, 600);
+          jf.getContentPane().setLayout(new BorderLayout());
+          jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
+          jf.addWindowListener(new java.awt.event.WindowAdapter() {
+            @Override
+            public void windowClosing(java.awt.event.WindowEvent e) {
+              jf.dispose();
+              m_framePoppedUp = false;
+            }
+          });
+          jf.setVisible(true);
+          m_popupFrame = jf;
+        } else {
+          m_popupFrame.toFront();
+        }
       } catch (Exception ex) {
-	ex.printStackTrace();
-	m_framePoppedUp = false;
+        ex.printStackTrace();
+        m_framePoppedUp = false;
       }
     } else if (request.equals(""Clear all plots"")) {
-        m_visPanel.removeAllPlots();
-        m_visPanel.validate(); m_visPanel.repaint();
-        m_visPanel = null;
-        m_masterPlot = null;
-        m_offscreenPlotData = null;
+      m_visPanel.removeAllPlots();
+      m_visPanel.validate();
+      m_visPanel.repaint();
+      m_visPanel = null;
+      m_masterPlot = null;
+      m_offscreenPlotData = null;
     } else {
       throw new IllegalArgumentException(request
-					 + "" not supported (Model Performance Chart)"");
+          + "" not supported (Model Performance Chart)"");
     }
   }",NotBuggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to error plots in the visualization panel. Specifically, it mentions that the master plot was not being updated for subsequent incoming `VisualizableErrorEvents` after the initial master plot was set. The code diff doesn't show the actual fix, but it shows the surrounding code. The code diff does not contain any changes related to the master plot or `VisualizableErrorEvents`. Therefore, based on the commit message and the lack of relevant changes in the provided code diff, it's impossible to confirm the bug fix. However, the commit message explicitly states that a bug was fixed.

**Buggy**"
weka,20081.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,174 +1,179 @@
   public synchronized void acceptDataSet(VisualizableErrorEvent e) {
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     if (!GraphicsEnvironment.isHeadless()) {
       if (m_visPanel == null) {
         m_visPanel = new VisualizePanel();
       }
-      if (m_masterPlot == null) {
-        m_masterPlot = e.getDataSet();
-      }
+
+      m_masterPlot = e.getDataSet();
+
       try {
         m_visPanel.setMasterPlot(m_masterPlot);
       } catch (Exception ex) {
-        System.err.println(""Problem setting up visualization (ModelPerformanceChart)"");
+        System.err
+            .println(""Problem setting up visualization (ModelPerformanceChart)"");
         ex.printStackTrace();
       }
       m_visPanel.validate();
       m_visPanel.repaint();
     } else {
       m_headlessEvents = new ArrayList<EventObject>();
       m_headlessEvents.add(e);
     }
-    
+
     if (m_imageListeners.size() > 0 && !m_processingHeadlessEvents) {
       // configure the renderer (if necessary)
       setupOffscreenRenderer();
-     
-      m_offscreenPlotData = new ArrayList<Instances>();      
+
+      m_offscreenPlotData = new ArrayList<Instances>();
       Instances predictedI = e.getDataSet().getPlotInstances();
       if (predictedI.classAttribute().isNominal()) {
-        
+
         // split the classes out into individual series.
         // add a new attribute to hold point sizes - correctly
-        // classified instances get default point size (2); 
+        // classified instances get default point size (2);
         // misclassified instances get point size (5).
         // WekaOffscreenChartRenderer can take advantage of this
         // information - other plugin renderers may or may not
         // be able to use it
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
         newInsts.setClassIndex(predictedI.classIndex());
-        
+
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = 2; // default shape size
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
-        
+
         // predicted class attribute is always actualClassIndex - 1
         Instances[] classes = new Instances[newInsts.numClasses()];
         for (int i = 0; i < newInsts.numClasses(); i++) {
           classes[i] = new Instances(newInsts, 0);
           classes[i].setRelationName(newInsts.classAttribute().value(i));
         }
         Instances errors = new Instances(newInsts, 0);
         int actualClass = newInsts.classIndex();
         for (int i = 0; i < newInsts.numInstances(); i++) {
           Instance current = newInsts.instance(i);
-          classes[(int)current.classValue()].add((Instance)current.copy());
-          
+          classes[(int) current.classValue()].add((Instance) current.copy());
+
           if (current.value(actualClass) != current.value(actualClass - 1)) {
-            Instance toAdd = (Instance)current.copy();
-            
+            Instance toAdd = (Instance) current.copy();
+
             // larger shape for an error
             toAdd.setValue(toAdd.numAttributes() - 1, 5);
-            
+
             // swap predicted and actual class value so
             // that the color plotted for the error series
             // is that of the predicted class
             double actualClassV = toAdd.value(actualClass);
             double predictedClassV = toAdd.value(actualClass - 1);
             toAdd.setValue(actualClass, predictedClassV);
             toAdd.setValue(actualClass - 1, actualClassV);
-              
-            errors.add(toAdd);            
+
+            errors.add(toAdd);
           }
         }
-        
+
         errors.setRelationName(""Errors"");
         m_offscreenPlotData.add(errors);
-        
+
         for (int i = 0; i < classes.length; i++) {
           m_offscreenPlotData.add(classes[i]);
         }
-  
+
       } else {
         // numeric class - have to make a new set of instances
         // with the point sizes added as an additional attribute
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
 
         int[] shapeSizes = e.getDataSet().getShapeSize();
 
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = shapeSizes[i];
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
         newInsts.setRelationName(predictedI.classAttribute().name());
         m_offscreenPlotData.add(newInsts);
       }
-      
+
       List<String> options = new ArrayList<String>();
-      
+
       String additional = ""-color="" + predictedI.classAttribute().name()
-        + "",-hasErrors"";
+          + "",-hasErrors"";
       if (m_additionalOptions != null && m_additionalOptions.length() > 0) {
         additional += "","" + m_additionalOptions;
         try {
           additional = m_env.substitute(additional);
-        } catch (Exception ex) { }
-      }            
+        } catch (Exception ex) {
+        }
+      }
       String[] optionsParts = additional.split("","");
       for (String p : optionsParts) {
         options.add(p.trim());
       }
-      
-//      if (predictedI.classAttribute().isNumeric()) {
+
+      // if (predictedI.classAttribute().isNumeric()) {
       options.add(""-shapeSize=@@size@@"");
-//      }
-      
+      // }
+
       String xAxis = m_xAxis;
       try {
         xAxis = m_env.substitute(xAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String yAxis = m_yAxis;
       try {
         yAxis = m_env.substitute(yAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String width = m_width;
       String height = m_height;
       int defWidth = 500;
       int defHeight = 400;
       try {
         width = m_env.substitute(width);
         height = m_env.substitute(height);
-        
+
         defWidth = Integer.parseInt(width);
         defHeight = Integer.parseInt(height);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       try {
-        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth, defHeight, 
-            m_offscreenPlotData, xAxis, yAxis, options);
+        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth,
+            defHeight, m_offscreenPlotData, xAxis, yAxis, options);
 
         ImageEvent ie = new ImageEvent(this, osi);
         notifyImageListeners(ie);
       } catch (Exception e1) {
         e1.printStackTrace();
-      }      
+      }
     }
   }",Buggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code diff modifies the `acceptDataSet` method to ensure the `m_masterPlot` is updated with each incoming `VisualizableErrorEvent`. The original code only set `m_masterPlot` if it was initially null, which caused subsequent events to not update the plot unless manually cleared. The modified code now sets `m_masterPlot = e.getDataSet();` before calling `m_visPanel.setMasterPlot(m_masterPlot);`, ensuring the plot is updated with each new event. This aligns with the commit message, which states that the bug was related to the master plot not being updated for subsequent events.

**Buggy**"
weka,18379.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,44 +1,52 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
        // KOML?
        if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
           KOML.write(expFile.getAbsolutePath(), m_Exp);
        }
        else
        // XML?
        if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
           XMLExperiment xml = new XMLExperiment(); 
           xml.write(expFile, m_Exp);
        }
        // binary
        else {
           FileOutputStream fo = new FileOutputStream(expFile);
           ObjectOutputStream oo = new ObjectOutputStream(
                                   new BufferedOutputStream(fo));
           oo.writeObject(m_Exp);
           oo.close();
        }
       
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates ""bug fixes"" and the code changes involve modifications to the file saving logic in the `saveExperiment` method. Specifically, the code now checks the selected file filter in the file chooser to determine the appropriate file extension to add if the user doesn't provide one. The previous logic had a complex conditional statement to determine the file extension. The new logic is more straightforward and robust, handling different file types based on the selected filter. The try-catch block remains, indicating that error handling was already in place. The changes suggest a bug fix related to how file extensions were handled during the save operation.

**Buggy**"
weka,8611.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,31 +1,29 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
     
     double [] probs = new double[instance.numClasses()];
     int attIndex;
     
     for (int j = 0; j < instance.numClasses(); j++) {
       probs[j] = 1;
       Enumeration enumAtts = instance.enumerateAttributes();
       attIndex = 0;
       while (enumAtts.hasMoreElements()) {
 	Attribute attribute = (Attribute) enumAtts.nextElement();
 	if (!instance.isMissing(attribute)) {
 	  if (attribute.isNominal()) {
 	    probs[j] *= m_Counts[j][attIndex][(int)instance.value(attribute)];
 	  } else {
 	    probs[j] *= normalDens(instance.value(attribute),
 				   m_Means[j][attIndex],
 				   m_Devs[j][attIndex]);}
 	}
 	attIndex++;
       }
-      System.out.println(probs[j] + "" "");
       probs[j] *= m_Priors[j];
     }
-    System.out.println();
 
     // Normalize probabilities
     Utils.normalize(probs);
 
     return probs;
   }",NotBuggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes and the elimination of superfluous output. The code diff removes `System.out.println()` statements, which aligns with the ""eliminated some superfluous output"" part of the commit message. The removal of print statements doesn't inherently indicate a bug fix in the sense of correcting incorrect logic or handling exceptions. However, removing unnecessary output can be considered a fix for a performance or usability issue. Given the commit message explicitly mentions ""bug fixes"", and the removal of superfluous output can be seen as a minor improvement, it's plausible that other bug fixes were included in the GUI additions mentioned in the commit message, even if they are not directly visible in this diff.

**Buggy**
"
weka,18418.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,47 +1,55 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       setExperiment(exp);
       System.err.println(""Opened experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions and the removal of superfluous output from NaiveBayesSimple. The provided diff focuses on the `openExperiment` method, specifically how the file extension is handled when opening experiment files.

The original code had a complex `if` condition to check the file extension. The updated code simplifies this by checking the selected file filter from the `JFileChooser` and appending the appropriate extension if it's missing. This change addresses a potential issue where the file extension might not be correctly handled, leading to errors when opening experiment files. The added logic ensures the correct file extension is appended based on the selected filter (Experiment, KOML, or XML).

The changes also include a try-catch block that handles exceptions during the file opening process, displaying an error message to the user if something goes wrong. This is a standard error-handling practice and suggests that the original code might have lacked proper error reporting.

Based on the commit message and the code diff, the changes appear to be related to fixing bugs in the GUI, specifically in how experiment files are opened and handled. The improved file extension handling and error reporting indicate that the original code had issues that needed to be addressed.

**Buggy**"
weka,18378.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,50 +1,58 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-            || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-            || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       
       if (!setExperiment(exp)) {
 	if (m_modePanel != null) m_modePanel.switchToAdvanced(exp);
       }
       System.err.println(""Opened experiment:\n"" + exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions ""bug fixes"" related to GUI additions. The code changes modify how the file extension is handled when opening an experiment file. Specifically, the code now checks the selected file filter in the `JFileChooser` and appends the appropriate extension if it's missing. This suggests that there was a bug where the file extension was not being correctly handled, leading to issues when opening experiment files. The addition of the `else if` blocks and the checks on `m_FileChooser.getFileFilter()` indicate a correction of a faulty logic.

**Buggy**"
weka,18419.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,43 +1,51 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          KOML.write(expFile.getAbsolutePath(), m_Exp);
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          xml.write(expFile, m_Exp);
       }
       // binary
       else {
          FileOutputStream fo = new FileOutputStream(expFile);
          ObjectOutputStream oo = new ObjectOutputStream(
                                  new BufferedOutputStream(fo));
          oo.writeObject(m_Exp);
          oo.close();
       }
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI additions and the elimination of superfluous output from NaiveBayesSimple. The provided diff focuses on the `saveExperiment` method, specifically how the file extension is handled when saving an experiment.

The original code had a complex `if` condition to check if the file name ended with the correct extension. If not, it would append the `.experiment` extension. The modified code simplifies this logic by using the `getFileFilter` method of the `JFileChooser` to determine the correct extension to append based on the selected file type. This change makes the code more readable and maintainable. It also fixes a potential bug where the wrong extension might be appended if the file filter was not correctly set. The code now correctly handles `.experiment`, `.koml`, and `.xml` extensions based on the selected filter. The try-catch block remains the same, indicating no changes in error handling.

The changes in the diff align with the commit message's claim of bug fixes related to GUI additions. The file saving logic is part of the GUI, and the changes address a potential issue with how file extensions are handled.

Therefore, the changes indicate a bug fix.
**Buggy**
"
weka,24330.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,15 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    // Tell the resultproducer to send results to us
+    m_ResultProducer.setResultListener(this);
+    findKeyIndex();
+    if (m_KeyIndex == -1) {
+      throw new Exception(""No key field called "" + m_KeyFieldName
+			  + "" produced by ""
+			  + m_ResultProducer.getClass().getName());
+    }
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message explicitly states ""Fixed bug where sometimes the sub-ResultProducer would not be correctly connected."" This strongly suggests that the commit addresses a bug.

**Diff Analysis:**

The diff shows modifications within the `preProcess()` method. Let's break down the changes:

1.  `m_ResultProducer.setResultListener(this);`: This line sets the current object (`this`) as the result listener for the `m_ResultProducer`. This is a crucial step in establishing the connection between the two components. If this was missing previously, it could explain why the `ResultProducer` was not correctly connected in some cases.

2.  `findKeyIndex();`: This line calls a method to find the index of a key field.

3.  `if (m_KeyIndex == -1) { ... throw new Exception(...) }`: This block checks if the key index was found. If not (i.e., `m_KeyIndex` is -1), it throws an exception indicating that the key field is missing. This is an important error check that could prevent further processing from failing silently.

**Reasoning:**

The commit message indicates a bug related to the connection of a `ResultProducer`. The diff shows that a `setResultListener` call was added, which is essential for establishing communication between the `ResultProducer` and the current object. The added null check and exception handling for the key index also contribute to more robust error handling. The changes directly address the issue described in the commit message.

**Conclusion:**

**Buggy**
"
weka,24356.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,5 +1,6 @@
   public void setResultProducer(ResultProducer newResultProducer) {
-    
+
     m_ResultProducer = newResultProducer;
+    m_ResultProducer.setResultListener(this);
     findKeyIndex();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message explicitly states ""Fixed bug where sometimes the sub-ResultProducer would not be correctly connected."" This strongly suggests a bug fix related to the connection or linking of `ResultProducer` instances.

**Code Diff Analysis:**

The code diff shows a modification to the `setResultProducer` method. Specifically, the line `m_ResultProducer.setResultListener(this);` has been added.

**Reasoning:**

1.  **Alignment:** The commit message mentions a problem with the `ResultProducer` not being correctly connected. The code change adds a line that sets the `ResultListener` of the `ResultProducer` to `this`. This action directly relates to establishing a connection or communication channel between the current object and the `ResultProducer`.
2.  **Bug Fix Indication:** The addition of `m_ResultProducer.setResultListener(this);` suggests that previously, the `ResultListener` was not being set. This could lead to the current object not receiving updates or results from the `ResultProducer`, which would explain the ""not correctly connected"" issue described in the commit message. By setting the listener, the bug is likely being fixed.
3.  **Error Handling/Logical Correction:** This change is a logical correction to ensure that the `ResultListener` is properly set when a new `ResultProducer` is assigned. This is essential for the correct functioning of the system, as it allows the object to receive results from the producer.

**Conclusion:**

The commit message and code diff are highly aligned. The added code directly addresses the issue described in the commit message, and the change appears to be a bug fix related to the proper connection of `ResultProducer` instances.

**Buggy**
"
weka,24326.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,71 +1,63 @@
   protected void doAverageResult(Object [] template) throws Exception {
 
     // Generate the key and ask whether the result is required
     String [] newKey = new String [template.length - 1];
     System.arraycopy(template, 0, newKey, 0, m_KeyIndex);
     System.arraycopy(template, m_KeyIndex + 1,
 		     newKey, m_KeyIndex,
 		     template.length - m_KeyIndex - 1);
-    /*
-    System.err.println(""new key: "" + DatabaseUtils.arrayToString(newKey));
-    */
     if (m_ResultListener.isResultRequired(this, newKey)) {
       Object [] resultTypes = m_ResultProducer.getResultTypes();
       Stats [] stats = new Stats [resultTypes.length];
       for (int i = 0; i < stats.length; i++) {
 	stats[i] = new Stats();
       }
       Object [] result = getResultTypes();
       int numMatches = 0;
       for (int i = 0; i < m_Keys.size(); i++) {
 	Object [] currentKey = (Object [])m_Keys.elementAt(i);
 	// Skip non-matching keys
 	if (!matchesTemplate(template, currentKey)) {
 	  continue;
 	}
 	// Add the results to the stats accumulator
 	Object [] currentResult = (Object [])m_Results.elementAt(i);
 	numMatches++;
-	/*
-	System.err.println(""Match: "" + DatabaseUtils.arrayToString(currentKey)
-			   + "" -- ""
-			   + DatabaseUtils.arrayToString(currentResult));
-	*/
 	for (int j = 0; j < resultTypes.length; j++) {
 	  if (resultTypes[j] instanceof Double) {
 	    if (currentResult[j] == null) {
 	      throw new Exception(""Null numeric result field found:\n""
 				  + DatabaseUtils.arrayToString(currentKey)
 				  + "" -- ""
 				  + DatabaseUtils
 				  .arrayToString(currentResult));
 	    }
 	    double currentVal = ((Double)currentResult[j]).doubleValue();
 	    stats[j].add(currentVal);
 	  }
 	}
       }
       if (numMatches != m_ExpectedResultsPerAverage) {
 	throw new Exception(""Expected "" + m_ExpectedResultsPerAverage
 			    + "" results matching key \""""
 			    + DatabaseUtils.arrayToString(template)
 			    + ""\"" but got ""
 			    + numMatches);
       }
       result[0] = new Double(numMatches);
       Object [] currentResult = (Object [])m_Results.elementAt(0);
       int k = 1;
       for (int j = 0; j < resultTypes.length; j++) {
 	if (resultTypes[j] instanceof Double) {
 	  stats[j].calculateDerived();
 	  result[k++] = new Double(stats[j].mean);
 	  if (getCalculateStdDevs()) {
 	    result[k++] = new Double(stats[j].stdDev);
 	  }
 	} else {
 	  result[k++] = currentResult[j];
 	}
       }
       m_ResultListener.acceptResult(this, newKey, result);      
     }
   }",NotBuggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to connecting the sub-ResultProducer correctly. The code diff focuses on the `doAverageResult` method, which calculates and averages results. The diff doesn't show any changes directly related to connecting or initializing ResultProducers. However, the code includes checks for null values and the expected number of results, and throws exceptions if these conditions are not met. These checks suggest that the averaging process might have been failing under certain circumstances, possibly due to incorrect connections or data retrieval from the sub-ResultProducer. The removal of commented-out debugging lines further supports the idea that the code was modified to address a specific issue.

**Buggy**
"
weka,24324.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,33 +1,31 @@
   public void doRun(int run) throws Exception {
-
+    
     if (m_Instances == null) {
       throw new Exception(""No Instances set"");
     }
-    // Tell the resultproducer to send results to us
-    m_ResultProducer.setResultListener(this);
     m_ResultProducer.setInstances(m_Instances);
-    
+
     // Clear the collected results
     m_Keys.removeAllElements();
     m_Results.removeAllElements();
     
     //    System.err.println(""Starting run "" + run);
     // Collect all the results
     // Should be smarter -- first doing a run collecting only the keys
     // from isResultRequired(), then determining if the average is required,
     // then getting the actual results.
     m_ResultProducer.doRun(run);
 
     // Average the results collected
     //System.err.println(""Number of results collected: "" + m_Keys.size());
 
     // Check that the keys only differ on the selected key field
     checkForMultipleDifferences();
 
     Object [] template = (Object [])((Object [])m_Keys.elementAt(0)).clone();
     template[m_KeyIndex] = null;
     // Check for duplicate keys
     checkForDuplicateKeys(template);
     // Calculate the average and submit it if necessary
     doAverageResult(template);
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The diff shows that `m_ResultProducer.setResultListener(this);` has been removed. This line likely establishes the connection between the main class and the sub-ResultProducer. Removing it could potentially fix a bug where the connection was not being established correctly in some cases, aligning with the commit message.

**Buggy**
"
weka,24382.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,8 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    m_ResultProducer.setResultListener(this);
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The code diff shows that `m_ResultProducer.setResultListener(this)` is added. This line ensures that the current object is set as a listener for the `m_ResultProducer`. If this listener was not set previously, it could lead to the sub-ResultProducer not being correctly connected, as described in the commit message. Therefore, the code change directly addresses the bug mentioned in the commit message.

**Buggy**
"
weka,19514.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,58 +1,66 @@
   public boolean eventGeneratable(String eventName) {
     if (eventName.compareTo(""trainingSet"") == 0) { 
       if (m_trainingProvider == null) {
 	return false;
       } else {
 	if (m_trainingProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_trainingProvider).
 	      eventGeneratable(""trainingSet"")) {
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""dataSet"") == 0) { 
       if (m_dataProvider == null) {
-	m_connectedFormat = null;
-	notifyDataFormatListeners();
+	if (m_instanceProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_dataProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_dataProvider).
 	      eventGeneratable(""dataSet"")) {
 	    m_connectedFormat = null;
 	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""instance"") == 0) { 
       if (m_instanceProvider == null) {
+	if (m_dataProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_instanceProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_instanceProvider).
 	      eventGeneratable(""instance"")) {
+	    m_connectedFormat = null;
+	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""testSet"") == 0) {
       if (m_testProvider == null) {
 	return false;
       } else {
 	if (m_testProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_testProvider).
 	      eventGeneratable(""testSet"")) {
 	    return false;
 	  }
 	}
       }
     }
     return true;
   }",Buggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug in format handling"" suggests a correction related to how data formats are managed within the code.

The diff shows modifications within the `eventGeneratable` method, specifically concerning the handling of ""dataSet"" and ""instance"" events. The changes introduce additional checks for `m_instanceProvider` and `m_dataProvider` being null before returning false and clearing the `m_connectedFormat`. The `notifyDataFormatListeners()` call is also added within the `instance` event check.

The original code only checked `m_dataProvider` for the ""dataSet"" event and `m_instanceProvider` for the ""instance"" event. The added checks suggest that the code was not handling cases where both `m_dataProvider` and `m_instanceProvider` could be null, leading to a potential error or unexpected behavior. The added `notifyDataFormatListeners()` call also indicates a fix related to how listeners are notified about format changes.

These changes strongly suggest a bug fix related to format handling, as the commit message indicates.

**Buggy**"
weka,19490.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,17 +1,18 @@
   public void acceptInstance(InstanceEvent e) {
     if (e.getStatus() == InstanceEvent.FORMAT_AVAILABLE) {
       //      Instances dataSet = e.getInstance().dataset();
       m_connectedFormat = e.getStructure();
       
       //      System.err.println(""Assigning class column..."");
       assignClass(m_connectedFormat);
       notifyInstanceListeners(e);
 
       // tell any listening customizers (or other interested parties)
+      System.err.println(""Notifying customizer..."");
       notifyDataFormatListeners();
     } else {
       //      Instances dataSet = e.getInstance().dataset();
       //      assignClass(dataSet);
       notifyInstanceListeners(e);
     }
   }",NotBuggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to format handling. The code change adds a `System.err.println(""Notifying customizer..."");` statement within the `if (e.getStatus() == InstanceEvent.FORMAT_AVAILABLE)` block.

Reasoning:

1.  **Commit Message:** The phrase ""Fixed bug in format handling"" suggests that the commit addresses an issue specifically related to how data formats are processed.
2.  **Code Change:** The addition of `System.err.println(""Notifying customizer..."");` is likely a debugging statement. Debugging statements are often added when investigating or fixing a bug. The placement of this statement within the `FORMAT_AVAILABLE` block further suggests that the bug was related to the handling of data format availability.
3.  **Relevance:** The commit message and code change are related. The commit message states a bug fix in format handling, and the code change adds a debugging statement in the section of code that handles format availability.

Conclusion:

**Buggy**
"
weka,25728.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,35 +1,37 @@
     protected void performBoostingCV() throws Exception{			
 	
 	//completed iteration keeps track of the number of iterations that have been
 	//performed in every fold (some might stop earlier than others). 
 	//Best iteration is selected only from these.
 	int completedIterations = m_maxIterations;
 	
 	Instances allData = new Instances(m_train);
 	
 	allData.stratify(m_numFoldsBoosting);	      
 
 	double[] error = new double[m_maxIterations + 1];	
 	
+        SimpleLinearRegression[][] backup = m_regressions;
+
 	for (int i = 0; i < m_numFoldsBoosting; i++) {
 	    //split into training/test data in fold
 	    Instances train = allData.trainCV(m_numFoldsBoosting,i);
 	    Instances test = allData.testCV(m_numFoldsBoosting,i);
 
 	    //initialize LogitBoost
 	    m_numRegressions = 0;
-	    m_regressions = initRegressions();
+	    m_regressions = copyRegressions(backup);
 
 	    //run LogitBoost iterations
 	    int iterations = performBoosting(train,test,error,completedIterations);	    
 	    if (iterations < completedIterations) completedIterations = iterations;	    
 	}
 
 	//determine iteration with minimum error over the folds
 	int bestIteration = getBestIteration(error,completedIterations);
 
 	//rebuild model on all of the training data
 	m_numRegressions = 0;
-        m_regressions = initRegressions();
+        m_regressions = backup;
 	performBoosting(bestIteration);
     }",Buggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to cross-validation performed locally at each node. The code diff shows changes within the `performBoostingCV` method, specifically concerning the initialization and management of `m_regressions`.

Here's a breakdown of the changes and their implications:

1.  **`SimpleLinearRegression[][] backup = m_regressions;`**: This line creates a backup of the `m_regressions` array before the cross-validation folds are processed.

2.  **`m_regressions = copyRegressions(backup);`**: Inside the loop iterating through the cross-validation folds, the `m_regressions` array is initialized by copying the backup. This ensures that each fold starts with a fresh copy of the regressions, preventing interference between folds.

3.  **`m_regressions = backup;`**: After the cross-validation loop, the `m_regressions` array is restored to the original backup. This ensures that the final model is built using the original regressions.

The original code initialized `m_regressions` using `initRegressions()` in each fold, which would reset the regressions for each fold, leading to incorrect cross-validation results. The backup and copy mechanism ensures that each fold operates on a consistent set of regressions and that the final model uses the original regressions. This addresses a bug where the cross-validation process was not properly isolated between folds.

Therefore, the changes in the code are directly related to fixing a bug in the cross-validation process.

**Buggy**"
weka,25729.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,22 +1,13 @@
     protected SimpleLinearRegression[][] copyRegressions(SimpleLinearRegression[][] a)	
         throws Exception {
     
         SimpleLinearRegression[][] result = initRegressions();
         for (int i = 0; i < a.length; i++) {
             for (int j = 0; j < a[i].length; j++) {
                 if (j != m_numericDataHeader.classIndex()) {
                     result[i][j].addModel(a[i][j]);
                 }
             }
         }
-
-        /*        SimpleLinearRegression[][] result = null;
-        try {
-            SerializedObject so = new SerializedObject(a);
-            result = (SimpleLinearRegression[][])so.getObject();
-        } catch (Exception ex) {
-            System.err.println(""Can't copy array of simple linear regression objects."");
-            System.err.println(ex);
-            }*/
         return result;
     }",NotBuggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to cross-validation performed locally at each node. The code diff shows a change in the `copyRegressions` method. Specifically, the commented-out code block using `SerializedObject` for copying the `SimpleLinearRegression` array has been removed. The current implementation iterates through the input array `a` and copies the models to the `result` array, skipping the class index.

The removal of the `SerializedObject` approach and the adoption of a direct copy mechanism suggest that the original serialization method might have been causing issues during local cross-validation, potentially leading to incorrect model states or errors. The new implementation ensures a more reliable and direct copying of the regression models, which aligns with the bug fix described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,20414.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,15 +1,15 @@
   protected void newStructure() throws Exception {
     
     m_Loader.reset();
     
     // Set environment variables
     if (m_Loader instanceof EnvironmentHandler && m_env != null) {
       try {
         ((EnvironmentHandler)m_Loader).setEnvironment(m_env);
       }catch (Exception ex) {
       }
     }
     m_dataFormat = m_Loader.getStructure();
-//    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
-//    notifyStructureAvailable(m_dataFormat);
+    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
+    notifyStructureAvailable(m_dataFormat);
   }",NotBuggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps"" clearly states that a bug related to configuration changes not propagating correctly has been addressed. This suggests that the code changes should involve ensuring that downstream steps are notified or updated when configuration changes occur.

**Diff Analysis:**

The diff shows a change in the `newStructure()` method. Specifically, two lines that were previously commented out have been uncommented:

```java
-//    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
-//    notifyStructureAvailable(m_dataFormat);
+    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
+    notifyStructureAvailable(m_dataFormat);
```

The uncommented code now explicitly calls `notifyStructureAvailable(m_dataFormat)`. This method likely notifies listeners (downstream steps) that the data structure (which is based on the configuration) has changed.

**Reasoning:**

The commit message describes a bug where configuration changes weren't being sent to downstream steps. The diff shows that the fix involves uncommenting code that specifically notifies listeners about changes to the data structure. This aligns perfectly with the commit message's description of the bug and its resolution. The fact that this notification was previously commented out strongly suggests that this was the root cause of the bug.

**Conclusion:**

**Buggy**
"
weka,20407.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,4 +1,11 @@
   public void setDB(boolean flag){
   
       m_dbSet = flag;
+      if (m_dbSet) {
+        try {
+          newStructure();
+        } catch (Exception e) {
+          e.printStackTrace();
+        }
+      }
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps"" clearly indicates a bug fix related to configuration changes not propagating correctly. The message suggests that when the configuration changes (specifically, new files are selected or the database configuration is changed), these changes were not being reflected in subsequent steps or processes.

**Diff Analysis:**

The diff shows a modification to the `setDB(boolean flag)` method.  Specifically, when the `m_dbSet` flag is set to `true`, the code now calls `newStructure()`.  The `newStructure()` call is wrapped in a `try-catch` block to handle potential exceptions. The `printStackTrace()` call suggests that any exceptions during the `newStructure()` call are simply logged to the console.

**Reasoning:**

The commit message describes a bug related to configuration changes not being propagated. The diff shows that when the database configuration is set (m_dbSet = true), the `newStructure()` method is called. This suggests that `newStructure()` is responsible for updating or re-initializing the downstream steps based on the new database configuration. The addition of the `try-catch` block indicates that the original code might have been failing when `newStructure()` threw an exception, thus preventing the configuration changes from being applied. The fix involves calling `newStructure()` when the database configuration is set and handling any exceptions that might occur during this process. The exception handling, even if basic, suggests an attempt to prevent the original failure and ensure the configuration changes are applied.

**Conclusion:**

The changes in the diff directly address the bug described in the commit message. The addition of the `newStructure()` call and the `try-catch` block strongly suggest that the original code was not correctly handling database configuration changes, leading to the bug described in the commit message. Therefore, this is a bug fix.

**Buggy**
"
weka,19057.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,6 +1,41 @@
   private void setUpOther() {
     removeAll();
     add(m_LoaderEditor, BorderLayout.CENTER);
+    
+    JPanel buttonsP = new JPanel();
+    buttonsP.setLayout(new FlowLayout());
+    JButton ok,cancel;
+    buttonsP.add(ok = new JButton(""OK""));
+    buttonsP.add(cancel=new JButton(""Cancel""));
+    ok.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under an OK condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingOK();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    cancel.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under a CANCEL condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingCancel();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    
+    add(buttonsP, BorderLayout.SOUTH);
+    
     validate();
     repaint();
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for an issue where configuration changes were not being propagated to downstream steps. The diff introduces an ""OK"" and ""Cancel"" button panel with action listeners. These buttons trigger `m_LoaderEditor.closingOK()` or `m_LoaderEditor.closingCancel()` respectively, which likely handle the propagation of configuration changes. The `m_parentWindow.dispose()` call suggests closing the configuration window after the user confirms or cancels the changes. This aligns with the commit message, as the new buttons and their actions seem to address the issue of applying or discarding configuration changes.

**Buggy**
"
titan,4498.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,4 +1,4 @@
         public Builder set(String path, Object value) {
-            super.set(path, value);
+            writeConfiguration.set(path, value);
             return this;
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",Buggy,"The commit message indicates a change in how TitanFactory.Builder handles configuration keys, specifically to accommodate Elasticsearch's reserved namespace. The problem is that the original implementation used a ModifiableConfiguration which validates config keys against ConfigOption, which is not suitable for arbitrary Elasticsearch config options. The solution is to switch to WriteConfiguration, which is based on strings and doesn't perform validation. The commit message also mentions a test added to verify the fix. The phrase ""Fixes #1114"" clearly indicates that this commit addresses a bug. The code diff confirms that the `set` method now uses `writeConfiguration.set(path, value)` instead of `super.set(path, value)`, which aligns with the commit message's explanation of switching to WriteConfiguration.

**Buggy**
"
titan,353.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,16 +1,16 @@
     public Map<ConfigElement.PathIdentifier,Object> getAll() {
         Map<ConfigElement.PathIdentifier,Object> result = Maps.newHashMap();
 
         for (String key : config.getKeys("""")) {
             Preconditions.checkArgument(StringUtils.isNotBlank(key));
             try {
                 ConfigElement.PathIdentifier pid = ConfigElement.parse(getRootNamespace(),key);
                 Preconditions.checkArgument(pid.element.isOption() && !pid.lastIsUmbrella);
                 result.put(pid,get((ConfigOption)pid.element,pid.umbrellaElements));
             } catch (IllegalArgumentException e) {
-                log.info(""Ignored configuration entry for {} since it does not map to an option"",key,e);
+                log.debug(""Ignored configuration entry for {} since it does not map to an option"",key,e);
                 continue;
             }
         }
         return result;
     }",NotBuggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",Buggy,"The commit message indicates a change in how TitanFactory.Builder handles configuration keys, specifically to accommodate Elasticsearch's reserved namespace. The change involves using a WriteConfiguration instead of a ModifiableConfiguration to avoid validation issues with arbitrary keys. The commit message also mentions a failing test that is fixed by the change.

The code diff shows a change in the log level from `info` to `debug` in the `getAll()` method when a configuration entry does not map to an option. This change suggests that the original logging level was too verbose for a non-critical issue.

The commit message explicitly states that it ""Fixes #1114"", which strongly indicates a bug fix. The change in log level from `info` to `debug` further supports this, as it suggests a refinement in how non-critical issues are handled, which is a common characteristic of bug fixes.

Therefore, the changes indicate a bug fix.
**Buggy**
"
titan,4499.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,3 +1,5 @@
         public TitanGraph open() {
-            return TitanFactory.open(super.getConfiguration());
+            ModifiableConfiguration mc = new ModifiableConfiguration(GraphDatabaseConfiguration.ROOT_NS,
+                    writeConfiguration.copy(), BasicConfiguration.Restriction.NONE);
+            return TitanFactory.open(mc);
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",Buggy,"The commit message indicates a change in how TitanFactory.Builder handles configuration, specifically to accommodate arbitrary keys from Elasticsearch. The original implementation used `ModifiableConfiguration`, which requires validation of config keys against `ConfigOption`. The change replaces this with `WriteConfiguration`, which is based on strings and doesn't enforce validation.

The provided diff snippet shows a modification within the `open()` method of a builder class. It replaces the direct use of `super.getConfiguration()` with a new `ModifiableConfiguration` created from a `WriteConfiguration`. This aligns with the commit message's intention to use a string-based configuration to avoid validation issues with arbitrary keys.

The commit message also mentions a test added to verify the changes, and that it fails without the change. Finally, the commit message indicates that this commit fixes issue #1114, which strongly suggests a bug fix.

Therefore, the changes indicate a bug fix.
**Buggy**
"
spring-boot,5691.json,abe3d3852761430c9f5f1368d82509d0c19ac4d9,"@@ -1,12 +1,11 @@
 	private Stream<Wrapper> getLoadOnStartupWrappers(Container[] children) {
 		Map<Integer, List<Wrapper>> grouped = new TreeMap<>();
 		for (Container child : children) {
 			Wrapper wrapper = (Wrapper) child;
 			int order = wrapper.getLoadOnStartup();
 			if (order >= 0) {
-				grouped.computeIfAbsent(order, ArrayList::new);
-				grouped.get(order).add(wrapper);
+				grouped.computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper);
 			}
 		}
 		return grouped.values().stream().flatMap(List::stream);
 	}",Buggy,"Fix OoM error when starting Tomcat with max int load on startup

Fixes gh-17927
",Buggy,"The commit message indicates a fix for an OutOf Memory (OoM) error when starting Tomcat with a maximum integer load on startup, referencing issue gh-17927. The code diff modifies the `getLoadOnStartupWrappers` method, specifically how wrappers are grouped based on their `loadOnStartup` order.

The original code used `grouped.computeIfAbsent(order, ArrayList::new)` followed by `grouped.get(order).add(wrapper)`. The modified code combines these two operations into a single `grouped.computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper)`.

While the change itself doesn't immediately scream ""bug fix,"" the context provided by the commit message suggests that the original approach might have contributed to the OOM error under specific circumstances (likely related to concurrent access or memory allocation). The `computeIfAbsent` method is designed to be atomic, which could be relevant if the map `grouped` is accessed by multiple threads. Also, the original code could potentially lead to a race condition if `grouped.get(order)` returns null after the `computeIfAbsent` call but before the `add` call, although this is unlikely given the implementation of `TreeMap`.

The change is subtle, but given the commit message's emphasis on fixing an OOM error, it's reasonable to assume that the original code had a flaw that could lead to excessive memory consumption or other issues under high load conditions. The modified code likely addresses this by ensuring that the list is created and the wrapper is added atomically, potentially preventing race conditions or other memory-related problems.

**Buggy**
"
spring-boot,164.json,9c5f207e2221226c9cfdb1970bb2ed2946e142d1,"@@ -1,10 +1,9 @@
 		public ResourceConfigCustomizer resourceConfigCustomizer(
 				final ObjectMapper objectMapper) {
 			addJaxbAnnotationIntrospectorIfPresent(objectMapper);
 			return (ResourceConfig config) -> {
-				JerseyAutoConfiguration.this.config.register(JacksonFeature.class);
-				JerseyAutoConfiguration.this.config.register(
-						new ObjectMapperContextResolver(objectMapper),
+				config.register(JacksonFeature.class);
+				config.register(new ObjectMapperContextResolver(objectMapper),
 						ContextResolver.class);
 			};
 		}",Buggy,"Fix merge error
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix merge error"" suggests that the changes are intended to resolve conflicts or inconsistencies introduced during a merge operation. This often implies that the original code had a bug or was incorrect due to the merge.

2.  **Code Diff Analysis:**
    *   The diff shows that `JerseyAutoConfiguration.this.config.register` has been changed to `config.register`.
    *   The removal of `JerseyAutoConfiguration.this` suggests that the code was likely referencing the wrong `config` object after the merge. This could lead to incorrect registration of features and context resolvers, potentially causing the application to malfunction.

3.  **Bug Fix Indication:**
    *   The commit message explicitly states that it's fixing a merge error.
    *   The code change corrects the way features and context resolvers are registered, which likely resolves an issue introduced during the merge.
    *   The change ensures that the correct `config` object is used, preventing potential runtime errors or unexpected behavior.

**Conclusion:**

The commit message and the code diff strongly suggest that this commit is fixing a bug introduced during a merge. The change ensures that the correct configuration object is used, which is essential for the proper functioning of the application.

**Buggy**"
spring-boot,7647.json,809a3965c961752bcbbc4bcae488f34be062d480,"@@ -1,12 +1,8 @@
 	public Iterable<Tag> tags(ClientRequest request, ClientResponse response, Throwable throwable) {
 		Tag method = WebClientExchangeTags.method(request);
 		Tag uri = WebClientExchangeTags.uri(request);
 		Tag clientName = WebClientExchangeTags.clientName(request);
-		if (response != null) {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(response),
-					WebClientExchangeTags.outcome(response));
-		}
-		else {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(throwable));
-		}
+		return Arrays.asList(method, uri, clientName,
+				(response != null) ? WebClientExchangeTags.status(response) : WebClientExchangeTags.status(throwable),
+				WebClientExchangeTags.outcome(response));
 	}",Buggy,"Add missing outcome tag for WebClient metrics

On error cases, the ""outcome"" tag would be missing from recorded metrics
for the `WebClient`.

This commit fixes this issue and improves the reference documentation by
mentioning the tag values used for error cases, when the client response
is not received (I/O errors, client error, etc).

Fixes gh-17219
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states that the ""outcome"" tag was missing from the recorded metrics for the `WebClient` in error cases. It also mentions that the fix addresses this issue and improves documentation. The phrase ""Fixes gh-17219"" further suggests a bug fix.

2.  **Diff Analysis:**
    *   The original code had a conditional block: if `response` was not null, it included `WebClientExchangeTags.status(response)` and `WebClientExchangeTags.outcome(response)`. Otherwise, if `response` was null (implying an error), it only included `WebClientExchangeTags.status(throwable)`. The `outcome` tag was missing in the error case.
    *   The modified code removes the conditional block and uses a ternary operator to include either `WebClientExchangeTags.status(response)` or `WebClientExchangeTags.status(throwable)` based on whether `response` is null. Critically, `WebClientExchangeTags.outcome(response)` is now *always* included, regardless of whether `response` is null. This ensures that the ""outcome"" tag is present even in error scenarios.

3.  **Alignment and Bug Fix Indication:** The code change directly addresses the issue described in the commit message. The original code had a clear omission (missing ""outcome"" tag in error cases), and the modified code corrects this omission. This strongly indicates a bug fix.

**Conclusion:**

**Buggy**
"
spring-boot,6747.json,3153117429756fc94489c1c9350b2149435a3b3a,"@@ -1,10 +1,13 @@
 	public boolean isAncestorOf(ConfigurationPropertyName name) {
+		if (this.equals(EMPTY)) {
+			return true;
+		}
 		ConfigurationPropertyName candidate = (name == null ? null : name.getParent());
 		while (candidate != null) {
 			if (candidate.equals(this)) {
 				return true;
 			}
 			candidate = candidate.getParent();
 		}
 		return false;
 	}",Buggy,"Fix ConfigurationPropertyName ancestor bug

Fix an issue with `ConfigurationPropertyName` where the `isAncesorOf`
method would not work with `ConfigurationPropertyName.EMPTY`

See gh-9000
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""Fix ConfigurationPropertyName ancestor bug"" clearly states that the commit is intended to fix a bug related to the `isAncestorOf` method in the `ConfigurationPropertyName` class. It specifically mentions an issue where the method wouldn't work correctly with `ConfigurationPropertyName.EMPTY`. The reference to ""gh-9000"" suggests a reported issue that this commit addresses.

2.  **Code Diff Analysis:**
    *   The code diff modifies the `isAncestorOf` method.
    *   A new condition `if (this.equals(EMPTY)) { return true; }` is added at the beginning of the method. This condition explicitly handles the case where the current `ConfigurationPropertyName` instance is `EMPTY`.
    *   The original logic (iterating through parents) remains the same.

3.  **Alignment and Bug Fix Indication:**
    *   The code change directly addresses the issue described in the commit message. The added condition ensures that `EMPTY.isAncestorOf(name)` returns `true` for any `name`.
    *   The original logic likely failed when `this` was `EMPTY` because the `while` loop would never execute, leading to an incorrect `false` result. The new condition corrects this.
    *   The change is a clear bug fix because it corrects incorrect behavior of the `isAncestorOf` method in a specific case.

**Conclusion:**

**Buggy**
"
spring-boot,6206.json,a657a28f58e0eb8487eede9c60062186d850408d,"@@ -1,18 +1,18 @@
 		private void readUnicode() throws IOException {
 			this.character = 0;
 			for (int i = 0; i < 4; i++) {
 				int digit = this.reader.read();
-				if (digit > -'0' && digit <= '9') {
+				if (digit >= '0' && digit <= '9') {
 					this.character = (this.character << 4) + digit - '0';
 				}
-				else if (digit > -'a' && digit <= 'f') {
+				else if (digit >= 'a' && digit <= 'f') {
 					this.character = (this.character << 4) + digit - 'a' + 10;
 				}
-				else if (digit > -'A' && digit <= 'F') {
+				else if (digit >= 'A' && digit <= 'F') {
 					this.character = (this.character << 4) + digit - 'A' + 10;
 				}
 				else {
-					throw new IllegalArgumentException(""Malformed \\uxxxx encoding."");
+					throw new IllegalStateException(""Malformed \\uxxxx encoding."");
 				}
 			}
 		}",Buggy,"Fix properties unicode value decoding

Fix a range error when checking for unicode hex chars.

Fixes gh-12716
",Buggy,"The commit message indicates a fix for unicode value decoding in properties files, specifically addressing a range error when checking for unicode hex characters. The code diff shows changes in the `readUnicode` method, which is responsible for reading and decoding unicode characters represented in the `\uxxxx` format.

The original code used comparisons like `digit > -'0'` which is incorrect. It should be `digit >= '0'`. Similar issues existed for checking 'a' to 'f' and 'A' to 'F'. The diff corrects these comparisons to use `>=` instead of `> -`. This indicates a bug in the original code where certain valid unicode hex characters might have been incorrectly rejected.

Additionally, the exception thrown when a malformed unicode encoding is encountered is changed from `IllegalArgumentException` to `IllegalStateException`. While both are runtime exceptions, `IllegalStateException` is generally used when a method has been invoked at an illegal or inappropriate time, which seems more fitting in this context.

The changes in the code directly address the issue described in the commit message, indicating a bug fix related to unicode decoding.

**Buggy**
"
spring-boot,2030.json,eea83e935eda0f05e1aaa11a38b0e7e379527f40,"@@ -1,10 +1,10 @@
 	private List<String> getData(int errorStatus) {
-		HttpStatus errorHttpStatus = HttpStatus.resolve(errorStatus);
 		List<String> data = new ArrayList<>();
 		data.add(""error/"" + errorStatus);
-		if (errorHttpStatus != null) {
-			data.add(""error/"" + SERIES_VIEWS.get(errorHttpStatus.series()));
+		HttpStatus.Series series = HttpStatus.Series.resolve(errorStatus);
+		if (series != null) {
+			data.add(""error/"" + SERIES_VIEWS.get(series));
 		}
 		data.add(""error/error"");
 		return data;
 	}",Buggy,"Resolve httpstatus error series using raw int code

See gh-16691
",Buggy,"The commit message indicates a resolution of HTTP status errors by using the raw integer code. The code diff shows that `HttpStatus.resolve(errorStatus)` is no longer directly used. Instead, `HttpStatus.Series.resolve(errorStatus)` is used to resolve the series directly from the integer code. The condition `errorHttpStatus != null` is replaced with `series != null`. This change suggests that the original code might have had issues resolving the `HttpStatus` directly, and the fix involves resolving the `HttpStatus.Series` instead. This indicates a bug fix.

**Buggy**"
guava,22893.json,201bb362a9b6de3fec6507ff435ac6022b92da7b,"@@ -1,7 +1,8 @@
   public void addListener(Runnable runnable, Executor executor) {
+    Listener listener = new Listener(runnable, executor);
     if (isDone()) {
-      executor.execute(runnable);
+      listener.execute();
     } else {
-      listeners.add(new Listener(runnable, executor));
+      listeners.add(listener);
     }
   }",Buggy,"Add and use helper methods like assertDone() in tests.
This found no bugs -- well, maybe one if you count the inconsistent propagation of interruption from the delegate, though I had suspected as much already.

Add another ""misbehaving listener"" test, this one for the case in which the Future is already done.
This found a bug, which I've fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=85074586
",Buggy,"The commit message indicates the addition of helper methods for testing and the discovery of a bug related to a ""misbehaving listener"" when the Future is already done. The provided code diff modifies the `addListener` method. Specifically, it creates a `Listener` object before checking if the future is done. If the future is done, it now calls `listener.execute()` instead of `executor.execute(runnable)`. If the future is not done, it adds the `Listener` object to the `listeners` list. This change suggests a fix related to how listeners are handled when the future is already done, aligning with the commit message's mention of a bug found in this scenario.

**Buggy**"
guava,16853.json,1ed32d483d0e583f7a245b60932c8c291c9a1e37,"@@ -1,19 +1,19 @@
-  static <E, E2 extends E> ImmutableMultiset<E> copyFromEntries(
-      Collection<Entry<E2>> entries) {
+  static <E> ImmutableMultiset<E> copyFromEntries(
+      Collection<? extends Entry<? extends E>> entries) {
     long size = 0;
     ImmutableMap.Builder<E, Integer> builder = ImmutableMap.builder();
-    for (Entry<E2> entry : entries) {
+    for (Entry<? extends E> entry : entries) {
       int count = entry.getCount();
       if (count > 0) {
         // Since ImmutableMap.Builder throws an NPE if an element is null, no
         // other null checks are needed.
         builder.put(entry.getElement(), count);
         size += count;
       }
     }
 
     if (size == 0) {
       return of();
     }
     return new RegularImmutableMultiset<E>(builder.build(), Ints.saturatedCast(size));
   }",Buggy,"Fix ImmutableMultiset 1.6.0u24 compilation error.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix ImmutableMultiset 1.6.0u24 compilation error"" clearly states that the purpose of the commit is to resolve a compilation error specifically in version 1.6.0u24 of `ImmutableMultiset`. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `copyFromEntries` method of `ImmutableMultiset`.  Specifically, the generic type declarations have been modified:

*   **Original:**
    ```java
    static <E, E2 extends E> ImmutableMultiset<E> copyFromEntries(
        Collection<Entry<E2>> entries)
    ```

*   **Modified:**
    ```java
    static <E> ImmutableMultiset<E> copyFromEntries(
        Collection<? extends Entry<? extends E>> entries)
    ```

The change involves:

1.  **Removal of `E2`:** The generic type parameter `E2` is removed.
2.  **Wildcard Introduction:** The `Collection` type is changed to `Collection<? extends Entry<? extends E>>`.

**Reasoning:**

The original code used a separate generic type `E2` that was constrained to be a subtype of `E`. This could lead to compilation errors if the `Entry` objects in the input `Collection` had a type that didn't exactly match `E`.

The modified code uses wildcards (`? extends E`) to allow the `Entry` objects to have elements of any type that is a subtype of `E`. This makes the method more flexible and likely resolves the compilation error mentioned in the commit message. The use of `? extends` indicates that the method is designed to handle a variety of subtypes of `E`, which is a common pattern when dealing with generic collections. The removal of `E2` simplifies the generic type declaration.

**Conclusion:**

The commit message explicitly mentions a compilation error fix. The code changes involve modifications to generic type declarations, which likely resolves the compilation issue. This indicates a bug fix related to type compatibility.

**Buggy**
"
guava,15762.json,2b7e8589db7c8ba987c6a960114a3194a7c2fe0e,"@@ -1,8 +1,9 @@
   @Override public int compare(Comparable left, Comparable right) {
-    checkNotNull(right); // left null is caught later
+    checkNotNull(left); // for GWT
+    checkNotNull(right);
     if (left == right) {
       return 0;
     }
 
     return left.compareTo(right);
   }",Buggy,"Make SerializableTester.reserialize a no-op under GWT so that test authors can use it as part of larger test methods.
With that done, eliminate the equivalent, awful hack in RangeTest.
Further, with bug 5599623 fixed, run OrderingTest under GWT.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=25672151
",Buggy,"The commit message indicates several changes: making `SerializableTester.reserialize` a no-op under GWT, removing a hack in `RangeTest`, and running `OrderingTest` under GWT after fixing bug 5599623.

The provided diff shows a change in a `compare` method. Specifically, `checkNotNull(left)` is added. The original code only checked `right` for null, assuming `left` null would be caught later. The added check `checkNotNull(left)` seems to be related to making the code work under GWT, as indicated by the comment ""// for GWT"". This suggests a bug or incompatibility existed with GWT that required the explicit null check for `left`. The commit message mentions running `OrderingTest` under GWT after a bug fix, which could be related to this change.

The addition of `checkNotNull(left)` suggests a potential null pointer exception could occur in GWT if `left` is null, which was not explicitly handled before. This indicates a bug fix.

**Buggy**
"
guava,19040.json,20a42756c477b03c0123870b763148f4a18bdc8e,"@@ -1,39 +1,39 @@
   public static long checkedPow(long b, int k) {
     checkNonNegative(""exponent"", k);
     if (b >= -2 & b <= 2) {
       switch ((int) b) {
         case 0:
           return (k == 0) ? 1 : 0;
         case 1:
           return 1;
         case (-1):
           return ((k & 1) == 0) ? 1 : -1;
         case 2:
           checkNoOverflow(k < Long.SIZE - 1);
           return 1L << k;
         case (-2):
           checkNoOverflow(k < Long.SIZE);
           return ((k & 1) == 0) ? (1L << k) : (-1L << k);
         default:
           throw new AssertionError();
       }
     }
     long accum = 1;
     while (true) {
       switch (k) {
         case 0:
           return accum;
         case 1:
           return checkedMultiply(accum, b);
         default:
           if ((k & 1) != 0) {
             accum = checkedMultiply(accum, b);
           }
           k >>= 1;
           if (k > 0) {
-            checkNoOverflow(b <= FLOOR_SQRT_MAX_LONG);
+            checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);
             b *= b;
           }
       }
     }
   }",Buggy,"Fix LongMath.checkedPow to correctly error out on large negative inputs, and fix LongMathTest to test a full range of long inputs.  See https://github.com/google/guava/issues/2036 .
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=92571001
",Buggy,"The commit message indicates a fix for `LongMath.checkedPow` concerning large negative inputs and an update to `LongMathTest` to test a full range of long inputs. The diff focuses on the `checkedPow` method.

The key change in the diff is the condition within the `if (k > 0)` block:

```diff
-            checkNoOverflow(b <= FLOOR_SQRT_MAX_LONG);
+            checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);
```

The original code only checked if `b` was less than or equal to `FLOOR_SQRT_MAX_LONG`. The updated code now checks if `b` is within the range `[-FLOOR_SQRT_MAX_LONG, FLOOR_SQRT_MAX_LONG]`. This change addresses the issue of incorrect behavior with large negative inputs, as described in the commit message and the linked issue.  The original code would not have detected an overflow when squaring a large negative number, leading to incorrect results or exceptions being thrown at the wrong time.

The change directly relates to correcting an error in the `checkedPow` method's handling of negative inputs, which constitutes a bug fix.

**Buggy**"
guava,14536.json,93ce68b41636faafb78d391dcf720e06154862da,"@@ -1,6 +1,6 @@
   private static void closeAll(BaseStream<?, ?>[] toClose) {
     for (BaseStream<?, ?> stream : toClose) {
-      // TODO(b/198102330): Catch exceptions, rethrowing later with extras as suppressed exceptions.
+      // TODO(b/80534298): Catch exceptions, rethrowing later with extras as suppressed exceptions.
       stream.close();
     }
   }",Buggy,"Fix ""bug"" link that was actually a CL.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265457436
",Buggy,"The commit message indicates a fix related to a ""bug"" link, which was actually a CL (changelist). The diff shows a change in a TODO comment, specifically updating the issue number from `b/198102330` to `b/80534298`. This suggests that the original issue number in the TODO was incorrect, and the commit corrects it to the correct issue number. This correction of an incorrect issue number qualifies as a bug fix, as it rectifies an inaccuracy in the codebase.

**Buggy**"
guava,16180.json,ac579e0d20dd45ac8a713789e4708858a19fa894,"@@ -1,6 +1,5 @@
   public static <E extends Comparable> int binarySearch(List<? extends E> list, E e,
       KeyPresentBehavior presentBehavior, KeyAbsentBehavior absentBehavior) {
     checkNotNull(e);
-    return binarySearch(
-        list, checkNotNull(e), Ordering.natural(), presentBehavior, absentBehavior);
+    return binarySearch(list, e, Ordering.natural(), presentBehavior, absentBehavior);
   }",Buggy,"Fix some bugs related to calling Preconditions.checkNotNull on expressions
that are definitely non-null.

Created via a work-in-progress error-prone check for unnecessary null checks.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=81594383
",Buggy,"The commit message indicates a fix for bugs related to unnecessary `Preconditions.checkNotNull` calls. The diff shows the removal of a `checkNotNull(e)` call within the `binarySearch` method, where `e` was already checked for null on the previous line. This aligns perfectly with the commit message, indicating that the change is indeed a bug fix that removes redundant null checks.

Reasoning:

1.  **Commit Message Analysis:** The commit message explicitly states the fix addresses bugs related to unnecessary `checkNotNull` calls.
2.  **Diff Analysis:** The diff shows the removal of `checkNotNull(e)` which is redundant.
3.  **Relevance:** The code modification directly corresponds to the problem described in the commit message. The original code had a redundant null check, which has been removed.

Conclusion: **Buggy**"
guava,17257.json,13800999134027e799b9250d261dd2c2a969183d,"@@ -1,12 +1,12 @@
   public static <K, V> MapDifference<K, V> difference(
       Map<? extends K, ? extends V> left, Map<? extends K, ? extends V> right,
       Equivalence<? super V> valueEquivalence) {
     Preconditions.checkNotNull(valueEquivalence);
 
-    Map<K, V> onlyOnLeft = newHashMap();
-    Map<K, V> onlyOnRight = new HashMap<K, V>(right); // will whittle it down
-    Map<K, V> onBoth = newHashMap();
-    Map<K, MapDifference.ValueDifference<V>> differences = newHashMap();
+    Map<K, V> onlyOnLeft = newLinkedHashMap();
+    Map<K, V> onlyOnRight = new LinkedHashMap<K, V>(right); // will whittle it down
+    Map<K, V> onBoth = newLinkedHashMap();
+    Map<K, MapDifference.ValueDifference<V>> differences = newLinkedHashMap();
     doDifference(left, right, valueEquivalence, onlyOnLeft, onlyOnRight, onBoth, differences);
     return new MapDifferenceImpl<K, V>(onlyOnLeft, onlyOnRight, onBoth, differences);
   }",Buggy,"Fix hash map ordering bug.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=74779605
",Buggy,"The commit message ""Fix hash map ordering bug"" suggests that the commit addresses an issue related to the order of elements in a hash map.

The diff replaces `newHashMap()` with `newLinkedHashMap()` and `new HashMap<K, V>(right)` with `new LinkedHashMap<K, V>(right)`. `newHashMap()` creates a `HashMap`, which does not guarantee any specific order of elements. `newLinkedHashMap()` creates a `LinkedHashMap`, which maintains the order in which elements are inserted.

The change from `HashMap` to `LinkedHashMap` directly relates to preserving the insertion order of elements in the map. This aligns perfectly with the commit message indicating a fix for a hash map ordering bug. The original code used `HashMap`, which doesn't guarantee order, while the corrected code uses `LinkedHashMap`, which does. This change likely fixes a bug where the order of elements in the map was not being preserved as expected.

**Buggy**"
guava,15442.json,61110e175fc90fa2e8d675c8de85dff4ed699b07,"@@ -1,6 +1,6 @@
   public void forEachEntry(ObjIntConsumer<? super E> action) {
     checkNotNull(action);
-    for (int i = 0; i < size(); i++) {
+    for (int i = 0; i < length; i++) {
       action.accept(elementSet.asList().get(i), getCount(i));
     }
   }",Buggy,"Fix embarrassing forEachEntry bug in ImmutableSortedMultiset.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=151063670
",Buggy,"The commit message indicates a bug fix related to the `forEachEntry` method in `ImmutableSortedMultiset`. The diff shows a change in the loop condition from `i < size()` to `i < length`. This suggests that the original code was incorrectly using the `size()` method, potentially leading to an out-of-bounds access or incorrect iteration. The change to `length` likely corrects this issue, aligning with the commit message's intention to fix a bug.

**Buggy**"
guava,22117.json,44d99fc5f83f06f577f61cf63569604d3142d96a,"@@ -1,3 +1,5 @@
     final void addInitialException(Set<Throwable> seen) {
-      addCausalChain(seen, trustedGetException());
+      if (!isCancelled()) {
+        addCausalChain(seen, trustedGetException());
+      }
     }",Buggy,"Recognize that the reason for an early completion might be cancellation rather than exception.
This fixes the bug ""com.google.common.util.concurrent.AbstractFuture$Cancellation cannot be cast to com.google.common.util.concurrent.AbstractFuture$Failure""

TODO(cpovirk): Write basher tests at some point.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=96784416
",Buggy,"The commit message indicates a fix for a casting issue related to cancellation and exceptions in `com.google.common.util.concurrent.AbstractFuture`. The diff shows that the `addCausalChain` method, which processes exceptions, is now only called if the future is not cancelled. This change directly addresses the stated bug where a `Cancellation` object was being incorrectly cast to a `Failure` object, likely because the exception was being accessed even after the future was cancelled. The added `isCancelled()` check prevents this incorrect casting by skipping the exception processing when the future has been cancelled. This is a clear indication of a bug fix.

**Buggy**"
pmd,6789.json,b86f0aec59762277d67960ed30249f1f8de815c4,"@@ -1,3 +1,12 @@
     public Object visit(ASTConditionalExpression node, Object data) {
-        return node.isTernary() ? sumChildrenComplexities(node, data) + 2 : 1;
+        // bool comp of guard clause + complexity of last two children (= total - 1)
+
+        if (node.isTernary()) {
+            ASTExpression wrapper = new ASTExpression(Integer.MAX_VALUE);
+            wrapper.jjtAddChild(node.jjtGetChild(0), 0);
+            int boolCompTernary = CycloMetric.booleanExpressionComplexity(wrapper);
+
+            return boolCompTernary + sumChildrenComplexities(node, data) - 1;
+        }
+        return 1;
     }",Buggy,"Fix npath bugs with ternary
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix npath bugs with ternary"" clearly states that the commit addresses bugs related to the npath complexity calculation of ternary expressions. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff modifies the `visit(ASTConditionalExpression node, Object data)` method, which is responsible for calculating the complexity of conditional expressions (including ternary operators).

Here's a breakdown of the changes:

1.  **Original Code:**
    ```java
    return node.isTernary() ? sumChildrenComplexities(node, data) + 2 : 1;
    ```
    This code calculates the complexity of a ternary expression by summing the complexities of its children and adding 2.

2.  **Modified Code:**
    ```java
    if (node.isTernary()) {
        ASTExpression wrapper = new ASTExpression(Integer.MAX_VALUE);
        wrapper.jjtAddChild(node.jjtGetChild(0), 0);
        int boolCompTernary = CycloMetric.booleanExpressionComplexity(wrapper);

        return boolCompTernary + sumChildrenComplexities(node, data) - 1;
    }
    return 1;
    ```
    The modified code introduces a more sophisticated calculation for ternary expressions:

    *   It extracts the condition part of the ternary expression.
    *   It calculates the boolean expression complexity of the condition using `CycloMetric.booleanExpressionComplexity()`.
    *   It sums the boolean expression complexity with the complexities of the children and subtracts 1.

**Reasoning:**

The original code's simple ""+ 2"" approach likely had inaccuracies in calculating the npath complexity of ternary expressions, especially when the condition part itself contained boolean operators. The new code explicitly calculates the boolean expression complexity of the condition, which is a more accurate way to determine the npath complexity. The commit message explicitly mentions ""npath bugs,"" and the code changes directly address the calculation of npath complexity for ternary expressions. The changes involve a more accurate calculation of complexity, which strongly suggests that the previous calculation was incorrect, thus indicating a bug.

**Conclusion:**

**Buggy**
"
pmd,7930.json,be11288e2e0855a52298fdedb8363e66684b2252,"@@ -1 +1 @@
-    public Object visit(ASTIfStatement node, Object data){openScope(node);return data;}

+    public Object visit(ASTSwitchStatement node, Object data){openScope(node);return data;}",Buggy,"Fixed bug in symbol table; it wasn't creating a scope level when it hit a switch statement


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1388 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix in the symbol table related to scope creation within switch statements. The provided diff shows a change in the `visit` method, specifically from `ASTIfStatement` to `ASTSwitchStatement`. This change suggests that the original code was incorrectly handling scope creation for `if` statements instead of `switch` statements. The commit message and the code change align, indicating that the bug fix involves ensuring a new scope level is created when the symbol table encounters a switch statement.

**Buggy**"
pmd,1467.json,5c049b6946c8a2fddc780312e6783a539935feaa,"@@ -1,38 +1,42 @@
     public void apply( List acus, RuleContext ctx ) {
 		visitAll( acus, ctx );
 
 		double deviation = 0.0;
 		double minimum = 0.0;
 		
 		if (hasProperty(""sigma"")) {
 			deviation = getStdDev();
 			double sigma = getDoubleProperty(""sigma"");
 			
 			minimum = getMean() + (sigma * deviation);
 		}
 	
 		if (hasProperty(""minimum"")) {
 			double mMin = getDoubleProperty(""minimum"");
 			if (mMin > minimum) {
                 minimum = mMin;
             }
 		} 
 
 		SortedSet newPoints = applyMinimumValue(dataPoints, minimum);
 			
 		if (hasProperty(""topscore"")) {
 			int topScore = getIntProperty(""topscore"");
 			if (newPoints.size() >= topScore) {
 		    	newPoints = 
 		    		applyTopScore(newPoints, topScore);
 			}
 		}
 		
 		makeViolations(ctx, newPoints);
-		
-		double low = ((DataPoint) dataPoints.first()).getScore();
-		double high = ((DataPoint) dataPoints.last()).getScore();
-	
+
+        double low = 0.0d;
+        double high = 0.0d;
+        if (!dataPoints.isEmpty()) {
+            low = ((DataPoint) dataPoints.first()).getScore();
+            high = ((DataPoint) dataPoints.last()).getScore();
+        }
+
 		ctx.getReport().addMetric( new Metric( this.getName(), low, high,
 		                                       getMean(), getStdDev()));
     }",Buggy,"fixed bug which caused Metrics stuff to fail on interfaces - it was calling first() on an empty SortedSet


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@994 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to calling `first()` on an empty `SortedSet`. The diff shows that the code now checks if `dataPoints` is empty before calling `first()` and `last()`. If it's empty, `low` and `high` are set to 0.0d. This prevents the `NoSuchElementException` that would occur when calling `first()` on an empty set. The changes align with the commit message and clearly indicate a bug fix.

**Buggy**"
pmd,658.json,05948ab5d813a0fe683625ee6da797360f1c8bd8,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed bug 1183032 - The XMLRenderer no longer throws a SimpleDateFormat exception when run with JDK 1.3.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3433 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to `SimpleDateFormat` throwing an exception when the `XMLRenderer` is run with JDK 1.3. The code diff modifies the `SimpleDateFormat` pattern from `""yyyy-MM-dd'T'HH:mm:ss.SSSZ""` to `""yyyy-MM-dd'T'HH:mm:ss.SSS""`. The `Z` in the original pattern represents the time zone, which might not be supported or might be implemented differently in JDK 1.3, potentially causing the reported exception. Removing `Z` makes the pattern compatible with JDK 1.3, thus fixing the bug.

**Buggy**"
pmd,658.json,90eb57ade9a06904052833ea92076cc787b68615,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'hh:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed [ pmd-Bugs-1100196 ] timestamp attribute in xml should use 24h clock


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3126 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a fix for a bug related to the timestamp attribute in XML, specifically addressing the use of a 24-hour clock format.

The diff shows a change in the `SimpleDateFormat` pattern. The original pattern used ""hh"" which represents a 12-hour clock, while the modified pattern uses ""HH"" which represents a 24-hour clock. This change directly aligns with the commit message's intention to use a 24-hour clock format for the timestamp.

Therefore, the changes indicate a bug fix.

**Buggy**"
pmd,2488.json,051136af5fe667edd815597c68bab215b4f49001,"@@ -1,7 +1,7 @@
     private boolean isLegalPath(String path, LanguageConfig config) {
     	String[] extensions = config.extensions();
     	for (int i=0; i<extensions.length; i++) {
-    		if (path.endsWith(extensions[i])) return true;
+    		if (path.endsWith(extensions[i]) && extensions[i].length() > 0) return true;
     	}
     	return false;
     }",Buggy,"Fixed bug 1593292 - The CPD GUI now works with the 'by extension' option selected.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4833 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a bug fix related to the CPD GUI when the 'by extension' option is selected. The code change modifies the `isLegalPath` method, specifically the condition within the loop. The original code only checked if the path ends with a certain extension. The modified code adds an additional check to ensure that the extension's length is greater than 0. This suggests that the original code might have incorrectly identified paths as legal when the extension was an empty string, which would cause unexpected behavior when the 'by extension' option is selected. Therefore, the change addresses a bug.

**Buggy**"
pmd,6071.json,aeffeaec3d7763ca46f208be9f93ee2036ce8003,"@@ -1,8 +1,8 @@
-  public Object childrenAccept(JspParserVisitor visitor, Object data) {
-    if (children != null) {
-      for (int i = 0; i < children.length; ++i) {
-        children[i].jjtAccept(visitor, data);
-      }
-    }
-    return data;
-  }

+    public Object childrenAccept(JspParserVisitor visitor, Object data) {
+        if (children != null) {
+            for (int i = 0; i < children.length; ++i) {
+                ((SimpleNode)children[i]).jjtAccept(visitor, data);
+            }
+        }
+        return data;
+    }",Buggy,"Fixed compilation problems, JSP tests are not yet working, but all other tests run


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4208 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates that the commit fixes compilation problems. The diff shows a change in the `childrenAccept` method where `children[i].jjtAccept(visitor, data)` is cast to `((SimpleNode)children[i]).jjtAccept(visitor, data)`. This suggests that the original code was causing a compilation error because `children[i]` was not guaranteed to be a `SimpleNode`, and the `jjtAccept` method was not available on the original type. This change fixes that compilation error by explicitly casting the child to `SimpleNode`. This is a bug fix.

**Buggy**"
pmd,7746.json,109f458dbf0d6b8c64d77943f9523d98700162d9,"@@ -1,7 +1,10 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
-        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
-            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        // for a real class/interface the 3rd parent is a ClassOrInterfaceDeclaration,
+        // for anonymous classes, the parent is e.g. a AllocationExpression
+        Node potentialTypeDeclaration = getNthParent(3);
+
+        if (potentialTypeDeclaration instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) potentialTypeDeclaration).isInterface();
         }
         return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces

*   Remember old state to allow nesting
*   Fix ASTMethodDeclaration.isInterfaceMember
*   Extended tests
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message ""Fixes #793 [java] Parser error with private method in nested classes in interfaces"" clearly states that this commit is intended to resolve a bug related to parsing Java code. Specifically, it addresses an issue where the parser incorrectly handles private methods declared within nested classes inside interfaces. The message also mentions:

*   ""Remember old state to allow nesting"": This suggests a change in how the parser manages its state when encountering nested structures.
*   ""Fix ASTMethodDeclaration.isInterfaceMember"": This points to a correction in the `isInterfaceMember` method of the `ASTMethodDeclaration` class, which is likely responsible for determining if a method is a member of an interface.
*   ""Extended tests"": This indicates that new tests have been added to verify the fix.

**2. Analyzing the Code Diff:**

The code diff focuses on the `isInterfaceMember` method within what appears to be the `ASTMethodDeclaration` class. The original implementation checked if the method's parent was an `ASTClassOrInterfaceBody` and then if *that* parent was an `ASTClassOrInterfaceDeclaration` representing an interface.

The modified code changes the logic to get the 3rd parent node and check if it is an `ASTClassOrInterfaceDeclaration`. This suggests the original logic was flawed in certain scenarios, likely involving nested classes or anonymous classes, where the parent-child relationship might not have been as expected. The new logic appears to be more robust in identifying the correct `ASTClassOrInterfaceDeclaration`.

**3. Reasoning:**

*   The commit message explicitly states that it's a bug fix.
*   The code diff modifies the `isInterfaceMember` method, which is directly related to determining if a method belongs to an interface.
*   The original logic was likely incorrect in handling nested or anonymous classes, leading to parsing errors.
*   The modified logic addresses this issue by traversing up the parent hierarchy to find the correct `ASTClassOrInterfaceDeclaration`.
*   The addition of tests further supports the claim that this commit fixes a bug.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix. The original implementation of `isInterfaceMember` had a flaw that caused parsing errors in specific scenarios (nested classes in interfaces), and the modified code corrects this flaw.

**Buggy**
"
pmd,7746.json,64b862eef965aaa39c17db1808063e2f129d7057,"@@ -1,4 +1,7 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceDeclaration clz = getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
-        return clz != null && clz.isInterface();
+        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
+        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        }
+        return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixes #793 [java] Parser error with private method in nested classes in interfaces"" explicitly states that the change addresses a parser error related to private methods within nested classes in interfaces. This strongly suggests a bug fix.

2.  **Code Diff Analysis:**
    *   The original code directly retrieved the `ASTClassOrInterfaceDeclaration` node.
    *   The modified code retrieves the `ASTClassOrInterfaceBody` node first. Then, it checks if the parent of the body is an `ASTClassOrInterfaceDeclaration`. Finally, it checks if the declaration is an interface.
    *   The change suggests that the original code was not correctly identifying the parent class/interface declaration in the case of nested classes within interfaces. The new code introduces an intermediate step to correctly identify the parent declaration.

3.  **Bug Fix Indication:** The original code likely failed when parsing code with nested classes inside interfaces, leading to a parser error. The modified code corrects this by traversing the AST correctly to identify the parent class/interface declaration. This resolves the parser error, which is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
pmd,7746.json,078ec6e2c8fdee2c51c2fb6a9bf6c6220d2a7032,"@@ -1,3 +1,4 @@
     public boolean isInterfaceMember() {
-        return ((ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class)).isInterface();
+        ASTClassOrInterfaceDeclaration clz = (ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
+        return clz != null && clz.isInterface();
     }",Buggy,"Fixed bug 1400754 - A NPE is no longer thrown on certain JDK 1.5 enum usages.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4127 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed bug 1400754 - A NPE is no longer thrown on certain JDK 1.5 enum usages"" clearly states that the commit addresses a bug. Specifically, it mentions a `NullPointerException` (NPE) that occurred when dealing with JDK 1.5 enums. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff shows a change in the `isInterfaceMember()` method. The original code directly casts the result of `getFirstParentOfType(ASTClassOrInterfaceDeclaration.class)` to `ASTClassOrInterfaceDeclaration` and then calls `isInterface()` on it. If `getFirstParentOfType()` returns `null` (meaning no such parent exists), this would indeed cause an NPE.

The modified code adds a null check:
```java
ASTClassOrInterfaceDeclaration clz = (ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
return clz != null && clz.isInterface();
```
Now, the code first checks if `clz` is not null before calling `clz.isInterface()`. This prevents the NPE if no `ASTClassOrInterfaceDeclaration` parent is found.

**Relevance and Conclusion:**

The code change directly addresses the issue described in the commit message. The added null check prevents the `NullPointerException` that was occurring in certain scenarios (likely related to JDK 1.5 enum usages, as stated in the message). The change is a clear bug fix.

**Buggy**
"
pmd,7774.json,0f2e98bf9112c38ee071489286a8d827ae02a460,"@@ -1,3 +1,3 @@
     public String getPackageNameImage() {
-        return ((ASTName)jjtGetChild(0)).getImage();
+	return ((ASTName) jjtGetChild(this.jjtGetNumChildren() - 1)).getImage();
     }",Buggy,"bug fix: typecast exception in LoosePackageCoupling for annotation before package name


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6077 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""bug fix: typecast exception in LoosePackageCoupling for annotation before package name"" clearly indicates a bug fix. It specifies that a typecast exception was occurring in the `LoosePackageCoupling` class, specifically related to handling annotations appearing before the package name. This suggests the original code was not correctly parsing or handling cases where annotations preceded the package declaration.

**Diff Analysis:**

The diff shows a change in the `getPackageNameImage()` method.  The original code:

```java
return ((ASTName)jjtGetChild(0)).getImage();
```

was modified to:

```java
return ((ASTName) jjtGetChild(this.jjtGetNumChildren() - 1)).getImage();
```

The original code always assumed the package name was the first child (`jjtGetChild(0)`). The modified code now retrieves the last child (`jjtGetChild(this.jjtGetNumChildren() - 1)`).

**Reasoning:**

The change in the code directly addresses the issue described in the commit message. If annotations are present before the package name, the package name will no longer be the first child node. By accessing the *last* child node, the code now correctly retrieves the package name regardless of preceding annotations. The original code would have attempted to cast the annotation node to `ASTName`, resulting in a `ClassCastException` (which is a typecast exception). The modified code avoids this exception by correctly identifying the `ASTName` node representing the package name.

**Conclusion:**

The commit message and code changes are highly relevant. The code modification directly fixes the typecast exception described in the commit message. This is a clear indication of a bug fix.

**Buggy**
"
pmd,4667.json,64e193fcb69b2fbd3ccdeeccecae0b45f02ff055,"@@ -1,11 +1,21 @@
     public Object visit(ASTUserClass node, Object data) {
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
+        List<ASTVariableDeclaration> variableDecls = node.findDescendantsOfType(ASTVariableDeclaration.class);
+        for (ASTVariableDeclaration varDecl : variableDecls) {
+            findSafeLiterals(varDecl);
+        }
+
+        List<ASTFieldDeclaration> fieldDecl = node.findDescendantsOfType(ASTFieldDeclaration.class);
+        for (ASTFieldDeclaration fDecl : fieldDecl) {
+            findSafeLiterals(fDecl);
+        }
+
         List<ASTNewObjectExpression> newObjects = node.findDescendantsOfType(ASTNewObjectExpression.class);
         for (ASTNewObjectExpression newObj : newObjects) {
             checkNewObjects(newObj, data);
         }
         return data;
     }",Buggy,"Bug fix - contd
",Buggy,"The commit message ""Bug fix - contd"" is vague, but it suggests that the commit is intended to fix a bug.

The code changes involve adding two loops that iterate through variable declarations and field declarations within a user class node. Inside these loops, the `findSafeLiterals` method is called for each variable and field declaration. The original code only processed `ASTNewObjectExpression` nodes.

Given the commit message indicates a bug fix and the code changes introduce new logic related to variable and field declarations, it's plausible that the original code was missing these checks, leading to a bug. The added code likely addresses this oversight.

Therefore, the changes indicate a bug fix.

**Buggy**"
pmd,1144.json,5ca779d9e255f093175f8b738e3feca1c9758742,"@@ -1,11 +1,13 @@
     public Node getNthParent(int n) {
-	Node result = null;
-	for (int i = 0; i < n; i++) {
-	    if (result == null) {
-		result = this.jjtGetParent();
-	    } else {
-		result = result.jjtGetParent();
-	    }
-	}
-	return result;
+        if (n <= 0) {
+            throw new IllegalArgumentException();
+        }
+        Node result = this.jjtGetParent();
+        for (int i = 1; i < n; i++) {
+            if (result == null) {
+                return null;
+            }
+            result = result.jjtGetParent();
+        }
+        return result;
     }",Buggy,"bug fix: getNthParent() in AbstractNode was looping if argument was higher than ancestor number

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6238 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"Okay, let's analyze the provided Git commit message and diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""bug fix: getNthParent() in AbstractNode was looping if argument was higher than ancestor number"" clearly states that the commit addresses a bug in the `getNthParent()` method of the `AbstractNode` class. The bug involved an infinite loop when the input argument `n` (representing the desired ancestor level) was greater than the actual number of available ancestors.

**Diff Analysis:**

The diff shows the changes made to the `getNthParent()` method. Let's break down the changes:

1.  **Input Validation:**
    *   A check `if (n <= 0)` is added at the beginning of the method. This throws an `IllegalArgumentException` if `n` is zero or negative, which is a reasonable validation.

2.  **Looping Logic Correction:**
    *   The original code initialized `result` to `null` and then iterated `n` times, getting the parent node in each iteration. If `n` was larger than the number of available parents, the loop would continue, and `result` would eventually become `null`, but the loop would not terminate.
    *   The corrected code initializes `result` to the immediate parent (`this.jjtGetParent()`). The loop now iterates from `i = 1` to `n - 1`. Inside the loop, it checks if `result` is `null`. If it is, it means we've reached the root of the tree (or a point where there are no more parents), and the method immediately returns `null`. This prevents the infinite loop.

**Reasoning:**

The commit message and the diff align perfectly. The original code had a flaw where it would loop indefinitely if the requested ancestor level was beyond the actual number of ancestors. The corrected code addresses this by:

*   Adding input validation to handle invalid `n` values.
*   Adding a check within the loop to see if the current node has a parent. If not, it returns `null`, preventing the infinite loop.

The changes clearly indicate a bug fix related to incorrect looping behavior.

**Conclusion:**

**Buggy**
"
pmd,4676.json,3079ce26a85a9ed32d86bd039606c87decf23c82,"@@ -1,6 +1,6 @@
     private void checkForSharingDeclaration(ApexNode<?> node, Object data, boolean sharingFound) {
-        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) && Helper.foundAnySOQLorSOSL(node);
+        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) || Helper.foundAnySOQLorSOSL(node);
         if (!sharingFound && !Helper.isTestMethodOrClass(node) && foundAnyDMLorSOQL) {
             addViolation(data, node);
         }
     }",Buggy,"Bug fix
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Analyze the Code Change:**

*   The code change modifies a boolean expression within the `checkForSharingDeclaration` method.
*   Specifically, it changes `&&` (AND) to `||` (OR) in the `foundAnyDMLorSOQL` variable assignment.
*   The original condition `Helper.foundAnyDML(node) && Helper.foundAnySOQLorSOSL(node)` required both DML and SOQL/SOSL to be present for the condition to be true.
*   The modified condition `Helper.foundAnyDML(node) || Helper.foundAnySOQLorSOSL(node)` requires either DML or SOQL/SOSL to be present for the condition to be true.
*   This suggests that the original logic was too restrictive, potentially missing cases where a sharing declaration violation should have been flagged.

**2. Analyze the Commit Message:**

*   The commit message is simply ""Bug fix"". This is a very generic commit message, but it does explicitly state that the change is intended to fix a bug.

**3. Reasoning:**

*   The code change modifies a boolean condition, making it less restrictive. This suggests that the original condition was incorrect and led to missed violations.
*   The commit message confirms that the change is a bug fix.
*   The combination of the code change and the commit message strongly suggests that the original code had a logical error that was causing it to not correctly identify sharing declaration violations in certain cases.

**Conclusion:**

**Buggy**
"
pmd,31.json,18ea8600ece06f02ae605f9fb1c19c07a4e02c6a,"@@ -1,7 +1,7 @@
     public static boolean applies(Rule rule, LanguageVersion languageVersion) {
         final LanguageVersion min = rule.getMinimumLanguageVersion();
-        final LanguageVersion max = rule.getMinimumLanguageVersion();
+        final LanguageVersion max = rule.getMaximumLanguageVersion();
         return rule.getLanguage().equals(languageVersion.getLanguage())
                 && (min == null || min.compareTo(languageVersion) <= 0)
                 && (max == null || max.compareTo(languageVersion) >= 0);
     }",Buggy,"Fix problem that some rules where not executed - max language version was
determined wrongly
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**1. Commit Message Analysis:**

The commit message ""Fix problem that some rules where not executed - max language version was determined wrongly"" clearly indicates a bug fix. The message states that some rules were not being executed due to an incorrect determination of the maximum language version.

**2. Code Diff Analysis:**

The code diff shows a change in the `applies` method. Specifically, the line:

```java
final LanguageVersion max = rule.getMinimumLanguageVersion();
```

has been changed to:

```java
final LanguageVersion max = rule.getMaximumLanguageVersion();
```

This change directly addresses the issue described in the commit message. The original code was incorrectly using the *minimum* language version to determine the *maximum* language version, which would prevent rules from being executed when the language version was higher than the minimum but lower than the intended maximum.

**3. Relevance and Bug Fix Identification:**

The code change is directly relevant to the commit message. The diff corrects the logic for determining the maximum language version, which aligns with the stated goal of fixing the issue where some rules were not being executed. The original code contained a logical error (using `getMinimumLanguageVersion` instead of `getMaximumLanguageVersion`), and the diff corrects this error. This confirms that the change is a bug fix.

**Conclusion:**

The commit message and code diff are consistent and indicate a bug fix. The code was modified to correct a logical error that caused rules to be skipped.

**Buggy**
"
pmd,2559.json,b745f331b8ca490e0046b566b5a94a2eb10aca84,"@@ -1,19 +1,17 @@
     public void tokenize(SourceCode sourceCode, Tokens tokenEntries) {
         StringBuilder buffer = sourceCode.getCodeBuffer();
         try (Reader reader = new StringReader(buffer.toString())) {
-            final TokenFilter tokenFilter = new JavaCCTokenFilter(new ObjectiveCTokenManager(reader));
+            ObjectiveCTokenManager tokenManager = new ObjectiveCTokenManager(reader);
+            tokenManager.setFileName(sourceCode.getFileName());
+            final TokenFilter tokenFilter = new JavaCCTokenFilter(tokenManager);
             Token currentToken = (Token) tokenFilter.getNextToken();
             while (currentToken != null) {
                 tokenEntries.add(new TokenEntry(currentToken.image, sourceCode.getFileName(), currentToken.beginLine));
                 currentToken = (Token) tokenFilter.getNextToken();
             }
-            tokenEntries.add(TokenEntry.getEOF());
-            System.err.println(""Added "" + sourceCode.getFileName());
-        } catch (TokenMgrError err) {
-            err.printStackTrace();
-            System.err.println(""Skipping "" + sourceCode.getFileName() + "" due to parse error"");
-            tokenEntries.add(TokenEntry.getEOF());
         } catch (IOException e) {
             e.printStackTrace();
+        } finally {
+            tokenEntries.add(TokenEntry.getEOF());
         }
     }",Buggy,"CPD: Fix error handling for lexical errors

* TokenMgrError must not be caught by the tokenizer. This is handled
  by CPD itself
* The token managers need to know the filename for proper error messages
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""CPD: Fix error handling for lexical errors"" explicitly states that the commit addresses error handling, specifically for lexical errors encountered during the tokenization process in CPD (Copy/Paste Detector). The message highlights two key changes:

1.  `TokenMgrError must not be caught by the tokenizer. This is handled by CPD itself`: This suggests that the previous code incorrectly caught `TokenMgrError` within the tokenizer, and the fix moves this error handling to a higher level (CPD).
2.  `The token managers need to know the filename for proper error messages`: This indicates that the token manager was missing filename information, leading to potentially unhelpful error messages.

**Git Diff Analysis:**

The diff shows modifications to the `tokenize` method. Let's break down the changes:

1.  `ObjectiveCTokenManager tokenManager = new ObjectiveCTokenManager(reader);`: Creates an instance of `ObjectiveCTokenManager`.
2.  `tokenManager.setFileName(sourceCode.getFileName());`:  Sets the filename on the `tokenManager` instance. This directly addresses the second point in the commit message.
3.  `final TokenFilter tokenFilter = new JavaCCTokenFilter(tokenManager);`: Passes the token manager to the filter.
4.  `System.err.println(""Added "" + sourceCode.getFileName());` and `err.printStackTrace(); System.err.println(""Skipping "" + sourceCode.getFileName() + "" due to parse error"");`: These lines related to printing to `System.err` and printing the stack trace of the error and skipping the file have been removed.
5.  The `catch (TokenMgrError err)` block has been removed. This directly addresses the first point in the commit message, indicating that `TokenMgrError` is no longer caught within the `tokenize` method.
6.  `finally { tokenEntries.add(TokenEntry.getEOF()); }`: The `tokenEntries.add(TokenEntry.getEOF());` is now in a `finally` block. This ensures that the EOF token is always added, even if exceptions occur.

**Reasoning:**

The commit message and the code changes are highly aligned. The diff directly implements the changes described in the commit message: removing the `TokenMgrError` catch block and setting the filename on the token manager. The removal of the `catch` block and the addition of a `finally` block to ensure `EOF` is always added strongly suggest that the previous code had a bug in its error handling logic. Specifically, the tokenizer was likely swallowing `TokenMgrError` exceptions, preventing them from being handled correctly at a higher level. Also, the missing filename in the token manager would have resulted in less informative error messages.

**Conclusion:**

**Buggy**
"
pmd,2220.json,9075cb005c37782d5248485d8628d90dff4a238f,"@@ -1,10 +1,14 @@
     public List<V> valueFrom(String valueString) throws IllegalArgumentException {
+        if (StringUtil.isEmpty(valueString)) {
+            return Collections.emptyList();
+        }
+
         String[] strValues = valueString.split(Pattern.quote("""" + multiValueDelimiter()));
 
         List<V> values = new ArrayList<>(strValues.length);
-        for (int i = 0; i < strValues.length; i++) {
-            values.add(createFrom(strValues[i]));
+        for (String strValue : strValues) {
+            values.add(createFrom(strValue));
         }
 
         return values;
     }",Buggy,"Fix bug with empty value strings
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix bug with empty value strings"" clearly states that the commit is intended to resolve a bug related to how the code handles empty value strings.

**Code Diff Analysis:**

The code diff introduces a check at the beginning of the `valueFrom` method:

```java
if (StringUtil.isEmpty(valueString)) {
    return Collections.emptyList();
}
```

This code block explicitly handles the case where the input `valueString` is empty.  If the input string is empty, the method now returns an empty list, preventing further processing.  Without this check, the subsequent `split` operation on an empty string might have led to unexpected behavior (e.g., an array with a single empty string element) or even an exception, depending on the implementation of `StringUtil.isEmpty` and the behavior of `split` with an empty input.

The original code iterated using an index `i`, which is replaced by a for-each loop. This change is not directly related to the bug fix, but it improves readability.

**Reasoning:**

The commit message and the code diff are highly aligned. The code change directly addresses the issue described in the commit message: handling empty value strings. The added `if` condition prevents the code from proceeding with potentially problematic operations when the input string is empty, which strongly suggests a bug fix. The previous code likely had undefined or incorrect behavior when `valueString` was empty, and the new code provides a defined and reasonable outcome (returning an empty list).

**Conclusion:**

**Buggy**
"
pmd,4201.json,8ffaffc567168576ecfc1176fbfab03b5f9896b0,"@@ -1,20 +1,20 @@
 	void calculateLineNumbers(SourceCodePositioner positioner) {
 		if (!hasRealLoc()) {
 			return;
 		}
 
 		RealLoc loc = (RealLoc) node.getLoc();
 		int startOffset = loc.startIndex;
 		int endOffset = loc.endIndex;
+		// end column will be interpreted as inclusive, while endOffset/endIndex is exclusive
+		endOffset -= 1;
 
 		this.beginLine = positioner.lineNumberFromOffset(startOffset);
 		this.beginColumn = positioner.columnFromOffset(this.beginLine, startOffset);
 		this.endLine = positioner.lineNumberFromOffset(endOffset);
-		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset) - 1; // end
-																					// column
-																					// is
-																					// inclusive
+		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset);
+
 		if (this.endColumn < 0) {
 			this.endColumn = 0;
 		}
 	}",Buggy,"Fixes #1485 Analysis of some apex classes cause a stackoverflow error
Test file must use windows line endings
Fixes Up2Go/pmd#36
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixes #1485 Analysis of some apex classes cause a stackoverflow error. Test file must use windows line endings. Fixes Up2Go/pmd#36"" clearly states that the commit addresses a stack overflow error that occurred during the analysis of certain Apex classes. It also mentions a requirement for Windows line endings in a test file and fixes another issue. The primary focus is on resolving the stack overflow, which is a critical bug.

**Diff Analysis:**

The diff modifies the `calculateLineNumbers` method. The key change is:

*   `endOffset -= 1;` is added before calculating `endLine` and `endColumn`.
*   `this.endColumn = positioner.columnFromOffset(this.endLine, endOffset) - 1;` is changed to `this.endColumn = positioner.columnFromOffset(this.endLine, endOffset);`

The original code subtracted 1 from the column offset. The new code removes this subtraction. The comment `// end column will be interpreted as inclusive, while endOffset/endIndex is exclusive` explains the reasoning behind this change. The `endOffset` is exclusive, and the column calculation should be inclusive.

**Reasoning:**

The commit message states that a stack overflow error was occurring. While the diff itself doesn't directly show how the stack overflow was *caused*, the modification to the `endColumn` calculation suggests a potential off-by-one error in how line and column numbers were being determined. Incorrect line/column calculations, especially when used in recursive or iterative processes, *can* lead to stack overflow errors in certain scenarios (e.g., infinite loops or unbounded recursion due to incorrect position information). The change ensures that the `endColumn` is calculated inclusively, which is likely to resolve the stack overflow issue.

The change also includes a check to ensure `endColumn` is not negative, which is a defensive programming practice that can prevent further issues.

Given the commit message explicitly stating a bug fix for a stack overflow and the code changes related to line/column number calculations, it's highly probable that this commit addresses a bug.

**Conclusion:**

**Buggy**
"
pmd,7241.json,733c871b9690e787c9137aceb34f4338e2617533,"@@ -1,3 +1,3 @@
     public boolean isAnonymousClass() {
-        return jjtGetParent().hasDescendantOfType(ASTClassOrInterfaceBody.class);
+        return jjtGetParent().getFirstChildOfType(ASTClassOrInterfaceBody.class) != null;
     }",Buggy,"Fix nested anonymous class bug with type resolution
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix nested anonymous class bug with type resolution"" clearly states that the commit addresses a bug related to nested anonymous classes and their type resolution. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `isAnonymousClass()` method.  The original implementation used `hasDescendantOfType(ASTClassOrInterfaceBody.class)` to check if the parent node has a descendant of type `ASTClassOrInterfaceBody`. The modified implementation uses `getFirstChildOfType(ASTClassOrInterfaceBody.class) != null`.

**Reasoning:**

The change from `hasDescendantOfType` to `getFirstChildOfType` suggests that the original implementation was incorrectly identifying anonymous classes in certain nested scenarios.  `hasDescendantOfType` would return true if *any* descendant was of the specified type, even if it wasn't the direct parent's child representing the anonymous class's body.  `getFirstChildOfType` is more specific, checking only the immediate children.

The commit message and the code change align well. The original code likely had a bug where it incorrectly identified non-anonymous classes as anonymous due to the broad search scope of `hasDescendantOfType`. The fix narrows the scope to only consider the direct children, which is a more accurate way to determine if the current node is part of an anonymous class.

**Conclusion:**

**Buggy**
"
hbase,30032.json,b635414e8337be7bc14ca6ae605749c35569b4f7,"@@ -1,31 +1,36 @@
   private Result regroupResults(final Result result) throws IOException {
     partialResultsRow = result.getRow();
     partialResults.add(result);
     partialResultsCellSizes += result.size();
     if (scan.getBatch() > 0 && partialResultsCellSizes >= scan.getBatch()) {
       Cell[] cells = new Cell[scan.getBatch()];
       int count = 0;
       boolean stale = false;
       while (count < scan.getBatch()) {
         Result res = partialResults.poll();
         stale = stale || res.isStale();
         if (res.size() + count <= scan.getBatch()) {
           System.arraycopy(res.rawCells(), 0, cells, count, res.size());
           count += res.size();
         } else {
           int len = scan.getBatch() - count;
           System.arraycopy(res.rawCells(), 0, cells, count, len);
           Cell[] remainingCells = new Cell[res.size() - len];
           System.arraycopy(res.rawCells(), len, remainingCells, 0, res.size() - len);
           Result remainingRes = Result.create(remainingCells, res.getExists(), res.isStale(),
               res.mayHaveMoreCellsInRow());
           partialResults.addFirst(remainingRes);
           count = scan.getBatch();
         }
       }
       partialResultsCellSizes -= scan.getBatch();
+      if (partialResultsCellSizes == 0) {
+        // We have nothing in partialResults, clear the flags to prevent returning empty Result
+        // when next result belongs to the next row.
+        clearPartialResults();
+      }
       return Result.create(cells, null, stale,
           partialResultsCellSizes > 0 || result.mayHaveMoreCellsInRow());
     }
     return null;
   }",Buggy,"HBASE-15484 Correct the semantic of batch and partial - amend to fix bug and revise the JavaDoc for related APIs.
",Buggy,"The commit message indicates a bug fix related to the `batch` and `partial` functionalities, along with JavaDoc revisions. The code diff focuses on the `regroupResults` method, which seems to handle partial results based on the `scan.getBatch()` size.

The core change lies within the `if (partialResultsCellSizes == 0)` block. When `partialResultsCellSizes` becomes zero, the code now calls `clearPartialResults()`. This suggests a scenario where, without this fix, an empty `Result` might be returned incorrectly when the next result belongs to a new row. This indicates a bug where the state related to partial results wasn't being cleared properly, leading to incorrect behavior.

Therefore, the code change addresses a bug related to how partial results are handled, particularly when the accumulated cell sizes match the batch size, and the logic needs to reset for the next row.

**Buggy**"
hbase,10174.json,537a3caccd22e069e8b026a4ba7da419fdb68324,"@@ -1,53 +1,53 @@
   static Status splitLog(String name, CancelableProgressable p, Configuration conf,
       RegionServerServices server, LastSequenceId sequenceIdChecker, WALFactory factory) {
     Path walDir;
     FileSystem fs;
     try {
       walDir = CommonFSUtils.getWALRootDir(conf);
       fs = walDir.getFileSystem(conf);
     } catch (IOException e) {
       LOG.warn(""Resigning, could not find root dir or fs"", e);
       return Status.RESIGNED;
     }
     try {
       if (!processSyncReplicationWAL(name, conf, server, fs, walDir)) {
         return Status.DONE;
       }
     } catch (IOException e) {
       LOG.warn(""failed to process sync replication wal {}"", name, e);
       return Status.RESIGNED;
     }
     // TODO have to correctly figure out when log splitting has been
     // interrupted or has encountered a transient error and when it has
     // encountered a bad non-retry-able persistent error.
     try {
       SplitLogWorkerCoordination splitLogWorkerCoordination =
           server.getCoordinatedStateManager() == null ? null
               : server.getCoordinatedStateManager().getSplitLogWorkerCoordination();
       if (!WALSplitter.splitLogFile(walDir, fs.getFileStatus(new Path(walDir, name)), fs, conf, p,
         sequenceIdChecker, splitLogWorkerCoordination, factory, server)) {
         return Status.PREEMPTED;
       }
     } catch (InterruptedIOException iioe) {
-      LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, iioe);
+      LOG.warn(""Resigning, interrupted splitting WAL {}"", name, iioe);
       return Status.RESIGNED;
     } catch (IOException e) {
       if (e instanceof FileNotFoundException) {
         // A wal file may not exist anymore. Nothing can be recovered so move on
-        LOG.warn(""Done, WAL {} does not exist anymore"", filename, e);
+        LOG.warn(""Done, WAL {} does not exist anymore"", name, e);
         return Status.DONE;
       }
       Throwable cause = e.getCause();
       if (e instanceof RetriesExhaustedException && (cause instanceof NotServingRegionException
           || cause instanceof ConnectException || cause instanceof SocketTimeoutException)) {
-        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", name, e);
         return Status.RESIGNED;
       } else if (cause instanceof InterruptedException) {
-        LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, interrupted splitting WAL {}"", name, e);
         return Status.RESIGNED;
       }
-      LOG.warn(""Error splitting WAL {}"", filename, e);
+      LOG.warn(""Error splitting WAL {}"", name, e);
       return Status.ERR;
     }
     return Status.DONE;
   }",Buggy," HBASE-24574 Procedure V2 - Distributed WAL Splitting => LOGGING (#1912)
 Addendum 2 fix compile error.
",Buggy,"The commit message indicates a fix for a compile error and relates to logging improvements within the Procedure V2 distributed WAL splitting process. The diff focuses on the `splitLog` method, specifically addressing logging within the catch blocks. The filename variable is replaced with the name variable in the log messages. This change suggests a correction to ensure the correct WAL name is logged in case of exceptions or interruptions during the splitting process. This correction aligns with the commit message's mention of logging improvements and indicates a bug fix related to incorrect logging.

**Buggy**"
hbase,36767.json,cb85bf2e0aa48700bf635183302bdb28a36cb635,"@@ -1,21 +1,24 @@
   public void doFilter(ServletRequest req, ServletResponse rsp,
       FilterChain chain) throws IOException, ServletException {
     HttpServletRequest request = (HttpServletRequest)req;
     HttpServletResponse response = (HttpServletResponse)rsp;
     String contentEncoding = request.getHeader(""content-encoding"");
     String acceptEncoding = request.getHeader(""accept-encoding"");
     String contentType = request.getHeader(""content-type"");
     if ((contentEncoding != null) &&
         (contentEncoding.toLowerCase().indexOf(""gzip"") > -1)) {
       request = new GZIPRequestWrapper(request);
     }
     if (((acceptEncoding != null) &&
           (acceptEncoding.toLowerCase().indexOf(""gzip"") > -1)) ||
         ((contentType != null) && mimeTypes.contains(contentType))) {
       response = new GZIPResponseWrapper(response);
     }
     chain.doFilter(request, response);
-    if ((response instanceof GZIPResponseWrapper)) {
-      ((GZIPResponseStream)response.getOutputStream()).finish();
+    if (response instanceof GZIPResponseWrapper) {
+      OutputStream os = response.getOutputStream();
+      if (os instanceof GZIPResponseStream) {
+        ((GZIPResponseStream)os).finish();
+      }
     }
   }",Buggy,"HBASE-3275 [rest] No gzip/deflat content encoding support; fix error handling in GzipFilter

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1082792 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""HBASE-3275 [rest] No gzip/deflat content encoding support; fix error handling in GzipFilter"" indicates two main changes: adding gzip/deflate content encoding support and fixing error handling in the GzipFilter.

The code diff focuses on the GzipFilter. The key change is within the `if (response instanceof GZIPResponseWrapper)` block.  Instead of directly casting the response's output stream to `GZIPResponseStream` and calling `finish()`, it now checks if the output stream is an instance of `GZIPResponseStream` before casting and calling `finish()`. This change addresses a potential `ClassCastException` if the output stream is not a `GZIPResponseStream`, which is an error handling improvement.

The commit message mentions ""fix error handling in GzipFilter"", and the code change directly addresses a potential `ClassCastException`, indicating a bug fix related to error handling.

Therefore, the changes indicate a bug fix.
**Buggy**
"
hbase,3377.json,65bcf55892efefb72b41fcf6c9974688578b5021,"@@ -1,11 +1,11 @@
   private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,
     final String value) {
     if (clazz == null || value == null) {
       return null;
     }
     try {
-      return T.valueOf(clazz, value);
+      return Enum.valueOf(clazz, value);
     } catch (IllegalArgumentException e) {
       return null;
     }
   }",Buggy,"HBASE-23653 Expose content of meta table in web ui; addendum (#1061)

Fix error prone problem

Signed-off-by: Nick Dimiduk <ndimiduk@apache.org>
Signed-off-by: Viraj Jasani <vjasani@apache.org>
Signed-off-by: stack <stack@apache.org>",Buggy,"The commit message indicates an addendum to HBASE-23653, focusing on fixing an ""error prone problem."" The code change involves replacing `T.valueOf(clazz, value)` with `Enum.valueOf(clazz, value)` within the `tryValueOf` method. This adjustment likely addresses a potential issue where the original code might not correctly handle enum value lookups in certain scenarios, possibly related to type inference or class loading. The `IllegalArgumentException` catch block remains, suggesting the code still gracefully handles cases where the provided string doesn't match any enum constant. This change directly relates to correcting a potential error in enum handling.

**Buggy**"
hbase,8735.json,8c4b09dfbaf53fd770fe3963df6095fc690f2ef5,"@@ -1,33 +1,34 @@
     private void _readMvccVersion(int offsetFromPos) {
       // This is Bytes#bytesToVint inlined so can save a few instructions in this hot method; i.e.
       // previous if one-byte vint, we'd redo the vint call to find int size.
       // Also the method is kept small so can be inlined.
       byte firstByte = blockBuffer.getByteAfterPosition(offsetFromPos);
       int len = WritableUtils.decodeVIntSize(firstByte);
       if (len == 1) {
         this.currMemstoreTS = firstByte;
       } else {
         int remaining = len -1;
         long i = 0;
         offsetFromPos++;
         if (remaining >= Bytes.SIZEOF_INT) {
-          i = blockBuffer.getIntAfterPosition(offsetFromPos);
+          // The int read has to be converted to unsigned long so the & op
+          i = (blockBuffer.getIntAfterPosition(offsetFromPos) & 0x00000000ffffffffL);
           remaining -= Bytes.SIZEOF_INT;
           offsetFromPos += Bytes.SIZEOF_INT;
         }
         if (remaining >= Bytes.SIZEOF_SHORT) {
           short s = blockBuffer.getShortAfterPosition(offsetFromPos);
           i = i << 16;
           i = i | (s & 0xFFFF);
           remaining -= Bytes.SIZEOF_SHORT;
           offsetFromPos += Bytes.SIZEOF_SHORT;
         }
         for (int idx = 0; idx < remaining; idx++) {
           byte b = blockBuffer.getByteAfterPosition(offsetFromPos + idx);
           i = i << 8;
           i = i | (b & 0xFF);
         }
         currMemstoreTS = (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
       }
       this.currMemstoreTSLen = len;
     }",Buggy,"HBASE-16624 Fix MVCC DeSerialization bug in the HFileScannerImpl

Change-Id: Ia970619ac7369d24ed432e827319dfdca16143c2

Signed-off-by: stack <stack@apache.org>
",Buggy,"The commit message indicates a fix for an MVCC deserialization bug in `HFileScannerImpl`. The code diff modifies the `_readMvccVersion` method, specifically addressing how an integer is read from the `blockBuffer` and converted to an unsigned long. The change `i = (blockBuffer.getIntAfterPosition(offsetFromPos) & 0x00000000ffffffffL);` suggests that the original code might have been incorrectly interpreting the integer as a signed value, leading to incorrect MVCC timestamp values. The `& 0x00000000ffffffffL` operation ensures that the integer is treated as an unsigned long, which is crucial for correct MVCC timestamp deserialization. This directly relates to the bug described in the commit message.

**Buggy**"
hbase,28257.json,78d532e5f344edda04fb9ce44bef9cd79e0d1935,"@@ -1,14 +1,14 @@
-  public void validatePut(final Put put) throws IllegalArgumentException{
+  public static void validatePut(Put put, int maxKeyValueSize) throws IllegalArgumentException {
     if (put.isEmpty()) {
       throw new IllegalArgumentException(""No columns to insert"");
     }
     if (maxKeyValueSize > 0) {
       for (List<Cell> list : put.getFamilyCellMap().values()) {
         for (Cell cell : list) {
           if (KeyValueUtil.length(cell) > maxKeyValueSize) {
             throw new IllegalArgumentException(""KeyValue size too large"");
           }
         }
       }
     }
   }",Buggy,"HBASE-12086 Fix bug of HTableMultipliexer

Signed-off-by: Elliott Clark <eclark@apache.org>
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-12086 Fix bug of HTableMultipliexer"" explicitly states that the commit is intended to fix a bug within the `HTableMultipliexer`. This is a strong indicator that the changes are bug-related.

**Code Diff Analysis:**

The code diff shows a change in the `validatePut` method. Specifically:

1.  The method signature has been modified to include `static` and take an additional argument `int maxKeyValueSize`.

2.  The code within the method checks if `maxKeyValueSize` is greater than 0. If it is, it iterates through the cells in the `Put` object and checks if the size of any `KeyValue` exceeds `maxKeyValueSize`. If it does, an `IllegalArgumentException` is thrown.

**Reasoning:**

The addition of the `maxKeyValueSize` parameter and the size validation logic suggests that there was a potential issue where large `KeyValue` objects could be inserted, potentially causing problems in `HTableMultipliexer`. By adding this validation, the code now prevents excessively large `KeyValue` objects from being inserted, which could be a fix for a bug related to data size limits or buffer overflows. The fact that the method is now static also indicates a change in how it is intended to be used, potentially to address a bug related to object state or dependencies.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix related to handling large `KeyValue` objects in the `HTableMultipliexer`.

**Buggy**"
hbase,3341.json,05378cbf69957b76e8559185ea20235242a8b2e6,"@@ -1,5 +1,5 @@
       protected void encode(ChannelHandlerContext channelHandlerContext,
                             ClusterStatus clusterStatus, List<Object> objects) {
-        ClusterStatusProtos.ClusterStatus csp = clusterStatus.convert();
+        ClusterStatusProtos.ClusterStatus csp = ProtobufUtil.convert(clusterStatus);
         objects.add(new DatagramPacket(Unpooled.wrappedBuffer(csp.toByteArray()), isa));
       }",Buggy,"HBASE-15609 Addendum fix compilation error
",Buggy,"The commit message ""HBASE-15609 Addendum fix compilation error"" indicates that the commit is intended to fix a compilation error.

The code diff shows a change in the `encode` method where `clusterStatus.convert()` is replaced with `ProtobufUtil.convert(clusterStatus)`. This suggests that the original code `clusterStatus.convert()` either did not exist or was not accessible in the given context, leading to a compilation error. The change replaces it with a call to `ProtobufUtil.convert(clusterStatus)`, which is likely a static utility method that performs the same conversion and is accessible, thus resolving the compilation error.

Since the commit message explicitly states that the change fixes a compilation error, and the code diff confirms a change that addresses a compilation issue, it is highly likely that this commit is a bug fix.

**Buggy**"
hbase,1117.json,8c74d177f68bbd5412cef96dc33f16ba33ff7875,"@@ -1,43 +1,43 @@
   public void refreshSources(String peerId) throws IOException {
     String terminateMessage = ""Peer "" + peerId +
       "" state or config changed. Will close the previous replication source and open a new one"";
     ReplicationPeer peer = replicationPeers.getPeer(peerId);
     ReplicationSourceInterface src = createSource(peerId, peer);
     // synchronized on latestPaths to avoid missing the new log
     synchronized (this.latestPaths) {
       ReplicationSourceInterface toRemove = this.sources.put(peerId, src);
       if (toRemove != null) {
         LOG.info(""Terminate replication source for "" + toRemove.getPeerId());
         toRemove.terminate(terminateMessage);
       }
       for (SortedSet<String> walsByGroup : walsById.get(peerId).values()) {
         walsByGroup.forEach(wal -> src.enqueueLog(new Path(this.logDir, wal)));
       }
     }
     LOG.info(""Startup replication source for "" + src.getPeerId());
     src.startup();
 
     List<ReplicationSourceInterface> toStartup = new ArrayList<>();
     // synchronized on oldsources to avoid race with NodeFailoverWorker
     synchronized (this.oldsources) {
       List<String> previousQueueIds = new ArrayList<>();
       for (ReplicationSourceInterface oldSource : this.oldsources) {
         if (oldSource.getPeerId().equals(peerId)) {
           previousQueueIds.add(oldSource.getQueueId());
           oldSource.terminate(terminateMessage);
           this.oldsources.remove(oldSource);
         }
       }
       for (String queueId : previousQueueIds) {
         ReplicationSourceInterface replicationSource = createSource(queueId, peer);
         this.oldsources.add(replicationSource);
         for (SortedSet<String> walsByGroup : walsByIdRecoveredQueues.get(queueId).values()) {
           walsByGroup.forEach(wal -> src.enqueueLog(new Path(wal)));
         }
         toStartup.add(replicationSource);
       }
     }
-    for (ReplicationSourceInterface replicationSource : oldsources) {
+    for (ReplicationSourceInterface replicationSource : toStartup) {
       replicationSource.startup();
     }
   }",Buggy,"HBASE-20082 Fix findbugs errors only on master which are introduced by HBASE-19397
",Buggy,"The commit message indicates a fix for FindBugs errors introduced by a previous commit. The code diff shows a change in the loop that starts up replication sources. Specifically, the loop iterates over `toStartup` instead of `oldsources`. This suggests that the original code might have been incorrectly starting up replication sources, potentially leading to unexpected behavior or errors. The change ensures that only the newly created replication sources are started, aligning with the intended logic.

Therefore, the change addresses a bug related to the startup of replication sources.

**Buggy**"
hbase,5759.json,741d0a4511b9c397b2eb821aef49e858e217bf1e,"@@ -1,22 +1,22 @@
   public synchronized void moveTables(
       Set<TableName> tableNames, String groupName) throws IOException {
     if (groupName != null && !rsGroupMap.containsKey(groupName)) {
       throw new DoNotRetryIOException(""Group ""+groupName+"" does not exist or is a special group"");
     }
 
     Map<String,RSGroupInfo> newGroupMap = Maps.newHashMap(rsGroupMap);
     for(TableName tableName: tableNames) {
       if (tableMap.containsKey(tableName)) {
-        RSGroupInfo src = new RSGroupInfo(rsGroupMap.get(tableMap.get(tableName)));
+        RSGroupInfo src = new RSGroupInfo(newGroupMap.get(tableMap.get(tableName)));
         src.removeTable(tableName);
         newGroupMap.put(src.getName(), src);
       }
       if(groupName != null) {
         RSGroupInfo dst = new RSGroupInfo(newGroupMap.get(groupName));
         dst.addTable(tableName);
         newGroupMap.put(dst.getName(), dst);
       }
     }
 
     flushConfig(newGroupMap);
   }",Buggy,"HBASE-16430 Fix RegionServer Group's bug when moving multiple tables (Guangxu Cheng)
",Buggy,"The commit message indicates a bug fix related to moving multiple tables in RegionServer Groups. The code diff modifies the `moveTables` method. Specifically, the line `RSGroupInfo src = new RSGroupInfo(rsGroupMap.get(tableMap.get(tableName)));` is changed to `RSGroupInfo src = new RSGroupInfo(newGroupMap.get(tableMap.get(tableName)));`.

The original code used `rsGroupMap` to get the source RSGroupInfo. However, when moving multiple tables, the `rsGroupMap` is not updated within the loop. This means that if a table is moved from one group to another, subsequent tables that were originally in the same source group would still be associated with the old group in `rsGroupMap`. This is because `rsGroupMap` is the original map, not the updated one.

The corrected code uses `newGroupMap` instead, which is a copy of `rsGroupMap` that is updated within the loop. This ensures that the source RSGroupInfo is retrieved from the updated map, reflecting the changes made by previous iterations of the loop. This fixes the bug where moving multiple tables from the same source group would not correctly update the group information for subsequent tables.

Therefore, the change addresses a bug in the logic of moving multiple tables.

**Buggy**"
hbase,13091.json,07e93458fe69b7701ea2045926b4afe30d166cb5,"@@ -1,3 +1,4 @@
   public Cell forceCopyOfBigCellInto(Cell cell) {
-    throw new IllegalStateException(""This is an Immutable MemStoreLAB."");
+    MemStoreLAB mslab = this.mslabs.get(0);
+    return mslab.forceCopyOfBigCellInto(cell);
   }",Buggy,"HBASE-19930: Fixing the bug, in the rare case when there is a merge into CCM and the one of the old segments has a big cell allocated on-heap and it needs to be copied to the MSLAB
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-19930: Fixing the bug, in the rare case when there is a merge into CCM and the one of the old segments has a big cell allocated on-heap and it needs to be copied to the MSLAB"" clearly states that the commit is intended to fix a bug. The message describes a specific scenario where the bug occurs: during a merge into a CCM (likely referring to a Compacting Cell Memory) when an old segment contains a large cell allocated on-heap that needs to be copied to the MSLAB (MemStoreLAB).

**Git Diff Analysis:**

The diff shows a change within the `forceCopyOfBigCellInto` method.

*   **Original Code:** The original implementation throws an `IllegalStateException` with the message ""This is an Immutable MemStoreLAB."" This implies that the method was not intended to be called on an `ImmutableMemStoreLAB`.

*   **Modified Code:** The modified code retrieves the first `MemStoreLAB` from `this.mslabs` and delegates the `forceCopyOfBigCellInto` call to that `MemStoreLAB` instance.

**Reasoning:**

The original code's `IllegalStateException` suggests that there was a situation where `forceCopyOfBigCellInto` was incorrectly being called on an `ImmutableMemStoreLAB`. The modified code attempts to handle this situation by delegating the call to a `MemStoreLAB`. This delegation suggests that the `ImmutableMemStoreLAB` now needs to handle the case of copying big cells, which it previously did not. The commit message describes a specific scenario (merge into CCM with a big cell) that likely triggers this incorrect call. The code change addresses this scenario by delegating the operation to the underlying MSLAB. This resolves the `IllegalStateException` and allows the big cell to be copied.

**Conclusion:**

The commit message explicitly states that it's fixing a bug. The code change addresses a specific scenario described in the commit message, which involves an `IllegalStateException` and the need to copy a large cell. The modification resolves the exception and enables the cell copy. Therefore, the changes indicate a bug fix.

**Buggy**"
hbase,34108.json,c222e2b4862045d5ef7040103e1c50b6593dda20,"@@ -1,50 +1,50 @@
     public void map(LongWritable offset, Text value,
       Context context)
     throws IOException {
       byte[] lineBytes = value.getBytes();
 
       try {
         TsvParser.ParsedLine parsed = parser.parse(
             lineBytes, value.getLength());
         ImmutableBytesWritable rowKey =
           new ImmutableBytesWritable(lineBytes,
               parsed.getRowKeyOffset(),
               parsed.getRowKeyLength());
 
         Put put = new Put(rowKey.copyBytes());
         for (int i = 0; i < parsed.getColumnCount(); i++) {
           if (i == parser.getRowKeyColumnIndex()) continue;
           KeyValue kv = new KeyValue(
               lineBytes, parsed.getRowKeyOffset(), parsed.getRowKeyLength(),
               parser.getFamily(i), 0, parser.getFamily(i).length,
               parser.getQualifier(i), 0, parser.getQualifier(i).length,
               ts,
               KeyValue.Type.Put,
               lineBytes, parsed.getColumnOffset(i), parsed.getColumnLength(i));
           put.add(kv);
         }
         context.write(rowKey, put);
       } catch (BadTsvLineException badLine) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
               badLine.getMessage());
           badLineCount.increment(1);
           return;
         } else {
           throw new IOException(badLine);
         }
       } catch (IllegalArgumentException e) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
-              badLine.getMessage());
+              e.getMessage());
           badLineCount.increment(1);
           return;
         } else {
-          throw new IOException(badLine);
+          throw new IOException(e);
         }
       } catch (InterruptedException e) {
         e.printStackTrace();
       }
     }",Buggy,"HBASE-3711 - amend patch to fix compilation error

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1086747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error. The diff shows changes in the `map` function, specifically within the `catch` blocks for `IllegalArgumentException`.

In the original code, inside the `catch` block for `IllegalArgumentException`, the code was printing the message from `badLine` (which is a `BadTsvLineException`) instead of the message from the caught `IllegalArgumentException` (`e`). Also, it was throwing a new `IOException` with `badLine` instead of `e`.

The corrected code now prints the message from `e` and throws a new `IOException` with `e`, which aligns with catching an `IllegalArgumentException`. This correction addresses a logical error where the wrong exception's message was being printed and the wrong exception was being re-thrown. This is a bug fix.

**Buggy**"
ant,5957.json,79a591966485337a98ec87cfcda01218969b73da,"@@ -1,164 +1,164 @@
     public void execute() throws BuildException {
 
         if ( (sourceFileSets.size() == 0) && (sourceFileLists.size() == 0) ) { 
           throw new BuildException(""At least one <srcfileset> or <srcfilelist> element must be set"");
         }
 
         if ( (targetFileSets.size() == 0) && (targetFileLists.size() == 0) ) {
           throw new BuildException(""At least one <targetfileset> or <targetfilelist> element must be set"");
         }
 
         long now = (new Date()).getTime();
         /*
           If we're on Windows, we have to munge the time up to 2 secs to
           be able to check file modification times.
           (Windows has a max resolution of two secs for modification times)
         */
         if (Os.isFamily(""windows"")) {
             now += 2000;
         }
 
         //
         // Grab all the target files specified via filesets
         //
         Vector  allTargets         = new Vector();
         long oldestTargetTime = 0;
         File oldestTarget = null;
         Enumeration enumTargetSets = targetFileSets.elements();
         while (enumTargetSets.hasMoreElements()) {
                  
            FileSet targetFS          = (FileSet) enumTargetSets.nextElement();
            DirectoryScanner targetDS = targetFS.getDirectoryScanner(project);
            String[] targetFiles      = targetDS.getIncludedFiles();
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFS.getDir(project), targetFiles[i]);
               allTargets.addElement(dest);
 
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
 
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
 
         //
         // Grab all the target files specified via filelists
         //
         boolean upToDate            = true;
         Enumeration enumTargetLists = targetFileLists.elements();
         while (enumTargetLists.hasMoreElements()) {
                  
            FileList targetFL    = (FileList) enumTargetLists.nextElement();
            String[] targetFiles = targetFL.getFiles(project);
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFL.getDir(project), targetFiles[i]);
               if (!dest.exists()) {
                  log(targetFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                  upToDate = false;
                  continue;
               }
               else {
                  allTargets.addElement(dest);
               }
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
+
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
         if (oldestTarget != null) {
             log(oldestTarget + "" is oldest target file"", Project.MSG_VERBOSE);
         } else { 
             // no target files, then we cannot remove any target files and
             // skip the following tests right away
             upToDate = false;
         }
 
         //
         // Check targets vs source files specified via filelists
         //
         if (upToDate) {
            Enumeration enumSourceLists = sourceFileLists.elements();
            while (upToDate && enumSourceLists.hasMoreElements()) {
           
               FileList sourceFL         = (FileList) enumSourceLists.nextElement();
               String[] sourceFiles      = sourceFL.getFiles(project);
 
-              int i = 0;
-              do {
+              for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFL.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (!src.exists()) {
                     log(sourceFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                     upToDate = false;
                     break;
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
-              } while (upToDate && (++i < sourceFiles.length) );
+              }
            }
         }
 
         //
         // Check targets vs source files specified via filesets
         //
         if (upToDate) {
            Enumeration enumSourceSets = sourceFileSets.elements();
            while (upToDate && enumSourceSets.hasMoreElements()) {
           
               FileSet sourceFS          = (FileSet) enumSourceSets.nextElement();
               DirectoryScanner sourceDS = sourceFS.getDirectoryScanner(project);
               String[] sourceFiles      = sourceDS.getIncludedFiles();
 
               for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFS.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
               }
            }
         }
 
         if (!upToDate) {
            log(""Deleting all target files. "", Project.MSG_VERBOSE);
            for (Enumeration e = allTargets.elements(); e.hasMoreElements(); ) {
               File fileToRemove = (File)e.nextElement();
               log(""Deleting file "" + fileToRemove.getAbsolutePath(), Project.MSG_VERBOSE);
               fileToRemove.delete();
            }
         }
         
 
     } //-- execute",Buggy,"fix boundary checking problem similiar to the one fixed for bug report 4290

Actually this wouldn't throw an ArrayIndexOutOuBounds- but a
BuildException as FileList.getFiles barfs out on empty lists, but that
way it is more consistent.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271456 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a boundary checking problem, similar to a previous bug fix. It also mentions that the original code might not throw an `ArrayIndexOutOfBoundsException` but a `BuildException` due to `FileList.getFiles` behavior with empty lists. The change aims for more consistency.

The diff replaces a `do...while` loop with a `for` loop in the section that checks targets against source files specified via filelists. The original `do...while` loop incremented `i` and checked `upToDate` in the loop condition, which could potentially lead to an `ArrayIndexOutOfBoundsException` if `upToDate` became false after accessing `sourceFiles[i]` but before the loop condition check. The new `for` loop incorporates the `upToDate` check directly into the loop condition, preventing the out-of-bounds access.

The change addresses a potential issue where the loop might continue even after `upToDate` becomes false, potentially leading to an out-of-bounds access on the `sourceFiles` array. This indicates a bug fix.

**Buggy**"
ant,9265.json,12ae031068c8dc69dc744f1a4060aba054975801,"@@ -1,16 +1,18 @@
     public static IntrospectionHelper getHelper(Project p, Class c) {
-        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c);
-        if (ih == null) {
+        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c.getName());
+        // If a helper cannot be found, or if the helper is for another
+        // classloader, create a new IH
+        if (ih == null || ih.bean != c) {
             ih = new IntrospectionHelper(c);
             if (p != null) {
                 // #30162: do *not* cache this if there is no project, as we
                 // cannot guarantee that the cache will be cleared.
-                HELPERS.put(c, ih);
+                HELPERS.put(c.getName(), ih);
             }
         }
         if (p != null) {
             // Cleanup at end of project
             p.addBuildListener(ih);
         }
         return ih;
     }",Buggy,"Fix for OOME with <*ant*> and <typedef>
Bugzilla report 28283 and 33061

IH had a map of class->IH objects. The
class is the typedefed class and IH is the
attributes, elements etc of that class.
This works fine, except that the class is kept
until the build ends, this means that the classloader
for the class is also kept, a classloader contains
pointers to all the classes loaded by it - so a lot
of memory can be blocked.
When ant, or antcall is used and the called project
typedef the antcontrib, these will be new classloaders,
hence the memory being used up.

The fix is to use the name of the class, check if the IH
in the map is the same class, and if not replace that IH.




git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@465073 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a fix for an OutOfMemoryError (OOME) related to the caching of `IntrospectionHelper` objects in Ant. The issue arises from the fact that the `HELPERS` map used the class itself as the key, leading to classloader leaks when typedefs are used in ant or antcall tasks.

The code diff modifies the `getHelper` method to use the class name (`c.getName()`) as the key in the `HELPERS` map instead of the class object itself. Additionally, it checks if the existing `IntrospectionHelper` is associated with the same class. If not, it creates a new `IntrospectionHelper`. This addresses the classloader leak issue by allowing the old classloader and its associated classes to be garbage collected.

The commit message and the code diff are highly relevant. The change directly addresses the memory leak issue described in the commit message. The use of class name as key and the check for class equality are crucial for resolving the problem.

Therefore, the changes indicate a bug fix.

**Buggy**"
ant,9998.json,f40cbc60b34a952432e7abfb70181d0feabd4dd6,"@@ -1,13 +1,13 @@
     public String removeLeadingPath(File leading, File path) {
-        String l = normalize(leading.getAbsolutePath()).getAbsolutePath();
+        // if leading's path ends with a slash, it will be stripped by
+        // normalize - we always add one so we never think /foo was a
+        // parent directory of /foobar
+        String l = normalize(leading.getAbsolutePath()).getAbsolutePath()
+            + File.separator;
         String p = normalize(path.getAbsolutePath()).getAbsolutePath();
         if (p.startsWith(l)) {
-            String result = p.substring(l.length());
-            if (result.startsWith(File.separator)) {
-                result = result.substring(File.separator.length());
-            }
-            return result;
+            return p.substring(l.length());
         } else {
             return p;
         }
     }",Buggy,"Merge over a bug-fix needed to get jakarta-tomcat built by Gump.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272855 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Merge over a bug-fix needed to get jakarta-tomcat built by Gump"" clearly states that the commit is intended to fix a bug. The phrase ""bug-fix"" is a strong indicator. The mention of ""jakarta-tomcat built by Gump"" suggests the bug was preventing the successful build of jakarta-tomcat in the Gump continuous integration environment.

**Code Diff Analysis:**

The code diff modifies the `removeLeadingPath` method. Let's break down the changes:

1.  **Leading Path Normalization:**
    -   Original: `String l = normalize(leading.getAbsolutePath()).getAbsolutePath();`
    -   Modified: `String l = normalize(leading.getAbsolutePath()).getAbsolutePath() + File.separator;`

    The change adds `File.separator` to the end of the normalized leading path. The comment explains the reasoning: ""if leading's path ends with a slash, it will be stripped by normalize - we always add one so we never think /foo was a parent directory of /foobar"". This suggests the original code had a problem where it incorrectly identified a directory as a parent when it wasn't, specifically when the leading path ended with a slash.

2.  **Result Handling:**
    -   Original:
        ```java
        String result = p.substring(l.length());
        if (result.startsWith(File.separator)) {
            result = result.substring(File.separator.length());
        }
        return result;
        ```
    -   Modified:
        ```java
        return p.substring(l.length());
        ```

    The original code attempted to remove a leading separator from the result. However, since the leading path is now guaranteed to end with a separator, this check is no longer necessary, simplifying the code.

**Reasoning:**

The code changes address a specific edge case where the `removeLeadingPath` method could incorrectly identify parent directories due to the normalization process stripping trailing slashes. The added `File.separator` ensures consistent behavior and prevents this misidentification. The simplification of the result handling further supports the idea that the original code had a flaw that needed correction. The commit message explicitly states this is a bug fix.

**Conclusion:**

**Buggy**
"
ant,3709.json,d3f03ad754ffdd5d27796dc492ca2db38a7bf444,"@@ -1,95 +1,95 @@
         private void checkIncludePatterns() {
             Hashtable newroots = new Hashtable();
             // put in the newroots vector the include patterns without
             // wildcard tokens
             for (int icounter = 0; icounter < includes.length; icounter++) {
                 String newpattern =
                     SelectorUtils.rtrimWildcardTokens(includes[icounter]);
                 newroots.put(newpattern, includes[icounter]);
             }
             if (remotedir == null) {
                 try {
                     remotedir = ftp.printWorkingDirectory();
                 } catch (IOException e) {
                     throw new BuildException(""could not read current ftp directory"",
                         getLocation());
                 }
             }
             AntFTPFile baseFTPFile = new AntFTPRootFile(ftp, remotedir);
             rootPath = baseFTPFile.getAbsolutePath();
             // construct it
             if (newroots.containsKey("""")) {
                 // we are going to scan everything anyway
-                scandir(remotedir, """", true);
+                scandir(rootPath, """", true);
             } else {
                 // only scan directories that can include matched files or
                 // directories
                 Enumeration enum2 = newroots.keys();
 
                 while (enum2.hasMoreElements()) {
                     String currentelement = (String) enum2.nextElement();
                     String originalpattern = (String) newroots.get(currentelement);
                     AntFTPFile myfile = new AntFTPFile(baseFTPFile, currentelement);
                     boolean isOK = true;
                     boolean traversesSymlinks = false;
                     String path = null;
 
                     if (myfile.exists()) {
                         if (remoteSensitivityChecked
                             && remoteSystemCaseSensitive && isFollowSymlinks()) {
                             // cool case,
                             //we do not need to scan all the subdirs in the relative path
                             path = myfile.getFastRelativePath();
                         } else {
                             // may be on a case insensitive file system.  We want
                             // the results to show what's really on the disk, so
                             // we need to double check.
                             try {
                                 path = myfile.getRelativePath();
                                 traversesSymlinks = myfile.isTraverseSymlinks();
                             }  catch (IOException be) {
                                 throw new BuildException(be, getLocation());
                             } catch (BuildException be) {
                                 isOK = false;
 
                             }
                         }
                     } else {
                         isOK = false;
                     }
                     if (isOK) {
                         currentelement = path.replace(remoteFileSep.charAt(0), File.separatorChar);
                         if (!isFollowSymlinks()
                             && traversesSymlinks) {
                             continue;
                         }
 
                         if (myfile.isDirectory()) {
                             if (isIncluded(currentelement)
                                 && currentelement.length() > 0) {
                                 accountForIncludedDir(currentelement, myfile, true);
                             }  else {
                                 if (currentelement.length() > 0) {
                                     if (currentelement.charAt(currentelement
                                                               .length() - 1)
                                         != File.separatorChar) {
                                         currentelement =
                                             currentelement + File.separatorChar;
                                     }
                                 }
                                 scandir(myfile.getAbsolutePath(), currentelement, true);
                             }
                         } else {
                             if (isCaseSensitive
                                 && originalpattern.equals(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             } else if (!isCaseSensitive
                                        && originalpattern
                                        .equalsIgnoreCase(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             }
                         }
                     }
                 }
             }
         }",Buggy,"Merge from ANT_16_BRANCH
Fix problem with non absolute remote dirs
PR: 23833


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix problem with non absolute remote dirs"" suggests a bug fix related to how remote directories are handled, specifically when they are not absolute paths.

The code diff modifies the `checkIncludePatterns` method within an Ant FTP task. The key change is replacing `scandir(remotedir, """", true)` with `scandir(rootPath, """", true)`.

Here's the reasoning:

1.  **Problem Context:** The original code used `remotedir` directly in the `scandir` method when scanning everything. If `remotedir` was not an absolute path, this could lead to incorrect file scanning or errors.

2.  **Solution:** The `rootPath` variable is derived from `remotedir` using `baseFTPFile.getAbsolutePath()`. This ensures that `rootPath` always contains the absolute path of the remote directory. By using `rootPath` in `scandir`, the code guarantees that the scanning starts from the correct absolute location, resolving the issue with non-absolute remote directories.

3.  **Bug Fix Indication:** The change directly addresses the problem described in the commit message. It corrects a potential error scenario where non-absolute remote directories would cause incorrect behavior.

Therefore, the code change aligns with the commit message and indicates a bug fix.

**Conclusion: Buggy**"
ant,8803.json,bb975e67e154a84b5dd47b5a5b24f853758dc7a5,"@@ -1,15 +1,15 @@
     protected void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
-            if (nested instanceof DataType) {
+            if (nested != null) {
                 pushAndInvokeCircularReferenceCheck((DataType) nested, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",Buggy,"Fix the problem of instanceof test always return true.

This instanceof test will always return true because DataType is the superclass of variable nested's class ResourceComparator. The variable nested is not initialized, it would be better to do a null test rather than an instanceof test.
http://findbugs.sourceforge.net/bugDescriptions.html#BC_VACUOUS_INSTANCEOF
",Buggy,"The commit message indicates a fix for a problem where an `instanceof` test always returns true, leading to incorrect behavior. The explanation points out that `DataType` is a superclass of `nested`'s class, making the `instanceof` check redundant. The suggested fix is to replace the `instanceof` check with a null check.

The diff shows the change from `if (nested instanceof DataType)` to `if (nested != null)`. This change directly addresses the issue described in the commit message by replacing the problematic `instanceof` check with a null check. This prevents the code from proceeding under the incorrect assumption that `nested` is always a `DataType` instance, and handles the case where `nested` is null.

The commit message and the code change are highly relevant. The change clearly fixes the bug described in the commit message.

**Buggy**"
ant,3291.json,3549562783e517c3fbf00a6e914bc48d624847cc,"@@ -1,25 +1,25 @@
     public void execute() throws BuildException {
 
         // first off, make sure that we've got a from and to extension
         if (fromExtension == null || toExtension == null || srcDir == null) {
-            throw new BuildException(""srcDir, destDir, fromExtension and toExtension attributes must be set!"");
+            throw new BuildException(""srcDir, fromExtension and toExtension attributes must be set!"");
         }
 
         // scan source and dest dirs to build up rename list
         DirectoryScanner ds = getDirectoryScanner(srcDir);
 
         String[] files = ds.getIncludedFiles();
 
         Hashtable renameList = scanDir(srcDir, files);
 
         Enumeration e = renameList.keys();
         File fromFile = null;
         File toFile = null;
         while (e.hasMoreElements()) {
             fromFile = (File)e.nextElement();
             toFile = (File)renameList.get(fromFile);
             if (toFile.exists() && replace) toFile.delete();
             if (!fromFile.renameTo(toFile)) throw new BuildException(""Rename from: '"" + fromFile + ""' to '"" + toFile + ""' failed."");
         }
 
     }",Buggy,"fixed comments and error message
Submitted by: dIon Gillard <dion@multitask.com.au>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267640 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""fixed comments and error message"" suggests that the changes involve correcting comments and improving an error message. This directly points to a potential bug fix scenario, as error messages are often refined to provide more accurate or helpful information when an error condition is encountered.

**Git Diff Analysis:**

The diff shows a change in the `execute()` method of a class. Specifically, the error message thrown when `fromExtension`, `toExtension`, or `srcDir` is null has been modified. The original message was:

```java
throw new BuildException(""srcDir, destDir, fromExtension and toExtension attributes must be set!"");
```

The modified message is:

```java
throw new BuildException(""srcDir, fromExtension and toExtension attributes must be set!"");
```

The `destDir` attribute has been removed from the error message. This indicates that the original error message was incorrect or misleading, as it incorrectly stated that `destDir` was a required attribute.

**Reasoning:**

The commit message explicitly states that an error message was fixed. The diff confirms this by showing a modification to an error message. The removal of `destDir` from the error message suggests that it was previously incorrectly included, leading to a potentially confusing or misleading error message for users. This correction of an inaccurate error message qualifies as a bug fix.

**Conclusion:**

**Buggy**
"
ant,1040.json,cdf128015bad588566b90d913fba4b346460d82d,"@@ -1,45 +1,45 @@
     public void execute() throws BuildException {
         Method setProjectM = null;
         try {
             Class c = proxy.getClass();
             setProjectM = 
                 c.getMethod(""setProject"", new Class[] {Project.class});
             if (setProjectM != null) {
                 setProjectM.invoke(proxy, new Object[] {getProject()});
             }
         } catch (NoSuchMethodException e) {
             // ignore this if the class being used as a task does not have
             // a set project method.
         } catch (Exception ex) {
             log(""Error setting project in "" + proxy.getClass(), 
                 Project.MSG_ERR);
             throw new BuildException(ex);
         }
 
 
         Method executeM = null;
         try {
             Class c = proxy.getClass();
             executeM = c.getMethod(""execute"", new Class[0]);
             if (executeM == null) {
                 log(""No public execute() in "" + proxy.getClass(), 
                     Project.MSG_ERR);
                 throw new BuildException(""No public execute() in "" 
                     + proxy.getClass());
             }
             executeM.invoke(proxy, null);
             return; 
         } catch (java.lang.reflect.InvocationTargetException ie) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             Throwable t = ie.getTargetException();
             if (t instanceof BuildException) {
                 throw ((BuildException) t);
             } else {
                 throw new BuildException(t);
             }
         } catch (Exception ex) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             throw new BuildException(ex);
         }
 
     }",Buggy,"Fix for 20499:
When a proxied task throws a build exception, or other
exception, the taskadapter reports this at
error level. This is incorrect as the
intent of taskadapter is to transparently adapt
a task, the exception should be reported
at verbose level.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the taskadapter was incorrectly reporting exceptions at the error level, and the intent is to report them at the verbose level.

The code diff shows two changes:
1. `log(""Error in "" + proxy.getClass(), Project.MSG_ERR);` is changed to `log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);` within the `InvocationTargetException` catch block.
2. `log(""Error in "" + proxy.getClass(), Project.MSG_ERR);` is changed to `log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);` within the general `Exception` catch block.

These changes directly address the issue described in the commit message by reducing the log level from `MSG_ERR` to `MSG_VERBOSE`. This indicates a bug fix where the logging level was incorrect.

**Buggy**
"
ant,6876.json,320e41bc89d8ac60180cbea1c4110dea68cebc33,"@@ -1,3 +1,9 @@
-    public void setTag(String p) {
-	this.tag = p;
-    }

+    public void setTag(String p) { 
+        // Check if not real tag => set it to null 
+        if (p != null) { 
+            if (p.trim().equals("""")) 
+                p = null; 
+        } 
+
+        this.tag = p; 
+    }",Buggy,"Fixed problem when an empty tag was supplied.

Submitted by:	Jean-Noel Gadreau <jngadreau@activcard.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267670 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an issue that occurs when an empty tag is supplied. The code diff shows that the `setTag` method is modified to handle empty tags. Specifically, the code checks if the input tag `p` is not null. If it's not null, it trims the tag and checks if the trimmed tag is empty. If it is, the tag is set to null. This change directly addresses the problem described in the commit message. The code modification ensures that empty tags are treated as null, which likely resolves a bug or unexpected behavior when an empty tag was previously used.

**Buggy**"
ant,5580.json,c807cf91c6fa47ef199cce3ae19a9d0e74fb4b18,"@@ -1,28 +1,28 @@
     private int removeEmptyDirectories(File dir, boolean removeIfEmpty,
                                        Set preservedEmptyDirectories) {
         int removedCount = 0;
-        if (!preservedEmptyDirectories.contains(dir) && dir.isDirectory()) {
+        if (dir.isDirectory()) {
             File[] children = dir.listFiles();
             for (int i = 0; i < children.length; ++i) {
                 File file = children[i];
                 // Test here again to avoid method call for non-directories!
-                if (!preservedEmptyDirectories.contains(file)
-                    && file.isDirectory()) {
+                if (file.isDirectory()) {
                     removedCount +=
                         removeEmptyDirectories(file, true,
                                                preservedEmptyDirectories);
                 }
             }
             if (children.length > 0) {
                 // This directory may have become empty...
                 // We need to re-query its children list!
                 children = dir.listFiles();
             }
-            if (children.length < 1 && removeIfEmpty) {
+            if (children.length < 1 && removeIfEmpty
+                && !preservedEmptyDirectories.contains(dir)) {
                 log(""Removing empty directory: "" + dir, Project.MSG_DEBUG);
                 dir.delete();
                 ++removedCount;
             }
         }
         return removedCount;
     }",Buggy,"properly handle non-recursive excludes (breaks one other test because of the same slowscan bug that I'm going to fix later)

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@727978 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to handling non-recursive excludes and mentions a related ""slowscan bug."" The diff modifies the `removeEmptyDirectories` method.

The original code had a check `!preservedEmptyDirectories.contains(dir)` at the beginning of the method and inside the loop. The modified code removes this check at the beginning of the method and adds `&& !preservedEmptyDirectories.contains(dir)` to the condition before deleting the directory.

The change suggests that the original code might have incorrectly skipped removing empty directories in some cases when they should have been removed, or incorrectly attempted to remove directories that should have been preserved. The added condition `&& !preservedEmptyDirectories.contains(dir)` ensures that only directories not in the `preservedEmptyDirectories` set are deleted. This looks like a bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**
"
ant,244.json,99684cfd77a3851d098824e9eb871cd6c087934f,"@@ -1,79 +1,79 @@
     private byte[] createCentralFileHeader(ZipEntry ze, ByteBuffer name, long lfhOffset,
                                            boolean needsZip64Extra) throws IOException {
         byte[] extra = ze.getCentralDirectoryExtra();
 
         // file comment length
         String comm = ze.getComment();
         if (comm == null) {
             comm = """";
         }
 
         ByteBuffer commentB = getEntryEncoding(ze).encode(comm);
         final int nameLen = name.limit() - name.position();
         final int commentLen = commentB.limit() - commentB.position();
         int len= CFH_FILENAME_OFFSET + nameLen + extra.length + commentLen;
         byte[] buf = new byte[len];
 
         System.arraycopy(CFH_SIG,  0, buf, CFH_SIG_OFFSET, WORD);
 
         // version made by
         // CheckStyle:MagicNumber OFF
         putShort((ze.getPlatform() << 8) | (!hasUsedZip64 ? DATA_DESCRIPTOR_MIN_VERSION : ZIP64_MIN_VERSION),
                 buf, CFH_VERSION_MADE_BY_OFFSET);
 
         final int zipMethod = ze.getMethod();
         final boolean encodable = zipEncoding.canEncode(ze.getName());
         putShort(versionNeededToExtract(zipMethod, needsZip64Extra), buf, CFH_VERSION_NEEDED_OFFSET);
         getGeneralPurposeBits(zipMethod, !encodable && fallbackToUTF8).encode(buf, CFH_GPB_OFFSET);
 
         // compression method
         putShort(zipMethod, buf, CFH_METHOD_OFFSET);
 
 
         // last mod. time and date
         ZipUtil.toDosTime(calendarInstance, ze.getTime(), buf, CFH_TIME_OFFSET);
 
         // CRC
         // compressed length
         // uncompressed length
         putLong(ze.getCrc(), buf, CFH_CRC_OFFSET);
         if (ze.getCompressedSize() >= ZIP64_MAGIC
                 || ze.getSize() >= ZIP64_MAGIC) {
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_COMPRESSED_SIZE_OFFSET);
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_ORIGINAL_SIZE_OFFSET);
         } else {
             putLong(ze.getCompressedSize(), buf, CFH_COMPRESSED_SIZE_OFFSET);
             putLong(ze.getSize(), buf, CFH_ORIGINAL_SIZE_OFFSET);
         }
 
         putShort(nameLen, buf, CFH_FILENAME_LENGTH_OFFSET);
 
         // extra field length
         putShort(extra.length, buf, CFH_EXTRA_LENGTH_OFFSET);
 
         putShort(commentLen, buf, CFH_COMMENT_LENGTH_OFFSET);
 
         // disk number start
         System.arraycopy(ZERO, 0, buf, CFH_DISK_NUMBER_OFFSET, SHORT);
 
         // internal file attributes
         putShort(ze.getInternalAttributes(), buf, CFH_INTERNAL_ATTRIBUTES_OFFSET);
 
         // external file attributes
         putLong(ze.getExternalAttributes(), buf, CFH_EXTERNAL_ATTRIBUTES_OFFSET);
 
         // relative offset of LFH
         putLong(Math.min(lfhOffset, ZIP64_MAGIC), buf, CFH_LFH_OFFSET);
 
         // file name
         System.arraycopy(name.array(), name.arrayOffset(), buf, CFH_FILENAME_OFFSET, nameLen);
 
         int extraStart = CFH_FILENAME_OFFSET + nameLen;
         System.arraycopy(extra, 0, buf, extraStart, extra.length);
 
-        int commentStart = extraStart + commentLen;
+        int commentStart = extraStart + extra.length;
 
         // file comment
         System.arraycopy(commentB.array(), commentB.arrayOffset(), buf, commentStart, commentLen);
         return buf;
     }",Buggy,"yet another potential AIOBException in zip package

Bug found and fix provided by Earl Hood
",Buggy,"The commit message indicates a fix for a potential `ArrayIndexOutOfBoundsException` (AIOOBE) in the zip package, attributed to Earl Hood. The code diff focuses on the `createCentralFileHeader` method, which constructs the central directory file header for a zip entry.

The key change is in calculating `commentStart`.  The original code had `int commentStart = extraStart + commentLen;`, while the corrected code has `int commentStart = extraStart + extra.length;`.

The original calculation incorrectly used `commentLen` instead of `extra.length` when determining the starting position for copying the comment into the buffer. This would lead to an `ArrayIndexOutOfBoundsException` if `extra.length` was larger than `commentLen`, as the comment would be written beyond the allocated buffer size. The corrected code uses `extra.length`, ensuring the comment is written to the correct offset after the extra data.

This change directly addresses a potential AIOOBE, confirming the bug fix.

**Buggy**"
ant,6044.json,e2a62c3179e855f7e102af8701aff81d6ba3c7e8,"@@ -1,56 +1,55 @@
         public int read(char[] cbuf, int off, int len)
             throws IOException {
 
             int amountRead = 0;
             while (pos < sourceFiles.size() || (needAddSeparator)) {
                 if (needAddSeparator) {
                     cbuf[off] = eolString.charAt(lastPos++);
                     if (lastPos >= eolString.length()) {
                         lastPos = 0;
                         needAddSeparator = false;
                         pos++;
                     }
                     len--;
                     off++;
                     amountRead++;
                     if (len == 0) {
                         return amountRead;
                     }
                     continue;
                 }
-
                 int nRead = getReader().read(cbuf, off, len);
                 if (nRead == -1 || nRead == 0) {
                     reader.close();
                     reader = null;
                     if (fixLastLine && isMissingEndOfLine()) {
                         needAddSeparator = true;
                         lastPos = 0;
                     } else {
                         pos++;
                     }
                 } else {
                     if (fixLastLine) {
                         for (int i = nRead;
                                  i > (nRead - lastChars.length);
                                  --i) {
                             if (i < 0) {
                                 break;
                             }
-                            addLastChar(cbuf[off + i]);
+                            addLastChar(cbuf[off + i - 1]);
                         }
                     }
                     len -= nRead;
                     off += nRead;
                     amountRead += nRead;
                     if (len == 0) {
                         return amountRead;
                     }
                 }
             }
             if (amountRead == 0) {
                 return -1;
             } else {
                 return amountRead;
             }
         }",Buggy,"Fix off-by-one error in concat with fixlastline=""yes""
PR: 25464
Obtained from: wang liang


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275773 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an off-by-one error when `fixlastline` is set to ""yes"". The code diff shows a change within the `read` method, specifically in the loop that iterates to store the last characters read from the input stream. The original code `addLastChar(cbuf[off + i]);` has been changed to `addLastChar(cbuf[off + i - 1]);`. This suggests that the original code was accessing an element one position beyond the intended boundary, which is a classic off-by-one error. The corrected code now accesses the correct element, aligning with the commit message's intention to fix an off-by-one error.

**Buggy**"
ant,8293.json,feb03ce2d86cb52fbe8ce55b6b74dfe2536924b1,"@@ -1,5 +1,5 @@
-    protected Object clone() throws CloneNotSupportedException {
+    public Object clone() throws CloneNotSupportedException {
         Assertions that = (Assertions) super.clone();
         that.assertionList = (ArrayList) assertionList.clone();
         return that;
     }",Buggy,"added cloning support; bug #27218


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@276160 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""added cloning support; bug #27218"" strongly suggests that the changes are related to fixing a bug (bug #27218) and adding cloning functionality.

**Diff Analysis:**

The diff shows a change in the `clone()` method:

-   The visibility of the method is changed from `protected` to `public`.

**Reasoning:**

1.  **Cloning Support:** The commit message explicitly mentions ""added cloning support."" The code change makes the `clone()` method public, which is essential for enabling external classes to clone instances of the `Assertions` class. If the method was protected, only classes within the same package or subclasses could clone `Assertions` objects.
2.  **Bug Fix:** The commit message also mentions ""bug #27218"".  The addition of cloning support could be directly related to fixing a bug where cloning was either not possible or not working correctly. Making the `clone()` method public is a logical step in enabling cloning functionality, which could resolve a bug related to object duplication or state management.

**Conclusion:**

The changes align with the commit message, which indicates the addition of cloning support and a bug fix. Therefore, the changes likely address a bug related to the lack of cloning functionality or a problem with the existing cloning mechanism.

**Buggy**"
ant,4097.json,84d3e4a158b155664d401f0d8b57124504532b74,"@@ -1,3 +1,11 @@
     public void setGlib(String superGrammar) {
-        this.superGrammar = superGrammar;
+        String sg = null;
+        if (Os.isFamily(""dos"")) {
+            sg = superGrammar.replace('\\','/');
+        }
+        else
+        {
+            sg = superGrammar;
+        }
+        this.superGrammar = sg;
     }",Buggy,"All the tests in ANTLRTest.java involving a super-grammar file were failing on Windows.
This change fixes the problem.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274716 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for failing tests on Windows related to a ""super-grammar"" file. The diff shows a change in the `setGlib` method, specifically addressing file path handling. The code now replaces backslashes with forward slashes in the `superGrammar` path if the operating system is Windows (""dos"" family). This suggests that the original code was not handling Windows-style paths correctly, leading to test failures. This is a bug fix because the original code had a defect in how it processed file paths on Windows.

**Buggy**"
ant,8414.json,08413ada31e6a9e9dbed3073158d17123d5028e0,"@@ -1,4 +1,6 @@
     private boolean hasPatterns(PatternSet ps) {
-        return ps.getIncludePatterns(getProject()).length > 0
-            || ps.getExcludePatterns(getProject()).length > 0;
+        String[] includePatterns = ps.getIncludePatterns(getProject());
+        String[] excludePatterns = ps.getExcludePatterns(getProject());
+        return (includePatterns != null && includePatterns.length > 0)
+            || (includePatterns != null && excludePatterns.length > 0);
     }",Buggy,"Fix Bug 42397: NPE in <path><files refid>

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@540055 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix Bug 42397: NPE in <path><files refid>"" suggests that the commit addresses a NullPointerException. The code diff modifies the `hasPatterns` method by adding null checks for `includePatterns` before accessing its `length` property. This change directly relates to preventing a potential NullPointerException, which aligns with the commit message's intention to fix a bug.

Reasoning:
1. **Commit Message Analysis:** The commit message explicitly states that it fixes a NullPointerException (NPE).
2. **Code Diff Analysis:** The code diff introduces null checks for the `includePatterns` variable before accessing its `length` property. This is a common practice to prevent NullPointerExceptions.
3. **Relevance:** The code change directly addresses the issue described in the commit message, indicating a bug fix related to a potential NullPointerException.

Conclusion:
**Buggy**
"
ant,7366.json,2a5857c384ef5a9e02b4264be44bf67f3a584d57,"@@ -1,48 +1,48 @@
     public PlanarImage executeDrawOperation() {
         log(""\tCreating Rectangle w="" + width + "" h="" + height + "" arcw=""
             + arcwidth + "" arch="" + archeight);
         BufferedImage bi = new BufferedImage(width, height, BufferedImage.TYPE_4BYTE_ABGR_PRE);
 
         Graphics2D graphics = (Graphics2D) bi.getGraphics();
 
         if (!stroke.equals(""transparent"")) {
             BasicStroke bStroke = new BasicStroke(stroke_width);
             graphics.setColor(ColorMapper.getColorByName(stroke));
             graphics.setStroke(bStroke);
 
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.drawRoundRect(0, 0, width, height, arcwidth, archeight);
             } else {
                 graphics.drawRect(0, 0, width, height);
             }
         }
 
         if (!fill.equals(""transparent"")) {
             graphics.setColor(ColorMapper.getColorByName(fill));
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.fillRoundRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2),
                     arcwidth, archeight);
             } else {
                 graphics.fillRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2));
             }
         }
 
 
         final int size = instructions.size();
         for (int i = 0; i < size; i++) {
             ImageOperation instr = ((ImageOperation) instructions.elementAt(i));
             if (instr instanceof DrawOperation) {
                 PlanarImage img = ((DrawOperation) instr).executeDrawOperation();
                 graphics.drawImage(img.getAsBufferedImage(), null, 0, 0);
             } else if (instr instanceof TransformOperation) {
-                graphics = (Graphics2D) bi.getGraphics();
                 PlanarImage image
                     = ((TransformOperation) instr)
                     .executeTransformOperation(PlanarImage.wrapRenderedImage(bi));
                 bi = image.getAsBufferedImage();
+                graphics = (Graphics2D) bi.getGraphics();
             }
         }
         return PlanarImage.wrapRenderedImage(bi);
     }",Buggy,"port image type bug fixes from Java8 refactoring in master
",Buggy,"The commit message indicates bug fixes related to image types, specifically ported from Java8 refactoring. Let's analyze the diff.

The key change is the movement of the line `graphics = (Graphics2D) bi.getGraphics();` inside the `else if (instr instanceof TransformOperation)` block.  Previously, it was outside the `else if` block.

Before the change, the code obtained a new `Graphics2D` object *before* the `TransformOperation` was executed. After the transformation, the code was still using the *old* `Graphics2D` object. This would lead to the transformation not being reflected in the final image, which is a bug.

The fix is to get a *new* `Graphics2D` object *after* the transformation has been applied to the `BufferedImage`. This ensures that subsequent drawing operations are performed on the transformed image.

This change directly addresses a logical error in how the `Graphics2D` object was being handled after a transformation, which constitutes a bug fix.

**Buggy**"
ant,9982.json,869b123c0437855adde003d242e4e084e8b35731,"@@ -1,30 +1,31 @@
     public File normalize(final String path) {
         Stack s = new Stack();
         String[] dissect = dissect(path);
         s.push(dissect[0]);
 
         StringTokenizer tok = new StringTokenizer(dissect[1], File.separator);
         while (tok.hasMoreTokens()) {
             String thisToken = tok.nextToken();
             if (""."".equals(thisToken)) {
                 continue;
             } else if ("".."".equals(thisToken)) {
                 if (s.size() < 2) {
-                    throw new BuildException(""Cannot resolve path "" + path);
+                    // Cannot resolve it, so skip it.
+                    return new File(path);
                 }
                 s.pop();
             } else { // plain component
                 s.push(thisToken);
             }
         }
         StringBuffer sb = new StringBuffer();
         for (int i = 0; i < s.size(); i++) {
             if (i > 1) {
                 // not before the filesystem root and not after it, since root
                 // already contains one
                 sb.append(File.separatorChar);
             }
             sb.append(s.elementAt(i));
         }
         return new File(sb.toString());
     }",Buggy,"#40281: ""Cannot resolve path"" error thrown gratuitously.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@432379 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the code change addresses an issue where a ""Cannot resolve path"" error was being thrown unnecessarily. The diff shows a modification in the `normalize` method where, previously, if the path could not be resolved (due to too many "".."" components), a `BuildException` was thrown. Now, instead of throwing an exception, the method simply returns a `File` object representing the original, unresolved path. This change aligns with the commit message, suggesting that the exception was indeed being thrown in cases where it wasn't strictly necessary, and the new behavior avoids the unnecessary exception. This is a bug fix because the previous behavior was throwing an exception when it could have returned a valid (though unresolved) path.

**Buggy**"
ant,7113.json,2fbb2e62ab0d3d323a75c6baf00a2e675c7fd6f5,"@@ -1,46 +1,55 @@
     protected void slowScan() {
         synchronized (slowScanLock) {
             if (haveSlowResults) {
                 return;
             }
             if (slowScanning) {
                 while (slowScanning) {
                     try {
                         slowScanLock.wait();
                     } catch (InterruptedException e) {
                     }
                 }
                 return;
             }
             slowScanning = true;
         }
         try {
             synchronized (this) {
 
+                // set in/excludes to reasonable defaults if needed:
+                boolean nullIncludes = (includes == null);
+                includes = nullIncludes ? new String[] {""**""} : includes;
+                boolean nullExcludes = (excludes == null);
+                excludes = nullExcludes ? new String[0] : excludes;
+
                 String[] excl = new String[dirsExcluded.size()];
                 dirsExcluded.copyInto(excl);
         
                 String[] notIncl = new String[dirsNotIncluded.size()];
                 dirsNotIncluded.copyInto(notIncl);
         
                 for (int i = 0; i < excl.length; i++) {
                     if (!couldHoldIncluded(excl[i])) {
                         scandir(new File(basedir, excl[i]),
                                 excl[i] + File.separator, false);
                     }
                 }
                 for (int i = 0; i < notIncl.length; i++) {
                     if (!couldHoldIncluded(notIncl[i])) {
                         scandir(new File(basedir, notIncl[i]),
                                 notIncl[i] + File.separator, false);
                     }
                 }
+                clearCaches();
+                includes = nullIncludes ? null : includes;
+                excludes = nullExcludes ? null : excludes;
             }
         } finally {
             synchronized (slowScanLock) {
                 haveSlowResults = true;
                 slowScanning = false;
                 slowScanLock.notifyAll();
             }
         }
     }",Buggy,"Not 100% sure this fixes the entire bug, but it eliminates NPEs for me.
PR: 34722


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278219 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a potential fix for a bug that causes NullPointerExceptions (NPEs). The code diff reveals changes that initialize `includes` and `excludes` to default values if they are null before proceeding with the scan. After the scan, the code resets `includes` and `excludes` to null if they were initially null. This suggests the original code was susceptible to NPEs if `includes` or `excludes` were not properly initialized. The changes made are designed to prevent these NPEs. Therefore, the changes indicate a bug fix.

**Buggy**"
ant,9433.json,fef3ea39f8bd474add292bb6437df6cbd22e1ba7,"@@ -1,47 +1,49 @@
     private static void buildJrePackages() {
         jrePackages = new Vector<String>();
         switch(javaVersionNumber) {
             case VERSION_1_9:
             case VERSION_1_8:
             case VERSION_1_7:
+                jrePackages.addElement(""jdk"");
+                // fall through
             case VERSION_1_6:
             case VERSION_1_5:
                 //In Java1.5, the apache stuff moved.
                 jrePackages.addElement(""com.sun.org.apache"");
                 //fall through.
             case VERSION_1_4:
                 if (javaVersionNumber == VERSION_1_4) {
                     jrePackages.addElement(""org.apache.crimson"");
                     jrePackages.addElement(""org.apache.xalan"");
                     jrePackages.addElement(""org.apache.xml"");
                     jrePackages.addElement(""org.apache.xpath"");
                 }
                 jrePackages.addElement(""org.ietf.jgss"");
                 jrePackages.addElement(""org.w3c.dom"");
                 jrePackages.addElement(""org.xml.sax"");
                 // fall through
             case VERSION_1_3:
                 jrePackages.addElement(""org.omg"");
                 jrePackages.addElement(""com.sun.corba"");
                 jrePackages.addElement(""com.sun.jndi"");
                 jrePackages.addElement(""com.sun.media"");
                 jrePackages.addElement(""com.sun.naming"");
                 jrePackages.addElement(""com.sun.org.omg"");
                 jrePackages.addElement(""com.sun.rmi"");
                 jrePackages.addElement(""sunw.io"");
                 jrePackages.addElement(""sunw.util"");
                 // fall through
             case VERSION_1_2:
                 jrePackages.addElement(""com.sun.java"");
                 jrePackages.addElement(""com.sun.image"");
                 // are there any here that we forgot?
                 // fall through
             case VERSION_1_1:
             default:
                 //things like sun.reflection, sun.misc, sun.net
                 jrePackages.addElement(""sun"");
                 jrePackages.addElement(""java"");
                 jrePackages.addElement(""javax"");
                 break;
         }
     }",Buggy,"fix Bug 59556 - support ""jdk"" package for Java 7+

Patch based on Chris Hegarty (Oracle) work.
""jdk"" package has been introduced in JDK7 but is not known from Ant.
""jdk.net.Sockets"" has been chosen as test class because it is available in JDK7, JDK8 and JDK9.
",Buggy,"The commit message indicates that the change fixes a bug where the ""jdk"" package was not recognized by Ant for Java 7 and later versions. The diff adds ""jdk"" to the `jrePackages` list for Java versions 1.7, 1.8, and 1.9. This aligns with the commit message's intention to support the ""jdk"" package for Java 7+. The addition of the ""jdk"" package to the list of JRE packages addresses the bug where it was previously missing.

**Buggy**"
lucene-solr,36661.json,d815a3608b2742dad7ed0c0b6233fdea1653c285,"@@ -1,33 +1,33 @@
   private final void count(List<MatchingDocs> matchingDocs) throws IOException {
     IntsRef scratch  = new IntsRef();
     for(MatchingDocs hits : matchingDocs) {
       OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
       FixedBitSet bits = hits.bits;
     
       final int length = hits.bits.length();
       int doc = 0;
       while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
         ords.get(doc, scratch);
         for(int i=0;i<scratch.length;i++) {
-          ++counts[scratch.ints[i]];
+          counts[scratch.ints[scratch.offset+i]]++;
         }
         ++doc;
       }
     }
 
     // nocommit we could do this lazily instead:
 
     // Rollup any necessary dims:
     for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
       String dim = ent.getKey();
       FacetsConfig.DimConfig ft = ent.getValue();
       if (ft.hierarchical && ft.multiValued == false) {
         int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
         // It can be -1 if this field was declared in the
         // config but never indexed:
         if (dimRootOrd > 0) {
           counts[dimRootOrd] += rollup(children[dimRootOrd]);
         }
       }
     }
   }",Buggy,"LUCENE-5339: migrate some more tests; fix 'ignores IntsRef.offset bug' in TaxoFacetCounts; add FacetTestCase.getFacetCounts

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1543506 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to `IntsRef.offset` in `TaxoFacetCounts`. The code diff shows a change in the `count` method where `counts[scratch.ints[i]]` is updated to `counts[scratch.ints[scratch.offset+i]]`. This change correctly accounts for the offset within the `IntsRef`, which was previously ignored, leading to incorrect counts.

**Buggy**
"
lucene-solr,26469.json,4336de5e81d66e255abbface1bddf8c18e500a90,"@@ -1,16 +1,16 @@
   private void recordOffsetDiff(int inputLength, int outputLength) {
     if (inputLength == outputLength) {
       charCount += outputLength;
       return;
     }
     final int diff = inputLength - outputLength;
     final int cumuDiff = getLastCumulativeDiff();
     if (diff < 0) {
       for (int i = 1;  i <= -diff; ++i) {
         addOffCorrectMap(charCount + i, cumuDiff - i);
       }
     } else {
-      addOffCorrectMap(charCount + Math.min(1, outputLength), cumuDiff + diff);
+      addOffCorrectMap(charCount + outputLength, cumuDiff + diff);
     }
     charCount += outputLength;
   }",Buggy,"LUCENE-5547: fix bug in offset correction

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1580829 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5547: fix bug in offset correction"" clearly indicates a bug fix related to offset correction.

The code diff modifies the `recordOffsetDiff` method, specifically the line `addOffCorrectMap(charCount + Math.min(1, outputLength), cumuDiff + diff);` is changed to `addOffCorrectMap(charCount + outputLength, cumuDiff + diff);`.

The original code used `Math.min(1, outputLength)` which would effectively add an offset correction at `charCount + 1` if `outputLength` was greater than 1, otherwise it would add the offset correction at `charCount + outputLength`. The corrected code always adds the offset correction at `charCount + outputLength`. This suggests that the original logic was flawed and could lead to incorrect offset calculations, thus indicating a bug.

Therefore, the code change aligns with the commit message, indicating a bug fix.

**Buggy**"
lucene-solr,19683.json,748cb61e824feefe8a6ef0fb2735af183c4afd5a,"@@ -1,9 +1,9 @@
   public IndexOutput createOutput(String name, IOContext context)
       throws IOException {
     if (SEGMENTS_GEN.equals(name)) {
-      return NULL_WRITER;
+      return new NullIndexOutput();
     }
     HdfsFileWriter writer = new HdfsFileWriter(getFileSystem(), new Path(
         hdfsDirPath, name));
     return new HdfsIndexOutput(writer);
   }",Buggy,"SOLR-4916: Fix bugs & usage of NullIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1502167 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-4916: Fix bugs & usage of NullIndexOutput"" indicates that the commit addresses bug fixes and potentially improves the usage of `NullIndexOutput`.

The code diff shows a change in the `createOutput` method. Specifically, when the `name` is equal to `SEGMENTS_GEN`, the code now returns `new NullIndexOutput()` instead of `NULL_WRITER`.

The change from `NULL_WRITER` to `new NullIndexOutput()` suggests that `NULL_WRITER` might have been problematic or incorrectly used. Creating a new instance of `NullIndexOutput` could be a fix for a bug related to shared state or incorrect behavior of the original `NULL_WRITER`.

Therefore, the changes indicate a bug fix.

**Buggy**"
lucene-solr,50349.json,b8feb92271c62b3bc1c24e2e3610ecd1cf646eab,"@@ -1,54 +1,60 @@
-  public double surfaceDistance(final GeoPoint p1, final GeoPoint p2) {
-    final double latA = p1.getLatitude();
-    final double lonA = p1.getLongitude();
-    final double latB = p2.getLatitude();
-    final double lonB = p2.getLongitude();
+  public double surfaceDistance(final GeoPoint pt1, final GeoPoint pt2) {
+    final double L = pt2.getLongitude() - pt1.getLongitude();
+    final double U1 = Math.atan((1.0-flattening) * Math.tan(pt1.getLatitude()));
+    final double U2 = Math.atan((1.0-flattening) * Math.tan(pt2.getLatitude()));
 
-    final double L = lonB - lonA;
-    final double oF = 1.0 - this.flattening;
-    final double U1 = Math.atan(oF * Math.tan(latA));
-    final double U2 = Math.atan(oF * Math.tan(latB));
-    final double sU1 = Math.sin(U1);
-    final double cU1 = Math.cos(U1);
-    final double sU2 = Math.sin(U2);
-    final double cU2 = Math.cos(U2);
+    final double sinU1 = Math.sin(U1);
+    final double cosU1 = Math.cos(U1);
+    final double sinU2 = Math.sin(U2);
+    final double cosU2 = Math.cos(U2);
 
-    double sigma, sinSigma, cosSigma;
-    double cos2Alpha, cos2SigmaM;
-    
+    final double dCosU1CosU2 = cosU1 * cosU2;
+    final double dCosU1SinU2 = cosU1 * sinU2;
+
+    final double dSinU1SinU2 = sinU1 * sinU2;
+    final double dSinU1CosU2 = sinU1 * cosU2;
+
+
     double lambda = L;
-    double iters = 100;
+    double lambdaP = Math.PI * 2.0;
+    int iterLimit = 0;
+    double cosSqAlpha;
+    double sinSigma;
+    double cos2SigmaM;
+    double cosSigma;
+    double sigma;
+    double sinAlpha;
+    double C;
+    double sinLambda, cosLambda;
 
     do {
-      final double sinLambda = Math.sin(lambda);
-      final double cosLambda = Math.cos(lambda);
-      sinSigma = Math.sqrt((cU2 * sinLambda) * (cU2 * sinLambda) + (cU1 * sU2 - sU1 * cU2 * cosLambda)
-          * (cU1 * sU2 - sU1 * cU2 * cosLambda));
-      if (Math.abs(sinSigma) < Vector.MINIMUM_RESOLUTION)
+      sinLambda = Math.sin(lambda);
+      cosLambda = Math.cos(lambda);
+      sinSigma = Math.sqrt((cosU2*sinLambda) * (cosU2*sinLambda) +
+                                    (dCosU1SinU2 - dSinU1CosU2 * cosLambda) * (dCosU1SinU2 - dSinU1CosU2 * cosLambda));
+
+      if (sinSigma==0.0) {
         return 0.0;
-
-      cosSigma = sU1 * sU2 + cU1 * cU2 * cosLambda;
+      }
+      cosSigma = dSinU1SinU2 + dCosU1CosU2 * cosLambda;
       sigma = Math.atan2(sinSigma, cosSigma);
-      final double sinAlpha = cU1 * cU2 * sinLambda / sinSigma;
-      cos2Alpha = 1.0 - sinAlpha * sinAlpha;
-      cos2SigmaM = cosSigma - 2.0 * sU1 * sU2 / cos2Alpha;
+      sinAlpha = dCosU1CosU2 * sinLambda / sinSigma;
+      cosSqAlpha = 1.0 - sinAlpha * sinAlpha;
+      cos2SigmaM = cosSigma - 2.0 * dSinU1SinU2 / cosSqAlpha;
 
-      final double c = this.flattening * 0.625 * cos2Alpha * (4.0 + this.flattening * (4.0 - 3.0 * cos2Alpha));
-      final double lambdaP = lambda;
-      lambda = L + (1.0 - c) * this.flattening * sinAlpha * (sigma + c * sinSigma * (cos2SigmaM + c * cosSigma *
-          (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)));
-      if (Math.abs(lambda - lambdaP) < Vector.MINIMUM_RESOLUTION)
-        break;
-    } while (--iters > 0);
+      if (Double.isNaN(cos2SigmaM))
+        cos2SigmaM = 0.0;  // equatorial line: cosSqAlpha=0
+      C = flattening / 16.0 * cosSqAlpha * (4.0 + flattening * (4.0 - 3.0 * cosSqAlpha));
+      lambdaP = lambda;
+      lambda = L + (1.0 - C) * flattening * sinAlpha *
+        (sigma + C * sinSigma * (cos2SigmaM + C * cosSigma * (-1.0 + 2.0 * cos2SigmaM *cos2SigmaM)));
+    } while (Math.abs(lambda-lambdaP) > Vector.MINIMUM_RESOLUTION && ++iterLimit < 40);
 
-    if (iters == 0)
-      return 0.0;
+    final double uSq = cosSqAlpha * this.squareRatio;
+    final double A = 1.0 + uSq / 16384.0 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
+    final double B = uSq / 1024.0 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
+    final double deltaSigma = B * sinSigma * (cos2SigmaM + B / 4.0 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)-
+                                        B / 6.0 * cos2SigmaM * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
 
-    final double uSq = cos2Alpha * this.squareRatio;
-    final double A = 1.0 + uSq * 0.00006103515625 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
-    final double B = uSq * 0.0009765625 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
-    final double deltaSigma = B * sinSigma * (cos2SigmaM + B * 0.25 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM) - B * 0.16666666666666666666667 * cos2SigmaM
-            * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
-
-    return this.c * A * (sigma - deltaSigma);
+    return c * A * (sigma - deltaSigma);
   }",Buggy,"LUCENE-7139: fix bugs in geo3d's vincenty distance implementation
",Buggy,"The commit message indicates that the changes fix bugs in the geo3d's vincenty distance implementation. The diff shows a significant rewrite of the `surfaceDistance` method, which calculates the distance between two points on the Earth's surface using the Vincenty formula. The original code had a fixed iteration limit and could return 0.0 if the limit was reached. The updated code includes a more robust iterative approach with a different convergence check and a different iteration limit. The changes also include checks for edge cases like when `sinSigma` is 0.0, which could lead to division by zero errors. The variable names are also changed to be more descriptive. These changes suggest that the original implementation had accuracy or convergence issues, which are now addressed in the updated code.

**Buggy**"
lucene-solr,8422.json,dfdd1b7363afa9e56a47fec7b92960468edd7763,"@@ -1,37 +1,37 @@
   public List<ZkCoreNodeProps> getReplicaProps(String collection,
       String shardId, String thisNodeName, String coreName, String stateFilter) {
     CloudState cloudState = this.cloudState;
     if (cloudState == null) {
       return null;
     }
     Map<String,Slice> slices = cloudState.getSlices(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
           ""Could not find collection in zk: "" + collection + "" ""
               + cloudState.getCollections());
     }
     
     Slice replicas = slices.get(shardId);
     if (replicas == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST, ""Could not find shardId in zk: "" + shardId);
     }
     
     Map<String,ZkNodeProps> shardMap = replicas.getShards();
     List<ZkCoreNodeProps> nodes = new ArrayList<ZkCoreNodeProps>(shardMap.size());
     String filterNodeName = thisNodeName + ""_"" + coreName;
     for (Entry<String,ZkNodeProps> entry : shardMap.entrySet()) {
       ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
-      String coreNodeName = nodeProps.getNodeName() + ""_"" + coreName;
+      String coreNodeName = nodeProps.getNodeName() + ""_"" + nodeProps.getCoreName();
       if (cloudState.liveNodesContain(nodeProps.getNodeName()) && !coreNodeName.equals(filterNodeName)) {
         if (stateFilter == null || stateFilter.equals(nodeProps.getState())) {
           nodes.add(nodeProps);
         }
       }
     }
     if (nodes.size() == 0) {
       // no replicas - go local
       return null;
     }
 
     return nodes;
   }",Buggy,"SOLR-3108: Error in SolrCloud's replica lookup code when replica's are hosted in same Solr instance (fix only)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1242212 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-3108: Error in SolrCloud's replica lookup code when replica's are hosted in same Solr instance (fix only)"" indicates a bug fix related to replica lookup in SolrCloud when replicas are hosted on the same Solr instance.

The diff shows a change in the `getReplicaProps` method. Specifically, the line `String coreNodeName = nodeProps.getNodeName() + ""_"" + coreName;` is changed to `String coreNodeName = nodeProps.getNodeName() + ""_"" + nodeProps.getCoreName();`.

The original code constructs `coreNodeName` using the `coreName` parameter passed to the method, while the corrected code uses `nodeProps.getCoreName()`. This suggests that the original code was incorrectly using the method parameter `coreName` instead of the core name associated with the node properties (`nodeProps`). This could lead to incorrect comparisons and thus incorrect replica lookups, especially when multiple cores are running on the same node.

The commit message explicitly mentions a fix for an error in replica lookup, and the code change directly addresses a potential issue in how core names are constructed during the lookup process. This strongly suggests that the change is indeed a bug fix.

Therefore, the final answer is:
**Buggy**"
lucene-solr,10932.json,56476fb8c70486c21324879f1bd1efc79df92ffb,"@@ -1,16 +1,24 @@
   public Map<String, Metric> getMetrics() {
     final Map<String, Metric> metrics = new HashMap<>();
 
     try {
       final ObjectName on = new ObjectName(""java.lang:type=OperatingSystem"");
       // verify that it exists
-      mBeanServer.getMBeanInfo(on);
+      MBeanInfo info = mBeanServer.getMBeanInfo(on);
+      // collect valid attributes
+      Set<String> attributes = new HashSet<>();
+      for (MBeanAttributeInfo ai : info.getAttributes()) {
+        attributes.add(ai.getName());
+      }
       for (String metric : METRICS) {
-        metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        // verify that an attribute exists before attempting to add it
+        if (attributes.contains(metric)) {
+          metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        }
       }
     } catch (JMException ignored) {
       log.debug(""Unable to load OperatingSystem MBean"", ignored);
     }
 
     return metrics;
   }",Buggy,"SOLR-9805 Fix assertion error on Windows where SystemLoadAverage is reported as -1.
Don't expose non-existent attributes.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""SOLR-9805 Fix assertion error on Windows where SystemLoadAverage is reported as -1. Don't expose non-existent attributes."" indicates a bug fix related to an assertion error on Windows when `SystemLoadAverage` is reported as -1. It also mentions not exposing non-existent attributes.

2.  **Code Diff:**
    *   The code retrieves MBean information using `mBeanServer.getMBeanInfo(on)`.
    *   It iterates through a set of predefined metrics (`METRICS`).
    *   Critically, it now checks if an attribute exists in the MBean *before* attempting to create a `JmxAttributeGauge` for it. This is done using `attributes.contains(metric)`.
    *   A `try-catch` block handles `JMException`, logging a debug message if the OperatingSystem MBean cannot be loaded.

**Reasoning:**

The original code appears to have been blindly creating `JmxAttributeGauge` instances for all metrics in the `METRICS` list, regardless of whether those attributes actually existed in the MBean. This could lead to exceptions or incorrect behavior if an attribute was not available (as described in the commit message with `SystemLoadAverage` on Windows).

The modified code addresses this by:

1.  Fetching the MBean's attribute information using `mBeanServer.getMBeanInfo(on)`.
2.  Creating a set of valid attribute names.
3.  Checking if a metric exists as an attribute in the MBean before creating the `JmxAttributeGauge`.

This change directly aligns with the commit message's intention to ""Don't expose non-existent attributes"" and fixes the assertion error on Windows where `SystemLoadAverage` is reported as -1. The added check prevents the code from attempting to access attributes that don't exist, which would cause the error described in the commit message. The `try-catch` block was already present, but the added check makes the code more robust.

**Conclusion:**

**Buggy**
"
lucene-solr,28559.json,f55c8d1247fa9183f90bf222e827727ba0dee481,"@@ -1,11 +1,11 @@
     public DocsAndPositionsEnum reset(int[] postings, byte[] payloadBytes) {
       this.postings = postings;
       upto = 0;
       skipPositions = 0;
       startOffset = -1;
       endOffset = -1;
       docID = -1;
       payloadLength = 0;
-      payload.bytes = payloadBytes;
+      this.payloadBytes = payloadBytes;
       return this;
     }",Buggy,"fix bugs in DirectPF's lowFreq d-and-p-enum, set payload.bytes/offset/length in getPayload, also skip payload pointer correctly when scanning over deleted docs in nextDoc

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1364070 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""fix bugs in DirectPF's lowFreq d-and-p-enum, set payload.bytes/offset/length in getPayload, also skip payload pointer correctly when scanning over deleted docs in nextDoc"" explicitly states that it fixes bugs. It mentions specific areas of concern:

1.  ""bugs in DirectPF's lowFreq d-and-p-enum"": Indicates a bug fix related to the DirectPostingsFormat's low-frequency document and positions enumeration.
2.  ""set payload.bytes/offset/length in getPayload"": Suggests a bug where payload information (bytes, offset, length) was not being correctly set in the `getPayload` method.
3.  ""skip payload pointer correctly when scanning over deleted docs in nextDoc"": Points to a bug in the `nextDoc` method where the payload pointer was not being advanced correctly when skipping deleted documents.

**Git Diff Analysis:**

The provided diff shows a change in the `reset` method:

`-      payload.bytes = payloadBytes;`
`+      this.payloadBytes = payloadBytes;`

This change replaces `payload.bytes` with `this.payloadBytes`.  Without seeing the class definition, it's difficult to be 100% certain, but this strongly suggests that `payload` was likely a local variable or a field that wasn't correctly associated with the instance's payload data. By assigning `payloadBytes` to `this.payloadBytes`, the code ensures that the instance's `payloadBytes` field is correctly initialized with the provided payload data. This aligns with the commit message's mention of setting `payload.bytes` correctly.

**Reasoning:**

The commit message clearly indicates bug fixes. The diff shows a change that appears to correct the assignment of payload bytes, which is directly related to one of the issues mentioned in the commit message. The change ensures that the correct payload data is used, which is essential for correct indexing and retrieval.

**Conclusion:**

**Buggy**
"
lucene-solr,2958.json,785bebbcbd8f77ccc6d75acf3fb3d42ee29770fc,"@@ -1,4 +1,4 @@
     public String mapSafeElement(String name) {
       String lowerName = name.toLowerCase(Locale.ROOT);
-      return lowerName.equals(""br"") ? null : lowerName;
+      return (lowerName.equals(""br"") || lowerName.equals(""body"")) ? null : lowerName;
     }",Buggy,"SOLR-8981 remove ""don't test with java-9"" commands; fix bug introduced by TIKA-995 -- doubling of body elements in HTML tags; add copyright info for Jackcess.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8981 remove ""don't test with java-9"" commands; fix bug introduced by TIKA-995 -- doubling of body elements in HTML tags; add copyright info for Jackcess"" suggests the following:

1.  **Removal of Java-9 test exclusions:** This is likely a maintenance task.
2.  **Bug fix related to TIKA-995:**  The core of the message indicates a fix for a bug where HTML `body` elements were being doubled, a problem introduced by a previous change (TIKA-995).
3.  **Copyright information update:** This is a metadata update.

The key part is the mention of a bug fix related to doubled `body` elements.

**Git Diff Analysis:**

The diff shows a change in the `mapSafeElement` method:

```diff
--- a/src/main/java/org/apache/solr/handler/extraction/HTMLStripCharFilter.java
+++ b/src/main/java/org/apache/solr/handler/extraction/HTMLStripCharFilter.java
@@ -1,4 +1,4 @@
     public String mapSafeElement(String name) {
       String lowerName = name.toLowerCase(Locale.ROOT);
-      return lowerName.equals(""br"") ? null : lowerName;
+      return (lowerName.equals(""br"") || lowerName.equals(""body"")) ? null : lowerName;
     }
```

The original code only filtered out `<br>` elements. The modified code now also filters out `<body>` elements. The method `mapSafeElement` seems to be used to determine which HTML elements should be considered ""safe"" and thus not stripped. Returning `null` likely indicates that the element should be stripped.

**Relevance Assessment:**

The commit message explicitly states a bug where `body` elements were being doubled. The code change modifies the `mapSafeElement` method to now filter out `body` elements, which aligns perfectly with the bug description in the commit message. The original code only filtered `<br>` tags, and the change adds `body` to the list of filtered tags. This suggests that the original logic was flawed, and the fix addresses this flaw.

**Conclusion:**

The commit message describes a bug fix, and the code change directly addresses the described bug by preventing `body` elements from being treated in a way that leads to duplication. Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,21433.json,c0e5935900ef5f0c65b006f3071d2132143dda58,"@@ -1,24 +1,25 @@
   public void init(NamedList args) {
 
-    String patternParam = args.remove(PATTERN_PARAM).toString();
+    Object patternParam = args.remove(PATTERN_PARAM);
 
     if(patternParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + PATTERN_PARAM);
     }
+    
     try {
-      pattern = Pattern.compile(patternParam);      
+      pattern = Pattern.compile(patternParam.toString());      
     } catch (PatternSyntaxException e) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Invalid regex: "" + patternParam, e);
     }                                
 
-    String replacementParam = args.remove(REPLACEMENT_PARAM).toString();
+    Object replacementParam = args.remove(REPLACEMENT_PARAM);
     if(replacementParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + REPLACEMENT_PARAM);
     }
-    replacement = Matcher.quoteReplacement(replacementParam);
+    replacement = Matcher.quoteReplacement(replacementParam.toString());
 
     super.init(args);
   }",Buggy,"fix a small bug around these null checks

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1357983 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix a small bug around these null checks"" suggests a bug fix related to null checks.

The code diff shows modifications in the `init` method. Specifically, the code now assigns the result of `args.remove(PATTERN_PARAM)` and `args.remove(REPLACEMENT_PARAM)` to `Object` variables instead of directly calling `.toString()` on the result. This change is followed by a null check on these variables. The `.toString()` method is then called on the variables after the null check.

This change addresses a potential `NullPointerException` that could occur if `args.remove(PATTERN_PARAM)` or `args.remove(REPLACEMENT_PARAM)` returned `null`. By storing the result in an `Object` variable and checking for `null` before calling `.toString()`, the code prevents the `NullPointerException`.

The changes align with the commit message, indicating a bug fix related to null checks.

**Buggy**"
lucene-solr,48864.json,cd5b023219a4e7cd060e88799581704d15820edf,"@@ -1,11 +1,11 @@
     void mergeAt(int i) {
       assert stackSize > i + 1;
       final int l = runBase(i+1);
       final int pivot = runBase(i);
       final int h = runEnd(i);
       merge(l, pivot, h, pivot - l, h - pivot);
-      for (int j = 1; j <= i+1; ++j) {
+      for (int j = i + 1; j > 0; --j) {
         setRunEnd(j, runEnd(j-1));
       }
       --stackSize;
     }",Buggy,"LUCENE-4839: Fix bug in SorterTemplate.timSort().


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1457315 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-4839: Fix bug in SorterTemplate.timSort()"" indicates a bug fix in the `timSort()` method of the `SorterTemplate` class.

The code diff shows a change in the `mergeAt` method. Specifically, the loop `for (int j = 1; j <= i+1; ++j)` is changed to `for (int j = i + 1; j > 0; --j)`. This change in loop direction suggests a correction to the logic of how run ends are set after a merge operation. The original loop iterated in the wrong direction, potentially leading to incorrect run end values and thus a bug in the sorting algorithm. The corrected loop iterates in the correct direction, ensuring that run ends are updated properly.

The change directly relates to the commit message, as it fixes a bug in the `timSort()` implementation, which is part of the `SorterTemplate` class. The modification to the loop suggests a logical correction to the algorithm's behavior.

**Buggy**
"
lucene-solr,38904.json,008bc74bebef96414f19118a267dbf982aba58b9,"@@ -1,30 +1,30 @@
     void add(int startOffset, int endOffset) {
       assert enumIdx == -1 : ""bad state"";
 
       // loop backwards since we expect a match at the end or close to it.  We expect O(1) not O(N).
       int pairIdx = numPairs - 1;
       for (; pairIdx >= 0; pairIdx--) {
         int iStartOffset = startOffsets[pairIdx];
         int iEndOffset = endOffsets[pairIdx];
         int cmp = Integer.compare(iStartOffset, startOffset);
         if (cmp == 0) {
           cmp = Integer.compare(iEndOffset, endOffset);
         }
         if (cmp == 0) {
           return; // we already have this offset-pair for this term
         } else if (cmp < 0) {
           break; //we will insert offsetPair to the right of pairIdx
         }
       }
       // pairIdx is now one position to the left of where we insert the new pair
       // shift right any pairs by one to make room
       final int shiftLen = numPairs - (pairIdx + 1);
       if (shiftLen > 0) {
-        System.arraycopy(startOffsets, pairIdx + 2, startOffsets, pairIdx + 3, shiftLen);
-        System.arraycopy(endOffsets, pairIdx + 2, endOffsets, pairIdx + 3, shiftLen);
+        System.arraycopy(startOffsets, pairIdx + 1, startOffsets, pairIdx + 2, shiftLen);
+        System.arraycopy(endOffsets, pairIdx + 1, endOffsets, pairIdx + 2, shiftLen);
       }
       // now we can place the offset pair
       startOffsets[pairIdx + 1] = startOffset;
       endOffsets[pairIdx + 1] = endOffset;
       numPairs++;
     }",Buggy,"LUCENE-8365: Fix ArrayIndexOutOfBoundsException in UnifiedHighlighter

This fixes a ""off by one"" error in the UnifiedHighlighter's code that
is only triggered when two nested SpanNearQueries contain the same term.

Signed-off-by: Simon Willnauer <simonw@apache.org>
Closes #408
",Buggy,"The commit message indicates a fix for an `ArrayIndexOutOfBoundsException` in the `UnifiedHighlighter` when nested `SpanNearQuery` instances contain the same term. The code diff modifies the `add` method, specifically the `System.arraycopy` calls. The original code had `pairIdx + 2` as the source and `pairIdx + 3` as the destination, which would cause an out-of-bounds write when `pairIdx + 3` is equal to `startOffsets.length`. The corrected code changes the source to `pairIdx + 1` and the destination to `pairIdx + 2`, which shifts the array elements correctly and avoids the exception. This confirms that the change addresses a bug.

**Buggy**
"
lucene-solr,2699.json,bee8d7ccb32bc23bd808f729493631b60a64bffb,"@@ -1,18 +1,9 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      boolean aIsNull = a.getValue() == null;
-      boolean bIsNull = b.getValue() == null;
-
-      if (aIsNull && bIsNull) return 0;
-
-      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
-        return aIsNull ? 1
-          : bIsNull ? -1
-          : a.getValue().compareTo(b.getValue());
+      if( direction == FacetSortDirection.ASCENDING ){
+        return a.getValue().compareTo(b.getValue());
       } else {
-        return aIsNull ? -1
-          : bIsNull ? 1
-          : b.getValue().compareTo(a.getValue());
+        return b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"Revert ""SOLR-9981: Performance improvements and bug fixes for the Analytics component""

This reverts commit a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Revert ""SOLR-9981: Performance improvements and bug fixes for the Analytics component"""" clearly indicates that this commit is undoing a previous commit (a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3) that was intended to provide performance improvements and bug fixes for the Analytics component. The fact that it's a revert suggests that the original commit likely introduced new issues or failed to adequately address the existing ones.

**Code Diff Analysis:**

The code diff shows changes within the `comparator` method related to `FacetSortDirection`. The original commit likely added handling for null values in the `Expression` objects being compared. The revert removes this null handling.

Specifically, the original code included checks for `a.getValue() == null` and `b.getValue() == null`. If both were null, it returned 0. If only one was null, it placed nulls either at the end (for ascending sort) or at the beginning (for descending sort).

The reverted code removes these null checks, directly comparing `a.getValue()` and `b.getValue()` without considering nulls. This could lead to a `NullPointerException` if either `a.getValue()` or `b.getValue()` is null.

**Reasoning:**

Since the commit message is a revert of a commit that claimed to fix bugs, and the code diff removes null checks that could prevent `NullPointerException`s, it's highly probable that the original commit introduced a bug or had unintended consequences that necessitated its removal. The removal of null handling is a strong indicator that the original changes were problematic.

**Conclusion:**

**Buggy**
"
lucene-solr,2699.json,a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3,"@@ -1,9 +1,18 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      if( direction == FacetSortDirection.ASCENDING ){
-        return a.getValue().compareTo(b.getValue());
+      boolean aIsNull = a.getValue() == null;
+      boolean bIsNull = b.getValue() == null;
+
+      if (aIsNull && bIsNull) return 0;
+
+      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
+        return aIsNull ? 1
+          : bIsNull ? -1
+          : a.getValue().compareTo(b.getValue());
       } else {
-        return b.getValue().compareTo(a.getValue());
+        return aIsNull ? -1
+          : bIsNull ? 1
+          : b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"SOLR-9981: Performance improvements and bug fixes for the Analytics component
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-9981: Performance improvements and bug fixes for the Analytics component"" explicitly states that the commit includes bug fixes. This is a strong indicator that the diff might contain a bug fix.

**Code Diff Analysis:**

The code diff modifies the `comparator` method within the `Expression` class. The original code directly compared `a.getValue()` and `b.getValue()` without considering the possibility of null values. The modified code introduces null checks (`aIsNull` and `bIsNull`) before performing the comparison.

Specifically, the changes address the scenario where either `a.getValue()` or `b.getValue()` (or both) could be null. The updated logic handles null values by placing them either at the beginning or end of the sorted list, depending on the `FacetSortDirection`. For ascending order, nulls are placed last, and for descending order, nulls are placed first.

**Reasoning:**

The original code would likely throw a `NullPointerException` if either `a.getValue()` or `b.getValue()` were null. The updated code prevents this exception by explicitly handling null values. This is a clear indication of a bug fix, as the original code had a potential error that has now been resolved. The handling of null values in sorting is a common source of bugs.

**Conclusion:**

**Buggy**
"
lucene-solr,5640.json,6e4924cbfc828506550fd27b0350e3f12c572746,"@@ -1,65 +1,65 @@
   private ResultSetValueSelector[] constructValueSelectors(ResultSetMetaData metadata) throws SQLException{
     ResultSetValueSelector[] valueSelectors = new ResultSetValueSelector[metadata.getColumnCount()];
     
     for(int columnIdx = 0; columnIdx < metadata.getColumnCount(); ++columnIdx){
       
       final int columnNumber = columnIdx + 1; // cause it starts at 1        
-      final String columnName = metadata.getColumnName(columnNumber);
+      final String columnName = metadata.getColumnLabel(columnNumber);
       String className = metadata.getColumnClassName(columnNumber);
       String typeName = metadata.getColumnTypeName(columnNumber);
             
       if(directSupportedTypes.contains(className)){
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Object obj = resultSet.getObject(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj;
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Short.class.getName() == className){
+      else if(Short.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Short obj = resultSet.getShort(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Integer.class.getName() == className){
+      else if(Integer.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Integer obj = resultSet.getInt(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Float.class.getName() == className){
+      else if(Float.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Float obj = resultSet.getFloat(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.doubleValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
       else{
         throw new SQLException(String.format(Locale.ROOT, ""Unable to determine the valueSelector for column '%s' (col #%d) of java class '%s' and type '%s'"", columnName, columnNumber, className, typeName));
       }
     }
     
     return valueSelectors;
   }",Buggy,"Update dependencies and fix minor errors
",Buggy,"The commit message indicates dependency updates and minor error fixes.

The code diff focuses on the `constructValueSelectors` method, which dynamically creates `ResultSetValueSelector` objects based on the metadata of a `ResultSet`.

Here's a breakdown of the changes:

1.  **`metadata.getColumnName` to `metadata.getColumnLabel`**: This change replaces `getColumnName` with `getColumnLabel`. `getColumnName` returns the column's name as defined in the database schema, while `getColumnLabel` returns the suggested column title for use in display. Using `getColumnLabel` is generally preferred as it can be more user-friendly. This is a bug fix if the original code was displaying the wrong column name to the user.

2.  **`Short.class.getName() == className` to `Short.class.getName().equals(className)` (and similar for Integer and Float)**: This change replaces `==` with `.equals()` for String comparison. Using `==` to compare strings checks for reference equality, which is often incorrect. `.equals()` checks for value equality, which is the correct way to compare strings. This is a bug fix because the original code might not have correctly identified the column type, leading to incorrect value selection.

The changes directly address potential issues in how column names are retrieved and how column types are identified. These are indeed minor error fixes.

**Buggy**
"
lucene-solr,26464.json,96150badce8234cac00a23c2d5da55545e0be958,"@@ -1,16 +1,24 @@
-  private int readInputToBuffer() throws IOException {
-    final int len = input.read(tmpBuffer);
-    if (len == -1) {
-      inputFinished = true;
-      return 0;
+  private void readInputToBuffer() throws IOException {
+    while (true) {
+      // CharacterUtils.fill is supplementary char aware
+      final boolean hasRemainingChars = CharacterUtils.fill(tmpBuffer, input);
+
+      assert tmpBuffer.getOffset() == 0;
+      inputBuffer.append(tmpBuffer.getBuffer(), 0, tmpBuffer.getLength());
+
+      if (hasRemainingChars == false) {
+        inputFinished = true;
+        break;
+      }
+
+      final int lastCodePoint = Character.codePointBefore(tmpBuffer.getBuffer(), tmpBuffer.getLength());
+      if (normalizer.isInert(lastCodePoint)) {
+        // we require an inert char so that we can normalize content before and
+        // after this character independently
+        break;
+      }
     }
-    inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
-    // so we treat all surrogates as non-inert for simplicity
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
-      return len;
-    } else return len + readInputToBuffer();
   }",Buggy,"LUCENE-7956: Fixed potential stack overflow error in ICUNormalizer2CharFilter.
",Buggy,"The commit message indicates a fix for a potential stack overflow error in `ICUNormalizer2CharFilter`. The diff shows a change in the `readInputToBuffer` method.

The original code recursively calls `readInputToBuffer` if the last character read is not inert and not a surrogate. This recursion could potentially lead to a stack overflow if a long sequence of non-inert characters is encountered.

The modified code uses a `while` loop instead of recursion to read input into the buffer. The loop continues until the input is finished or an inert character is encountered. This iterative approach eliminates the risk of stack overflow.

The changes align with the commit message and address a potential stack overflow error.

**Buggy**"
lucene-solr,26464.json,63d67496cfae2cbe4611b294c18d285e89c8e254,"@@ -1,14 +1,16 @@
   private int readInputToBuffer() throws IOException {
     final int len = input.read(tmpBuffer);
     if (len == -1) {
       inputFinished = true;
       return 0;
     }
     inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isHighSurrogate(tmpBuffer[len-1])) {
+    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
+    // so we treat all surrogates as non-inert for simplicity
+    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
       return len;
     } else return len + readInputToBuffer();
   }",Buggy,"fix bug in buffering logic of this charfilter

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1586473 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix bug in buffering logic of this charfilter"" suggests a bug fix related to how the character filter buffers data.

The code diff modifies the `readInputToBuffer` method. Specifically, it changes the condition within the `if` statement:

- Original: `normalizer.isInert(tmpBuffer[len - 1]) && !Character.isHighSurrogate(tmpBuffer[len-1])`
- Modified: `normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])`

The change replaces `Character.isHighSurrogate` with `Character.isSurrogate`. The comment added explains the reasoning: ""this loop depends on 'isInert' (changes under normalization) but looks only at characters. so we treat all surrogates as non-inert for simplicity"". This indicates that the original code was not correctly handling surrogate characters, potentially leading to incorrect buffering or normalization. The change ensures that both high and low surrogates are treated as non-inert. This is a logical correction to handle surrogate characters properly, which can be considered a bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**
"
lucene-solr,15099.json,07b37ff26becf214bae000dff08b9091d31327a8,"@@ -1,14 +1,12 @@
   private boolean predefinedPermissionAppliesToRequest(Permission predefinedPermission, AuthorizationContext context) {
-    if (context.getHandler() instanceof PermissionNameProvider) {
+    if (predefinedPermission.wellknownName == PermissionNameProvider.Name.ALL) {
+      return true; //'ALL' applies to everything!
+    } else if (! (context.getHandler() instanceof PermissionNameProvider)) {
+      return false; // We're not 'ALL', and the handler isn't associated with any other predefined permissions
+    } else {
       PermissionNameProvider handler = (PermissionNameProvider) context.getHandler();
       PermissionNameProvider.Name permissionName = handler.getPermissionName(context);
-      if (permissionName == null || !predefinedPermission.name.equals(permissionName.name)) {
-        return false;
-      }
-    } else {
-      //all is special. it can match any
-      if(predefinedPermission.wellknownName != PermissionNameProvider.Name.ALL) return false;
-    }
 
-    return true;
+      return permissionName != null && predefinedPermission.name.equals(permissionName.name);
+    }
   }",Buggy,"SOLR-13355: Obey 'ALL' for handlers with other predefined perms

Prior to this commit, RuleBasedAuthorizationPlugin would check for the
predefined 'ALL' permission only when the endpoint being hit wasn't
associated with another predefined-permission.

This resulted in some very unintuitive behavior. For example, the
permission {name:all, role:admin} would correctly prevent a
role:foo user from accessing /admin/info/properties, but would allow
write access to /admin/authorization because of the SECURITY_EDIT
predefined perm associated with that endpoint.

This commit fixes this bug so that the 'all' permission is always
consulted whether or not the endpoint is associated with other predefined
permissions.
",Buggy,"The commit message describes a bug where the 'ALL' permission was not being correctly applied to handlers with other predefined permissions. The code change modifies the `predefinedPermissionAppliesToRequest` method to ensure that the 'ALL' permission is always checked, regardless of whether the endpoint is associated with other predefined permissions. The original code had a logic flaw where it would only check for the 'ALL' permission if the handler wasn't associated with another predefined permission. The modified code prioritizes checking for 'ALL' first and then proceeds to check for other predefined permissions only if it's not 'ALL'. This correction aligns with the bug described in the commit message.

**Buggy**"
lucene-solr,14965.json,0857bb60d3bb5bd6d8d851ce2e01edd9bde310ea,"@@ -1,20 +1,21 @@
     public void refresh(SolrQueryRequest req, SolrQueryResponse rsp, PayloadObj<String> payload) {
       String p = payload.get();
       if (p == null) {
         payload.addError(""Package null"");
         return;
       }
       PackageLoader.Package pkg = coreContainer.getPackageLoader().getPackage(p);
       if (pkg == null) {
         payload.addError(""No such package: "" + p);
         return;
       }
-
+      //first refresh my own
+      packageLoader.notifyListeners(p);
       for (String s : coreContainer.getPackageStoreAPI().shuffledNodes()) {
         Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(),
             coreContainer.getZkController().zkStateReader.getBaseUrlForNodeName(s).replace(""/solr"", ""/api"") + ""/cluster/package?wt=javabin&omitHeader=true&refreshPackage="" + p,
             Utils.JAVABINCONSUMER);
       }
 
 
     }",Buggy,"SOLR-13662, SOLR-13822: Fixing bug with refresh API, un-ignoring the test and new reference guide on Package Management
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-13662, SOLR-13822: Fixing bug with refresh API, un-ignoring the test and new reference guide on Package Management"" explicitly states that it's fixing a bug with the refresh API. This is a strong indicator that the diff is related to a bug fix. The message also mentions un-ignoring a test, which suggests that the test was previously failing due to the bug.

**Code Diff Analysis:**

The code diff shows a modification to the `refresh` method. Let's break down the changes:

1.  **Error Handling:** The code already had error handling for null package names and non-existent packages. This suggests that the original code considered these cases, but the bug might have been in a different area.
2.  **Package Refresh Logic:** The core change is the addition of `packageLoader.notifyListeners(p);` before iterating through the nodes. This line seems to be triggering a refresh of the package locally *before* attempting to refresh it on other nodes. This is a crucial step, as it ensures the current node has the latest package information before propagating the refresh to other nodes. Without this, nodes might be refreshing based on outdated information, potentially leading to inconsistencies or errors.
3.  **Node Shuffling:** The code iterates through shuffled nodes, which is likely to avoid all nodes hitting the same server at once. This is related to performance and not likely to be a bug fix.
4.  **Remote Refresh:** The code uses `Utils.executeGET` to refresh the package on other nodes. This part seems to be related to distributing the package refresh across the cluster.

**Reasoning:**

The addition of `packageLoader.notifyListeners(p);` strongly suggests a bug fix. It addresses a potential issue where the local package information might not be up-to-date before attempting to refresh other nodes. This could lead to inconsistent states across the cluster or errors during the refresh process. The commit message confirms this by explicitly stating that it's fixing a bug in the refresh API.

**Conclusion:**

**Buggy**
"
lucene-solr,19226.json,9e78be40c338005b75609a3b123778aea822bcf0,"@@ -1,7 +1,7 @@
   public void setNextReader(LeafReaderContext context) throws IOException {
-    if (globalDocValues instanceof MultiDocValues.MultiSortedDocValues) {
+    if (ordinalMap != null) {
       toGlobal = ordinalMap.getGlobalOrds(context.ord);
-      docValues = DocValues.getSorted(context.reader(), field);
     }
+    docValues = DocValues.getSorted(context.reader(), field);
     lastDocID = 0;
   }",Buggy,"SOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Commit Message Analysis:**

The commit message ""SOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents"" clearly indicates a bug fix related to string doc-values during document export. The message suggests an issue with how these values are set or reset.

**Code Diff Analysis:**

The code diff modifies the `setNextReader` method.  Here's a breakdown:

*   **Original Code:**
    ```java
    if (globalDocValues instanceof MultiDocValues.MultiSortedDocValues) {
        toGlobal = ordinalMap.getGlobalOrds(context.ord);
        docValues = DocValues.getSorted(context.reader(), field);
    }
    ```

*   **Modified Code:**
    ```java
    if (ordinalMap != null) {
        toGlobal = ordinalMap.getGlobalOrds(context.ord);
    }
    docValues = DocValues.getSorted(context.reader(), field);
    ```

The change moves the line `docValues = DocValues.getSorted(context.reader(), field);` outside of the `if` block.  The original code only retrieved the `docValues` if `globalDocValues` was an instance of `MultiDocValues.MultiSortedDocValues`. The modified code *always* retrieves the `docValues` when a new reader is set.

**Reasoning:**

The original code appears to have a conditional logic that might have prevented the `docValues` from being correctly set in certain scenarios (specifically, when `globalDocValues` was not a `MultiSortedDocValues`). This could lead to incorrect or missing string doc-values during document export, which aligns with the commit message's description of a bug related to setting/resetting string doc-values. By unconditionally retrieving `docValues`, the modified code ensures that the `docValues` are always set, potentially fixing the bug. The change in logic suggests a correction to how doc values are handled, indicating a bug fix.

**Conclusion:**

**Buggy**
"
lucene-solr,38500.json,1d35bd0ea8beb635997c3cf131ded8ebe58d15a9,"@@ -1,9 +1,9 @@
   public void clearFieldsAfter(Calendar cal, int field) {
     int assertEra = -1;
     assert (assertEra = (((Calendar)cal.clone()).get(Calendar.ERA))) >= 0;//a trick to only get this if assert enabled
     //note: Calendar.ERA == 0;
-    for (int f = field+1; f <= Calendar.MILLISECOND; f++) {
+    for (int f = field + 1; f <= Calendar.MILLISECOND; f++) {
       cal.clear(f);
     }
-    assert ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
+    assert field + 1 == Calendar.ERA || ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
   }",Buggy,"LUCENE-7278: DRPT: fix bug in assert statement
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7278: DRPT: fix bug in assert statement"" clearly states that the commit is intended to fix a bug within an assert statement. This strongly suggests the presence of a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `clearFieldsAfter` method. Specifically, the assert statement at the end of the method has been changed.

Original assert:
```java
assert ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
```

Modified assert:
```java
assert field + 1 == Calendar.ERA || ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
```

The original assert statement was checking if the ERA field of a cloned calendar was equal to `assertEra`. The modified assert statement adds a condition `field + 1 == Calendar.ERA` before the original check. This suggests that the original assert statement was failing under certain conditions, potentially due to a Calendar underflow issue when `field + 1` is equal to `Calendar.ERA`. The added condition seems to be a workaround or a more precise check to avoid false assertion failures.

**Reasoning:**

The commit message explicitly mentions a bug fix in an assert statement. The diff shows a modification to an assert statement, adding a condition that likely prevents false assertion failures. This aligns with the commit message and strongly indicates that the change is indeed a bug fix. The original assert was likely too strict and triggered incorrectly under specific circumstances. The modified assert statement corrects this behavior.

**Conclusion:**

**Buggy**
"
lucene-solr,13814.json,d7f397056bcb52564bf5cbffa96152aa74c0fa1f,"@@ -1,28 +1,29 @@
   public Query parse() throws SyntaxError {
     Query q = super.parse();
     if (!(q instanceof BooleanQuery)) {
       return q;
     }
     BooleanQuery obq = (BooleanQuery)q;
     Collection<Query> should = new ArrayList<Query>();
     Collection<BooleanClause> prohibOrReq = new ArrayList<BooleanClause>();
     BooleanQuery newq = new BooleanQuery();
 
     for (BooleanClause clause : obq.getClauses()) {
       if(clause.isProhibited() || clause.isRequired()) {
         prohibOrReq.add(clause);
       } else {
         BooleanQuery bq = new BooleanQuery();
         bq.add(clause);
         should.add(bq);
       }
     }
     if (should.size() > 0) {
       DisjunctionMaxQuery dmq = new DisjunctionMaxQuery(should, tie);
       newq.add(dmq, BooleanClause.Occur.SHOULD);
     }
     for(BooleanClause c : prohibOrReq) {
       newq.add(c);
     }
+    newq.setBoost(obq.getBoost());
     return newq;
   }",Buggy,"SOLR-4785: Fixed bug with missing boost on toplevel query

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1486898 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-4785: Fixed bug with missing boost on toplevel query"" suggests a bug fix related to a missing boost value on the top-level query.

The code diff modifies the `parse()` method. Specifically, it adds the line `newq.setBoost(obq.getBoost());` before returning the `newq` object. This line sets the boost of the new query (`newq`) to be the same as the boost of the original query (`obq`).

Before this change, the boost of the original query (`obq`) was not being propagated to the new query (`newq`) that was being constructed. This would result in the query not having the correct boost value, which aligns with the bug described in the commit message. The added line fixes this issue by explicitly setting the boost of the new query.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**
"
lucene-solr,20938.json,88eb9719fa0df2ed0477ec2bb9f20c8ab1644f85,"@@ -1,15 +1,15 @@
   private static boolean matchPath(String path, String name){
     List<String> pathSplit = StrUtils.splitSmart(path, '/');
     List<String> nameSplit = StrUtils.splitSmart(name, '/');
     for (int i = 0; i < nameSplit.size(); i++) {
       String s = nameSplit.get(i);
       String ps = pathSplit.size()>i ?  pathSplit.get(i) :null;
       if(ps == null) return false;
       if(s.equals(ps)) continue;
       if(""*"".equals(ps) && nameSplit.size()==i+1) return true;
       if(""**"".equals(ps)) return true;
       return false;
     }
-    return true;
+    return false;
 
   }",Buggy,"SOLR-6365 bug fix matching wrong  name when it is a shorter prefix of path

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1649996 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-6365 bug fix matching wrong name when it is a shorter prefix of path"" indicates a bug fix related to path matching logic. The code diff modifies the `matchPath` method, specifically the return statement after the loop.

The original code returned `true` after the loop, which would incorrectly indicate a match even if the name was a shorter prefix of the path and didn't fully match. The modified code returns `false` after the loop, which is the correct behavior when the loop completes without finding a mismatch, indicating that the name is a shorter, non-matching prefix.

The change directly addresses the problem described in the commit message. The original code had a logical error that caused incorrect path matching, and the modification corrects this error.

**Buggy**
"
lucene-solr,46829.json,71c65184562499eba365d166fe3fabe0dbdc747b,"@@ -1,37 +1,37 @@
   public static final BinaryDocValues emptyBinary() {
     return new BinaryDocValues() {
       private int doc = -1;
       
       @Override
       public int advance(int target) {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public boolean advanceExact(int target) throws IOException {
         doc = target;
-        return true;
+        return false;
       }
       
       @Override
       public int docID() {
         return doc;
       }
       
       @Override
       public int nextDoc() {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public long cost() {
         return 0;
       }
 
       @Override
       public BytesRef binaryValue() {
         assert false;
         return null;
       }
     };
   }",Buggy,"LUCENE-7462: Fix buggy advanceExact impl of empty binary doc values.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7462: Fix buggy advanceExact impl of empty binary doc values"" explicitly states that the commit is intended to fix a bug in the `advanceExact` implementation for empty binary doc values. This strongly suggests that the diff should contain a correction to erroneous behavior.

**Git Diff Analysis:**

The diff focuses on the `emptyBinary()` method, which returns a `BinaryDocValues` instance representing an empty set of binary document values.  The key change is within the `advanceExact(int target)` method:

- **Original Code:**
  ```java
  @Override
  public boolean advanceExact(int target) throws IOException {
    doc = target;
    return true;
  }
  ```

- **Modified Code:**
  ```java
  @Override
  public boolean advanceExact(int target) throws IOException {
    doc = target;
    return false;
  }
  ```

The return value of `advanceExact` has been changed from `true` to `false`.

**Reasoning:**

The `advanceExact` method in `DocIdSetIterator` (which `BinaryDocValues` extends) is supposed to return `true` if the iterator is currently on the `target` document, and `false` otherwise. In the context of an *empty* `BinaryDocValues`, `advanceExact` should *never* be on the `target` document because there are no documents. Therefore, the original implementation was incorrect, always returning `true` regardless of the input. The corrected code now returns `false`, which is the correct behavior for an empty `BinaryDocValues`.

The change directly addresses the bug described in the commit message. The original code had a logical error that caused it to behave incorrectly in the specific case of empty binary doc values.

**Conclusion:**

The commit message clearly indicates a bug fix, and the code change directly addresses a logical error in the `advanceExact` method of an empty `BinaryDocValues` implementation. Therefore, the diff represents a bug fix.

**Buggy**
"
lucene-solr,21965.json,876573650b72adbf1ca005fe0f33607140c23841,"@@ -1,28 +1,28 @@
   public static SchemaField getAndCheckVersionField(IndexSchema schema) 
     throws SolrException {
-    final String errPrefix = VERSION_FIELD + ""field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
+    final String errPrefix = VERSION_FIELD + "" field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
     SchemaField sf = schema.getFieldOrNull(VERSION_FIELD);
 
     if (null == sf) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" does not exist)"");
     }
     if ( !sf.indexed() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not indexed"");
     }
     if ( !sf.stored() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not stored"");
     }
     if ( sf.multiValued() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is multiValued"");
     }
     
     return sf;
   }",Buggy,"SOLR-5259: Fix typo in error message when _version_ field is missing

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1525620 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a typo in an error message related to the `_version_` field. The diff shows a change in the `errPrefix` string within the `getAndCheckVersionField` method. Specifically, a space was added after `VERSION_FIELD` in the error message. This aligns perfectly with the commit message's description of fixing a typo. This change improves the clarity and readability of the error message, which is a bug fix.

**Buggy**"
lucene-solr,17330.json,be813bd0aefcf480f854a05c7880494da5e8c8bf,"@@ -1,21 +1,45 @@
   public void validateRouteValue(AddUpdateCommand cmd) throws SolrException {
 
     final Instant docTimestamp =
         parseRouteKey(cmd.getSolrInputDocument().getFieldValue(getRouteField()));
 
     // FUTURE: maybe in some cases the user would want to ignore/warn instead?
     if (docTimestamp.isAfter(Instant.now().plusMillis(getMaxFutureMs()))) {
       throw new SolrException(BAD_REQUEST,
           ""The document's time routed key of "" + docTimestamp + "" is too far in the future given "" +
               ROUTER_MAX_FUTURE + ""="" + getMaxFutureMs());
     }
 
     // Although this is also checked later, we need to check it here too to handle the case in Dimensional Routed
     // aliases where one can legally have zero collections for a newly encountered category and thus the loop later
     // can't catch this.
-    Instant startTime = parseRouteKey(start);
+
+    // SOLR-13760 - we need to fix the date math to a specific instant when the first document arrives.
+    // If we don't do this DRA's with a time dimension have variable start times across the other dimensions
+    // and logic gets much to complicated, and depends too much on queries to zookeeper. This keeps life simpler.
+    // I have to admit I'm not terribly fond of the mutation during a validate method however.
+    Instant startTime;
+    try {
+      startTime = Instant.parse(start);
+    } catch (DateTimeParseException e) {
+      startTime = DateMathParser.parseMath(new Date(), start).toInstant();
+      SolrCore core = cmd.getReq().getCore();
+      ZkStateReader zkStateReader = core.getCoreContainer().getZkController().zkStateReader;
+      Aliases aliases = zkStateReader.getAliases();
+      Map<String, String> props = new HashMap<>(aliases.getCollectionAliasProperties(aliasName));
+      start = DateTimeFormatter.ISO_INSTANT.format(startTime);
+      props.put(ROUTER_START, start);
+
+      // This could race, but it only occurs when the alias is first used and the values produced
+      // should all be identical and who wins won't matter (baring cases of Date Math involving seconds,
+      // which is pretty far fetched). Putting this in a separate thread to ensure that any failed
+      // races don't cause documents to get rejected.
+      core.runAsync(() -> zkStateReader.aliasesManager.applyModificationAndExportToZk(
+          (a) -> aliases.cloneWithCollectionAliasProperties(aliasName, props)));
+
+    }
     if (docTimestamp.isBefore(startTime)) {
       throw new SolrException(BAD_REQUEST, ""The document couldn't be routed because "" + docTimestamp +
           "" is before the start time for this alias "" +start+"")"");
     }
   }",Buggy,"SOLR-13760 - restore viability of date math in TRA start property (#879)

* SOLR-13760 - restore viability of date math in TRA start property by 
fixing the start date for time routed aliases
upon the receipt of the first document to avoid problems
with date math calculations required by DRA's
",Buggy,"The commit message ""SOLR-13760 - restore viability of date math in TRA start property"" indicates a fix related to date math calculations in time-routed aliases (TRA). The code diff shows a modification in the `validateRouteValue` method. Specifically, the code now attempts to parse the `start` property as an `Instant`. If it fails, it uses `DateMathParser` to calculate the start time and then updates the alias properties in ZooKeeper with the calculated start time in ISO_INSTANT format. This ensures that the date math is fixed to a specific instant when the first document arrives, addressing the issue of variable start times across dimensions. The error handling involves catching `DateTimeParseException` and using `DateMathParser` as a fallback. The code also includes a mechanism to update the alias properties in ZooKeeper asynchronously. This aligns with the commit message's description of fixing the start date for time-routed aliases upon the receipt of the first document to avoid problems with date math calculations.

**Buggy**
"
lucene-solr,11507.json,75e69c5198c02e6635eed274b03ea759ef1c4818,"@@ -1,19 +1,19 @@
     public void collect(int doc) throws IOException{
       int valuesDocID = leafOutcomeValue.docID();
       if (valuesDocID < doc) {
-        valuesDocID = leafOutcomeValue.advance(valuesDocID);
+        valuesDocID = leafOutcomeValue.advance(doc);
       }
       int outcome;
       if (valuesDocID == doc) {
         outcome = (int) leafOutcomeValue.longValue();
       } else {
         outcome = 0;
       }
 
       outcome = trainingParams.positiveLabel == outcome? 1 : 0;
       if (outcome == 1) {
         positiveDocsSet.set(context.docBase + doc);
       }
       docsSet.set(context.docBase+doc);
 
     }",Buggy,"SOLR-9549: Fix bug in advancing docValues
",Buggy,"The commit message ""SOLR-9549: Fix bug in advancing docValues"" indicates a bug fix related to advancing docValues.

The code diff shows a change in the `collect` method where `leafOutcomeValue.advance(valuesDocID)` is replaced with `leafOutcomeValue.advance(doc)`.

The original code was attempting to advance the `leafOutcomeValue` to `valuesDocID`, which is the *current* `valuesDocID`. This doesn't make sense, as it would either do nothing or potentially skip over the correct document. The corrected code advances to the *current* document `doc`, which is the intended behavior. This indicates a logical correction in how the docValues are advanced, which aligns with the commit message indicating a bug fix.

**Buggy**"
lucene-solr,6041.json,e4d4e582a0049de34990fcff3df5fb220f14ee4b,"@@ -1,5 +1,13 @@
   public void open() throws IOException {
-    Map<String, List<Tuple>> lets = streamContext.getLets();
-    List<Tuple> tuples = lets.get(name);
-    tupleIterator = tuples.iterator();
+    Map<String, Object> lets = streamContext.getLets();
+    Object o = lets.get(name);
+    List l = null;
+    if(o instanceof List) {
+        l = (List)o;
+      if(l.get(0) instanceof Tuple) {
+        tupleIterator = l.iterator();
+      } else {
+        throw new IOException(""Get was not passed a list of tuples:""+o.getClass());
+      }
+    }
   }",Buggy,"SOLR-10559: Fixed compilation error
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""SOLR-10559: Fixed compilation error"" explicitly states that the change addresses a compilation error. This strongly suggests a bug fix.

2.  **Code Diff Analysis:**
    *   The original code directly casts the value retrieved from `streamContext.getLets()` to `List<Tuple>`. This assumes that the value associated with the key `name` is always a `List<Tuple>`.
    *   The modified code introduces a check to ensure the object `o` retrieved from `streamContext.getLets()` is an instance of `List`. It then checks if the first element of the list is an instance of `Tuple`. If either of these checks fails, an `IOException` is thrown.
    *   The change from `Map<String, List<Tuple>>` to `Map<String, Object>` in the `lets` variable indicates a change in the type of data stored in the map. This suggests that the original code was too restrictive and caused a compilation error when the map contained a value that was not a `List<Tuple>`.
    *   The added `instanceof` checks and the `IOException` handling are typical of bug fixes that address type mismatches or unexpected data types.

3.  **Alignment:** The code changes directly address the compilation error mentioned in the commit message by adding type checking and handling cases where the retrieved object is not of the expected type.

**Conclusion:**

The commit message and the code changes are strongly aligned. The code modifications introduce type checking and error handling to address a compilation error caused by an incorrect type assumption. This clearly indicates a bug fix.

**Buggy**"
lucene-solr,48392.json,05d62a357711d1e4e850a5d2fb7336bf0a7acf24,"@@ -1,7 +1,9 @@
   public static double haversinSortKey(double lat1, double lon1, double lat2, double lon2) {
     double x1 = lat1 * TO_RADIANS;
     double x2 = lat2 * TO_RADIANS;
     double h1 = 1 - cos(x1 - x2);
     double h2 = 1 - cos((lon1 - lon2) * TO_RADIANS);
-    return h1 + cos(x1) * cos(x2) * h2;
+    double h = h1 + cos(x1) * cos(x2) * h2;
+    // clobber crazy precision so subsequent rounding does not create ties.
+    return Double.longBitsToDouble(Double.doubleToRawLongBits(h) & 0xFFFFFFFFFFFFFFF8L);
   }",Buggy,"LUCENE-7185: fix tie-breaker sort bug
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7185: fix tie-breaker sort bug"" explicitly states that the commit is intended to fix a bug related to tie-breaking during sorting. This strongly suggests that the code changes are bug-related.

**Code Diff Analysis:**

The code diff modifies the `haversinSortKey` method, which calculates a sort key based on Haversine distance. The original code simply returned `h1 + cos(x1) * cos(x2) * h2`. The modified code introduces a new variable `h` to store the intermediate result and then performs a bitwise operation on the double representation of `h` before returning it.

Specifically, `Double.doubleToRawLongBits(h) & 0xFFFFFFFFFFFFFFF8L` clears the least significant 3 bits of the long representation of the double. This has the effect of reducing the precision of the double value. The comment ""// clobber crazy precision so subsequent rounding does not create ties."" explains the purpose of this change: to prevent ties during sorting due to excessive precision in the calculated sort key.

**Reasoning:**

The commit message clearly indicates a bug fix related to tie-breaking in sorting. The code modification reduces the precision of the calculated sort key, which directly addresses the issue of ties during sorting. The comment in the code further confirms this interpretation. The original code likely resulted in very similar sort keys for different locations, leading to unstable or incorrect sorting behavior. By reducing the precision, the likelihood of ties is reduced, thus fixing the bug.

**Conclusion:**

The commit message and code changes are highly relevant to each other and clearly indicate a bug fix.

**Buggy**
"
lucene-solr,50327.json,97a5295f075d37b1a31c5e77e85f7a9934cae096,"@@ -1,5 +1,5 @@
   public double getLongitude() {
     if (Math.abs(x) < MINIMUM_RESOLUTION && Math.abs(y) < MINIMUM_RESOLUTION)
       return 0.0;
-    return Math.atan2(y,z);
+    return Math.atan2(y,x);
   }",Buggy,"LUCENE-6487: Geo3D with WGS84 patch from Karl: fix bug in GeoPoint.getLongitude with test
from https://reviews.apache.org/r/34744/diff/raw/

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene6487@1682357 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-6487: Geo3D with WGS84 patch from Karl: fix bug in GeoPoint.getLongitude with test"" indicates a bug fix in the `GeoPoint.getLongitude` method. The diff shows a change in the `getLongitude` method from `Math.atan2(y,z)` to `Math.atan2(y,x)`. This change suggests a correction in the calculation of the longitude, which aligns with the commit message's claim of fixing a bug. The original code used `z` which is likely incorrect and the corrected code uses `x` which is more likely to be the correct coordinate for longitude calculation.

**Buggy**
"
lucene-solr,22328.json,942750c33fc97c7f021c4831b61cb617f5cccf24,"@@ -1,69 +1,69 @@
     private long readWord(final int position) {
         if(position < 0) {
             throw new ArrayIndexOutOfBoundsException(position);
         }
 
         // First bit of the word
-        final long firstBitIndex = (position * wordLength);
+        final long firstBitIndex = ((long)position) * ((long)wordLength);
         final int firstByteIndex = (bytePadding + (int)(firstBitIndex / BITS_PER_BYTE));
         final int firstByteSkipBits = (int)(firstBitIndex % BITS_PER_BYTE);
 
         // Last bit of the word
         final long lastBitIndex = (firstBitIndex + wordLength - 1);
         final int lastByteIndex = (bytePadding + (int)(lastBitIndex / BITS_PER_BYTE));
         final int lastByteBitsToConsume;
 
         final int bitsAfterByteBoundary = (int)((lastBitIndex + 1) % BITS_PER_BYTE);
         // If the word terminates at the end of the last byte, consume the whole
         // last byte.
         if(bitsAfterByteBoundary == 0) {
             lastByteBitsToConsume = BITS_PER_BYTE;
         } else {
             // Otherwise, only consume what is necessary.
             lastByteBitsToConsume = bitsAfterByteBoundary;
         }
 
         if(lastByteIndex >= bytes.length) {
             throw new ArrayIndexOutOfBoundsException(""Word out of bounds of backing array."");
         }
 
         // Accumulator
         long value = 0;
 
         // --------------------------------------------------------------------
         // First byte
         final int bitsRemainingInFirstByte = (BITS_PER_BYTE - firstByteSkipBits);
         final int bitsToConsumeInFirstByte = Math.min(bitsRemainingInFirstByte, wordLength);
         long firstByte = (long)bytes[firstByteIndex];
 
         // Mask off the bits to skip in the first byte.
         final long firstByteMask = ((1L << bitsRemainingInFirstByte) - 1L);
         firstByte &= firstByteMask;
         // Right-align relevant bits of first byte.
         firstByte >>>= (bitsRemainingInFirstByte - bitsToConsumeInFirstByte);
 
         value |= firstByte;
 
         // If the first byte contains the whole word, short-circuit.
         if(firstByteIndex == lastByteIndex) {
             return value;
         }
 
         // --------------------------------------------------------------------
         // Middle bytes
         final int middleByteCount = (lastByteIndex - firstByteIndex - 1);
         for(int i=0; i<middleByteCount; i++) {
             final long middleByte = (bytes[firstByteIndex + i + 1] & BYTE_MASK);
             // Push middle byte onto accumulator.
             value <<= BITS_PER_BYTE;
             value |= middleByte;
         }
 
         // --------------------------------------------------------------------
         // Last byte
         long lastByte = (bytes[lastByteIndex] & BYTE_MASK);
         lastByte >>= (BITS_PER_BYTE - lastByteBitsToConsume);
         value <<= lastByteBitsToConsume;
         value |= lastByte;
         return value;
     }",Buggy,"SOLR-7954: Fixed an integer overflow bug in the HyperLogLog code used by the 'cardinality' option of stats.field to prevent ArrayIndexOutOfBoundsException in a distributed search when a large precision is selected and a large number of values exist in each shard

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1697969 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an integer overflow bug in the HyperLogLog code, which could lead to an `ArrayIndexOutOfBoundsException` during distributed search. The overflow occurred when calculating indices into the byte array representing the HyperLogLog data structure, especially with large precision values and a high number of values per shard.

The code diff focuses on the `readWord` method, which reads a word (a sequence of bits) from the byte array. The changes involve casting `position` and `wordLength` to `long` when calculating `firstBitIndex`:

```java
-        final long firstBitIndex = (position * wordLength);
+        final long firstBitIndex = ((long)position) * ((long)wordLength);
```

This change is crucial because if `position * wordLength` exceeds the maximum value of an `int`, it will wrap around, leading to a negative or small positive value for `firstBitIndex`.  This incorrect `firstBitIndex` would then be used to calculate `firstByteIndex`, potentially resulting in a negative or out-of-bounds array access, causing the `ArrayIndexOutOfBoundsException`. By casting to `long` before the multiplication, the overflow is avoided, and the correct index is calculated.

The rest of the code in `readWord` includes checks for negative `position` and out-of-bounds `lastByteIndex`, which are standard error handling. The core fix is the cast to `long` to prevent the integer overflow.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
lucene-solr,21543.json,3c436362810e80ce036d8785eb03abbda2d10014,"@@ -1,42 +1,42 @@
   private static List<TypeMapping> parseTypeMappings(NamedList args) {
     List<TypeMapping> typeMappings = new ArrayList<TypeMapping>();
     List<Object> typeMappingsParams = args.getAll(TYPE_MAPPING_PARAM);
     for (Object typeMappingObj : typeMappingsParams) {
       if (null == typeMappingObj) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param cannot be null"");
       }
       if ( ! (typeMappingObj instanceof NamedList) ) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param must be a <lst>"");
       }
       NamedList typeMappingNamedList = (NamedList)typeMappingObj;
 
       Object fieldTypeObj = typeMappingNamedList.remove(FIELD_TYPE_PARAM);
       if (null == fieldTypeObj) {
         throw new SolrException(SERVER_ERROR,
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       if ( ! (fieldTypeObj instanceof CharSequence)) {
         throw new SolrException(SERVER_ERROR, ""'"" + FIELD_TYPE_PARAM + ""' init param must be a <str>"");
       }
       if (null != typeMappingNamedList.get(FIELD_TYPE_PARAM)) {
         throw new SolrException(SERVER_ERROR,
-            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
+            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> may contain only one '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       String fieldType = fieldTypeObj.toString();
 
       Collection<String> valueClasses
           = FieldMutatingUpdateProcessorFactory.oneOrMany(typeMappingNamedList, VALUE_CLASS_PARAM);
       if (valueClasses.isEmpty()) {
         throw new SolrException(SERVER_ERROR, 
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain at least one '"" + VALUE_CLASS_PARAM + ""' <str>"");
       }
       typeMappings.add(new TypeMapping(fieldType, valueClasses));
 
       if (0 != typeMappingNamedList.size()) {
         throw new SolrException(SERVER_ERROR, 
             ""Unexpected '"" + TYPE_MAPPING_PARAM + ""' init sub-param(s): '"" + typeMappingNamedList.toString() + ""'"");
       }
       args.remove(TYPE_MAPPING_PARAM);
     }
     return typeMappings;
   }",Buggy,"SOLR-4894: fix error message

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1503275 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-4894: fix error message"" indicates that the purpose of this commit is to correct an error message.

The diff shows a change in the error message thrown when a `NamedList` contains more than one `FIELD_TYPE_PARAM`. The original message stated that each `<lst/>` *must* contain a `FIELD_TYPE_PARAM`, which is incorrect since it's already checked for existence earlier. The corrected message states that each `<lst/>` *may* contain only one `FIELD_TYPE_PARAM`. This aligns with the commit message's intention to fix an error message.

Therefore, the changes indicate a bug fix.
**Buggy**
"
lucene-solr,39255.json,33d18a0c599a5bc294f9503a6b8fa3e326f589a7,"@@ -1,114 +1,116 @@
   private void  setInternalDependencyProperties() {
     log(""Loading module dependencies from: "" + moduleDependenciesPropertiesFile, verboseLevel);
     Properties moduleDependencies = new Properties();
     try (InputStream inputStream = new FileInputStream(moduleDependenciesPropertiesFile);
          Reader reader = new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {
       moduleDependencies.load(reader);
     } catch (FileNotFoundException e) {
       throw new BuildException(""Properties file does not exist: "" + moduleDependenciesPropertiesFile.getPath());
     } catch (IOException e) {
       throw new BuildException(""Exception reading properties file "" + moduleDependenciesPropertiesFile.getPath(), e);
     }
     Map<String,SortedSet<String>> testScopeDependencies = new HashMap<>();
     Map<String, String> testScopePropertyKeys = new HashMap<>();
     for (Map.Entry entry : moduleDependencies.entrySet()) {
       String newPropertyKey = (String)entry.getKey();
       StringBuilder newPropertyValue = new StringBuilder();
       String value = (String)entry.getValue();
       Matcher matcher = MODULE_DEPENDENCIES_COORDINATE_KEY_PATTERN.matcher(newPropertyKey);
       if ( ! matcher.matches()) {
         throw new BuildException(""Malformed module dependencies property key: '"" + newPropertyKey + ""'"");
       }
       String antProjectName = matcher.group(1);
       boolean isTest = null != matcher.group(2);
       String artifactName = antProjectToArtifactName(antProjectName);
       newPropertyKey = artifactName + (isTest ? "".internal.test"" : "".internal"") + "".dependencies""; // Add "".internal""
       if (isTest) {
         testScopePropertyKeys.put(artifactName, newPropertyKey);
       }
       if (null == value || value.isEmpty()) {
         allProperties.setProperty(newPropertyKey, """");
         Map<String,SortedSet<String>> scopedDependencies
             = isTest ? testScopeDependencies : internalCompileScopeDependencies;
         scopedDependencies.put(artifactName, new TreeSet<String>());
       } else {
         // Lucene analysis modules' build dirs do not include hyphens, but Solr contribs' build dirs do
         String origModuleDir = antProjectName.replace(""analyzers-"", ""analysis/"");
+        // Exclude the module's own build output, in addition to UNWANTED_INTERNAL_DEPENDENCIES
         Pattern unwantedInternalDependencies = Pattern.compile
-            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
+            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""/"" // require dir separator 
+             + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
         SortedSet<String> sortedDeps = new TreeSet<>();
         for (String dependency : value.split("","")) {
           matcher = SHARED_EXTERNAL_DEPENDENCIES_PATTERN.matcher(dependency);
           if (matcher.find()) {
             String otherArtifactName = matcher.group(1);
             boolean isTestScope = null != matcher.group(2) && matcher.group(2).length() > 0;
             otherArtifactName = otherArtifactName.replace('/', '-');
             otherArtifactName = otherArtifactName.replace(""lucene-analysis"", ""lucene-analyzers"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-solr-"", ""solr-"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-"", ""solr-"");
             if ( ! otherArtifactName.equals(artifactName)) {
               Map<String,Set<String>> sharedDeps
                   = isTest ? interModuleExternalTestScopeDependencies : interModuleExternalCompileScopeDependencies;
               Set<String> sharedSet = sharedDeps.get(artifactName);
               if (null == sharedSet) {
                 sharedSet = new HashSet<>();
                 sharedDeps.put(artifactName, sharedSet);
               }
               if (isTestScope) {
                 otherArtifactName += "":test"";
               }
               sharedSet.add(otherArtifactName);
             }
           }
           matcher = unwantedInternalDependencies.matcher(dependency);
           if (matcher.find()) {
             continue;  // skip external (/(test-)lib/), and non-jar and unwanted (self) internal deps
           }
           String artifactId = dependencyToArtifactId(newPropertyKey, dependency);
           String groupId = ""org.apache."" + artifactId.substring(0, artifactId.indexOf('-'));
           String coordinate = groupId + ':' + artifactId;
           sortedDeps.add(coordinate);
         }
         if (isTest) {  // Don't set test-scope properties until all compile-scope deps have been seen
           testScopeDependencies.put(artifactName, sortedDeps);
         } else {
           internalCompileScopeDependencies.put(artifactName, sortedDeps);
           for (String dependency : sortedDeps) {
             int splitPos = dependency.indexOf(':');
             String groupId = dependency.substring(0, splitPos);
             String artifactId = dependency.substring(splitPos + 1);
             appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, false, false, null, null);
           }
           if (newPropertyValue.length() > 0) {
             newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
           }
           allProperties.setProperty(newPropertyKey, newPropertyValue.toString());
         }
       }
     }
     // Now that all compile-scope dependencies have been seen, include only those test-scope
     // dependencies that are not also compile-scope dependencies.
     for (Map.Entry<String,SortedSet<String>> entry : testScopeDependencies.entrySet()) {
       String module = entry.getKey();
       SortedSet<String> testDeps = entry.getValue();
       SortedSet<String> compileDeps = internalCompileScopeDependencies.get(module);
       if (null == compileDeps) {
         throw new BuildException(""Can't find compile scope dependencies for module "" + module);
       }
       StringBuilder newPropertyValue = new StringBuilder();
       for (String dependency : testDeps) {
         // modules with separate compile-scope and test-scope POMs need their compile-scope deps
         // included in their test-scope deps.
         if (modulesWithSeparateCompileAndTestPOMs.contains(module) || ! compileDeps.contains(dependency)) {
           int splitPos = dependency.indexOf(':');
           String groupId = dependency.substring(0, splitPos);
           String artifactId = dependency.substring(splitPos + 1);
           appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, true, false, null, null);
         }
       }
       if (newPropertyValue.length() > 0) {
         newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
       }
       allProperties.setProperty(testScopePropertyKeys.get(module), newPropertyValue.toString());
     }
   }",Buggy,"LUCENE-6607: Fix spatial3d module's Maven config - include dependency interpolation sites, make packaging jar instead of pom, don't skip deploy phase, etc.; and fix GetMavenDependenciesTask to exclude a module's build artifacts only if the build dir fully matches, rather than a prefix (this bug caused lucene-spatial's test dependency on the lucene-spatial3d jar to be left out of the generated POM, because lucene/build/spatial3d matched the regex for lucene-spatial's build output dir: 'lucene/build/spatial', i.e. with no dir separator)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1690842 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to how the `GetMavenDependenciesTask` excludes a module's build artifacts. The bug caused an incorrect exclusion of the `lucene-spatial3d` jar from the generated POM for `lucene-spatial`.

The code diff modifies the regular expression used to identify unwanted internal dependencies. Specifically, it changes:

```
(""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
```

to:

```
(""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""/"" // require dir separator
             + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
```

The addition of `""/""` enforces that the `origModuleDir` must be followed by a directory separator to be considered a match. This fixes the bug where a prefix match (e.g., `lucene/build/spatial3d` matching `lucene/build/spatial`) would incorrectly exclude the dependency.

The commit message clearly states the bug and the code change directly addresses it by refining the regular expression for excluding internal dependencies.

**Buggy**
"
lucene-solr,46217.json,4cfc012bfa66d31b129495b258b5537663f37a10,"@@ -1,4 +1,7 @@
   public IndexWriterConfig setCodec(Codec codec) {
+    if (codec == null) {
+      throw new NullPointerException();
+    }
     this.codec = codec;
     return this;
   }",Buggy,"fix test bug (and fix IWC to fail immediately if you do this)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4547@1439519 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix test bug (and fix IWC to fail immediately if you do this)"" indicates a bug fix. The code change adds a null check for the `codec` parameter in the `setCodec` method of the `IndexWriterConfig` class. If `codec` is null, a `NullPointerException` is thrown. This change ensures that the `IndexWriterConfig` fails immediately if a null codec is provided, which aligns with the commit message's intention to fix a test bug and improve the failure behavior of `IndexWriterConfig`.

Reasoning:
1. The commit message explicitly mentions ""fix test bug,"" suggesting a bug fix.
2. The code change adds a null check to prevent a potential `NullPointerException` later in the code.
3. The commit message also mentions ""fix IWC to fail immediately if you do this,"" which directly relates to the added null check and the immediate failure it causes.

Based on the commit message and the code change, the changes indicate a bug fix.

**Buggy**"
lucene-solr,49999.json,a14972d7ae2357ab7150909332a04c9a1a94474c,"@@ -1,3 +1,3 @@
     int docId() {
-      return doc > 1 ? NumericDocValues.NO_MORE_DOCS : doc;
+      return doc > 0 ? NumericDocValues.NO_MORE_DOCS : doc;
     }",Buggy,"LUCENE-8055: MemoryIndex.MemoryDocValuesIterator returns 2 documents instead of 1

Fixes a bug if there is a DV field in the MemoryIndex the
`MemoryIndex.MemoryDocValuesIterator` will return 2 documents instead of 1.
",Buggy,"The commit message indicates a bug fix in `MemoryIndex.MemoryDocValuesIterator` where it was returning two documents instead of one. The provided diff shows a change in the `docId()` method. The original code had `doc > 1 ? NumericDocValues.NO_MORE_DOCS : doc`, which would return `NumericDocValues.NO_MORE_DOCS` if `doc` was greater than 1. The corrected code changes this to `doc > 0 ? NumericDocValues.NO_MORE_DOCS : doc`. This change ensures that `NumericDocValues.NO_MORE_DOCS` is returned when `doc` is greater than 0, effectively limiting the iterator to a single document (doc ID 0) before signaling the end. This aligns with the commit message's description of the bug and its fix.

**Buggy**
"
lucene-solr,5155.json,ebd130b7e2720a4a5cfc3c542461f61932caadf0,"@@ -1,44 +1,54 @@
   public Tuple read() throws IOException {
 
     if(finished) {
       Map<String,Object> m = new HashMap<>();
       m.put(""EOF"", true);
       return new Tuple(m);
     } else {
       finished = true;
       Map<String, Object> values = new HashMap<>();
 
-      String plot= stringParams.get(""type"");
-      StreamEvaluator xvalues = evaluatorParams.get(""x"");
-      StreamEvaluator yvalues = evaluatorParams.get(""y"");
+      // add all string based params
+      // these could come from the context, or they will just be treated as straight strings
+      for(Entry<String,String> param : stringParams.entrySet()){
+        if(streamContext.getLets().containsKey(param.getValue())){
+          values.put(param.getKey(), streamContext.getLets().get(param.getValue()));
+        }
+        else{
+          values.put(param.getKey(), param.getValue());
+        }
+      }
 
-      List<Number> y = (List<Number>)yvalues.evaluateOverContext();
-      List<Number> x = null;
+      // add all evaluators
+      for(Entry<String,StreamEvaluator> param : evaluatorParams.entrySet()){
+        values.put(param.getKey(), param.getValue().evaluateOverContext());
+      }
 
-      if(xvalues == null) {
+      List<Number> y = (List<Number>)values.get(""y"");
+      List<Number> x = (List<Number>)values.get(""x"");
+
+      if(x == null) {
         //x is null so add a sequence
         x = new ArrayList();
         for(int i=0; i<y.size(); i++) {
           x.add(i+1);
         }
-      } else {
-        x = (List<Number>) xvalues.evaluateOverContext();
       }
 
       List<List<Number>> xy = new ArrayList();
       for(int i=0; i<x.size(); i++) {
         List<Number> pair = new ArrayList();
         pair.add(x.get(i));
         pair.add(y.get(i));
         xy.add(pair);
       }
 
-      values.put(""plot"", plot);
+      values.put(""plot"", values.get(""type""));
       values.put(""data"", xy);
 
       Tuple tup = new Tuple(values);
       tup.fieldLabels = fieldLabels;
       tup.fieldNames = fieldNames;
       return tup;
     }
   }",Buggy,"SOLR-10802: Fix problem with variable assignment
",Buggy,"The commit message ""SOLR-10802: Fix problem with variable assignment"" suggests a bug fix related to how variables are assigned.

The code diff shows the following changes:

1.  **String Parameter Handling:** The code now iterates through `stringParams` and checks if the value associated with a parameter exists as a ""let"" in the `streamContext`. If it does, the value from the context is used; otherwise, the original string value is used. This suggests a fix related to how string parameters are resolved, potentially addressing an issue where the wrong values were being assigned.
2.  **Evaluator Parameter Handling:** The code iterates through `evaluatorParams` and evaluates each evaluator, putting the result into the `values` map. This is a general way of handling evaluator parameters.
3.  **X and Y Value Retrieval:** The code now retrieves the 'y' and 'x' values from the `values` map, using keys ""y"" and ""x"" respectively.
4.  **Conditional X Value Generation:** The code checks if `x` is null. If it is, it generates a sequence of numbers.
5.  **Plot Value Assignment:** The code now assigns the value of `values.get(""type"")` to the ""plot"" key in the `values` map. This fixes a bug where the value of ""type"" was not being assigned to ""plot"".

The changes strongly suggest a bug fix related to variable assignment, specifically how parameters are resolved and assigned to the `values` map. The original code had hardcoded variable assignments. The updated code uses a more dynamic approach, retrieving values from either the `streamContext` or directly from the parameter values. The final change, assigning `values.get(""type"")` to ""plot"", directly addresses a variable assignment issue.

**Buggy**"
lucene-solr,7768.json,e81dd4e870d2a9b27e1f4366e92daa6dba054da8,"@@ -1,5 +1,6 @@
     public boolean isNodeAlive(String node) {
-      if (zkClientClusterStateProvider != null && zkClientClusterStateProvider.getLiveNodes().contains(node))
-        return true;
+      if (zkClientClusterStateProvider != null) {
+        return zkClientClusterStateProvider.getLiveNodes().contains(node);
+      }
       return true;
     }",Buggy,"SOLR-12977: fixed bug
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-12977: fixed bug"" is a clear indicator that the commit is intended to address a bug. The ""fixed bug"" phrase leaves no room for ambiguity.

**Code Diff Analysis:**

The code diff modifies the `isNodeAlive` method. Here's a breakdown:

*   **Original Code:**

    ```java
    public boolean isNodeAlive(String node) {
      if (zkClientClusterStateProvider != null && zkClientClusterStateProvider.getLiveNodes().contains(node))
        return true;
      return true;
    }
    ```

*   **Modified Code:**

    ```java
    public boolean isNodeAlive(String node) {
      if (zkClientClusterStateProvider != null) {
        return zkClientClusterStateProvider.getLiveNodes().contains(node);
      }
      return true;
    }
    ```

The change involves restructuring the `if` statement. The original code had a potential issue: if `zkClientClusterStateProvider` was *not* null, but `zkClientClusterStateProvider.getLiveNodes().contains(node)` was false, the method would still return `true`. This means the method would incorrectly report a node as alive even if it wasn't in the list of live nodes.

The modified code corrects this. Now, if `zkClientClusterStateProvider` is not null, the method returns the result of `zkClientClusterStateProvider.getLiveNodes().contains(node)`, which is the correct behavior. If `zkClientClusterStateProvider` *is* null, the method returns `true`.

**Relevance and Bug Fix Indication:**

The code change directly addresses a logical error in the original implementation. The original code could return an incorrect result, indicating a bug. The modified code fixes this by ensuring the correct logic is executed. The commit message aligns perfectly with the code change, confirming that a bug was fixed.

**Conclusion:**

**Buggy**
"
lucene-solr,13947.json,9cfba4a728e38a7e6c59c60a377420abc769be46,"@@ -1,39 +1,45 @@
   public static QParser getParser(String qstr, String parserName, boolean allowLocalParams, SolrQueryRequest req) throws SyntaxError {
     // SolrParams localParams = QueryParsing.getLocalParams(qstr, req.getParams());
     if (parserName == null) {
       parserName = QParserPlugin.DEFAULT_QTYPE;//""lucene""
     }
     String stringIncludingLocalParams = qstr;
     ModifiableSolrParams localParams = null;
     SolrParams globalParams = req.getParams();
     boolean valFollowedParams = true;
     int localParamsEnd = -1;
 
     if (allowLocalParams && qstr != null && qstr.startsWith(QueryParsing.LOCALPARAM_START)) {
       localParams = new ModifiableSolrParams();
       localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localParams, globalParams);
 
       String val = localParams.get(QueryParsing.V);
       if (val != null) {
         // val was directly specified in localParams via v=<something> or v=$arg
         valFollowedParams = false;
         //TODO if remainder of query string after '}' is non-empty, then what? Throw error? Fall back to lucene QParser?
       } else {
         // use the remainder of the string as the value
         valFollowedParams = true;
         val = qstr.substring(localParamsEnd);
         localParams.set(QueryParsing.V, val);
       }
 
       parserName = localParams.get(QueryParsing.TYPE,parserName);
-      qstr = localParams.get(""v"");
+      qstr = localParams.get(QueryParsing.V);
     }
 
     QParserPlugin qplug = req.getCore().getQueryPlugin(parserName);
+    if (qplug == null) {
+      // there should a way to include parameter for which parsing failed
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
+          ""invalid query parser '"" + parserName + (stringIncludingLocalParams == null?
+              ""'"": ""' for query '"" + stringIncludingLocalParams + ""'""));
+    }
     QParser parser =  qplug.createParser(qstr, localParams, req.getParams(), req);
 
     parser.stringIncludingLocalParams = stringIncludingLocalParams;
     parser.valFollowedParams = valFollowedParams;
     parser.localParamsEnd = localParamsEnd;
     return parser;
   }",Buggy,"SOLR-13187: Fix NPE when invalid qParser is specified

* When non-existent qParser is specified return 400 error code
* SOLR-13197: Fix NPE when createQParser is called in StatsField
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when an invalid `qParser` is specified. The code diff adds a check for `qplug` being null after attempting to retrieve it from `req.getCore().getQueryPlugin(parserName)`. If `qplug` is null, meaning the specified `qParser` does not exist, the code now throws a `SolrException` with a `BAD_REQUEST` error code, including the invalid parser name in the error message. This prevents the NPE that would have occurred if the code proceeded to call `qplug.createParser()` on a null `qplug`.

Therefore, the code change directly addresses the bug described in the commit message by adding error handling for the case where an invalid `qParser` is specified.

**Buggy**
"
lucene-solr,47147.json,2d422a995af8132e3d39d5ef576216a3105c457a,"@@ -1,9 +1,14 @@
   public void close() throws IOException {
     try (final OutputStream o = os) {
       // We want to make sure that os.flush() was running before close:
       // BufferedOutputStream may ignore IOExceptions while flushing on close().
-      // TODO: this is no longer an issue in Java 8:
-      // http://hg.openjdk.java.net/jdk8/tl/jdk/rev/759aa847dcaf
-      o.flush();
+      // We keep this also in Java 8, although it claims to be fixed there,
+      // because there are more bugs around this! See:
+      // # https://bugs.openjdk.java.net/browse/JDK-7015589
+      // # https://bugs.openjdk.java.net/browse/JDK-8054565
+      if (!flushedOnClose) {
+        flushedOnClose = true; // set this BEFORE calling flush!
+        o.flush();
+      }
     }
   }",Buggy,"LUCENE-6152: Fix double close bug in OutputStreamIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1648724 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-6152: Fix double close bug in OutputStreamIndexOutput"" suggests a bug fix related to closing an `OutputStreamIndexOutput`. The code diff modifies the `close()` method. It introduces a `flushedOnClose` boolean to track whether the stream has been flushed during the close operation. The code now conditionally flushes the output stream only if it hasn't been flushed before. The commit message also mentions issues in Java 8 related to flushing on close, indicating that this change is intended to address potential problems with stream handling. The introduction of `flushedOnClose` and the conditional flush operation strongly suggest a fix for a potential double-close or incorrect flushing scenario.

**Buggy**"
lucene-solr,5763.json,8dddd88d3054596b7afb536429b42792145fdffe,"@@ -1,51 +1,49 @@
   protected void constructStreams() throws IOException {
 
     try {
 
       ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();
       ClusterState clusterState = zkStateReader.getClusterState();
 
       //System.out.println(""Connected to zk an got cluster state."");
 
       Collection<Slice> slices = clusterState.getActiveSlices(this.collection);
 
       if(slices == null) {
-
-        String colLower = this.collection.toLowerCase(Locale.getDefault());
         //Try case insensitive match
         for(String col : clusterState.getCollections()) {
-          if(col.toLowerCase(Locale.getDefault()).equals(colLower)) {
+          if(col.equalsIgnoreCase(this.collection)) {
             slices = clusterState.getActiveSlices(col);
             break;
           }
         }
 
         if(slices == null) {
           throw new Exception(""Collection not found:"" + this.collection);
         }
       }
 
       params.put(""distrib"",""false""); // We are the aggregator.
 
       for(Slice slice : slices) {
         Collection<Replica> replicas = slice.getReplicas();
         List<Replica> shuffler = new ArrayList();
         for(Replica replica : replicas) {
           shuffler.add(replica);
         }
 
         Collections.shuffle(shuffler, new Random());
         Replica rep = shuffler.get(0);
         ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);
         String url = zkProps.getCoreUrl();
         SolrStream solrStream = new SolrStream(url, params);
         if(streamContext != null) {
           solrStream.setStreamContext(streamContext);
         }
         solrStream.setFieldMappings(this.fieldMappings);
         solrStreams.add(solrStream);
       }
     } catch (Exception e) {
       throw new IOException(e);
     }
   }",Buggy,"Fix more locale bugs in SQL handler ans streams
",Buggy,"The commit message indicates a fix for locale bugs in SQL handler and streams. The code diff modifies the way collection names are compared in the `constructStreams` method. Specifically, it changes from using `toLowerCase(Locale.getDefault())` to `equalsIgnoreCase()`. This suggests that the original code was potentially failing to find the correct collection due to locale-specific differences in case conversion. The change to `equalsIgnoreCase()` makes the comparison case-insensitive and locale-independent, which aligns with the commit message's intention to fix locale bugs.

Therefore, the changes indicate a bug fix.

**Buggy**"
lucene-solr,21252.json,ad453aeeda705e120fb53ba5e8c607cb0a13d85d,"@@ -1,10 +1,10 @@
   private void ensureLog() {
     if (tlog == null) {
-      String newLogName = String.format(Locale.ENGLISH, ""%s.%019d"", TLOG_NAME, id);
+      String newLogName = String.format(Locale.ENGLISH, LOG_FILENAME_PATTERN, TLOG_NAME, id);
       try {
         tlog = new TransactionLog(new File(tlogDir, newLogName), globalStrings);
       } catch (IOException e) {
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, ""Can't open new tlog!"", e);
       }
     }
   }",Buggy,"SOLR-3206: fixed the test, will follow up with a root problem fix too.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1297927 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""SOLR-3206: fixed the test, will follow up with a root problem fix too."" indicates that the commit is related to fixing a test. The diff shows a change in the `ensureLog` method where the `newLogName` is formatted using `LOG_FILENAME_PATTERN` instead of a hardcoded format. This change likely aims to fix a test that relies on the log file name format. The commit message explicitly mentions a test fix, and the code change aligns with that. Therefore, the changes indicate a bug fix.

**Buggy**"
lucene-solr,23008.json,da832d4f3aa2e87cf1947ef4373977778ea6d2e0,"@@ -1,30 +1,30 @@
       public Map.Entry<String, String> next() {
         if (!hasNext()) {
           throw new NoSuchElementException();
         }
 
         if (headerValue == null || !headerValue.hasMoreElements()) {
           headerName = headerNameIt.nextElement();
           headerValue = request.getHeaders(headerName);
         }
 
         String key = headerName;
         String val = headerValue.nextElement();
 
-        return new Map.Entry<>() {
+        return new Map.Entry<String, String>() {
           @Override
           public String getKey() {
             return key;
           }
 
           @Override
           public String getValue() {
             return val;
           }
 
           @Override
           public String setValue(String value) {
             throw new UnsupportedOperationException();
           }
         };
       }",Buggy,"SOLR-13434: Fixes problem on Java 8 build
",Buggy,"The commit message ""SOLR-13434: Fixes problem on Java 8 build"" indicates a bug fix related to Java 8.

The diff involves changes within the `next()` method of an iterator, specifically in how it retrieves and returns HTTP header name-value pairs. The change involves explicitly specifying the generic types `<String, String>` for the `Map.Entry` being created.

The original code:
```java
return new Map.Entry<>() { ... };
```

The modified code:
```java
return new Map.Entry<String, String>() { ... };
```

In Java 8, the diamond operator (`<>`) was introduced to infer generic types in certain contexts. However, there might be situations where the compiler struggles to infer the correct types, especially with anonymous classes like the one used here. This could lead to compilation errors or unexpected behavior specifically in Java 8 builds.

By explicitly specifying the generic types `<String, String>`, the code ensures that the `Map.Entry` is created with the correct types, resolving the potential compilation or runtime issues on Java 8. This aligns with the commit message indicating a fix for a Java 8 build problem.

**Buggy**
"
lucene-solr,22004.json,9374fcc6e91c841772fd6d26a7599abcd01dba38,"@@ -1,8 +1,8 @@
   public Query rewrite(IndexReader reader) throws IOException {
     Query rewritten = in.rewrite(reader);
     if (rewritten != in) {
-      return new DeleteByQueryWrapper(in, schema);
+      return new DeleteByQueryWrapper(rewritten, schema);
     } else {
       return this;
     }
   }",Buggy,"LUCENE-5666: fix rewrite bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666@1594418 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5666: fix rewrite bug"" indicates a bug fix related to the rewrite functionality.

The code diff shows a change in the `rewrite` method of a class, likely `DeleteByQueryWrapper`.  The original code `return new DeleteByQueryWrapper(in, schema);` was changed to `return new DeleteByQueryWrapper(rewritten, schema);`. This suggests that the original code was incorrectly using the original query `in` instead of the rewritten query `rewritten` when constructing the `DeleteByQueryWrapper`. This is a logical correction, as the purpose of rewriting a query is to optimize or transform it before execution. Failing to use the rewritten query would defeat this purpose and potentially lead to incorrect results.

The change directly addresses the bug mentioned in the commit message by ensuring that the rewritten query is used in the `DeleteByQueryWrapper`.

**Buggy**
"
lucene-solr,12659.json,30de6c512ae51391159b28e0250a9482f12690c7,"@@ -1,23 +1,24 @@
   public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
     if (SortingMergePolicy.isSorted(context.reader(), sort)) {
       // segment is sorted, can early-terminate
       return new FilterLeafCollector(super.getLeafCollector(context)) {
+        private int numCollected;
 
         @Override
         public void collect(int doc) throws IOException {
           super.collect(doc);
           if (++numCollected >= numDocsToCollect) {
             throw new CollectionTerminatedException();
           }
         }
 
         @Override
         public boolean acceptsDocsOutOfOrder() {
           return false;
         }
 
       };
     } else {
       return super.getLeafCollector(context);
     }
   }",Buggy,"LUCENE-5623: fix bug in earlyterminatingcollector, fix test to be reproducible and more evil

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1588953 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-5623: fix bug in earlyterminatingcollector, fix test to be reproducible and more evil"" indicates a bug fix in the `earlyterminatingcollector`.

The code diff shows a modification within the `getLeafCollector` method. Specifically, when the segment is sorted, a `FilterLeafCollector` is returned. Inside this collector, the `collect` method increments `numCollected` and throws a `CollectionTerminatedException` when `numCollected` reaches `numDocsToCollect`. This mechanism seems to be related to early termination. The addition of `numCollected` and the conditional termination suggest a fix to ensure the collector terminates at the correct time. The `acceptsDocsOutOfOrder` method returning `false` further supports the early termination logic.

Based on the commit message and the code changes, the changes indicate a bug fix related to early termination logic.

**Buggy**
"
jna,352.json,af1ef9330f4ef7d4f3618d5c4edc8e49b2b4862a,"@@ -1,5 +1,5 @@
-    public static final String getPrivateProfileString(final String appName, final String keyName, final String defaultValue, final String fileName) {
+    public static final String getPrivateProfileString(final String lpAppName, final String lpKeyName, final String lpDefault, final String lpFileName) {
         final char buffer[] = new char[1024];
         Kernel32.INSTANCE.GetPrivateProfileString(appName, keyName, defaultValue, buffer, new DWORD(buffer.length), fileName);
         return Native.toString(buffer);
     }",Buggy,"fix javadoc errors/warnings
",NotBuggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""fix javadoc errors/warnings"" indicates that the changes are primarily focused on improving the documentation of the code, specifically addressing errors or warnings generated during Javadoc processing. This suggests that the changes are likely cosmetic or related to documentation clarity, rather than functional bug fixes.

**Code Diff Analysis:**

The code diff shows a change in the parameter names of the `getPrivateProfileString` method. Specifically, `appName` is changed to `lpAppName`, `keyName` to `lpKeyName`, and `defaultValue` to `lpDefault`.

**Reasoning:**

The change in parameter names strongly suggests that this commit is related to improving the Javadoc. The `lp` prefix is a common convention (likely inherited from Windows API naming) and is being applied to the parameter names. This change would likely resolve Javadoc warnings related to parameter names not matching the expected format or style. The code's functionality remains the same. The core logic of calling `Kernel32.INSTANCE.GetPrivateProfileString` and converting the result to a String is unchanged. The parameter name changes are purely cosmetic and do not address any underlying bug in the code's functionality.

**Conclusion:**

**NotBuggy**
"
jna,3664.json,fce337b823bcc6130313cd71740b053abf681368,"@@ -1,7 +1,7 @@
     public void read() {
         boolean returnWide = original instanceof WString[];
         for (int si=0;si < original.length;si++) {
             String s = getPointer(si * Pointer.SIZE).getString(0, wide);
-            original[si] = returnWide ? new WString(s) : s; 
+            original[si] = returnWide ? new WString(s) : (Object)s; 
         }
     }",Buggy,"Fix 1.4 compile error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@294 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message ""Fix 1.4 compile error"" suggests that the change addresses a compilation issue in Java version 1.4.

The diff shows a modification in the line `original[si] = returnWide ? new WString(s) : s;`. It's changed to `original[si] = returnWide ? new WString(s) : (Object)s;`.

In Java 1.4, the ternary operator had stricter type-checking rules. When the two possible results of the ternary operator had different types, the compiler sometimes struggled to find a common type. In this case, `new WString(s)` and `s` (a `String`) are different types. By explicitly casting `s` to `(Object)`, the code provides a common supertype, resolving the compilation error in Java 1.4. This is a workaround for a type inference limitation in older Java compilers. The original code likely caused a compile-time error in Java 1.4.

Therefore, the change directly addresses a bug related to compilation errors in older Java versions.

**Buggy**
"
jna,3945.json,563931750bb0f5af89da6d55030bdfd4c8854906,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return callFlags + options.hashCode() + peer.hashCode();
+        return callFlags + options.hashCode() + super.hashCode();
     }",Buggy,"fix compiler error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@1143 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message ""fix compiler error"" suggests that the commit aims to resolve a compilation issue. The diff shows a change in the `hashCode()` method where `peer.hashCode()` is replaced with `super.hashCode()`.

Reasoning:

1.  **Compiler Error Indication:** The commit message explicitly mentions fixing a compiler error. This strongly suggests that the original code (`peer.hashCode()`) was causing a compilation problem.
2.  **`peer` vs. `super`:** The change from `peer.hashCode()` to `super.hashCode()` is significant. If `peer` was not properly defined or accessible in the context of the `hashCode()` method, it could indeed lead to a compiler error. Replacing it with `super.hashCode()` would call the `hashCode()` method of the superclass, which is a valid operation and likely resolves the compilation issue.
3.  **Bug Fix:** Compiler errors are bugs. Fixing a compiler error is, by definition, a bug fix.

Conclusion:

**Buggy**
"
jna,4035.json,40f00b1dcba4c24b37452d05cb3118bdbc1480f3,"@@ -1,29 +1,31 @@
     public void setValue(long value) {
         long truncated = value;
         switch (size) {
         case 1:
             truncated = (byte) value;
             this.value = new Byte((byte) value);
             break;
         case 2:
             truncated = (short) value;
             this.value = new Short((short) value);
             break;
         case 4:
             truncated = (int) value;
             this.value = new Integer((int) value);
             break;
         case 8:
             this.value = new Long(value);
             break;
         default:
             throw new IllegalArgumentException(""Unsupported size: "" + size);
         }
-        long mask = (-1L >> size * 8) << (size * 8);
-        if ((value < 0 && truncated != value)
-            || (value >= 0 && (mask & value) != 0)) {
-            throw new IllegalArgumentException(""Argument (0x""
-                + Long.toHexString(value) + "") exceeds native capacity (""
-                + size + "" bytes)"");
+        if (size < 8) {
+            long mask = ~((1L << (size*8)) - 1);
+            if ((value < 0 && truncated != value)
+                    || (value >= 0 && (mask & value) != 0)) {
+                throw new IllegalArgumentException(""Argument value 0x""
+                        + Long.toHexString(value) + "" exceeds native capacity (""
+                        + size + "" bytes) mask=0x"" + Long.toHexString(mask));
+            }
         }
     }",Buggy,"Fix bug checking IntegerType limits

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@391 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message ""Fix bug checking IntegerType limits"" suggests a bug fix related to how integer type limits are checked.

The code diff modifies the `setValue` method, specifically the part that checks if the provided `value` exceeds the native capacity based on the `size`.

The original code had a potentially incorrect mask calculation: `long mask = (-1L >> size * 8) << (size * 8);`. This calculation might not produce the correct mask for all sizes, especially when `size` is close to 8.

The modified code introduces a condition `if (size < 8)` and calculates the mask as `long mask = ~((1L << (size*8)) - 1);`. This new calculation appears to be a more accurate way to determine the mask for checking the limits of integer types with sizes less than 8 bytes. The exception message is also updated to include the mask value, which can be helpful for debugging.

The change in mask calculation and the conditional check based on `size` strongly suggest that the original code had a bug in checking the limits of `IntegerType`. The commit message accurately reflects this bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**
"
spring-framework,28571.json,582864802e49f0cc47575be00194e4194d7b1dc5,"@@ -1,8 +1,11 @@
 	public ModelMap getModel() {
 		if (useDefaultModel()) {
 			return this.defaultModel;
 		}
 		else {
-			return (this.redirectModel != null) ? this.redirectModel : new ModelMap();
+			if (this.redirectModel == null) {
+				this.redirectModel = new ModelMap();
+			}
+			return this.redirectModel;
 		}
 	}",Buggy,"Ensure RedirectModel is initialized

This commit fixes an old bug in ModelAndViewContainer where getModel
returns a new ModelMap instance that isn't saved and re-used.

Issue: SPR-14045
",Buggy,"The commit message indicates a bug fix related to the initialization of `RedirectModel` in `ModelAndViewContainer`. It states that `getModel` was returning a new `ModelMap` instance that wasn't being saved and reused, leading to a bug. The provided diff shows that the code was modified to ensure that `this.redirectModel` is initialized if it's null before being returned. This confirms that the change is indeed a bug fix, as it addresses the issue of the `ModelMap` not being properly initialized and reused.

**Buggy**
"
spring-framework,5833.json,ee95f171c8a934080421e802fa6868843af1c45f,"@@ -1,4 +1,4 @@
 	public PropertySources getAppliedPropertySources() throws IllegalStateException {
-		Assert.state(this.appliedPropertySources != null, ""PropertySources have not get been applied"");
+		Assert.state(this.appliedPropertySources != null, ""PropertySources have not yet been applied"");
 		return this.appliedPropertySources;
 	}",Buggy,"Update PropertySourcesPlaceholderConfigurer.java

Fix the error message when PropertySources are not applied",Buggy,"The commit message indicates an update to `PropertySourcesPlaceholderConfigurer.java` to fix an error message when `PropertySources` are not applied.

The diff shows a change in the error message from ""PropertySources have not get been applied"" to ""PropertySources have not yet been applied"". This is a minor text correction to improve the clarity and correctness of the error message.

The change directly addresses the issue described in the commit message. The original message had a grammatical error (""get been""), which has been corrected. This improves the user experience by providing a clearer and more understandable error message. This is a bug fix because the original message was grammatically incorrect.

**Buggy**"
spring-framework,20130.json,7e799295e55f5e5984a1040a016169ea0883bc2d,"@@ -1,10 +1,10 @@
 	public HttpInputMessage beforeBodyRead(HttpInputMessage request, MethodParameter parameter,
-			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
+			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) throws IOException {
 
 		for (RequestBodyAdvice advice : getMatchingAdvice(parameter, RequestBodyAdvice.class)) {
 			if (advice.supports(parameter, targetType, converterType)) {
 				request = advice.beforeBodyRead(request, parameter, targetType, converterType);
 			}
 		}
 		return request;
 	}",Buggy,"Fix compile error
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix compile error"" strongly suggests that the change addresses a compilation issue. Compilation errors typically arise from syntax errors, type mismatches, missing imports, or other code-level problems that prevent the code from being compiled into executable form.

2.  **Git Diff Analysis:** The diff shows a change in the `beforeBodyRead` method signature. Specifically, it adds `throws IOException` to the method declaration.

3.  **Connecting the Dots:** Adding `throws IOException` to a method signature is a common fix for a compile-time error related to unhandled exceptions. If the method body (which is not shown in the diff, but we can infer) performs operations that can throw an `IOException`, and the method doesn't declare that it throws the exception, the compiler will flag an error. The fix is to either handle the exception within the method (using a `try-catch` block) or declare that the method throws the exception, pushing the responsibility of handling it to the calling method.

4.  **Conclusion:** The commit message indicates a compilation error, and the diff shows a change that is a standard solution for unhandled exception compile errors. Therefore, the changes indicate a bug fix.

**Buggy**"
spring-framework,24540.json,f084b632864685d8e65d0cdfa719a813824fc59f,"@@ -1,31 +1,36 @@
 	private static String decodeHeaderFieldParam(String input) {
 		Assert.notNull(input, ""Input String should not be null"");
 		int firstQuoteIndex = input.indexOf('\'');
 		int secondQuoteIndex = input.indexOf('\'', firstQuoteIndex + 1);
 		// US_ASCII
 		if (firstQuoteIndex == -1 || secondQuoteIndex == -1) {
 			return input;
 		}
 		Charset charset = Charset.forName(input.substring(0, firstQuoteIndex));
 		Assert.isTrue(UTF_8.equals(charset) || ISO_8859_1.equals(charset),
 				""Charset should be UTF-8 or ISO-8859-1"");
 		byte[] value = input.substring(secondQuoteIndex + 1, input.length()).getBytes(charset);
 		ByteArrayOutputStream bos = new ByteArrayOutputStream();
 		int index = 0;
 		while (index < value.length) {
 			byte b = value[index];
 			if (isRFC5987AttrChar(b)) {
 				bos.write((char) b);
 				index++;
 			}
-			else if (b == '%') {
-				char[] array = { (char)value[index + 1], (char)value[index + 2]};
-				bos.write(Integer.parseInt(String.valueOf(array), 16));
+			else if (b == '%' && index < value.length - 2) {
+				char[] array = new char[]{(char) value[index + 1], (char) value[index + 2]};
+				try {
+					bos.write(Integer.parseInt(String.valueOf(array), 16));
+				}
+				catch (NumberFormatException ex) {
+					throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT, ex);
+				}
 				index+=3;
 			}
 			else {
-				throw new IllegalArgumentException(""Invalid header field parameter format (as defined in RFC 5987)"");
+				throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT);
 			}
 		}
 		return new String(bos.toByteArray(), charset);
 	}",Buggy,"Fix ""array index out of bounds"" problem reported by LGTM.com
",Buggy,"The commit message indicates a fix for an ""array index out of bounds"" problem. The code diff modifies the `decodeHeaderFieldParam` method.

Specifically, the change adds a check `index < value.length - 2` before accessing `value[index + 1]` and `value[index + 2]`. This check prevents an `ArrayIndexOutOfBoundsException` when the input string ends with a '%' character, and there aren't enough characters following it to form a valid hexadecimal representation. Additionally, a try-catch block is added to handle `NumberFormatException` which might occur during parsing of the hexdecimal representation. The exception is rethrown as an `IllegalArgumentException` with a more descriptive message.

The original code would throw an `ArrayIndexOutOfBoundsException` if the input string ended with a '%' character because the code would attempt to access `value[index + 1]` and `value[index + 2]` even if they were outside the bounds of the array. The added check and exception handling effectively address this issue.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
sonarqube,18348.json,9b57d0b379935e053ef050b0379c65fc32810bc1,"@@ -1,3 +1,3 @@
   public boolean isEnabled() {
-    return getRule().isEnabled();
+    return getRule()!=null && getRule().isEnabled();
   }",Buggy,"Fix bug on deprecated profiles
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix bug on deprecated profiles"" clearly indicates a bug fix related to deprecated profiles.

2.  **Code Diff:** The code diff shows a change in the `isEnabled()` method. The original code `return getRule().isEnabled();` directly calls `isEnabled()` on the result of `getRule()`. The modified code `return getRule()!=null && getRule().isEnabled();` adds a null check before calling `isEnabled()`.

3.  **Bug Fix Inference:** The addition of the null check suggests that the original code could throw a `NullPointerException` if `getRule()` returned `null`. This scenario likely occurred when dealing with deprecated profiles (as stated in the commit message), where the rule might not be properly initialized or might have been removed. The null check prevents the exception, thus fixing a bug.

4.  **Alignment:** The code change directly addresses the issue described in the commit message. The bug was likely a `NullPointerException` occurring when `getRule()` returned null, and the fix is the added null check.

**Conclusion:**

**Buggy**
"
sonarqube,4866.json,56963334491068e70db0695c7fc36d9c40bfca9a,"@@ -1,6 +1,7 @@
   private void checkNoOtherMetricWithTargetKey(DbSession dbSession, MetricDto metricInDb, MetricDto template) {
-    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, template.getKey());
+    String targetKey = template.getKey();
+    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, targetKey);
     if (isMetricFoundInDb(metricWithTargetKey) && !metricInDb.getId().equals(metricWithTargetKey.getId())) {
-      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, ""A me metric exists with the key: "" + metricInDb.getKey());
+      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, String.format(""The key '%s' is already used by an existing metric."", targetKey));
     }
   }",Buggy,"SONAR-6572 WS metrics/update fix error message when updating with an existing key
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** ""SONAR-6572 WS metrics/update fix error message when updating with an existing key""

    *   The commit message explicitly states that it's a ""fix"" related to the error message displayed when attempting to update a metric with a key that already exists. This strongly suggests a bug fix. The original error message was not informative enough.

2.  **Git Diff:**

    *   The code change is within the `checkNoOtherMetricWithTargetKey` method. This method likely validates that a metric key being used for an update isn't already in use by another metric.
    *   The original code constructs the error message using `metricInDb.getKey()`. This is problematic because the error is triggered by `template.getKey()` conflicting with an existing key.
    *   The modified code stores `template.getKey()` in a variable `targetKey` and uses `targetKey` in the error message. The error message is also changed to be more descriptive.

**Reasoning:**

The commit message and the code diff are highly correlated. The commit message indicates a fix to an error message, and the diff shows the error message being updated to provide more context about the conflicting key. The original code used the wrong key in the error message, which is clearly a bug. The corrected code uses the correct key and provides a more informative message, resolving the bug.

**Conclusion:**

**Buggy**
"
sonarqube,4786.json,7e1ea5d43a0fd5231baddfc22bc4c76209622968,"@@ -1,14 +1,12 @@
   public void define(WebService.NewController controller) {
     WebService.NewAction action = controller.createAction(""uninstall"")
       .setPost(true)
       .setDescription(""Uninstalls the plugin specified by its key."" +
         ""<br/>"" +
-        ""Plugin information is retrieved from Update Center."" +
-        ""<br/>"" +
-        ""Requires user to be authenticated with Administer System permissions"")
+        ""Requires user to be authenticated with Administer System permissions."")
       .setHandler(this);
 
     action.createParam(PARAM_KEY)
       .setDescription(""The key identifying the plugin to uninstall"")
       .setRequired(true);
   }",Buggy,"SONAR-6380 fix error in action description
",Buggy,"The commit message ""SONAR-6380 fix error in action description"" suggests a correction to the description of a web service action.

The diff shows a change in the `setDescription` method's argument. Specifically, the line ""Plugin information is retrieved from Update Center."" and the following ""<br/>"" tag have been removed. This aligns perfectly with the commit message, indicating a fix to an error in the action description. It's a minor textual correction, but it directly addresses the issue described in the commit message.

**Buggy**
"
sonarqube,15877.json,51f8fa984e4a2c11ededba97e6b2c0a2fe29bb02,"@@ -1,8 +1,8 @@
   public String toString() {
     return Objects.toStringHelper(this)
       .add(""key"", qpKey)
       .add(""name"", qpName)
       .add(""language"", languageKey)
-      .add(""rulesUpdatedAt"", rulesUpdatedAt)
+      .add(""rulesUpdatedAt"", rulesUpdatedAt.getTime())
       .toString();
   }",Buggy,"fix some coverage flaws
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix some coverage flaws"" suggests that the changes are aimed at improving code coverage. This could involve fixing issues that prevented certain parts of the code from being tested or reached during testing. It doesn't explicitly state a bug fix, but improving coverage often reveals underlying bugs.

**Git Diff Analysis:**

The diff shows a change in the `toString()` method. Specifically, the `rulesUpdatedAt` field, which was previously passed directly to `Objects.toStringHelper`, is now being passed as `rulesUpdatedAt.getTime()`. This implies that `rulesUpdatedAt` is likely a `java.util.Date` or `java.time.Instant` object (or similar), and the `getTime()` method is being called to get the timestamp as a long value.

**Reasoning:**

1.  **Data Type Mismatch:** The original code likely had a data type mismatch. `Objects.toStringHelper` might not have been able to handle the `rulesUpdatedAt` object directly in a way that produced a meaningful string representation for the coverage tools. By calling `getTime()`, the code is now providing a long value (timestamp), which is a more standard and predictable data type for string conversion.

2.  **Coverage Improvement:** This change likely improves code coverage because the `toString()` method is now more likely to execute without errors or unexpected behavior. If the original code threw an exception or produced an unreadable string, it could have prevented coverage tools from properly analyzing the code.

3.  **Potential Bug:** The original code's failure to properly handle the `rulesUpdatedAt` object in the `toString()` method could be considered a bug. While it might not have caused a critical application error, it could have led to incorrect or incomplete information in logs or debugging output, and it definitely hindered test coverage.

**Conclusion:**

The change addresses a potential issue in how the `rulesUpdatedAt` field was being handled in the `toString()` method, likely improving code coverage. This suggests that the original code had a flaw that needed to be corrected.

**Buggy**
"
sonarqube,20277.json,5e5fc1731d0e99ab4a457fb9eeee121f342d1f33,"@@ -1,7 +1,7 @@
   static String[] getListFromProperty(Map<String, String> properties, String key) {
     String propValue = properties.get(key);
     if (propValue != null) {
-      return DefaultConfiguration.parseAsCsv(ProjectDefinition.SOURCES_PROPERTY, propValue);
+      return DefaultConfiguration.parseAsCsv(key, propValue);
     }
     return new String[0];
   }",Buggy,"SONAR-10122 fix misleading error message for multivalue properties

when parsing fails in ProjectReactor
it concerns only sonar.sources, sonar.tests and sonar.modules
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-10122 fix misleading error message for multivalue properties"" clearly states that the purpose of the commit is to correct a misleading error message related to multivalue properties. The message also specifies that the issue occurs during parsing in ProjectReactor and affects `sonar.sources`, `sonar.tests`, and `sonar.modules` properties.

**Git Diff Analysis:**

The diff shows a change in the `getListFromProperty` method. Specifically, the first argument to `parseAsCsv` is changed from `ProjectDefinition.SOURCES_PROPERTY` to `key`.

**Reasoning:**

The original code was using `ProjectDefinition.SOURCES_PROPERTY` as the property key when calling `parseAsCsv`, regardless of which property (`sonar.sources`, `sonar.tests`, or `sonar.modules`) was actually being parsed. This would lead to a misleading error message if the parsing failed for `sonar.tests` or `sonar.modules`, as the error message would incorrectly refer to `sonar.sources`.

The corrected code now passes the actual property key (`key`) to `parseAsCsv`. This ensures that the error message will correctly identify the property that caused the parsing failure, thus fixing the misleading error message.

The commit message aligns perfectly with the code change. The code change directly addresses the issue described in the commit message. The change corrects an error in how the property key is passed to the parsing function, which directly relates to the misleading error message.

**Conclusion:**

The changes indicate a bug fix. The original code had a flaw that led to misleading error messages, and the corrected code resolves this issue.

**Buggy**
"
sonarqube,1092.json,e1ecfa7d65c33df5ee054e57e2b5d696da038154,"@@ -1,12 +1,10 @@
   public TokenQueue chunk(Reader reader) {
-    CodeReaderConfiguration codeReaderConfiguration = new CodeReaderConfiguration();
-    codeReaderConfiguration.setBufferCapacity(BUFFER_CAPACITY);
-    CodeReader code = new CodeReader(reader, codeReaderConfiguration);
+    CodeReader code = new CodeReader(reader);
     TokenQueue queue = new TokenQueue();
     try {
       channelDispatcher.consume(code, queue);
       return queue;
     } catch (Exception e) {
       throw new DuplicationsException(""Unable to lex source code at line : "" + code.getLinePosition() + "" and column : "" + code.getColumnPosition(), e);
     }
   }",Buggy,"SONAR-2923 and SONAR-2632: Fixed some side-effect compilations error in other modules
",Buggy,"The commit message indicates a fix for side-effect compilation errors in other modules, related to SONAR-2923 and SONAR-2632. The code diff shows a change in the `chunk` method where the `CodeReaderConfiguration` is no longer explicitly created and configured. Instead, a `CodeReader` is directly instantiated with the `reader`. Also, the `CodeReaderConfiguration` is not used anymore.

The removal of the `CodeReaderConfiguration` and its associated settings suggests a simplification or correction in how the `CodeReader` is initialized. This could be a fix for a configuration issue that was causing compilation errors in other modules, as stated in the commit message. The exception handling remains the same, so there's no direct change related to error handling.

Given the commit message and the code changes, it's likely that the changes address a bug related to the configuration of the `CodeReader` that was causing compilation issues.

**Buggy**"
voldemort,8473.json,a8fa1f7a2beb35aa81fc82167eb061901506d6ac,"@@ -1,21 +1,17 @@
     public static long copyLarge(Reader input, Writer output, long limit) throws IOException {
         char[] buffer = new char[DEFAULT_BUFFER_SIZE];
         long count = 0;
         int n = 0;
         long remaining = limit;
         while(remaining > 0) {
-            if(remaining > DEFAULT_BUFFER_SIZE) {
-                n = input.read(buffer);
-            } else {
-                char[] remainingbuffer = new char[(int) remaining];
-                n = input.read(remainingbuffer);
-            }
+            n = (remaining > DEFAULT_BUFFER_SIZE) ? input.read(buffer)
+                                                 : input.read(buffer, 0, (int) remaining);
             if(n == -1) {
                 break;
             }
             output.write(buffer, 0, n);
             count += n;
             remaining -= n;
         }
         return count;
     }",Buggy,"fix bug in last read
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""fix bug in last read"" strongly suggests that the commit addresses a bug related to the final read operation within the `copyLarge` method. This implies that there was an issue with how the code handled the last portion of the input when a `limit` was specified.

**2. Code Diff Analysis:**

The code diff focuses on the `while` loop condition `remaining > 0` and the read operation within it. The original code had a conditional block:

```java
if(remaining > DEFAULT_BUFFER_SIZE) {
    n = input.read(buffer);
} else {
    char[] remainingbuffer = new char[(int) remaining];
    n = input.read(remainingbuffer);
}
```

This code attempts to read either a full buffer or a smaller buffer if the remaining amount to read is less than the default buffer size. The problem with the original code is that when `remaining < DEFAULT_BUFFER_SIZE`, it creates a new buffer `remainingbuffer` with size `remaining` and reads into it. However, the `output.write` call still uses the original `buffer` and writes `n` characters from it, which is incorrect. The new `remainingbuffer` is never used for writing.

The corrected code simplifies this logic with a ternary operator:

```java
n = (remaining > DEFAULT_BUFFER_SIZE) ? input.read(buffer)
                                     : input.read(buffer, 0, (int) remaining);
```

This version always reads into the `buffer`, but when `remaining` is less than `DEFAULT_BUFFER_SIZE`, it uses the three-argument `read` method to read only up to `remaining` characters into the `buffer`. This ensures that the `output.write` call uses the correct number of characters read from the input.

**3. Reasoning:**

The commit message explicitly states a bug fix. The code diff shows a change in how the last read operation is handled when a limit is specified. The original code had a logical flaw where it created a new buffer but didn't use it for writing, leading to incorrect output. The corrected code addresses this flaw by using the three-argument `read` method to read only the remaining characters into the original buffer. This aligns with the commit message and indicates a bug fix.

**Conclusion:**

**Buggy**
"
voldemort,10824.json,6980fd513d4c6545813e2294f5ac10afaa3bcbb6,"@@ -1,26 +1,29 @@
     public HintedHandoffStrategy updateHintedHandoffStrategy(StoreDefinition storeDef,
                                                              Cluster cluster) {
         if(HintedHandoffStrategyType.CONSISTENT_STRATEGY.toDisplay()
                                                         .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                            .toDisplay()) == 0) {
             Integer hintPrefListSize = storeDef.getHintPrefListSize();
+
+            // Default value for hint pref list size = replication factor
             if(null == hintPrefListSize) {
-                if(cluster.getNumberOfNodes() > 6)
-                    hintPrefListSize = cluster.getNumberOfNodes() / 2;
+                if(cluster.getNumberOfNodes() == storeDef.getReplicationFactor())
+                    hintPrefListSize = storeDef.getReplicationFactor() - 1;
                 else
-                    hintPrefListSize = cluster.getNumberOfNodes();
+                    hintPrefListSize = storeDef.getReplicationFactor();
             }
+
             return new ConsistentHandoffStrategy(cluster,
                                                  hintPrefListSize,
                                                  enableZoneRouting,
                                                  clientZoneId);
         } else if(HintedHandoffStrategyType.TO_ALL_STRATEGY.toDisplay()
                                                            .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                               .toDisplay()) == 0) {
             return new HandoffToAllStrategy(cluster, enableZoneRouting, clientZoneId);
         } else {
             throw new VoldemortException(""HintedHandoffStrategyType:""
                                          + storeDef.getHintedHandoffStrategyType()
                                          + "" not handled by "" + this.getClass());
         }
     }",Buggy,"Fixed another bug in Consistent strategy + Updated stores.xml so as to get EndToEnd Test running
",Buggy,"The commit message indicates a bug fix in the Consistent strategy. The code changes modify the logic for determining the `hintPrefListSize` when it's not explicitly defined in the `storeDef`. Specifically, the condition `cluster.getNumberOfNodes() > 6` has been changed to `cluster.getNumberOfNodes() == storeDef.getReplicationFactor()`. Also, the value assigned to `hintPrefListSize` when the condition is true has been changed from `cluster.getNumberOfNodes() / 2` to `storeDef.getReplicationFactor() - 1`. These changes suggest a correction to the original logic, implying a bug fix.

**Buggy**
"
voldemort,9755.json,97bfa6751eba8afb13bfc801afa47d2b6bd9af1d,"@@ -1,3 +1,3 @@
     public double get99thWaitTimeMs() {
-        return this.histogramWaitMs.getQuantile(0.99);
+        return (double) (this.histogramWaitNs.getQuantile(0.99)) / Time.NS_PER_MS;
     }",Buggy,"Fixed bug and verified monitoring feature of q99th wait time
",Buggy,"The commit message indicates a bug fix related to the 99th percentile wait time and verification of a monitoring feature. The code change modifies the calculation of `get99thWaitTimeMs()`. It appears the original code was directly returning a value from `histogramWaitMs`, while the corrected code retrieves the value from `histogramWaitNs` and divides it by `Time.NS_PER_MS`. This suggests that the original code was likely returning a value in nanoseconds instead of milliseconds, which is a bug. The change ensures the returned value is in milliseconds as intended.

Reasoning:
1. The commit message explicitly mentions ""Fixed bug"".
2. The code change involves a calculation related to time units (nanoseconds to milliseconds).
3. The original code likely had an incorrect unit, leading to misinterpretation of the 99th percentile wait time.
4. The corrected code addresses this unit mismatch, indicating a bug fix.

Conclusion: **Buggy**"
voldemort,9005.json,c07b777533cdbdfe4042258dbf44f2d9d51e3bc0,"@@ -1,286 +1,283 @@
     public void run() {
         Object message = messageEvent.getMessage();
         if(message instanceof CoordinatorStoreClientRequest) {
             CoordinatorStoreClientRequest storeClientRequestObject = (CoordinatorStoreClientRequest) message;
             this.requestObject = storeClientRequestObject.getRequestObject();
             this.storeClient = storeClientRequestObject.getStoreClient();
 
             // This shouldn't ideally happen.
             if(this.requestObject != null) {
 
                 switch(requestObject.getOperationType()) {
                     case VoldemortOpCode.GET_METADATA_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET Metadata request received."");
                         }
 
                         try {
 
                             String queryStoreName = ByteUtils.getString(this.requestObject.getKey()
                                                                                           .get(),
                                                                         ""UTF-8"");
                             StoreDefinition storeDef = StoreDefinitionUtils.getStoreDefinitionWithName(this.coordinatorMetadata.getStoresDefs(),
                                                                                                        queryStoreName);
                             String serializerInfoXml = RestUtils.constructSerializerInfoXml(storeDef);
                             GetMetadataResponseSender metadataResponseSender = new GetMetadataResponseSender(messageEvent,
                                                                                                              serializerInfoXml.getBytes());
 
                             metadataResponseSender.sendResponse(this.coordinatorPerfStats,
                                                                 true,
                                                                 this.requestObject.getRequestOriginTimeInMs());
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET Metadata successful !"");
                             }
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET METADATA request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET request received."");
                         }
 
                         try {
                             boolean keyExists = false;
                             List<Versioned<byte[]>> versionedValues = this.storeClient.getWithCustomTimeout(this.requestObject);
                             if(versionedValues == null || versionedValues.size() == 0) {
                                 if(this.requestObject.getValue() != null) {
                                     if(versionedValues == null) {
                                         versionedValues = new ArrayList<Versioned<byte[]>>();
                                     }
                                     versionedValues.add(this.requestObject.getValue());
                                     keyExists = true;
 
                                 }
                             } else {
                                 keyExists = true;
                             }
 
                             if(keyExists) {
                                 GetResponseSender responseConstructor = new GetResponseSender(messageEvent,
                                                                                               requestObject.getKey(),
                                                                                               versionedValues,
                                                                                               this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET successful !"");
                                 }
 
                             } else {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key does not exist"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_ALL_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET ALL request received."");
                         }
 
                         try {
                             Map<ByteArray, List<Versioned<byte[]>>> versionedResponses = this.storeClient.getAllWithCustomTimeout(this.requestObject);
                             if(versionedResponses == null
                                || versionedResponses.values().size() == 0) {
                                 logger.error(""Error when doing getall. Keys do not exist."");
 
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Error when doing getall. Keys do not exist."");
                             } else {
                                 GetAllResponseSender responseConstructor = new GetAllResponseSender(messageEvent,
                                                                                                     versionedResponses,
                                                                                                     this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET ALL successful !"");
                                 }
 
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET ALL request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     // TODO: Implement this in the next pass
                     case VoldemortOpCode.GET_VERSION_OP_CODE:
 
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming get version request"");
                         }
 
                         try {
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET versions request successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET VERSION request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getVersionErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getVersionErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.PUT_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""PUT request received."");
                         }
 
                         try {
                             VectorClock successfulPutVC = null;
 
                             if(this.requestObject.getValue() != null) {
                                 successfulPutVC = ((VectorClock) this.storeClient.putVersionedWithCustomTimeout(this.requestObject)).clone();
                             } else {
                                 successfulPutVC = ((VectorClock) this.storeClient.putWithCustomTimeout(this.requestObject)).clone();
                             }
 
                             PutResponseSender responseConstructor = new PutResponseSender(messageEvent,
                                                                                           successfulPutVC,
                                                                                           this.storeClient.getStoreName(),
                                                                                           this.requestObject.getKey());
                             responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                              true,
                                                              this.requestObject.getRequestOriginTimeInMs());
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""PUT successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""PUT request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                putErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            putErrorHandler.handleExceptions(messageEvent, e);
                         }
 
                         break;
 
                     case VoldemortOpCode.DELETE_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming delete request"");
                         }
 
                         try {
                             boolean isDeleted = this.storeClient.deleteWithCustomTimeout(this.requestObject);
                             if(isDeleted) {
                                 DeleteResponseSender responseConstructor = new DeleteResponseSender(messageEvent,
                                                                                                     this.storeClient.getStoreName(),
                                                                                                     this.requestObject.getKey());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""DELETE request successful !"");
                                 }
 
                             } else {
                                 logger.error(""Requested Key with the specified version does not exist"");
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key with the specified version does not exist"");
                             }
 
                         } catch(Exception e) {
 
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""DELETE request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                deleteErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            deleteErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     default:
                         System.err.println(""Illegal operation."");
                         return;
-
                 }
 
             }
-
         }
-
     }",Buggy,"fix duplicate error message bug and add large value size test for coordinator
",Buggy,"The commit message indicates a fix for a duplicate error message bug. The diff shows changes in the exception handling logic for various Voldemort operations (GET_METADATA, GET, GET_ALL, GET_VERSION, PUT, DELETE). Specifically, the code moves the `getErrorHandler.handleExceptions(messageEvent, e)` call inside the `else` block of the timeout check. This ensures that the generic exception handler is only called when the exception is not due to a timeout, preventing the duplicate error message caused by both the timeout check and the generic handler writing an error response. The changes in exception handling logic across multiple operations suggest a bug fix related to error reporting. Therefore, the commit likely addresses a bug.

**Buggy**"
voldemort,7694.json,d1b041d2e3ef3148bfcfae07d6e2609a5aa0ac45,"@@ -1,33 +1,33 @@
     public VAdminProto.UpdateMetadataPairResponse handleUpdateMetadataPair(VAdminProto.UpdateMetadataPairRequest request) {
         VAdminProto.UpdateMetadataPairResponse.Builder response = VAdminProto.UpdateMetadataPairResponse.newBuilder();
         try {
             ByteArray clusterKey = ProtoUtils.decodeBytes(request.getClusterKey());
             ByteArray storesKey = ProtoUtils.decodeBytes(request.getStoresKey());
             String clusterKeyString = ByteUtils.getString(clusterKey.get(), ""UTF-8"");
             String storesKeyString = ByteUtils.getString(storesKey.get(), ""UTF-8"");
 
             if(MetadataStore.METADATA_KEYS.contains(clusterKeyString)
                && MetadataStore.METADATA_KEYS.contains(storesKeyString)) {
 
                 Versioned<byte[]> clusterVersionedValue = ProtoUtils.decodeVersioned(request.getClusterValue());
                 Versioned<byte[]> storesVersionedValue = ProtoUtils.decodeVersioned(request.getStoresValue());
 
                 metadataStore.writeLock.lock();
                 try {
                     logger.info(""Updating metadata for keys '"" + clusterKeyString + ""'"" + "" and '""
                                 + storesKeyString + ""'"");
                     metadataStore.put(clusterKey, clusterVersionedValue, null);
-                    metadataStore.put(storesKey, storesVersionedValue, null);
+                    metadataStore.updateStoreDefinitions(storesVersionedValue);
                     logger.info(""Successfully updated metadata for keys '"" + clusterKeyString + ""'""
                                 + "" and '"" + storesKeyString + ""'"");
                 } finally {
                     metadataStore.writeLock.unlock();
                 }
             }
         } catch(VoldemortException e) {
             response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
             logger.error(""handleUpdateMetadataPair failed for request("" + request.toString() + "")"",
                          e);
         }
         return response.build();
     }",Buggy,"Bug fix in RebalanceRebootstrapConsistencyTest and AdminServiceRequestHandler. Adding wrapper for updateRemoteStoreDefList in AdminClient
",Buggy,"The commit message indicates a bug fix related to `RebalanceRebootstrapConsistencyTest`, `AdminServiceRequestHandler`, and the `AdminClient`. Specifically, it mentions adding a wrapper for `updateRemoteStoreDefList` in `AdminClient`.

The provided code diff is from `AdminServiceRequestHandler.java`. It replaces `metadataStore.put(storesKey, storesVersionedValue, null);` with `metadataStore.updateStoreDefinitions(storesVersionedValue);`. This change suggests that the previous direct `put` operation on the `metadataStore` for store definitions was incorrect or insufficient, and a dedicated `updateStoreDefinitions` method is now used. This likely addresses a bug where store definitions were not being updated correctly or consistently. The surrounding code includes error handling with a `try-catch` block that catches `VoldemortException`, logs the error, and sets an error code in the response.

The change in `AdminServiceRequestHandler` from directly putting the stores value to using `updateStoreDefinitions` strongly suggests a bug fix related to how store definitions are updated. The commit message also mentions `AdminClient` which is not present in the diff, but the change in `AdminServiceRequestHandler` aligns with the bug fix described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**
"
voldemort,7656.json,1f057d7e3c68dc4d67387a5485ac6793d3feb8b7,"@@ -1,31 +1,33 @@
     public VAdminProto.RebalanceStateChangeResponse handleRebalanceStateChange(VAdminProto.RebalanceStateChangeRequest request) {
-
         VAdminProto.RebalanceStateChangeResponse.Builder response = VAdminProto.RebalanceStateChangeResponse.newBuilder();
 
-        try {
-            // Retrieve all values first
-            List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
-            for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
-                rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+        synchronized(rebalancer) {
+            try {
+                // Retrieve all values first
+                List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
+                for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
+                    rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+                }
+
+                Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
+
+                boolean swapRO = request.getSwapRo();
+                boolean changeClusterMetadata = request.getChangeClusterMetadata();
+                boolean changeRebalanceState = request.getChangeRebalanceState();
+                boolean rollback = request.getRollback();
+
+                rebalancer.rebalanceStateChange(cluster,
+                                                rebalancePartitionsInfo,
+                                                swapRO,
+                                                changeClusterMetadata,
+                                                changeRebalanceState,
+                                                rollback);
+            } catch(VoldemortException e) {
+                response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
+                logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
+                             + "")"", e);
             }
-
-            Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
-
-            boolean swapRO = request.getSwapRo();
-            boolean changeClusterMetadata = request.getChangeClusterMetadata();
-            boolean changeRebalanceState = request.getChangeRebalanceState();
-            boolean rollback = request.getRollback();
-
-            rebalancer.rebalanceStateChange(cluster,
-                                            rebalancePartitionsInfo,
-                                            swapRO,
-                                            changeClusterMetadata,
-                                            changeRebalanceState,
-                                            rollback);
-        } catch(VoldemortException e) {
-            response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
-            logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
-                         + "")"", e);
         }
+
         return response.build();
     }",Buggy,"fix two concurrency bugs during the termination of donor-based rebalancing
",Buggy,"The commit message indicates a fix for two concurrency bugs during the termination of donor-based rebalancing. The code diff introduces a `synchronized` block around the core logic of the `handleRebalanceStateChange` method, synchronizing on the `rebalancer` object. This suggests that the previous implementation was susceptible to race conditions or other concurrency-related issues when multiple threads were interacting with the `rebalancer` during the rebalancing process, especially during termination. The synchronization aims to ensure that only one thread can execute the rebalancing logic at a time, thus preventing concurrent access and modification of shared resources, which could lead to data corruption or inconsistent state.

**Buggy**"
voldemort,6967.json,9f2e07c6e18984df4d2428dd51accd37534e9a68,"@@ -1,11 +1,12 @@
     public void stop() {
         logger.info(""Stopping "" + getType().getDisplayName());
         synchronized(this) {
             if(!isStarted()) {
                 logger.info(""The service is already stopped, ignoring duplicate attempt."");
+                return;
             }
 
             stopInner();
             isStarted.set(false);
         }
     }",Buggy,"Fixed the re-introduction of BindExceptions upon start up of some recently added tests:

All of the following tests directly used the method ServerTestUtils.startVoldemortServer
  test/unit/voldemort/client/ClientRegistryTest.java
  test/unit/voldemort/client/AdminFetchTest.java
  test/unit/voldemort/client/EndToEndRebootstrapTest.java
  test/unit/voldemort/store/system/AsyncMetadataVersionManagerTest.java
  test/unit/voldemort/store/system/SystemStoreTest.java

This method of starting Voldemort servers is susceptible to
BindException errors. Unless something very fancy is being done, the
method ServerTestUtils.startVoldemortCluster should be used to start a
cluster within a test.

test/unit/voldemort/client/ClientRegistryTest.java extended from
TestCase. This is a ""Junit3"" idiom that we should not use in new
tests. The Junit4 approach to tests relies solely on annotation.
",Buggy,"The commit message indicates a fix for BindException errors occurring during the startup of certain tests. It also mentions a shift from JUnit3 to JUnit4 style tests. The provided diff snippet shows a modification to the `stop()` method of a service, adding a check to prevent stopping an already stopped service. This change doesn't directly relate to BindException or JUnit versions. However, preventing duplicate stop attempts could indirectly address resource contention issues that might lead to BindExceptions in some scenarios.

The connection between the commit message and the code change is weak, but the change itself is a defensive programming practice. Given the context of fixing BindExceptions, it's plausible that this change contributes to a more robust startup/shutdown process, thereby reducing the likelihood of BindExceptions.

**Buggy**
"
voldemort,8213.json,46a5b4d7e916e3fe0a9a77659599d74709f18c2d,"@@ -1,55 +1,56 @@
     private void write(DataOutputStream output, Object object, Object type) throws IOException {
         try {
             if(type instanceof Map) {
                 if(object != null && !(object instanceof Map))
                     throw new SerializationException(""Expected Map, but got "" + object.getClass()
                                                      + "": "" + object);
                 writeMap(output, (Map<String, Object>) object, (Map<String, Object>) type);
             } else if(type instanceof List) {
                 if(object != null && !(object instanceof List))
                     throw new SerializationException(""Expected List but got "" + object.getClass()
                                                      + "": "" + object);
                 writeList(output, (List<Object>) object, (List<Object>) type);
             } else if(type instanceof JsonTypes) {
                 JsonTypes jsonType = (JsonTypes) type;
                 switch(jsonType) {
                     case STRING:
                         writeString(output, (String) object);
                         break;
                     case INT8:
                         writeInt8(output, (Byte) object);
                         break;
                     case INT16:
                         writeInt16(output, coerceToShort(object));
                         break;
                     case INT32:
                         writeInt32(output, coerceToInteger(object));
                         break;
                     case INT64:
                         writeInt64(output, coerceToLong(object));
                         break;
                     case FLOAT32:
                         writeFloat32(output, coerceToFloat(object));
                         break;
                     case FLOAT64:
                         writeFloat64(output, coerceToDouble(object));
                         break;
                     case DATE:
                         writeDate(output, coerceToDate(object));
                         break;
                     case BYTES:
                         writeBytes(output, (byte[]) object);
                         break;
                     case BOOLEAN:
                         writeBoolean(output, (Boolean) object);
+                        break;
                     default:
                         throw new SerializationException(""Unknown type: "" + type);
                 }
             }
         } catch(ClassCastException e) {
             // simpler than doing every test
             throw new SerializationException(""Expected type "" + type
                                              + "" but got object of incompatible type ""
                                              + object.getClass().getName() + ""."");
         }
     }",Buggy,"Fix serialization bug with writing boolean values--missing break in case statement.
",Buggy,"The commit message indicates a fix for a serialization bug related to writing boolean values. The provided code diff shows the addition of a `break;` statement after the `writeBoolean` call within the `BOOLEAN` case of a switch statement. Without this `break`, the code would fall through to the next case, causing incorrect serialization. This directly addresses the bug described in the commit message.

**Buggy**"
voldemort,9151.json,ff128a8ad5de83bc55b8cec80d7775e61816e4e6,"@@ -1,16 +1,21 @@
     public List<Versioned<T>> resolveConflicts(List<Versioned<T>> items) {
+        List<ClockEntry> maxClock = null;
         if(items.size() <= 1) {
             return items;
         } else {
             Versioned<T> max = items.get(0);
             long maxTime = ((VectorClock) items.get(0).getVersion()).getTimestamp();
+            maxClock = ((VectorClock) items.get(0).getVersion()).getEntries();
             for(Versioned<T> versioned: items) {
                 VectorClock clock = (VectorClock) versioned.getVersion();
                 if(clock.getTimestamp() > maxTime) {
                     max = versioned;
                     maxTime = ((VectorClock) versioned.getVersion()).getTimestamp();
                 }
+                maxClock = VectorClock.maxClockList(maxClock, clock.getEntries());
             }
-            return Collections.singletonList(max);
+            Versioned<T> maxTimeClockVersioned = new Versioned<T>(max.getValue(),
+                                                                  new VectorClock(maxClock, maxTime));
+            return Collections.singletonList(maxTimeClockVersioned);
         }
     }",Buggy,"TimeBasedInconsistency resolover bug fix
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""TimeBasedInconsistency resolver bug fix"" clearly states that the commit is intended to fix a bug related to the `TimeBasedInconsistency` resolver. This suggests that there was an issue in how the resolver was handling time-based inconsistencies, and the current commit aims to correct it.

**Git Diff Analysis:**

The diff shows changes within the `resolveConflicts` method. Let's break down the changes:

1.  `List<ClockEntry> maxClock = null;`: A new variable `maxClock` is initialized to `null`. This variable seems to be intended to store the maximum clock entries encountered during the conflict resolution process.

2.  `maxClock = ((VectorClock) items.get(0).getVersion()).getEntries();`: The `maxClock` is initialized with the clock entries of the first item.

3.  `maxClock = VectorClock.maxClockList(maxClock, clock.getEntries());`: Inside the loop, `maxClock` is updated by comparing it with the clock entries of the current `versioned` item using `VectorClock.maxClockList`. This suggests that the code is trying to find the maximum clock entries across all conflicting versions.

4.  `Versioned<T> maxTimeClockVersioned = new Versioned<T>(max.getValue(), new VectorClock(maxClock, maxTime));`: A new `Versioned` object `maxTimeClockVersioned` is created using the value of `max` and a new `VectorClock` constructed from `maxClock` and `maxTime`.

5.  `return Collections.singletonList(maxTimeClockVersioned);`: The method now returns a list containing the newly created `maxTimeClockVersioned` object.

**Reasoning:**

The original code simply returned the `Versioned<T>` object with the highest timestamp. The modified code now merges the clock entries of all items to create a new `VectorClock` using `VectorClock.maxClockList`. This suggests that the original code was not correctly handling the vector clock entries when resolving conflicts, potentially leading to inconsistencies. The new code aims to resolve this by merging the clock entries, which is a common practice in conflict resolution with vector clocks. The creation of a new `Versioned` object with the merged clock and the maximum timestamp strongly suggests a fix for an inconsistency issue.

**Conclusion:**

The commit message and the code changes align, and the changes indicate a bug fix related to time-based inconsistencies in the conflict resolution logic. The code now merges clock entries, which likely addresses the inconsistency issue.

**Buggy**
"
voldemort,11028.json,c3773f409dffb3761aaeba6811d3ed6836263987,"@@ -1,15 +1,15 @@
     public static void validateUserStoreNamesOnNode(AdminClient adminClient,
                                                  Integer nodeId,
                                                  List<String> storeNames) {
         List<StoreDefinition> storeDefList = adminClient.metadataMgmtOps.getRemoteStoreDefList(nodeId)
                                                                         .getValue();
-        List<String> storeNameList = Lists.newArrayList();
+        Map<String, Boolean> existingStoreNames = new HashMap<String, Boolean>();
         for(StoreDefinition storeDef: storeDefList) {
-            storeNameList.add(storeDef.getName());
+          existingStoreNames.put(storeDef.getName(), true);
         }
         for(String storeName: storeNames) {
-            if(!storeNameList.contains(storeName)) {
+            if(!Boolean.TRUE.equals(existingStoreNames.get(storeName))) {
                 Utils.croak(""Store "" + storeName + "" does not exist!"");
             }
         }
     }",Buggy,"Fix a vadmin error
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix a vadmin error"" strongly suggests that the changes are intended to correct a bug in the vadmin tool.

2.  **Code Changes:**
    *   The original code used a `List<String>` called `storeNameList` to store the names of existing stores. It then iterated through the provided `storeNames` and checked if each name was present in the `storeNameList` using `storeNameList.contains(storeName)`.
    *   The modified code replaces the `List<String>` with a `Map<String, Boolean>` called `existingStoreNames`. It populates this map with store names as keys and `true` as values. The check for the existence of a store name is then done using `existingStoreNames.get(storeName)` and comparing the result to `Boolean.TRUE`.

3.  **Bug Fix Indication:**
    *   The change from a `List` to a `Map` suggests a potential performance improvement or a fix for an issue related to how store names were being checked. Using a `Map` provides O(1) lookup time, whereas `List.contains()` has O(n) complexity. This is especially important if the number of store names is large.
    *   The use of `Boolean.TRUE.equals(existingStoreNames.get(storeName))` is a null-safe way to check if a store name exists in the map. If `existingStoreNames.get(storeName)` returns `null` (meaning the store name is not in the map), the `equals()` method will return `false` without throwing a `NullPointerException`. This indicates a potential bug where the code might have previously thrown a `NullPointerException` if a store name was not found.

**Conclusion:**

The changes in the code, along with the commit message, strongly suggest that this commit is a bug fix. The use of a `Map` for faster lookups and the null-safe check using `Boolean.TRUE.equals()` indicate that the original code might have had performance issues or was susceptible to `NullPointerException`.

**Buggy**"
voldemort,755.json,0548406c86c8ed3af48c0a59586f8dd03d3aefdd,"@@ -1,11 +1,10 @@
             public void nodeUnavailable(Node node) {
                 if(logger.isInfoEnabled())
-                    logger.info(""Node "" + node
-                                + "" has been marked as unavailable, destroying socket pool"");
+                    logger.info(node + "" has been marked as unavailable, destroying socket pool"");
 
                 // Kill the socket pool for this node...
                 SocketDestination destination = new SocketDestination(node.getHost(),
                                                                       node.getSocketPort(),
                                                                       config.getRequestFormatType());
                 socketPool.close(destination);
             }",Buggy,"Fixed inconsistent error message.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed inconsistent error message"" suggests that the change addresses an issue where error messages were not uniform or predictable. This implies a bug fix related to how errors were reported.

**Git Diff Analysis:**

The diff shows a change within the `nodeUnavailable` method. Specifically, the change modifies the log message:

- **Before:** `logger.info(""Node "" + node + "" has been marked as unavailable, destroying socket pool"");`
- **After:** `logger.info(node + "" has been marked as unavailable, destroying socket pool"");`

The change removes the explicit string concatenation `""Node "" + node` and relies on the `node` object's `toString()` method to provide the node's representation in the log message.

**Reasoning:**

The original code explicitly prepended ""Node "" to the node's representation in the log message. If the `node` object's `toString()` method already included ""Node "" or a similar prefix, the log message would have been inconsistent (e.g., ""Node Node123 has been marked as unavailable...""). By removing the explicit ""Node "" prefix, the code now relies solely on the `node` object's `toString()` method, ensuring a consistent format for the log message.

This change directly addresses the issue of inconsistent error messages, as the log message is now more predictable and uniform. It's a bug fix because it corrects an undesirable behavior (inconsistent error messages) and improves the clarity and reliability of the logging.

**Conclusion:**

**Buggy**
"
voldemort,151.json,a36d1fe39e88e79875bef39bc0ce10419d8cbcf2,"@@ -1,8 +1,8 @@
-    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml) {
+    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml, int connectionTimeoutSec, int socketTimeoutSec) {
         ClientConfig config = new ClientConfig().setBootstrapUrls(url)
-                .setConnectionTimeout(15,TimeUnit.SECONDS)
+                .setConnectionTimeout(connectionTimeoutSec ,TimeUnit.SECONDS)
                 .setFetchAllStoresXmlInBootstrap(fetchAllStoresXml);
 
-        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(60);
+        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(socketTimeoutSec);
         return new AdminClient(adminConfig, config);
     }",Buggy,"Made admin connection/socket timeout configurable in BnP.

Also changed the default socket timeout to 180 seconds.

This fixes the following problem: when a node is unreachable and
completely shut down, requests to it will time out, which takes
60 seconds. When BnP notices this, it will reach one of the live
nodes in the cluster and ask it to deal with the failure. The
live node will try to talk to the dead node, which will also
take 60 seconds to time out. By the time the live node decides
that the dead node is unreachable, and responds to the BnP job,
the BnP job will have already timed out. Then, the BnP job will
think that the HandleFailedFetchRequest could not complete
successfully (even though it did in fact complete successfully)
and BnP HA will be aborted.

The solution is that the BnP job's socket timeout must be greater
than the server's default connection timeout.

This was not an issue before when we had insanely long time outs,
but those time outs have been reduced considerably in commit
34debd34c5896b6c2a01b1012e89dd1a3a0a0242. This is likely when we
regressed on the handling of this failure mode.
",Buggy,"The commit message describes a problem where BnP HA was being aborted due to socket timeouts. The issue arose because the BnP job's socket timeout was shorter than the server's connection timeout when dealing with unreachable nodes. The commit message indicates that the solution involves making the admin connection/socket timeout configurable and increasing the default socket timeout.

The diff shows modifications to the `createAdminClient` method. It now accepts `connectionTimeoutSec` and `socketTimeoutSec` as parameters, which are then used to configure the `ClientConfig` and `AdminClientConfig` respectively. This change directly addresses the problem described in the commit message by allowing the socket timeout to be configured.

The commit message clearly indicates a bug fix, as it describes a specific failure mode and the steps taken to resolve it. The code changes align with the commit message by making the relevant timeout values configurable.

**Buggy**"
voldemort,132.json,da4a1bf5015df49a9e8ba867a90d4dab7f0ba194,"@@ -1,29 +1,29 @@
-    public static void main(String[] args) {
+    public static void main(String[] args) throws Exception {
         // Validate arguments
         if (args.length < 1) {
             logger.error(""Please provide a job config file name as the argument to this script."");
             System.exit(1);
         }
         String fileName = args[0];
 
         // Load config
         logger.info(""Extracting config properties out of: "" + fileName);
         Props props = null;
         try {
             props = new Props(null, fileName);
         } catch (IOException e) {
             logger.error(""Exception while reading config file!"", e);
             System.exit(1);
         }
 
         // Run job
         VoldemortBuildAndPushJob job = new VoldemortBuildAndPushJob(""shell-job"", props);
         try {
             job.run();
         } catch (Exception e) {
             logger.error(""Exception while running BnP job!"", e);
             System.exit(1);
         }
         logger.info(""BnP job finished successfully (:"");
         System.exit(0);
     }",Buggy,"Fixed a compilation error introduced by last commit. (#468)

",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a compilation error introduced by last commit. (#468)"" clearly states that the commit is intended to fix a compilation error that was introduced in a previous commit. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a single change:

- `public static void main(String[] args) {`
+ `public static void main(String[] args) throws Exception {`

The change adds `throws Exception` to the `main` method signature. In Java, if a method can potentially throw a checked exception, it must either handle the exception within the method or declare that it throws the exception. The addition of `throws Exception` indicates that the code within the `main` method might throw an exception that is not being caught within the `main` method itself. Without this `throws Exception` declaration, the code would indeed fail to compile if any unhandled checked exceptions were thrown within the `main` method.

**Reasoning:**

The commit message indicates a fix for a compilation error. The diff shows the addition of `throws Exception` to the `main` method signature. This aligns with the commit message because adding `throws Exception` would resolve a compilation error caused by an unhandled exception within the main method. The previous commit likely introduced a call to a method that throws a checked exception, and this commit fixes the resulting compilation error by declaring that `main` throws the exception.

**Conclusion:**

**Buggy**
"
voldemort,9849.json,41cf6a7204e1b9821833bf9f6022f61abd10f007,"@@ -1,17 +1,17 @@
     private VectorClock readVersion(String key) {
         try {
             File versionFile = new File(getVersionDirectory(), key);
             if(!versionFile.exists()) {
                 // bootstrap file save default clock as version.
-                VectorClock clock = new VectorClock();
+                VectorClock clock = new VectorClock(0);
                 writeVersion(key, clock);
                 return clock;
             } else {
                 // read the version file and return version.
                 String hexCode = FileUtils.readFileToString(versionFile, ""UTF-8"");
                 return new VectorClock(Hex.decodeHex(hexCode.toCharArray()));
             }
         } catch(Exception e) {
             throw new VoldemortException(""Failed to read Version for Key:"" + key, e);
         }
     }",Buggy,"fixed a bug that throws exceptions for some tests
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""fixed a bug that throws exceptions for some tests"" clearly states that the commit aims to resolve a bug that was causing exceptions during test execution. This suggests that the code changes should address an issue that was leading to unexpected exceptions.

**Git Diff Analysis:**

The diff focuses on the `readVersion` method. The key change is in the `if(!versionFile.exists())` block:

-   **Original Code:** `VectorClock clock = new VectorClock();`
-   **Modified Code:** `VectorClock clock = new VectorClock(0);`

The original code creates a `VectorClock` object using the default constructor (likely initializing with default values, possibly an empty clock). The modified code explicitly initializes the `VectorClock` with a value of 0.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly mentions fixing a bug related to exceptions during tests.
2.  **Code Change Relevance:** The code change involves initializing a `VectorClock` with an explicit value (0) instead of using the default constructor when the version file doesn't exist.
3.  **Potential Bug Scenario:** It's plausible that the default constructor of `VectorClock` was creating a state that was causing issues later on, leading to exceptions in certain test scenarios. By explicitly initializing with 0, the code might be avoiding this problematic state.
4.  **Error Handling:** The `try-catch` block remains unchanged, indicating that the fix is not directly related to error handling but rather to the initial state of the `VectorClock`.

**Conclusion:**

Based on the commit message and the code changes, it's highly likely that this commit addresses a bug where the default initialization of `VectorClock` was leading to exceptions during tests. The explicit initialization with 0 seems to be a targeted fix for this issue.

**Buggy**"
cassandra,15770.json,3740f815c21254bd625ad1cbe8d47aa657727a83,"@@ -1,26 +1,29 @@
     public boolean maybeWaitForArchiving(String name)
     {
         Future<?> f = archivePending.remove(name);
         if (f == null)
             return true; // archiving disabled
 
         try
         {
             f.get();
         }
         catch (InterruptedException e)
         {
             throw new AssertionError(e);
         }
         catch (ExecutionException e)
         {
-            if (e.getCause() instanceof IOException)
+            if (e.getCause() instanceof RuntimeException)
             {
-                logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name);
-                return false;
+                if (e.getCause().getCause() instanceof IOException)
+                {
+                    logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name, e.getCause().getCause());
+                    return false;
+                }
             }
             throw new RuntimeException(e);
         }
 
         return true;
     }",Buggy,"Fix bugs in commit log archiving startup behavior

patch by Ariel Weisberg; reviewed by Branimir Lambov for CASSANDRA-10593
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states ""Fix bugs in commit log archiving startup behavior"". This strongly suggests the commit addresses one or more bugs.

2.  **Code Changes:**
    *   The `catch (ExecutionException e)` block is modified. Specifically, the original code checked if `e.getCause()` was an instance of `IOException`. The modified code now checks if `e.getCause()` is an instance of `RuntimeException` and then checks if `e.getCause().getCause()` is an instance of `IOException`.
    *   The logging statement within the `IOException` check is updated to include the nested cause `e.getCause().getCause()` in the log message. This provides more detailed information about the root cause of the archiving failure.
    *   The original code threw a `RuntimeException` wrapping the `ExecutionException`. The updated code preserves this behavior.

3.  **Bug Fix Indication:**
    *   The change in exception handling suggests a bug where the original code was not correctly identifying `IOException`s that were wrapped within `RuntimeException`s. This could have led to incorrect behavior when archiving failed.
    *   The improved logging provides better diagnostics, which is a common characteristic of bug fixes.
    *   The commit message aligns with the code changes, indicating that the changes are intended to fix a bug related to commit log archiving.

**Conclusion:**

The commit message and code changes strongly suggest that this commit addresses a bug related to exception handling during commit log archiving. The changes ensure that nested `IOException`s are properly caught and logged, preventing potential issues during startup.

**Buggy**
"
cassandra,5689.json,02030dd658e6ca0cdb8987fd72e0454066e1b6d6,"@@ -1,4 +1,7 @@
     public boolean contains(T position)
     {
-        return Range.contains(left, right, position) || left.equals(position);
+        // Range.contains doesnt work correctly if left == right because for
+        // Range that means a wrapping range that select the whole ring. So we
+        // must explicitely handle this case
+        return left.equals(position) || (!left.equals(right) && Range.contains(left, right, position));
     }",Buggy,"Fix LCS bug with sstables containing only 1 row

patch by slebresne; reviewed by jbellis for CASSANDRA-4411
",Buggy,"Okay, let's analyze this Git diff and commit message.

**1. Understanding the Commit Message:**

The commit message ""Fix LCS bug with sstables containing only 1 row"" indicates a bug fix related to the Local Consistency System (LCS) in Cassandra, specifically when dealing with SSTables (Sorted String Tables) that contain only one row. The message suggests that there was an issue in how the LCS handled such SSTables.

**2. Analyzing the Code Diff:**

The code diff modifies the `contains(T position)` method. The original implementation was:

```java
return Range.contains(left, right, position) || left.equals(position);
```

The modified implementation is:

```java
return left.equals(position) || (!left.equals(right) && Range.contains(left, right, position));
```

The change introduces a check `!left.equals(right)` before calling `Range.contains()`.  The comment explains the reason: `Range.contains` doesn't work correctly if `left == right` because, for `Range`, that means a wrapping range that selects the whole ring. The code explicitly handles the case where `left` and `right` are equal.

**3. Connecting the Commit Message and Code Diff:**

The commit message and the code diff are strongly related. The code change addresses a specific scenario where `left` and `right` are equal, which likely occurs when an SSTable contains only one row (or a single range). The original implementation incorrectly handled this case, leading to a bug. The modified code now correctly handles this scenario.

**4. Bug Fix Identification:**

The code change clearly fixes a bug. The original code had a flaw in its logic that caused incorrect behavior when `left` and `right` were equal. The modified code corrects this flaw by adding an explicit check for this condition. The commit message confirms that this change is a bug fix related to SSTables with a single row.

**Conclusion:**

Buggy
"
cassandra,13729.json,967a2cfe179548835d5e8c1640889420ce0d40ce,"@@ -1,13 +1,13 @@
     private void updateCrc()
     {
-        if (crcPosition == buffer.position() | crcUpdateDisabled)
+        if (crcPosition == buffer.position() || crcUpdateDisabled)
             return;
 
         assert crcPosition >= 0 && crcPosition < buffer.position();
 
         ByteBuffer unprocessed = buffer.duplicate();
         unprocessed.position(crcPosition)
                    .limit(buffer.position());
 
         crc.update(unprocessed);
     }",Buggy,"8630: fixed coverity defects
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""8630: fixed coverity defects"" explicitly states that the commit addresses defects identified by Coverity, a static analysis tool. This strongly suggests that the changes are intended to fix bugs or potential issues in the code.

**Code Diff Analysis:**

The code diff shows a change in the `updateCrc` method within a class that seems to be related to CRC (Cyclic Redundancy Check) calculation. The change is from `crcPosition == buffer.position() | crcUpdateDisabled` to `crcPosition == buffer.position() || crcUpdateDisabled`.

The original code uses a bitwise OR operator (`|`) instead of a logical OR operator (`||`). In Java, the bitwise OR operator evaluates both operands regardless of the value of the first operand. The logical OR operator (`||`) short-circuits, meaning it only evaluates the second operand if the first operand is false.

**Reasoning:**

Coverity likely flagged the use of the bitwise OR operator (`|`) as a potential defect. In the context of the `if` condition, the intention is to skip the CRC update if either `crcPosition` is equal to `buffer.position()` or `crcUpdateDisabled` is true. Using the bitwise OR operator could lead to unexpected behavior if `crcPosition == buffer.position()` has side effects or is computationally expensive, as the second operand `crcUpdateDisabled` would always be evaluated.

The change to the logical OR operator (`||`) corrects this potential issue by ensuring that the second operand is only evaluated when necessary. This is a typical bug fix scenario where a static analysis tool identifies a potential issue, and the developer corrects the code to align with the intended logic.

**Conclusion:**

The commit message explicitly states that the changes are to fix Coverity defects. The code diff shows a change from a bitwise OR to a logical OR, which is a common bug fix scenario. Therefore, the changes indicate a bug fix.

**Buggy**
"
cassandra,14898.json,50c1987265f8ddaf5032a7ace07b2462aba4f09b,"@@ -1,38 +1,38 @@
     static Set<List<String>> getCompactionBuckets(List<String> files, long min)
     {
-    	Map<List<String>, Long> buckets = new NonBlockingHashMap<List<String>, Long>();
+    	Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
     	for(String fname : files)
     	{
     		File f = new File(fname);
     		long size = f.length();
 
     		boolean bFound = false;
             // look for a bucket containing similar-sized files:
             // group in the same bucket if it's w/in 50% of the average for this bucket,
             // or this file and the bucket are all considered ""small"" (less than `min`)
             for (List<String> bucket : buckets.keySet())
     		{
                 long averageSize = buckets.get(bucket);
                 if ((size > averageSize/2 && size < 3*averageSize/2)
                     || ( size < min && averageSize < min))
     			{
                     // remove and re-add because adding changes the hash
                     buckets.remove(bucket);
     				averageSize = (averageSize + size) / 2 ;
                     bucket.add(fname);
                     buckets.put(bucket, averageSize);
     				bFound = true;
     				break;
     			}
     		}
             // no similar bucket found; put it in a new one
     		if(!bFound)
     		{
                 ArrayList<String> bucket = new ArrayList<String>();
                 bucket.add(fname);
                 buckets.put(bucket, size);
     		}
     	}
 
         return buckets.keySet();
     }",Buggy,"fix getCompactionBuckets -- something is broken in NonBlockingHashMap when removing and re-adding the same collection mid-iteration; it ends up with multiple references to that collection.  going back to ConcurrentHashMap fixes the problem.  (any additional overhead from CHM is negligible here.)  patch by jbellis; reviewed by Eric Evans for #57

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/trunk@764004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug in `NonBlockingHashMap` related to removing and re-adding the same collection during iteration, leading to multiple references to the collection. The code diff replaces `NonBlockingHashMap` with `ConcurrentHashMap`. This change directly addresses the bug described in the commit message. The commit message also mentions that the overhead from using `ConcurrentHashMap` is negligible, further supporting the idea that this change is a targeted fix for a specific problem.

**Buggy**
"
cassandra,6678.json,7b083a4e083442cb6e6bf34735753c6103d88654,"@@ -1,8 +1,6 @@
     public static void rescheduleFailedTasks()
     {
-        for (SnapshotDeletingTask task : failedTasks)
-        {
-            failedTasks.remove(task);
+        Runnable task;
+        while ( null != (task = failedTasks.poll()))
             ScheduledExecutors.nonPeriodicTasks.submit(task);
-        }
     }",Buggy,"Fix coverity errors with SSTableDeletingTask and SnapshotDeletingtask

Patch by jmckenzie; reviewed by stefania for CASSANDRA-10222
",Buggy,"The commit message indicates a fix for Coverity errors related to `SSTableDeletingTask` and `SnapshotDeletingTask`. The provided code diff focuses on the `rescheduleFailedTasks` method, which appears to be related to handling failed tasks, potentially including the mentioned task types.

The original code used a for-each loop to iterate through `failedTasks` and remove each task. However, removing elements from a list while iterating through it using a for-each loop can lead to `ConcurrentModificationException`. The corrected code replaces the for-each loop with a `while` loop using `failedTasks.poll()`. This change suggests that the original code had a potential bug related to concurrent modification during task rescheduling. The use of `poll()` also indicates that `failedTasks` is likely a queue, which is a more appropriate data structure for managing tasks to be rescheduled.

Therefore, the change addresses a potential bug related to concurrent modification and task rescheduling, aligning with the commit message's intention to fix errors.

**Buggy**"
cassandra,9236.json,2a656e6bd5e1f5ac8161b65a211dd8bed74b7def,"@@ -1,10 +1,10 @@
     private MerkleTree getMerkleTree(Token t)
     {
         for (Range<Token> range : merkleTrees.keySet())
         {
             if (range.contains(t))
                 return merkleTrees.get(range);
         }
 
-        return null;
+        throw new AssertionError(""Expected tree for token "" + t);
     }",Buggy,"Fixed coverity defects
Reviewed by Marcus Olsson for CASSANDRA-5220
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fixed coverity defects"" strongly suggests that the commit addresses issues identified by the Coverity static analysis tool. Coverity often flags potential bugs, so this is a good indicator of a bug fix. The reference to CASSANDRA-5220 further supports this, as it likely points to a specific bug report or issue.

**2. Code Diff Analysis:**

The code diff shows a change in the `getMerkleTree` method.  The original code returned `null` if no `MerkleTree` was found for the given token `t`. The modified code now throws an `AssertionError` with a descriptive message indicating that a tree was expected for the token.

**3. Reasoning:**

*   **`null` return is problematic:** Returning `null` in a situation where a `MerkleTree` is expected can lead to NullPointerExceptions (NPEs) later in the code. This is a common source of bugs.
*   **`AssertionError` indicates a programming error:** Throwing an `AssertionError` signals that the program is in an unexpected state. In this case, it means that the token `t` should have always been within one of the ranges in `merkleTrees`. This suggests a potential logic error in how the ranges are managed or how tokens are assigned to them.
*   **Coverity's role:** Coverity likely flagged the `null` return as a potential defect because it could lead to an NPE. The fix replaces the `null` return with an `AssertionError`, which is a more robust way to handle the unexpected situation and helps in debugging.

**4. Conclusion:**

The change from returning `null` to throwing an `AssertionError` strongly suggests a bug fix. The original code could have led to unexpected behavior (NPEs) if a `MerkleTree` was not found. The `AssertionError` provides a more informative error message and helps to identify the root cause of the problem. The commit message ""Fixed coverity defects"" further reinforces this conclusion.

**Buggy**
"
cassandra,20480.json,6991556e431a51575744248a4c484270c4f918c9,"@@ -1,4 +1,4 @@
     public static void truncate()
     {
-        ALL.forEach(table -> getSchemaCFS(table).truncateBlocking());
+        ALL.reverse().forEach(table -> getSchemaCFS(table).truncateBlocking());
     }",Buggy,"Fix startup problems due to schema tables not completely flushed

patch by Stefania Alborghetti; reviewed by Aleksey Yeschenko for CASSANDRA-12213.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix startup problems due to schema tables not completely flushed"" clearly indicates a bug fix. The problem is related to schema tables not being properly flushed, leading to startup issues.

**Git Diff Analysis:**

The diff shows a change in the `truncate()` method. Specifically, the order in which the schema tables are truncated is reversed.

*   **Before:** `ALL.forEach(table -> getSchemaCFS(table).truncateBlocking());`
*   **After:** `ALL.reverse().forEach(table -> getSchemaCFS(table).truncateBlocking());`

The change suggests that the original order of truncation was causing a problem, likely due to dependencies between the schema tables. By reversing the order, the tables are now truncated in an order that avoids the issue, ensuring a clean state and preventing startup problems.

**Relevance and Bug Fix Indication:**

The commit message and the code change are highly relevant. The commit message describes a problem with schema table flushing, and the code change modifies the order in which these tables are truncated. This strongly suggests that the original order was the root cause of the startup problems, and reversing the order fixes the bug. The change addresses a logical error in the truncation process.

**Conclusion:**

Based on the commit message and the code changes, the commit clearly addresses a bug related to schema table flushing during startup.

**Buggy**"
cassandra,17609.json,0f977c597a6b70984de96bcc49474acdb12ad2ea,"@@ -1,24 +1,24 @@
     public int compare(Composite c1, Composite c2)
     {
         // This method assumes that simple composites never have an EOC != NONE. This assumption
         // stands in particular on the fact that a Composites.EMPTY never has a non-NONE EOC. If
         // this ever change, we'll need to update this.
 
         if (isByteOrderComparable)
         {
             // toByteBuffer is always cheap for simple types, and we keep virtual method calls to a minimum:
             // hasRemaining will always be inlined, as will most of the call-stack for BBU.compareUnsigned
             ByteBuffer b1 = c1.toByteBuffer();
             ByteBuffer b2 = c2.toByteBuffer();
             if (!b1.hasRemaining() || !b2.hasRemaining())
                 return b1.hasRemaining() ? 1 : (b2.hasRemaining() ? -1 : 0);
             return ByteBufferUtil.compareUnsigned(b1, b2);
         }
 
         boolean c1isEmpty = c1.isEmpty();
         boolean c2isEmpty = c2.isEmpty();
         if (c1isEmpty || c2isEmpty)
-            return c1isEmpty ? 1 : (c2isEmpty ? -1 : 0);
+            return !c1isEmpty ? 1 : (!c2isEmpty ? -1 : 0);
 
         return type.compare(c1.get(0), c2.get(0));
     }",Buggy,"Fix CellName comparison bugs

Patch by tjake; reviewed by bes for CASSANDRA-7227
",Buggy,"The commit message ""Fix CellName comparison bugs"" suggests that the commit addresses and resolves bugs related to cell name comparison.

The code diff modifies the comparison logic when either `c1` or `c2` is empty. The original code returns 1 if `c1` is empty and -1 if `c2` is empty. The corrected code returns 1 if `c1` is *not* empty and -1 if `c2` is *not* empty. This indicates a logical correction in handling empty composites during comparison, which aligns with the commit message's intention to fix comparison bugs. The original logic was inverted.

Therefore, the code changes indicate a bug fix.

**Buggy**
"
cassandra,16419.json,653dcc63e0f6b0c5b3c3592beb1e0dd3bc1ee0cf,"@@ -1,4 +1,4 @@
     public Long compose(ByteBuffer bytes)
     {
-        return ByteBufferUtil.toLong(bytes);
+        return CounterContext.instance().total(bytes);
     }",Buggy,"fix merge problem with CHANGES.txt, use right call in AbstractCommutativeType.compose()

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@1082155 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates two changes: fixing a merge problem with `CHANGES.txt` and using the right call in `AbstractCommutativeType.compose()`. The provided diff only shows a change in the `compose` method of an unspecified class, replacing `ByteBufferUtil.toLong(bytes)` with `CounterContext.instance().total(bytes)`. This change directly relates to the second part of the commit message, suggesting a correction in the method's implementation. The original call `ByteBufferUtil.toLong(bytes)` likely produced an incorrect result, and the new call `CounterContext.instance().total(bytes)` is intended to fix this, indicating a bug fix.

**Buggy**
"
jgit,9755.json,d2600693bd5fb8bda20bae41467132668caa1e14,"@@ -1,34 +1,40 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
-		while (tokenCount < 3 && nextSpace < lineEnd) {
+		while (tokenCount < 3 && nextSpace <= lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin,
 						nextSpace - tokenBegin - 1, UTF_8);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
-				String commitToken = new String(buf, tokenBegin,
-						nextSpace - tokenBegin - 1, UTF_8);
+				String commitToken;
+				if (nextSpace > lineEnd + 1) {
+					commitToken = new String(buf, tokenBegin,
+							lineEnd - tokenBegin + 1, UTF_8);
+				} else {
+					commitToken = new String(buf, tokenBegin,
+							nextSpace - tokenBegin - 1, UTF_8);
+				}
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit,
 						RawParseUtils.decode(buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
 		if (tokenCount == 2)
 			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix off-by-one error in RebaseTodoFile when reading a todo file

Commit messages of length 1 were not read. 'lineEnd' is the offset
of the last character in the line before the terminating LF or CR-LF,
and 'nextSpace' is actually the offset of the character _after_ the
next space. With a one-character commit message, nextSpace == lineEnd.

The code also assumes the commit message to be optional, but actually
failed in that case because it read beyond the line ending. Fix that,
too.

Add a test case for reading a todo file.

Bug: 546245
Change-Id: I368d63615930ea2398a6230e756442fd88870654
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"The commit message indicates an ""off-by-one error"" and a failure when the commit message is optional. The code diff modifies the `while` loop condition from `nextSpace < lineEnd` to `nextSpace <= lineEnd`. This change addresses the off-by-one error, as the loop now includes the last character. Additionally, the code adds a condition `if (nextSpace > lineEnd + 1)` to handle cases where the commit message is optional or very short, preventing reading beyond the line ending. These changes directly relate to fixing the bugs described in the commit message.

Therefore, the answer is:
**Buggy**"
jgit,9755.json,4feace2b9ecb90fe591c4317403f6d8e6309287e,"@@ -1,32 +1,34 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
 		while (tokenCount < 3 && nextSpace < lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 				String commitToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit, RawParseUtils.decode(
 						buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
+		if (tokenCount == 2)
+			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix parsing Rebase todo lines when commit message is missing

Bug: 422253
Change-Id: I9739b16c91d2df31a481360a712d3479a4eeee2e
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message indicates a fix for parsing ""rebase todo"" lines when the commit message is missing. The code change adds a condition to handle the case where `tokenCount` is 2. This scenario occurs when the action and commit ID are present, but the commit message is missing. In this case, a new `RebaseTodoLine` is created with an empty string as the commit message. This aligns with the commit message's intent to handle missing commit messages in ""rebase todo"" lines.

**Buggy**"
jgit,168.json,846ef78a02edceb99940d7aa92dcd2462a85c602,"@@ -1,24 +1,24 @@
 	private RefTree rebuild(RefDatabase refdb) throws IOException {
 		RefTree tree = RefTree.newEmptyTree();
 		List<org.eclipse.jgit.internal.storage.reftree.Command> cmds
 			= new ArrayList<>();
 
 		Ref head = refdb.exactRef(HEAD);
 		if (head != null) {
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					head));
 		}
 
 		for (Ref r : refdb.getRefs(RefDatabase.ALL).values()) {
-			if (r.getName().equals(txnCommitted)
+			if (r.getName().equals(txnCommitted) || r.getName().equals(HEAD)
 					|| r.getName().startsWith(txnNamespace)) {
 				continue;
 			}
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					db.peel(r)));
 		}
 		tree.apply(cmds);
 		return tree;
 	}",Buggy,"Fix RebuildRefTree trying to add HEAD twice to RefTree

14dfa70520 fixed the problem that HEAD wasn't added to the reftree when
rebuilding the reftree in an empty repository where HEAD isn't yet
resolvable. Since non-resolvable refs are filtered out by
RefDatabase.getRefs(ALL) we have to add HEAD to the reftree explicitly
in this special case.

This fix resulted in another bug: rebuilding the reftree in a repository
which has a resolvable HEAD failed with a DirCacheNameConflictException
in RefTree.apply(). If HEAD is resolvable RefDatabase.getRefs(ALL) does
not filter out HEAD. This results in two identical CREATE commands for
HEAD which RefTree.apply() refuses to execute.

Fix this by no longer creating a duplicate CREATE command for HEAD.

See: I46cbc2611b9ae683ef7319dc46af277925dfaee5
Change-Id: I58dd6bcdef88820aa7de29761d43e2edfa18fcbe
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a bug fix related to rebuilding the RefTree. The previous fix (14dfa70520) introduced a new bug where HEAD was added twice to the RefTree, leading to a DirCacheNameConflictException. The current commit aims to resolve this issue by preventing the duplicate creation of the HEAD command.

The code diff modifies the loop that iterates through the refs obtained from `refdb.getRefs(RefDatabase.ALL)`. It adds a condition `|| r.getName().equals(HEAD)` to the `if` statement, effectively skipping the addition of a command for HEAD if it's already present in the refs. This aligns with the commit message's intention to avoid duplicate CREATE commands for HEAD.

The commit message clearly describes a bug and the code change directly addresses it.

**Buggy**"
jgit,8367.json,ec97912762754ee88f1af5ed80e993c545778242,"@@ -1,26 +1,26 @@
 	public int match(final RawCharSequence rcs) {
 		final int needleLen = needle.length;
 		final byte first = needle[0];
 
 		final byte[] text = rcs.buffer;
 		int matchPos = rcs.startPtr;
 		final int maxPos = rcs.endPtr - needleLen;
 
-		OUTER: for (; matchPos < maxPos; matchPos++) {
+		OUTER: for (; matchPos <= maxPos; matchPos++) {
 			if (neq(first, text[matchPos])) {
-				while (++matchPos < maxPos && neq(first, text[matchPos])) {
+				while (++matchPos <= maxPos && neq(first, text[matchPos])) {
 					/* skip */
 				}
-				if (matchPos == maxPos)
+				if (matchPos > maxPos)
 					return -1;
 			}
 
-			int si = ++matchPos;
+			int si = matchPos + 1;
 			for (int j = 1; j < needleLen; j++, si++) {
 				if (neq(needle[j], text[si]))
 					continue OUTER;
 			}
-			return matchPos - 1;
+			return matchPos;
 		}
 		return -1;
 	}",Buggy,"Fix multiple bugs in RawSubStringPattern used by MessageRevFilter

* Match at end of input was not handled correctly.
* When more than one character matched but not all, the next character
  was not considered as a match start (e.g. pattern ""abab"" didn't match
  input ""abaabab"").

Bug: 409144
Change-Id: Ia44682c618bfbb927f5567c194227421d222a160
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates that the changes fix two bugs in `RawSubStringPattern`. The first bug is related to matching at the end of the input, and the second bug is about incorrect handling of overlapping matches.

The code diff modifies the `match` method. The loop condition `matchPos < maxPos` is changed to `matchPos <= maxPos`. This change addresses the first bug, where the pattern might not be matched if it occurs at the very end of the input.

The line `while (++matchPos < maxPos && neq(first, text[matchPos]))` is changed to `while (++matchPos <= maxPos && neq(first, text[matchPos]))`. This change is consistent with the fix for matching at the end of the input.

The line `int si = ++matchPos;` is changed to `int si = matchPos + 1;`. This change, along with the change from `return matchPos - 1;` to `return matchPos;`, seems to be related to the second bug, where overlapping matches were not correctly handled. By incrementing `matchPos` before assigning it to `si` in the original code, the algorithm was skipping a potential match start.

The changes in the code align with the description of the bugs in the commit message. The modifications address error handling and logical corrections related to the pattern matching algorithm.

**Buggy**"
jgit,6829.json,5e44bfa3ad462e1220426492c53606c6a643a970,"@@ -1,6 +1,10 @@
 	private boolean isNoNewlineAtEndOfFile(FileHeader fh) {
-		HunkHeader lastHunk = fh.getHunks().get(fh.getHunks().size() - 1);
+		List<? extends HunkHeader> hunks = fh.getHunks();
+		if (hunks == null || hunks.isEmpty()) {
+			return false;
+		}
+		HunkHeader lastHunk = hunks.get(hunks.size() - 1);
 		RawText lhrt = new RawText(lastHunk.getBuffer());
-		return lhrt.getString(lhrt.size() - 1).equals(
-				""\""); //$NON-NLS-1$
+		return lhrt.getString(lhrt.size() - 1)
+				.equals(""\""); //$NON-NLS-1$
 	}",Buggy,"Fix ApplyCommand which doesn't work if patch adds empty file

Bug: 548219
Change-Id: Ibb32132a38e54508a24489322da58ddfd80a1d9a
Signed-off-by: Anton Khodos <khodosanton@gmail.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for the `ApplyCommand` that fails when a patch adds an empty file. The code diff modifies the `isNoNewlineAtEndOfFile` method.

Here's a step-by-step analysis:

1.  **Context:** The `isNoNewlineAtEndOfFile` method likely checks if the last line of a file ends with a newline character. This is relevant to patch application because the presence or absence of a newline can affect how the patch is applied.
2.  **Problem:** The original code directly accesses the last hunk of a file header's hunks list without checking if the list is empty. If the patch adds an empty file, the hunks list will be empty, leading to an `IndexOutOfBoundsException`.
3.  **Solution:** The modified code adds a check to ensure that the `hunks` list is not `null` or empty before attempting to access the last element. If the list is empty, it returns `false`, indicating that there is no newline at the end of the (empty) file. This prevents the `IndexOutOfBoundsException`.
4.  **Bug Fix:** The added null and empty check directly addresses the issue described in the commit message, which is that the `ApplyCommand` fails when a patch adds an empty file. The original code had a bug that caused an exception when processing empty files.

Therefore, the changes indicate a bug fix.

**Buggy**"
jgit,9865.json,3e2ec7e5e81a489a326d872aa0670119ff2c1152,"@@ -1,8 +1,8 @@
 	public int compareTo(LfsPointer o) {
 		int x = getOid().compareTo(o.getOid());
 		if (x != 0) {
 			return x;
 		}
 
-		return (int) (getSize() - o.getSize());
+		return Long.compare(getSize(), o.getSize());
 	}",Buggy,"Fix BadComparable error flagged by error prone

Running recent error prone version complaining on that code:

LfsPointer.java:171: error: [BadComparable] Possible sign flip from
narrowing conversion
		return (int) (getSize() - o.getSize());
		       ^
    (see https://errorprone.info/bugpattern/BadComparable)
  Did you mean 'return Long.compare(getSize(), o.getSize());'?

Bug: 562756
Change-Id: I0522f1025319a9290c448a064fbafdb4b16d1d59
Signed-off-by: David Ostrovsky <david@ostrovsky.org>
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message explicitly states that the change is to fix a ""BadComparable"" error flagged by Error Prone. It even includes the specific Error Prone message and a link to the bug pattern documentation. The message also mentions a bug number (562756). This strongly suggests a bug fix.

2.  **Git Diff:** The diff shows a change in the `compareTo` method of the `LfsPointer` class. The original code used a narrowing conversion `(int) (getSize() - o.getSize())` to compare the sizes, which could lead to a sign flip and incorrect comparison results if the difference between the sizes exceeds the range of an `int`. The corrected code uses `Long.compare(getSize(), o.getSize())`, which is the recommended way to compare `long` values and avoids the potential sign flip.

3.  **Relevance:** The commit message and the code change are perfectly aligned. The code change directly addresses the ""BadComparable"" error mentioned in the commit message. The original code had a potential bug due to the narrowing conversion, and the corrected code fixes it by using the appropriate `Long.compare` method.

**Conclusion:**

The commit message and the code diff clearly indicate a bug fix. The original code had a potential issue with the comparison logic, and the change resolves it.

**Buggy**"
jgit,7605.json,56ee8117802a672e80011ee07ea6b253a5b7d0a9,"@@ -1,13 +1,17 @@
 	public boolean isPathSuffix(final byte[] p, final int pLen) {
 		final AbstractTreeIterator t = currentHead;
 		final byte[] c = t.path;
 		final int cLen = t.pathLen;
-		int ci;
 
-		for (ci = 1; ci < cLen && ci < pLen; ci++) {
-			if (c[cLen-ci] != p[pLen-ci])
+		for (int i = 1; i <= pLen; i++) {
+			// Pattern longer than current path
+			if (i > cLen)
+				return false;
+			// Current path doesn't match pattern
+			if (c[cLen - i] != p[pLen - i])
 				return false;
 		}
 
+		// Whole pattern tested -> matches
 		return true;
 	}",Buggy,"Fix bugs in TreeWalk#isPathSuffix used by PathSuffixFilter

* It didn't check the first character in the pattern due to a off-by-one
  error. Spotted by James Roper.
* It returned true even when pattern was longer than current path, e.g.
  it returned that "".txt"" is suffix of ""txt"".

Bug: 411999
Change-Id: I9fbcd68a11fb57cc49956b70c387a47271a0424f
Signed-off-by: Robin Stocker <robin@nibor.org>
",Buggy,"The commit message indicates two bug fixes in `TreeWalk#isPathSuffix`: an off-by-one error in the initial character check and an incorrect return value when the pattern is longer than the current path.

The code diff modifies the `isPathSuffix` method. The original code iterated from `ci = 1`, potentially skipping the first character. The corrected code iterates from `i = 1` to `pLen` inclusive, addressing the off-by-one error. Additionally, the new code includes a check `if (i > cLen) return false;`, which handles the case where the pattern is longer than the current path, ensuring the method returns `false` in such scenarios. These changes directly address the bugs described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
jgit,7675.json,5b55498b16267102b021a47b3a7c0bdbfae63e71,"@@ -1,5 +1,3 @@
 	protected byte[] idSubmodule(final Entry e) {
-		if (repository == null)
-			return idSubmodule(getDirectory(), e);
-		return super.idSubmodule(e);
+		return idSubmodule(getDirectory(), e);
 	}",Buggy,"Fix FileTreeIterator.idSubmodule(Entry)

FileTreeIterator was calling by mistake
WorkingTreeIterator.idSubmodule(Entry). Instead it should always compute
idSubmodule on its own.

Change-Id: Id1b988aded06939b1d7edd2671e34bf756896c0e
",Buggy,"The commit message indicates a bug fix related to how `FileTreeIterator` determines the ID of a submodule. It states that `FileTreeIterator` was incorrectly calling `WorkingTreeIterator.idSubmodule(Entry)` instead of computing the ID on its own. The provided diff shows that the code has been modified to remove the conditional check `if (repository == null)` and the call to `super.idSubmodule(e)`. Now, it always calls `idSubmodule(getDirectory(), e)`. This change aligns with the commit message, suggesting that the previous implementation was indeed a bug.

Reasoning:

1.  **Commit Message Analysis:** The commit message explicitly mentions a mistake in `FileTreeIterator.idSubmodule(Entry)` where it was calling the wrong method. This strongly suggests a bug fix.
2.  **Diff Analysis:** The diff shows the removal of a conditional statement and a call to the superclass's method. This change directly addresses the issue described in the commit message, indicating that the previous behavior was incorrect.
3.  **Alignment:** The code change aligns perfectly with the commit message's description of the bug and its resolution.

Conclusion: **Buggy**"
jgit,4217.json,e60b9e1879f8774e1afe07be4224605045f49eec,"@@ -1,8 +1,9 @@
 	private long getEffectiveRacyThreshold() {
 		long timestampResolution = fileStoreAttributeCache
 				.getFsTimestampResolution().toNanos();
 		long minRacyInterval = fileStoreAttributeCache.getMinimalRacyInterval()
 				.toNanos();
-		// add a 30% safety margin
-		return Math.max(timestampResolution, minRacyInterval) * 13 / 10;
+		long max = Math.max(timestampResolution, minRacyInterval);
+		// safety margin: factor 2.5 below 100ms otherwise 1.25
+		return max < 100_000_000L ? max * 5 / 2 : max * 5 / 4;
 	}",Buggy,"FileSnapshot: fix bug with timestamp thresholding

Increase the safety factor to 2.5x for extra safety if max of measured
timestamp resolution and measured minimal racy threshold is < 100ms, use
1.25 otherwise since for large filesystem resolution values the
influence of finite resolution of the system clock should be negligible.

Before, not yet using the newly introduced minRacyThreshold measurement,
the threshold was 1.1x FS resolution, and we could issue the
following sequence of events,

  start
  create-file
  read-file (currentTime)
  end

which had the following timestamps:

  create-file 1564589081998
  start 1564589082002
  read 1564589082003
  end 1564589082004

In this case, the difference between create-file and read is 5ms,
which exceeded the 4ms FS resolution, even though the events together
took just 2ms of runtime.

Reproduce with:
  bazel test --runs_per_test=100 \
    //org.eclipse.jgit.test:org_eclipse_jgit_internal_storage_file_FileSnapshotTest

The file system timestamp resolution is 4ms in this case.

This code assumes that the kernel and the JVM use the same clock that
is synchronized with the file system clock. This seems plausible,
given the resolution of System.currentTimeMillis() and the latency for
a gettimeofday system call (typically ~1us), but it would be good to
justify this with specifications.

Also cover a source of flakiness: if the test runs under extreme load,
then we could have

  start
  create-file
  <long delay>
  read
  end

which would register as an unmodified file. Avoid this by skipping the
test if end-start is too big.

[msohn]:
- downported from master to stable-5.1
- skip test if resolution is below 10ms
- adjust safety factor to 1.25 for resolutions above 100ms

Change-Id: I87d2cf035e01c44b7ba8364c410a860aa8e312ef
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>
",Buggy,"The commit message describes a bug related to timestamp thresholding in file snapshots. The original threshold calculation was insufficient, leading to false positives where files were incorrectly identified as unmodified due to the interplay between file system resolution and the timing of events. The commit message provides a specific scenario where this could occur, along with steps to reproduce the issue.

The fix involves increasing the safety factor applied to the timestamp resolution and minimal racy interval. The safety factor is now dynamically adjusted based on the magnitude of the maximum of the timestamp resolution and the minimal racy interval. If this maximum is less than 100ms, a safety factor of 2.5 is applied; otherwise, a safety factor of 1.25 is used. This adjustment aims to provide a larger safety margin for smaller resolutions, where the influence of clock resolution is more significant.

The code diff directly reflects this change by modifying the `getEffectiveRacyThreshold()` method. The original calculation, which added a 30% safety margin, is replaced with the new logic that incorporates the dynamic safety factor.

The commit message clearly indicates a bug fix, and the code diff implements the described solution.

**Buggy**
"
jgit,5424.json,35b01dac4c81542b195169e3b7365c12a165232c,"@@ -1,45 +1,41 @@
 	public void fillTo(final int highMark) throws MissingObjectException,
 			IncorrectObjectTypeException, IOException {
 		if (walker == null || size > highMark)
 			return;
 
-		Generator p = walker.pending;
-		RevCommit c = p.next();
+		RevCommit c = walker.next();
 		if (c == null) {
-			walker.pending = EndGenerator.INSTANCE;
 			walker = null;
 			return;
 		}
 		enter(size, (E) c);
 		add((E) c);
-		p = walker.pending;
 
 		while (size <= highMark) {
 			int index = size;
 			Block s = contents;
 			while (index >> s.shift >= BLOCK_SIZE) {
 				s = new Block(s.shift + BLOCK_SHIFT);
 				s.contents[0] = contents;
 				contents = s;
 			}
 			while (s.shift > 0) {
 				final int i = index >> s.shift;
 				index -= i << s.shift;
 				if (s.contents[i] == null)
 					s.contents[i] = new Block(s.shift - BLOCK_SHIFT);
 				s = (Block) s.contents[i];
 			}
 
 			final Object[] dst = s.contents;
 			while (size <= highMark && index < BLOCK_SIZE) {
-				c = p.next();
+				c = walker.next();
 				if (c == null) {
-					walker.pending = EndGenerator.INSTANCE;
 					walker = null;
 					return;
 				}
 				enter(size++, (E) c);
 				dst[index++] = c;
 			}
 		}
 	}",Buggy,"Fix RevCommitList to work with subclasses of RevWalk

Bug: 321502
Change-Id: Ic4bc49a0da90234271aea7c0a4e344a1c3620cfc
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for `RevCommitList` to work correctly with subclasses of `RevWalk`. The code diff modifies the way the next commit is retrieved from the walker. Specifically, it replaces `walker.pending.next()` with `walker.next()`. It also removes the setting of `walker.pending` to `EndGenerator.INSTANCE` when `walker.next()` returns null. These changes suggest that the previous implementation had issues when dealing with subclasses of `RevWalk`, possibly due to incorrect handling of the commit generation process. The removal of setting `walker.pending` to `EndGenerator.INSTANCE` also indicates a correction in how the end of the commit list is handled.

Therefore, the changes indicate a bug fix.

**Buggy**"
jgit,5825.json,b07db609089749ed49a7f0b1fb3841a8f74110c2,"@@ -1,13 +1,16 @@
 	static int count(String s, char c, boolean ignoreFirstLast) {
 		int start = 0;
 		int count = 0;
-		while (true) {
+		int length = s.length();
+		while (start < length) {
 			start = s.indexOf(c, start);
-			if (start == -1)
+			if (start == -1) {
 				break;
-			if (!ignoreFirstLast || (start != 0 && start != s.length()))
+			}
+			if (!ignoreFirstLast || (start != 0 && start != length - 1)) {
 				count++;
+			}
 			start++;
 		}
 		return count;
 	}",Buggy,"Fix off-by-one error in Strings.count()

Change-Id: I0667b1624827d1cf0cc1b81f86c7bb44eafd68a7
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix off-by-one error in Strings.count()"" clearly states that the commit intends to correct an off-by-one error within the `Strings.count()` method. This strongly suggests a bug fix.

**Code Diff Analysis:**

1.  **`length` variable:** A new variable `length` is introduced to store the length of the input string `s`. This is a minor optimization, but it also suggests that the length of the string is important for the logic.
2.  **Loop condition:** The `while` loop condition is changed from `while (true)` to `while (start < length)`. This change is crucial. The original `while (true)` loop relied solely on the `s.indexOf()` method returning -1 to terminate. If the `indexOf` method never returned -1, the loop would continue indefinitely. The new condition ensures that the loop terminates when `start` reaches the end of the string. This is a common pattern to prevent infinite loops and is often related to fixing index-related errors.
3.  **`ignoreFirstLast` condition:** The condition `start != s.length()` is changed to `start != length - 1`. This is the most significant change. The original condition would exclude the last character of the string from being counted when `ignoreFirstLast` is true. However, `s.length()` returns the length of the string, and the last index is `length - 1`. Therefore, the original condition was indeed off-by-one.

**Relevance and Bug Fix Identification:**

The code changes directly address the issue described in the commit message. The modification to the loop condition prevents potential infinite loops, and the adjustment to the `ignoreFirstLast` condition corrects the off-by-one error when excluding the first and last characters. The changes are highly relevant to the commit message and clearly indicate a bug fix.

**Conclusion:**

**Buggy**
"
jgit,6619.json,51a5cc7f1a1033664ee2fb760ed217b665b12b34,"@@ -1,6 +1,6 @@
 	private static int findForwardLine(IntList lines, int idx, int ptr) {
 		final int end = lines.size() - 2;
-		while (idx < end && lines.get(idx + 2) <= ptr)
+		while (idx < end && lines.get(idx + 2) < ptr)
 			idx++;
 		return idx;
 	}",Buggy,"Fix diff when first text is the start of the other

The problem occurred when the first text ends in the middle
of the last line of the other text and the first text has no
end of line.

Bug: 344975
Change-Id: I1f0dd9f8062f2148a7c1341c9122202e082ad19d
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
",Buggy,"The commit message indicates a fix for a specific scenario in diff calculation where the first text ends within the last line of the second text, and the first text lacks an end-of-line character. The bug report number ""344975"" further confirms this is a bug fix.

The code change modifies the condition in the `while` loop from `lines.get(idx + 2) <= ptr` to `lines.get(idx + 2) < ptr`. This change affects how the `findForwardLine` method searches for a suitable line index. The original condition could potentially include an incorrect line index in certain edge cases, which aligns with the problem described in the commit message. The change ensures the loop continues as long as the line position at `idx + 2` is strictly less than `ptr`, thus correcting the line matching logic.

Therefore, the code change directly addresses the bug described in the commit message by refining the line matching logic within the `findForwardLine` method.

**Buggy**"
jgit,6447.json,e0e52cb0110a908959f7df13c5e66bf911bad74f,"@@ -1,6 +1,6 @@
 	private boolean result(Candidate n) throws IOException {
 		n.beginResult(revPool);
 		outCandidate = n;
 		outRegion = n.regionList;
-		return true;
+		return outRegion != null;
 	}",Buggy,"Fix NPE in BlameGenerator.getSourceStart()

Bug: 499543
Change-Id: I99f6ebb1c3ceea20e8ca093acbe824c9f0362d45
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `BlameGenerator.getSourceStart()`. The provided code diff modifies the `result(Candidate n)` method to return `outRegion != null` instead of always returning `true`. This change directly addresses a potential NPE by ensuring that the method only returns true if `outRegion` is not null. If `outRegion` is null, the method will return `false`, preventing a potential NPE from occurring when `outRegion` is subsequently used. This aligns with the commit message's intention to fix an NPE.

**Buggy**"
jgit,6994.json,0505657d6a7dd4575a64ddfb5c0928870fe5843b,"@@ -1,9 +1,9 @@
 	public CommitCommand setAll(boolean all) {
 		checkCallable();
-		if (!only.isEmpty())
+		if (all && !only.isEmpty())
 			throw new JGitInternalException(MessageFormat.format(
 					JGitText.get().illegalCombinationOfArguments, ""--all"", //$NON-NLS-1$
 					""--only"")); //$NON-NLS-1$
 		this.all = all;
 		return this;
 	}",Buggy,"commit command: allow to specify path(s) argument(s)

This fixes the command below:

jgit commit a -m ""added file a""

which currently fails with:

org.eclipse.jgit.api.errors.JGitInternalException: The combination of
arguments --all and --only is not allowed

Bug: 484973
Change-Id: I37a4ccd68101a66520ef99110f7aa0cbdcc8beba
Signed-off-by: Andrey Loskutov <loskutov@gmx.de>
",Buggy,"The commit message indicates a bug fix for a specific scenario where using both `--all` and `--only` flags in the `commit` command leads to a `JGitInternalException`. The provided diff modifies the `setAll` method to check if `all` is true *and* `only` is not empty before throwing the exception. This suggests that the original code was throwing the exception even when `all` was false, which is now corrected. The commit message explicitly mentions the bug number (484973) and the problematic command, further confirming that this commit addresses a bug.

**Buggy**"
jgit,7416.json,e56d50a94bfe60663bd589da61e07b9461f2c5f7,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
+		// Add the previous message without header (i.e first line)
+		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
+		sb.append(""\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
 			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
-		// Add the previous message without header (i.e first line)
-		sb.append(""\n"");
-		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Interactive rebase: Fix order of commit messages on squash

Bug: 431214
Change-Id: I295bfdc5751545b046d7fe7efc3f8b39ab4f5415
Signed-off-by: Stefan Lay <stefan.lay@sap.com>
Signed-off-by: Robin Rosennberg <robin.rosenberg@dewire.com>
",Buggy,"The commit message indicates a fix related to the order of commit messages during a squash operation in an interactive rebase. The code diff modifies the `composeSquashMessage` method, specifically reordering the placement of the previous squash message and the current commit message being processed. The original code appended the previous squash message *after* the current commit message. The modified code now appends the previous squash message *before* the current commit message. This reordering suggests that the original order was incorrect, leading to a bug in how squash messages were composed. Therefore, the change is a bug fix.

**Buggy**"
jgit,7416.json,8339a07e8314d6a40e15252bcc736a46c0aca0ea,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
-			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r]+)"",
+			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
 		// Add the previous message without header (i.e first line)
 		sb.append(""\n"");
 		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Fix FIXUP error for blank lines in interactive rebase

Empty lines of discarded commit messages were added to the commit
message because they were not commented out properly.

Bug: 422246
Change-Id: I263e8a6b30a3392d8b4f09c0695505068a0a485d
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message indicates a fix for a bug where empty lines in discarded commit messages were incorrectly added to the commit message during an interactive rebase. The bug number is 422246.

The code diff modifies the `composeSquashMessage` method. Specifically, it changes the regular expression used to replace newline characters in the commit message. The original regex `([\n\r]+)` matches one or more occurrences of newline or carriage return characters. The modified regex `([\n\r])` matches a single newline or carriage return character. The replacement string `$1# ` adds a `#` character after each newline or carriage return, effectively commenting out the line.

The original regex `([\n\r]+)` would collapse multiple consecutive newline characters into a single newline followed by `# `. This would lead to empty lines not being commented out. The corrected regex `([\n\r])` ensures that each newline is followed by `# `, thus correctly commenting out empty lines.

The change aligns with the commit message and fixes the described bug.

**Buggy**
"
jgit,9377.json,325cb35ccd6108eaf9e6c3ec6343ca6988f10a21,"@@ -1,6 +1,8 @@
 	private ObjectId idFor(int objType, byte[] raw) {
 		if (skipList != null) {
-			return new ObjectInserter.Formatter().idFor(objType, raw);
+			try (ObjectInserter.Formatter fmt = new ObjectInserter.Formatter()) {
+				return fmt.idFor(objType, raw);
+			}
 		}
 		return null;
 	}",Buggy,"[infer] Fix resource leak in ObjectChecker

Bug: 509385
Change-Id: I6b6ff5b721d959eb0708003a40c8f97d6826ac46
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a resource leak in `ObjectChecker`. The diff shows that the `ObjectInserter.Formatter` is now used in a try-with-resources block. This ensures that the formatter is closed properly after use, preventing a potential resource leak. The change aligns with the commit message and addresses a bug.

**Buggy**"
weka,20283.json,8215c432375cb2ff1bbda2825426fa0535b77834,"@@ -1,25 +1,13 @@
-  public EventSetDescriptor [] getEventSetDescriptors() {
+  public EventSetDescriptor[] getEventSetDescriptors() {
     try {
-      EventSetDescriptor [] esds = 
-      { new EventSetDescriptor(DataSource.class, 
-                               ""dataSet"", 
-                               DataSourceListener.class, 
-                               ""acceptDataSet""),
-        new EventSetDescriptor(DataSource.class, 
-                               ""instance"", 
-                               InstanceListener.class, 
-                               ""acceptInstance""),
-        new EventSetDescriptor(TrainingSetProducer.class, 
-                               ""trainingSet"", 
-                               TrainingSetListener.class, 
-                               ""acceptTrainingSet""),
-        new EventSetDescriptor(TestSetProducer.class, 
-                               ""testSet"", 
-                               TestSetListener.class, 
-                               ""acceptTestSet"")  };
+      EventSetDescriptor[] esds = {
+        new EventSetDescriptor(DataSource.class, ""dataSet"",
+          DataSourceListener.class, ""acceptDataSet""),
+        new EventSetDescriptor(DataSource.class, ""instance"",
+          InstanceListener.class, ""acceptInstance""), };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to event types produced by Appender and the routine determining event generation. The code diff removes `TrainingSetProducer` and `TestSetProducer` event set descriptors, suggesting these were incorrectly specified or causing issues. The removal of these descriptors aligns with the commit message's claim of fixing a bug in the specification of event types. The try-catch block suggests that the original code might have been prone to exceptions during the creation of the EventSetDescriptors.

**Buggy**"
weka,26539.json,88c70a2f184a0b7b2d27a48ebc04083d0d800049,"@@ -1,3 +1,6 @@
   public static List<String> getAllMetricNames() {
-    return Evaluation.getAllEvaluationMetricNames();
+    List<String> metrics = getBuiltInMetricNames();
+    metrics.addAll(getPluginMetricNames());
+
+    return metrics;
   }",Buggy,"Fixed a bug in the getAllMetricNames() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10919 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug in the getAllMetricNames() method"" clearly states that the commit is intended to address a bug within the `getAllMetricNames()` method. This sets the expectation that the code changes will be related to correcting some erroneous behavior or unintended consequence in that method.

**Git Diff Analysis:**

The diff shows a modification to the `getAllMetricNames()` method.  The original implementation simply returned the result of `Evaluation.getAllEvaluationMetricNames()`. The modified implementation now does the following:

1.  Gets built-in metric names using `getBuiltInMetricNames()`.
2.  Gets plugin metric names using `getPluginMetricNames()`.
3.  Combines both lists into a single list called `metrics`.
4.  Returns the combined `metrics` list.

**Reasoning:**

The change suggests that the original implementation of `getAllMetricNames()` was incomplete or incorrect. It appears that it was only returning built-in metric names and not including metric names from plugins. The fix involves retrieving both types of metric names and merging them into a single list before returning. This strongly indicates that the original method had a bug where it was not returning all available metric names, and the commit fixes this bug by including plugin metric names. The commit message aligns perfectly with the code changes.

**Conclusion:**

**Buggy**
"
weka,20282.json,289721c0795b80e42b2664ffaa47dfab08f7ceac,"@@ -1,6 +1,7 @@
   private String statusMessagePrefix() {
     return getCustomName() + ""$"" + hashCode() + ""|""
-    + ((m_Filter instanceof OptionHandler) 
+    + ((m_Filter instanceof OptionHandler &&
+        Utils.joinOptions(((OptionHandler)m_Filter).getOptions()).length() > 0) 
         ? Utils.joinOptions(((OptionHandler)m_Filter).getOptions()) + ""|""
             : """");
   }",Buggy,"Fixed a minor status logging bug.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4797 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed a minor status logging bug"" suggests a correction to the status logging mechanism.

The code diff modifies the `statusMessagePrefix()` method. Specifically, it adds a condition to check if the `m_Filter` is an instance of `OptionHandler` AND if the length of the joined options obtained from `m_Filter` is greater than 0.

This change suggests that the previous implementation might have been logging status messages incorrectly or incompletely when the `m_Filter` was an `OptionHandler` but had no options. The added condition ensures that options are only included in the status message if they exist, which aligns with fixing a logging bug.

Therefore, the changes indicate a bug fix.

**Buggy**
"
weka,19858.json,06022e54131102f3817c490551c2babc818ef2e2,"@@ -1,47 +1,49 @@
   protected void newFlow() {
     m_newFlowBut.setEnabled(false);
 
     String user = m_viewer.getUser();
     String password = m_viewer.getPassword();
     String uRL = m_viewer.getURL();
     String query = m_viewer.getQuery();
 
     if (query == null) {
       query = """";
     }
 
     try {
       DatabaseLoader dbl = new DatabaseLoader();
       dbl.setUser(user);
       dbl.setPassword(password);
       dbl.setUrl(uRL);
       dbl.setQuery(query);
 
       BeanContextSupport bc = new BeanContextSupport();
       bc.setDesignTime(true);
 
       Loader loaderComp = new Loader();
       bc.add(loaderComp);
       loaderComp.setLoader(dbl);
 
       KnowledgeFlowApp singleton = KnowledgeFlowApp.getSingleton();
       m_mainPerspective.addTab(""DBSource"");
-      /*
-       * BeanInstance beanI = new
-       * BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
-       * .getNumTabs() - 1), loaderComp, 50, 50, m_mainPerspective.getNumTabs()
-       * - 1);
-       */
-      Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
-        .getNumTabs() - 1);
-      Vector<BeanConnection> connections = BeanConnection
-        .getConnections(m_mainPerspective.getNumTabs() - 1);
-      singleton.integrateFlow(beans, connections, true, false);
+
+      // The process of creating a BeanInstance integrates will result
+      // in it integrating itself into the flow in the specified tab
+      new BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
+        .getNumTabs() - 1), loaderComp, 50, 50,
+        m_mainPerspective.getNumTabs()
+        - 1);
+
+      // Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
+      // .getNumTabs() - 1);
+      // Vector<BeanConnection> connections = BeanConnection
+      // .getConnections(m_mainPerspective.getNumTabs() - 1);
+      // singleton.integrateFlow(beans, connections, true, false);
       singleton.setActivePerspective(0); // switch back to the main perspective
 
       m_newFlowBut.setEnabled(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a bug where the code that creates the DatabaseLoader component when the new flow button is pressed got accidently commented out.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11286 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the code for creating the DatabaseLoader component was accidentally commented out. The diff shows that a block of code related to creating a `BeanInstance` for the `loaderComp` was uncommented. The commented-out code involved creating `BeanInstance`, retrieving beans and connections, and integrating the flow. The uncommented code directly creates a `BeanInstance`, which integrates the loader component into the flow. This aligns with the commit message's claim of fixing a bug where the component creation was commented out.

Therefore, the change indicates a bug fix.
**Buggy**
"
weka,5643.json,a6289b3f0cf2a2301bce6c1a9f33e7f522996b43,"@@ -1,24 +1,28 @@
   protected Instance mergeInstances(Instance source, Instance dest) {
 
     Instances outputFormat = outputFormatPeek();
     double[] vals = new double[outputFormat.numAttributes()];
     for(int i = 0; i < vals.length; i++) {
       if ((i != outputFormat.classIndex()) && (m_SelectedCols.isInRange(i))) {
         if (source != null) {
           vals[i] = source.value(i);
         } else {
           vals[i] = Utils.missingValue();
         }
       } else {
         vals[i] = dest.value(i);
       }
     }
     Instance inst = null;
     if (dest instanceof SparseInstance) {
       inst = new SparseInstance(dest.weight(), vals);
     } else {
       inst = new DenseInstance(dest.weight(), vals);
     }
-    inst.setDataset(dest.dataset());
+    // inst.setDataset(dest.dataset());
+    // push() sets the dataset to the output format, however, if
+    // a preview transformation is being done then push() does not
+    // get called, so set the output format correctly here.
+    inst.setDataset(outputFormat);
     return inst;
   }",Buggy,"Fixed a bug that affected the structure (dataset) assigned to a transformed instance when doing a preview transformation

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13412 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the dataset assigned to a transformed instance during a preview transformation. The code diff shows that the line `inst.setDataset(dest.dataset());` has been commented out and replaced with `inst.setDataset(outputFormat);`. The comment explains that `push()` normally sets the dataset to the output format, but this doesn't happen during preview transformations. Therefore, the dataset is explicitly set to `outputFormat` in this case. This change directly addresses the bug described in the commit message.

**Buggy**"
weka,31430.json,ac9fe6291a2243cdf039b445dd9190343c4c6d51,"@@ -1,18 +1,13 @@
   public final Matrix transpose() {
 
     int nr = m_Elements.length, nc = m_Elements[0].length;
-    Matrix b;
-    try {
-      b = (Matrix)clone();
-    } catch (CloneNotSupportedException ex) {
-      b = new Matrix(nr, nc);
-    }
+    Matrix b = new Matrix(nc, nr);
 
     for(int i = 0;i < nc; i++) {
       for(int j = 0; j < nr; j++) {
 	b.m_Elements[i][j] = m_Elements[j][i];
       }
     }
 
     return b;
   }",Buggy,"Fixed the matrix transpose problem introduced with last checkin.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@773 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a matrix transpose problem introduced in the last check-in. The code diff shows that the `transpose()` method was modified.

Here's a breakdown of the changes:

1.  **Original Code:** The original code attempts to create the transposed matrix `b` by cloning the current matrix. If cloning fails (CloneNotSupportedException), it creates a new matrix with the same dimensions as the original. This is incorrect because the transposed matrix should have dimensions swapped (rows become columns and vice versa).

2.  **Modified Code:** The modified code directly creates a new matrix `b` with the correct dimensions for the transposed matrix (nc, nr), where nc is the number of columns and nr is the number of rows of the original matrix.

3.  **Loop Logic:** The loop logic for assigning values to the transposed matrix `b` remains the same, correctly swapping the row and column indices: `b.m_Elements[i][j] = m_Elements[j][i];`.

The change directly addresses the incorrect dimensions of the transposed matrix, which was a bug. The commit message aligns perfectly with the code changes.

**Buggy**
"
weka,30779.json,a24ded419dad69fad9e5bdfca523043e633de4a1,"@@ -1,32 +1,34 @@
     public Instance nextElement(Instances dataset) {
       Instance	result;
       
       result = null;
       
       if (isIncremental()) {
 	// is there still an instance in the buffer?
 	if (m_IncrementalBuffer != null) {
 	  result              = m_IncrementalBuffer;
 	  m_IncrementalBuffer = null;
 	}
 	else {
 	  try {
 	    result = m_Loader.getNextInstance(dataset);
 	  }
 	  catch (Exception e) {
 	    e.printStackTrace();
 	    result = null;
 	  }
 	}
       }
       else {
 	if (m_BatchCounter < m_BatchBuffer.numInstances()) {
 	  result = m_BatchBuffer.instance(m_BatchCounter);
 	  m_BatchCounter++;
 	}
       }
 
-      result.setDataset(dataset);
+      if (result != null) {
+        result.setDataset(dataset);
+      }
       
       return result;
     }",Buggy,"Fixed a bug where a null pointer could get dereferenced in the nextElement() method of DataSource.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6417 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a potential null pointer dereference in the `nextElement()` method of the `DataSource`.

The diff shows that the `result` variable, which can be assigned `null` in several branches (e.g., when `m_IncrementalBuffer` is null and `m_Loader.getNextInstance()` throws an exception, or when `isIncremental()` is false and `m_BatchCounter` is not less than `m_BatchBuffer.numInstances()`), is now checked for null before calling `result.setDataset(dataset)`. This prevents a `NullPointerException` if `result` is indeed null.

The code change directly addresses the potential null pointer dereference described in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,14864.json,8ed966e4e1e65cf3ca1691aac3767c74ad03ae50,"@@ -1,214 +1,211 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
-    }
-    else {
+    } else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
-    }
-    else {
+    } else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
-    lookup.put(hashC, """");
+    lookup.put(hashC, new Double(best_merit));
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
-	  done = 2;
-	  sd = SELECTION_FORWARD;
-	}
-      else {
+        done = 2;
+        sd = SELECTION_FORWARD;
+      } else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
-	  }
-	  else {
+	  } else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
-
+          
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
 	      size++;
-	    }
-	    else {
+	    } else {
 	      temp_group.clear(i);
 	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
+	    
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
-
-	      if (m_debug) {
-		System.out.print(""Group: "");
-		printGroup(tt, m_numAttribs);
-		System.out.println(""Merit: "" + merit);
-	      }
-
-	      // is this better than the best?
-	      if (sd == SELECTION_FORWARD) {
-		z = ((merit - best_merit) > 0.00001);
-	      }
-	      else {
-		if (merit == best_merit) {
-		  z = (size < best_size);
-		} else {
-		  z = (merit >  best_merit);
-		} 
-	      }
-
-	      if (z) {
-		added = true;
-		stale = 0;
-		best_merit = merit;
-		//		best_size = (size + best_size);
-		best_size = size;
-		best_group = (BitSet)(temp_group.clone());
-	      }
-
+	      
+	      // insert this one in the hashtable
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
-	      // insert this one in the list and in the hash table
-	      Object [] add = new Object[1];
-	      add[0] = tt.clone();
-	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
-	      lookup.put(hashC, """");
-	      insertCount++;
+    	      lookup.put(hashC, new Double(merit));
+    	      insertCount++;
 	    } else {
-	      cacheHits++;
+	      merit = ((Double)lookup.get(hashC)).doubleValue();
+	      cacheHits++;  
+	    }
+	    
+	    // insert this one in the list
+	    Object[] add = new Object[1];
+	    add[0] = tt.clone();
+	    bfList.addToList(add, merit);
+	    
+	    if (m_debug) {
+	      System.out.print(""Group: "");
+	      printGroup(tt, m_numAttribs);
+	      System.out.println(""Merit: "" + merit);
+	    }
+
+	    // is this better than the best?
+	    if (sd == SELECTION_FORWARD) {
+	      z = ((merit - best_merit) > 0.00001);
+	    } else {
+	      if (merit == best_merit) {
+		z = (size < best_size);
+	      } else {
+		z = (merit >  best_merit);
+	      } 
+	    }
+
+	    if (z) {
+	      added = true;
+	      stale = 0;
+	      best_merit = merit;
+	      //		best_size = (size + best_size);
+	      best_size = size;
+	      best_group = (BitSet)(temp_group.clone());
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
 	      size--;
-	    }
-	    else {
+	    } else {
 	      temp_group.set(i);
 	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Added Martin Guetlein's bug fix for merit caching


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4162 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to merit caching. The code diff shows changes in how the merit values are stored and retrieved from the `lookup` Hashtable. Previously, the code only stored a placeholder value ("""") in the `lookup` table. Now, the actual merit value (a Double object) is stored. Also, the code now retrieves the merit value from the `lookup` table if the subset has been seen before. This suggests that the previous implementation was not correctly caching and reusing merit values, which could lead to incorrect results or performance issues.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,14864.json,89300e74cf24e09a676e8b2c132e4381f57905ac,"@@ -1,208 +1,216 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
     lookup.put(hashC, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
 	  done = 2;
 	  sd = SELECTION_FORWARD;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
+	      size++;
 	    }
 	    else {
 	      temp_group.clear(i);
+	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == SELECTION_FORWARD) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
-		z = ((merit >= best_merit) && ((size) < best_size));
+		if (merit == best_merit) {
+		  z = (size < best_size);
+		} else {
+		  z = (merit >  best_merit);
+		} 
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		//		best_size = (size + best_size);
 		best_size = size;
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
 	      // insert this one in the list and in the hash table
 	      Object [] add = new Object[1];
 	      add[0] = tt.clone();
 	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
 	      lookup.put(hashC, """");
 	      insertCount++;
 	    } else {
 	      cacheHits++;
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
+	      size--;
 	    }
 	    else {
 	      temp_group.set(i);
+	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed bug in backward mode


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2250 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug in backward mode"" suggests a bug fix related to the backward search direction in the feature selection algorithm.

The diff shows a change in the condition for accepting a new best subset during backward selection. The original condition was:
```java
z = ((merit >= best_merit) && ((size) < best_size));
```
The modified condition is:
```java
if (merit == best_merit) {
  z = (size < best_size);
} else {
  z = (merit >  best_merit);
}
```
This change ensures that if a subset has the same merit as the current best, it's only accepted if it's smaller (fewer attributes). If the merit is greater than the current best, it's accepted regardless of size. This fixes a potential issue where the algorithm might not select the smallest subset with the best merit in backward mode.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,14864.json,316eaac24eb968e7441b31b4e54c4ba10926a75a,"@@ -1,191 +1,189 @@
   public int[] search (int[] startSet, ASEvaluation ASEval, Instances data)
     throws Exception
   {
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (startSet != null) {
       m_starting = startSet;
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable((int)(200.0*m_numAttribs*1.5));
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {if (m_searchDirection == -1) {
       m_starting = new int[m_numAttribs];
 
       // init initial subset to all attributes
       for (i = 0, j = 0; i < m_numAttribs; i++) {
 	if (i != m_classIndex) {
 	  best_group.set(i);
 	  m_starting[j++] = i;
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_numAttribs - 1;
       m_totalEvals++;
     }
     }
 
+    // evaluate the initial subset
+    best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     bfList.addToList(best_group, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     lookup.put(tt, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == 0) // bi-directional search
 	{
 	  done = 2;
 	  sd = 1;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getGroup().clone());
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == 1) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == 1) {
 	      temp_group.set(i);
 	    }
 	    else {
 	      temp_group.clear(i);
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 
 	    if (lookup.containsKey(tt) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == 1) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
 		z = ((merit >= best_merit) && ((size + sd) < best_size));
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		best_size = (size + best_size);
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      // insert this one in the list and in the hash table
 	      bfList.addToList(tt, merit);
 	      lookup.put(tt, """");
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == 1) {
 	      temp_group.clear(i);
 	    }
 	    else {
 	      temp_group.set(i);
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = -1;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed small bug in evaluation of an empty subset.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@169 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed small bug in evaluation of an empty subset"" suggests a bug fix related to how the code handles empty subsets during evaluation.

The diff shows that the initial evaluation of the `best_merit` was moved after the initialization of the `best_group` based on the `m_starting` set. This change ensures that `best_merit` is always initialized with the correct value based on the initial subset, even if it's empty. Previously, the initial evaluation was performed before the `best_group` was fully initialized, potentially leading to incorrect results when the initial subset was empty.

This change directly addresses the scenario described in the commit message, indicating a bug fix.

**Buggy**"
weka,20981.json,6c77ccd674985be37160a0788865e371bf1cb5e1,"@@ -1,49 +1,53 @@
   private void saveLayout(int tabIndex, boolean showDialog) {
     // m_loadB.setEnabled(false);
     // m_saveB.setEnabled(false);
     java.awt.Color bckC = getBackground();
 
     File sFile = m_mainKFPerspective.getFlowFile(tabIndex);
     int returnVal = JFileChooser.APPROVE_OPTION;
+    boolean shownDialog = false;
 
     if (showDialog || sFile.getName().equals(""-NONE-"")) {
       returnVal = m_FileChooser.showSaveDialog(this);
+      shownDialog = true;
     }
 
     if (returnVal == JFileChooser.APPROVE_OPTION) {
       // temporarily remove this panel as a property changle listener from
       // each bean
 
       Vector beans = BeanInstance.getBeanInstances(tabIndex);
       detachFromLayout(beans);
 
-      // determine filename
-      sFile = m_FileChooser.getSelectedFile();
+      // determine filename (if necessary)
+      if (shownDialog) {
+        sFile = m_FileChooser.getSelectedFile();
+      }
 
       // add extension if necessary
       if (m_FileChooser.getFileFilter() == m_KfFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION)) {
           sFile = new File(sFile.getParent(), sFile.getName() + FILE_EXTENSION);
         }
       } else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + KOML.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XStreamFilter) {
         if (!sFile.getName().toLowerCase()
             .endsWith(XStream.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + XStream.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION_XML)) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + FILE_EXTENSION_XML);
         }
       }
 
       saveLayout(sFile, m_mainKFPerspective.getCurrentTabIndex(), false);
       m_mainKFPerspective.setFlowFile(tabIndex, sFile);
     }
   }",Buggy,"Fixed a bug where a tab would get saved with the filename associated with another tab

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9860 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix where a tab would get saved with the filename associated with another tab. The code diff modifies the `saveLayout` method.

Here's a breakdown of the changes:

1.  **`boolean shownDialog = false;`**: A new boolean variable `shownDialog` is introduced and initialized to `false`. This variable is used to track whether the save dialog was actually shown to the user.
2.  **`shownDialog = true;`**: Inside the `if (showDialog || sFile.getName().equals(""-NONE-""))` block, `shownDialog` is set to `true` if the save dialog is displayed.
3.  **`if (shownDialog) { sFile = m_FileChooser.getSelectedFile(); }`**: The code that retrieves the selected file from the `JFileChooser` is now conditional. It only executes if `shownDialog` is `true`.

Reasoning:

The original code unconditionally retrieved the selected file from the `JFileChooser` using `sFile = m_FileChooser.getSelectedFile();`. However, the save dialog is only shown under certain conditions (either `showDialog` is true or the current file name is ""-NONE-""). If the dialog isn't shown, the `JFileChooser` might still contain the file selected from a previous save operation on *another* tab. This would cause the current tab to be saved with the filename from the other tab, which is the bug described in the commit message.

The fix ensures that the filename is only retrieved from the `JFileChooser` if the dialog was actually shown during the current save operation. This prevents the incorrect filename from being used.

Conclusion: **Buggy**
"
weka,37320.json,24265153c621e05ff04b8fd1e06a4dfd6a63950a,"@@ -1,12 +1,13 @@
   public void addStepOutputListener(StepOutputListener listener,
     String outputConnectionName) {
     List<StepOutputListener> listenersForConnectionType =
       m_outputListeners.get(outputConnectionName);
     if (listenersForConnectionType == null) {
       listenersForConnectionType = new ArrayList<StepOutputListener>();
+      m_outputListeners.put(outputConnectionName, listenersForConnectionType);
     }
 
     if (!listenersForConnectionType.contains(listener)) {
       listenersForConnectionType.add(listener);
     }
   }",Buggy,"Fixed a bug in the step output listener framework

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12498 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the step output listener framework. The code diff shows that a missing `m_outputListeners.put(outputConnectionName, listenersForConnectionType);` line was added within the `addStepOutputListener` method. This line is crucial for storing the list of listeners for a given output connection name. Without it, the listeners would not be properly associated with the connection, leading to a bug where the listeners might not receive the output.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**
"
weka,21662.json,ac1cf6e094d4d41b6cff6e286f9abb0cdd58e95b,"@@ -1,9 +1,9 @@
   public void addObject(String name, Object o) {
     String nameCopy = name;
     int i = 0;
-    while (m_Results.containsKey(nameCopy)) {
+    while (m_Objs.containsKey(nameCopy)) {
       nameCopy = name + ""_"" + i++;
     }
 
     m_Objs.put(nameCopy, o);
   }",Buggy,"Fixed bug, introduced in the last change, that affected accessing of objects stored against results

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13734 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to accessing objects stored against results, which was introduced in the last change. The code diff shows a change in the `addObject` method. Specifically, the `while` loop condition was changed from `m_Results.containsKey(nameCopy)` to `m_Objs.containsKey(nameCopy)`.

The original code was checking if the `nameCopy` already exists in `m_Results` to avoid naming conflicts when adding a new object. However, the object is actually being stored in `m_Objs`. This discrepancy would lead to an infinite loop if `nameCopy` exists in `m_Results` but not in `m_Objs`, because the loop would continue to increment `i` and create new `nameCopy` values, but it would never find a `nameCopy` that exists in `m_Results`.

The corrected code checks if `nameCopy` exists in `m_Objs`, which is where the object is actually stored. This fixes the potential infinite loop and ensures that the `addObject` method correctly handles naming conflicts.

The change directly addresses the bug described in the commit message.

**Buggy**"
weka,32460.json,c3cf13ff2162159996a765cb4fcfb11c86359d6e,"@@ -1,23 +1,23 @@
   private static void initStemmers() {
     Vector<String> classnames;
     int i;
 
     if (m_Stemmers != null) {
       return;
     }
 
     m_Stemmers = new Vector<String>();
 
     if (!m_Present) {
       return;
     }
 
     classnames = GenericObjectEditor.getClassnames(SNOWBALL_PROGRAM);
     // try dynamic discovery if not in props file
     if (classnames.size() == 0) {
       classnames = ClassDiscovery.find(SNOWBALL_PROGRAM, PACKAGE_EXT);
-      for (i = 0; i < classnames.size(); i++) {
-        m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
-      }
+    }
+    for (i = 0; i < classnames.size(); i++) {
+      m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
     }
   }",Buggy,"Fixed a bug that prevented snowball classes from being found when static entries in GenericObjectEditor.props are being used (instead of dynamic class discovery)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where snowball classes weren't found when using static entries in `GenericObjectEditor.props`. The code diff modifies the `initStemmers` method.

Here's a breakdown of the changes and how they relate to the bug fix:

1.  **Original Code:** The original code first checks if `classnames` obtained from `GenericObjectEditor.getClassnames(SNOWBALL_PROGRAM)` is empty. If it is, it attempts dynamic discovery using `ClassDiscovery.find(SNOWBALL_PROGRAM, PACKAGE_EXT)`. The stemmers are added to `m_Stemmers` only after dynamic discovery.

2.  **Modified Code:** The modified code removes the conditional block that only adds stemmers after dynamic discovery. Now, it adds stemmers to `m_Stemmers` regardless of whether the classnames were obtained from `GenericObjectEditor.props` or `ClassDiscovery.find`.

Reasoning:

The original code had a flaw: If `GenericObjectEditor.getClassnames` returned an empty list (meaning the static entries in `GenericObjectEditor.props` were not found or not configured), the code would proceed to dynamic discovery. However, even if dynamic discovery *did* find classes, the code that added the stemmer names to `m_Stemmers` was *inside* the conditional block that was only executed if `classnames.size() == 0` *after* the call to `GenericObjectEditor.getClassnames`. This meant that if `GenericObjectEditor.getClassnames` returned an empty list, and `ClassDiscovery.find` returned a non-empty list, the stemmers would *not* be added to `m_Stemmers`.

The fix removes this conditional block, ensuring that stemmers are always added to `m_Stemmers` regardless of whether they were found statically or dynamically. This aligns perfectly with the commit message, which states that the bug prevented snowball classes from being found when static entries in `GenericObjectEditor.props` are used. The bug was not in the finding, but in the subsequent processing of the found classes.

Conclusion:

**Buggy**
"
weka,24188.json,639ea63bec73dc387b27664947c01effdfc97c4c,"@@ -1,153 +1,153 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             k;
     int             size;
     String          line;
     int             indexBase;
     int             indexSecond;
     StringBuffer    head;
     StringBuffer    body;
     StringBuffer    foot;
     int[]           startMeans;
     int[]           startSigs;
     int             maxLength;
 
     result     = new StringBuffer();
     head       = new StringBuffer();
     body       = new StringBuffer();
     foot       = new StringBuffer();
     cells      = toArray();
     startMeans = new int[getColCount()];
     startSigs  = new int[getColCount() - 1];
     maxLength  = 0;
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n, true, true);
       for (i = 1; i < cells.length - 1; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // index of base column in array
     indexBase = 1;
     if (getShowStdDev())
       indexBase++;
 
     // index of second column in array
     indexSecond = indexBase + 1;
     if (getShowStdDev())
       indexSecond++;
 
     // output data (without ""(v/ /*)"")
     j = 0;
     k = 0;
     for (i = 1; i < cells.length - 1; i++) {
       line = """";
       
       for (n = 0; n < cells[0].length; n++) {
         // record starts
         if (i == 1) {
           if (isMean(n)) {
             startMeans[j] = line.length();
             j++;
           }
 
           if (isSignificance(n)) {
             startSigs[k] = line.length();
             k++;
           }
         }
         
         if (n == 0) {
           line += padString(cells[i][n], getRowNameWidth());
-          line += padString(""("" + Utils.doubleToString(getCount(i-1), 0) + "")"", 
+          line += padString(""("" + Utils.doubleToString(getCount(getDisplayRow(i-1)), 0) + "")"", 
                         getCountWidth(), true);
         }
         else {
           // additional space before means
           if (isMean(n))
             line += ""  "";
 
           // print cell
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))              
                 line += ""("" + cells[i][n] + "")"";
               else
                 line += "" "" + cells[i][n] + "" "";
             }
             else
               line += "" "" + cells[i][n];
           }
           else {
             line += "" "" + cells[i][n];
           }
         }
 
         // add separator after base column
         if (n == indexBase)
           line += "" |"";
       }
 
       // record overall length
       if (i == 1)
         maxLength = line.length();
       
       body.append(line + ""\n"");
     }
 
     // column names
     line = padString(cells[0][0], startMeans[0]);
     i    = -1;
     for (n = 1; n < cells[0].length; n++) {
       if (isMean(n)) {
         i++;
 
         if (i == 0)
           line = padString(line, startMeans[i] - getCountWidth());
         else if (i == 1)
           line = padString(line, startMeans[i] - "" |"".length());
         else if (i > 1)
           line = padString(line, startMeans[i]);
         
         if (i == 1)
           line += "" |"";
         
         line += "" "" + cells[0][n];
       }
     }
     line = padString(line, maxLength);
     head.append(line + ""\n"");
     head.append(line.replaceAll(""."", ""-"") + ""\n"");
     body.append(line.replaceAll(""."", ""-"") + ""\n"");
 
     // output wins/losses/ties
     if (getColCount() > 1) {
       line = padString(cells[cells.length - 1][0], startMeans[1]-2, true) + "" |"";
       i    = 0;
       for (n = 1; n < cells[cells.length - 1].length; n++) {
         if (isSignificance(n)) {
           line = padString(
                   line, startSigs[i] + 1 - cells[cells.length - 1][n].length());
           line += "" "" + cells[cells.length - 1][n];
           i++;
         }
       }
       line = padString(line, maxLength);
     }
     else {
       line = padString(cells[cells.length - 1][0], line.length() - 2) + "" |"";
     }
     foot.append(line + ""\n"");
     
     // assemble output
     result.append(head.toString());
     result.append(body.toString());
     result.append(foot.toString());
 
     return result.toString();
   }",Buggy,"Fixed bug in ResultMatrixPlainText, which meant that displayed counts could be out of line with corresponding dataset and results. Changed TTesters to not throw an exception when missing values are encountered.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2816 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in `ResultMatrixPlainText` related to the alignment of displayed counts with the dataset and results. The diff shows a change in the `toStringMatrix()` method, specifically in the line that constructs the row name string. The original code used `getCount(i-1)`, while the corrected code uses `getCount(getDisplayRow(i-1))`. This suggests that the original code was using an incorrect index to retrieve the count, leading to the misalignment issue described in the commit message. The change ensures that the correct row index is used to fetch the count, thus fixing the bug. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,17500.json,d109992d4145875b41832b4a599e8cc7b7e5d4a2,"@@ -1,71 +1,74 @@
   protected void setNumeric() {
     m_isNumeric = true;
     /*      m_maxC = mxC;
 	    m_minC = mnC; */
 
     double min=Double.POSITIVE_INFINITY;
     double max=Double.NEGATIVE_INFINITY;
     double value;
 
     for (int i=0;i<m_Instances.numInstances();i++) {
       if (!m_Instances.instance(i).isMissing(m_cIndex)) {
 	value = m_Instances.instance(i).value(m_cIndex);
 	if (value < min) {
 	  min = value;
 	}
 	if (value > max) {
 	  max = value;
 	}
       }
     }
      
+    // handle case where all values are missing
+    if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
     m_minC = min; m_maxC = max;
 
     int whole = (int)Math.abs(m_maxC);
     double decimal = Math.abs(m_maxC) - whole;
     int nondecimal;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
     m_precisionC = (decimal > 0) 
       ? (int)Math.abs(((Math.log(Math.abs(m_maxC)) / 
 				      Math.log(10))))+2
       : 1;
     if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
       m_precisionC = 1;
     }
 
     String maxStringC = Utils.doubleToString(m_maxC,
 					     nondecimal+1+m_precisionC
 					     ,m_precisionC);
     if (m_labelMetrics != null) {
       m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
     }
 
     whole = (int)Math.abs(m_minC);
     decimal = Math.abs(m_minC) - whole;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
      m_precisionC = (decimal > 0) 
        ? (int)Math.abs(((Math.log(Math.abs(m_minC)) / 
 				      Math.log(10))))+2
       : 1;
      if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
        m_precisionC = 1;
      }
     
      maxStringC = Utils.doubleToString(m_minC,
 				       nondecimal+1+m_precisionC
 				       ,m_precisionC);
      if (m_labelMetrics != null) {
        if (m_labelMetrics.stringWidth(maxStringC) > m_HorizontalPad) {
 	 m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
        }
      }
 
     setOn(true);
     this.repaint();
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1219 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug caused when all numeric values are missing"" suggests a bug fix related to handling missing numeric values.

The code diff introduces a check `if (min == Double.POSITIVE_INFINITY) min = max = 0.0;` after iterating through the instances to find the minimum and maximum values.  If `min` remains `Double.POSITIVE_INFINITY` after the loop, it means that no non-missing numeric values were found in the attribute. In this case, both `min` and `max` are set to 0.0. This addresses the scenario where all numeric values are missing, preventing potential issues later in the code when `min` or `max` are used.

The added code directly addresses the problem described in the commit message.

**Buggy**"
weka,31181.json,feacc664ba9b84371e98b74802bc9f6a36edcff4,"@@ -1,19 +1,19 @@
   public String[] getOptions() {
 
     Vector<String> result = new Vector<String>();
 
     result.add(""-R"");
     result.add(getAttributeIndices());
 
     if (getInvertSelection()) {
       result.add(""-V"");
     }
 
-    result.add(""F"");
+    result.add(""-F"");
     result.add("""" + getFilterSpec());
 
-    result.add(""D"");
+    result.add(""-D"");
     result.add("""" + getDistanceSpec());
 
     return result.toArray(new String[result.size()]);
   }",Buggy,"Small bug fix in getOptions().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11186 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Small bug fix in getOptions()"" clearly indicates that the changes are intended to fix a bug within the `getOptions()` method. The message is concise and directly points to the area of concern.

**Code Diff Analysis:**

The diff shows modifications within the `getOptions()` method. Specifically, the changes involve adding hyphens (""-"") to the beginning of the string literals ""F"" and ""D"" when adding them to the `result` vector. These string literals appear to be option flags or identifiers.

**Reasoning:**

The addition of hyphens to option flags is a common convention in command-line interfaces and configuration settings. The absence of these hyphens could have led to incorrect parsing or interpretation of the options, resulting in unexpected behavior or errors. The commit message explicitly states a bug fix, and the code changes directly address a potential issue with how options are formatted within the `getOptions()` method. This strongly suggests that the original code had a bug where the options ""F"" and ""D"" were not being correctly formatted, and the addition of the hyphens resolves this.

**Conclusion:**

**Buggy**
"
weka,36114.json,f20281f569296add23cdf098a2ef5d7c1c9101a3,"@@ -1,31 +1,31 @@
   public static boolean hasInterface(Class intf, Class cls) {
     Class[]       intfs;
     int           i;
     boolean       result;
     Class         currentclass;
     
     result       = false;
     currentclass = cls;
     do {
       // check all the interfaces, this class implements
       intfs = currentclass.getInterfaces();
       for (i = 0; i < intfs.length; i++) {
         if (intfs[i].equals(intf)) {
           result = true;
           break;
         }
       }
 
       // get parent class
       if (!result) {
         currentclass = currentclass.getSuperclass();
         
-        // topmost class reached?
-        if (currentclass.equals(Object.class))
+        // topmost class reached or no superclass?
+        if ( (currentclass == null) || (currentclass.equals(Object.class)) )
           break;
       }
     } 
     while (!result);
       
     return result;
   }",Buggy,"fixed bug with interfaces (return null as superclass)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2976 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to interfaces, specifically addressing a scenario where the superclass is incorrectly handled. The code changes modify the loop condition that checks for the superclass. The original code only checked if the current class was `Object.class`, which represents the topmost class. The modified code now also checks if `currentclass` is `null`, indicating that there is no superclass. This change directly addresses the bug described in the commit message, where interfaces might not have been correctly identified due to an incorrect handling of the superclass.

**Buggy**
"
weka,36431.json,a91eb3b6273ee2b25218e3fd6a1520a0f11b6e6e,"@@ -1,17 +1,18 @@
   public void stepInit() throws WekaException {
     m_isReset = true;
+    m_streamingData = null;
 
     // see if the specified downstream steps are connected
     m_validTrueStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfTrueStep)) != null;
     m_validFalseStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfFalseStep)) != null;
 
     m_incomingStructure = null;
 
     if (m_expressionString == null || m_expressionString.length() == 0) {
       throw new WekaException(""No expression defined!"");
     }
   }",Buggy,"Fixed a small bug in FlowByExpression - object holding streaming data output did not get set to null in reset() method

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13301 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a small bug in FlowByExpression - object holding streaming data output did not get set to null in reset() method"" clearly states that a bug was fixed. The bug was related to the `FlowByExpression` class, and specifically, the `reset()` method was not properly nullifying the object responsible for holding streaming data output.

**Git Diff Analysis:**

The Git diff shows a modification within the `stepInit()` method. The line `m_streamingData = null;` has been added at the beginning of the method.

**Reasoning:**

1.  **Method Name:** The commit message mentions `reset()`, but the diff modifies `stepInit()`. It's possible that `stepInit()` is called during the reset process or serves a similar purpose. We need to consider the context of how `stepInit()` is used.

2.  **Variable `m_streamingData`:** The commit message refers to an object holding streaming data output. The diff initializes `m_streamingData` to null. This aligns with the description in the commit message.

3.  **Bug Fix Indication:** The commit message explicitly states that a bug was fixed. The code change directly addresses the issue described in the commit message by ensuring that `m_streamingData` is set to null. This suggests that the previous value of `m_streamingData` was not being cleared properly during reset/initialization, potentially leading to incorrect behavior or memory leaks.

**Conclusion:**

The commit message and the code change are consistent with each other and indicate a bug fix. The `m_streamingData` variable was not being properly reset, which could lead to issues. The change addresses this by explicitly setting it to null.

**Buggy**"
weka,29966.json,a5e2d1ef0f62ebac46d095c840ea7ff4a9145b48,"@@ -1,172 +1,192 @@
   public static void main(String[] args) {
     try {
       if (args.length == 0 || args[0].equalsIgnoreCase(""-h"") ||
           args[0].equalsIgnoreCase(""-help"")) {
-        System.err.println(""Usage:\n\tweka.Run [-no-scan | -no-load] <scheme name [scheme options]>"");
+        System.err.println(""Usage:\n\tweka.Run [-no-scan] [-no-load] <scheme name [scheme options]>"");
         System.exit(1);
       }
+      boolean noScan = false;
+      boolean noLoad = false;
       if (args[0].equals(""-list-packages"")) {
         weka.core.WekaPackageManager.loadPackages(true);
         System.exit(0);
-      } else if (!args[0].equals(""-no-load"")) {
+      } else if (args[0].equals(""-no-load"")) {
+        noLoad = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-scan"")) {
+            noScan = true;
+          }
+        }
+      } else if (args[0].equals(""-no-scan"")) {
+        noScan = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-load"")) {
+            noLoad = true;
+          }
+        }
+      }
+      
+      if (!noLoad) {
         weka.core.WekaPackageManager.loadPackages(false);
       }
       
+      int schemeIndex = 0;
+      if (noLoad && noScan) {
+        schemeIndex = 2;
+      } else if (noLoad || noScan) {
+        schemeIndex = 1;
+      }
+      
       String schemeToRun = null;
       String[] options = null;
-      if (args[0].equals(""-no-scan"") || args[0].equals(""-no-load"")) {
-        if (args.length < 2) {
-          System.err.println(""No scheme name given."");
-          System.exit(1);
-        }
-        schemeToRun = args[1];
-        options = new String[args.length - 2];
-        if (options.length > 0) {
-          System.arraycopy(args, 2, options, 0, options.length);
-        }
-      } else {
-        // scan packages for matches
-        schemeToRun = args[0];
-        options = new String[args.length - 1];
-        if (options.length > 0) {
-          System.arraycopy(args, 1, options, 0, options.length);
-        }
-
-        ArrayList<String> matches = weka.core.ClassDiscovery.find(args[0]);
+      
+      if (schemeIndex >= args.length) {
+        System.err.println(""No scheme name given."");
+        System.exit(1);
+      }
+      schemeToRun = args[schemeIndex];
+      options = new String[args.length - schemeIndex - 1];
+      if (options.length > 0) {
+        System.arraycopy(args, schemeIndex + 1, options, 0, options.length);
+      }
+      
+           
+      if (!noScan) {     
+        ArrayList<String> matches = weka.core.ClassDiscovery.find(schemeToRun);
         ArrayList<String> prunedMatches = new ArrayList<String>();
         // prune list for anything that isn't a runnable scheme      
         for (int i = 0; i < matches.size(); i++) {
           try {
             Object scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
                 matches.get(i));          
             if (scheme instanceof weka.classifiers.Classifier ||
                 scheme instanceof weka.clusterers.Clusterer ||
                 scheme instanceof weka.associations.Associator ||
                 scheme instanceof weka.attributeSelection.ASEvaluation ||
                 scheme instanceof weka.filters.Filter) {
               prunedMatches.add(matches.get(i));
             }
           } catch (Exception ex) {
             // ignore any classes that we can't instantiate due to no no-arg constructor
           }
         }
 
         if (prunedMatches.size() == 0) {
-          System.err.println(""Can't find scheme "" + args[0] + "", or it is not runnable."");
+          System.err.println(""Can't find scheme "" + schemeToRun + "", or it is not runnable."");
           System.exit(1);
         } else if (prunedMatches.size() > 1) {
           java.io.BufferedReader br = 
             new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
           boolean done = false;
           while (!done) {
             System.out.println(""Select a scheme to run, or <return> to exit:"");
             for (int i = 0; i < prunedMatches.size(); i++) {
               System.out.println(""\t"" + (i+1) + "") "" + prunedMatches.get(i));
             }
             System.out.print(""\nEnter a number > "");
             String choice = null;
             int schemeNumber = 0;
             try {
               choice = br.readLine();
               if (choice.equals("""")) {
                 System.exit(0);
               } else {
                 schemeNumber = Integer.parseInt(choice);
                 schemeNumber--;
                 if (schemeNumber >= 0 && schemeNumber < prunedMatches.size()) {
                   schemeToRun = prunedMatches.get(schemeNumber);
                   done = true;
                 }
               }
             } catch (java.io.IOException ex) {
               // ignore
             }
           }
         } else {
           schemeToRun = prunedMatches.get(0);
         }
       }
 
       Object scheme = null;
       try {
         scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
             schemeToRun);
       } catch (Exception ex) {
         System.err.println(schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       // now see which interfaces/classes this scheme implements/extends
       ArrayList<SchemeType> types = new ArrayList<SchemeType>();      
       if (scheme instanceof weka.classifiers.Classifier) {
         types.add(SchemeType.CLASSIFIER);
       }
       if (scheme instanceof weka.clusterers.Clusterer) {
         types.add(SchemeType.CLUSTERER);
       }
       if (scheme instanceof weka.associations.Associator) {
         types.add(SchemeType.ASSOCIATOR);
       }
       if (scheme instanceof weka.attributeSelection.ASEvaluation) {
         types.add(SchemeType.ATTRIBUTE_SELECTION);
       }
       if (scheme instanceof weka.filters.Filter) {
         types.add(SchemeType.FILTER);
       }
       
       SchemeType selectedType = null;
       if (types.size() == 0) {
         System.err.println("""" + schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       if (types.size() == 1) {
         selectedType = types.get(0);
       } else {
         java.io.BufferedReader br = 
           new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
         boolean done = false;
         while (!done) {
           System.out.println("""" + schemeToRun + "" can be executed as any of the following:"");
           for (int i = 0; i < types.size(); i++) {
             System.out.println(""\t"" + (i+1) + "") "" + types.get(i));
           }
           System.out.print(""\nEnter a number > "");
           String choice = null;
           int typeNumber = 0;
           try {
             choice = br.readLine();
             if (choice.equals("""")) {
               System.exit(0);
             } else {
               typeNumber = Integer.parseInt(choice);
               typeNumber--;
               if (typeNumber >= 0 && typeNumber < types.size()) {
                 selectedType = types.get(typeNumber);
                 done = true;
               }
             }
           } catch (java.io.IOException ex) {
             // ignore
           }
         }
       }
             
       if (selectedType == SchemeType.CLASSIFIER) {
         weka.classifiers.AbstractClassifier.runClassifier((weka.classifiers.Classifier)scheme, options);
       } else if (selectedType == SchemeType.CLUSTERER) {
         weka.clusterers.AbstractClusterer.runClusterer((weka.clusterers.Clusterer)scheme, options);
       } else if (selectedType == SchemeType.ATTRIBUTE_SELECTION) {
         weka.attributeSelection.ASEvaluation.runEvaluator((weka.attributeSelection.ASEvaluation)scheme, options);
       } else if (selectedType == SchemeType.ASSOCIATOR) {
         weka.associations.AbstractAssociator.runAssociator((weka.associations.Associator)scheme, options);
       } else if (selectedType == SchemeType.FILTER) {
         weka.filters.Filter.runFilter((weka.filters.Filter)scheme, options);
       }
     } 
     catch (Exception e) {
       if (    ((e.getMessage() != null) && (e.getMessage().indexOf(""General options"") == -1))
 	   || (e.getMessage() == null) )
 	e.printStackTrace();
       else
 	System.err.println(e.getMessage());
     }
   }",Buggy,"Fixed a bug that prevented the -no-scan and -no-load options to be used at the same time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6676 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where the `-no-scan` and `-no-load` options could not be used together.

The diff introduces the following changes:

1.  **Updated Usage Message:** The usage message is updated to reflect that both `-no-scan` and `-no-load` can be specified.
2.  **Added Boolean Flags:** Two boolean flags, `noScan` and `noLoad`, are introduced to track whether these options are enabled.
3.  **Option Parsing:** The code now correctly parses the command-line arguments to set the `noScan` and `noLoad` flags. It handles cases where either or both options are present.
4.  **Scheme Index Calculation:** The `schemeIndex` is calculated based on the presence of `noScan` and `noLoad` flags to determine the starting index of the scheme name in the `args` array.
5.  **Conditional Package Loading:** The `weka.core.WekaPackageManager.loadPackages(false)` is now conditionally called based on the `noLoad` flag.
6.  **Conditional Class Discovery:** The class discovery logic is now conditionally executed based on the `noScan` flag.

These changes indicate that the code was modified to correctly handle the `-no-scan` and `-no-load` options when used together, which aligns with the commit message's claim of fixing a bug related to these options.

**Buggy**"
weka,6813.json,017098ae02f2d6bdf3fb5d25b0d3ccedd8c46b7e,"@@ -1,28 +1,35 @@
   protected static Vector instanceToVector(Instance toProcess, int classIndex) {
     if (toProcess instanceof SparseInstance) {
       int classModifier = classIndex >= 0 ? 1 : 0;
+      if (classModifier > 0) {
+        double classValue = toProcess.classValue();
+        if (classValue == 0) {
+          classModifier = 0; // class is sparse
+        }
+      }
       SparseInstance toProcessSparse = ((SparseInstance) toProcess);
       int[] indices = new int[toProcessSparse.numValues() - classModifier];
       double[] values = new double[toProcessSparse.numValues() - classModifier];
       int index = 0;
       for (int i = 0; i < toProcessSparse.numValues(); i++) {
         if (toProcessSparse.index(i) != classIndex) {
           indices[index] = toProcessSparse.index(i);
           values[index++] = toProcessSparse.valueSparse(i);
         }
       }
-      return Vectors.sparse(toProcess.numValues(), indices, values);
+      return Vectors.sparse(
+        toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0), indices, values);
     } else {
       if (classIndex < 0) {
         return Vectors.dense(toProcess.toDoubleArray());
       }
       double[] independent = new double[toProcess.numAttributes() - 1];
       int index = 0;
       for (int i = 0; i < toProcess.numAttributes(); i++) {
         if (i != classIndex) {
           independent[index++] = toProcess.value(i);
         }
       }
       return Vectors.dense(independent);
     }
   }",Buggy,"Fixed a bug in sparse instance to sparse Vector conversion.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14868 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the conversion of a sparse instance to a sparse vector. The code diff shows modifications within the `instanceToVector` method, specifically when handling `SparseInstance`.

Here's a breakdown of the changes and their implications:

1.  **Class Modifier Logic**: The code now includes a check to determine if the class value is sparse. If the class value is 0, the `classModifier` is set to 0. This suggests that the previous implementation might have incorrectly handled sparse class values, potentially leading to incorrect vector representations.
2.  **Vector Size**: The size of the sparse vector is now calculated as `toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0)`. Previously, it was `toProcess.numValues()`. This change is significant because `numValues()` in a `SparseInstance` refers to the number of non-zero values, while `numAttributes()` refers to the total number of attributes. The new calculation ensures that the vector size corresponds to the number of attributes, excluding the class index if it exists. This correction likely addresses an issue where the sparse vector was created with an incorrect size, leading to errors or unexpected behavior.

The changes address a specific issue related to handling sparse class values and calculating the correct size of the sparse vector. These modifications directly relate to the bug fix mentioned in the commit message.

**Conclusion:** Buggy
"
weka,1604.json,a3134adec9386c387ea691ca59e46a35f35f858d,"@@ -1,175 +1,179 @@
   public void buildClassifier(Instances data) throws Exception {
     // can classifier handle the data?
     getCapabilities().testWithFail(data);
 
     m_errorsFromR = new ArrayList<String>();
 
     if (m_modelHash == null) {
       m_modelHash = """" + hashCode();
     }
 
     data = new Instances(data);
     data.deleteWithMissingClass();
 
     if (data.numInstances() == 0 || data.numAttributes() == 1) {
       if (data.numInstances() == 0) {
         System.err
           .println(""No instances with non-missing class - using ZeroR model"");
       } else {
         System.err.println(""Only the class attribute is present in ""
           + ""the data - using ZeroR model"");
       }
       m_zeroR = new ZeroR();
       m_zeroR.buildClassifier(data);
       return;
     }
 
     // remove useless attributes
     m_removeUseless = new RemoveUseless();
     m_removeUseless.setInputFormat(data);
     data = Filter.useFilter(data, m_removeUseless);
 
     if (!m_dontReplaceMissingValues) {
       m_missingFilter = new ReplaceMissingValues();
       m_missingFilter.setInputFormat(data);
       data = Filter.useFilter(data, m_missingFilter);
     }
 
     data = handleZeroFrequencyNominalValues(data);
 
     m_serializedModel = null;
 
     if (!m_initialized) {
       init();
 
       if (!m_mlrAvailable) {
         throw new Exception(
           ""MLR is not available for some reason - can't continue!"");
       }
     } else {
       // unload and then reload MLR to try and clear any errors/inconsistent
       // state
       reloadMLR();
     }
 
     m_baseLearnerLibraryAvailable = false;
     loadBaseLearnerLibrary();
     if (!m_baseLearnerLibraryAvailable) {
       throw new Exception(""Library ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getIDStr() + "" for learner ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable()
         + "" is not available for some reason - can't continue!"");
     }
 
     RSession eng = null;
     eng = RSession.acquireSession(this);
     eng.setLog(this, m_logger);
     eng.clearConsoleBuffer(this);
 
+    // Change to a temporary directory where we have write access
+    // in case an mlr scheme tries to write a local file
+    eng.parseAndEval(this, ""setwd(tempdir())"");
+
     // Set seed for random number generator in R in a data-dependent manner
     eng.parseAndEval(this,
       ""set.seed("" + data.getRandomNumberGenerator(getSeed()).nextInt() + "")"");
 
     // clean up any previous model
     // suffix model identifier with hashcode of this object
     eng.parseAndEval(this, ""remove(weka_r_model"" + m_modelHash + "")"");
 
     // transfer training data into a data frame in R
     Object[] result = RUtils.instancesToDataFrame(eng, this, data, ""mlr_data"");
     m_cleanedAttNames = (String[])result[0];
     m_cleanedAttValues = (String[][])result[1];
 
     try {
       String mlrIdentifier =
         MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable();
 
       if (data.classAttribute().isNumeric()
         && mlrIdentifier.startsWith(""classif"")) {
         throw new Exception(""Training instances has a numeric class but ""
           + ""selected R learner is a classifier!"");
       } else if (data.classAttribute().isNominal()
         && mlrIdentifier.startsWith(""regr"")) {
         throw new Exception(""Training instances has a nominal class but ""
           + ""selected R learner is a regressor!"");
       }
 
       m_errorsFromR.clear();
       // make classification/regression task
       String taskString = null;
       if (data.classAttribute().isNominal()) {
         taskString = ""task <- makeClassifTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       } else {
         taskString = ""task <- makeRegrTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       }
 
       if (m_Debug) {
         System.err.println(""Prediction task: "" + taskString);
       }
 
       eng.parseAndEval(this, taskString);
       // eng.parseAndEval(this, ""print(task)"");
       checkForErrors();
 
       m_schemeProducesProbs = schemeProducesProbabilities(mlrIdentifier, eng);
 
       String probs =
         (data.classAttribute().isNominal() && m_schemeProducesProbs)
           ? "", predict.type = \""prob\"""" : """";
       String learnString = null;
       if (m_schemeOptions != null && m_schemeOptions.length() > 0) {
         learnString = ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs
           + "", "" + m_schemeOptions + "")"";
       } else {
         learnString =
           ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs + "")"";
       }
 
       if (m_Debug) {
         System.err.println(""Make a learner object: "" + learnString);
       }
 
       eng.parseAndEval(this, learnString);
       checkForErrors();
 
       // eng.parseAndEval(this, ""print(l)"");
 
       // train model
       eng.parseAndEval(this,
         ""weka_r_model"" + m_modelHash + "" <- train(l, task)"");
 
       checkForErrors();
 
       // get the model for serialization
       REXP serializedRModel = eng.parseAndEval(this,
         ""serialize(weka_r_model"" + m_modelHash + "", NULL)"");
 
       checkForErrors();
 
       m_modelText = new StringBuffer();
 
       // get the textual representation
       eng.parseAndEval(this,
         ""print(getLearnerModel(weka_r_model"" + m_modelHash + ""))"");
       m_modelText.append(eng.getConsoleBuffer(this));
 
       // now try and serialize the model
       XStream xs = new XStream();
       String xml = xs.toXML(serializedRModel);
       if (xml != null && xml.length() > 0) {
         m_serializedModel = new StringBuffer();
         m_serializedModel.append(xml);
       }
     } catch (Exception ex) {
       ex.printStackTrace();
       // remove R training data frame after completion
       eng.parseAndEval(this, ""remove(mlr_data)"");
       RSession.releaseSession(this);
 
       throw new Exception(ex.getMessage());
     }
 
     eng.parseAndEval(this, ""remove(mlr_data)"");
     RSession.releaseSession(this);
   }",Buggy,"Fixed problem for learning algorithms such as xgboost that create files. Now MLRClassifier creates and uses a temporary directory.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a problem where learning algorithms (like xgboost) create files. The code change introduces a temporary directory for the MLRClassifier to use, which aligns with the commit message. The line `eng.parseAndEval(this, ""setwd(tempdir())"");` changes the working directory to a temporary directory. This suggests a bug fix related to file creation and permissions.

**Buggy**
"
weka,23659.json,04f417158535105b2ada4d2cb15709a24d2924e2,"@@ -1,125 +1,125 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             size;
 
     result  = new StringBuffer();
     cells   = toArray();
 
     result.append(  ""\\begin{table}[thb]\n\\caption{\\label{labelname}""
                   + ""Table Caption}\n"");
     if (!getShowStdDev())
       result.append(""\\footnotesize\n"");
     else
       result.append(""\\scriptsize\n"");
 
     // output the column alignment characters
     // one for the dataset name and one for the comparison column
     if (!getShowStdDev()) {
       result.append(  ""{\\centering \\begin{tabular}{""
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     );
     } else {
       // dataset, mean, std dev
       result.append(  ""{\\centering \\begin{tabular}{"" 
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""c""                     // +/-
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""r""                     // stddev
                     );
     }
 
     for (j = 1; j < getColCount(); j++) {
       if (getColHidden(j))
         continue;
       if (!getShowStdDev())
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
       else 
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""c""                   // +/-
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""r""                   // stddev
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
     }
     result.append(""}\n\\\\\n\\hline\n"");
     if (!getShowStdDev())
-      result.append(""Dataset & "" + getColName(0));
+      result.append(""Dataset & "" + cells[0][1]);
     else
-      result.append(""Dataset & \\multicolumn{3}{c}{"" + getColName(0) + ""}"");
+      result.append(""Dataset & \\multicolumn{3}{c}{"" + cells[0][1] + ""}"");
 
     // now do the column names (numbers)
-    for (j = 1; j < getColCount(); j++) {
-      if (getColHidden(j))
+    for (j = 2; j < cells[0].length; j++) {
+      if (!isMean(j))
         continue;
       if (!getShowStdDev())
-        result.append(""& "" + getColName(j) + "" & "");
+        result.append(""& "" + cells[0][j] + "" & "");
       else
-        result.append(""& \\multicolumn{4}{c}{"" + getColName(j) + ""} "");
+        result.append(""& \\multicolumn{4}{c}{"" + cells[0][j] + ""} "");
     }
     result.append(""\\\\\n\\hline\n"");
 
     // change ""_"" to ""-"" in names
     for (i = 1; i < cells.length; i++)
       cells[i][0] = cells[i][0].replace('_', '-');
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n);
       for (i = 1; i < cells.length; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // output data (w/o wins/ties/losses)
     for (i = 1; i < cells.length - 1; i++) {
       for (n = 0; n < cells[0].length; n++) {
         if (n == 0) {
           result.append(padString(cells[i][n], getRowNameWidth()));
         }
         else {
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))
                 result.append("" & $\\pm$ & "");
               else
                 result.append("" &       & "");
             }
             else
               result.append("" & "");
           }
           else {
             result.append("" & "");
           }
           result.append(cells[i][n]);
         }
       }
       
       result.append(""\\\\\n"");
     }
 
     result.append(""\\hline\n\\multicolumn{"" + cells[0].length + ""}{c}{$\\circ$, $\\bullet$""
 		  +"" statistically significant improvement or degradation}""
 		  +""\\\\\n\\end{tabular} "");
     if (!getShowStdDev())     
       result.append(""\\footnotesize "");
     else
       result.append(""\\scriptsize "");
     
     result.append(""\\par}\n\\end{table}""
 		  +""\n"");
      
     return result.toString();
   }",Buggy,"fixed bug with column names (weren't in the correct order)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2393 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to the order of column names in the output. The diff shows changes in how column names are retrieved and used in the `toStringMatrix()` method. Specifically, the code now uses `cells[0][j]` instead of `getColName(j)` to access column names. Also, the loop condition has been changed from `j < getColCount()` to `j < cells[0].length`. These changes suggest that the previous implementation was not correctly retrieving or ordering the column names, which is a bug.

**Buggy**"
weka,36869.json,1a86d86af7ad235af95da7469c4252a5adf77d13,"@@ -1,27 +1,29 @@
   protected void checkPendingStreaming() throws WekaException {
     try {
       m_streamingFilter.batchFinished();
       Instances structureCopy =
         m_streamingFilter.getOutputFormat().stringFreeStructure();
       while (m_streamingFilter.numPendingOutput() > 0) {
         getStepManager().throughputUpdateStart();
         Instance filteredI = m_streamingFilter.output();
         if (m_stringAttsPresent) {
           for (int i = 0; i < filteredI.numAttributes(); i++) {
-            String val = filteredI.stringValue(i);
-            structureCopy.attribute(i).setStringValue(val);
-            filteredI.setValue(i, 0);
+            if (filteredI.attribute(i).isString() && ! filteredI.isMissing(i)) {
+              String val = filteredI.stringValue(i);
+              structureCopy.attribute(i).setStringValue(val);
+              filteredI.setValue(i, 0);
+            }
           }
           filteredI.setDataset(structureCopy);
         }
         m_incrementalData
           .setPayloadElement(StepManager.CON_INSTANCE, filteredI);
         if (!isStopRequested()) {
           getStepManager().outputData(m_incrementalData);
         }
         getStepManager().throughputUpdateEnd();
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",Buggy,"Fixed a small bug in processing of string attributes

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13321 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug related to string attribute processing. The code diff modifies the loop that iterates through the attributes of a filtered instance. Specifically, it adds a condition `filteredI.attribute(i).isString() && ! filteredI.isMissing(i)` before processing the string value. This condition ensures that the code only attempts to process string values for attributes that are actually strings and are not missing. This suggests that the previous code might have been trying to process non-string attributes as strings or process missing string attributes, leading to an error. The added check prevents this, aligning with the commit message's claim of fixing a bug in string attribute processing.

**Buggy**"
weka,29259.json,a01e554f4dee170f439264d96f61f10c13bb316c,"@@ -1,58 +1,58 @@
   public double[] pairwiseCoupling(double[][] n, double[][] r) {
 
     // Initialize p and u array
     double[] p = new double[r.length];
     for (int i =0; i < p.length; i++) {
       p[i] = 1.0 / (double)p.length;
     }
     double[][] u = new double[r.length][r.length];
     for (int i = 0; i < r.length; i++) {
       for (int j = i + 1; j < r.length; j++) {
 	u[i][j] = 0.5;
       }
     }
 
     // firstSum doesn't change
     double[] firstSum = new double[p.length];
     for (int i = 0; i < p.length; i++) {
       for (int j = i + 1; j < p.length; j++) {
 	firstSum[i] += n[i][j] * r[i][j];
 	firstSum[j] += n[i][j] * (1 - r[i][j]);
       }
     }
 
     // Iterate until convergence
     boolean changed;
     do {
       changed = false;
       double[] secondSum = new double[p.length];
       for (int i = 0; i < p.length; i++) {
 	for (int j = i + 1; j < p.length; j++) {
 	  secondSum[i] += n[i][j] * u[i][j];
 	  secondSum[j] += n[i][j] * (1 - u[i][j]);
 	}
       }
       for (int i = 0; i < p.length; i++) {
-	if (firstSum[i] == 0) {
+	if ((firstSum[i] == 0) || (secondSum[i] == 0)) {
 	  if (p[i] > 0) {
 	    changed = true;
 	  }
 	  p[i] = 0;
 	} else {
 	  double factor = firstSum[i] / secondSum[i];
 	  double pOld = p[i];
 	  p[i] *= factor;
 	  if (Math.abs(pOld - p[i]) > 1.0e-3) {
 	    changed = true;
 	  }
 	}
       }
       Utils.normalize(p);
       for (int i = 0; i < r.length; i++) {
 	for (int j = i + 1; j < r.length; j++) {
 	  u[i][j] = p[i] / (p[i] + p[j]);
 	}
       }
     } while (changed);
     return p;
   }",Buggy,"Fixed division by zero problem in pairwise coupling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a division by zero error in the `pairwiseCoupling` method. The diff shows a modification to the condition under which `p[i]` is set to 0. The original code only checked if `firstSum[i]` was zero. The modified code now checks if either `firstSum[i]` or `secondSum[i]` is zero before performing the division `firstSum[i] / secondSum[i]`. This change directly addresses the potential division by zero error.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,5830.json,4e28ba24b34efd31ec6954aa8312a90cd500ce2a,"@@ -1,12 +1,12 @@
   public void stepInit() throws WekaException {
     if ((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) &&
-      m_fileName == null || isEmpty(m_fileName.toString())) {
+      (m_fileName == null || isEmpty(m_fileName.toString()))) {
       throw new WekaException(""No forecaster specified!"");
     }
 
     m_isReset = true;
     m_isStreaming = false;
     m_overlayData = null;
     m_bufferedPrimeData = null;
     m_streamingData = new Data(StepManager.CON_INSTANCE);
   }",Buggy,"Fixed a bug that prevented initialization to complete successfully when the model being used is serialized into the metadata for the step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13349 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to initialization failure when the model is serialized into metadata. The code diff modifies a conditional statement in the `stepInit()` method. Specifically, it adds parentheses around `(m_fileName == null || isEmpty(m_fileName.toString()))`. This change likely fixes a logical error in the original condition. The original condition `(m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) && m_fileName == null || isEmpty(m_fileName.toString())` could have been misinterpreted due to operator precedence. The corrected version ensures that the check for `m_fileName` is only performed when `m_encodedForecaster` is null or ""-NONE-"". This aligns with the commit message's description of fixing an initialization issue.

**Buggy**"
weka,25919.json,92304dbdd06bfcf597e16639d7f8b1656d4269d9,"@@ -1,3 +1,4 @@
   public void setComputeAttributeImportance(boolean computeAttributeImportance) {
     m_computeAttributeImportance = computeAttributeImportance;
+    ((RandomTree)m_Classifier).setComputeImpurityDecreases(computeAttributeImportance);
   }",Buggy,"Fixed another small bug in option setting

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13190 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fixed another small bug in option setting"" clearly indicates a bug fix related to setting options. The use of ""another"" suggests this is not the first bug fix of this type.

**Code Diff Analysis:**

The code diff shows a change in the `setComputeAttributeImportance` method. Specifically, it adds a line that calls `((RandomTree)m_Classifier).setComputeImpurityDecreases(computeAttributeImportance);`. This line appears to be setting a property on the underlying classifier (`RandomTree`) based on the value of `computeAttributeImportance`.

**Reasoning:**

The commit message and the code diff are strongly related. The commit message mentions a bug fix in option setting, and the code modifies a setter method (`setComputeAttributeImportance`). The added line suggests that previously, setting `computeAttributeImportance` might not have correctly propagated the setting to the underlying `RandomTree` classifier, potentially leading to incorrect behavior. This is a classic scenario where an option setting was not fully applied, leading to a bug. The fix ensures that the `computeAttributeImportance` setting is correctly propagated to the `RandomTree` classifier by calling the `setComputeImpurityDecreases` method.

**Conclusion:**

The changes indicate a bug fix because the code modification addresses a potential issue where an option setting was not being correctly applied to the underlying classifier.

**Buggy**
"
weka,10363.json,e68621d5b0b175c4b155cb383a8c007a83d10875,"@@ -1,8 +1,8 @@
   public String globalInfo() {
-    return ""Iteratively fits a regression model by attempting to minimize absolute error, based on""
+    return ""Iteratively fits a regression model by attempting to minimize absolute error, using""
             + ""a base learner that minimizes weighted squared error.\n\n""
             + ""Weights are bounded from below by 1.0 / Utils.SMALL.\n\n""
             + ""Resamples data based on weights if base learner is not a WeightedInstancesHandler.\n\n""
       +""For more information see:\n\n""
       + getTechnicalInformation().toString();
   }",Buggy,"Updated Javadoc in IterativeAbsoluteErrorRegression. Fixed bug in globalInfo().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12364 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates two changes: Javadoc update and a bug fix in `globalInfo()`.

The provided diff shows a change in the `globalInfo()` method's return string. The phrase ""based on"" is replaced with ""using"". This change aligns with the commit message's claim of a bug fix and a Javadoc update, as the original wording might have been inaccurate or misleading, thus constituting a bug.

**Buggy**
"
weka,12909.json,1e5d54bb479e458b94b035c9d201dae6a4aec513,"@@ -1,45 +1,48 @@
   public void updateDefaultList() {
     
     ((ModelList.SortedListModel) m_ModelList.getModel()).clear();
     
     String ensemblePackageString = getPackageName();
     
     int index = m_DefaultFilesComboBox.getSelectedIndex();
     
     Vector classifiers = null;
     
     LibrarySerialization serialization;
     try {
       
       serialization = new LibrarySerialization();
       
       String defaultFileString = ensemblePackageString
       + m_DefaultFileNames[index].trim() + "".model.xml"";
       
       //System.out.println(defaultFileString);
-      
-      InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+    
+      ClassLoader thisLoader = getClass().getClassLoader();
+      // InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+      InputStream is = thisLoader.getResourceAsStream(defaultFileString);
       
       if (is == null) {
 	File f = new File(defaultFileString);
 	if (f.exists()) {
 	  System.out.println(""file existed: "" + f.getPath());
 	} else {
 	  System.out.println(""file didn't exist: "" + f.getPath());
 	}
 	
       }
       
-      classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      // classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      classifiers = (Vector) serialization.read(thisLoader.getResourceAsStream(defaultFileString));
       
       for (Iterator it = classifiers.iterator(); it.hasNext();) {
 	EnsembleLibraryModel model = m_ListModelsPanel.getLibrary().createModel((Classifier) it.next());
 	model.testOptions();
 	((ModelList.SortedListModel) m_ModelList.getModel()).add(model);
       }
       
     } catch (Exception e) {
       // TODO Auto-generated catch block
       e.printStackTrace();
     }
   }",Buggy,"Fixed a bug that was preventing a property file from being loaded correctly

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14340 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to loading a property file. The code diff shows changes in how the resource stream is obtained. Specifically, it switches from using `ClassLoader.getSystemResourceAsStream` to `getClass().getClassLoader().getResourceAsStream`. Additionally, there are debug print statements to check if the file exists. The change in resource loading and the debug statements strongly suggest an attempt to resolve an issue where the property file was not being found or loaded correctly. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,21385.json,1f61d35aae476dd90e8166897219ea0c8128e978,"@@ -1,173 +1,173 @@
   public void actionPerformed(ActionEvent e) {
     
     //JMenuItem m = (JMenuItem)e.getSource();
     
     if (e.getActionCommand() == null) {
       if (m_scaling == 0) {
 	repaint();
       }
       else {
 	animateScaling(m_nViewPos, m_nViewSize, m_scaling);
       }
     }
     else if (e.getActionCommand().equals(""Fit to Screen"")) {
       
       Dimension np = new Dimension();
       Dimension ns = new Dimension();
 
       getScreenFit(np, ns);
 
       animateScaling(np, ns, 10);
       
     }
     else if (e.getActionCommand().equals(""Center on Top Node"")) {
       
       int tpx = (int)(m_topNode.getCenter() * m_viewSize.width);   //calculate
       //the top nodes postion but don't adjust for where 
       int tpy = (int)(m_topNode.getTop() * m_viewSize.height);     //view is
       
       
       
       Dimension np = new Dimension(getSize().width / 2 - tpx, 
 				   getSize().width / 6 - tpy);
       
       animateScaling(np, m_viewSize, 10);
       
     }
     else if (e.getActionCommand().equals(""Auto Scale"")) {
       autoScale();  //this will figure the best scale value 
       //keep the focus on the middle of the screen and call animate
     }
     else if (e.getActionCommand().equals(""Visualize The Node"")) {
       //send the node data to the visualizer 
       if (m_focusNode >= 0) {
 	Instances inst;
 	if ((inst = m_nodes[m_focusNode].m_node.getInstances()) != null) {
 	  VisualizePanel pan = new VisualizePanel();
 	  pan.setInstances(inst);
 	  JFrame nf = new JFrame();
 	  nf.setSize(400, 300);
 	  nf.getContentPane().add(pan);
 	  nf.setVisible(true);
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
-					""availble Instances data for "" +
+					""available Instances data for "" +
 					""this Node."", ""Sorry!"",
 					JOptionPane.WARNING_MESSAGE); 
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" + 
 				      ""selected Node to perform "" +
 				      ""this operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE); 
       }
     }
     else if (e.getActionCommand().equals(""Create Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.ADD_CHILDREN, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decision Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Remove Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new 
 	    TreeDisplayEvent(TreeDisplayEvent.REMOVE_CHILDREN, 
 			     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""classify_child"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.CLASSIFY_CHILD, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""send_instances"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.SEND_INSTANCES, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Accept The Tree"")) {
       if (m_listener != null) {
 	//then send message to the listener saying that the tree is done
 	m_listener.userCommand(new TreeDisplayEvent(TreeDisplayEvent.ACCEPT,
 						  null));
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Sorry, there is no "" +
 				      ""available Decision Tree to "" +
 				      ""perform this operation on."",
 				      ""Sorry!"", 
 				      JOptionPane.WARNING_MESSAGE);
       }
     }
   }",Buggy,"fixed typo in error message


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3733 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states ""fixed typo in error message"". The diff shows a change from ""availble"" to ""available"" within a `JOptionPane.showMessageDialog` call. This aligns perfectly with the commit message, indicating a correction of a typographical error in an error message.
**Buggy**"
weka,26536.json,8a6133c51b18360a896a4e959c47ff6c31ed9425,"@@ -1,16 +1,12 @@
   protected void initializeWithPluginMetrics() {
     m_pluginMetrics.clear();
     List<AbstractEvaluationMetric> pluginMetrics = m_eval.getPluginMetrics();
     if (pluginMetrics != null && pluginMetrics.size() > 0) {
       for (AbstractEvaluationMetric m : pluginMetrics) {
-        if (m instanceof InformationRetrievalEvaluationMetric) {
-          List<String> statNames = m.getStatisticNames();
-          for (String s : statNames) {
-            m_pluginMetrics.put(s.toLowerCase(), m);
-          }
-        } else {
-          m_pluginMetrics.put(m.getMetricName().toLowerCase(), m);
+        List<String> statNames = m.getStatisticNames();
+        for (String s : statNames) {
+          m_pluginMetrics.put(s.toLowerCase(), m);
         }
       }
     }
   }",Buggy,"Fixed a small bug in the initializeWithPluginMetrics() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10890 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states ""Fixed a small bug in the initializeWithPluginMetrics() method."" This strongly suggests that the changes are intended to correct a previously existing issue.

2.  **Code Diff:** The code diff shows a modification within the `initializeWithPluginMetrics()` method. The original code had a conditional block:

    ```java
    if (m instanceof InformationRetrievalEvaluationMetric) {
      List<String> statNames = m.getStatisticNames();
      for (String s : statNames) {
        m_pluginMetrics.put(s.toLowerCase(), m);
      }
    } else {
      m_pluginMetrics.put(m.getMetricName().toLowerCase(), m);
    }
    ```

    This code was checking if the metric `m` was an instance of `InformationRetrievalEvaluationMetric`. If it was, it would iterate through the statistic names and put them into `m_pluginMetrics`. Otherwise, it would put the metric name into `m_pluginMetrics`.

    The modified code removes this conditional block, and simply iterates through the statistic names and puts them into `m_pluginMetrics` for *all* metrics.

    ```java
    List<String> statNames = m.getStatisticNames();
    for (String s : statNames) {
      m_pluginMetrics.put(s.toLowerCase(), m);
    }
    ```

3.  **Bug Fix Inference:** The removal of the conditional block suggests that the original code was not correctly handling all types of evaluation metrics. It seems that the intention is to always iterate through the statistic names, regardless of the metric type. The original code might have been skipping some metrics, or handling them incorrectly, leading to inaccurate or incomplete results. The change ensures that all statistic names are processed, which is likely the bug fix.

**Conclusion:**

The commit message and code diff strongly indicate a bug fix. The original code had a conditional block that was removed, suggesting that it was causing incorrect handling of certain evaluation metrics. The modified code ensures that all statistic names are processed, which is likely the intended behavior.

**Buggy**"
weka,21345.json,f05e160a2290644b91f34152ee153ba21fa688bd,"@@ -1,9 +1,21 @@
     public void setValueAt(Object aValue,
 			   int rowIndex,
 			   int columnIndex) {
 
       //      double value = ((Double) aValue).doubleValue();
       //      m_matrix.setElement(rowIndex, columnIndex, value);
-      m_matrix.setCell(rowIndex, columnIndex, aValue);
+      // try to parse it as a double first
+      Double val;
+      try {
+        val = new Double(((String)aValue));
+        double value = val.doubleValue();
+      } catch (Exception ex) {
+        val = null;
+      }
+      if (val == null) {
+        m_matrix.setCell(rowIndex, columnIndex, aValue);
+      } else {
+        m_matrix.setCell(rowIndex, columnIndex, val);
+      }
       fireTableCellUpdated(rowIndex, columnIndex);
     }",Buggy,"Fixed problem with setting cost values


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3268 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed problem with setting cost values"" strongly suggests that the commit addresses a bug related to how cost values are being set.

2.  **Code Diff:**
    *   The original code `m_matrix.setCell(rowIndex, columnIndex, aValue);` directly sets the cell value with the provided `aValue` object.
    *   The modified code introduces a `try-catch` block to parse `aValue` as a `Double`.
    *   If parsing is successful, the double value is used to set the cell.
    *   If parsing fails (an exception is caught), the original behavior of setting the cell with the original `aValue` is preserved.

3.  **Interpretation:**
    *   The original code likely had a bug where it wasn't handling cases where the `aValue` was a `String` representation of a number. This would cause issues when the underlying matrix expected a `Double` value.
    *   The `try-catch` block attempts to convert the `String` to a `Double` and use that value. This fixes the bug by ensuring that the matrix receives a `Double` value when appropriate.
    *   The `catch` block handles cases where the `String` cannot be parsed as a `Double`, preserving the original behavior. This is important for cases where the cell might legitimately contain a `String` value.

**Conclusion:**

The code changes introduce error handling and type conversion to address a problem with setting cost values, as indicated by the commit message. This strongly suggests that the commit fixes a bug.

**Buggy**
"
weka,32633.json,5bf47271c53b258212fcf284bc65101b75cd4309,"@@ -1,45 +1,45 @@
   protected static void loadPackageDirectory(File directory, boolean verbose)
       throws Exception {
     File[] contents = directory.listFiles();
 
     // make sure that jar files and lib directory get processed first
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith("".jar"")) {
         if (verbose) {
           System.out.println(""[Weka] loading "" + contents[i].getPath());
         }
         ClassloaderUtil.addFile(contents[i].getPath());
       } else if (contents[i].isDirectory()
           && contents[i].getName().equalsIgnoreCase(""lib"")) {
         // add any jar files in the lib directory to the classpath
         loadPackageDirectory(contents[i], verbose);
       }
     }
 
     // now any auxilliary files
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith(""Beans.props"")) {
         // KnowledgeFlow plugin -- add the Beans.props file to the list of
         // bean plugin props
 
         KnowledgeFlowApp.addToPluginBeanProps(contents[i]);
         KnowledgeFlowApp.disposeSingleton();
 
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""Explorer.props"")) {
         // Explorer tabs plugin
         // process the keys in the properties file and append/add values
         processExplorerProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GUIEditors.props"")) {
         // Editor for a particular component
         processGUIEditorsProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GenericPropertiesCreator.props"")) {
         processGenericPropertiesCreatorProps(contents[i]);
       } else if (contents[i].isFile()
-          && contents[i].getParent().endsWith(""PluginManager.props"")) {
+          && contents[i].getPath().endsWith(""PluginManager.props"")) {
         processPluginManagerProps(contents[i]);
       }
     }
   }",Buggy,"Fixed a bug in the processing of PluginManager.props files.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9314 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to processing ""PluginManager.props"" files. The diff modifies the condition that checks for ""PluginManager.props"" files.

The original condition `contents[i].getParent().endsWith(""PluginManager.props"")` checks if the *parent directory's name* ends with ""PluginManager.props"". This is incorrect because it should be checking if the *file's name* ends with ""PluginManager.props"".

The corrected condition `contents[i].getPath().endsWith(""PluginManager.props"")` checks if the *file's path* ends with ""PluginManager.props"", which is the intended behavior. This correction directly addresses the bug described in the commit message.

Therefore, the change indicates a bug fix.

**Buggy**"
weka,30120.json,4223a444b35556add50503527e26b24f82ea6784,"@@ -1,6 +1,6 @@
   public Instances trainCV(int numFolds, int numFold, Random random) {
 
-    Instances train = trainCV(numFold, numFold);
+    Instances train = trainCV(numFolds, numFold);
     train.randomize(random);
     return train;
   }",Buggy,"Fixed a bug that I just introduced with my new code.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1850 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""Fixed a bug that I just introduced with my new code"" strongly suggests that the commit is intended to correct a recently introduced error. This is a clear indication of a bug fix.

2.  **Diff Analysis:** The diff shows a change in the `trainCV` method. Specifically, the first argument to the `trainCV` method call within the `trainCV` method has been changed from `numFold` to `numFolds`.

3.  **Relevance Assessment:** The change from `numFold` to `numFolds` is likely a correction of an incorrect parameter being passed to the `trainCV` method. If `numFold` was passed instead of `numFolds`, the cross-validation would not be performed correctly, indicating a bug.

4.  **Conclusion:** Given the commit message explicitly stating a bug fix and the code change correcting a likely incorrect parameter, it is highly probable that this commit addresses a bug.

**Buggy**"
weka,20457.json,0fb7be37516d2f841ec1d8dc7949967049dd41f9,"@@ -1,5 +1,10 @@
   public void setText(String text) {
     m_currentContents = text;
-    m_combo.setSelectedItem(m_currentContents);
+    java.awt.Component theEditor = m_combo.getEditor().getEditorComponent();
+    if (theEditor instanceof JTextField) {
+      ((JTextField)theEditor).setText(text);
+    } else {
+      m_combo.setSelectedItem(m_currentContents);
+    }
     m_support.firePropertyChange("""", null, null);
   }",Buggy,"Fixed a bug in setting the value of the field.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@8116 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Java code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed a bug in setting the value of the field"" clearly suggests that the commit addresses a bug related to how a field's value is being set.

2.  **Code Diff:** The code diff modifies the `setText` method. The original code directly sets the selected item of a combo box (`m_combo`) using `m_combo.setSelectedItem(m_currentContents)`. The modified code introduces a check to determine if the editor component of the combo box is a `JTextField`. If it is, the text is set directly on the `JTextField` using `((JTextField)theEditor).setText(text)`. Otherwise, the original `setSelectedItem` method is used.

3.  **Alignment and Bug Fix Indication:** The code change addresses a specific scenario where the combo box's editor is a `JTextField`. The original code likely failed to correctly update the text field in this scenario, leading to a bug. The new code handles this case by directly setting the text on the `JTextField`, which aligns with the commit message's intention of fixing a bug in setting the value of the field. The addition of the `instanceof` check and the conditional logic strongly suggest a bug fix related to how the text field within the combo box was being updated.

**Conclusion:**

**Buggy**
"
weka,36417.json,5887d72aa9c3dc9d14f09c98ea60b54b3ea4a75c,"@@ -1,31 +1,40 @@
   public void processSecondary(Integer setNum, Integer maxSetNum, Data data,
     PairedDataHelper<weka.classifiers.Classifier> helper) throws WekaException {
 
     // trained classifier for this set number
     weka.classifiers.Classifier classifier =
       helper.getIndexedPrimaryResult(setNum);
 
     // test data
     Instances testSplit = data.getPrimaryPayload();
 
+    if (m_trainedClassifierHeader != null
+      && !testSplit.equalHeaders(m_trainedClassifierHeader)) {
+      if (!(m_trainedClassifier instanceof InputMappedClassifier)) {
+        throw new WekaException(
+          ""Structure of incoming data does not match ""
+            + ""that of the trained classifier"");
+      }
+    }
+
     // paired training data
     Instances trainingSplit =
       helper.getIndexedValueFromNamedStore(""trainingSplits"", setNum);
 
     getStepManager().logBasic(
       ""Dispatching model for set "" + setNum + "" out of "" + maxSetNum
         + "" to output"");
 
     Data batchClassifier =
       new Data(StepManager.CON_BATCH_CLASSIFIER, classifier);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET,
       trainingSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TESTSET,
       testSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, setNum);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM,
       maxSetNum);
     batchClassifier
       .setPayloadElement(StepManager.CON_AUX_DATA_LABEL, getName());
     getStepManager().outputData(batchClassifier);
   }",Buggy,"Fixed a bug where the Classifier step was not checking for compatible structure structure between training and test sets in the case where the classifier is not an InputMappedClassifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14678 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the `Classifier` step not checking for compatible structure between training and test sets when the classifier is not an `InputMappedClassifier`.

The code diff introduces a check to ensure that the structure of the incoming test data matches the structure of the data used to train the classifier (`m_trainedClassifierHeader`). This check is performed only when the classifier is not an `InputMappedClassifier`. If the structures don't match and the classifier is not an `InputMappedClassifier`, a `WekaException` is thrown, indicating an incompatibility issue.

This change directly addresses the bug described in the commit message by adding a validation step to prevent the use of incompatible data structures, which could lead to incorrect classification results or errors. The added exception handling further supports this conclusion.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,36360.json,a415ad7bb7e75ba3cda722d4c62bd27c01db961e,"@@ -1,78 +1,84 @@
   protected void createOffscreenPlot(Data data) {
     List<Instances> offscreenPlotData = new ArrayList<Instances>();
     Instances predictedI = data.getPrimaryPayload();
+    boolean colorSpecified = false;
 
-    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()) {
+    String additional = m_additionalOptions;
+    if (m_additionalOptions.length() > 0) {
+      additional = environmentSubstitute(additional);
+    }
+
+    if (!additional.contains(""-color"")
+      && m_offscreenRendererName.contains(""Weka Chart Renderer"")) {
+      // for WekaOffscreenChartRenderer only
+      if (additional.length() > 0) {
+        additional += "","";
+      }
+      if (predictedI.classIndex() >= 0) {
+        additional += ""-color="" + predictedI.classAttribute().name();
+      } else {
+        additional += ""-color=/last"";
+      }
+    } else {
+      colorSpecified = true;
+    }
+
+    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()
+      && !colorSpecified) {
       // set up multiple series - one for each class
       Instances[] classes = new Instances[predictedI.numClasses()];
       for (int i = 0; i < predictedI.numClasses(); i++) {
         classes[i] = new Instances(predictedI, 0);
         classes[i].setRelationName(predictedI.classAttribute().value(i));
       }
       for (int i = 0; i < predictedI.numInstances(); i++) {
         Instance current = predictedI.instance(i);
         classes[(int) current.classValue()].add((Instance) current.copy());
       }
       for (Instances classe : classes) {
         offscreenPlotData.add(classe);
       }
     } else {
       offscreenPlotData.add(new Instances(predictedI));
     }
 
     List<String> options = new ArrayList<String>();
-    String additional = m_additionalOptions;
-    if (m_additionalOptions.length() > 0) {
-      additional = environmentSubstitute(additional);
-    }
-
-    if (additional.contains(""-color"")) {
-      // for WekaOffscreenChartRenderer only
-      if (additional.length() > 0) {
-        additional += "","";
-      }
-      if (predictedI.classIndex() >= 0) {
-        additional += ""-color="" + predictedI.classAttribute().name();
-      } else {
-        additional += ""-color=/last"";
-      }
-    }
 
     String[] optionsParts = additional.split("","");
     for (String p : optionsParts) {
       options.add(p.trim());
     }
 
     // only need the x-axis (used to specify the attribute to plot)
     String xAxis = m_xAxis;
     xAxis = environmentSubstitute(xAxis);
 
     String width = m_width;
     String height = m_height;
     int defWidth = 500;
     int defHeight = 400;
     width = environmentSubstitute(width);
     height = environmentSubstitute(height);
 
     defWidth = Integer.parseInt(width);
     defHeight = Integer.parseInt(height);
 
     try {
       getStepManager().logDetailed(""Creating image"");
       BufferedImage osi =
         m_offscreenRenderer.renderHistogram(defWidth, defHeight,
           offscreenPlotData, xAxis, options);
 
       Data imageData = new Data(StepManager.CON_IMAGE, osi);
       String relationName = predictedI.relationName();
       if (relationName.length() > 10) {
         relationName = relationName.substring(0, 10);
       }
       imageData.setPayloadElement(StepManager.CON_AUX_DATA_TEXT_TITLE,
         relationName + "":"" + m_xAxis);
       getStepManager().outputData(imageData);
     } catch (Exception e1) {
       e1.printStackTrace();
     }
 
   }",Buggy,"Fixed a few bugs in offscreen rendering that affected stacked histograms

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13327 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code changes involve modifications to the `createOffscreenPlot` method, specifically addressing how color is specified for the offscreen rendering, especially when using `Weka Chart Renderer`. The commit message indicates that these changes are bug fixes related to offscreen rendering, particularly affecting stacked histograms.

Here's a breakdown of the changes and their relevance to bug fixing:

1.  **Color Specification Logic**: The code now includes a `colorSpecified` boolean to track whether the color has already been specified in the additional options. This is likely to prevent redundant or conflicting color specifications.
2.  **Conditional Color Addition**: The code now checks if the `-color` option is already present in the `additionalOptions` string before adding it. This prevents the addition of duplicate `-color` options, which could lead to unexpected behavior or errors.
3.  **Weka Chart Renderer Specific Logic**: The color specification logic is now specifically applied only when the `offscreenRendererName` contains ""Weka Chart Renderer"". This suggests that the bug was specific to this renderer.
4.  **Class Index Handling**: The code checks if `predictedI.classIndex()` is greater than or equal to 0 before adding the `-color` option with the class attribute name. This prevents errors when there is no class attribute.
5.  **Order of Operations**: The code now checks if a color is specified *before* splitting the data into multiple series based on class values. This ensures that the color is correctly applied to the entire dataset or to each series, depending on the context.

These changes suggest that the previous implementation had issues with color handling in offscreen rendering, especially when dealing with stacked histograms and the `Weka Chart Renderer`. The fixes address these issues by ensuring that color is specified correctly and consistently, preventing redundant specifications, and handling cases where there is no class attribute.

**Conclusion:**

**Buggy**
"
weka,9851.json,64d54abda520da9b2297ed56047ce86beda812b4,"@@ -1,113 +1,117 @@
   protected void makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, Instances test, FastVector modelError, int[][] sortedIndices,
       double[][] weights, double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini, boolean useErrorRate)
   throws Exception {
 
     if (BestFirstElements.size()==0) return;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     //BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return;
+      }
+
       BFTree nextSplitNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	  nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	  nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	  nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	  useHeuristic, useGini, useErrorRate);
       return;
 
     }
 
     // If gini gain or information gain is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
     }
 
     // gini gain or information gain is not 0
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       m_Attribute = att;
       if (att.isNumeric()) m_SplitValue = splitValue;
       else m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, m_Attribute,
 	  m_SplitValue, m_SplitString,
 	  sortedIndices, weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 	makeLeaf(train);
       }
 
       // split the node and cauculate error rate of this temporary tree
       else {
 	m_isLeaf = false;
 	m_Attribute = att;
 
 	makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, m_Attribute, useHeuristic, useGini);
 	for (int i=0; i<2; i++){
 	  m_Successors[i].makeLeaf(train);
 	}
 
 	Evaluation eval = new Evaluation(test);
 	eval.evaluateModel(root, test);
 	double error;
 	if (useErrorRate) error = eval.errorRate();
 	else error = eval.rootMeanSquaredError();
 	modelError.addElement(new Double(error));
       }
 
       if (BestFirstElements.size()!=0) {
 	FastVector nextSplitElement = (FastVector)BestFirstElements.elementAt(0);
 	BFTree nextSplitNode = (BFTree)nextSplitElement.elementAt(0);
 	nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	    nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	    nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	    nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	    useHeuristic, useGini,useErrorRate);
       }
     }
   }",Buggy,"Fixed null pntr exception due to the fact that BFTree's recursion depended on a bug in FastVector


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a null pointer exception in the BFTree algorithm, which was caused by a bug in FastVector related to recursion. The diff shows a check `if (BestFirstElements.size() == 0)` before accessing `BestFirstElements.elementAt(0)` within the `makeTree` method. This check prevents a potential null pointer exception when `BestFirstElements` is empty, which could occur during the recursive calls. The added check aligns with the commit message's description of fixing a null pointer exception.

**Buggy**
"
weka,28572.json,563360413e272237a4f3fe3033b8e50dfb132e27,"@@ -1,12 +1,18 @@
 	void replaceAtt(int nTargetNode, String sName, FastVector values) {
 		Attribute newAtt = new Attribute(sName, values);
 		if (m_Instances.classIndex() == nTargetNode) {
 			m_Instances.setClassIndex(-1);
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			m_Instances.setClassIndex(nTargetNode); */
+			
+			m_Instances.deleteAttributeAt(nTargetNode);
+			m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.setClassIndex(nTargetNode);
 		} else {
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
-			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			m_Instances.deleteAttributeAt(nTargetNode + 1); */
+		        m_Instances.deleteAttributeAt(nTargetNode);
+		        m_Instances.insertAttributeAt(newAtt, nTargetNode);
 		}
 	} // replaceAtt",Buggy,"Fixed a bug (caused indirectly by the change that disallows attributes with the same name in a set of instances) whereby an exception was generated by actions that mutated the values at a node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7834 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to actions that mutate values at a node, which caused an exception. The code diff shows changes in the `replaceAtt` method, specifically how attributes are inserted and deleted within the `m_Instances` object. The original code inserted a new attribute at `nTargetNode` and then deleted the attribute at `nTargetNode + 1`. The corrected code deletes the attribute at `nTargetNode` and then inserts the new attribute at `nTargetNode`. The commented-out code suggests an attempt to fix the issue. The changes in attribute insertion and deletion, along with the commit message, strongly suggest a bug fix.

**Buggy**"
weka,20177.json,b5bafa7c1b1d4a69d3060a9be3b02c4b280127b9,"@@ -1,10 +1,11 @@
   public static void addPlugin(String interfaceName, String name, 
       String concreteType) {
     if (PLUGINS.get(interfaceName) == null) {
       Map<String, String> pluginsOfInterfaceType = 
         new TreeMap<String, String>();
       pluginsOfInterfaceType.put(name, concreteType);
+      PLUGINS.put(interfaceName, pluginsOfInterfaceType);      
     } else {
       PLUGINS.get(interfaceName).put(name, concreteType);
     }
   }",Buggy,"Fixed a bug in the plugin registering mechanism.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7629 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug in the plugin registering mechanism"" clearly states that the commit addresses a bug related to how plugins are registered. This suggests that the code changes should involve modifications to the plugin registration logic.

**Git Diff Analysis:**

The diff shows a modification within the `addPlugin` method. Let's break down the changes:

*   **Original Code:**

    ```java
    if (PLUGINS.get(interfaceName) == null) {
      Map<String, String> pluginsOfInterfaceType = 
        new TreeMap<String, String>();
      pluginsOfInterfaceType.put(name, concreteType);
    } else {
      PLUGINS.get(interfaceName).put(name, concreteType);
    }
    ```

*   **Modified Code:**

    ```java
    if (PLUGINS.get(interfaceName) == null) {
      Map<String, String> pluginsOfInterfaceType = 
        new TreeMap<String, String>();
      pluginsOfInterfaceType.put(name, concreteType);
      PLUGINS.put(interfaceName, pluginsOfInterfaceType);      
    } else {
      PLUGINS.get(interfaceName).put(name, concreteType);
    }
    ```

The key change is the addition of the line `PLUGINS.put(interfaceName, pluginsOfInterfaceType);` inside the `if` block. This line is responsible for actually storing the newly created `pluginsOfInterfaceType` map into the `PLUGINS` map when a plugin of that interface type is registered for the first time.

**Reasoning:**

Without the added line, when a new `interfaceName` is encountered, the code creates a new `TreeMap` and puts the plugin name and concrete type into it. However, it *never* actually stores this map in the `PLUGINS` map. This would mean that subsequent attempts to register plugins for the same `interfaceName` would fail to retrieve the previously registered plugins, effectively losing the initial plugin registration.

The addition of `PLUGINS.put(interfaceName, pluginsOfInterfaceType);` fixes this by ensuring that the newly created map is stored in the `PLUGINS` map, making it accessible for future plugin registrations of the same interface type. This aligns perfectly with the commit message indicating a fix in the plugin registration mechanism.

**Conclusion:**

The code change directly addresses a flaw in the plugin registration logic, and the commit message accurately describes the fix. Therefore, the changes indicate a bug fix.

**Buggy**
"
weka,21148.json,bad512e53bc1d955e4847e14296af551c9f28459,"@@ -1,50 +1,50 @@
   public void acceptDataSet(DataSetEvent e) {
 
     m_busy = true;
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Processing batch..."");
     }
 
     try {
       makeOutputStructure(new Instances(e.getDataSet(), 0));
     } catch (Exception ex) {
       String msg = statusMessagePrefix()
         + ""ERROR: unable to create output instances structure."";
       if (m_log != null) {
         m_log.statusMessage(msg);
         m_log.logMessage(""[SubstringLabeler] "" + ex.getMessage());
       }
       stop();
 
       ex.printStackTrace();
       m_busy = false;
       return;
     }
 
     Instances toProcess = e.getDataSet();
 
     for (int i = 0; i < toProcess.numInstances(); i++) {
       Instance current = toProcess.instance(i);
       Instance result = null;
       try {
         result = m_matches.makeOutputInstance(current, true);
       } catch (Exception ex) {
         ex.printStackTrace();
       }
 
       if (result != null) {
         // m_outputStructure.add(result);
         m_matches.getOutputStructure().add(result);
       }
     }
 
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Finished."");
     }
 
     // notify listeners
-    DataSetEvent d = new DataSetEvent(this, m_matches.getInputStructure());
+    DataSetEvent d = new DataSetEvent(this, m_matches.getOutputStructure());
     notifyDataListeners(d);
 
     m_busy = false;
   }",Buggy,"Fixed a bug in batch mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11956 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in batch mode. The code diff modifies the `acceptDataSet` method, specifically the `DataSetEvent` constructor. The original code used `m_matches.getInputStructure()`, while the corrected code uses `m_matches.getOutputStructure()`. This suggests that the original code was incorrectly passing the input structure instead of the output structure, which would be a bug.

**Buggy**"
weka,36953.json,aeff9067d2f9056edfc89df194af976e07e4bd40,"@@ -1,105 +1,106 @@
   public void processIncoming(Data data) throws WekaException {
     Object modelToSave = null;
     Instances modelHeader = null;
     Integer setNum = null;
     Integer maxSetNum = null;
 
     if (data.getConnectionName().equals(StepManager.CON_INCREMENTAL_CLASSIFIER)) {
       if (m_incrementalHeader == null
         && !getStepManager().isStreamFinished(data)) {
         m_incrementalHeader =
           ((Instance) data
             .getPayloadElement(StepManager.CON_AUX_DATA_TEST_INSTANCE))
             .dataset();
       }
       if (getStepManager().isStreamFinished(data)
         || (m_incrementalSaveSchedule > 0
           && m_counter % m_incrementalSaveSchedule == 0 && m_counter > 0)) {
         modelToSave =
           (weka.classifiers.Classifier) data
             .getPayloadElement(StepManager.CON_INCREMENTAL_CLASSIFIER);
+        modelHeader = m_incrementalHeader;
       }
     } else {
       modelToSave = data.getPayloadElement(data.getConnectionName());
       modelHeader =
         (Instances) data
           .getPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET);
       setNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
       maxSetNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
       if (modelHeader == null) {
         modelHeader =
           (Instances) data.getPayloadElement(StepManager.CON_AUX_DATA_TESTSET);
       }
     }
 
     if (modelToSave != null) {
       if (modelToSave instanceof UpdateableBatchProcessor) {
         try {
           // make sure model cleans up before saving
           ((UpdateableBatchProcessor) modelToSave).batchFinished();
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
 
       if (modelHeader != null) {
         modelHeader = new Instances(modelHeader, 0);
       }
 
       getStepManager().processing();
       String prefix = getStepManager().environmentSubstitute(m_filenamePrefix);
       String relationName =
         m_includeRelationName && modelHeader != null ? modelHeader
           .relationName() : """";
       String setSpec =
         maxSetNum != null && setNum != null ? ""_"" + setNum + ""_"" + maxSetNum
           + ""_"" : """";
 
       String modelName = modelToSave.getClass().getCanonicalName();
       modelName =
         modelName.substring(modelName.lastIndexOf(""."") + 1, modelName.length());
       String filename = """" + prefix + relationName + setSpec + modelName;
       filename = sanitizeFilename(filename);
 
       String dirName =
         getStepManager().environmentSubstitute(m_directory.toString());
       File tempFile = new File(dirName);
       filename = tempFile.getAbsolutePath() + File.separator + filename;
 
       getStepManager().logBasic(
         ""Saving model "" + modelToSave.getClass().getCanonicalName() + "" to ""
           + filename + "".model"");
       getStepManager().statusMessage(
         ""Saving model: "" + modelToSave.getClass().getCanonicalName());
 
       ObjectOutputStream oos = null;
       try {
         oos =
           new ObjectOutputStream(new BufferedOutputStream(new FileOutputStream(
             new File(filename + "".model""))));
         oos.writeObject(modelToSave);
         if (modelHeader != null) {
           oos.writeObject(modelHeader);
         }
         oos.close();
       } catch (Exception ex) {
         throw new WekaException(ex);
       } finally {
         if (data.getConnectionName() != StepManager.CON_INCREMENTAL_CLASSIFIER
           || getStepManager().isStreamFinished(data)) {
           getStepManager().finished();
         }
         if (oos != null) {
           try {
             oos.close();
           } catch (Exception ex) {
             throw new WekaException(ex);
           }
         }
       }
     }
 
     m_counter++;
   }",Buggy,"Fixed a bug where the training data header was not getting saved for incremental models

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13175 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to saving the training data header for incremental models. The diff shows that `modelHeader = m_incrementalHeader;` is added when `modelToSave` is an incremental classifier. This suggests that previously, the `modelHeader` was not being correctly assigned for incremental models, leading to the bug described in the commit message.

**Buggy**"
weka,9988.json,ff62ff2731fed53f3db748855b97c8c08a893dca,"@@ -1,19 +1,19 @@
   protected synchronized void trimZeroDistances(Neighborhood n) {    
-    int index = 0;
+    int index = n.m_neighbors.numInstances();
     for (int i = 0; i < n.m_neighbors.numInstances(); i++) {
       if (n.m_distances[i] > 0) {
         index = i;
         break;
       }
     }
     
     if (index > 0) {
       // trim zero distances
       for (int i = 0; i < index; i++) {
         n.m_neighbors.remove(0);
       }
       double[] newDist = new double[n.m_distances.length - index];
       System.arraycopy(n.m_distances, index, newDist, 0, newDist.length);
       n.m_distances = newDist;
     }
   }",Buggy,"Fixed a bug in trimZeroDistances()

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9723 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug in trimZeroDistances()"" clearly states that the commit addresses a bug within the `trimZeroDistances()` method. This suggests that the code changes are intended to correct incorrect behavior.

**Code Diff Analysis:**

The code diff shows a modification to the initialization of the `index` variable.

- **Original Code:** `int index = 0;`
- **Modified Code:** `int index = n.m_neighbors.numInstances();`

The original code initializes `index` to 0. The modified code initializes `index` to the number of instances in `n.m_neighbors`.

**Reasoning:**

The original code's logic appears flawed. The intention seems to be to find the index of the first non-zero distance in the `n.m_distances` array. However, if all distances are zero, the loop will complete without updating `index`, leaving it at 0.  The subsequent `if (index > 0)` check would then always be false, preventing the trimming of zero distances even when they exist.

The corrected code initializes `index` to `n.m_neighbors.numInstances()`. If the loop doesn't find a non-zero distance, `index` will remain at this initial value. The `if (index > 0)` condition will then be true *only if* there are neighbors. This suggests that the intention is to trim zero distances only if there are neighbors and all distances are zero. If there are no neighbors, then there is nothing to trim.

The change addresses a potential issue where zero distances were not being trimmed correctly, especially when all distances in the neighborhood were zero. This aligns with the commit message indicating a bug fix.

**Conclusion:**

**Buggy**
"
weka,18815.json,0cd35ea8a7783c8ca9d16a71fc888af2d82c3e90,"@@ -1,69 +1,74 @@
   public static void main(String[] args) {
+
+    weka.core.logging.Logger.log(weka.core.logging.Logger.Level.INFO,
+      ""Logging started"");
+    WekaPackageManager.loadPackages(false, true, true);
+
     try {
       LookAndFeel.setLookAndFeel(WorkbenchDefaults.APP_ID,
         WorkbenchDefaults.APP_ID + "".lookAndFeel"", WorkbenchDefaults.LAF);
     } catch (IOException ex) {
       ex.printStackTrace();
     }
     weka.gui.GenericObjectEditor.determineClasses();
 
     try {
       if (System.getProperty(""os.name"").contains(""Mac"")) {
         System.setProperty(""apple.laf.useScreenMenuBar"", ""true"");
       }
       m_workbench = new WorkbenchApp();
       final javax.swing.JFrame jf =
         new javax.swing.JFrame(""Weka "" + m_workbench.getApplicationName());
       jf.getContentPane().setLayout(new java.awt.BorderLayout());
 
       Image icon =
         Toolkit.getDefaultToolkit().getImage(
           WorkbenchApp.class.getClassLoader().getResource(
             ""weka/gui/weka_icon_new_48.png""));
       jf.setIconImage(icon);
 
       jf.getContentPane().add(m_workbench, BorderLayout.CENTER);
       jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
       jf.pack();
       m_workbench.showMenuBar(jf);
       jf.setSize(1024, 768);
       jf.setVisible(true);
 
       if (args.length == 1) {
         System.err.println(""Loading instances from "" + args[0]);
         AbstractFileLoader loader = ConverterUtils.getLoaderForFile(args[0]);
         loader.setFile(new File(args[0]));
         m_workbench.getPerspectiveManager().getMainPerspective()
           .setInstances(loader.getDataSet());
       }
 
       Thread memMonitor = new Thread() {
         @Override
         public void run() {
           while (true) {
             // try {
             // System.out.println(""Before sleeping."");
             // Thread.sleep(10);
 
             if (m_Memory.isOutOfMemory()) {
               // clean up
               jf.dispose();
               m_workbench = null;
               System.gc();
 
               // display error
               System.err.println(""\ndisplayed message:"");
               m_Memory.showOutOfMemory();
               System.err.println(""\nexiting"");
               System.exit(-1);
             }
           }
         }
       };
 
       memMonitor.setPriority(Thread.MAX_PRIORITY);
       memMonitor.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a weird class loading bug that affected loading RPlugin

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15118 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a class loading bug specifically affecting RPlugin. The diff shows the addition of logging at the start of the `main` method using `weka.core.logging.Logger`, and the explicit loading of packages using `WekaPackageManager.loadPackages(false, true, true)`. These changes suggest an attempt to diagnose and resolve class loading issues, which aligns with the commit message. The explicit package loading might be a workaround or a fix to ensure RPlugin and its dependencies are loaded correctly, addressing the reported bug.

**Buggy**"
weka,30927.json,95a3f1d8e4e0087ec7760ebed797b134337b1feb,"@@ -1,31 +1,32 @@
   protected static String makeOptionStr(AbstractFileLoader loader) {
     StringBuffer result;
     Option option;
 
     result = new StringBuffer(""\nUsage:\n"");
     result.append(""\t"" + loader.getClass().getName().replaceAll("".*\\."", """"));
-    if (loader instanceof OptionHandler) {
-      result.append("" [options]"");
-    }
     result.append("" <"");
     String[] ext = loader.getFileExtensions();
     for (int i = 0; i < ext.length; i++) {
       if (i > 0) {
         result.append("" | "");
       }
       result.append(""file"" + ext[i]);
     }
-    result.append("">\n"");
+    result.append("">"");
+    if (loader instanceof OptionHandler) {
+      result.append("" [options]"");
+    }
+    result.append(""\n"");
 
     if (loader instanceof OptionHandler) {
       result.append(""\nOptions:\n\n"");
       Enumeration<Option> enm = ((OptionHandler) loader).listOptions();
       while (enm.hasMoreElements()) {
         option = enm.nextElement();
         result.append(option.synopsis() + ""\n"");
         result.append(option.description() + ""\n"");
       }
     }
 
     return result.toString();
   }",Buggy,"Fixed output bug in command-line help: file name needs to be given *before* options.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12103 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to the order of arguments in the command-line help output. Specifically, it states that the file name should be specified *before* the options.

The diff modifies the `makeOptionStr` method, which generates the help string for file loaders. The key change is the reordering of the parts that are appended to the `result` StringBuffer.

Before the change, the code appended ""[options]"" before the file name part (""<file.ext>""). After the change, the file name part is appended before the ""[options]"" part. This aligns perfectly with the commit message's description of the bug fix.

Therefore, the changes clearly indicate a bug fix.

**Buggy**"
weka,12790.json,bfb54a334c1bdae9044c51ab3cd1319734e8cab4,"@@ -1,98 +1,100 @@
     public void buildClusterer(Instances data) throws Exception
   {
     reset();
     meanInstance = new DenseInstance(data.numAttributes());
     for (int i = 0; i < data.numAttributes(); i++)
       meanInstance.setValue(i, data.meanOrMode(i));
     numInstances = data.numInstances();
 
     kMeans.setDistanceFunction(distanceFunction);
     kMeans.setMaxIterations(maxIterations);
     //    kMeans.setInitializeUsingKMeansPlusPlusMethod(initializeWithKMeansPlusPlus);
-    kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    if (initializeWithKMeansPlusPlus) {
+      kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    }
 
     /**
      * step 1: iterate over all restarts and possible k values, record CH-scores
      */
     Random r = new Random(m_Seed);
     double meanCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     double maxCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     int maxSeed[] = new int[maxNumClusters + 1 - minNumClusters];
 
     for (int i = 0; i < restarts; i++)
       {
         if (printDebug)
           System.out.println(""cascade> restarts: "" + (i + 1) + "" / "" + restarts);
 
         for (int k = minNumClusters; k <= maxNumClusters; k++)
           {
             if (printDebug)
               System.out.print(""cascade>  k:"" + k + "" "");
 
             int seed = r.nextInt();
             kMeans.setSeed(seed);
             kMeans.setNumClusters(k);
             kMeans.buildClusterer(data);
             double ch = getCalinskiHarabasz();
 
             int index = k - minNumClusters;
             meanCHs[index] = (meanCHs[index] * i + ch) / (double) (i + 1);
             if (i == 0 || ch > maxCHs[index])
               {
                 maxCHs[index] = ch;
                 maxSeed[index] = seed;
               }
 
             if (printDebug)
               System.out.println("" CH:"" + df.format(ch) + ""  W:""
                                  + df.format(kMeans.getSquaredError() / (double) (numInstances - kMeans.getNumClusters()))
                                  + "" (unweighted:"" + df.format(kMeans.getSquaredError()) + "")  B:""
                                  + df.format(getSquaredErrorBetweenClusters() / (double) (kMeans.getNumClusters() - 1))
                                  + "" (unweighted:"" + df.format(getSquaredErrorBetweenClusters()) + "") "");
           }
       }
     if (printDebug)
       {
         String s = ""cascade> max CH: [ "";
         for (int i = 0; i < maxSeed.length; i++)
           s += df.format(maxCHs[i]) + "" "";
         System.out.println(s + ""]"");
       }
     String s = ""cascade> mean CH: [ "";
     for (int i = 0; i < maxSeed.length; i++)
       s += df.format(meanCHs[i]) + "" "";
 
     finalMeanCH = s + ""]"";
     //    System.out.println(s + ""]"");
 
     /**
      * step 2: select k with best mean CH-score; select seed for max CH score for this k
      */
     int bestK = -1;
     double maxCH = -1;
     for (int k = minNumClusters; k <= maxNumClusters; k++)
       {
         int index = k - minNumClusters;
         if (bestK == -1 || meanCHs[index] > maxCH)
           {
             maxCH = meanCHs[index];
             bestK = k;
           }
       }
     if (manuallySelectNumClusters)
       {
         int selectedK = selectKManually(meanCHs, bestK);
         if (selectedK != -1)
           bestK = selectedK;
       }
     int bestSeed = maxSeed[bestK - minNumClusters];
 
     finalBestK = bestK;
     finalBestSeed = bestSeed;
     //    System.out.println(""cascade> k (yields highest mean CH): "" + bestK);
     //    System.out.println(""cascade> seed (highest CH for k="" + bestK + "") : "" + bestSeed);
 
     kMeans.setSeed(bestSeed);
     kMeans.setNumClusters(bestK);
     kMeans.buildClusterer(data);
   }",Buggy,"Fixed a small bug introduced with the last change.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10457 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the previous change. The code diff introduces a conditional statement around the line `kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));`. This line was previously unconditionally executed, but now it's only executed if `initializeWithKMeansPlusPlus` is true. This suggests that the previous change might have incorrectly forced the KMEANS_PLUS_PLUS initialization method, potentially leading to unexpected behavior or errors when a different initialization method was desired. The conditional statement corrects this, indicating a bug fix.

**Buggy**"
weka,31032.json,4ab412e6138508ec825027ffab22b044aedd6a09,"@@ -1,9 +1,11 @@
   private void initTokenizer(StreamTokenizer tokenizer) {
     tokenizer.resetSyntax();         
     tokenizer.whitespaceChars(0, (' '-1));    
     tokenizer.wordChars(' ','\u00FF');
     tokenizer.whitespaceChars(',',',');
     tokenizer.whitespaceChars('\t','\t');
     tokenizer.commentChar('%');
+    tokenizer.quoteChar('""');
+    tokenizer.quoteChar('\'');
     tokenizer.eolIsSignificant(true);
   }",Buggy,"Fixed bug introduced in r4914, where handling of quoted strings got deleted.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug introduced in revision r4914, related to the handling of quoted strings, has been fixed. The provided diff shows the addition of two lines: `tokenizer.quoteChar('""');` and `tokenizer.quoteChar('\'');`. These lines configure the `StreamTokenizer` to recognize double quotes (`""`) and single quotes (`'`) as quote characters. This directly addresses the issue described in the commit message, indicating that the code was modified to restore the handling of quoted strings, which was previously broken. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,28754.json,4718fad5f7d375b9878d9d345d7cefc62c223c70,"@@ -1,25 +1,26 @@
   protected synchronized void completedClassifier(int iteration,
       boolean success) {
-    m_completed++;
 
     if (!success) {
       m_failed++;
       if (m_Debug) {
         System.err.println(""Iteration "" + iteration + "" failed!"");
       }
+    } else {
+      m_completed++;
     }
 
     if (m_completed + m_failed == m_Classifiers.length) {
       if (m_failed > 0) {
         if (m_Debug) {
           System.err.println(""Problem building classifiers - some iterations failed."");
         }
       }
 
       // have to shut the pool down or program executes as a server
       // and when running from the command line does not return to the
       // prompt
       m_executorPool.shutdown();
       block(false);
     }
   }",Buggy,"Fixed a bug that prevented the sum of completed and failed classifiers from equaling the total number of classifiers when failures occur. This bug only affected the case when one or more failures occurred.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6266 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the calculation of completed and failed classifiers. The bug specifically occurs when there are failures, leading to an incorrect total count.

The code diff shows that the `m_completed++` statement was moved from the beginning of the method to inside the `else` block, which is executed only when `success` is true. This change ensures that `m_completed` is incremented only for successful classifiers. The original code incremented `m_completed` regardless of success, which would lead to an incorrect count when failures occurred. The condition `m_completed + m_failed == m_Classifiers.length` is used to determine when all classifiers have finished. The original code would increment `m_completed` even when a classifier failed, leading to the condition being met prematurely.

The change directly addresses the issue described in the commit message, ensuring that the sum of completed and failed classifiers equals the total number of classifiers, even when failures occur.

**Buggy**"
weka,36601.json,2b4d1e40b90f0675df910618594260c237945fec,"@@ -1,3 +1,3 @@
-  public boolean getStepMustRunSingleThreaded() {
+  public boolean stepMustRunSingleThreaded() {
     return getStepManager().getStepMustRunSingleThreaded();
   }",Buggy,"Fixed a bug that could cause Knowledge Flow files to fail to load. This was inadvertently introduced with the single threaded executor addition

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13700 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to Knowledge Flow files failing to load, which was introduced by the single-threaded executor addition. The code diff shows a change in the method name from `getStepMustRunSingleThreaded` to `stepMustRunSingleThreaded`.

The change in method name alone does not directly indicate a bug fix related to file loading. However, the commit message explicitly states that a bug was fixed. It's possible that this method name change is part of a larger set of changes that fixed the file loading issue, or that the method name change itself was related to the bug. Without more context, it's difficult to determine the exact relationship between the method name change and the bug fix. However, given the commit message, it is likely that this change is related to fixing the bug.

**Buggy**"
weka,1770.json,81174005aadc513eaeddade0fcb41f7c02f7cf21,"@@ -1,106 +1,107 @@
   protected void makeTree(Instances data, int totalInstances,
     int[][] sortedIndices, double[][] weights, double[] classProbs,
     double totalWeight, double minNumObj, boolean useHeuristic)
     throws Exception {
 
     // if no instances have reached this node (normally won't happen)
     if (totalWeight == 0) {
       m_Attribute = null;
       m_ClassValue = Utils.missingValue();
       m_Distribution = new double[data.numClasses()];
       return;
     }
 
     m_totalTrainInstances = totalInstances;
     m_isLeaf = true;
+    m_Successors = null;
 
     m_ClassProbs = new double[classProbs.length];
     m_Distribution = new double[classProbs.length];
     System.arraycopy(classProbs, 0, m_ClassProbs, 0, classProbs.length);
     System.arraycopy(classProbs, 0, m_Distribution, 0, classProbs.length);
     if (Utils.sum(m_ClassProbs) != 0) {
       Utils.normalize(m_ClassProbs);
     }
 
     // Compute class distributions and value of splitting
     // criterion for each attribute
     double[][][] dists = new double[data.numAttributes()][0][0];
     double[][] props = new double[data.numAttributes()][0];
     double[][] totalSubsetWeights = new double[data.numAttributes()][2];
     double[] splits = new double[data.numAttributes()];
     String[] splitString = new String[data.numAttributes()];
     double[] giniGains = new double[data.numAttributes()];
 
     // for each attribute find split information
     for (int i = 0; i < data.numAttributes(); i++) {
       Attribute att = data.attribute(i);
       if (i == data.classIndex()) {
         continue;
       }
       if (att.isNumeric()) {
         // numeric attribute
         splits[i] = numericDistribution(props, dists, att, sortedIndices[i],
           weights[i], totalSubsetWeights, giniGains, data);
       } else {
         // nominal attribute
         splitString[i] = nominalDistribution(props, dists, att,
           sortedIndices[i], weights[i], totalSubsetWeights, giniGains, data,
           useHeuristic);
       }
     }
 
     // Find best attribute (split with maximum Gini gain)
     int attIndex = Utils.maxIndex(giniGains);
     m_Attribute = data.attribute(attIndex);
 
     m_train = new Instances(data, sortedIndices[attIndex].length);
     for (int i = 0; i < sortedIndices[attIndex].length; i++) {
       Instance inst = data.instance(sortedIndices[attIndex][i]);
       Instance instCopy = (Instance) inst.copy();
       instCopy.setWeight(weights[attIndex][i]);
       m_train.add(instCopy);
     }
 
     // Check if node does not contain enough instances, or if it can not be
     // split,
     // or if it is pure. If does, make leaf.
     if (totalWeight < 2 * minNumObj || giniGains[attIndex] == 0
       || props[attIndex][0] == 0 || props[attIndex][1] == 0) {
       makeLeaf(data);
     }
 
     else {
       m_Props = props[attIndex];
       int[][][] subsetIndices = new int[2][data.numAttributes()][0];
       double[][][] subsetWeights = new double[2][data.numAttributes()][0];
 
       // numeric split
       if (m_Attribute.isNumeric()) {
         m_SplitValue = splits[attIndex];
       } else {
         m_SplitString = splitString[attIndex];
       }
 
       splitData(subsetIndices, subsetWeights, m_Attribute, m_SplitValue,
         m_SplitString, sortedIndices, weights, data);
 
       // If split of the node results in a node with less than minimal number of
       // isntances,
       // make the node leaf node.
       if (subsetIndices[0][attIndex].length < minNumObj
         || subsetIndices[1][attIndex].length < minNumObj) {
         makeLeaf(data);
         return;
       }
 
       // Otherwise, split the node.
       m_isLeaf = false;
       m_Successors = new SimpleCart[2];
       for (int i = 0; i < 2; i++) {
         m_Successors[i] = new SimpleCart();
         m_Successors[i].makeTree(data, m_totalTrainInstances, subsetIndices[i],
           subsetWeights[i], dists[attIndex][i],
           totalSubsetWeights[attIndex][i], minNumObj, useHeuristic);
       }
     }
   }",Buggy,"Fixed problem where unprune() step caused root node to not be labeled a leaf when it really is a leaf. This only caused problems when unpruned tree consists of a single node that is a leaf node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a specific scenario where the `unprune()` step incorrectly labels the root node as non-leaf when it should be a leaf, particularly when the unpruned tree consists of a single node.

The code diff shows a modification within the `makeTree` method. Specifically, the line `m_Successors = null;` is added after `m_isLeaf = true;`. This addition directly addresses the issue described in the commit message. When a node is determined to be a leaf, `m_isLeaf` is set to true, and now `m_Successors` is explicitly set to null. This ensures that the node is correctly identified as a leaf, even if it's the root node of a single-node tree. Without this change, there might be a case where `m_Successors` is not null, leading to misclassification.

The change directly relates to correcting a logical error in how leaf nodes are handled, particularly in the context of the `unprune()` operation.

**Buggy**
"
weka,17814.json,34abf3851dd64f8c49bdd8875debb6ec45310dc5,"@@ -1,27 +1,33 @@
   public void setInstancesFromFileQ() {
     
 //     if (m_IOThread == null) {
       int returnVal = m_FileChooser.showOpenDialog(this);
       if (returnVal == JFileChooser.APPROVE_OPTION) {
 	File selected = m_FileChooser.getSelectedFile();
 	
 	try
 	{
 	java.io.Reader r = new java.io.BufferedReader(
 				new java.io.FileReader(selected));
 	Instances i = new Instances(r);
 	setInstances(i);
 	
 	//dataFileLabel.setText(selected.getName());
-	dataFileLabel.setText(i.relationName());
+	String relationName = i.relationName();
+	String truncatedN = relationName;
+	if (relationName.length() > 25) {
+	  truncatedN = relationName.substring(0, 25) + ""..."";
+	}
+	dataFileLabel.setText(truncatedN);
+	dataFileLabel.setToolTipText(relationName);
 	} catch (Exception e)
 	{
 		JOptionPane.showMessageDialog(this,""Can't load at this time,\n""
 				    + ""currently busy with other IO"",
 				    ""Load Instances"",
 				    JOptionPane.WARNING_MESSAGE);
 		    e.printStackTrace();
 	
 	}
       }
   }",Buggy,"Fixed a bug that caused a widget layout problem when loading a dataset with a long relation name.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6482 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a widget layout problem caused by long relation names. The diff shows that the code now truncates the relation name if it exceeds 25 characters and sets the full relation name as a tooltip. This aligns with the commit message, suggesting a bug fix for the layout issue.

**Buggy**"
weka,24248.json,3a2501745298eb32d9cb3350d07eb15e93716bb9,"@@ -1,13 +1,19 @@
   public void calculateDerived() {
 
     mean = Double.NaN;
     stdDev = Double.NaN;
     if (count > 0) {
       mean = sum / count;
+      stdDev = Double.POSITIVE_INFINITY;
       if (count > 1) {
 	stdDev = sumSq - (sum * sum) / count;
 	stdDev /= (count - 1);
+        if (stdDev < 0) {
+          System.err.println(""Warning: stdDev value = "" + stdDev 
+                             + "" -- rounded to zero."");
+          stdDev = 0;
+        }
 	stdDev = Math.sqrt(stdDev);
       }
     }
   }",Buggy,"Fixed some NaN problems with Std Devs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@714 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fixed some NaN problems with Std Devs"" clearly indicates that the commit aims to resolve issues related to `NaN` (Not a Number) values in the calculation of standard deviations. This suggests a bug fix.

**Code Diff Analysis:**

1.  **Initialization of `stdDev`:** The code initializes `stdDev` to `Double.NaN` and then, within the `if (count > 0)` block, sets it to `Double.POSITIVE_INFINITY` before the main calculation. This could be a defensive measure to handle edge cases where the subsequent calculation might result in `NaN`.

2.  **Negative `stdDev` Check:** The code includes a check `if (stdDev < 0)`. If `stdDev` is negative, a warning message is printed to `System.err`, and `stdDev` is set to 0.  A standard deviation cannot be negative, so this is a clear indication of a bug fix, where a calculation was producing an invalid negative value. The code is correcting this invalid state.

**Relevance Assessment:**

The code changes directly address the issue mentioned in the commit message. The initialization to `Double.POSITIVE_INFINITY` and the check for negative `stdDev` values are both related to handling potential `NaN` or invalid values in the standard deviation calculation. The negative `stdDev` check is particularly telling, as it indicates a situation where the original calculation could produce an incorrect result, which is now being corrected.

**Conclusion:**

The commit message and the code diff are highly relevant. The code changes specifically address potential issues with `NaN` and invalid negative values in the standard deviation calculation, which aligns perfectly with the commit message's claim of fixing `NaN` problems. The addition of the check for negative `stdDev` is a strong indicator of a bug fix.

**Buggy**
"
weka,23181.json,2e12f8aa73f943180eb6c082e92698d3281baf35,"@@ -1,8 +1,23 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     if (filter == null) {
       m_AssociatorEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     m_AssociatorEditor.setCapabilitiesFilter(filter);
+    
+    m_StartBut.setEnabled(true);
+    // Check capabilities
+    Capabilities currentFilter = m_AssociatorEditor.getCapabilitiesFilter();
+    Associator associator = (Associator) m_AssociatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (associator != null && currentFilter != null && 
+        (associator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)associator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5384 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel"" clearly indicates a bug fix. The message specifies that the start button's state was not being correctly updated when a new data set was loaded.

**Git Diff Analysis:**

The diff shows modifications within the `updateCapabilitiesFilter` method. Here's a breakdown:

1.  **Initial Setup:** The code first handles the case where the `filter` is null, setting a new `Capabilities` object on the `m_AssociatorEditor`.
2.  **Setting the Filter:**  The `m_AssociatorEditor`'s capabilities filter is set using the provided `filter`.
3.  **Start Button Enablement (Initial):** `m_StartBut.setEnabled(true);` is called. This suggests an initial assumption that the start button should be enabled.
4.  **Capabilities Check:** The code then retrieves the current filter and associator. It checks if the associator implements the `CapabilitiesHandler` interface.
5.  **Capabilities Support Check:** If the associator implements `CapabilitiesHandler`, the code retrieves the associator's capabilities and checks if they support the current filter (using both `supportsMaybe` and `supports`).
6.  **Start Button Disablement (Conditional):** If the associator's capabilities do *not* support the current filter, the start button is disabled: `m_StartBut.setEnabled(false);`.

**Reasoning:**

The original code likely did not correctly update the start button's enabled/disabled state based on the capabilities of the currently selected associator and the provided filter. The added code now explicitly checks the capabilities and disables the start button if the associator does not support the filter. This aligns perfectly with the commit message, indicating that the changes address the described bug. The code ensures that the start button is only enabled when the associator's capabilities are compatible with the current filter.

**Conclusion:**

**Buggy**
"
weka,21328.json,be43f6517961a36acbbc66eeb4fe2bc861f6c154,"@@ -1,8 +1,13 @@
   public Object getValue() {
     String path = getAsText();
     if (path != null && path.length() > 0) {
       return new File(path);
     }
 
-    return new File(""."");
+    JFileChooser embeddedEditor = (JFileChooser) m_fileEditor.getCustomEditor();
+    if (embeddedEditor.getFileSelectionMode() == JFileChooser.DIRECTORIES_ONLY) {
+      return new File(""."");
+    } else {
+      return new File("""");
+    }
   }",Buggy,"Fixed a bug where '.' was returned when the field was empty in the case where the editor has been configured for files only (rather than directories)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13132 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states ""Fixed a bug where '.' was returned when the field was empty in the case where the editor has been configured for files only (rather than directories)"". This strongly suggests a bug fix. The message describes a specific scenario where an incorrect value (a `File` object representing the current directory ""."") was being returned when the file chooser was configured to select files only, and the input field was empty.

2.  **Diff Analysis:**
    *   The original code returned `new File(""."")` when the path was null or empty. This means that regardless of whether the file chooser was configured for files or directories, an empty path would always result in a `File` object representing the current directory.
    *   The modified code introduces a conditional check:
        *   It retrieves the `JFileChooser` used as an embedded editor.
        *   It checks the file selection mode of the `JFileChooser`.
        *   If the file selection mode is `JFileChooser.DIRECTORIES_ONLY`, it returns `new File(""."")`.
        *   Otherwise (if the file selection mode is not `JFileChooser.DIRECTORIES_ONLY`), it returns `new File("""")`. This returns an empty file path.

3.  **Alignment and Bug Fix Indication:** The code change aligns perfectly with the commit message. The original code had a bug where it always returned ""."" for an empty field. The corrected code now differentiates between the ""directories only"" mode and other modes (presumably ""files only"" or ""files and directories""). In the ""files only"" case, it now returns an empty string, which is a more appropriate behavior when the field is empty. This resolves the bug described in the commit message.

**Conclusion:**

**Buggy**
"
weka,6470.json,bae2e9e85c74323389279cac539860e8b88d49d2,"@@ -1,27 +1,26 @@
             public void run() {
               PythonSession session;
-              m_executeScriptBut.setEnabled(false);
               try {
                 session = PythonSession.acquireSession(PythonPanel.this);
-                m_logPanel.statusMessage(""Executing script..."");
                 List<String> outAndErr =
                   session.executeScript(m_scriptEditor.getText(),
                     m_debug.isSelected());
                 if (outAndErr.get(0).length() > 0) {
                   logMessage(outAndErr.get(0), null);
                 }
                 if (outAndErr.get(1).length() > 0) {
                   throw new WekaException(outAndErr.get(1));
                 }
                 refreshVarList(session);
                 checkDebug(session);
                 m_logPanel.statusMessage(""OK"");
               } catch (WekaException ex) {
                 ex.printStackTrace();
                 logMessage(null, ex);
                 m_logPanel.statusMessage(""An error occurred. See log."");
               } finally {
                 PythonSession.releaseSession(PythonPanel.this);
                 m_executeScriptBut.setEnabled(true);
+                revalidate();
               }
             }",Buggy,"Fixed a refresh bug that affected the variables list

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11810 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a refresh bug in the variables list. The diff shows a modification within the `run()` method, specifically the addition of `revalidate()` within the `finally` block. The `finally` block ensures that `revalidate()` is always called after the script execution, regardless of whether an exception occurred. This suggests that the variables list might not have been updating correctly in certain scenarios (e.g., after an exception), and `revalidate()` was added to force a refresh of the UI component displaying the variables list. This aligns with the commit message's description of a refresh bug.

**Buggy**
"
weka,31459.json,3b4b438af6d33017965dabcd7943e5de02464e2c,"@@ -1,29 +1,29 @@
   void forceDeleteAttributeAt(int position) {
 
     int index = locateIndex(position);
 
     m_NumAttributes--;
     if ((index >= 0) && (m_Indices[index] == position)) {
       int[] tempIndices = new int[m_Indices.length - 1];
       double[] tempValues = new double[m_AttValues.length - 1];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index);
       for (int i = index; i < m_Indices.length - 1; i++) {
 	tempIndices[i] = m_Indices[i + 1] - 1;
 	tempValues[i] = m_AttValues[i + 1];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     } else {
       int[] tempIndices = new int[m_Indices.length];
       double[] tempValues = new double[m_AttValues.length];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index + 1);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index + 1);
-      for (int i = index + 1; i < m_Indices.length - 1; i++) {
+      for (int i = index + 1; i < m_Indices.length; i++) {
 	tempIndices[i] = m_Indices[i] - 1;
 	tempValues[i] = m_AttValues[i];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     }
   }",Buggy,"Fixed bug in SparseInstance in forceAttributeDelete.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1236 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `forceAttributeDelete` method of the `SparseInstance` class. The diff shows changes in the `forceDeleteAttributeAt` method.

The core logic involves deleting an attribute at a given position and updating the indices and values arrays accordingly. The code handles two cases: when the attribute to be deleted is found at the expected index (`index >= 0 && m_Indices[index] == position`) and when it is not.

The bug fix appears to be in the `else` block, specifically in the loop condition. The original code had `i < m_Indices.length - 1`, while the corrected code has `i < m_Indices.length`. This suggests that the original code was not processing the last element of the `m_Indices` and `m_AttValues` arrays correctly when the attribute to be deleted was not found at the expected index. This could lead to incorrect index updates and potentially data corruption. The change fixes an off-by-one error, which is a common type of bug.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,29293.json,64d11bc2714c068b7bdba2410a82a0d50c430b55,"@@ -1,24 +1,24 @@
   public void setOptions(String[] options) throws Exception {
     String tmpStr;
 
     tmpStr = Utils.getOption('R', options);
     if (tmpStr.length() != 0) {
       setCombinationRule(new SelectedTag(tmpStr, TAGS_RULES));
     } else {
       setCombinationRule(new SelectedTag(AVERAGE_RULE, TAGS_RULES));
     }
 
     m_classifiersToLoad.clear();
     while (true) {
       String loadString = Utils.getOption('P', options);
       if (loadString.length() == 0) {
         break;
       }
 
       m_classifiersToLoad.add(loadString);
     }
 
-    setDoNotPrintModels(Utils.getFlag(""-do-not-print"", options));
+    setDoNotPrintModels(Utils.getFlag(""do-not-print"", options));
 
     super.setOptions(options);
   }",Buggy,"Fixed an option handling bug

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13518 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed an option handling bug"" clearly states that the commit addresses a bug related to how options are handled. This suggests that the code changes are intended to correct incorrect or unintended behavior in option processing.

**Git Diff Analysis:**

The diff shows a change in the `setOptions` method. Specifically, the line:

`-    setDoNotPrintModels(Utils.getFlag(""-do-not-print"", options));`
`+    setDoNotPrintModels(Utils.getFlag(""do-not-print"", options));`

has been modified. The change involves removing the leading hyphen (""-"") from the option name passed to `Utils.getFlag()`. This indicates that the code was previously looking for an option named ""-do-not-print"", but now it's looking for ""do-not-print"".

**Reasoning:**

The removal of the hyphen suggests that the original code had an error in how it was parsing the command-line options. It was expecting a hyphenated option name, but the actual option being passed (or intended to be passed) did not include the hyphen. This discrepancy would have caused the `setDoNotPrintModels` method to not be called correctly, leading to incorrect behavior. The fix corrects this by aligning the expected option name with the actual option name.

**Conclusion:**

The commit message and the code change are consistent with each other. The change directly addresses an error in option handling, which is precisely what the commit message describes. This indicates that the commit fixes a bug.

**Buggy**"
weka,9850.json,feaeb8e65657f876489351748361cd4cbc9a7fe5,"@@ -1,104 +1,107 @@
   protected boolean makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, int[][] sortedIndices, double[][] weights,
       double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini)
   throws Exception {
 
     if (BestFirstElements.size()==0) return false;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information gain of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     // If no enough data to split for this node or this node can not be split find next node to split.
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return false;
+      }
       BFTree nextNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       return root.makeTree(BestFirstElements, root, train,
 	  nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	  nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	  nextNode.m_Props, minNumObj, useHeuristic, useGini);
     }
 
     // If gini gain or information is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
       return false;
     }
 
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.m_Attribute = att;
       if (att.isNumeric()) nodeToSplit.m_SplitValue = splitValue;
       else nodeToSplit.m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, nodeToSplit.m_Attribute,
 	  nodeToSplit.m_SplitValue, nodeToSplit.m_SplitString,
 	  nodeToSplit.m_SortedIndices, nodeToSplit.m_Weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 
 	nodeToSplit.makeLeaf(train);
 	BFTree nextNode = (BFTree)
 	((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
 	return root.makeTree(BestFirstElements, root, train,
 	    nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	    nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	    nextNode.m_Props, minNumObj, useHeuristic, useGini);
       }
 
       // split the node
       else {
 	nodeToSplit.m_isLeaf = false;
 	nodeToSplit.m_Attribute = att;
 
 	nodeToSplit.makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, nodeToSplit.m_Attribute,useHeuristic,useGini);
 
 	for (int i=0; i<2; i++){
 	  nodeToSplit.m_Successors[i].makeLeaf(train);
 	}
 
 	return true;
       }
     }
   }",Buggy,"Fixed a bug that would occur occasionally when PREPRUNING was selected

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6949 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the `PREPRUNING` option. The code changes include adding a check for `BestFirstElements.size() == 0` after removing an element from `BestFirstElements` and before accessing `BestFirstElements.elementAt(0)`. This suggests that the code was previously attempting to access an element in an empty list, which would cause an error. The added check prevents this error from occurring, indicating a bug fix.

**Buggy**"
weka,27361.json,84effa0146bd9bb228110c26aaf7a40f959c9043,"@@ -1,3 +1,3 @@
-  public double [][] coefficents() {
+  public double [][] coefficients() {
     return m_Par;
   }",Buggy,"Fixed spelling mistake in method to get coefficients :-)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed spelling mistake in method to get coefficients :-)"" indicates a correction of a spelling error. The diff shows a change from `coefficents()` to `coefficients()`. This aligns perfectly with the commit message and represents a simple spelling fix. There's no indication of a bug fix in terms of logic, error handling, or exception handling.

**Buggy**
"
weka,13928.json,83d877a74c163b3242c845bd9ecc4d2857ef2fa1,"@@ -1,33 +1,37 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     int [] arrIdc = new int[m_BaseClassifiers.length+1];
     arrIdc[m_BaseClassifiers.length]=m_MetaFormat.numAttributes()-1;
     double [] classProbs = new double[m_BaseFormat.numClasses()];
     Instance newInst;
     double sum=0;
 
     for (int i = 0; i<m_MetaClassifiers.length; i++) {
       for (int j = 0; j<m_BaseClassifiers.length; j++)
           arrIdc[j]=m_BaseFormat.numClasses()*j+i;
 
-      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_makeIndicatorFilter.setAttributeIndex("""" + (m_MetaFormat.classIndex() + 1));
+      m_makeIndicatorFilter.setNumeric(true);
       m_makeIndicatorFilter.setValueIndex(i);
-
+      m_makeIndicatorFilter.setInputFormat(m_MetaFormat);
       m_makeIndicatorFilter.input(metaInstance(instance));
       m_makeIndicatorFilter.batchFinished();
       newInst = m_makeIndicatorFilter.output();
+
+      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_attrFilter.setInvertSelection(true);
       m_attrFilter.setInputFormat(m_makeIndicatorFilter.getOutputFormat());
       m_attrFilter.input(newInst);
       m_attrFilter.batchFinished();
       newInst = m_attrFilter.output();
 
       classProbs[i]=m_MetaClassifiers[i].classifyInstance(newInst);
       if (classProbs[i]>1) { classProbs[i]=1; }
       if (classProbs[i]<0) { classProbs[i]=0; }
       sum+= classProbs[i];
     }
 
     if (sum!=0) Utils.normalize(classProbs,sum);
 
     return classProbs;
   }",Buggy,"Fixed problem resulting from changing Range.java


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1815 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed problem resulting from changing Range.java"" indicates a bug fix related to changes in `Range.java`. The diff shows modifications to the `distributionForInstance` method. These modifications involve setting up and using `m_makeIndicatorFilter` and `m_attrFilter` to process instances. Specifically, the code now sets the attribute index for `m_makeIndicatorFilter`, sets it to numeric, sets the value index, and sets the input format. Additionally, the code now sets `m_attrFilter` to invert selection. These changes suggest an attempt to correct how attributes are being filtered and processed, which is a common area for bugs. The checks `if (classProbs[i]>1) { classProbs[i]=1; }` and `if (classProbs[i]<0) { classProbs[i]=0; }` also indicate that the code is handling potential out-of-range values, which is often a sign of bug fixing.

**Buggy**
"
weka,18243.json,5e2b216f64dbf04497ebd70c8db41a5f21e4ae37,"@@ -1,83 +1,86 @@
   protected void setInstancesFromDBaseQuery() {
 
     try {
       if (m_InstanceQuery == null) {
 	m_InstanceQuery = new InstanceQuery();
       }
       String dbaseURL = m_InstanceQuery.getDatabaseURL();
       String username = m_InstanceQuery.getUsername();
       String passwd = m_InstanceQuery.getPassword();
       /*dbaseURL = (String) JOptionPane.showInputDialog(this,
 					     ""Enter the database URL"",
 					     ""Query Database"",
 					     JOptionPane.PLAIN_MESSAGE,
 					     null,
 					     null,
 					     dbaseURL);*/
      
       
       
       DatabaseConnectionDialog dbd= new DatabaseConnectionDialog(null,dbaseURL,username);
       dbd.setVisible(true);
       
       //if (dbaseURL == null) {
       if (dbd.getReturnValue()==JOptionPane.CLOSED_OPTION) {
 	m_FromLab.setText(""Cancelled"");
 	return;
       }
       dbaseURL=dbd.getURL();
       username=dbd.getUsername();
       passwd=dbd.getPassword();
       m_InstanceQuery.setDatabaseURL(dbaseURL);
       m_InstanceQuery.setUsername(username);
       m_InstanceQuery.setPassword(passwd);
       m_InstanceQuery.setDebug(dbd.getDebug());
       
       m_InstanceQuery.connectToDatabase();
       if (!m_InstanceQuery.experimentIndexExists()) {
 	System.err.println(""not found"");
 	m_FromLab.setText(""No experiment index"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       System.err.println(""found"");
       m_FromLab.setText(""Getting experiment index"");
       Instances index = m_InstanceQuery.retrieveInstances(""SELECT * FROM ""
 				       + InstanceQuery.EXP_INDEX_TABLE);
       if (index.numInstances() == 0) {
 	m_FromLab.setText(""No experiments available"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;	
       }
       m_FromLab.setText(""Got experiment index"");
 
       DefaultListModel lm = new DefaultListModel();
       for (int i = 0; i < index.numInstances(); i++) {
 	lm.addElement(index.instance(i).toString());
       }
       JList jl = new JList(lm);
       jl.setSelectedIndex(0);
       int result;
       // display dialog only if there's not just one result!
       if (jl.getModel().getSize() != 1) {
         ListSelectorDialog jd = new ListSelectorDialog(null, jl);
         result = jd.showDialog();
       }
       else {
         result = ListSelectorDialog.APPROVE_OPTION;
       }
       if (result != ListSelectorDialog.APPROVE_OPTION) {
 	m_FromLab.setText(""Cancelled"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       Instance selInst = index.instance(jl.getSelectedIndex());
       Attribute tableAttr = index.attribute(InstanceQuery.EXP_RESULT_COL);
       String table = InstanceQuery.EXP_RESULT_PREFIX
 	+ selInst.toString(tableAttr);
       setInstancesFromDatabaseTable(table);
       
     } catch (Exception ex) {
        // 1. print complete stacktrace
        ex.printStackTrace();
        // 2. print message in panel
        m_FromLab.setText(""Problem reading database: '"" + ex.getMessage() + ""'"");
     }
   }",Buggy,"Fixed a couple of bugs where database connection was not being closed if close/cancel was pressed. Thanks to Miryam Gomez and Raquel Porras for the bug fix


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4243 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where the database connection was not being closed when the user pressed close or cancel. The diff shows that `m_InstanceQuery.disconnectFromDatabase()` is called before returning in several places: when the experiment index is not found, when there are no experiments available, and when the user cancels the list selection dialog. These additions ensure that the database connection is closed in these scenarios, which aligns with the commit message.

**Buggy**"
weka,19592.json,54c6ebfe7355bada5df3972c74469299c98fc417,"@@ -1,80 +1,82 @@
   protected void updateChart(double [] dataPoint) {
-    int iwidth = m_plotPanel.getWidth();
-    int iheight = m_plotPanel.getHeight();
+    //    int iwidth = m_plotPanel.getWidth();
+    //    int iheight = m_plotPanel.getHeight();
+    int iwidth = m_osi.getWidth(this);
+    int iheight = m_osi.getHeight(this);
 
     //    System.err.println(dataPoint[0]);
     if (dataPoint.length-1 != m_previousY.length) {
       m_previousY = new double [dataPoint.length-1];
       //      m_plotCount = 0;
       for (int i = 0; i < dataPoint.length-1; i++) {
 	m_previousY[i] = convertToPanelY(0);
       }
     }
 
     Graphics osg = m_osi.getGraphics();
     
     Graphics g = m_plotPanel.getGraphics();
 
     // paint the old scale onto the plot if a scale update has occured
     if (m_yScaleUpdate) {
       String maxVal = numToString(m_oldMax);
       String minVal = numToString(m_oldMin);
       String midVal = numToString((m_oldMax - m_oldMin) / 2.0);
       if (m_labelMetrics == null) {
 	m_labelMetrics = g.getFontMetrics(m_labelFont);
       }
       osg.setFont(m_labelFont);
       int wmx = m_labelMetrics.stringWidth(maxVal);
       int wmn = m_labelMetrics.stringWidth(minVal);
       int wmd = m_labelMetrics.stringWidth(midVal);
 
       int hf = m_labelMetrics.getAscent();
       osg.setColor(m_colorList[m_colorList.length-1]);
       osg.drawString(maxVal, iwidth-wmx, hf-2);
       osg.drawString(midVal, iwidth-wmd, (iheight / 2)+(hf / 2));
       osg.drawString(minVal, iwidth-wmn, iheight-1);
       m_yScaleUpdate = false;
       System.err.println(""Here"");
     }
 
     osg.copyArea(m_refreshWidth,0,iwidth-m_refreshWidth,
 		 iheight,-m_refreshWidth,0);
     osg.setColor(Color.black);
     osg.fillRect(iwidth-m_refreshWidth,0, iwidth, iheight);
 
     double pos;
     for (int i = 0; i < dataPoint.length-1; i++) {
       osg.setColor(m_colorList[(i % m_colorList.length)]);
       pos = convertToPanelY(dataPoint[i]);
       osg.drawLine(iwidth-m_refreshWidth, (int)m_previousY[i], 
 		   iwidth-1, (int)pos);
       m_previousY[i] = pos;
       if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 	// draw the actual y value onto the plot for this curve
 	String val = numToString(dataPoint[i]);
 	if (m_labelMetrics == null) {
 	  m_labelMetrics = g.getFontMetrics(m_labelFont);
 	}
 	int hf = m_labelMetrics.getAscent();
 	if (pos - hf < 0) {
 	  pos += hf;
 	}
 	int w = m_labelMetrics.stringWidth(val);
 	osg.setFont(m_labelFont);
 	osg.drawString(val, iwidth-w, (int)pos);
       }
     }
     
     // last element in the data point array contains the data point number
     if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 
       String xVal = """"+(int)dataPoint[dataPoint.length-1];
       osg.setColor(m_colorList[m_colorList.length-1]);
       int w = m_labelMetrics.stringWidth(xVal);
       osg.setFont(m_labelFont);
       osg.drawString(xVal, iwidth-w, iheight - 1);
     }
     g.drawImage(m_osi,0,0,m_plotPanel);
     //    System.err.println(""Finished"");
     //    m_plotCount++;
   }",Buggy,"Fixed scrolling problem under java 1.4 on Linux


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1484 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a scrolling problem under Java 1.4 on Linux. The code changes involve replacing `m_plotPanel.getWidth()` and `m_plotPanel.getHeight()` with `m_osi.getWidth(this)` and `m_osi.getHeight(this)`, respectively. This suggests that the original method of obtaining the width and height of the plot panel was not working correctly under the specified conditions, leading to the scrolling issue. The new method likely provides a more reliable way to get the dimensions, thus resolving the problem.

**Buggy**
"
weka,30975.json,32ee55c07bd6bb0d5afd27817fc05d405bd35c4f,"@@ -1,50 +1,67 @@
   public void writeIncremental(Instance inst) throws IOException {
     int writeMode = getWriteMode();
     Instances structure = getInstances();
 
     if (getRetrieval() == BATCH || getRetrieval() == NONE) {
       throw new IOException(""Batch and incremental saving cannot be mixed."");
     }
 
     if (writeMode == WAIT) {
       if (structure == null) {
         setWriteMode(CANCEL);
         if (inst != null) {
-          throw new IOException(
-            ""Structure (header Information) has to be set "" + ""in advance"");
+          throw new IOException(""Structure (header Information) has to be set ""
+            + ""in advance"");
         }
       } else {
-        m_dictionaryBuilder.reset();
-        try {
-          m_dictionaryBuilder.setup(structure);
-        } catch (Exception ex) {
-          throw new IOException(ex);
-        }
-        setWriteMode(WRITE);
+        setWriteMode(STRUCTURE_READY);
       }
       writeMode = getWriteMode();
     }
+    if (writeMode == CANCEL) {
+      cancel();
+    }
+
+    if (writeMode == STRUCTURE_READY) {
+      m_dictionaryBuilder.reset();
+      try {
+        m_dictionaryBuilder.setup(structure);
+      } catch (Exception ex) {
+        throw new IOException(ex);
+      }
+      setWriteMode(WRITE);
+      writeMode = getWriteMode();
+    }
 
     if (writeMode == WRITE) {
       if (structure == null) {
         throw new IOException(""No instances information available."");
       }
 
       if (inst != null) {
         m_dictionaryBuilder.processInstance(inst);
       } else {
+        try {
+          m_dictionaryBuilder.finalizeDictionary();
+        } catch (Exception e) {
+          throw new IOException(e);
+        }
         if (retrieveFile() == null && getWriter() == null) {
           if (getSaveBinaryDictionary()) {
             throw new IOException(
               ""Can't output binary dictionary to standard out!"");
           }
           m_dictionaryBuilder.saveDictionary(System.out);
         } else {
-          m_dictionaryBuilder.saveDictionary(getWriter());
+          if (getSaveBinaryDictionary()) {
+            m_dictionaryBuilder.saveDictionary(m_binaryStream);
+          } else {
+            m_dictionaryBuilder.saveDictionary(getWriter());
+          }
         }
 
         resetStructure();
         resetWriter();
       }
     }
   }",Buggy,"Fixed a bug in incremental mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in incremental mode. The code changes involve modifications to the `writeIncremental` method, specifically addressing the handling of the structure (header information) and dictionary building process.

Here's a breakdown of the changes:

1.  **State Management**: The introduction of `STRUCTURE_READY` state suggests a more controlled flow for setting up the dictionary builder. The code now transitions to this state after checking if the structure is available and before actually setting up the dictionary builder.

2.  **Dictionary Finalization**: The addition of `m_dictionaryBuilder.finalizeDictionary()` when `inst` is null indicates a necessary step to complete the dictionary building process before saving it. This is a strong indicator of a bug fix, as it suggests that the dictionary was not being properly finalized in the previous implementation.

3.  **Binary Dictionary Handling**: The code now checks `getSaveBinaryDictionary()` before saving the dictionary to either `m_binaryStream` or `getWriter()`. This suggests that the binary dictionary saving was not handled correctly before.

These changes collectively point towards a bug fix related to the incremental writing process, specifically in how the dictionary is built, finalized, and saved. The addition of `finalizeDictionary()` and the refined state management are key indicators that the previous implementation had flaws in handling the dictionary building process.

**Buggy**"
weka,20298.json,2de75b28460973901afd12db84cb5288c20a8918,"@@ -1,24 +1,23 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
-
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to tooltips for additional options in plugin renderers. The code diff modifies the `setupRendererOptsTipText` method, which is responsible for setting the tooltip text for the options label.

Here's a breakdown of the changes:

1.  **`PluginManager.getPluginInstance` argument:** The argument to `PluginManager.getPluginInstance` was changed from `""weka.gui.beans.OffscreenChartRender""` to `""weka.gui.beans.OffscreenChartRenderer""`. This suggests a typo in the original code that prevented the correct plugin from being loaded, thus causing the tooltip to not be set correctly.

The change directly addresses the problem described in the commit message. The original code had a typo that prevented the correct plugin from being loaded, leading to an incorrect or missing tooltip. The fix corrects this typo, ensuring the correct plugin is loaded and the tooltip is displayed as intended.

**Buggy**
"
weka,17871.json,e62dd28a920a99053a89d45c44b4520741764624,"@@ -1,91 +1,92 @@
   public static void main (String [] args) {
     try {
       if (args.length < 8) {
 	System.err.println(""Usage : BoundaryPanel <dataset> ""
 			   +""<class col> <xAtt> <yAtt> ""
 			   +""<base> <# loc/pixel> <kernel bandwidth> ""
 			   +""<display width> ""
 			   +""<display height> <classifier ""
 			   +""[classifier options]>"");
 	System.exit(1);
       }
       final javax.swing.JFrame jf = 
 	new javax.swing.JFrame(""Weka classification boundary visualizer"");
       jf.getContentPane().setLayout(new BorderLayout());
 
       System.err.println(""Loading instances from : ""+args[0]);
       java.io.Reader r = new java.io.BufferedReader(
 			 new java.io.FileReader(args[0]));
       final Instances i = new Instances(r);
       i.setClassIndex(Integer.parseInt(args[1]));
 
       //      bv.setClassifier(new Logistic());
       final int xatt = Integer.parseInt(args[2]);
       final int yatt = Integer.parseInt(args[3]);
       int base = Integer.parseInt(args[4]);
       int loc = Integer.parseInt(args[5]);
 
       int bandWidth = Integer.parseInt(args[6]);
       int panelWidth = Integer.parseInt(args[7]);
       int panelHeight = Integer.parseInt(args[8]);
 
       final String classifierName = args[9];
       final BoundaryPanel bv = new BoundaryPanel(panelWidth,panelHeight);
       bv.addActionListener(new ActionListener() {
 	  public void actionPerformed(ActionEvent e) {
 	    String classifierNameNew = 
 	      classifierName.substring(classifierName.lastIndexOf('.')+1, 
 				       classifierName.length());
 	    bv.saveImage(classifierNameNew+""_""+i.relationName()
 			 +""_X""+xatt+""_Y""+yatt+"".jpg"");
 	  }
 	});
 
       jf.getContentPane().add(bv, BorderLayout.CENTER);
       jf.setSize(bv.getMinimumSize());
       //      jf.setSize(200,200);
       jf.addWindowListener(new java.awt.event.WindowAdapter() {
 	  public void windowClosing(java.awt.event.WindowEvent e) {
 	    jf.dispose();
 	    System.exit(0);
 	  }
 	});
 
       jf.pack();
       jf.setVisible(true);
       //      bv.initialize();
       bv.repaint();
       
 
       String [] argsR = null;
       if (args.length > 10) {
+	System.err.println(""""+(args.length-10));
 	argsR = new String [args.length-10];
-	for (int j = 9; j < args.length; j++) {
+	for (int j = 10; j < args.length; j++) {
 	  argsR[j-10] = args[j];
 	}
       }
       Classifier c = Classifier.forName(args[9], argsR);
       KDDataGenerator dataGen = new KDDataGenerator();
       dataGen.setKernelBandwidth(bandWidth);
       bv.setDataGenerator(dataGen);
       bv.setNumSamplesPerRegion(loc);
       bv.setGeneratorSamplesBase(base);
       bv.setClassifier((DistributionClassifier)c);
       bv.setTrainingData(i);
       bv.setXAttribute(xatt);
       bv.setYAttribute(yatt);
 
       try {
 	// try and load a color map if one exists
 	FileInputStream fis = new FileInputStream(""colors.ser"");
 	ObjectInputStream ois = new ObjectInputStream(fis);
 	FastVector colors = (FastVector)ois.readObject();
 	bv.setColors(colors);	
       } catch (Exception ex) {
 	System.err.println(""No color map file"");
       }
       bv.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed bug in main method


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the main method.

The diff shows a change in the loop that populates the `argsR` array. The original loop started at index 9, while the corrected loop starts at index 10. Also, a print statement was added to print the length of the `argsR` array. This suggests that the original loop was likely causing an `ArrayIndexOutOfBoundsException` or incorrect argument parsing, which is a bug.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,30018.json,044bb44ae261d7619c1f9d9616f4e996330d7043,"@@ -1,77 +1,75 @@
   protected List<String> checkForNativeLibs(Package toLoad, File packageDir) {
     List<String> jarsForClassloaderToIgnore = new ArrayList<>();
 
     if (toLoad.getPackageMetaDataElement(""NativeLibs"") != null) {
       String nativeLibs =
         toLoad.getPackageMetaDataElement(""NativeLibs"").toString();
       if (nativeLibs.length() > 0) {
         String[] jarsWithLibs = nativeLibs.split("";"");
         for (String entry : jarsWithLibs) {
           String[] jarAndEntries = entry.split("":"");
           if (jarAndEntries.length != 2) {
             System.err
               .println(""Was expecting two entries for native lib spec - ""
                 + ""jar:comma-separated lib paths"");
             continue;
           }
           String jarPath = jarAndEntries[0].trim();
           String[] libPathsInJar = jarAndEntries[1].split("","");
           List<String> libsToInstall = new ArrayList<>();
-          List<String> libsToAddToPath = new ArrayList<>();
           // look at named libs and check if they are already in
           // $WEKA_HOME/native-libs - don't extract a second time, but DO
           // add entries to java.library.path
           for (String lib : libPathsInJar) {
             String libName = lib.trim().replace(""\\"", ""/"");
             if (!nativeLibInstalled(libName.substring(
               libName.lastIndexOf(""/"") + 1, libName.length()))) {
-              libsToInstall.add(libName);
+              libsToInstall.add(libName.substring(
+                libName.lastIndexOf(""/"") + 1, libName.length()));
             }
-            libsToAddToPath.add(libName.substring(libName.lastIndexOf(""/"") + 1,
-              libName.length()));
           }
 
           if (libsToInstall.size() > 0) {
             try {
               installNativeLibs(packageDir, jarPath, libsToInstall);
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
           /*
            * if (libsToAddToPath.size() > 0) {
            * addNativeLibsToLibsProp(libsToAddToPath); }
            */
         }
       }
     }
 
     // now check to see if there is a native loader to inject into the
     // root class loader
     if (toLoad.getPackageMetaDataElement(""InjectLoader"") != null) {
       String injectDetails =
         toLoad.getPackageMetaDataElement(""InjectLoader"").toString();
       String[] entries = injectDetails.split("";"");
 
       for (String entry : entries) {
         String jarPath = entry.trim();
         boolean rootClassLoader = false;
         if (jarPath.startsWith(""root|"")) {
           jarPath = jarPath.replace(""root|"", """");
           rootClassLoader = true;
         }
         String ignoreJar = jarPath.replace(""\\"", ""/"");
         ignoreJar = ignoreJar.substring(ignoreJar.lastIndexOf(""/"") + 1);
 
         jarsForClassloaderToIgnore.add(ignoreJar);
         try {
           WekaPackageClassLoaderManager.injectAllClassesInJar(new File(
             packageDir.toString() + File.separator + jarPath.trim()));
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
     }
 
     return jarsForClassloaderToIgnore;
   }",Buggy,"Fixed a bug in the native lib installation routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix in the native library installation routine.

The code diff modifies the `checkForNativeLibs` method, specifically within the loop that iterates through native libraries specified in the package metadata.

The key change is within the `libsToInstall.add(libName)` line. The original code added the full path of the library (`libName`) to the `libsToInstall` list. The modified code now adds only the library name (extracted using `libName.substring(libName.lastIndexOf(""/"") + 1, libName.length())`). This suggests that the installation routine was previously using the full path instead of just the library name, which could lead to incorrect installation or lookup of the native libraries.

The removal of `libsToAddToPath` and the commented-out block related to it further supports the idea that the original logic was flawed. The code now correctly extracts and installs the native libraries.

The `try-catch` blocks around `installNativeLibs` and `WekaPackageClassLoaderManager.injectAllClassesInJar` suggest that the code is handling potential exceptions during the installation and injection processes, which is good practice.

Based on the commit message and the code changes, it's clear that the commit addresses a bug in how native libraries were being installed. The change ensures that only the library name is used during installation, which likely resolves an issue with incorrect library paths.

**Buggy**"
weka,3629.json,34ee50c28e6d5f6dc2ea9bff29e6d09545fb601e,"@@ -1,38 +1,38 @@
   public void start() throws WekaException {
     if (getStepManager().numIncomingConnections() == 0) {
       getStepManager().processing();
       ClassLoader orig = Thread.currentThread().getContextClassLoader();
       try {
         String jobName = ""WekaKF:"" + m_runningJob.getJobName();
         Thread.currentThread()
           .setContextClassLoader(this.getClass().getClassLoader());
 
         List<StepManager> outConns =
           getStepManager().getOutgoingConnections().get(""success"");
         if (outConns != null) {
           for (StepManager manager : outConns) {
             jobName += ""+"" + ((AbstractSparkJob) ((StepManagerImpl) manager)
               .getManagedStep()).getUnderlyingJob().getJobName();
           }
         }
         m_runningJob.setJobName(jobName);
         getStepManager()
           .logBasic(""Starting Spark job as start point: "" + jobName);
 
         // we are a start point. Assumption is that we're the *only* start point
         // as things will break down if there are more than one.
         try {
           m_sparkLogAppender = m_runningJob.initJob(null);
         } catch (Exception ex) {
-          ex.printStackTrace();
+          m_runningJob = null;
           throw new WekaException(ex);
         }
         m_currentContext = m_runningJob.getSparkContext();
         m_cachingStrategy = m_runningJob.getCachingStrategy();
         m_outputDirectory = m_runningJob.getSparkJobConfig().getOutputDir();
       } finally {
         Thread.currentThread().setContextClassLoader(orig);
       }
       runJob();
     }
   }",Buggy,"Fixed a bug that prevented spark step options from being altered after a job had been run and suffered a failure

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12960 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that prevented spark step options from being altered after a job failure. The diff shows a change in the `start()` method where an exception during `m_runningJob.initJob(null)` now sets `m_runningJob = null` before throwing a `WekaException`. This suggests that the previous state of `m_runningJob` after a failed initialization was causing issues with subsequent modifications of spark step options, aligning with the commit message's description of the bug.

**Buggy**
"
weka,29409.json,c06f15670aa5dc47487a677160a087b8ab2d869a,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.1 $"");
+    return RevisionUtils.extract(""$Revision$"");
   }",Buggy,"Fixed a bug introduced by the improvements in derived fields where field definitions for inputs were not being set correctly.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5028 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix related to derived fields and input field definitions. The provided diff only changes the revision string. This change doesn't seem to be directly related to fixing a bug in derived fields or input field definitions. It's more of a metadata update.

**NotBuggy**"
weka,25760.json,b63980e747df9f6abe802dfdf83f00c171dec0ea,"@@ -1,17 +1,25 @@
     protected double[][] getCoefficients(){
 	double[][] coefficients = new double[m_numClasses][m_numericDataHeader.numAttributes() + 1];
 	for (int j = 0; j < m_numClasses; j++) {
 	    //go through simple regression functions and add their coefficient to the coefficient of
 	    //the attribute they are built on.
 	    for (int i = 0; i < m_numRegressions; i++) {
 		
 		double slope = m_regressions[j][i].getSlope();
 		double intercept = m_regressions[j][i].getIntercept();
 		int attribute = m_regressions[j][i].getAttributeIndex();
 		
 		coefficients[j][0] += intercept;
 		coefficients[j][attribute + 1] += slope;
 	    }
 	}
+        
+        // Need to multiply all coefficients by (J-1) / J
+        for (int j = 0; j < coefficients.length; j++) {
+          for (int i = 0; i < coefficients[0].length; i++) {
+            coefficients[j][i] *= (double)(m_numClasses - 1) / (double)m_numClasses;
+          }
+        }
+
 	return coefficients;
     }",Buggy,"Fixed bug in output of coefficients and intercept: they needed to be multiplied by (J-1)/J, where J is the number of classes. This affected LMT and SimpleLogistic.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3930 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the output of coefficients and intercept in LMT and SimpleLogistic. The fix involves multiplying the coefficients and intercept by (J-1)/J, where J is the number of classes.

The code diff shows that a loop has been added to iterate through the `coefficients` array and multiply each element by `(double)(m_numClasses - 1) / (double)m_numClasses`. This aligns perfectly with the description in the commit message. The added code directly addresses the bug described in the commit message.

**Buggy**"
weka,10869.json,d7b954bf6ad31fcb8c1e3fd0ed630c511674efa0,"@@ -1,103 +1,105 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         Matrix khatM = new UpperSymmDenseMatrix(m);
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khatM.set(i, j, m_Kernel.eval(i, j, m_Sample.instance(i)));
             }
         }
         m_Kernel.clean();
 
         if (m_Debug) {
             Matrix kbM = new DenseMatrix(n, m);
             for (int i = 0; i < n; i++) {
                 for (int j = 0; j < m; j++) {
                     kbM.set(i, j, m_Kernel.eval(-1, j, inputFormat.instance(i)));
                 }
             }
 
             // Calculate SVD of kernel matrix
             SVD svd = SVD.factorize(khatM);
 
             double[] singularValues = svd.getS();
             Matrix sigmaI = new UpperSymmDenseMatrix(m);
             for (int i = 0; i < singularValues.length; i++) {
                 if (singularValues[i] > SMALL) {
                     sigmaI.set(i, i, 1.0 / singularValues[i]);
                 }
             }
 
             System.err.println(""U :\n"" + svd.getU());
             System.err.println(""Vt :\n"" + svd.getVt());
             System.err.println(""Reciprocal of singular values :\n"" + sigmaI);
 
             Matrix pseudoInverse = svd.getU().mult(sigmaI, new DenseMatrix(m,m)).mult(svd.getVt(), new DenseMatrix(m,m));
 
             // Compute reduced-rank version
             Matrix khatr = kbM.mult(pseudoInverse, new DenseMatrix(n, m)).mult(kbM.transpose(new DenseMatrix(m, n)), new DenseMatrix(n,n));
 
             System.err.println(""Reduced rank matrix: \n"" + khatr);
         }
 
         // Compute weighting matrix
         if (getUseSVD()) {
             SVD svd = SVD.factorize(khatM);
             double[] e = svd.getS();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""U matrix :\n"" + svd.getU());
                 System.err.println(""Vt matrix :\n"" + svd.getVt());
                 System.err.println(""Singluar values \n"" + Utils.arrayToString(svd.getS()));
                 System.err.println(""Reciprocal of square root of singular values :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(svd.getVt(), new DenseMatrix(m,m));
         } else {
 
             SymmDenseEVD evd = SymmDenseEVD.factorize(khatM);
             double[] e = evd.getEigenvalues();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""Eigenvector matrix :\n"" + evd.getEigenvectors());
                 System.err.println(""Eigenvalues \n"" + Utils.arrayToString(evd.getEigenvalues()));
                 System.err.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(evd.getEigenvectors().transpose(), new DenseMatrix(m,m));
         }
 
         if (m_Debug) {
             System.err.println(""Weighting matrix: \n"" + m_WeightingMatrix);
         }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances(inputFormat.relationName(), atts, 0);
-        d.setClassIndex(d.numAttributes() - 1);
+        if (hasClass) {
+          d.setClassIndex(d.numAttributes() - 1);
+        }
         return d;
     }",Buggy,"Bug fix: class index in output was set to last attribute even if input data did not have class.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14365 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the class index in the output format. Specifically, the class index was being set to the last attribute even when the input data did not have a class attribute.

The code diff shows a modification within the `determineOutputFormat` method. The key change is the addition of a conditional statement around setting the class index:

```java
-        d.setClassIndex(d.numAttributes() - 1);
+        if (hasClass) {
+          d.setClassIndex(d.numAttributes() - 1);
+        }
```

This change ensures that the class index is only set if the input format actually has a class attribute (`hasClass` is true). This aligns perfectly with the commit message, indicating that the bug where the class index was incorrectly set has been fixed.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,10869.json,ff769b98dbd0a017a91ca677f90dc3750b239511,"@@ -1,67 +1,87 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         double[][] khat = new double[m][m];
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khat[i][j] = m_Kernel.eval(i, j, m_Sample.instance(i));
                 khat[j][i] = khat[i][j];
             }
         }
         Matrix khatM = new Matrix(khat);
-        /*double[][] kb = new double[m][n];
-        for (int i = 0; i < m; i++) {
-            for (int j = i; j < n; j++) {
-                kb[i][j] = m_Kernel.eval(-1, i, inputFormat.instance(i));
+
+        if (m_Debug) {
+            double[][] kb = new double[n][m];
+            for (int i = 0; i < n; i++) {
+                for (int j = 0; j < m; j++) {
+                    kb[i][j] = m_Kernel.eval(-1, j, inputFormat.instance(i));
+                }
             }
-        }
-        Matrix kbM = new Matrix(kb).transpose();*/
+            Matrix kbM = new Matrix(kb);
 
-        // Calculate SVD of kernel matrix
-        SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
+            // Calculate SVD of kernel matrix
+            SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
 
-        double[] singularValues = svd.getSingularValues();
-        Matrix sigmaI = new Matrix(m,m);
-        for (int i = 0; i < singularValues.length; i++) {
-            sigmaI.set(i, i, 1.0 / singularValues[i]);
+            double[] singularValues = svd.getSingularValues();
+            Matrix sigmaI = new Matrix(m, m);
+            for (int i = 0; i < singularValues.length; i++) {
+                if (singularValues[i] > 1e-6) {
+                    sigmaI.set(i, i, 1.0 / singularValues[i]);
+                }
+            }
+
+            System.out.println(""U :\n"" + svd.getU());
+            System.out.println(""V :\n"" + svd.getV());
+            System.out.println(""Reciprocal of singular values :\n"" + sigmaI);
+
+            Matrix pseudoInverse = svd.getV().times(sigmaI).times(svd.getU().transpose());
+
+            // Compute reduced-rank version
+            Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
+
+            System.out.println(""Reduced rank matrix: \n"" + khatr);
         }
 
-        m_WeightingMatrix = sigmaI.times(svd.getV().transpose());
-
-
-        /* Matrix pseudoInverse = svd.getV().transpose().times(sigmaI).times(svd.getU().transpose());
-
-        // Compute reduced-rank version
-        Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
-
-        // Get eigenvalues and eigenvectors of reduced-rank matrix
-        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatr);
+        // Get eigenvalues and eigenvectors
+        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatM);
         double[] e = evd.getRealEigenvalues();
         Matrix dhatr = new Matrix(e.length, e.length);
         for (int i  = 0; i < e.length; i++) {
-            dhatr.set(i, i, 1./Math.sqrt(e[i]));
+            dhatr.set(i, i, 1.0/Math.sqrt(e[i]));
         }
-        m_WeightingMatrix = dhatr.times(evd.getV()); */
+        if (m_Debug) {
+            System.out.println(""Eigenvector matrix :\n"" + evd.getV());
+            System.out.println(""Eigenvalue matrix \n"" + evd.getD());
+            System.out.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
+        }
+
+        //System.out.println(""Reconstructed matrix: \n"" + evd.getV().times(evd.getD()).times(evd.getV().inverse())
+
+        m_WeightingMatrix = dhatr.times(evd.getV().transpose());
+
+        if (m_Debug) {
+            System.out.println(""Weighting matrix: \n"" + m_WeightingMatrix);
+        }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances("""", atts, 0);
         d.setClassIndex(d.numAttributes() - 1);
         return d;
     }",Buggy,"Several bug fixes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12516 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Several bug fixes"" suggests that the changes are related to fixing bugs.

The code diff includes the following changes:

1.  Addition of a debug mode (`m_Debug`) that prints intermediate matrices and values.
2.  A check for singular values greater than a small threshold (`1e-6`) before inverting them. This is a common practice to avoid division by zero or very small numbers, which can lead to numerical instability.
3.  The original code had commented-out sections related to matrix calculations. The new code calculates the weighting matrix differently.
4.  The eigenvector matrix is transposed when calculating the weighting matrix.

The addition of the singular value check and the changes in matrix calculations strongly suggest that the original code had numerical stability issues or was producing incorrect results. These changes are likely bug fixes.

Therefore, the final answer is: **Buggy**"
weka,18657.json,c9a6740cbebd93b5978b7e993e314995eb4db94f,"@@ -1,17 +1,27 @@
   public void insertInstance(int index, boolean notify) {
     if (!m_IgnoreChanges) {
       addUndoPoint();
     }
     double[] vals = new double[m_Data.numAttributes()];
+
+    // set any string or relational attribute values to missing
+    // in the new instance, just in case this is the very first
+    // instance in the dataset.
+    for (int i = 0; i < m_Data.numAttributes(); i++) {
+      if (m_Data.attribute(i).isString()
+        || m_Data.attribute(i).isRelationValued()) {
+        vals[i] = Utils.missingValue();
+      }
+    }
     Instance toAdd = new DenseInstance(1.0, vals);
     if (index < 0) {
       m_Data.add(toAdd);
     } else {
       m_Data.add(index, toAdd);
     }
     if (notify) {
       notifyListener(new TableModelEvent(this, m_Data.numInstances() - 1,
         m_Data.numInstances() - 1, TableModelEvent.ALL_COLUMNS,
         TableModelEvent.INSERT));
     }
   }",Buggy,"Fixed a bug in the insertion of new instances. Now sets the value of relational and string attributes in the new instance to missing, just in case the new instance is the very first one in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12708 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug related to the insertion of new instances, specifically when the new instance is the first one in the dataset. The code diff shows that the values of relational and string attributes in the new instance are now being set to missing. This aligns with the commit message, as it addresses a potential issue where these attributes might not be properly initialized when the instance is the first one in the dataset. The added code iterates through the attributes and sets the value to missing if the attribute is either string or relation valued. This prevents potential issues when the new instance is the first one.

**Buggy**
"
weka,33458.json,b7e34d731ed42ab40ad353a159d93fb4247a9bc9,"@@ -1,24 +1,26 @@
   public String substitute(String source) throws Exception {
     // Grab each variable out of the string
     int index = source.indexOf(""${"");
 
     while (index >= 0) {
       index += 2;
       int endIndex = source.indexOf('}');
       if (endIndex >= 0 && endIndex > index +1) {
         String key = source.substring(index, endIndex);
 
         // look this sucker up
         String replace = m_envVars.get(key);
         if (replace != null) {
           String toReplace = ""${"" + key + ""}"";
           source = source.replace(toReplace, replace);
         } else {
           throw new Exception(""[Environment] Variable "" 
                               + key + "" doesn't seem to be set."");
         }
+      } else {
+        break;
       }
       index = source.indexOf(""${"");
     }
     return source;
   }",Buggy,"Fixed a bug that caused an infinite loop in substitute(). Now uses a TreeMap instead of a HashMap so that keys are kept in sorted order.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to an infinite loop in the `substitute()` method. It also mentions the use of `TreeMap` instead of `HashMap` for sorted keys.

The code diff shows a modification within the `while` loop condition. Specifically, an `else` block is added to handle the case where `endIndex` is not valid (either negative or not greater than `index + 1`). In this case, the loop is terminated using `break`. This addition directly addresses the potential for an infinite loop if the closing curly brace '}' is missing or misplaced in the input string.

The change directly relates to preventing an infinite loop, as stated in the commit message. The use of `TreeMap` isn't reflected in this diff, but the core bug fix is present.

**Buggy**
"
weka,24845.json,ceb6ad5f19eef0a321d4c4ad851a2bba76e1ebb0,"@@ -1,5 +1,4 @@
   public String globalInfo() {
     return ""Hierarchical clustering class.\n""
-      + ""Implements a number of classic agglomorative (i.e. bottom up) hierarchical clustering methods""
-      + ""based on ."";
+      + ""Implements a number of classic agglomerative (i.e., bottom up) hierarchical clustering methods."";
   }",Buggy,"Fixed bug in globalInfo() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13174 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states ""Fixed bug in globalInfo() method."" The diff shows a change in the `globalInfo()` method's return string. Specifically, it corrects ""agglomorative"" to ""agglomerative"" and replaces ""based on ."" with ""based on hierarchical clustering methods."". This indicates a correction of a typo and a grammatical improvement in the documentation string. While it's a minor correction, it aligns with the commit message's claim of fixing a bug (in the sense of an error) within the `globalInfo()` method.

Reasoning:
1. **Commit Message:** The commit message explicitly mentions a bug fix in the `globalInfo()` method.
2. **Code Diff:** The code diff shows a modification to the string returned by the `globalInfo()` method. The changes involve correcting a typo (""agglomorative"" to ""agglomerative"") and improving the sentence structure.
3. **Alignment:** The code change directly addresses the claim in the commit message. The typo correction and grammatical improvement can be considered a fix to an error (a bug) in the documentation.

Conclusion: **Buggy**"
weka,15454.json,be743132c9f71754ed911dce8ed0a797b9314166,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
-        ""\tNew field specification (name@type@value).\n""
+        ""\tNew field specification (name@type:value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
         ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed a bug in the help info for the -A option

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed a bug in the help info for the -A option"" suggests a correction to the help information displayed for the `-A` option.

The diff shows a change in the string used to describe the `-A` option. Specifically, `@type@value` has been changed to `@type:value`. This change directly relates to the help information for the `-A` option and appears to correct an error in the format specification. The original format used `@` as a separator between type and value, while the corrected format uses `:`. This indicates a bug fix in the way the help information was being displayed or interpreted.

Therefore, the changes indicate a bug fix.
**Buggy**
"
weka,15454.json,611c95b4ddf959002e0360d9c109a532143b3d2c,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
         ""\tNew field specification (name@type@value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
-        ""-A <name:type:value>""));
+        ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed an error in the listOptions output

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12731 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed an error in the listOptions output"" suggests a bug fix related to the output of the `listOptions` method.

The diff shows a change in the string used to describe the `-A` option. Specifically, the separator between name, type, and value has been changed from "":"" to ""@"" in the help text. This indicates that the previous output was incorrect and has been corrected. This aligns with the commit message indicating a fix.

Therefore, the change represents a bug fix.

**Buggy**"
weka,26007.json,6fcd46acab99a453150b3418dc7da518be7033d2,"@@ -1,43 +1,43 @@
   private String ruleToString() {
     StringBuffer text = new StringBuffer();
 
     if (m_splitAtts.length > 0) {
       text.append(""IF\n"");
 
       for (int i = m_splitAtts.length - 1; i >= 0; i--) {
 	text.append(""\t"" + m_covered.attribute(m_splitAtts[i]).name() + "" "");
 
 	if (m_relOps[i] == 0) {
 	  text.append(""<= "");
 	} else {
 	  text.append(""> "");
 	} 
 
 	text.append(Utils.doubleToString(m_splitVals[i], 1, 3) + ""\n"");
       } 
 
       text.append(""THEN\n"");
     } 
 
     if (m_ruleModel != null) {
       try {
 	text.append(m_ruleModel.printNodeLinearModel());
 	text.append("" ["" + m_numCovered/*m_covered.numInstances()*/);
 
 	if (m_globalAbsDev > 0.0) {
 	  text.append(""/""+Utils.doubleToString((100 * 
 						   m_ruleModel.
 						   rootMeanSquaredError() / 
-						   m_globalAbsDev), 1, 3) 
+						   m_globalStdDev), 1, 3) 
 		      + ""%]\n\n"");
 	} else {
 	  text.append(""]\n\n"");
 	} 
       } catch (Exception e) {
 	return ""Can't print rule"";
       } 
     } 
     
     //    System.out.println(m_instances);
     return text.toString();
   }",Buggy,"Fixed bug in rule output. Second number in the brackets at a leaf now reports the rms error as a percentage of the global standard deviation.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the rule output in a decision tree or rule-based model. Specifically, the second number in the brackets at a leaf node was incorrectly reporting the absolute deviation instead of the root mean squared error (RMS) as a percentage of the global standard deviation.

The code diff shows a change in the `ruleToString()` method. The original code used `m_globalAbsDev` in the calculation of the percentage, while the corrected code uses `m_globalStdDev`. This change aligns directly with the commit message, indicating that the RMS error is now being calculated as a percentage of the global standard deviation, which is a statistical measure of the spread of data around the mean.

The original code was likely reporting an incorrect value due to the use of the wrong variable. The change from `m_globalAbsDev` to `m_globalStdDev` suggests a correction of a previously existing error.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,14982.json,e15ee656dfd1f2636611269ded7d3ce754f975c0,"@@ -1,80 +1,90 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     int i, j;
 
     if (!(ASEval instanceof AttributeEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a"" 
 			   + ""Attribute evaluator!"");
     }
-    
-    if (ASEval instanceof AttributeTransformer) {
-      data = ((AttributeTransformer)ASEval).transformedHeader();
-    }
 
     m_numAttribs = data.numAttributes();
 
     if (ASEval instanceof UnsupervisedAttributeEvaluator) {
       m_hasClass = false;
     }
     else {
-      m_hasClass = true;
       m_classIndex = data.classIndex();
+      if (m_classIndex >= 0) {	
+	m_hasClass = true;
+      } else {
+	m_hasClass = false;
+      }
+    }
+
+    // get the transformed data and check to see if the transformer
+    // preserves a class index
+    if (ASEval instanceof AttributeTransformer) {
+      data = ((AttributeTransformer)ASEval).transformedHeader();
+      if (m_classIndex >= 0 && data.classIndex() >= 0) {
+	m_classIndex = data.classIndex();
+	m_hasClass = true;
+      }
     }
 
 
     m_startRange.setUpper(m_numAttribs - 1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     
     int sl=0;
     if (m_starting != null) {
       sl = m_starting.length;
     }
     if ((m_starting != null) && (m_hasClass == true)) {
       // see if the supplied list contains the class index
       boolean ok = false;
       for (i = 0; i < sl; i++) {
 	if (m_starting[i] == m_classIndex) {
 	  ok = true;
 	  break;
 	}
       }
       
       if (ok == false) {
 	sl++;
       }
     }
     else {
       if (m_hasClass == true) {
 	sl++;
       }
     }
 
 
     m_attributeList = new int[m_numAttribs - sl];
     m_attributeMerit = new double[m_numAttribs - sl];
 
     // add in those attributes not in the starting (omit list)
     for (i = 0, j = 0; i < m_numAttribs; i++) {
       if (!inStarting(i)) {
 	m_attributeList[j++] = i;
       }
     }
 
     AttributeEvaluator ASEvaluator = (AttributeEvaluator)ASEval;
 
     for (i = 0; i < m_attributeList.length; i++) {
       m_attributeMerit[i] = ASEvaluator.evaluateAttribute(m_attributeList[i]);
     }
 
     double[][] tempRanked = rankedAttributes();
     int[] rankedAttributes = new int[m_attributeList.length];
 
     for (i = 0; i < m_attributeList.length; i++) {
       rankedAttributes[i] = (int)tempRanked[i][0];
     }
 
     return  rankedAttributes;
   }",Buggy,"Fixed a bug with handling class index when used with attribute transformers


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2023 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to handling the class index, specifically when attribute transformers are involved.

The code diff shows several changes:

1.  **Class Index Handling**: The code now explicitly checks if `data.classIndex()` is non-negative before setting `m_hasClass` to `true`. This suggests a scenario where the class index might be invalid (e.g., -1), and the original code didn't handle it correctly.
2.  **Attribute Transformer Logic**: The code now gets the transformed header and checks if the transformer preserves a class index. If both the original and transformed data have valid class indices, it updates `m_classIndex` and `m_hasClass`. This addresses the issue where the class index might change after transformation.
3.  **Starting Set**: The code checks if the starting set contains the class index and adjusts the size of the starting set accordingly.

These changes strongly suggest that the original code had a bug where it didn't correctly handle the class index when using attribute transformers, potentially leading to incorrect attribute selection or errors. The updated code includes checks and adjustments to address this issue.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,8085.json,908420a70784469c92b61a26142bb14cd748d5d5,"@@ -1,57 +1,59 @@
   public void finalizeTask() throws Exception {
     if (m_classifier == null) {
       throw new Exception(""No classifier has been set"");
     }
 
     if (m_classifier instanceof UpdateableClassifier
       && !m_batchTrainedIncremental) {
       // nothing to do except possibly down-sample predictions for
       // auc/prc
       if (m_predFrac > 0) {
         ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(
           m_predFrac, m_seed);
       }
 
       return;
     }
 
     m_trainingHeader.compactify();
 
     Instances test = m_trainingHeader;
     Random r = new Random(m_seed);
     test.randomize(r);
     if (test.classAttribute().isNominal() && m_totalFolds > 1) {
       test.stratify(m_totalFolds);
     }
 
     if (m_totalFolds > 1 && m_foldNumber >= 1) {
       test = test.testCV(m_totalFolds, m_foldNumber - 1);
     }
 
     m_numTestInstances = test.numInstances();
 
-    if (m_classifier instanceof BatchPredictor) {
+    if (m_classifier instanceof BatchPredictor
+      && ((BatchPredictor) m_classifier)
+        .implementsMoreEfficientBatchPrediction()) {
 
       // this method always stores the predictions for AUC, so we need to get
-      // rid of them if we're note doing any AUC computation
+      // rid of them if we're not doing any AUC computation
       m_eval.evaluateModel(m_classifier, test);
-      if (m_predFrac < 0) {
+      if (m_predFrac <= 0) {
         ((AggregateableEvaluationWithPriors) m_eval).deleteStoredPredictions();
       }
     } else {
       for (int i = 0; i < test.numInstances(); i++) {
         if (m_predFrac > 0) {
           m_eval.evaluateModelOnceAndRecordPrediction(m_classifier,
             test.instance(i));
         } else {
           m_eval.evaluateModelOnce(m_classifier, test.instance(i));
         }
       }
     }
 
     // down-sample predictions for auc/prc
     if (m_predFrac > 0) {
       ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(m_predFrac,
         m_seed);
     }
   }",Buggy,"Fixed a bug (due to changes in BatchPredictor) that caused predictions for AUC, AUPRC to always be retained regardless of the user-specified fraction

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12342 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the retention of predictions for AUC and AUPRC, even when the user specifies a fraction for pruning. The code diff shows changes within the `finalizeTask` method, specifically when the classifier is an instance of `BatchPredictor`.

Here's a breakdown of the changes:

1.  **Condition for BatchPredictor:** The code now checks if the `BatchPredictor` also implements `implementsMoreEfficientBatchPrediction()`. This suggests that the bug was specific to a certain type of `BatchPredictor`.
2.  **Prediction Pruning:** The condition for deleting stored predictions has been changed from `m_predFrac < 0` to `m_predFrac <= 0`. This ensures that predictions are deleted when `m_predFrac` is 0, which likely represents a user's intention to not retain any predictions.

The changes directly address the issue described in the commit message. The modification to the `if` condition (`m_predFrac <= 0`) ensures that predictions are cleared when the user does not want them, fixing the bug where predictions were always retained.

**Buggy**"
weka,26472.json,aab16fcf5619d64b56a044bb7acaa276a2f2857b,"@@ -1,12 +1,15 @@
   public double matthewsCorrelationCoefficient(int classIndex) {
     double numTP = numTruePositives(classIndex);
     double numTN = numTrueNegatives(classIndex);
     double numFP = numFalsePositives(classIndex);
     double numFN = numFalseNegatives(classIndex);
     double n = (numTP * numTN) - (numFP * numFN);
     double d = (numTP + numFP) * (numTP + numFN) * (numTN + numFP)
         * (numTN + numFN);
     d = Math.sqrt(d);
+    if (d == 0) {
+      d = 1;
+    }
 
     return n / d;
   }",Buggy,"Fixed a bug in the MCC calculation - MCC should be zero if the denominator of the calculation is zero.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9101 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug in the Matthews Correlation Coefficient (MCC) calculation. Specifically, the MCC should be zero when the denominator of the calculation is zero.

The code diff modifies the `matthewsCorrelationCoefficient` method. It calculates the denominator `d` and then checks if `d` is zero. If it is, `d` is set to 1. This change prevents division by zero, which would result in an error or `NaN`. By setting `d` to 1 when it's initially zero, the MCC becomes `n / 1 = n`. However, the commit message states that the MCC should be zero if the denominator is zero. The code does not set the MCC to zero, but rather sets the denominator to 1. While this avoids a division by zero error, it does not fully implement the intended fix as described in the commit message. The code change avoids the error, but it does not return 0 when the denominator is zero.

Since the code change addresses a potential division by zero error and is related to the bug fix described in the commit message, it can be considered a bug fix.

**Buggy**"
weka,32917.json,db5c23a0208b68d40384c60c377e2e204c0b098e,"@@ -1,17 +1,17 @@
   public void setOptions(String[] options) throws Exception {
-    String nnSearchClass = Utils.getOption('D', options);
+    String nnSearchClass = Utils.getOption('A', options);
     if(nnSearchClass.length() != 0) {
       String nnSearchClassSpec[] = Utils.splitOptions(nnSearchClass);
       if(nnSearchClassSpec.length == 0) { 
         throw new Exception(""Invalid DistanceFunction specification string.""); 
       }
       String className = nnSearchClassSpec[0];
       nnSearchClassSpec[0] = """";
 
       setDistanceFunction( (DistanceFunction)
-                            Utils.forName( NearestNeighbourSearch.class, 
+                            Utils.forName( DistanceFunction.class, 
                                            className, nnSearchClassSpec) );
     }
     else 
       this.setDistanceFunction(new EuclideanDistance());  
   }",Buggy,"New Reorder filter. Improvement of docs for Copy filter. Improvements to ArffViewer. Bug fix in NearestNeighbourSearch option setting. All from Peter.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2325 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message explicitly states ""Bug fix in NearestNeighbourSearch option setting."" This is a strong indicator that the diff likely contains a bug fix related to how options are set for the `NearestNeighbourSearch` functionality.

**Git Diff Analysis:**

The diff focuses on the `setOptions` method.  Here's a breakdown of the changes:

1.  **Option Key Change:** `Utils.getOption('D', options)` is changed to `Utils.getOption('A', options)`. This suggests that the option key used to retrieve the NearestNeighbourSearch class was incorrect and has been corrected. This is a typical bug fix scenario.
2. **Class type in Utils.forName:** `NearestNeighbourSearch.class` is changed to `DistanceFunction.class`. This indicates that the code was trying to create an instance of `NearestNeighbourSearch` directly, but it should be creating an instance of `DistanceFunction` instead.

**Reasoning:**

The commit message explicitly mentions a bug fix in `NearestNeighbourSearch` option setting. The diff shows a change in the option key used to retrieve the NearestNeighbourSearch class and a change in the class type in `Utils.forName`. These changes directly relate to how options are parsed and used to configure the `NearestNeighbourSearch` functionality. The change in option key suggests a configuration error, and the change in class type suggests an incorrect instantiation of the DistanceFunction. These are strong indicators of a bug fix.

**Conclusion:**

**Buggy**
"
weka,15986.json,a67dbdf599b532b4d05eec72b79cc0a92a58bb0f,"@@ -1,27 +1,29 @@
   public boolean setInputFormat(Instances instanceInfo) throws Exception {
     
     super.setInputFormat(instanceInfo);
 
     int[] attsToDelete = new int[instanceInfo.numAttributes()];
     int numToDelete = 0;
     for (int i=0; i<instanceInfo.numAttributes(); i++) {
-      if (i == instanceInfo.classIndex()) continue; // skip class
+      if ((i == instanceInfo.classIndex() && !m_invert)) {
+	continue; // skip class
+      }
       if (instanceInfo.attribute(i).type() == m_attTypeToDelete)
 	attsToDelete[numToDelete++] = i;
     }
 
     int[] finalAttsToDelete = new int[numToDelete];
     System.arraycopy(attsToDelete, 0, finalAttsToDelete, 0, numToDelete);
     
     m_attributeFilter.setAttributeIndicesArray(finalAttsToDelete);
     m_attributeFilter.setInvertSelection(m_invert);
     
     boolean result = m_attributeFilter.setInputFormat(instanceInfo);
     Instances afOutputFormat = m_attributeFilter.getOutputFormat();
     
     // restore old relation name to hide attribute filter stamp
     afOutputFormat.setRelationName(instanceInfo.relationName());
 
     setOutputFormat(afOutputFormat);
     return result;
   }",Buggy,"Fixed bug.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1805 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug"" indicates a bug fix.

The code diff modifies the `setInputFormat` method. Specifically, the condition `i == instanceInfo.classIndex()` is changed to `(i == instanceInfo.classIndex() && !m_invert)`. This change suggests that the original condition was incorrect when `m_invert` was true, potentially leading to a bug where the class attribute was incorrectly processed. The added `!m_invert` condition seems to correct this behavior.

Therefore, the code change aligns with the commit message, indicating a bug fix.

**Buggy**"
weka,10085.json,7d3176f1d203310d133e13abf01de23e6eb5784f,"@@ -1,7 +1,5 @@
-    public int[] postProcess(int[] attributeSet) {
+  public void clean() {
 
     // save memory
     m_trainInstances = new Instances(m_trainInstances, 0);
-
-    return attributeSet;
   }",Buggy,"Fixed a bug when generating a ranking via GreedyStepwise

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug when generating a ranking via GreedyStepwise"" clearly states that the commit is intended to fix a bug related to the `GreedyStepwise` algorithm's ranking generation process.

**Git Diff Analysis:**

The diff shows the removal of the `postProcess` method and the addition of a `clean` method. The `clean` method sets `m_trainInstances` to a new `Instances` object with zero capacity, effectively clearing the training instances to save memory.

**Reasoning:**

1.  **Discrepancy between Commit Message and Code Changes:** The commit message mentions fixing a bug in ranking generation within `GreedyStepwise`. However, the code change involves a memory optimization by clearing the training instances. There's no direct evidence in the diff that the ranking generation logic itself has been modified. The `postProcess` method, which simply returned the input `attributeSet`, doesn't seem directly related to ranking generation bugs. The `clean` method is more about memory management.

2.  **Potential Bug Context (Inferred):** While the diff doesn't directly show a bug fix in ranking, it's *possible* that the memory optimization indirectly addresses a bug. For instance, if excessive memory usage during ranking was causing errors or incorrect results, then clearing the training instances could alleviate that issue. However, this is speculative.

3.  **Lack of Error Handling or Logic Correction:** The diff doesn't show any explicit error handling being added, any logical corrections to ranking algorithms, or any exception handling improvements. It's purely a memory management change.

**Conclusion:**

Given the discrepancy between the commit message (which mentions a ranking bug) and the code changes (which focus on memory optimization), and the lack of direct evidence of a ranking-related bug fix in the diff, it's difficult to definitively say that this commit fixes a bug in the ranking algorithm itself. However, the memory optimization *could* indirectly address a bug related to memory usage during ranking. Without more context, it's hard to be certain.

Therefore, I will err on the side of caution and classify this as **NotBuggy** because the diff doesn't directly show a bug fix related to ranking generation.

**NotBuggy**
"
weka,30928.json,656caee9c08b0537064158bcef54389068beb1eb,"@@ -1,55 +1,57 @@
   public static void runFileLoader(AbstractFileLoader loader, String[] options) {
     // help request?
     try {
       String[] tmpOptions = options.clone();
       if (Utils.getFlag('h', tmpOptions)) {
         System.err.println(""\nHelp requested\n"" + makeOptionStr(loader));
         return;
       }
     } catch (Exception e) {
       // ignore it
     }
 
     if (options.length > 0) {
+      String fileName = options[0];
+      options[0] = """";
       if (loader instanceof OptionHandler) {
         // set options
         try {
           ((OptionHandler) loader).setOptions(options);
           // find file
           for (int i = 0; i < options.length; i++) {
             if (options[i].length() > 0) {
               options = new String[] { options[i] };
               break;
             }
           }
         } catch (Exception ex) {
           System.err.println(makeOptionStr(loader));
           System.exit(1);
         }
       }
 
       try {
-        loader.setFile(new File(options[0]));
+        loader.setFile(new File(fileName));
         // incremental
         if (loader instanceof IncrementalConverter) {
           Instances structure = loader.getStructure();
           System.out.println(structure);
           Instance temp;
           do {
             temp = loader.getNextInstance(structure);
             if (temp != null) {
               System.out.println(temp);
             }
           } while (temp != null);
         }
         // batch
         else {
           System.out.println(loader.getDataSet());
         }
       } catch (Exception ex) {
         ex.printStackTrace();
       }
     } else {
       System.err.println(makeOptionStr(loader));
     }
   }",Buggy,"Bug fix: file name wasn't removed from options array.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to file name handling in the options array. The diff shows that the file name is now extracted from the `options` array and stored in the `fileName` variable. The original `options[0]` is then set to an empty string. Later, the `loader.setFile` method is called with the extracted `fileName`. This change addresses the issue where the file name was not properly removed from the options array, which could have led to incorrect option parsing or file loading. The code now correctly isolates the file name and passes it to the loader. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,21177.json,e137d2cd150f941d4ad96e9bcb3d3a27f7153b2e,"@@ -1,29 +1,33 @@
   protected int getIndexOfAttribute(Instances insts, String attName) {
     
+    if (attName == null) {
+      return -1;
+    }
+    
     // special first and last strings
     if (attName.equalsIgnoreCase(""/last"")) {
       return insts.numAttributes() - 1;
     }
     if (attName.equalsIgnoreCase(""/first"")) {
       return 0;
     }
     if (attName.startsWith(""/"")) {
       // try and parse remainder as a number
       String numS = attName.replace(""/"", """");
       try {
         int index = Integer.parseInt(numS);
         index--; // from 1-based to 0-based
         if (index >= 0 && index < insts.numAttributes()) {
           return index;
         }
       } catch (NumberFormatException e) {        
       }      
     }
     
     Attribute att = insts.attribute(attName);
     if (att != null) {
       return att.index();
     }
     
     return -1; // not found
   }",Buggy,"Small bug fix in the routine that finds attribute indexes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7639 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the routine that finds attribute indexes.

The diff introduces a null check for the `attName` parameter at the beginning of the method. If `attName` is null, the method now returns -1. This change prevents a potential NullPointerException if `insts.attribute(attName)` is called with a null `attName`.

The addition of the null check strongly suggests that a bug existed where a null `attName` could cause an error. Therefore, the changes indicate a bug fix.

**Buggy**"
weka,20328.json,8e2c885e0e2766ce262502a0ba98b112379f314f,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7689 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting the tool tip for additional options in plugin renderers.

The code diff shows a change in the `setupRendererOptsTipText` method. Specifically, the `PluginManager.getPluginInstance` method's first argument was changed from `""weka.gui.beans.OffscreenChartRender""` to `""weka.gui.beans.OffscreenChartRenderer""`. This suggests that the original code had a typo or incorrect class name, which would have prevented the plugin from being loaded correctly and the tool tip from being set. The `try-catch` block suggests that the code was already attempting to handle potential exceptions during plugin loading, but the incorrect class name would have caused an exception to be thrown.

The change directly addresses the problem described in the commit message, indicating a bug fix.

**Buggy**"
weka,20328.json,68eef00749a6bf58cbca2344dd6f91e526407d21,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
-      tipText = tipText.replace(""<html>"", ""<html>Comma separate list of options:<br>"");
+      tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
         Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a spelling mistake in a tooltip.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7635 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a spelling mistake in a tooltip. The diff shows a change from ""Comma separate list of options"" to ""Comma separated list of options"". This change corrects the spelling of ""separate"" to ""separated"". The code modification directly aligns with the commit message, indicating a bug fix related to a spelling error.

**Buggy**"
weka,3145.json,d00cb09c5d77936fe79ca4e6697fc4b8dde11de4,"@@ -1,50 +1,51 @@
   protected List<Instances> initializeWithRandomCenters(
     JavaRDD<Instance> dataset, Instances headerWithSummary, int numRuns,
     int numClusters) throws IOException, DistributedWekaException {
 
     Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     // sample all runs worth of initial centers in one hit
     // take twice as many as needed in case there are duplicates
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       try {
         seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
       } catch (NumberFormatException e) {
         // don't complain
       }
     }
 
     // oversample for > 1 cluster per run, so that we have some options if there
     // are duplicates in the list. numClusters == 1 will be used when seeding
     // the k-means|| initialization process
+    int oversampleFactor = numClusters > 1 ? 2 : 1;
     List<Instance> centerList =
-      dataset.takeSample(true, numClusters > 1 ? 2 : 1 * numRuns * numClusters,
-        seed);
+      dataset.takeSample(true, oversampleFactor * numRuns * numClusters,
+                         seed);
 
     // make sure that start points and header have been through any filters
     KMeansMapTask forFilteringOnly = new KMeansMapTask();
     try {
       forFilteringOnly.setOptions(Utils
         .splitOptions(environmentSubstitute(getKMeansMapTaskOpts())));
 
       // initialize sketches
       headerNoSummary = forFilteringOnly.init(headerWithSummary);
 
       for (int i = 0; i < centerList.size(); i++) {
         Instance filtered = forFilteringOnly.applyFilters(centerList.get(i));
         centerList.set(i, filtered);
       }
 
     } catch (Exception ex) {
       logMessage(ex);
       throw new DistributedWekaException(ex);
     }
 
     List<Instances> centreCandidates =
       KMeansMapTask.assignStartPointsFromList(numRuns, numClusters, centerList,
         headerNoSummary);
 
     return centreCandidates;
   }",Buggy,"Fixed a bug in the standard random initialization routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11926 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed a bug in the standard random initialization routine"" suggests a correction to the random initialization process.

The code diff modifies the `initializeWithRandomCenters` method. The key change is the introduction of an `oversampleFactor` which is set to 2 if `numClusters` is greater than 1, otherwise it's 1. This factor is then used in the `takeSample` method to determine the number of instances to sample from the dataset. The original code used `numClusters > 1 ? 2 : 1 * numRuns * numClusters` as the sample size, while the modified code uses `oversampleFactor * numRuns * numClusters`.

The comment ""oversample for > 1 cluster per run, so that we have some options if there are duplicates in the list. numClusters == 1 will be used when seeding the k-means|| initialization process"" explains the purpose of the oversampling. It seems the original code might have had issues with duplicate initial centers, especially when the number of clusters was greater than 1. The oversampling aims to mitigate this by providing more candidate centers to choose from.

The change directly addresses a potential issue with the random initialization process, specifically related to duplicate centers. This aligns with the commit message indicating a bug fix.

**Buggy**"
weka,26979.json,483a2a5d5184c1590893885c0a1be41649ba25de,"@@ -1,113 +1,113 @@
   public String toString() {
 
     StringBuffer text = new StringBuffer();
     int printed = 0;
 	
     if ((m_alpha == null) && (m_sparseWeights == null)) {
       return ""SMOreg : No model built yet."";
     }
     try {
       text.append(""SMOreg\n\n"");
 	    
       text.append(""Kernel used : \n"");
       if(m_useRBF) {
 	text.append(""  RBF kernel : K(x,y) = e^-("" + m_gamma + ""* <x-y,x-y>^2)"");
       } else if (m_exponent == 1){
 	text.append(""  Linear Kernel : K(x,y) = <x,y>"");
       } else {
 	if (m_featureSpaceNormalization) {
 	  if (m_lowerOrder){
 	    text.append(""  Normalized Poly Kernel with lower order : K(x,y) = (<x,y>+1)^"" + m_exponent + ""/"" + 
 			""((<x,x>+1)^"" + m_exponent + ""*"" + ""(<y,y>+1)^"" + m_exponent + "")^(1/2)"");		    
 	  } else {
 	    text.append(""  Normalized Poly Kernel : K(x,y) = <x,y>^"" + m_exponent + ""/"" + ""(<x,x>^"" + 
 			m_exponent + ""*"" + ""<y,y>^"" + m_exponent + "")^(1/2)"");
 	  }
 	} else {
 	  if (m_lowerOrder){
 	    text.append(""  Poly Kernel with lower order : K(x,y) = (<x,y> + 1)^"" + m_exponent);
 	  } else {
 	    text.append(""  Poly Kernel : K(x,y) = <x,y>^"" + m_exponent);		
 	  }
 	}
       }
       text.append(""\n\n"");
 
       // display the linear transformation
       String trans = """";
       if (m_filterType == FILTER_STANDARDIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(standardized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       } else if (m_filterType == FILTER_NORMALIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(normalized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       }
 
       // If machine linear, print weight vector
       if (!m_useRBF && m_exponent == 1.0) {
 	text.append(""Machine Linear: showing attribute weights, "");
 	text.append(""not support vectors.\n"");
 		
 	// We can assume that the weight vector is stored in sparse
 	// format because the classifier has been built
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	for (int i = 0; i < m_sparseWeights.length; i++) {
-	  if (i != (int)m_classIndex) {
+	  if (m_sparseIndices[i] != (int)m_classIndex) {
 	    if (printed > 0) {
 	      text.append("" + "");
 	    } else {
 	      text.append(""   "");
 	    }
 	    text.append(Utils.doubleToString(m_sparseWeights[i], 12, 4) +
 			"" * "");
 	    if (m_filterType == FILTER_STANDARDIZE) {
 	      text.append(""(standardized) "");
 	    } else if (m_filterType == FILTER_NORMALIZE) {
 	      text.append(""(normalized) "");
 	    }
 	    if (!m_checksTurnedOff) {
 	      text.append(m_data.attribute(m_sparseIndices[i]).name()+""\n"");
 	    } else {
 	      text.append(""attribute with index "" + 
 			  m_sparseIndices[i] +""\n"");
 	    }
 	    printed++;
 	  }
 	}
       } else {
 	text.append(""Support Vector Expansion :\n"");
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	printed = 0;
 	for (int i = 0; i < m_alpha.length; i++) {
 	  double val = m_alpha[i] - m_alpha_[i];
 	  if (java.lang.Math.abs(val) < 1e-4)
 	    continue;
 	  if (printed > 0) {
 	    text.append("" + "");
 	  } else {
 	    text.append(""   "");		    
 	  }
 	  text.append(Utils.doubleToString(val, 12, 4) 
 		      + "" * K[X("" + i + ""), X]\n"");
 	  printed++;
 	}
       }
       if (m_b > 0) {
 	text.append("" + "" + Utils.doubleToString(m_b, 12, 4));
       } else {
 	text.append("" - "" + Utils.doubleToString(-m_b, 12, 4));
       }
       if (m_useRBF || m_exponent != 1.0) {
 	text.append(""\n\nNumber of support vectors: "" + printed);
       }
       text.append(""\n\nNumber of kernel evaluations: "" + m_kernel.numEvals()+ ""\n"");
     } catch (Exception e) {
       return ""Can't print the classifier."";
     }
 
     return text.toString();
   }",Buggy,"Fixed bug in output of sparse linear machines (class index was not dealt with correctly (thanks, Bernhard)).


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2091 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the class index in the output of sparse linear machines. The phrase ""class index was not dealt with correctly"" suggests an error in how the class index was being handled, leading to incorrect output.

The code diff modifies a conditional statement within the `toString()` method. The original condition `if (i != (int)m_classIndex)` is changed to `if (m_sparseIndices[i] != (int)m_classIndex)`. This change suggests that the code was previously comparing the loop index `i` directly with the class index, which is incorrect when dealing with sparse weights. The correct approach, as implemented in the modified code, is to compare the index of the sparse weight vector `m_sparseIndices[i]` with the class index. This ensures that the class attribute is correctly excluded from the output of attribute weights.

The change aligns perfectly with the commit message, indicating a bug where the class index was not being handled correctly when printing the weights of a sparse linear machine.

**Buggy**"
weka,28766.json,78f99a6107b459a31b5ab63e2f403eb841974e27,"@@ -1,14 +1,19 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     double[] dist = new double[instance.numClasses()];
     switch (instance.classAttribute().type()) {
     case Attribute.NOMINAL:
-      dist[(int)classifyInstance(instance)] = 1.0;
+      double classification = classifyInstance(instance);
+      if (Instance.isMissingValue(classification)) {
+	return dist;
+      } else {
+	dist[(int)classification] = 1.0;
+      }
       return dist;
     case Attribute.NUMERIC:
       dist[0] = classifyInstance(instance);
       return dist;
     default:
       return dist;
     }
   }",Buggy,"Fixed bug that caused incorrect handling of unclassified instances in distributionForInstance()


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2006 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the handling of unclassified instances in the `distributionForInstance()` method. The code diff shows a modification within the `Attribute.NOMINAL` case. Specifically, it adds a check to see if the classification result from `classifyInstance(instance)` is a missing value using `Instance.isMissingValue(classification)`. If it is a missing value, the method returns the original `dist` array (which is initialized to all zeros). Otherwise, it proceeds to assign 1.0 to the corresponding class index in the `dist` array. This change directly addresses the scenario where `classifyInstance` might return a missing value, which would lead to an `ArrayIndexOutOfBoundsException` if not handled. Thus, the code change aligns with the commit message, indicating a bug fix.

**Buggy**"
weka,28908.json,b842fc63d279fec85c6c091749c5a8b41f36f775,"@@ -1,25 +1,25 @@
   protected Instances metaFormat(Instances instances) throws Exception {
 
     FastVector attributes = new FastVector();
     Instances metaFormat;
     Attribute attribute;
     int i = 0;
 
     for (int k = 0; k < m_Classifiers.length; k++) {
       Classifier classifier = (Classifier) getClassifier(k);
       String name = classifier.getClass().getName();
       if (m_BaseFormat.classAttribute().isNumeric()) {
 	attributes.addElement(new Attribute(name));
       } else {
 	for (int j = 0; j < m_BaseFormat.classAttribute().numValues(); j++) {
 	  attributes.addElement(new Attribute(name + "":"" + 
 					      m_BaseFormat
 					      .classAttribute().value(j)));
 	}
       }
     }
-    attributes.addElement(m_BaseFormat.classAttribute());
+    attributes.addElement(m_BaseFormat.classAttribute().copy());
     metaFormat = new Instances(""Meta format"", attributes, 0);
     metaFormat.setClassIndex(metaFormat.numAttributes() - 1);
     return metaFormat;
   }",Buggy,"Apply bug fix from Alexander K. Seewald <alexsee@oefai.at>


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix applied from an external source. The diff modifies the `metaFormat` method, specifically how the class attribute from `m_BaseFormat` is added to the `attributes` vector. Instead of directly adding the class attribute, a copy of it is now added using `m_BaseFormat.classAttribute().copy()`. This change likely addresses an issue where modifications to the meta-format's class attribute were inadvertently affecting the original `m_BaseFormat`'s class attribute, or vice versa. This is a common bug scenario when dealing with mutable objects. Therefore, the change is likely a bug fix.

**Buggy**"
weka,4826.json,6809d47f9b4ddab9de675679cc0cd2f38a66799c,"@@ -1,104 +1,103 @@
   protected void display(ArrayList<Prediction> preds, Attribute classAtt,
     int classValue) {
 
     if (preds == null) {
       JOptionPane.showMessageDialog(null, ""No data available for display!"");
       return;
     }
 
     // Remove prediction objects where either the prediction or the actual value are missing
     ArrayList<Prediction> newPreds = new ArrayList<>();
     for (Prediction p : preds) {
       if (!Utils.isMissingValue(p.actual()) && !Utils
         .isMissingValue(p.predicted())) {
         newPreds.add(p);
       }
     }
     preds = newPreds;
 
     ArrayList<Attribute> attributes = new ArrayList<>(1);
     attributes.add(new Attribute(""class_prob""));
     Instances data =
       new Instances(""class_probabilities"", attributes, preds.size());
 
     for (int i = 0; i < preds.size(); i++) {
       double[] inst =
         { ((NominalPrediction) preds.get(i)).distribution()[classValue] };
       data.add(new DenseInstance(preds.get(i).weight(), inst));
     }
 
     try {
       Discretize d = new Discretize();
       d.setUseEqualFrequency(true);
       d.setBins(
         Integer.max(1, (int) Math.round(Math.sqrt(data.sumOfWeights()))));
       d.setUseBinNumbers(true);
       d.setInputFormat(data);
       data = Filter.useFilter(data, d);
 
       int numBins = data.attribute(0).numValues();
       double[] sumClassProb = new double[numBins];
       double[] sumTrueClass = new double[numBins];
       double[] sizeOfBin = new double[numBins];
       for (int i = 0; i < data.numInstances(); i++) {
         int binIndex = (int) data.instance(i).value(0);
         sizeOfBin[binIndex] += preds.get(i).weight();
         sumTrueClass[binIndex] +=
           preds.get(i).weight() * ((((int) preds.get(i).actual())
             == classValue) ? 1.0 : 0.0);
         sumClassProb[binIndex] +=
           preds.get(i).weight() * ((NominalPrediction) preds.get(i))
             .distribution()[classValue];
       }
 
       ArrayList<Attribute> atts = new ArrayList<>(1);
       atts.add(new Attribute(""average_class_prob""));
       atts.add(new Attribute(""average_true_class_value""));
 
       // Collect data for plotting, making sure that 0,0 and 1,1 are included as invisible points
       Instances cdata =
         new Instances(""calibration_curve_data"", atts, numBins + 2);
       int[] shapeType = new int[numBins + 2];
       boolean[] connectPoint = new boolean[numBins + 2];
       for (int i = 0; i < numBins; i++) {
         double[] v = new double[2];
         v[0] = sumClassProb[i] / sizeOfBin[i];
         v[1] = sumTrueClass[i] / sizeOfBin[i];
         cdata.add(new DenseInstance(sizeOfBin[i], v));
         shapeType[i] = Plot2D.PLUS_SHAPE;
         connectPoint[i] = true;
       }
       double[] zero = new double[2];
       double[] one = new double[2];
       one[0] = 1.0;
       one[1] = 1.0;
       cdata.add(new DenseInstance(0.0, zero));
       cdata.add(new DenseInstance(0.0, one));
       shapeType[shapeType.length - 2] =
         -2; // Hack to make sure that corner points are invisible
       shapeType[shapeType.length - 1] = -2;
 
       PlotData2D plotInfo = new PlotData2D(cdata);
       plotInfo.setConnectPoints(connectPoint);
       plotInfo.setShapeType(shapeType);
-      plotInfo.setPlotName(
-        ""\""Calibration curve for class value \"" + classAtt.value(classValue)"");
+      plotInfo.setPlotName(""Calibration curve for class value "" + classAtt.value(classValue));
       VisualizePanel vp = new VisualizePanel();
       vp.setName(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       vp.setMasterPlot(plotInfo);
 
       JFrame frame = new JFrame(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       frame.setSize(1024, 800);
       frame.setContentPane(vp);
       frame.setVisible(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Committed one small bug fix in name of Instances object. Removed one unnecessary import.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14764 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the `Instances` object and the removal of an unnecessary import.

The code diff focuses on the `display` method, which generates a calibration curve for a given class value. The only change in the diff is the removal of double quotes around the plot name in the line:

```java
plotInfo.setPlotName(""Calibration curve for class value "" + classAtt.value(classValue));
```

The original code had unnecessary double quotes around the string, which would have resulted in the plot name being displayed with those quotes. Removing the quotes is indeed a bug fix, as it corrects the plot name to be displayed as intended.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,22844.json,0f8a283a8a350a9341d33f14127fb4d90fbe0b6c,"@@ -1,25 +1,40 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_ClassifierEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_ClassifierEditor.setCapabilitiesFilter(filterClass);
+    
+    // Check capabilities
+    m_StartBut.setEnabled(true);
+    Capabilities currentFilter = m_ClassifierEditor.getCapabilitiesFilter();
+    Classifier classifier = (Classifier) m_ClassifierEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (classifier != null && currentFilter != null && 
+        (classifier instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)classifier).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5382 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where the start button's enabled/disabled state wasn't updating correctly when a new dataset was set. The diff shows changes within the `updateCapabilitiesFilter` method. Specifically, the code now includes logic to check the capabilities of the classifier against the current filter and disables the start button (`m_StartBut`) if the classifier doesn't support the filter. This aligns with the commit message, as it directly addresses the issue of updating the start button's state based on the dataset. The added code ensures that the start button is only enabled if the classifier can handle the current dataset's capabilities.

**Buggy**
"
weka,15202.json,e6a516aae06f40c5da24d36795d1ee48b4688fb5,"@@ -1,11 +1,17 @@
   public void setOptions(String[] options) throws Exception {
     
     setAttributeIndices(Utils.getOption('R', options));
     setInvertSelection(Utils.getFlag('V', options));
-    setClassName(Utils.getOption('C', options));
-    setMethodName(Utils.getOption('M', options));
+    String classString = Utils.getOption('C', options);
+    if (classString.length() != 0) {
+      setClassName(classString);
+    }
+    String methodString = Utils.getOption('M', options);
+    if (methodString.length() != 0) {
+      setMethodName(methodString);
+    }
 
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",Buggy,"Fixed some option-handling bugs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2193 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed some option-handling bugs"" suggests that the changes in the diff address issues related to how options are processed.

The diff shows modifications in the `setOptions` method. Specifically, the code now checks if the `classString` and `methodString` obtained from the options have a length greater than 0 before setting the class name and method name, respectively. This suggests that previously, the code might have been setting empty class names or method names, potentially leading to errors or unexpected behavior. The added checks prevent this by ensuring that only non-empty strings are used to set these values. This is a bug fix because it addresses a potential issue where invalid or empty values could be assigned to class name and method name.

**Buggy**"
weka,23203.json,af433bd414a79a55cf517aeccf73cfc90d5c8089,"@@ -1,13 +1,14 @@
   public void setInstances(Instances inst) {
     m_Instances = inst;
     m_RelationNameLab.setText(m_Instances.relationName());
+    m_RelationNameLab.setToolTipText(m_Instances.relationName());
     m_NumInstancesLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + m_Instances.numInstances()));
     m_NumAttributesLab.setText("""" + m_Instances.numAttributes());
     m_sumOfWeightsLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + Utils.doubleToString(m_Instances.sumOfWeights(), 3)));
   }",Buggy,"Fixed a bug where the relation name would overlap the attributes. Now sets the tip text for the relation name label to be equal to the relation name (so that the full relation name can be seen when the mouse hovers over the name on the panel).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7229 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for an overlapping relation name issue by setting the tooltip text for the relation name label. The diff shows that `m_RelationNameLab.setToolTipText(m_Instances.relationName());` is added. This aligns perfectly with the commit message, suggesting a bug fix related to the display of the relation name.

**Buggy**"
weka,27297.json,334f1a26a1895da32f511cae40ee3143b87295d7,"@@ -1,18 +1,18 @@
   public double SVMOutput(Instance inst) throws Exception {
     
     double result = -m_b;
     // Is the machine linear?
     if (m_weights != null) {
       // Is weight vector stored in sparse format?
-      for (int i = 0; i < m_weights.length; i++) {
+      for (int i = 0; i < inst.numValues(); i++) {
 	if (inst.index(i) != m_classIndex) {
 	  result += m_weights[inst.index(i)] * inst.valueSparse(i);
 	}
       }
     } else {
       for (int i = m_supportVectors.getNext(-1); i != -1; i = m_supportVectors.getNext(i)) {
 	result += (m_alpha[i] - m_alphaStar[i]) * m_kernel.eval(-1, i, inst);
       }
     }
     return result;
   }",Buggy,"Fixed bug where a loop a loop iteration was not correct for sparse instances.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to loop iteration in sparse instances. The code diff shows a change in the loop condition from `i < m_weights.length` to `i < inst.numValues()`. This change suggests that the original loop condition was incorrect for sparse instances, potentially leading to incorrect calculations or array out-of-bounds exceptions. The new loop condition iterates through the values present in the sparse instance, which aligns with the commit message's description of fixing a loop iteration issue for sparse instances.

**Buggy**"
weka,9635.json,d31aa1c73607a81824ab967b78e8bf672d9c1a1b,"@@ -1,110 +1,110 @@
   protected void purgeTasks(long purgeInterval) {
     Date now = new Date();
     long nowMilli = now.getTime();
     boolean doPurge = false;
 
     List<WekaTaskEntry> taskList = m_taskMap.getTaskList();
     for (WekaTaskEntry t : taskList) {
       NamedTask task = m_taskMap.getTask(t);
       doPurge = false;
       if (!(task instanceof Scheduled)) {
         // Date lastExecuted = getExecutionTime(t);
         Date lastExecuted = t.getLastExecution();
         if (lastExecuted != null) {
           if (task.getTaskStatus().getExecutionStatus() == TaskStatusInfo.PROCESSING) {
             // don't purge executing tasks!!
             continue;
           }
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       } else {
         Date lastExecuted = t.getLastExecution();
         Date nextExecution =
           ((Scheduled) task).getSchedule().nextExecution(lastExecuted);
         if (nextExecution == null && lastExecuted != null) {
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       }
 
       if (doPurge) {
         PostMethod post = null;
         InputStream is = null;
 
         try {
           String url = ""http://"" + getHostname() + "":"" + getPort();
           url = url.replace("" "", ""%20"");
           url += PurgeTaskServlet.CONTEXT_PATH;
-          url += ""/?name="" + t.toString();
+	  url += ""/?name="" + URLEncoder.encode(t.toString(), ""UTF-8"");
           url += ""&client=Y"";
 
           post = new PostMethod(url);
           post.setDoAuthentication(true);
           post.addRequestHeader(new Header(""Content-Type"", ""text/plain""));
 
           // Get HTTP client
           HttpClient client =
             ConnectionManager.getSingleton().createHttpClient();
           ConnectionManager.addCredentials(client, m_username, m_password);
 
           // Execute request
           int result = client.executeMethod(post);
           // System.out.println(""[WekaServer] Response from master server : "" +
           // result);
           if (result == 401) {
             System.err.println(""[WekaServer] Unable to purge task""
               + "" - authentication required.\n"");
           } else {
             // the response
             is = post.getResponseBodyAsStream();
             ObjectInputStream ois = new ObjectInputStream(is);
             Object response = ois.readObject();
             if (response.toString().startsWith(WekaServlet.RESPONSE_ERROR)) {
               System.err.println(""[WekaServer] A problem occurred while ""
                 + ""trying to purge task ("" + t.toString() + ""): \n"" + ""\t""
                 + response.toString());
             } else {
               System.out.println(""[WekaServer] purged task: "" + t.toString());
             }
           }
         } catch (Exception ex) {
           System.err
             .println(""[WekaServer] A problem occurred while ""
               + ""trying to purge task ("" + t.toString() + ""): ""
               + ex.getMessage());
           ex.printStackTrace();
         } finally {
           if (is != null) {
             try {
               is.close();
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
 
           if (post != null) {
             post.releaseConnection();
           }
         }
       }
     }
   }",Buggy,"Fixed a bug in the purge task thread

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12578 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed a bug in the purge task thread"" suggests a bug fix.

The diff shows a change in how the task name is encoded in the URL when purging tasks. Specifically, the code now uses `URLEncoder.encode(t.toString(), ""UTF-8"")` to encode the task name `t.toString()` before including it in the URL. This is important because task names might contain characters that are not allowed in URLs, or that have special meanings in URLs (e.g., spaces, question marks, ampersands). Without proper encoding, these characters could cause the URL to be misinterpreted by the server, leading to errors in purging the task.

The previous code `url += ""/?name="" + t.toString();` was vulnerable to URL injection and could fail if the task name contained special characters. The updated code ensures that the task name is properly encoded, preventing these issues. This change directly addresses a potential bug where tasks could fail to be purged due to improperly formatted URLs.

Therefore, the changes indicate a bug fix related to URL encoding.

**Buggy**"
weka,33400.json,a69dfab4b09e9d52542306b240900fdbfb951df3,"@@ -1,145 +1,146 @@
   private Instance vectorizeInstance(Instance input, int[] offsetHolder,
     boolean retainStringAttValuesInMemory) throws Exception {
 
     if (!m_inputContainsStringAttributes) {
       return input;
     }
 
     if (m_inputFormat == null) {
       throw new Exception(""No input format available. Call setup() and ""
         + ""make sure a dictionary has been built first."");
     }
 
     if (m_consolidatedDict == null) {
       throw new Exception(""Dictionary hasn't been built or consolidated yet!"");
     }
 
     int indexOffset = 0;
     int classIndex = m_outputFormat.classIndex();
     Map<Integer, double[]> contained = new TreeMap<Integer, double[]>();
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (!m_selectedRange.isInRange(i)) {
         if (!m_inputFormat.attribute(i).isString()
           && !m_inputFormat.attribute(i).isRelationValued()) {
 
           // add nominal and numeric directly
           if (input.value(i) != 0.0) {
             contained.put(indexOffset, new double[] { input.value(i) });
           }
         } else {
           if (input.isMissing(i)) {
             contained.put(indexOffset, new double[] { Utils.missingValue() });
           } else if (m_inputFormat.attribute(i).isString()) {
             String strVal = input.stringValue(i);
             if (retainStringAttValuesInMemory) {
               double strIndex =
                 m_outputFormat.attribute(indexOffset).addStringValue(strVal);
               contained.put(indexOffset, new double[] { strIndex });
             } else {
               m_outputFormat.attribute(indexOffset).setStringValue(strVal);
               contained.put(indexOffset, new double[] { 0 });
             }
           } else {
             // relational
             if (m_outputFormat.attribute(indexOffset).numValues() == 0) {
               Instances relationalHeader =
                 m_outputFormat.attribute(indexOffset).relation();
 
               // hack to defeat sparse instances bug
               m_outputFormat.attribute(indexOffset).addRelation(
                 relationalHeader);
             }
             int newIndex =
               m_outputFormat.attribute(indexOffset).addRelation(
                 input.relationalValue(i));
             contained.put(indexOffset, new double[] { newIndex });
           }
         }
         indexOffset++;
       }
     }
 
     offsetHolder[0] = indexOffset;
 
     // dictionary entries
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (m_selectedRange.isInRange(i) && !input.isMissing(i)) {
         m_tokenizer.tokenize(input.stringValue(i));
 
         while (m_tokenizer.hasMoreElements()) {
           String word = m_tokenizer.nextElement();
           if (m_lowerCaseTokens) {
             word = word.toLowerCase();
           }
           word = m_stemmer.stem(word);
 
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount != null) {
             if (m_outputCounts) {
               double[] inputCount =
                 contained.get(idxAndDocCount[0] + indexOffset);
               if (inputCount != null) {
                 inputCount[0]++;
               } else {
                 contained.put(idxAndDocCount[0] + indexOffset,
                   new double[] { 1 });
               }
             } else {
               contained
                 .put(idxAndDocCount[0] + indexOffset, new double[] { 1 });
             }
           }
         }
       }
     }
 
     // TF transform
     if (m_TFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           val[0] = Math.log(val[0] + 1);
         }
       }
     }
 
     // IDF transform
     if (m_IDFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           String word = m_outputFormat.attribute(index).name();
+          word = word.substring(m_Prefix.length());
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount == null) {
             throw new Exception(""This should never occur"");
           }
           if (idxAndDocCount.length != 2) {
             throw new Exception(""Can't compute IDF transform as document ""
               + ""counts are not available"");
           }
           val[0] = val[0] * Math.log(m_count / (double) idxAndDocCount[1]);
         }
       }
     }
 
     double[] values = new double[contained.size()];
     int[] indices = new int[contained.size()];
     int i = 0;
     for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
       values[i] = e.getValue()[0];
       indices[i++] = e.getKey().intValue();
     }
 
     Instance inst =
       new SparseInstance(input.weight(), values, indices,
         m_outputFormat.numAttributes());
     inst.setDataset(m_outputFormat);
 
     if (m_normalize) {
       normalizeInstance(inst, indexOffset);
     }
 
     return inst;
   }",Buggy,"Fixed a bug that affected the -P option when used in conjuction with the IDF transform

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12931 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the `-P` option when used with the IDF transform. The diff shows a change within the `vectorizeInstance` method, specifically in the IDF transform section.

The added line `word = word.substring(m_Prefix.length());` suggests that the code was not correctly handling the attribute name when the `-P` option (presumably setting a prefix) was used. Without this substring operation, the `m_consolidatedDict.get(word)` call would likely fail, leading to incorrect IDF calculations or exceptions. This aligns with the commit message describing a bug fix related to the `-P` option and IDF transform.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,3374.json,b97c9f7d4a47bf18c449215421f95140d8c6c5ea,"@@ -1,222 +1,222 @@
   public boolean runJobWithContext(JavaSparkContext sparkContext)
     throws IOException, DistributedWekaException {
 
     m_currentContext = sparkContext;
     setJobStatus(JobStatus.RUNNING);
     boolean success = false;
 
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     // Make sure that we save out to a subdirectory of the output
     // directory
     String outputPath = environmentSubstitute(m_sjConfig.getOutputDir());
     outputPath = addSubdirToPath(outputPath, OUTPUT_SUBDIR);
 
     JavaRDD<Instance> inputData = null;
     Instances headerWithSummary = null;
     if (getDataset(TRAINING_DATA) != null) {
       inputData = getDataset(TRAINING_DATA).getDataset();
       headerWithSummary = getDataset(TRAINING_DATA).getHeaderWithSummary();
       logMessage(""RDD<Instance> dataset provided: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     if (inputData == null && headerWithSummary == null) {
       logMessage(""[Randomly shuffle data] Invoking ARFF Job..."");
       m_arffHeaderJob.setEnvironment(m_env);
       m_arffHeaderJob.setLog(getLog());
       m_arffHeaderJob.setStatusMessagePrefix(m_statusMessagePrefix);
       m_arffHeaderJob.setCachingStrategy(getCachingStrategy());
 
       // header job necessary?
       success = m_arffHeaderJob.runJobWithContext(sparkContext);
 
       if (!success) {
         setJobStatus(JobStatus.FAILED);
         statusMessage(""Unable to continue - creating the ARFF header failed!"");
         logMessage(""[Randomly shuffle data] Unable to continue - creating the ARFF header failed!"");
         return false;
       }
 
       Dataset d = m_arffHeaderJob.getDataset(TRAINING_DATA);
 
       headerWithSummary = d.getHeaderWithSummary();
       inputData = d.getDataset();
       logMessage(""Fetching RDD<Instance> dataset from ARFF job: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     /*
      * int minSlices = 1; if
      * (!DistributedJobConfig.isEmpty(m_sjConfig.getMinInputSlices())) { try {
      * minSlices = Integer
      * .parseInt(environmentSubstitute(m_sjConfig.getMinInputSlices())); } catch
      * (NumberFormatException e) { } }
      */
 
     /*
      * if (!m_cleanOutputDir) { // check for existing chunk files... String
      * pathPlusChunk = outputPath + ""/part-00000"";
      * logMessage(""[Randomly shuffle data] Checking output directory: "" +
      * outputPath); if (SparkJob.checkFileExists(pathPlusChunk)) {
      * logMessage(""[Randomly shuffle data] Output directory is populated "" +
      * ""with randomly shuffled chunk files already - "" + ""no need to execute."");
      * 
      * loadShuffledDataFiles(outputPath, sparkContext,
      * CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary),
      * minSlices); return true; } }
      */
 
     /*
      * // TODO revisit at some stage... Current assumption: if you // have
      * output from this job as serialized instances then you // are happy with
      * the shuffling and will not want to re-shuffle if
      * (m_sjConfig.getSerializedInput()) { throw new DistributedWekaException(
      * ""Randomly shuffling serialized Instance "" +
      * ""input is not supported yet.""); }
      */
 
     // clean the output directory
     SparkJob.deleteDirectory(outputPath);
 
     String inputFile = environmentSubstitute(m_sjConfig.getInputFile());
 
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
     }
     final Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     try {
       WekaClassifierSparkJob.setClassIndex(
         environmentSubstitute(m_classAttribute), headerNoSummary,
         !m_dontDefaultToLastAttIfClassNotSpecified);
 
     } catch (Exception e) {
       logMessage(e);
       throw new DistributedWekaException(e);
     }
 
     // find summary attribute for class (if set, otherwise just use the first
     // numeric on nominal attribute). We're using this simply to find out
     // the total number of instances in the dataset
     String className = null;
     if (headerNoSummary.classIndex() >= 0) {
       className = headerNoSummary.classAttribute().name();
     } else {
       for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
         if (headerNoSummary.attribute(i).isNumeric()
           || headerNoSummary.attribute(i).isNominal()) {
           className = headerNoSummary.attribute(i).name();
           break;
         }
       }
     }
     Attribute summaryClassAtt =
       headerWithSummary
         .attribute(CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX
           + className);
     if (summaryClassAtt == null) {
       throw new DistributedWekaException(
         ""Was unable to find the summary attribute for "" + ""the class: ""
           + className);
     }
 
     int totalNumInstances = 0;
     int numFoldSlices = 10;
     // summary attribute for getting the total number of instances
     Attribute summaryAttOrig = null;
     for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
       if (headerNoSummary.attribute(i).isNumeric()
         || headerNoSummary.attribute(i).isNominal()) {
         summaryAttOrig = headerNoSummary.attribute(i);
         break;
       }
     }
     String summaryName = summaryAttOrig.name();
     Attribute summaryAtt =
       headerWithSummary
         .attribute(
           CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX + summaryName);
     if (summaryAtt == null) {
       logMessage(""[RandomizedDataSparkJob] Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
       throw new DistributedWekaException(""Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
     }
 
-    if (summaryAtt.isNominal()) {
+    if (summaryAttOrig.isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     if (DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())
       && DistributedJobConfig.isEmpty(getNumInstancesPerShuffledSplit())) {
       logMessage(""[RandomizedDataSparkJob] Must specify either the number of ""
         + ""splits or the number of instances per split"");
       throw new DistributedWekaException(""Must specify either the number of ""
         + ""splits or the number of instances per split"");
     }
 
     if (!DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())) {
       numFoldSlices =
         Integer.parseInt(environmentSubstitute(getNumRandomlyShuffledSplits()));
     } else {
       int numInsts = 0;
       try {
         numInsts =
           Integer
             .parseInt(environmentSubstitute(getNumInstancesPerShuffledSplit()));
       } catch (NumberFormatException ex) {
         throw new DistributedWekaException(ex);
       }
 
       if (numInsts <= 0) {
         throw new DistributedWekaException(
           ""Number of instances per split must "" + ""be > 0"");
       }
 
       if (numInsts > totalNumInstances) {
         throw new DistributedWekaException(""Can't have more instances per split ""
           + ""than there are instances in the dataset!"");
       }
       double nc = (double) totalNumInstances / numInsts;
       nc = Math.ceil(nc);
       numFoldSlices = (int) nc;
     }
     logMessage(""[Randomly shuffle] creating "" + numFoldSlices + "" splits."");
 
     if (headerNoSummary.attribute(className).isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryClassAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryClassAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     logMessage(""[Randomly shuffle data] Num slices = "" + numFoldSlices);
 
     final Random random = new Random(seed);
     for (int i = 0; i < 20; i++) {
       random.nextInt();
     }
 
     performRandomShuffle(inputData, outputPath, numFoldSlices, random,
       headerWithSummary, headerNoSummary.classIndex());
 
     setJobStatus(JobStatus.FINISHED);
 
     return true;
   }",Buggy,"Fixed a bug in the calculation of the total number of instances in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to calculating the total number of instances in a dataset. The code diff shows a change in the condition used to determine if the summary attribute is nominal. The original code checks `summaryAtt.isNominal()`, while the corrected code checks `summaryAttOrig.isNominal()`. `summaryAtt` represents the summary attribute, while `summaryAttOrig` represents the original attribute. The total number of instances is calculated differently based on whether the *original* attribute is nominal or numeric. The original code was using the summary attribute to determine the type, which is incorrect. This change directly addresses the bug described in the commit message.

**Buggy**"
weka,4830.json,9202bc66506c46f20388951597366af165de4928,"@@ -1,76 +1,77 @@
   public void processIncoming(Data data) throws WekaException {
 
     if (isStopRequested()) {
       getStepManager().interrupted();
       return;
     }
 
     getStepManager().processing();
 
     Instances predictedInsts = data.getPrimaryPayload();
     int maxSetNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
     int setNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
 
     if (maxSetNum > 1 && getPoolSets()) {
       if (m_isReset) {
         m_pooledData = new Instances(predictedInsts);
       } else {
         m_pooledData.addAll(predictedInsts);
       }
     }
 
     m_isReset = false;
 
     if (getPoolSets() && setNum < maxSetNum) {
+      getStepManager().finished();
       return;
     }
 
     if (getPoolSets() && maxSetNum > 1) {
       predictedInsts = m_pooledData;
     }
 
     if (predictedInsts.classIndex() < 0) {
       throw new WekaException(""No class set in the predicted data!"");
     }
 
     int numAttributes = predictedInsts.numAttributes();
     int numClasses = predictedInsts.classAttribute().numValues();
 
     // we always produce a curve for the first label. The user can
     // always choose a label by using a ClassValuePicker step
     Attribute classAtt = predictedInsts.classAttribute();
     Attribute predictedLabelProbAtt =
       predictedInsts.attribute(numAttributes - numClasses);
 
     ArrayList<Prediction> preds = new ArrayList<>();
     for (int i = 0; i < predictedInsts.numInstances(); i++) {
       Instance current = predictedInsts.instance(i);
       double[] dist = new double[numClasses];
       dist[0] = current.value(predictedLabelProbAtt.index());
       double actual = current.classValue();
 
       preds.add(new NominalPrediction(actual, dist, current.weight()));
     }
 
     try {
       Instances curveInsts =
         CalibrationCurveUtils
           .getCalibrationCurveAsInstances(preds, classAtt, 0);
       curveInsts.setRelationName(""__"" + curveInsts.relationName());
 
       Instance zero = curveInsts.remove(curveInsts.numInstances() - 2);
       zero.setWeight(-1);
       curveInsts.add(0, zero);
       curveInsts.lastInstance().setWeight(-1);
       Data output = new Data(StepManager.CON_DATASET, curveInsts);
       output.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, 1);
       output.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM, 1);
       getStepManager().outputData(output);
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     getStepManager().finished();
   }",Buggy,"Small bug fix in reporting finished state for step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14772 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a small bug fix related to reporting the finished state for a step. The diff adds `getStepManager().finished();` within the `if (getPoolSets() && setNum < maxSetNum)` block. This suggests that the step wasn't being correctly marked as finished in this specific pooling scenario, which is a bug.

**Buggy**"
weka,36414.json,81a1e225e8da398eb2d17f911d2de0b246ac22aa,"@@ -1,3 +1,3 @@
     void setUpdateIncrementalClassifier(boolean update) {
-    m_updateIncrementalClassifier = true;
+    m_updateIncrementalClassifier = update;
   }",Buggy,"Fixed a bug in the set method for updating an incremental classifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13045 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `setUpdateIncrementalClassifier` method. The diff shows that the original code always set `m_updateIncrementalClassifier` to `true`, regardless of the input parameter `update`. The corrected code now assigns the value of the `update` parameter to `m_updateIncrementalClassifier`, which fixes the bug.

**Buggy**"
weka,15017.json,a9246f58e2844ebadfa2c2b12f3f4d15755f5006,"@@ -1,58 +1,59 @@
   protected Instances process(Instances instances) throws Exception {
     // initializing necessary?
     if (!m_Initialized) {
       // do we have a file to initialize with?
       if ((getInitFile() != null) && getInitFile().isFile()) {
 	DataSource source = new DataSource(getInitFile().getAbsolutePath());
 	Instances data = source.getDataSet();
 	m_InitFileClassIndex.setUpper(data.numAttributes() - 1);
 	data.setClassIndex(m_InitFileClassIndex.getIndex());
 	initFilter(data);
       }
       else {
 	initFilter(instances);
       }
     }
 
     // apply filters
     if (m_Missing != null)
       instances = Filter.useFilter(instances, m_Missing); 
     if (m_NominalToBinary != null)
       instances = Filter.useFilter(instances, m_NominalToBinary); 
     if (m_ActualFilter != null)
       instances = Filter.useFilter(instances, m_ActualFilter);
 
     // backup class attribute and remove it
     double[] classes = instances.attributeToDoubleArray(instances.classIndex());
     int classIndex = instances.classIndex();
+    Attribute classAttribute = (Attribute)instances.classAttribute().copy();
     instances.setClassIndex(-1);
     instances.deleteAttributeAt(classIndex);
 
     // generate new header
     FastVector atts = new FastVector();
     for (int j = 0; j < m_NumTrainInstances; j++)
       atts.addElement(new Attribute(""Kernel "" + j));
-    atts.addElement(new Attribute(""Class""));
+    atts.addElement(classAttribute);
     Instances result = new Instances(""Kernel"", atts, 0);
     result.setClassIndex(result.numAttributes() - 1);
 
     // compute matrix
     for (int i = 0; i < instances.numInstances(); i++) {
       double[] k = new double[m_NumTrainInstances + 1];
       
       for (int j = 0; j < m_NumTrainInstances; j++) {
 	double v = m_ActualKernel.eval(-1, j, instances.instance(i));
 	k[j] = v;
       }
       k[k.length - 1] = classes[i];
 
       // create new instance
       Instance in = new DenseInstance(1.0, k);
       result.add(in);    
     }
 
     if (!isFirstBatchDone())
       setOutputFormat(result);
     
     return result;
   }",Buggy,"Fixed bug in KernelFilter: now works with nominal class attributes as well.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug in the `KernelFilter` that prevented it from working correctly with nominal class attributes.

The code diff shows the following changes:

1.  **Storing the Class Attribute:**
    *   `Attribute classAttribute = (Attribute)instances.classAttribute().copy();`
        This line copies the class attribute before it is removed from the instances. This is crucial because the original class attribute is needed later to be added to the result `Instances`.

2.  **Adding the Class Attribute Back:**
    *   `atts.addElement(classAttribute);`
        Instead of creating a new ""Class"" attribute, the code now adds the copied `classAttribute` back to the `atts` vector. This ensures that the class attribute's properties (especially if it's nominal) are preserved.

These changes directly address the issue of handling nominal class attributes. By preserving the original class attribute and adding it back to the result, the filter can now correctly process instances with nominal class attributes. This aligns perfectly with the commit message.

**Buggy**"
weka,36274.json,8072e35dd2cf69509b7d8d77a0a509d3ff3f5202,"@@ -1,82 +1,86 @@
   protected static void addStepJSONtoFlowArray(JSONNode stepArray,
     StepManagerImpl stepManager) throws WekaException {
 
     JSONNode step = stepArray.addObjectArrayElement();
     step.addPrimitive(""class"", stepManager.getManagedStep().getClass()
       .getCanonicalName());
     // step.addPrimitive(STEP_NAME, stepManager.getManagedStep().getName());
     JSONNode properties = step.addObject(PROPERTIES);
     try {
       Step theStep = stepManager.getManagedStep();
       BeanInfo bi = Introspector.getBeanInfo(theStep.getClass());
       PropertyDescriptor[] stepProps = bi.getPropertyDescriptors();
 
       for (PropertyDescriptor p : stepProps) {
         if (p.isHidden() || p.isExpert()) {
           continue;
         }
 
         String name = p.getDisplayName();
         Method getter = p.getReadMethod();
         Method setter = p.getWriteMethod();
         if (getter == null || setter == null) {
           continue;
         }
         boolean skip = false;
         for (Annotation a : getter.getAnnotations()) {
           if (a instanceof NotPersistable) {
             skip = true;
             break;
           }
         }
         if (skip) {
           continue;
         }
 
         Object[] args = {};
         Object propValue = getter.invoke(theStep, args);
         if (propValue == null) {
           properties.addNull(name);
         } else if (propValue instanceof Boolean) {
           properties.addPrimitive(name, (Boolean) propValue);
         } else if (propValue instanceof Integer || propValue instanceof Long) {
           properties.addPrimitive(name,
             new Integer(((Number) propValue).intValue()));
         } else if (propValue instanceof Double) {
           properties.addPrimitive(name, (Double) propValue);
         } else if (propValue instanceof Number) {
           properties.addPrimitive(name,
             new Double(((Number) propValue).doubleValue()));
         } else if (propValue instanceof weka.core.converters.Loader) {
           addLoader(name, (weka.core.converters.Loader) propValue, properties);
         } else if (propValue instanceof weka.core.converters.Saver) {
           addSaver(name, (weka.core.converters.Saver) propValue, properties);
         } else if (propValue instanceof OptionHandler) {
           addOptionHandler(name, (OptionHandler) propValue, properties);
         } else if (propValue instanceof Enum) {
           addEnum(name, (Enum) propValue, properties);
+        } else if (propValue instanceof File) {
+          String fString = propValue.toString();
+          fString = fString.replace('\\', '/');
+          properties.addPrimitive(name, fString);
         } else {
           properties.addPrimitive(name, propValue.toString());
         }
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     JSONNode connections = step.addObject(CONNECTIONS);
     for (Map.Entry<String, List<StepManager>> e : stepManager.m_connectedByTypeOutgoing
       .entrySet()) {
       String connName = e.getKey();
       JSONNode connTypeArray = connections.addArray(connName);
       for (StepManager c : e.getValue()) {
         connTypeArray.addArrayElement(c.getName());
       }
     }
 
     if (stepManager.getStepVisual() != null) {
       String coords =
         """" + stepManager.getStepVisual().getX() + "",""
           + stepManager.getStepVisual().getY();
       step.addPrimitive(COORDINATES, coords);
     }
   }",Buggy,"Fixed a bug that affected the parsing of step properties involving files containing Windows separator charactors

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12964 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to parsing step properties, specifically addressing issues with file paths containing Windows separator characters (backslashes).

The code diff introduces a new `else if` block to handle properties of type `File`. Inside this block, the code retrieves the file path as a string, replaces all backslashes (`\`) with forward slashes (`/`), and then adds the modified string to the JSON properties. This replacement of backslashes with forward slashes suggests that the original code was not correctly handling Windows-style file paths, which use backslashes as separators. By converting them to forward slashes, the code ensures compatibility across different operating systems or environments where forward slashes are expected.

The change directly addresses the issue described in the commit message. The code modification is a bug fix because it corrects the incorrect handling of file paths with Windows separators.

**Buggy**"
weka,26684.json,f36aad315fb901cb71922f7ae08459999978c9d9,"@@ -1,65 +1,83 @@
   public Instances getCurve(FastVector predictions, int classIndex) {
 
     if ((predictions.size() == 0) ||
         (((NominalPrediction)predictions.elementAt(0))
          .distribution().length <= classIndex)) {
       return null;
     }
 
     double totPos = 0, totNeg = 0;
     double [] probs = getProbabilities(predictions, classIndex);
 
     // Get distribution of positive/negatives
     for (int i = 0; i < probs.length; i++) {
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(i);
       if (pred.actual() == Prediction.MISSING_VALUE) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with missing class value"");
         continue;
       }
       if (pred.weight() < 0) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with negative weight"");
         continue;
       }
       if (pred.actual() == classIndex) {
         totPos += pred.weight();
       } else {
         totNeg += pred.weight();
       }
     }
 
     Instances insts = makeHeader();
     int [] sorted = Utils.sort(probs);
     TwoClassStats tc = new TwoClassStats(totPos, totNeg, 0, 0);
+    double threshold = 0;
+    double cumulativePos = 0;
+    double cumulativeNeg = 0;
     for (int i = 0; i < sorted.length; i++) {
+
+      if ((i == 0) || (probs[sorted[i]] > threshold)) {
+	tc.setTruePositive(tc.getTruePositive() - cumulativePos);
+	tc.setFalseNegative(tc.getFalseNegative() + cumulativePos);
+	tc.setFalsePositive(tc.getFalsePositive() - cumulativeNeg);
+	tc.setTrueNegative(tc.getTrueNegative() + cumulativeNeg);
+	threshold = probs[sorted[i]];
+	insts.add(makeInstance(tc, threshold));
+	cumulativePos = 0;
+	cumulativeNeg = 0;
+	if (i == sorted.length - 1) {
+	  break;
+	}
+      }
+
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(sorted[i]);
+
       if (pred.actual() == Prediction.MISSING_VALUE) {
-        System.err.println(getClass().getName()
-                           + "" Skipping prediction with missing class value"");
-        continue;
+	System.err.println(getClass().getName()
+			   + "" Skipping prediction with missing class value"");
+	continue;
       }
       if (pred.weight() < 0) {
-        System.err.println(getClass().getName() 
-                           + "" Skipping prediction with negative weight"");
-        continue;
+	System.err.println(getClass().getName() 
+			   + "" Skipping prediction with negative weight"");
+	continue;
       }
       if (pred.actual() == classIndex) {
-        tc.setTruePositive(tc.getTruePositive() - pred.weight());
-        tc.setFalseNegative(tc.getFalseNegative() + pred.weight());
+	cumulativePos += pred.weight();
       } else {
-        tc.setFalsePositive(tc.getFalsePositive() - pred.weight());
-        tc.setTrueNegative(tc.getTrueNegative() + pred.weight());
+	cumulativeNeg += pred.weight();
       }
+
       /*
       System.out.println(tc + "" "" + probs[sorted[i]] 
                          + "" "" + (pred.actual() == classIndex));
       */
-      if ((i != (sorted.length - 1)) &&
+      /*if ((i != (sorted.length - 1)) &&
           ((i == 0) ||  
           (probs[sorted[i]] != probs[sorted[i - 1]]))) {
         insts.add(makeInstance(tc, probs[sorted[i]]));
-      }
+	}*/
     }
     return insts;
   }",Buggy,"Fixed bug in ThresholdCurve, which resulted in one instance being on the wrong side of the threshold.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2134 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the ThresholdCurve class. The diff shows changes in the `getCurve` method, specifically in how the threshold is determined and how true positive, false negative, false positive, and true negative values are updated. The original code iterated through the sorted probabilities and updated the TwoClassStats object directly in each iteration. The modified code introduces a `threshold` variable and accumulates positive and negative weights (`cumulativePos`, `cumulativeNeg`) until a new threshold is encountered. The TwoClassStats object is then updated based on these accumulated weights. This change suggests a correction in how the threshold curve is calculated, likely addressing an issue where instances were incorrectly classified due to the threshold being miscalculated.

Therefore, the changes indicate a bug fix.

**Buggy**"
weka,27269.json,8e4d3f359b3b7e7182d587aaab8f838daae5c8fa,"@@ -1,11 +1,14 @@
   public double eval(int id1, int id2, Instance inst1) 
     throws Exception {
 	
-    double div = Math.sqrt(super.eval(id1, id1, inst1) * 
-			   super.eval(id2, id2, m_data.instance(id2)));
-    if(div != 0){
+
+    double div = Math.sqrt(super.eval(id1, id1, inst1) * ((m_keys != null)
+                           ? super.eval(id2, id2, m_data.instance(id2))
+                           : super.eval(-1, -1, m_data.instance(id2))));
+
+    if(div != 0){      
       return super.eval(id1, id2, inst1) / div;
     } else {
       return 0;
     }
-  }

+  }",Buggy,"Fixed a bug in NormalizedPolyKernel that resulted in an attempted access to the cache for test instances


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed a bug in NormalizedPolyKernel that resulted in an attempted access to the cache for test instances"" clearly states that the commit addresses a bug. The bug involves an attempt to access the cache for test instances within the `NormalizedPolyKernel` class.

**Code Diff Analysis:**

The code diff modifies the `eval` method within (presumably) the `NormalizedPolyKernel` class. Let's break down the changes:

1.  **Original Code:**

    ```java
    double div = Math.sqrt(super.eval(id1, id1, inst1) *
    super.eval(id2, id2, m_data.instance(id2)));
    if(div != 0){
    return super.eval(id1, id2, inst1) / div;
    } else {
    return 0;
    }
    ```

2.  **Modified Code:**

    ```java
    double div = Math.sqrt(super.eval(id1, id1, inst1) * ((m_keys != null)
                           ? super.eval(id2, id2, m_data.instance(id2))
                           : super.eval(-1, -1, m_data.instance(id2))));

    if(div != 0){
      return super.eval(id1, id2, inst1) / div;
    } else {
      return 0;
    }
    ```

The key change is the introduction of a conditional expression `((m_keys != null) ? ... : ...)` within the calculation of `div`. This condition checks if `m_keys` is not null.

*   If `m_keys` is not null, the original code `super.eval(id2, id2, m_data.instance(id2))` is executed.
*   If `m_keys` *is* null, `super.eval(-1, -1, m_data.instance(id2))` is executed.

**Reasoning:**

The commit message mentions a bug related to accessing the cache for *test instances*. The code change introduces a check for `m_keys != null`. It's highly likely that `m_keys` is related to the training data or keys used for caching. When processing test instances, `m_keys` might be null. In the original code, `super.eval(id2, id2, m_data.instance(id2))` would be called regardless, potentially leading to an invalid cache access (since `id2` is an index related to training data). The modified code uses `super.eval(-1, -1, m_data.instance(id2))` when `m_keys` is null. The `-1` values likely indicate that the cache should *not* be used or that a default evaluation should be performed for test instances. This aligns with the commit message's description of the bug.

**Conclusion:**

The code modification introduces a conditional check that likely prevents an invalid cache access when processing test instances, which directly addresses the bug described in the commit message.

**Buggy**
"
weka,30125.json,4f854d24790f7ba7429f74355b69542147801667,"@@ -1,105 +1,104 @@
   public String toSummaryString() {
 
     StringBuffer result = new StringBuffer();
     result.append(""Relation Name:  "").append(relationName()).append('\n');
     result.append(""Num Instances:  "").append(numInstances()).append('\n');
     result.append(""Num Attributes: "").append(numAttributes()).append('\n');
     result.append('\n');
 
     result.append(Utils.padLeft("""", 5)).append(Utils.padRight(""Name"", 25));
     result.append(Utils.padLeft(""Type"", 5)).append(Utils.padLeft(""Nom"", 5));
     result.append(Utils.padLeft(""Int"", 5)).append(Utils.padLeft(""Real"", 5));
     result.append(Utils.padLeft(""Missing"", 12));
     result.append(Utils.padLeft(""Unique"", 12));
     result.append(Utils.padLeft(""Dist"", 6)).append('\n');
     Instances temp = new Instances(this);
     int total = temp.numInstances();
     for (int i = 0; i < numAttributes(); i++) {
       Attribute a = attribute(i);
       temp.sort(i);
       int intCount = 0, realCount = 0, missingCount = 0;
       int distinctCount = 0, uniqueCount = 0, currentCount = 0;
       double prev = Instance.missingValue();
       for (int j = 0; j < temp.numInstances(); j++) {
 	Instance current = temp.instance(j);
 	if (current.isMissing(i)) {
 	  missingCount = temp.numInstances() - j;
 	  break;
 	}
 	if (Utils.eq(current.value(i), prev)) {
 	  currentCount++;
 	} else {
 	  distinctCount++;
 	  if (currentCount == 1) {
 	    uniqueCount++;
 	  }
 	  if (currentCount > 0) {
 	    if (Utils.eq(prev, (double)((int)prev))) {
 	      intCount += currentCount;
 	    } else {
 	      realCount += currentCount;
 	    }
 	  }
 	  currentCount = 1;
 	  prev = current.value(i);
 	}
       }
       if (currentCount == 1) {
 	uniqueCount++;
       }
       if (currentCount > 0) {
 	if (Utils.eq(prev, (double)((int)prev))) {
 	  intCount += currentCount;
 	} else {
 	  realCount += currentCount;
 	}
       }
-      
       result.append(Utils.padLeft("""" + (i + 1), 4)).append(' ');
       result.append(Utils.padRight(a.name(), 25)).append(' ');
-      int percent;
+      long percent;
       switch (a.type()) {
       case Attribute.NOMINAL:
 	result.append(Utils.padLeft(""Nom"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.NUMERIC:
 	result.append(Utils.padLeft(""Num"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.STRING:
 	result.append(Utils.padLeft(""Str"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       default:
 	result.append(Utils.padLeft(""???"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       }
       result.append(Utils.padLeft("""" + missingCount, 5)).append("" /"");
-      percent = 100 * missingCount / total;
+      percent = Math.round(100.0 * missingCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + uniqueCount, 5)).append("" /"");
-      percent = 100 * uniqueCount / total;
+      percent = Math.round(100.0 * uniqueCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + distinctCount, 5)).append(' ');
       result.append('\n');
     }
     return result.toString();
   }",Buggy,"- Fixed small rounding problem in summary method.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@105 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a rounding problem in the summary method. The diff shows changes in the `toSummaryString()` method, specifically in the calculation of percentages. The type of the `percent` variable is changed from `int` to `long`, and the calculation is modified to use `Math.round(100.0 * ... / total)` instead of `100 * ... / total`. This suggests that the original calculation was truncating the decimal part of the percentage, leading to inaccurate results. The use of `Math.round` ensures that the percentage is rounded to the nearest whole number, addressing the rounding problem.

**Buggy**"
weka,24450.json,d1fccde7b3f541c040561dbcfd7c68890d26bfad,"@@ -1,37 +1,37 @@
   public double logDensity(double[] valuePassed) {
     double[] value = valuePassed.clone();
     double logProb = 0;
     // calculate mean subtractions
     double[] subtractedMean = new double[value.length];
     for (int i = 0; i < value.length; i++) {
       subtractedMean[i] = value[i] - mean[i];
     }
-
+    value = subtractedMean.clone();
     double[][] L = this.chol.getL().getArray();
     int n = this.chol.getL().getRowDimension();
     // Solve L*Y = B;
     for (int k = 0; k < this.chol.getL().getRowDimension(); k++) {
       for (int i = 0; i < k; i++) {
         value[k] -= value[i] * L[k][i];
       }
 
       value[k] /= L[k][k];
     }
 
     // Solve L'*X = Y;
     for (int k = n - 1; k >= 0; k--) {
       for (int i = k + 1; i < n; i++) {
         value[k] -= value[i] * L[i][k];
       }
       value[k] /= L[k][k];
     }
 
     // compute dot product
     double innerProduct = 0;
     // do a fast dot product
     for (int i = 0; i < value.length; i++) {
       innerProduct += value[i] * subtractedMean[i];
     }
     logProb = lnconstant - innerProduct * 0.5;
     return logProb;
   }",Buggy,"Bug fixes from Uday

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10459 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Bug fixes from Uday"" indicates that the changes in the diff are intended to fix bugs.

The code modifies the `logDensity` method. Specifically, the line `value = subtractedMean.clone();` is inserted after calculating `subtractedMean`. The original code used the input `value` array directly in subsequent calculations after subtracting the mean. This could lead to unexpected behavior if the caller expected the original `value` array to remain unchanged. By cloning `subtractedMean` into `value`, the original input `value` array is preserved. The dot product calculation uses `subtractedMean`, which is correct.

This change addresses a potential bug where the input `value` array was being modified, which is a side effect that might not be intended.

**Buggy**
"
weka,15350.json,21d3ace55220b46f80de8bfd5ac1a94760fe142b,"@@ -1,39 +1,38 @@
   protected Instances process(Instances instances) throws Exception {
 
     Instances result = getOutputFormat();
 
     for (Instance inst : instances) {
       if (instances.numAttributes() < result.numAttributes()) { // Do we actually need to add an attribute?
         double[] newVals = new double[result.numAttributes()];
         for (int i = 0; i < inst.numValues(); i++) {
           newVals[inst.index(i)] = inst.valueSparse(i);
         }
         String value = """";
         for (int i = 0; i < inst.numAttributes(); i++) {
           if (instances.attribute(i).isNominal() && m_Attributes.isInRange(i) && i != instances.classIndex()) {
             if (Utils.isMissingValue(newVals[i])) {
               value = null;
               break;
             } else {
               value += (value.length() > 0) ? ""_x_"" + instances.attribute(i).value((int) newVals[i]) :
                       instances.attribute(i).value((int) newVals[i]);
             }
           }
         }
         if (value == null) {
           newVals[newVals.length - 1] = Double.NaN;
         } else {
           newVals[newVals.length - 1] = result.attribute(result.numAttributes() - 1).indexOfValue(value);;
         }
-        if (inst instanceof DenseInstance) {
-          result.add(new DenseInstance(inst.weight(), newVals));
-        } else {
-          result.add(new SparseInstance(inst.weight(), newVals));
-        }
+        Instance newInst = inst.copy(newVals);
+        copyValues(newInst, false, inst.dataset(), result);
+        result.add(newInst);
       } else {
+        copyValues(inst, false, inst.dataset(), result);
         result.add(inst);
       }
     }
 
     return result;
   }",Buggy,"Bug fix: CartesianProduct should now work correctly with string and relational attributes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15075 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the CartesianProduct functionality, specifically addressing issues with string and relational attributes.

The code diff modifies the `process` method. The original code iterates through the attributes of the input instance and constructs a string `value` by concatenating the values of nominal attributes within a specified range. If any of these values are missing, the `value` is set to `null`, and the corresponding value in the new instance is set to `NaN`. The new code replaces the original code with a copy of the instance, and then calls the `copyValues` method.

The change in the code suggests an attempt to correctly handle different attribute types (string and relational) within the CartesianProduct operation. The original implementation might have had issues with these attribute types, leading to incorrect results or exceptions. The new code copies the instance and calls the `copyValues` method, which likely handles the different attribute types correctly.

Therefore, the changes indicate a bug fix.

**Buggy**
"
weka,32640.json,f8fa07eb4f3b6e38a4952f907df8574bab292919,"@@ -1,3 +1,3 @@
   public static synchronized void loadPackages(boolean verbose) {
-    loadPackages(verbose, true, true);
+    loadPackages(verbose, false, true);
   }",Buggy,"Standard loadPackages() call was not invoking full class discovery due to a bug introduced with the latest changes - fixed.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10392 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the `loadPackages()` call was not invoking full class discovery due to a bug introduced with the latest changes, and that this bug has been fixed. The diff shows a change in the `loadPackages()` method call, specifically changing the second argument from `true` to `false`. This argument likely controls whether full class discovery is performed. The change from `true` to `false` suggests that the previous value was incorrect and causing the issue described in the commit message. Therefore, this change is likely a bug fix.

Reasoning:
1. **Commit Message:** The commit message explicitly mentions a bug related to full class discovery in the `loadPackages()` method.
2. **Code Diff:** The code diff shows a change in the `loadPackages()` method call, modifying a boolean argument that likely controls full class discovery.
3. **Alignment:** The code change directly addresses the issue described in the commit message. The change from `true` to `false` suggests a correction of a previous incorrect value that was preventing full class discovery.

Conclusion: **Buggy**
"
weka,32072.json,e68cbb019cd730299b6a056ba88d78a04e771a90,"@@ -1,18 +1,17 @@
   protected void initialize() {
-    String part;
+    String part = """";
     URLClassLoader sysLoader;
     URL[] urls;
 
     m_Cache = new Hashtable<String, HashSet<String>>();
 
     sysLoader = (URLClassLoader) getClass().getClassLoader();
     urls = sysLoader.getURLs();
     for (URL url : urls) {
+      part = url.toString();
       if (VERBOSE) {
         System.out.println(""Classpath-part: "" + part);
       }
-
-      part = url.toString();
       initFromClasspathPart(part);
     }
   }",Buggy,"Fixed a compilation problem that occurs when VERBOSE is set to true.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11202 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Java code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states ""Fixed a compilation problem that occurs when VERBOSE is set to true."" This strongly suggests a bug fix related to the `VERBOSE` flag.

2.  **Code Diff Analysis:**
    *   The code diff involves the `initialize()` method.
    *   The key change is the initialization of the `part` variable to `""""` at the beginning of the method: `String part = """";`.
    *   The original code declared `part` but didn't initialize it.
    *   The code then enters a loop that iterates through URLs obtained from the class loader.
    *   Inside the loop, there's a conditional block: `if (VERBOSE) { System.out.println(""Classpath-part: "" + part); }`.
    *   The original code had the `if (VERBOSE)` block *before* `part = url.toString();`. This means that if `VERBOSE` was true, the code would attempt to print the value of `part` *before* it had been assigned a value. This would lead to a compilation error or runtime error if `part` was used uninitialized.

3.  **Alignment and Bug Fix Indication:**
    *   The code change directly addresses the issue described in the commit message. By initializing `part` to an empty string, the code avoids the compilation error or runtime error that would occur when `VERBOSE` is true and the `System.out.println` statement is executed before `part` is assigned a value.
    *   The change is a clear fix for a bug related to the `VERBOSE` flag.

**Conclusion:**

The code change aligns perfectly with the commit message, and the change resolves a potential compilation or runtime error related to the `VERBOSE` flag. Therefore, the commit indicates a bug fix.

**Buggy**"
weka,21163.json,65f17814a1d6158b5dc0b85d70c32f30efca4908,"@@ -1,18 +1,26 @@
   public EventSetDescriptor [] getEventSetDescriptors() {
     try {
       EventSetDescriptor [] esds =  
       { new EventSetDescriptor(PredictionAppender.class, 
-			       ""dataSet"",
-			       DataSourceListener.class,
-			       ""acceptDataSet""),
-	new EventSetDescriptor(DataSource.class, 
-			       ""instance"",
-			       InstanceListener.class,
-			       ""acceptInstance"")
-         };
+	  ""dataSet"",
+	  DataSourceListener.class,
+      ""acceptDataSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""instance"",
+	  InstanceListener.class,
+      ""acceptInstance""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""trainingSet"",
+	  TrainingSetListener.class,
+      ""acceptTrainingSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""testSet"",
+	  TestSetListener.class,
+      ""acceptTestSet"")
+      };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a small bug and added event sets for training at test set events.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3814 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed a small bug and added event sets for training at test set events"" suggests two types of changes: a bug fix and the addition of new event sets.

The code diff shows the addition of two new `EventSetDescriptor` objects: one for ""trainingSet"" and another for ""testSet"". These new event sets are associated with `TrainingSetListener` and `TestSetListener` respectively. This part of the diff aligns with the ""added event sets for training at test set events"" part of the commit message.

The code also includes a try-catch block that prints the stack trace if an exception occurs, and returns null. This is a general error handling mechanism, and doesn't necessarily indicate a bug fix. However, the commit message explicitly mentions ""fixed a small bug"". The addition of the new event sets could have exposed a pre-existing bug, or the bug could have been related to how event sets were handled previously. Without more context, it's difficult to pinpoint the exact nature of the bug. However, given the commit message's explicit mention of a bug fix, and the addition of new event sets which might have triggered the bug, it's reasonable to assume that the changes are related to fixing a bug.

**Buggy**
"
weka,32626.json,1d67da514cfac522915affd97cf003f12258f2f5,"@@ -1,46 +1,47 @@
   protected static void establishMirror() {
     if (m_offline) {
       return;
     }
 
     try {
       String mirrorListURL =
-        ""http://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
+        ""https://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
 
       URLConnection conn = null;
       URL connURL = new URL(mirrorListURL);
 
       if (PACKAGE_MANAGER.setProxyAuthentication(connURL)) {
         conn = connURL.openConnection(PACKAGE_MANAGER.getProxy());
       } else {
         conn = connURL.openConnection();
       }
 
       conn.setConnectTimeout(10000); // timeout after 10 seconds
       conn.setReadTimeout(10000);
 
       BufferedReader bi =
         new BufferedReader(new InputStreamReader(conn.getInputStream()));
 
       REP_MIRROR = bi.readLine();
 
       bi.close();
       if (REP_MIRROR != null && REP_MIRROR.length() > 0) {
         // use the mirror if it is different from the primary repo
         // and the user hasn't specified an explicit repo via the
         // property
         if (!REP_MIRROR.equals(PRIMARY_REPOSITORY) && !USER_SET_REPO) {
 
           log(weka.core.logging.Logger.Level.INFO,
             ""[WekaPackageManager] Package manager using repository mirror: ""
               + REP_MIRROR);
 
           REP_URL = new URL(REP_MIRROR);
         }
       }
     } catch (Exception ex) {
+      ex.printStackTrace();
       log(weka.core.logging.Logger.Level.WARNING,
         ""[WekaPackageManager] The repository meta data mirror file seems ""
           + ""to be unavailable ("" + ex.getMessage() + "")"");
     }
   }",Buggy,"Package mirror settings now accessed via https instead of http. Fixes an error that gets printed to the log

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14973 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a change from `http` to `https` for accessing package mirror settings, aiming to fix an error logged previously. The diff confirms this change in the `mirrorListURL` from `http` to `https`. Additionally, the catch block now prints the stack trace. This suggests that the previous `http` access might have been failing, leading to the logged error, and the change to `https` is intended to resolve this. The printing of the stack trace in the catch block can help in debugging if the connection fails.

**Buggy**"
weka,22555.json,6b82052500cf75da83431ce041e21658720d1816,"@@ -1,21 +1,21 @@
   public void layoutEditor() {
     m_stepToBlockBox.setEditable(true);
 
     StepManager sm = getStepToEdit().getStepManager();
     List<StepManagerImpl> flowSteps =
       getMainPerspective().getCurrentLayout().getFlow().getSteps();
     for (StepManagerImpl smi : flowSteps) {
       m_stepToBlockBox.addItem(smi.getName());
     }
 
     JPanel p = new JPanel(new BorderLayout());
-    p.setBorder(BorderFactory.createTitledBorder(""Choose class attribute""));
+    p.setBorder(BorderFactory.createTitledBorder(""Choose step to wait for""));
     p.add(m_stepToBlockBox, BorderLayout.NORTH);
 
     add(p, BorderLayout.CENTER);
 
     String userSelected = ((Block) getStepToEdit()).getStepToWaitFor();
     if (userSelected != null) {
       m_stepToBlockBox.setSelectedItem(userSelected);
     }
   }",Buggy,"Fixed a bug in a label

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12606 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fixed a bug in a label"" is quite vague. It suggests that a bug related to a label (likely a UI label) has been corrected.  The lack of detail makes it difficult to immediately understand the nature of the bug.

**2. Code Diff Analysis:**

The code diff primarily involves changes to the `layoutEditor()` method, which seems to be responsible for configuring the layout of an editor panel. The key changes are:

*   **Border Title Change:** The border title of a JPanel is changed from ""Choose class attribute"" to ""Choose step to wait for"".
*   **Functionality Alignment:** The code iterates through steps and adds their names to a combo box (`m_stepToBlockBox`). The combo box is then used to select a step to wait for.

**3. Relevance Assessment:**

The change in the border title from ""Choose class attribute"" to ""Choose step to wait for"" strongly suggests that the original label was incorrect or misleading. This indicates a bug fix related to the user interface and the intended functionality. The code now correctly reflects the purpose of selecting a ""step to wait for"".

**4. Conclusion:**

Based on the analysis, the commit message and the code diff are aligned. The change in the label directly addresses a UI bug where the label was misleading or incorrect. Therefore, the changes indicate a bug fix.

**Buggy**
"
weka,10201.json,2b9d972bd34de6f7911f39d55f81ab6194452ebe,"@@ -1,14 +1,16 @@
   public Instance getNextInstance(Instances structure) throws IOException {
     if (getRetrieval() == BATCH) {
       throw new IOException(""Cannot mix getting instances in both incremental and batch modes"");
     }
     m_structure = structure;
     setRetrieval(INCREMENTAL);
 
     //Have we read all the data?
     if ((m_currentTimeSlot == 0 && m_dataSet.TDIM == 0) || (m_currentTimeSlot < m_dataSet.TDIM)) {
-      return new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      Instance inst = new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      inst.setDataset(m_structure);
+      return inst;
      } else {
       return null;
     }
   }",Buggy,"Fixed bug in incremental loading mode: instance was not assigned to a dataset.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to incremental loading mode where an instance was not being assigned to a dataset.

The code diff shows that in the `getNextInstance` method, when retrieving instances in incremental mode, a new `SparseInstance` is created. The original code returned this instance directly. The modified code now assigns the `m_structure` (which represents the dataset) to the newly created instance using `inst.setDataset(m_structure)` before returning it.

This change directly addresses the issue described in the commit message, confirming that the bug where the instance was not assigned to a dataset has been fixed.

**Buggy**
"
weka,23962.json,f48889f6b6237d7493f1e78fd7c3c417b636f27f,"@@ -1,20 +1,24 @@
   protected String doubleToString(double d, int prec) {
     String result;
     int currentPrec;
     int i;
 
     result = Utils.doubleToString(d, prec);
 
+    if (prec <= 0 || Double.isInfinite(d) || Double.isNaN(d)) {
+      return result;
+    }
+
     // decimal point?
     if (result.indexOf(""."") == -1) {
       result += ""."";
     }
 
     // precision so far?
     currentPrec = result.length() - result.indexOf(""."") - 1;
     for (i = currentPrec; i < prec; i++) {
       result += ""0"";
     }
 
     return result;
   }",Buggy,"Fixed bug in output of experiment results occurring when number was NaN (or infinite). In that case, no decimal point and 0s should be added. Also, decimal point is no longer printed if user requests 0 decimal places (e.g., if the user requests precision 0).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13955 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message explicitly states ""Fixed bug in output of experiment results occurring when number was NaN (or infinite)."" This clearly indicates a bug fix. It also describes the specific issue: when a number is NaN or infinite, the output formatting was incorrect. The message further explains the fix: no decimal point and zeros should be added in such cases, and the decimal point should be omitted if the user requests zero decimal places.

**Code Diff Analysis:**

The code diff modifies the `doubleToString` method. Let's break down the changes:

1.  **`if (prec <= 0 || Double.isInfinite(d) || Double.isNaN(d))`**: This condition checks if the requested precision is zero or less, or if the input `double` value `d` is either infinite or NaN.
2.  **`return result;`**: If the condition in step 1 is true, the method immediately returns the original `result` string (obtained from `Utils.doubleToString(d, prec)`).

The original code appended a decimal point and trailing zeros to the string representation of the double, regardless of whether the number was NaN or infinite, or whether the user requested 0 decimal places. The added `if` statement now prevents this behavior in these specific cases, aligning with the commit message's description of the bug fix.

**Reasoning:**

The commit message explicitly states a bug fix related to the incorrect formatting of NaN or infinite numbers and the handling of zero decimal places. The code diff introduces a condition that specifically addresses these cases by preventing the addition of decimal points and trailing zeros when the input is NaN, infinite, or when zero decimal places are requested. This aligns perfectly with the commit message. The change ensures that the output is correctly formatted according to the specified precision and handles special numerical values appropriately.

**Conclusion:**

**Buggy**
"
weka,17296.json,9e4a9dc3de5bc88ccad6654a7df72b7c8807bb97,"@@ -1,76 +1,86 @@
   private void determineBounds() {
      double value,min,max;
     
     if (m_plotInstances != null && 
 	m_plotInstances.numAttributes() > 0 &&
 	m_plotInstances.numInstances() > 0) {
       // x bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_xIndex).isNominal()) {
 	m_minX = 0;
 	m_maxX = m_plotInstances.attribute(m_xIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_xIndex)) {
 	    value = m_plotInstances.instance(i).value(m_xIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+	
 	m_minX = min; m_maxX = max;
 	if (min == max) {
 	  m_maxX += 0.05;
 	  m_minX -= 0.05;
 	}
       }
 
       // y bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_yIndex).isNominal()) {
 	m_minY = 0;
 	m_maxY = m_plotInstances.attribute(m_yIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	    value = m_plotInstances.instance(i).value(m_yIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
 	m_minY = min; m_maxY = max;
 	if (min == max) {
 	  m_maxY += 0.05;
 	  m_minY -= 0.05;
 	}
       }
       
       // colour bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
 
       for (int i=0;i<m_plotInstances.numInstances();i++) {
 	if (!m_plotInstances.instance(i).isMissing(m_cIndex)) {
 	  value = m_plotInstances.instance(i).value(m_cIndex);
 	  if (value < min) {
 	    min = value;
 	  }
 	  if (value > max) {
 	    max = value;
 	  }
 	}
       }
+
+      // handle case where all values are missing
+      if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
       m_minC = min; m_maxC = max;
     }
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1220 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that occurs when all numeric values are missing. The diff shows that the code has been modified to handle this specific case for the x, y, and color (c) attributes.

Specifically, the code now checks if `min` remains equal to `Double.POSITIVE_INFINITY` after iterating through all instances. This condition implies that all values for the attribute are missing. If this condition is met, both `min` and `max` are set to 0.0. This prevents potential issues like `NaN` or incorrect bounds when all values are missing.

The changes directly address the scenario described in the commit message. The addition of the `if (min == Double.POSITIVE_INFINITY) min = max = 0.0;` blocks clearly handles the case where all numeric values are missing, thus fixing the bug.

**Buggy**"
weka,20037.json,af57a617ece4a265b0e23698d4b5ab07ebd17dc5,"@@ -1,12 +1,13 @@
   public boolean eventGeneratable(String eventName) {
     if (m_listenee == null) {
       return false;
     }
 
     if (m_listenee instanceof EventConstraints) {
-      if (!((EventConstraints)m_listenee).eventGeneratable(""classifier"")) {
+      if (!((EventConstraints)m_listenee).
+	  eventGeneratable(""batchClassifier"")) {
 	return false;
       }
     }
     return true;
   }",Buggy,"Fixed eventGeneratable bug


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1631 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Fixed eventGeneratable bug"" strongly suggests that the commit addresses a bug related to the `eventGeneratable` method.

2.  **Code Diff Analysis:**
    *   The code diff shows a change within the `eventGeneratable` method. Specifically, the argument passed to the `eventGeneratable` method of the `EventConstraints` interface has been changed from `""classifier""` to `""batchClassifier""`.
    *   The original code was checking if the event ""classifier"" was generatable, while the corrected code checks for ""batchClassifier"". This suggests that the original check was incorrect, and the change aims to fix this error.

3.  **Relevance and Bug Fix Indication:**
    *   The commit message directly states a bug fix.
    *   The code change involves correcting the event name being checked, which is a logical correction.
    *   The change likely fixes a scenario where the wrong event was being checked, potentially leading to incorrect behavior or missed events.

**Conclusion:**

The commit message and the code diff strongly suggest that this commit fixes a bug related to event generation. The change involves correcting the event name being checked, indicating a logical correction.

**Buggy**"
weka,17545.json,9d199c8ab7badc5fe3d5f2680f4caf570d1806f2,"@@ -1,132 +1,133 @@
   private void paintData(Graphics gx) {
 
     for (int j=0;j<m_plots.size();j++) {
       PlotData2D temp_plot = (PlotData2D)(m_plots.elementAt(j));
 
       for (int i=0;i<temp_plot.m_plotInstances.numInstances();i++) {
 	if (temp_plot.m_plotInstances.instance(i).isMissing(m_xIndex) ||
 	    temp_plot.m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	} else {
 	  double x = (temp_plot.m_pointLookup[i][0] + 
 		      temp_plot.m_pointLookup[i][2]);
 	  double y = (temp_plot.m_pointLookup[i][1] + 
 		      temp_plot.m_pointLookup[i][3]);
 
 	  double prevx = 0;
 	  double prevy = 0;
 	  if (i > 0) {
 	    prevx = (temp_plot.m_pointLookup[i - 1][0] + 
 			temp_plot.m_pointLookup[i - 1][2]);
 	    prevy = (temp_plot.m_pointLookup[i - 1][1] + 
 			temp_plot.m_pointLookup[i - 1][3]);
 	  }
 
 	  int x_range = (int)x - m_XaxisStart;
 	  int y_range = (int)y - m_YaxisStart;
 
 	  if (x_range >= 0 && y_range >= 0) {
 	    if (m_drawnPoints[x_range][y_range] == i 
 		|| m_drawnPoints[x_range][y_range] == 0
 		|| temp_plot.m_displayAllPoints == true) {
 	      m_drawnPoints[x_range][y_range] = i;
 	      if (temp_plot.m_plotInstances.attribute(m_cIndex).isNominal()) {
 		if (temp_plot.m_plotInstances.attribute(m_cIndex).numValues() >
 		    m_colorList.size() && 
 		    !temp_plot.m_useCustomColour) {
 		  extendColourMap(temp_plot.m_plotInstances.
 				  attribute(m_cIndex).numValues());
 		}
 
 		Color ci;
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  ci = Color.gray;
 		} else {
 		  int ind = (int)temp_plot.m_plotInstances.instance(i).
 		    value(m_cIndex);
 		  ci = (Color)m_colorList.elementAt(ind);
 		}
 
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);	    
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
-
+		       drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+				     temp_plot.m_shapeType[i],gx);
 		    } else {
-		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      } else {
 		double r;
 		Color ci = null;
 		if (!temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  r = (temp_plot.m_plotInstances.instance(i).
 		       value(m_cIndex) - m_minC) / (m_maxC - m_minC);
 		  r = (r * 240) + 15;
 		  ci = new Color((int)r,150,(int)(255-r));
 		} else {
 		  ci = Color.gray;
 		}
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      }
 	    }
 	  }
 	}
       }
     }
   }",Buggy,"Fixed bug that caused some lines to be drawn when visualizing
classifier errors.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@977 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug that caused unwanted lines to be drawn during the visualization of classifier errors.

The diff shows a modification within the `paintData` method, specifically in the section that handles drawing data points when `temp_plot.m_shapeType[i]` is not `CONST_AUTOMATIC_SHAPE` and `temp_plot.m_connectPoints[i]` is true. The original code had a call to `drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i], temp_plot.m_shapeType[i],gx);` in the else block, which was incorrect. The corrected code moves this call into the if block.

This change suggests that the previous code was not correctly handling the drawing of lines between points, potentially leading to incorrect or unwanted lines being drawn. The fix ensures that the `drawDataPoint` method is called with the correct parameters, resolving the bug described in the commit message.

**Buggy**"
weka,30087.json,4eeaa9f5f013cff1e7007f66a4529ac662b22751,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
     for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
-    if (m_ClassIndex >= position) {
+    if (m_ClassIndex > position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt(): class index was incremented when class attribute was replaced.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12100 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `replaceAttributeAt()` method related to the class index. Specifically, the message states that the class index was incorrectly incremented when a class attribute was replaced.

The code diff shows a change in the condition for incrementing `m_ClassIndex`. The original condition was `m_ClassIndex >= position`, which has been changed to `m_ClassIndex > position`.

Reasoning:

The original condition `m_ClassIndex >= position` would increment `m_ClassIndex` even when the attribute being replaced was at the same position as the class index. This is incorrect because replacing the class attribute should not increment the class index; instead, it should leave it unchanged. The corrected condition `m_ClassIndex > position` only increments `m_ClassIndex` when the replaced attribute is at a position *before* the class attribute, which is the correct behavior.

Therefore, the change in the code directly addresses the bug described in the commit message.

**Buggy**"
weka,30087.json,e4ae148238732024ecdbbd5e7b3ec87d4457ebb0,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
-    for (int i = position; i < m_Attributes.size(); i++) {
+    for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
     if (m_ClassIndex >= position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt() in Instances that I introduced in the modifications I made today.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11339 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the `replaceAttributeAt()` method of the `Instances` class. The message explicitly states that the bug was introduced in recent modifications.

The diff shows a change in the loop condition from `i = position` to `i = position + 1` in the `replaceAttributeAt()` method. This suggests that the original loop was likely skipping an attribute when replacing it, leading to incorrect state in the `m_Attributes` and `m_NamesToAttributeIndices` data structures. The fix ensures that all attributes after the replaced attribute are correctly processed and updated in the new list and map. The setting of missing values and the class index adjustment also look like they could have been affected by the original bug.

Therefore, the change addresses a bug in the logic of the `replaceAttributeAt()` method.

**Buggy**"
weka,29873.json,c5e5a38b7cb456c56a091a2581ec85b5e9c97afa,"@@ -1,16 +1,16 @@
   public double convictionForRule(AprioriItemSet premise,
 				   AprioriItemSet consequence,
 				   int premiseCount,
 				   int consequenceCount) {
     double num = 
-      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) *
+      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) /
        (double)m_totalTransactions;
     double denom = 
       ((premiseCount - consequence.m_counter)+1);
     
     if (num < 0 || denom < 0) {
       System.err.println(""*** ""+num+"" ""+denom);
       System.err.println(""premis count: ""+premiseCount+"" consequence count ""+consequenceCount+"" total trans ""+m_totalTransactions);
     }
     return num / denom;
   }",Buggy,"Fixed bug in the computation of the conviction measure


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2297 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Fixed bug in the computation of the conviction measure"" suggests a bug fix related to the calculation of the conviction measure.

The code diff modifies the `convictionForRule` method. Specifically, the numerator calculation is changed from `(double)premiseCount * (double)(m_totalTransactions - consequenceCount) * (double)m_totalTransactions` to `(double)premiseCount * (double)(m_totalTransactions - consequenceCount) / (double)m_totalTransactions`. This change involves replacing a multiplication with a division by `m_totalTransactions`.

The original code likely had a bug where the numerator was incorrectly multiplied by `m_totalTransactions` instead of dividing by it. The corrected code now divides by `m_totalTransactions`, which is the correct operation for calculating the conviction measure.

The added `if` block with `System.err.println` statements is likely for debugging purposes to check for negative values in the numerator or denominator during the calculation. This further supports the idea of a bug fix.

Therefore, the code change aligns with the commit message, indicating a bug fix in the computation of the conviction measure.

**Buggy**"
weka,25075.json,51f46d90e1a9e13dbc38b52a664024090bef89d4,"@@ -1,68 +1,98 @@
   protected void adjustCanopies(double[] densities) {
     if (m_numClustersRequested < 0) {
       assignCanopiesToCanopyCenters();
 
       m_trainingData = new Instances(m_canopies, 0);
       return;
     }
 
     // more canopies than requested?
     if (m_canopies.numInstances() > m_numClustersRequested) {
       int[] sortedIndexes = Utils.stableSort(densities);
 
       Instances finalCanopies = new Instances(m_canopies, 0);
       int count = 0;
       for (int i = sortedIndexes.length - 1; count < m_numClustersRequested; i--) {
         finalCanopies.add(m_canopies.instance(sortedIndexes[i]));
         count++;
       }
 
       m_canopies = finalCanopies;
+      List<double[][]> tempCanopyCenters = new ArrayList<double[][]>();
+      List<double[]> tempT2Dists = new ArrayList<double[]>();
+      List<double[]> tempMissings = new ArrayList<double[]>();
+
+      // make sure that the center sums, densities and missing counts are
+      // aligned with the new canopy list
+      count = 0;
+      for (int i = sortedIndexes.length - 1; count < finalCanopies
+        .numInstances(); i--) {
+        tempCanopyCenters.add(m_canopyCenters.get(sortedIndexes[i]));
+        tempT2Dists.add(m_canopyT2Density.get(sortedIndexes[i]));
+        tempMissings.add(m_canopyNumMissingForNumerics.get(sortedIndexes[i]));
+        count++;
+      }
+      m_canopyCenters = tempCanopyCenters;
+      m_canopyT2Density = tempT2Dists;
+      m_canopyNumMissingForNumerics = tempMissings;
+
     } else if (m_canopies.numInstances() < m_numClustersRequested
       && m_trainingData != null && m_trainingData.numInstances() > 0) {
 
       // make up the difference with randomly selected instances (if possible)
       Random r = new Random(getSeed());
       for (int i = 0; i < 10; i++) {
         r.nextInt();
       }
       HashMap<DecisionTableHashKey, Integer> initC = new HashMap<DecisionTableHashKey, Integer>();
       DecisionTableHashKey hk = null;
 
       // put the existing canopies in the lookup
       for (int i = 0; i < m_canopies.numInstances(); i++) {
         try {
           hk = new DecisionTableHashKey(m_canopies.instance(i),
             m_canopies.numAttributes(), true);
 
           initC.put(hk, null);
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
 
       for (int j = m_trainingData.numInstances() - 1; j >= 0; j--) {
         int instIndex = r.nextInt(j + 1);
         try {
           hk = new DecisionTableHashKey(m_trainingData.instance(instIndex),
             m_trainingData.numAttributes(), true);
         } catch (Exception e) {
           e.printStackTrace();
         }
         if (!initC.containsKey(hk)) {
-          m_canopies.add(m_trainingData.instance(instIndex));
+          Instance newInstance = m_trainingData.instance(instIndex);
+          m_canopies.add(newInstance);
+
+          double[] density = new double[1];
+          density[0] = 1.0;
+          m_canopyT2Density.add(density);
+
+          double[][] center = new double[newInstance.numAttributes()][0];
+          double[] numMissingNumerics = new double[newInstance.numAttributes()];
+          updateCanopyCenter(newInstance, center, numMissingNumerics);
+          m_canopyCenters.add(center);
+          m_canopyNumMissingForNumerics.add(numMissingNumerics);
+
           initC.put(hk, null);
         }
         m_trainingData.swap(j, instIndex);
 
         if (m_canopies.numInstances() == m_numClustersRequested) {
           break;
         }
       }
     }
 
     assignCanopiesToCanopyCenters();
 
     // save memory
     m_trainingData = new Instances(m_canopies, 0);
   }",Buggy,"Fixed a bug that affected aggregation of Canopy clusterers - canopy center sums were not getting adjusted after the final number of canopies was determined.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10597 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the aggregation of Canopy clusterers, specifically that canopy center sums were not being adjusted after the final number of canopies was determined.

The code diff shows that when the number of canopies exceeds the requested number of clusters, the code now reorders `m_canopyCenters`, `m_canopyT2Density`, and `m_canopyNumMissingForNumerics` to align with the new canopy list. This reordering ensures that the canopy center sums are correctly associated with the final set of canopies, which directly addresses the bug described in the commit message.

Additionally, when the number of canopies is less than the requested number of clusters, the code adds new instances to `m_canopies` and also updates `m_canopyT2Density`, `m_canopyCenters`, and `m_canopyNumMissingForNumerics` accordingly. This ensures that the canopy center sums are properly adjusted when new canopies are added.

Therefore, the code changes align with the commit message and indicate a bug fix.

**Buggy**"
weka,37063.json,2a4806e3c2bd860a4e7b0496b5b05ff72dfdf46b,"@@ -1,84 +1,85 @@
   public void start() throws WekaException {
     if (getStepManager().numOutgoingConnections() > 0) {
       weka.datagenerators.DataGenerator generator = getDataGenerator();
       if (getStepManager()
         .numOutgoingConnectionsOfType(StepManager.CON_DATASET) > 0) {
         getStepManager().processing();
         StringWriter output = new StringWriter();
         try {
           generator.setOutput(new PrintWriter(output));
           getStepManager().statusMessage(""Generating..."");
           getStepManager().logBasic(""Generating data"");
           weka.datagenerators.DataGenerator.makeData(generator,
             generator.getOptions());
           Instances instances =
             new Instances(new StringReader(output.toString()));
 
           if (!isStopRequested()) {
             Data outputData = new Data(StepManager.CON_DATASET, instances);
             getStepManager().outputData(outputData);
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         if (isStopRequested()) {
           getStepManager().interrupted();
         } else {
           getStepManager().finished();
         }
       } else {
         // streaming case
         try {
           if (!generator.getSingleModeFlag()) {
             throw new WekaException(""Generator does not support ""
               + ""incremental generation, so cannot be used with ""
               + ""outgoing 'instance' connections"");
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         String stm =
           getName() + ""$"" + hashCode() + 99 + ""| overall flow throughput -|"";
         m_flowThroughput =
           new StreamThroughput(stm, ""Starting flow..."",
             ((StepManagerImpl) getStepManager()).getLog());
 
         try {
           getStepManager().logBasic(""Generating..."");
           generator.setDatasetFormat(generator.defineDataFormat());
 
           for (int i = 0; i < generator.getNumExamplesAct(); i++) {
+            m_flowThroughput.updateStart();
             getStepManager().throughputUpdateStart();
             if (isStopRequested()) {
               getStepManager().interrupted();
               return;
             }
 
             // over all examples to be produced
             Instance inst = generator.generateExample();
             m_incrementalData.setPayloadElement(StepManager.CON_INSTANCE, inst);
             getStepManager().throughputUpdateEnd();
 
             getStepManager().outputData(m_incrementalData);
             m_flowThroughput.updateEnd(((StepManagerImpl) getStepManager())
               .getLog());
           }
 
           if (isStopRequested()) {
             ((StepManagerImpl) getStepManager()).getLog().statusMessage(
               stm + ""remove"");
             getStepManager().interrupted();
             return;
           }
           m_flowThroughput.finished(((StepManagerImpl) getStepManager())
             .getLog());
 
           // signal end of input
           m_incrementalData.clearPayload();
           getStepManager().throughputFinished(m_incrementalData);
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
     }
   }",Buggy,"Fixed a bug in the overall throughput status

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13209 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to the ""overall throughput status."" The code diff shows a modification within the streaming case of the `start()` method, specifically the addition of `m_flowThroughput.updateStart();` before `getStepManager().throughputUpdateStart();`. This suggests that the start time for throughput calculation wasn't being correctly recorded, leading to inaccurate throughput status. This is a bug fix.

**Buggy**"
weka,33600.json,4fbfba6d434318852d50cf855de7282fd21ceae3,"@@ -1,62 +1,63 @@
   public boolean isCompatibleBaseSystem() throws Exception {
 
     String baseSystemName = m_packageManager.getBaseSystemName();
     String systemVersion = m_packageManager.getBaseSystemVersion().toString();
     // System.err.println(""Base system version "" + systemVersion);
 
-    String dependencies = getPackageMetaDataElement(""Depends"").toString();
+    String dependencies = getPackageMetaDataElement(""Depends"") == null
+      ? null : getPackageMetaDataElement(""Depends"").toString();
     if (dependencies == null) {
       return true;
     }
-
+    
     boolean ok = true;
     StringTokenizer tok = new StringTokenizer(dependencies, "","");
     while (tok.hasMoreTokens()) {
       String nextT = tok.nextToken().trim();
       String[] split = splitNameVersion(nextT);
       if (split[0].startsWith(baseSystemName.toLowerCase())) {
         // check the system version
         if (split[1] != null) {
           if (split.length == 3) {
             VersionPackageConstraint.VersionComparison constraint =
               VersionPackageConstraint.getVersionComparison(split[1]);
             if (!VersionPackageConstraint.checkConstraint(systemVersion,
               constraint, split[2])) {
               ok = false;
               break;
             }
           } else {
             // construct a ""dummy"" package for the base system
             Map<String, String> baseMap = new HashMap<String, String>();
             baseMap.put(""PackageName"", ""weka"");
 
             baseMap.put(""Version"", systemVersion);
             Package basePackage =
               new DefaultPackage(null, m_packageManager, baseMap);
 
             VersionRangePackageConstraint versionRConstraint =
               new VersionRangePackageConstraint(basePackage);
             VersionPackageConstraint.VersionComparison comp1 =
               VersionPackageConstraint.getVersionComparison(split[1]);
             VersionPackageConstraint.VersionComparison comp2 =
               VersionPackageConstraint.getVersionComparison(split[3]);
 
             versionRConstraint.setRangeConstraint(split[2], comp1, split[4],
               comp2);
 
             if (!versionRConstraint.checkConstraint(basePackage)) {
               ok = false;
               break;
             }
           }
           /*
            * int comparisonResult =
            * VersionPackageConstraint.compare(systemVersion, split[2]); ok =
            * versionOK(split[1], comparisonResult); if (!ok) { break; }
            */
         }
       }
     }
 
     return ok;
   }",Buggy,"Fixed a bug in DefaultPackage that would result in npe when checking dependencies for a malformed Description.props (that did not list at least a base Weka dependency).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15152 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `DefaultPackage` when checking dependencies for a malformed `Description.props` file. This file is expected to list at least a base Weka dependency.

The code diff modifies the way the ""Depends"" element from the package metadata is retrieved. Specifically, the code now checks if `getPackageMetaDataElement(""Depends"")` returns null. If it does, the `dependencies` variable is set to null, preventing a potential NullPointerException when `.toString()` is called on a null object.

The original code directly called `.toString()` on the result of `getPackageMetaDataElement(""Depends"")` without checking for null. If `Description.props` was malformed and didn't contain a ""Depends"" element, `getPackageMetaDataElement(""Depends"")` would return null, leading to an NPE when `.toString()` was invoked. The modified code handles this case by explicitly checking for null and assigning null to the `dependencies` variable, thus preventing the NPE.

This change directly addresses the bug described in the commit message by adding a null check to prevent the NPE.

**Buggy**"
weka,17605.json,d6ac3ed81063bb608413aa9d346e6e8e8b868874,"@@ -1,220 +1,221 @@
 	  public void mouseClicked(MouseEvent e) {
 	    
 	    if ((m_sIndex == 2 || m_sIndex == 3) && 
 		(m_createShape || 
 		 (e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK)) {
 	      if (m_createShape) {
 		//then it has been started already.
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
-		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
+		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK &&
+                    !e.isAltDown()) {
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribX(e.getX())));
 		  
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribY(e.getY())));
 		  
 		  m_newMousePos.width = e.getX();
 		  m_newMousePos.height = e.getY();
 		  g.drawLine((int)Math.ceil
 			     (m_plot2D.convertToPanelX
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 2)).
 			       doubleValue())),
 			     
 			     (int)Math.ceil
 			     (m_plot2D.convertToPanelY
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 1)).
 			       doubleValue())),
 			     m_newMousePos.width, m_newMousePos.height);
 		  
 		}
 		else if (m_sIndex == 3) {
 		  //then extend the lines to infinity 
 		  //(100000 or so should be enough).
 		  //the area is selected by where the user right clicks 
 		  //the mouse button
 		  
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 5) {
 		    double cx = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()));
 		    
 		    double cx2 = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue())) - 
 		      cx;
 		    
 		    cx2 *= 50000;
 		    
 		    double cy = Math.ceil
 		      (m_plot2D.
 		       convertToPanelY(((Double)m_shapePoints.
 					elementAt(m_shapePoints.size() - 3)).
 				       doubleValue()));
 		    double cy2 = Math.ceil
 		      (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					  elementAt(m_shapePoints.size() - 1)).
 					  doubleValue())) - cy;
 		    cy2 *= 50000;
 			    
 		    
 		    double cxa = Math.ceil(m_plot2D.convertToPanelX
 					   (((Double)m_shapePoints.
 					     elementAt(3)).
 					    doubleValue()));
 		    double cxa2 = Math.ceil(m_plot2D.convertToPanelX
 					    (((Double)m_shapePoints.
 					      elementAt(1)).
 					     doubleValue())) - cxa;
 		    cxa2 *= 50000;
 		    
 		    
 		    double cya = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(4)).
 			doubleValue()));
 		    double cya2 = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(2)).
 			doubleValue())) - cya;
 		    
 		    cya2 *= 50000;
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cxa2 + cxa)), 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cy2 + cy)), 
 		       m_shapePoints.size() - 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cx2 + cx)), 
 		       m_shapePoints.size() - 2);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cya2 + cya)), 2);
 		    
 		    
 		    //determine how infinity line should be built
 		    
 		    cy = Double.POSITIVE_INFINITY;
 		    cy2 = Double.NEGATIVE_INFINITY;
 		    if (((Double)m_shapePoints.elementAt(1)).
 			doubleValue() > 
 			((Double)m_shapePoints.elementAt(3)).
 			doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt(2)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt(4)).
 			  doubleValue()) {
 			cy = ((Double)m_shapePoints.elementAt(2)).
 			  doubleValue();
 		      }
 		    }
 		    if (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue() > 
 			((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 3)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 1)).doubleValue()) {
 			cy2 = ((Double)m_shapePoints.lastElement()).
 			  doubleValue();
 		      }
 		    }
 		    m_shapePoints.addElement(new Double(cy));
 		    m_shapePoints.addElement(new Double(cy2));
 		    
 		    if (!inPolyline(m_shapePoints, m_plot2D.convertToAttribX
 				    (e.getX()), 
 				    m_plot2D.convertToAttribY(e.getY()))) {
 		      Double tmp = (Double)m_shapePoints.
 			elementAt(m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(m_shapePoints.lastElement(), 
 			 m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(tmp, m_shapePoints.size() - 1);
 		    }
 		    
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		  
 		}
 		else {
 		  //then close the shape
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 7) {
 		    m_shapePoints.addElement(m_shapePoints.elementAt(1));
 		    m_shapePoints.addElement(m_shapePoints.elementAt(2));
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 			   
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		}
 		g.dispose();
 		//repaint();
 	      }
 	      else if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
 		//then this is the first point
 		m_createShape = true;
 		m_shapePoints = new FastVector(17);
 		m_shapePoints.addElement(new Double(m_sIndex));
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribX(e.getX()))); //the new point
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribY(e.getY())));
 		m_newMousePos.width = e.getX();      //the temp mouse point
 		m_newMousePos.height = e.getY();
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
 		g.drawLine((int)Math.ceil
 			   (m_plot2D.convertToPanelX(((Double)m_shapePoints.
 					     elementAt(1)).doubleValue())),
 			   (int)Math.ceil
 			   (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					     elementAt(2)).doubleValue())),
 			   m_newMousePos.width, m_newMousePos.height);
 		g.dispose();
 	      }
 	    }
 	    else {
 	      if ((e.getModifiers() & InputEvent.BUTTON1_MASK) == 
 		  InputEvent.BUTTON1_MASK) {
 		
 		m_plot2D.searchPoints(e.getX(),e.getY(), false);
 	      } else {
 		m_plot2D.searchPoints(e.getX(), e.getY(), true);
 	      }
 	    }
 	  }",Buggy,"Fixed bug that prevented polyline and polygon selection options from working with single button mice. The Alt key can now be held down to simulate button 2


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for an issue where polyline and polygon selection options were not working correctly with single-button mice. The solution implemented involves using the Alt key as a substitute for the second mouse button.

The code diff modifies the `mouseClicked` method to check if the Alt key is pressed during the creation of shapes (polylines or polygons). Specifically, the condition `(e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK` is extended to `(e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK && !e.isAltDown()`. This change ensures that when creating shapes, the primary mouse button (BUTTON1_MASK) is pressed, and the Alt key is *not* pressed. If the Alt key is pressed, it's treated as a different mouse button action (simulating button 2).

This modification directly addresses the problem described in the commit message. The original code likely assumed the availability of a second mouse button for completing or modifying shapes, which caused issues for users with single-button mice. The added Alt key check provides an alternative input method, resolving the bug.

**Buggy**
"
weka,23551.json,0f79454f25618e64781b12a24a482ef8a0441bca,"@@ -1,44 +1,44 @@
   public Instance generateExample() throws Exception {
     Instance result;
     Random rand;
     double x;
     double y;
     double[] atts;
     Instance inst;
 
-    result = null;
     rand = getRandom();
 
     if (m_DatasetFormat == null) {
       throw new Exception(""Dataset format not defined."");
     }
 
     // random x
     x = rand.nextDouble();
     // fit into range
     x = x * (getMaxRange() - getMinRange()) + getMinRange();
 
     // generate y
     atts = new double[1];
     atts[0] = x;
     inst = new DenseInstance(1.0, atts);
+    inst.setDataset(m_RawData);
     m_Filter.input(inst);
     m_Filter.batchFinished();
     inst = m_Filter.output();
 
     // noise
     y = inst.value(1) + getAmplitude() * m_NoiseRandom.nextGaussian()
       * getNoiseRate() * getNoiseVariance();
 
     // generate attributes
     atts = new double[m_DatasetFormat.numAttributes()];
 
     atts[0] = x;
     atts[1] = y;
     result = new DenseInstance(1.0, atts);
 
     // dataset reference
     result.setDataset(m_DatasetFormat);
 
     return result;
   }",Buggy,"Fixed problem that caused assertion to fail when executing unit test.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11504 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a failing unit test assertion. The diff shows a modification where `inst.setDataset(m_RawData);` is added. This line likely sets the dataset for the instance `inst` before it's used in the filter, which could be the cause of the assertion failure in the unit test. Without this line, the filter might be operating on an instance without a defined dataset, leading to unexpected behavior and a failed assertion. This change directly addresses the problem described in the commit message.

**Buggy**
"
weka,21037.json,05cc4c3b0752474605f5ea11f2c85e4a9c3d6043,"@@ -1,16 +1,37 @@
   public boolean eventGeneratable(String eventName) {
 
-    if (!m_listeneeTypes.contains(eventName)) {
-      return false;
+    if (eventName.equals(""instance"")) {
+
+      if (!m_listeneeTypes.contains(eventName)) {
+        return false;
+      }
+
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints
+          && !((EventConstraints) listenee).eventGeneratable(eventName)) {
+          return false;
+        }
+      }
     }
 
-    for (Object listenee : m_listenees.values()) {
-      if (listenee instanceof EventConstraints) {
-        if (!((EventConstraints) listenee).eventGeneratable(eventName)) {
-          return false;
+    if (eventName.equals(""dataSet"") || eventName.equals(""trainingSet"")
+      || eventName.equals(""testSet"")) {
+
+      if (!m_listeneeTypes.contains(""dataSet"")
+        && !m_listeneeTypes.contains(""trainingSet"")
+        && !m_listeneeTypes.contains(""testSet"")) {
+        return false;
+      }
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints) {
+          if (!((EventConstraints) listenee).eventGeneratable(""dataSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""trainingSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""testSet"")) {
+            return false;
+          }
         }
       }
     }
 
     return true;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10149 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to event types produced by `Appender` and a routine that determines event generation eligibility. The code changes modify the `eventGeneratable` method, specifically focusing on the ""instance"", ""dataSet"", ""trainingSet"", and ""testSet"" event types.

The original code simply checked if the event name was contained in `m_listeneeTypes` and then iterated through listeners to check constraints. The modified code introduces specific handling for ""instance"", ""dataSet"", ""trainingSet"", and ""testSet"" events.

For the ""instance"" event, it now explicitly checks if `m_listeneeTypes` contains ""instance"" before iterating through listeners.

For ""dataSet"", ""trainingSet"", and ""testSet"" events, it checks if `m_listeneeTypes` contains any of these three events before iterating through listeners and checking their constraints for all three events.

These changes suggest a bug fix because the original logic might have been insufficient to handle these specific event types correctly. The new logic adds specific checks and constraints for these event types, indicating an attempt to correct a previously flawed behavior.

Therefore, the changes indicate a bug fix.

**Buggy**
"
weka,23139.json,50e14ddcea9246c4b6c83142b493163b1e9b862c,"@@ -1,27 +1,42 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_AttributeEvaluatorEditor.setCapabilitiesFilter(new Capabilities(null));
       m_AttributeSearchEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_AttributeEvaluatorEditor.setCapabilitiesFilter(filterClass);
     m_AttributeSearchEditor.setCapabilitiesFilter(filterClass);
+    
+    m_StartBut.setEnabled(true);
+    // check capabilities...
+    Capabilities currentFilter = m_AttributeEvaluatorEditor.getCapabilitiesFilter();
+    ASEvaluation evaluator = (ASEvaluation) m_AttributeEvaluatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (evaluator != null && currentFilter != null && 
+        (evaluator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)evaluator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5385 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug where the start button's enabled/disabled state wasn't correctly updated when a new dataset was set. The diff shows that the `updateCapabilitiesFilter` method now includes logic to explicitly enable the start button (`m_StartBut.setEnabled(true)`) and then conditionally disable it based on the capabilities of the selected attribute evaluator and the current filter. This aligns with the commit message's description of the bug fix. The added code checks if the evaluator supports the current filter's capabilities and disables the start button if it doesn't. This suggests a logical correction to ensure the start button is only enabled when the selected evaluator can handle the current dataset's capabilities.

**Buggy**
"
xerces2-j,5527.json,2c3c54c0011dbb81897f1f1484b2a1da2d9bce5a,"@@ -1,3 +1,3 @@
     protected boolean versionSupported(String version) {
-        return version.equals(""1.1"");
+        return (version.equals(""1.1"") || version.equals(""1.0""));
     } // versionSupported(String):  boolean",Buggy,"fix for bug 18789; thanks to Neil Delima for the patch


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319325 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (bug 18789) and credits Neil Delima for the patch. The code change modifies the `versionSupported` method to now accept both ""1.1"" and ""1.0"" as supported versions. This suggests that the previous implementation only supported ""1.1"", which was a bug. The change expands the supported versions, aligning with the bug fix mentioned in the commit message.

**Buggy**"
xerces2-j,1404.json,4e11453e19d58d4481d2367028b0cb86c0deafeb,"@@ -1,62 +1,66 @@
 	protected void checkUnboundNamespacePrefixedNode (Node node) throws IOException{
 
 		if (fNamespaces) {
 
    			if (DEBUG) {
 			    System.out.println(""==>serializeNode(""+node.getNodeName()+"") [Entity Reference - Namespaces on]"");
 				System.out.println(""==>Declared Prefix Count: "" + fNSBinder.getDeclaredPrefixCount());
 				System.out.println(""==>Node Name: "" + node.getNodeName());
 				System.out.println(""==>First Child Node Name: "" + node.getFirstChild().getNodeName());
 				System.out.println(""==>First Child Node Prefix: "" + node.getFirstChild().getPrefix());
 				System.out.println(""==>First Child Node NamespaceURI: "" + node.getFirstChild().getNamespaceURI());			
 			}
 
 		
 			Node child, next;
 	        for (child = node.getFirstChild(); child != null; child = next) {
 	            next = child.getNextSibling();
 			    if (DEBUG) {
 			        System.out.println(""==>serializeNode(""+child.getNodeName()+"") [Child Node]"");
 			        System.out.println(""==>serializeNode(""+child.getPrefix()+"") [Child Node Prefix]"");
 	            }    
 	
 		 	    //If a NamespaceURI is not declared for the current
 		 	    //node's prefix, raise a fatal error.
 		 	    String prefix = child.getPrefix();
+                prefix = (prefix == null || 
+                        prefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(prefix);
 		 	    if (fNSBinder.getURI(prefix) == null && prefix != null) {
 					fatalError(""The replacement text of the entity node '"" 
 								+ node.getNodeName()  
 								+ ""' contains an element node '"" 
 								+ child.getNodeName() 
 								+ ""' with an undeclared prefix '"" 
 								+ prefix + ""'."");
 		 	    }	
 
 				if (child.getNodeType() == Node.ELEMENT_NODE) {
 					
 					NamedNodeMap attrs = child.getAttributes();
 					
 					for (int i = 0; i< attrs.getLength(); i++ ) {
 						
 				 	    String attrPrefix = attrs.item(i).getPrefix();
+                        attrPrefix = (attrPrefix == null || 
+                                attrPrefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(attrPrefix);
 				 	    if (fNSBinder.getURI(attrPrefix) == null && attrPrefix != null) {
 							fatalError(""The replacement text of the entity node '"" 
 										+ node.getNodeName()  
 										+ ""' contains an element node '"" 
 										+ child.getNodeName() 
 										+ ""' with an attribute '"" 
 										+ attrs.item(i).getNodeName() 										
 										+ ""' an undeclared prefix '"" 
 										+ attrPrefix + ""'."");
 				 	    }	
 						
 					}	
 
 				}
 					
 				if (child.hasChildNodes()) {
 					checkUnboundNamespacePrefixedNode(child);
 				}	
 	        }
 		}    
 	}",Buggy,"Fixing JIRA Bug #1043:
http://issues.apache.org/jira/browse/XERCESJ-1043

Looking up a namespace URI from a NamespaceContext requires that we pass in the interned
String from the SymbolTable as the prefix. We were failing to do that when checking for unbound
prefixes. Should be fixed now.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320220 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug (XERCESJ-1043) where namespace URIs were not being correctly looked up due to the prefix not being interned using the SymbolTable. The code diff shows that the `prefix` and `attrPrefix` variables are now being interned using `fSymbolTable.addSymbol(prefix)` before being used to look up the URI in `fNSBinder.getURI(prefix)`. This aligns with the commit message's description of the bug and its fix. The added code ensures that the prefix used for namespace lookup is the interned version from the SymbolTable, which is crucial for correct namespace resolution.

**Buggy**"
xerces2-j,8183.json,a01d7a8db8b4fb1ff2a2f8c62736b56bb2d84d63,"@@ -1,12 +1,14 @@
-    public boolean equals(XMLGrammarDescription desc) {
+    public boolean equals(Object descObj) {
+        if(!(descObj instanceof XMLGrammarDescription)) return false;
+        XMLGrammarDescription desc = (XMLGrammarDescription)descObj;
         if (!getGrammarType().equals(desc.getGrammarType())) {
             return false;
         }
         if (fTargetNamespace != null && fTargetNamespace.equals(((XSDDescription)desc).getTargetNamespace())) {
             return true;
         }
         else if (fTargetNamespace == null && ((XSDDescription)desc).getTargetNamespace() == null) {
             return true;
         }
         return false;
     }",Buggy,"fix a couple of bugs with schema preparsing:  make sure the error reporter knows about schemas; and make sure XSDDescription .equals method actually works in contexts such as Hashtables (the parameter should be an Object, not an XMLGrammarDescription).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318470 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the commit fixes a bug in the `equals` method of the `XSDDescription` class. The diff shows that the parameter type of the `equals` method has been changed from `XMLGrammarDescription` to `Object`. This change is necessary to ensure that the `equals` method works correctly in contexts such as `Hashtable`, where the parameter type is `Object`. The code now checks if the input object is an instance of `XMLGrammarDescription` before casting it. This confirms that the change is a bug fix.

**Buggy**
"
xerces2-j,8285.json,f33e487490b168dd0b29e7df099a1832fb79ee79,"@@ -1,84 +1,85 @@
     private int scanMixed(QName element) throws Exception {
 
         int valueIndex = -1;  // -1 is special value for #PCDATA
         int prevNodeIndex = -1;
         boolean starRequired = false;
         int[] valueSeen = new int[32];
         int valueCount = 0;
         boolean dupAttrType = false;
         int nodeIndex = -1;
 
         while (true) {
             if (fValidationEnabled) {
                 for (int i=0; i<valueCount;i++) {
                     if ( valueSeen[i] == valueIndex) {
                         dupAttrType = true;
                         break;
                     }
                 }
             }
             if (dupAttrType && fValidationEnabled) {
                 reportRecoverableXMLError(XMLMessages.MSG_DUPLICATE_TYPE_IN_MIXED_CONTENT,
                                           XMLMessages.VC_NO_DUPLICATE_TYPES,
                                           valueIndex);
                 dupAttrType = false;
 
             }
             else {
                 try {
                     valueSeen[valueCount] = valueIndex;
                 }
                 catch (ArrayIndexOutOfBoundsException ae) {
                     int[] newArray = new int[valueSeen.length*2];
                     System.arraycopy(valueSeen,0,newArray,0,valueSeen.length);
+                    valueSeen = newArray;
                     valueSeen[valueCount] = valueIndex;
                 }
                 valueCount++;
 
                 nodeIndex = fDTDGrammar.addUniqueLeafNode(valueIndex);
             }
 
             checkForPEReference(false);
             if (!fEntityReader.lookingAtChar('|', true)) {
                 if (!fEntityReader.lookingAtChar(')', true)) {
                     reportFatalXMLError(XMLMessages.MSG_CLOSE_PAREN_REQUIRED_IN_MIXED,
                                         XMLMessages.P51_CLOSE_PAREN_REQUIRED,
                                         element.rawname);
                     return -1;
                 }
                 decreaseParenDepth();
                 if (nodeIndex == -1) {
                     nodeIndex = prevNodeIndex;
                 } else if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 if (fEntityReader.lookingAtChar('*', true)) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_ZERO_OR_MORE, nodeIndex);
                 } else if (starRequired) {
                     reportFatalXMLError(XMLMessages.MSG_MIXED_CONTENT_UNTERMINATED,
                                         XMLMessages.P51_UNTERMINATED,
                                         fStringPool.toString(element.rawname),
                                         fDTDGrammar.getContentSpecNodeAsString(nodeIndex));
                     return -1;
                 }
                 return nodeIndex;
             }
             if (nodeIndex != -1) {
                 if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 prevNodeIndex = nodeIndex;
             }
             starRequired = true;
             checkForPEReference(false);
             checkForElementTypeWithPEReference(fEntityReader, ')', fElementRefQName);
             valueIndex = fElementRefQName.rawname;
             if (valueIndex == -1) {
                 reportFatalXMLError(XMLMessages.MSG_ELEMENT_TYPE_REQUIRED_IN_MIXED_CONTENT,
                                     XMLMessages.P51_ELEMENT_TYPE_REQUIRED,
                                     element.rawname);
                 return -1;
             }
         }
 
     } // scanMixed(QName):int",Buggy,"Bug - fix is to assign the array reference


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315690 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Bug - fix is to assign the array reference"" indicates a bug fix related to array handling. The diff shows an `ArrayIndexOutOfBoundsException` being caught, and the fix involves creating a new array with double the size, copying the contents of the old array to the new array, and then assigning the new array to the `valueSeen` variable. This directly addresses the potential out-of-bounds issue, which confirms the bug fix.

**Buggy**"
xerces2-j,5598.json,c08116e59d73beba99136a5c0d0b920e74a6c891,"@@ -1,33 +1,33 @@
     synchronized RangeToken getCaseInsensitiveToken() {
         if (this.icaseCache != null)
             return this.icaseCache;
             
         RangeToken uppers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < this.ranges.length;  i += 2) {
             for (int ch = this.ranges[i];  ch <= this.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     uppers.addRange(ch, ch);
                 else {
                     char uch = Character.toUpperCase((char)ch);
                     uppers.addRange(uch, uch);
                 }
             }
         }
         RangeToken lowers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < uppers.ranges.length;  i += 2) {
             for (int ch = uppers.ranges[i];  ch <= uppers.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     lowers.addRange(ch, ch);
                 else {
-                    char uch = Character.toUpperCase((char)ch);
+                    char uch = Character.toLowerCase((char)ch);
                     lowers.addRange(uch, uch);
                 }
             }
         }
         lowers.mergeRanges(uppers);
         lowers.mergeRanges(this);
         lowers.compactRanges();
 
         this.icaseCache = lowers;
         return lowers;
     }",Buggy,"Fix a problem with range tokens where lower case characters where not added (in a case insensitive mode)

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@827769 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for range tokens in case-insensitive mode where lowercase characters were not being added. The diff shows a change in the `getCaseInsensitiveToken` method. Specifically, within the loop that iterates through the uppercase ranges, the line `char uch = Character.toUpperCase((char)ch);` has been changed to `char uch = Character.toLowerCase((char)ch);`. This change suggests that previously, the code was incorrectly converting characters to uppercase when it should have been converting them to lowercase. This aligns perfectly with the commit message, indicating a bug fix related to case-insensitive handling of range tokens.

**Buggy**"
xerces2-j,2798.json,c12fd694626841e15a23993537a61d1ef75260c4,"@@ -1,3 +1,6 @@
     public TypeInfo getSchemaTypeInfo(){
-      return this;
+        if(needsSyncData()) {
+            synchronizeData();
+        }
+        return this;
     }",Buggy,"A TypeInfo.getTypeName(...) bug fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320508 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in `TypeInfo.getTypeName(...)`. The provided code diff modifies the `getSchemaTypeInfo()` method. The change introduces a check `needsSyncData()` and calls `synchronizeData()` if needed before returning `this`. This suggests that the original implementation might have returned an unsynchronized or outdated `TypeInfo` object under certain conditions, which could be considered a bug. The fix ensures that the `TypeInfo` object is synchronized before being returned, addressing a potential issue with outdated or inconsistent data.

**Buggy**"
xerces2-j,4988.json,860cb3b9c2ad281211d6d52beb871b2439720fa5,"@@ -1,3 +1,6 @@
     public static Element getParent(Element elem) {
-        return (Element)elem.getParentNode();
+        Node parent = elem.getParentNode();
+        if (parent instanceof Element)
+            return (Element)parent;
+        return null;
     } // getParent(Element):Element",Buggy,"Fixed some small bugs. Now we can have a sequence of two elements.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317613 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fixed some small bugs. Now we can have a sequence of two elements."" indicates a bug fix related to handling a sequence of two elements. The original code likely had an issue when dealing with such sequences.

**Code Diff Analysis:**

The code diff modifies the `getParent` method.  The original code directly casted the parent node to an `Element`. The modified code now checks if the parent node is an instance of `Element` before casting. If the parent is not an `Element`, it returns `null`.

**Reasoning:**

The original code would throw a `ClassCastException` if the parent node was not an `Element`. This could happen if the parent node was a `Document`, `DocumentFragment`, or other type of `Node`. The updated code handles this case gracefully by returning `null`. This prevents the exception and ensures that the `getParent` method returns a valid `Element` or `null` if no such parent exists. The commit message mentions fixing a bug, and the code change clearly addresses a potential `ClassCastException`, which is a type of bug. The added null check makes the code more robust. The commit message ""Now we can have a sequence of two elements"" does not seem directly related to the code change. However, the code change fixes a bug.

**Conclusion:**

**Buggy**
"
xerces2-j,4757.json,c7c7140fbc0baf115a90871a337515ebc33c7265,"@@ -1,45 +1,45 @@
     public XMLInputSource resolveEntity(XMLResourceIdentifier resourceIdentifier)
             throws XNIException, IOException {
         
         if (fEntityResolver != null) {
             
             String pubId = resourceIdentifier.getPublicId();
-            String sysId = resourceIdentifier.getExpandedSystemId();
+            String sysId = resourceIdentifier.getLiteralSystemId();
             String baseURI = resourceIdentifier.getBaseSystemId();
             String name = null;
             if (resourceIdentifier instanceof XMLDTDDescription) {
                 name = ""[dtd]"";
             }
             else if (resourceIdentifier instanceof XMLEntityDescription) {
                 name = ((XMLEntityDescription) resourceIdentifier).getEntityName();
             }
             
             // When both pubId and sysId are null, the user's entity resolver
             // can do nothing about it. We'd better not bother calling it.
             // This happens when the resourceIdentifier is a GrammarDescription,
             // which describes a schema grammar of some namespace, but without
             // any schema location hint. -Sg
             if (pubId == null && sysId == null) {
                 return null;
             }
             
             // Resolve using EntityResolver2
             try {
                 InputSource inputSource = 
                     fEntityResolver.resolveEntity(name, pubId, baseURI, sysId);
                 return (inputSource != null) ? createXMLInputSource(inputSource, baseURI) : null;
             }
             // error resolving entity
             catch (SAXException e) {
                 Exception ex = e.getException();
                 if (ex == null) {
                     ex = e;
                 }
                 throw new XNIException(ex);
             }   
         }
         
         // unable to resolve entity
         return null;
         
     } // resolveEntity(XMLResourceIdentifier):XMLInputSource",Buggy,"Fixing a bug. The systemId passed to EntityResolver2.resolveEntity may be an absolute or relative URI.
That is it should be the literal system identifier, not the expanded one which resolved from the base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to how `systemId` is handled in `EntityResolver2.resolveEntity`. It specifies that the literal system identifier, not the expanded one, should be passed.

The code diff replaces `resourceIdentifier.getExpandedSystemId()` with `resourceIdentifier.getLiteralSystemId()`. This change directly addresses the bug described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,1574.json,474a683ae11a84aaa7da8861708189c9d0c5d551,"@@ -1,11 +1,14 @@
     public boolean isNonEscapingElement( String tagName )
     {
         int i;
 
-        if ( _nonEscapingElements == null )
-            return false;
+        if ( _nonEscapingElements == null ) {
+            // non escaping was not set
+            // by default output value for elements as unescaped
+            return true;
+        }
         for ( i = 0 ; i < _nonEscapingElements.length ; ++i )
             if ( _nonEscapingElements[ i ].equals( tagName ) )
                 return true;
         return false;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=2389


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318097 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=2389"" clearly states that this commit is intended to fix a bug. The bugzilla link provides further context, but we can proceed with the analysis based on the diff itself.

**Code Diff Analysis:**

The code diff modifies the `isNonEscapingElement` method. The original code returned `false` if `_nonEscapingElements` was `null`. The modified code now returns `true` if `_nonEscapingElements` is `null`, along with a comment explaining that this means non-escaping was not explicitly set, and therefore the default behavior should be to treat elements as unescaped.

**Reasoning:**

The change in logic suggests a bug fix. The original code likely had an incorrect default behavior when `_nonEscapingElements` was not initialized. Returning `false` in that case could have led to elements being incorrectly escaped when they should not have been. The modified code corrects this by returning `true` when `_nonEscapingElements` is `null`, effectively treating elements as unescaped by default. This aligns with the commit message indicating a bug fix. The added comment further supports this interpretation.

**Conclusion:**

**Buggy**
"
xerces2-j,5596.json,1543bbc70bee5fdf7d5abd54532f9b21e7617de5,"@@ -1,86 +1,86 @@
     protected void intersectRanges(Token token) {
         RangeToken tok = (RangeToken)token;
         if (tok.ranges == null || this.ranges == null)
             return;
         this.icaseCache = null;
         this.sortRanges();
         this.compactRanges();
         tok.sortRanges();
         tok.compactRanges();
 
         int[] result = new int[this.ranges.length+tok.ranges.length];
         int wp = 0, src1 = 0, src2 = 0;
         while (src1 < this.ranges.length && src2 < tok.ranges.length) {
             int src1begin = this.ranges[src1];
             int src1end = this.ranges[src1+1];
             int src2begin = tok.ranges[src2];
             int src2end = tok.ranges[src2+1];
             if (src1end < src2begin) {          // Not overlapped
                                                 // src1: o-----o
                                                 // src2:         o-----o
                                                 // res:  empty
                                                 // Reuse src2
                 src1 += 2;
             } else if (src1end >= src2begin
                        && src1begin <= src2end) { // Overlapped
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // src2:      o----o
                                                 // src2:          o----o
                                                 // src2:  o------------o
-                if (src2begin <= src2begin && src1end <= src2end) {
+                if (src2begin <= src1begin && src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:  o------------o
                                                 // res:     o--------o
                                                 // Reuse src2
                     result[wp++] = src1begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else if (src2begin <= src1begin) {
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // res:     o--o
                                                 // Reuse the rest of src1
                     result[wp++] = src1begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                     src2 += 2;
                 } else if (src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:          o----o
                                                 // res:           o--o
                                                 // Reuse src2
                     result[wp++] = src2begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else {
                                                 // src1:    o--------o
                                                 // src2:      o----o
                                                 // res:       o----o
                                                 // Reuse the rest of src1
                     result[wp++] = src2begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                 }
             } else if (src2end < src1begin) {
                                                 // Not overlapped
                                                 // src1:          o-----o
                                                 // src2: o----o
                 src2 += 2;
             } else {
                 throw new RuntimeException(""Token#intersectRanges(): Internal Error: [""
                                            +this.ranges[src1]
                                            +"",""+this.ranges[src1+1]
                                            +""] & [""+tok.ranges[src2]
                                            +"",""+tok.ranges[src2+1]
                                            +""]"");
             }
         }
         while (src1 < this.ranges.length) {
             result[wp++] = this.ranges[src1++];
             result[wp++] = this.ranges[src1++];
         }
         this.ranges = new int[wp];
         System.arraycopy(result, 0, this.ranges, 0, wp);
                                                 // this.ranges is sorted and compacted.
     }",Buggy,"Fixing JIRA Bug #1224:
http://issues.apache.org/jira/browse/XERCESJ-1224

Correcting a typo in intersectRanges(). Thanks to Dave Brosius 
for catching this error and suggesting how to correct it.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@504431 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a typo in the `intersectRanges()` method, specifically mentioned in the context of JIRA bug XERCESJ-1224. The message also acknowledges Dave Brosius for identifying and suggesting the correction.

The diff shows a change within the `intersectRanges()` method. The condition `if (src2begin <= src2begin && src1end <= src2end)` has been corrected to `if (src2begin <= src1begin && src1end <= src2end)`. This correction directly addresses a logical error where `src2begin` was being compared to itself, which is always true and thus not the intended logic. The corrected condition now compares `src2begin` with `src1begin`, which is logically sound for determining the intersection of ranges.

The change directly fixes a logical error within the range intersection logic.

**Buggy**"
xerces2-j,3365.json,163dc12c725a4e9bc243970c82016cb18998c25c,"@@ -1,34 +1,37 @@
     public TimeZone getTimeZone(int defaultZoneoffset) {
         TimeZone result = null;
         int zoneoffset = getTimezone();
 
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             zoneoffset = defaultZoneoffset;
         }
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             result = TimeZone.getDefault();
         } else {
             // zoneoffset is in minutes. Convert to custom timezone id format.
             char sign = zoneoffset < 0 ? '-' : '+';
             if (sign == '-') {
                 zoneoffset = -zoneoffset;
             }
             int hour = zoneoffset / 60;
             int minutes = zoneoffset - (hour * 60);
 
             // Javadoc for java.util.TimeZone documents max length
             // for customTimezoneId is 8 when optional ':' is not used.
             // Format is 
             // ""GMT"" ('-'|''+') (digit digit?) (digit digit)?
             //                   hour          minutes
             StringBuffer customTimezoneId = new StringBuffer(8);
             customTimezoneId.append(""GMT"");
             customTimezoneId.append(sign);
             customTimezoneId.append(hour);
             if (minutes != 0) {
+                if (minutes < 10) {
+                    customTimezoneId.append('0');
+                }
                 customTimezoneId.append(minutes);
             }
             result = TimeZone.getTimeZone(customTimezoneId.toString());
         }
         return result;
     }",Buggy,"Fixing JIRA Bug #1243:
http://issues.apache.org/jira/browse/XERCESJ-1243

When we're building the TimeZone string if minutes < 10 we need to prepend a zero
to conform to the format expected by TimeZone.getTimeZone().


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@524223 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug (XERCESJ-1243) related to the formatting of the TimeZone string. Specifically, it mentions that when the minutes value is less than 10, a leading zero needs to be prepended to conform to the format expected by `TimeZone.getTimeZone()`.

The code diff shows that a conditional statement `if (minutes < 10)` has been added. Inside this condition, `customTimezoneId.append('0')` is called, which prepends a '0' to the minutes value when it's less than 10. This aligns perfectly with the description in the commit message. The purpose is to ensure that the TimeZone string is correctly formatted, which implies that the previous format was incorrect and causing issues.

Therefore, the changes clearly indicate a bug fix.

**Buggy**"
xerces2-j,4419.json,0414900df065f25d451157d2c0f4d9ed5079d3a8,"@@ -1,13 +1,18 @@
     public void startParameterEntity (String name,
     XMLResourceIdentifier identifier,
     String encoding,
     Augmentations augs) throws XNIException {
         if (DEBUG_EVENTS) {
             System.out.println (""==>startParameterEntity: ""+name);
             if (DEBUG_BASEURI) {
                 System.out.println (""   expandedSystemId: ""+identifier.getExpandedSystemId ());
                 System.out.println (""   baseURI:""+ identifier.getBaseSystemId ());
             }
         }
+        if (augs != null && fInternalSubset != null && 
+            !fInDTDExternalSubset && 
+            Boolean.TRUE.equals(augs.getItem(Constants.ENTITY_SKIPPED))) {
+            fInternalSubset.append(name).append("";\n"");
+        }
         fBaseURIStack.push (identifier.getExpandedSystemId ());
     }",Buggy,"Fixing JIRA Bug #1114:
http://issues.apache.org/jira/browse/XERCESJ-1114

If a parameter entity is skipped the parameter entity reference 
should be written into the internal subset string.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@441668 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug (XERCESJ-1114) where a parameter entity reference was not being written to the internal subset string when the parameter entity was skipped.

The code diff adds a conditional block that checks if the entity was skipped (`Boolean.TRUE.equals(augs.getItem(Constants.ENTITY_SKIPPED))`) and if so, appends the entity name to the `fInternalSubset` string. This aligns directly with the bug description in the commit message. The code change addresses the reported issue by ensuring that skipped parameter entity references are now correctly included in the internal subset string.

**Buggy**"
xerces2-j,2331.json,c8ed9f7617b7aef095f3e03f97aefda48972f356,"@@ -1,178 +1,197 @@
     public short compareTreePosition(Node other) {
         // Questions of clarification for this method - to be answered by the
         // DOM WG.   Current assumptions listed - LM
         // 
         // 1. How do ENTITY nodes compare?  
         //    Current assumption: TREE_POSITION_DISCONNECTED, as ENTITY nodes 
         //    aren't really 'in the tree'
         //
         // 2. How do NOTATION nodes compare?
         //    Current assumption: TREE_POSITION_DISCONNECTED, as NOTATION nodes
         //    aren't really 'in the tree'
         //
         // 3. Are TREE_POSITION_ANCESTOR and TREE_POSITION_DESCENDANT     
         //    only relevant for nodes that are ""part of the document tree""?   
         //     <outer>
         //         <inner  myattr=""true""/>
         //     </outer>
         //    Is the element node ""outer"" considered an ancestor of ""myattr""?
         //    Current assumption: No.                                     
         //
         // 4. How do children of ATTRIBUTE nodes compare (with eachother, or  
         //    with children of other attribute nodes with the same element)    
         //    Current assumption: Children of ATTRIBUTE nodes are treated as if 
         //    they they are the attribute node itself, unless the 2 nodes 
         //    are both children of the same attribute. 
         //
         // 5. How does an ENTITY_REFERENCE node compare with it's children? 
         //    Given the DOM, it should precede its children as an ancestor. 
         //    Given ""document order"",  does it represent the same position?     
         //    Current assumption: An ENTITY_REFERENCE node is an ancestor of its
         //    children.
         //
         // 6. How do children of a DocumentFragment compare?   
         //    Current assumption: If both nodes are part of the same document 
         //    fragment, there are compared as if they were part of a document. 
 
         
         // If the nodes are the same...
         if (this==other) 
           return (TREE_POSITION_SAME_NODE | TREE_POSITION_EQUIVALENT);
         
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         short thisType = this.getNodeType();
         short otherType = other.getNodeType();
 
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         if (thisType == Node.ENTITY_NODE || 
             thisType == Node.NOTATION_NODE ||
             otherType == Node.ENTITY_NODE ||
             otherType == Node.NOTATION_NODE ) {
           return TREE_POSITION_DISCONNECTED; 
         }
 
         // Find the ancestor of each node, and the distance each node is from 
         // its ancestor.
         // During this traversal, look for ancestor/descendent relationships 
         // between the 2 nodes in question. 
         // We do this now, so that we get this info correct for attribute nodes 
         // and their children. 
 
         Node node; 
         Node thisAncestor = this;
         Node otherAncestor = other;
         int thisDepth=0;
         int otherDepth=0;
         for (node=this; node != null; node = node.getParentNode()) {
             thisDepth +=1;
             if (node == other) 
               // The other node is an ancestor of this one.
               return (TREE_POSITION_ANCESTOR | TREE_POSITION_PRECEDING);
             thisAncestor = node;
         }
 
         for (node=other; node!=null; node=node.getParentNode()) {
             otherDepth +=1;
             if (node == this) 
               // The other node is a descendent of the reference node.
               return (TREE_POSITION_DESCENDANT | TREE_POSITION_FOLLOWING);
             otherAncestor = node;
         }
         
        
         Node thisNode = this;
         Node otherNode = other;
 
         int thisAncestorType = thisAncestor.getNodeType();
         int otherAncestorType = otherAncestor.getNodeType();
 
         // if the ancestor is an attribute, get owning element. 
         // we are now interested in the owner to determine position.
 
         if (thisAncestorType == Node.ATTRIBUTE_NODE)  {
            thisNode = ((AttrImpl)thisAncestor).getOwnerElement();
         }
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
            otherNode = ((AttrImpl)otherAncestor).getOwnerElement();
         }
 
         // Before proceeding, we should check if both ancestor nodes turned
         // out to be attributes for the same element
         if (thisAncestorType == Node.ATTRIBUTE_NODE &&  
             otherAncestorType == Node.ATTRIBUTE_NODE &&  
             thisNode==otherNode)              
             return TREE_POSITION_EQUIVALENT;
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
  
         // Note:  the following 2 loops are quite close to the ones above.
         // May want to common them up.  LM.
         if (thisAncestorType == Node.ATTRIBUTE_NODE) {
             thisDepth=0;
             for (node=thisNode; node != null; node=node.getParentNode()) {
                 thisDepth +=1;
                 if (node == otherNode) 
                   // The other node is an ancestor of the owning element
+                  {
                   return TREE_POSITION_PRECEDING;
+                  }
                 thisAncestor = node;
             }
         }
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
             otherDepth=0;
             for (node=otherNode; node != null; node=node.getParentNode()) {
                 otherDepth +=1;
                 if (node == thisNode) 
                   // The other node is a descendent of the reference 
                   // node's element
                   return TREE_POSITION_FOLLOWING;
                 otherAncestor = node;
             }
         }
 
         // thisAncestor and otherAncestor must be the same at this point,  
         // otherwise, we are not in the same tree or document fragment
         if (thisAncestor != otherAncestor) 
           return TREE_POSITION_DISCONNECTED; 
 
-        // Determine which node is of the greatest depth.  
+      
+        // Go up the parent chain of the deeper node, until we find a node 
+        // with the same depth as the shallower node
+
         if (thisDepth > otherDepth) {
           for (int i=0; i<thisDepth - otherDepth; i++)
             thisNode = thisNode.getParentNode();
+          // Check if the node we have reached is in fact ""otherNode"". This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""precedes"" this.
+          if (thisNode == otherNode) 
+            return TREE_POSITION_PRECEDING;
         }
+ 
         else {
           for (int i=0; i<otherDepth - thisDepth; i++)
             otherNode = otherNode.getParentNode();
+          // Check if the node we have reached is in fact ""thisNode"".  This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""follows"" this.
+          if (otherNode == thisNode) 
+            return TREE_POSITION_FOLLOWING;
         }
-          
+             
         // We now have nodes at the same depth in the tree.  Find a common 
         // ancestor.                                   
         Node thisNodeP, otherNodeP;
         for (thisNodeP=thisNode.getParentNode(),
                   otherNodeP=otherNode.getParentNode();
              thisNodeP!=otherNodeP;) {
              thisNode = thisNodeP;
              otherNode = otherNodeP;
              thisNodeP = thisNodeP.getParentNode();
              otherNodeP = otherNodeP.getParentNode();
         }
 
+        // At this point, thisNode and otherNode are direct children of 
+        // the common ancestor.  
         // See whether thisNode or otherNode is the leftmost
+
         for (Node current=thisNodeP.getFirstChild(); 
                   current!=null;
                   current=current.getNextSibling()) {
                if (current==otherNode) {
                  return TREE_POSITION_PRECEDING;
                }
                else if (current==thisNode) {
                  return TREE_POSITION_FOLLOWING;
                }
         }
         // REVISIT:  shouldn't get here.   Should probably throw an 
         // exception
         return 0;
 
     }",Buggy,"Fix for bug 13054


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318777 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to bug 13054. The code changes involve modifications to the `compareTreePosition` method, which compares the positions of two nodes in a DOM tree. The changes include additional checks and logic to handle cases where the nodes are attributes or have different depths in the tree. Specifically, the code now checks if the nodes reached after traversing up the parent chain are equal to each other, which can happen in the case of attributes. This suggests that the original code had a bug where it was not correctly comparing the positions of attribute nodes or nodes with different depths. The addition of these checks and the corresponding return values (TREE_POSITION_PRECEDING or TREE_POSITION_FOLLOWING) indicate that the code is now handling these cases correctly, thus fixing the bug.

**Buggy**"
xerces2-j,4598.json,1ab22779af45496ed09d0d35aa50271c6e881b33,"@@ -1,3 +1,4 @@
     public void setLocale(Locale locale) throws XNIException {
+    	super.setLocale(locale);
         fErrorReporter.setLocale(locale);
     } // setLocale(Locale)",Buggy,"fix for bug id 5927. Thanks to 'Taki' for the solution.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to bug ID 5927, and it acknowledges the contributor of the solution. The code diff shows a modification in the `setLocale` method where `super.setLocale(locale)` is added. This suggests that the original implementation might have been missing a call to the superclass's `setLocale` method, potentially leading to incorrect locale handling or initialization in the parent class. The addition of this line likely addresses a bug where the locale was not being properly propagated or initialized in the class hierarchy.

**Buggy**"
xerces2-j,1891.json,61370c8cd1f67817077ae8b356b0f69747471746,"@@ -1,72 +1,74 @@
     protected boolean matchChildSequence(QName element, int event)
             throws XNIException {
     	
         // need to resize fCurrentChildSequence
         if (fCurrentChildDepth >= fCurrentChildSequence.length) {
             int tmpCurrentChildSequence[] = new int[fCurrentChildSequence.length];
             System.arraycopy(fCurrentChildSequence, 0, tmpCurrentChildSequence,
                     0, fCurrentChildSequence.length);
 
             // Increase the size by a factor of 2 (?)
             fCurrentChildSequence = new int[fCurrentChildDepth * 2];
             System.arraycopy(tmpCurrentChildSequence, 0, fCurrentChildSequence,
                     0, tmpCurrentChildSequence.length);
         }
 
         //     
         if (fIsResolveElement) {
             // start
             if (event == XPointerPart.EVENT_ELEMENT_START) {
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildDepth++;
 
                 // reset the current child position 
                 fCurrentChildPosition = 1;
 
                 //if (!fSchemeNameFound) {
                 if ((fCurrentChildDepth <= fFoundDepth) || (fFoundDepth == 0)) {
                     if (checkMatch()) {
                         fIsElementFound = true;
                         fFoundDepth = fCurrentChildDepth;
                     } else {
                         fIsElementFound = false;
                         fFoundDepth = 0;
                     }
                 }
 
             } else if (event == XPointerPart.EVENT_ELEMENT_END) {
                 if (fCurrentChildDepth == fFoundDepth) {
                     fIsElementFound = true;
                 } else if (((fCurrentChildDepth < fFoundDepth) && (fFoundDepth != 0))
                         || ((fCurrentChildDepth > fFoundDepth) // or empty element found
                         && (fFoundDepth == 0))) {
                     fIsElementFound = false;
                 }
 
                 // reset array position of last child
                 fCurrentChildSequence[fCurrentChildDepth] = 0;
 
                 fCurrentChildDepth--;
                 fCurrentChildPosition = fCurrentChildSequence[fCurrentChildDepth] + 1;
                 
             } else if (event == XPointerPart.EVENT_ELEMENT_EMPTY) {
 
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildPosition++;
 
                 // Donot check for empty elements if the empty element is 
                 // a child of a found parent element 
-                //if (!fIsElementFound) {
-                    if (checkMatch()) {
-                        fIsElementFound = true;
+                if (checkMatch()) {
+                    if (!fIsElementFound) {
                         fWasOnlyEmptyElementFound = true;
                     } else {
-                        fIsElementFound = false;
+                        fWasOnlyEmptyElementFound = false;
                     }
-                //} 
-                
+                    fIsElementFound = true;
+                } else {
+                    fIsElementFound = false;
+                    fWasOnlyEmptyElementFound = false;
+                }  
             }
         }
 
         return fIsElementFound;
     }",Buggy,"Fixing JIRA Bug #1134: http://issues.apache.org/jira/browse/XERCESJ-1134

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@415823 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to JIRA issue XERCESJ-1134. The code diff modifies the `matchChildSequence` method, specifically addressing the handling of empty elements. The changes include adding a check `if (!fIsElementFound)` before setting `fWasOnlyEmptyElementFound` to true, and also setting `fWasOnlyEmptyElementFound` to false if the checkMatch fails. These modifications suggest that the original code had a bug in how it handled empty elements within the XPointer processing logic, potentially leading to incorrect identification of elements. Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,7694.json,60697c3db7e640a090be94027b7b5e35be8de0e4,"@@ -1,19 +1,21 @@
     public synchronized XSObjectList getAnnotations() {
         if(fAnnotations != null) 
             return fAnnotations;
 
         // do this in two passes to avoid inaccurate array size
         int totalAnnotations = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             totalAnnotations += fGrammarList[i].fNumAnnotations;
         }
         XSAnnotationImpl [] annotations = new XSAnnotationImpl [totalAnnotations];
         int currPos = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             SchemaGrammar currGrammar = fGrammarList[i];
-            System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
-            currPos += currGrammar.fNumAnnotations;
+            if (currGrammar.fNumAnnotations > 0) {
+                System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
+                currPos += currGrammar.fNumAnnotations;
+            }
         }
         fAnnotations = new XSObjectListImpl(annotations, annotations.length);
         return fAnnotations;
     }",Buggy,"Fixing Jira Bug #968:
http://nagoya.apache.org/jira/browse/XERCESJ-968

If no annotations exist for a SchemaGrammar the array
of annotations is never created. For such schema docs
we were passing null to System.arrayCopy which caused
a NullPointerException to be thrown. This is now fixed.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319923 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException that occurred when no annotations existed for a SchemaGrammar. The code diff shows that a check `if (currGrammar.fNumAnnotations > 0)` has been added before calling `System.arraycopy`. This check prevents the `System.arraycopy` method from being called with a null `currGrammar.fAnnotations` when `currGrammar.fNumAnnotations` is 0, which would cause a NullPointerException. The code change directly addresses the issue described in the commit message.

**Buggy**"
xerces2-j,2219.json,5a14bab0762a851069a18eebd7f278b943f88c00,"@@ -1,20 +1,20 @@
     public Object getTypeInfo(int nodeIndex) {
         if (nodeIndex == -1) {
             return null;
         }
 
         int chunk = nodeIndex >> CHUNK_SHIFT;
         int index = nodeIndex & CHUNK_MASK;
         
         
-        Object value = fNodeValue[chunk][index];
+        Object value = fNodeValue[chunk] != null ? fNodeValue[chunk][index] : null;
         if (value != null) {
             fNodeValue[chunk][index] = null;
             RefCount c = (RefCount) fNodeValue[chunk][CHUNK_SIZE];
             c.fCount--;
             if (c.fCount == 0) {
                 fNodeValue[chunk] = null;
             }
         }
         return value;
     }",Buggy,"Fix a bug introduced in the new getTypeInfo method (DOM L3)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318962 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `getTypeInfo` method. The code diff shows a change in how the `value` is retrieved from the `fNodeValue` array. Specifically, a null check `fNodeValue[chunk] != null` is added before accessing `fNodeValue[chunk][index]`. This suggests that the original code was throwing a `NullPointerException` when `fNodeValue[chunk]` was null, which is now being handled. This aligns with the commit message indicating a bug fix.

**Buggy**"
xerces2-j,4324.json,53ec54f2822c25f6815aa81b3b2eb8f46dd3caf5,"@@ -1,91 +1,91 @@
     protected void configurePipeline() {
         if (fCurrentDVFactory != fDatatypeValidatorFactory) {
             fCurrentDVFactory = fDatatypeValidatorFactory;
             // use XML 1.0 datatype library
             setProperty(DATATYPE_VALIDATOR_FACTORY, fCurrentDVFactory);
         }
 
         // setup DTD pipeline
         if (fCurrentDTDScanner != fDTDScanner) {
-			fCurrentDTDScanner = fDTDScanner;
+            fCurrentDTDScanner = fDTDScanner;
             setProperty(DTD_SCANNER, fCurrentDTDScanner);
             setProperty(DTD_PROCESSOR, fDTDProcessor);
-            fDTDScanner.setDTDHandler(fDTDProcessor);
-            fDTDProcessor.setDTDSource(fDTDScanner);
-            fDTDProcessor.setDTDHandler(fDTDHandler);
-            if (fDTDHandler != null) {
-                 fDTDHandler.setDTDSource(fDTDProcessor);
-            }
+        }
+        fDTDScanner.setDTDHandler(fDTDProcessor);
+        fDTDProcessor.setDTDSource(fDTDScanner);
+        fDTDProcessor.setDTDHandler(fDTDHandler);
+        if (fDTDHandler != null) {
+            fDTDHandler.setDTDSource(fDTDProcessor);
+        }
 
-            fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
-            fDTDProcessor.setDTDContentModelSource(fDTDScanner);
-            fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
-            if (fDTDContentModelHandler != null) {
-                fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
-            }            
+        fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
+        fDTDProcessor.setDTDContentModelSource(fDTDScanner);
+        fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
+        if (fDTDContentModelHandler != null) {
+            fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
         }
 
         // setup document pipeline
         if (fFeatures.get(NAMESPACES) == Boolean.TRUE) {
             if (fCurrentScanner != fNamespaceScanner) {
                 fCurrentScanner = fNamespaceScanner;
                 setProperty(DOCUMENT_SCANNER, fNamespaceScanner);
                 setProperty(DTD_VALIDATOR, fDTDValidator);
             }
             fNamespaceScanner.setDTDValidator(fDTDValidator);
             fNamespaceScanner.setDocumentHandler(fDTDValidator);
             fDTDValidator.setDocumentSource(fNamespaceScanner);
             fDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fDTDValidator);
             }
             fLastComponent = fDTDValidator;
         } else {
             // create components
             if (fNonNSScanner == null) {
                 fNonNSScanner = new XMLDocumentScannerImpl();
                 fNonNSDTDValidator = new XMLDTDValidator();
                 // add components
                 addComponent((XMLComponent) fNonNSScanner);
                 addComponent((XMLComponent) fNonNSDTDValidator);
             }
             if (fCurrentScanner != fNonNSScanner) {
                 fCurrentScanner = fNonNSScanner;
                 setProperty(DOCUMENT_SCANNER, fNonNSScanner);
                 setProperty(DTD_VALIDATOR, fNonNSDTDValidator);
             }
 
             fNonNSScanner.setDocumentHandler(fNonNSDTDValidator);
             fNonNSDTDValidator.setDocumentSource(fNonNSScanner);
             fNonNSDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fNonNSDTDValidator);
             }
             fLastComponent = fNonNSDTDValidator;
         }
 
         // add XML Schema validator if needed
         if (fFeatures.get(XMLSCHEMA_VALIDATION) == Boolean.TRUE) {
             // If schema validator was not in the pipeline insert it.
             if (fSchemaValidator == null) {
                 fSchemaValidator = new XMLSchemaValidator();
                 // add schema component
                 setProperty(SCHEMA_VALIDATOR, fSchemaValidator);
                 addCommonComponent(fSchemaValidator);
-				fSchemaValidator.reset(this);
+                fSchemaValidator.reset(this);
                 // add schema message formatter
                 if (fErrorReporter.getMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN) == null) {
                     XSMessageFormatter xmft = new XSMessageFormatter();
                     fErrorReporter.putMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN, xmft);
                 }
 
             }
             fLastComponent.setDocumentHandler(fSchemaValidator);
             fSchemaValidator.setDocumentSource(fLastComponent);
             fSchemaValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fSchemaValidator);
             }
             fLastComponent = fSchemaValidator;
         }
     } // configurePipeline()",Buggy,"Fixing a bug in XML10 configurePipeline: we should always setup DTD pipeline (was only
setup if scanner has not been changed)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319532 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the DTD pipeline setup in `configurePipeline`. The original code only set up the DTD pipeline if the scanner had not been changed. The corrected code now sets up the DTD pipeline unconditionally. The diff shows that the code block responsible for setting up the DTD pipeline has been moved outside the `if` condition. This ensures that the DTD pipeline is always configured, regardless of whether the scanner has changed. This aligns with the commit message's intention to fix the bug.

**Buggy**"
xerces2-j,4808.json,70b3c80f83b7fcd277b5905ccde6e49f5e6274bb,"@@ -1,4 +1,4 @@
     public String getType(String qname) {
         int index = getIndex(qname);
-        return index != -1 ? fAttributes[index].type : null;
+        return index != -1 ? getType(index): null;
     } // getType(String):String",Buggy,"fixing bug 15584.  Thanks to Michael Glavassevich for pointing out this bug in a SAX context; it was equally an XNI bug, however.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to SAX and XNI contexts, specifically mentioned as bug 15584. The code change replaces direct access to the `fAttributes` array with a call to `getType(index)`. This suggests that the original code might have had an issue with how attribute types were being retrieved, potentially leading to incorrect or null values in certain scenarios. The change aims to correct this behavior by using a dedicated method for retrieving the type based on the index.

Given the explicit mention of a bug fix in the commit message and the code change addressing a potential issue with attribute type retrieval, it is highly likely that this commit addresses a bug.

**Buggy**
"
xerces2-j,2281.json,f905e13bf111fee815eabfc522f4dfc0949fb422,"@@ -1,3 +1,3 @@
     public void setType(XSTypeDefinition type) {
-        type = type;
-    }

+        this.type = type;
+    }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=17064
Patch submitter: Michael Glavassevich


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319093 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to a reported issue. The provided diff shows a change in the `setType` method. The original code `type = type;` is a no-op, meaning it doesn't actually assign the input `type` to the class member `type`. The corrected code `this.type = type;` properly assigns the input `type` to the class member `type`. This correction directly addresses a bug where the type was not being set correctly.

**Buggy**"
xerces2-j,3095.json,955ef6b2ca713901bd702935c131a09390af1389,"@@ -1,9 +1,8 @@
     int indexOf(Node child, Node parent) {
-        Node node;
-        int i = 0;
         if (child.getParentNode() != parent) return -1;
-        for(node = child; node!= null; node=node.getPreviousSibling()) {
+        int i = 0;
+        for(Node node = parent.getFirstChild(); node!= child; node=node.getNextSibling()) {
             i++;
         }
         return i;
     }",Buggy,"applied patch from Lynn Monson:
fixed a bug in indexOf which was off by 1


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316616 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""applied patch from Lynn Monson: fixed a bug in indexOf which was off by 1"" clearly states that the purpose of the commit is to fix a bug in the `indexOf` method. The bug is described as being ""off by 1,"" which suggests an incorrect index calculation.

**Diff Analysis:**

The diff shows a modification to the `indexOf` method. Let's break down the changes:

*   **Original Code:**
    ```java
    int indexOf(Node child, Node parent) {
        Node node;
        int i = 0;
        if (child.getParentNode() != parent) return -1;
        for(node = child; node!= null; node=node.getPreviousSibling()) {
            i++;
        }
        return i;
    }
    ```

*   **Modified Code:**
    ```java
    int indexOf(Node child, Node parent) {
        if (child.getParentNode() != parent) return -1;
        int i = 0;
        for(Node node = parent.getFirstChild(); node!= child; node=node.getNextSibling()) {
            i++;
        }
        return i;
    }
    ```

The key changes are:

1.  **Initialization and Loop Start:** The original code initialized `node` to `child` and iterated through the previous siblings of `child`. The modified code initializes `node` to the first child of `parent` and iterates through the next siblings until it reaches `child`.
2.  **Logic Correction:** The original code was counting backwards from the child node using `getPreviousSibling()`. This approach is flawed because it assumes the child node is the starting point for determining its index within the parent's children. The corrected code iterates forward from the first child of the parent, which is the correct way to determine the index.

**Reasoning:**

The commit message explicitly mentions a bug fix related to an ""off by 1"" error in the `indexOf` method. The code changes directly address this issue by correcting the logic used to calculate the index of a child node within its parent. The original code's approach of traversing previous siblings was incorrect, leading to the inaccurate index calculation. The corrected code iterates from the parent's first child, ensuring the index is calculated correctly. The change in logic directly corresponds to fixing the described bug.

**Conclusion:**

The commit message and the code changes are highly relevant to each other. The code modification clearly addresses the bug described in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,3693.json,b23b90432763872d07b931251896abce3e1e90e6,"@@ -1,39 +1,39 @@
         public InputSource resolveEntity(String name, String publicId, 
                 String baseURI, String systemId) throws SAXException, IOException {
             if (fEntityResolver != null) {
                 LSInput lsInput = fEntityResolver.resolveResource(XML_TYPE, null, publicId, systemId, baseURI);
                 if (lsInput != null) {
                     final String pubId = lsInput.getPublicId();
                     final String sysId = lsInput.getSystemId();
                     final String baseSystemId = lsInput.getBaseURI();
                     final Reader charStream = lsInput.getCharacterStream();
                     final InputStream byteStream = lsInput.getByteStream();
                     final String data = lsInput.getStringData();
                     final String encoding = lsInput.getEncoding();
 
                     /**
                      * An LSParser looks at inputs specified in LSInput in
                      * the following order: characterStream, byteStream,
                      * stringData, systemId, publicId. For consistency
                      * with the DOM Level 3 Load and Save Recommendation
                      * use the same lookup order here.
                      */
                     InputSource inputSource = new InputSource();
                     inputSource.setPublicId(pubId);
-                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(systemId, baseSystemId) : systemId);
+                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(sysId, baseSystemId) : sysId);
                     
                     if (charStream != null) {
                         inputSource.setCharacterStream(charStream);
                     }
                     else if (byteStream != null) {
                         inputSource.setByteStream(byteStream);
                     }
                     else if (data != null && data.length() != 0) {
                         inputSource.setCharacterStream(new StringReader(data));
                     }
                     inputSource.setEncoding(encoding);
                     return inputSource;
                 }
             }
             return null;
         }",Buggy,"Fixing a minor bug. The InputSource returned from the resolution forwarder should 
contain the system identifier returned from the application's LSResourceResolver 
not the one which was passed to the resolveEntity() method.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@374971 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `resolveEntity` method. Specifically, it mentions that the `InputSource` returned by the resolution forwarder should contain the system identifier returned from the application's `LSResourceResolver`, not the one passed to the `resolveEntity()` method.

The diff shows a change in the line:

```diff
-                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(systemId, baseSystemId) : systemId);
+                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(sysId, baseSystemId) : sysId);
```

The original code used `systemId` in the `setSystemId` method of the `InputSource`, while the corrected code uses `sysId`. According to the commit message, `sysId` is the system identifier returned from the application's `LSResourceResolver`, and `systemId` is the one passed to the `resolveEntity()` method. The change aligns perfectly with the commit message, indicating that the original code had a bug where it was using the wrong system identifier.

Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,1803.json,6c7a16c3202d0e50e4cd05449e9a5a9963f80e1b,"@@ -1,6 +1,7 @@
     public int hashCode() {
         if (uri != null) {
-            return uri.hashCode() + localpart.hashCode();
+            return uri.hashCode() + 
+                ((localpart != null) ? localpart.hashCode() : 0);
         }
-        return rawname.hashCode();
+        return (rawname != null) ? rawname.hashCode() : 0;
     } // hashCode():int",Buggy,"Fixing Jira Bug #997:
http://nagoya.apache.org/jira/browse/XERCESJ-997

Fixing an NPE which may occur in the hashCode method
when either the localpart or rawname fields are null. This
seems to only be a problem for external users of this class.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that could occur in the `hashCode` method when either `localpart` or `rawname` fields are null. The commit message explicitly mentions ""Fixing an NPE"".

The code diff shows that the `hashCode` method has been modified to handle null values for `localpart` and `rawname`.  Specifically, the code now checks if `localpart` and `rawname` are null before calling their `hashCode` methods. If either is null, it uses 0 as the hash code contribution. This prevents the NPE that could occur if `localpart` or `rawname` were null and their `hashCode` methods were called.

The changes in the code directly address the issue described in the commit message. The commit message states that it is fixing a bug. The code changes clearly show a fix for a potential NullPointerException.

**Buggy**"
xerces2-j,3399.json,0df32768beedf6778651639c24d234d5b70f3b6e,"@@ -1,30 +1,33 @@
     private static void organizeParts(
         String whole,
         String[] parts,
         int[] partsIndex,
         int len,
         String tokens)
         throws IllegalArgumentException {
 
         int idx = tokens.length();
         for (int i = len - 1; i >= 0; i--) {
+            if (parts[i] == null) {
+                throw new IllegalArgumentException(whole);
+            }
             int nidx =
                 tokens.lastIndexOf(
                     parts[i].charAt(parts[i].length() - 1),
                     idx - 1);
             if (nidx == -1) {
                 throw new IllegalArgumentException(whole);
                 // ,partsIndex[i]+parts[i].length()-1);
             }
 
             for (int j = nidx + 1; j < idx; j++) {
                 parts[j] = null;
             }
             idx = nidx;
             parts[idx] = parts[i];
             partsIndex[idx] = partsIndex[i];
         }
         for (idx--; idx >= 0; idx--) {
             parts[idx] = null;
         }
     }",Buggy,"Fixing JIRA Bug #1416: http://issues.apache.org/jira/browse/XERCESJ-1416. An NPE could occur when the value of the duration is invalid. Check for this condition and throw the correct exception.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@906803 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that could occur when the duration value is invalid. The code diff adds a null check for `parts[i]` inside the `organizeParts` method. If `parts[i]` is null, an `IllegalArgumentException` is thrown with the `whole` string as the argument. This aligns with the commit message's description of checking for an invalid duration value and throwing the correct exception. The added null check prevents the potential NPE, thus fixing the bug.

**Buggy**"
xerces2-j,3918.json,3260ba200fec37c5d074e5bb96ebd856e494ebe9,"@@ -1,14 +1,22 @@
     public void setFeature(String name, boolean value)
         throws ParserConfigurationException, SAXNotRecognizedException, 
 		SAXNotSupportedException
     {
         // XXX This is ugly.  We have to collect the features and then
         // later create an XMLReader to verify the features.
         if (features == null) {
             features = new Hashtable();
         }
         features.put(name, new Boolean(value));
 
         // Test the feature by possibly throwing SAX exceptions
-        newSAXParserImpl();
+        try {
+            newSAXParserImpl();
+        } catch (SAXNotSupportedException e) {
+            features.remove(name);
+            throw e;
+        } catch (SAXNotRecognizedException e) {
+            features.remove(name);
+            throw e;
+        }
     }",Buggy,"Fix setFeature() bug provided by Kohsuke Kawaguchi


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317014 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `setFeature()` method. The diff shows that the `newSAXParserImpl()` method call, which is intended to test the feature, is now enclosed in a `try-catch` block. If `newSAXParserImpl()` throws a `SAXNotSupportedException` or `SAXNotRecognizedException`, the code now removes the feature from the `features` map and re-throws the exception. This suggests that the previous implementation did not handle these exceptions correctly, potentially leaving the `features` map in an inconsistent state. The added exception handling directly addresses a bug where unsupported or unrecognized features were not properly handled, leading to incorrect state.

**Buggy**"
xerces2-j,5468.json,7b91d52436909c5274ee9d683e37e15b4d73c03b,"@@ -1,111 +1,116 @@
     public void print(Node node) {
 
         // is there anything to do?
         if ( node == null ) {
             return;
         }
 
         int type = node.getNodeType();
         switch ( type ) {
         // print document
         case Node.DOCUMENT_NODE: {
                 if ( !canonical ) {
                     String  Encoding = this.getWriterEncoding();
                     if ( Encoding.equalsIgnoreCase( ""DEFAULT"" ) )
                         Encoding = ""UTF-8"";
                     else if ( Encoding.equalsIgnoreCase( ""Unicode"" ) )
                         Encoding = ""UTF-16"";
                     else
                         Encoding = MIME2Java.reverse( Encoding );
 
                     out.println(""<?xml version=\""1.0\"" encoding=\""""+
                                 Encoding + ""\""?>"");
                 }
-                print(((Document)node).getDocumentElement());
+                //print(((Document)node).getDocumentElement());
+                
+                NodeList children = node.getChildNodes(); 
+                for ( int iChild = 0; iChild < children.getLength(); iChild++ ) { 
+                    print(children.item(iChild)); 
+                } 
                 out.flush();
                 break;
             }
 
             // print element with attributes
         case Node.ELEMENT_NODE: {
                 out.print('<');
                 out.print(node.getNodeName());
                 Attr attrs[] = sortAttributes(node.getAttributes());
                 for ( int i = 0; i < attrs.length; i++ ) {
                     Attr attr = attrs[i];
                     out.print(' ');
                     out.print(attr.getNodeName());
                     out.print(""=\"""");
                     out.print(normalize(attr.getNodeValue()));
                     out.print('""');
                 }
                 out.print('>');
                 NodeList children = node.getChildNodes();
                 if ( children != null ) {
                     int len = children.getLength();
                     for ( int i = 0; i < len; i++ ) {
                         print(children.item(i));
                     }
                 }
                 break;
             }
 
             // handle entity reference nodes
         case Node.ENTITY_REFERENCE_NODE: {
                 if ( canonical ) {
                     NodeList children = node.getChildNodes();
                     if ( children != null ) {
                         int len = children.getLength();
                         for ( int i = 0; i < len; i++ ) {
                             print(children.item(i));
                         }
                     }
                 } else {
                     out.print('&');
                     out.print(node.getNodeName());
                     out.print(';');
                 }
                 break;
             }
 
             // print cdata sections
         case Node.CDATA_SECTION_NODE: {
                 if ( canonical ) {
                     out.print(normalize(node.getNodeValue()));
                 } else {
                     out.print(""<![CDATA["");
                     out.print(node.getNodeValue());
                     out.print(""]]>"");
                 }
                 break;
             }
 
             // print text
         case Node.TEXT_NODE: {
                 out.print(normalize(node.getNodeValue()));
                 break;
             }
 
             // print processing instruction
         case Node.PROCESSING_INSTRUCTION_NODE: {
                 out.print(""<?"");
                 out.print(node.getNodeName());
                 String data = node.getNodeValue();
                 if ( data != null && data.length() > 0 ) {
                     out.print(' ');
                     out.print(data);
                 }
-                out.print(""?>"");
+                out.println(""?>"");
                 break;
             }
         }
 
         if ( type == Node.ELEMENT_NODE ) {
             out.print(""</"");
             out.print(node.getNodeName());
             out.print('>');
         }
 
         out.flush();
 
     } // print(Node)",Buggy,"Fix submitted by Ed Stub. Fixes problem reported by Stephane.RAULT@r2isa... where we
are no reporting PI's


PR:
Obtained from:
Submitted by:
Reviewed by:


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316112 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a problem where processing instructions (PIs) were not being reported correctly. The diff shows changes in how the `print` method handles `DOCUMENT_NODE` and `PROCESSING_INSTRUCTION_NODE` types.

Specifically, for `DOCUMENT_NODE`, the original code printed only the document element, but the corrected code iterates through all child nodes of the document and prints each one. This ensures that all elements within the document, including processing instructions, are processed.

For `PROCESSING_INSTRUCTION_NODE`, the original code prints ""<?...?>"" on a single line, while the corrected code prints ""<?...?>"" followed by a newline character. This change might be related to how the output is formatted or interpreted by the reporting mechanism.

These code modifications directly address the issue described in the commit message, indicating a bug fix related to the handling and reporting of processing instructions.

**Buggy**"
xerces2-j,684.json,d1fd0668a071ddcf44c6bdf1b50030bab12d18ee,"@@ -1,25 +1,26 @@
     public synchronized void deleteRow( int index )
     {
         Node    child;
         
         child = getFirstChild();
         while ( child != null )
         {
             if ( child instanceof HTMLTableRowElement )
             {
                 if ( index == 0 )
                 {
                     removeChild ( child );
                     return;
                 }
+                --index;
             }
             else
             if ( child instanceof HTMLTableSectionElementImpl )
             {
                 index = ( (HTMLTableSectionElementImpl) child ).deleteRowX( index );
                 if ( index < 0 )
                     return;
             }
             child = child.getNextSibling();
         }
     }",Buggy,"Fixing bug in HTML DOM (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=18744). Deletion of rows other than row 0 were failing siliently.

Thanks to Brett Sutton for the fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319291 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

*   **Explicit Bug Fix:** The commit message clearly states ""Fixing bug in HTML DOM"" and references a specific bug report (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=18744). This strongly suggests a bug fix.
*   **Problem Description:** The message describes the bug: ""Deletion of rows other than row 0 were failing silently."" This indicates a functional issue where deleting rows at indices other than 0 was not working as expected.
*   **Acknowledgement:** The message thanks Brett Sutton for the fix, further suggesting that this commit addresses a reported problem.

**Code Diff Analysis:**

*   **Targeted Change:** The diff focuses on the `deleteRow` method within what appears to be an HTML table implementation. This aligns with the commit message's mention of row deletion issues.
*   **Logic Correction:** The key change is the addition of `--index;` inside the `if (child instanceof HTMLTableRowElement)` block. This suggests that the original code was not correctly decrementing the index when iterating through rows, leading to the incorrect row being targeted for deletion (or no row at all when `index` was not 0 initially).
*   **Impact:** The added line ensures that the `index` is decremented for each `HTMLTableRowElement` encountered. This is crucial for correctly identifying the row to be deleted when the target `index` is greater than 0.

**Reasoning:**

The commit message explicitly states a bug fix related to deleting rows in an HTML table. The code diff shows a modification to the `deleteRow` method that corrects the index handling logic. The original code likely failed to decrement the index correctly, causing the wrong row (or no row) to be deleted when the target index was not 0. The added `--index;` line fixes this logic error, ensuring that the correct row is identified and deleted. The combination of a clear bug description in the commit message and a logical correction in the code strongly indicates a bug fix.

**Conclusion:**

**Buggy**
"
xerces2-j,5580.json,af847434ae5a2c3408cae3c4b738622800c93e4f,"@@ -1,43 +1,43 @@
     private static void buildCaseInsensitiveMap() {
         caseInsensitiveMap = new int[INITIAL_CHUNK_COUNT][][];
         for (int i=0; i<INITIAL_CHUNK_COUNT; i++) {
             caseInsensitiveMap[i] = new int[CHUNK_SIZE][];
         }
         
         int lc, uc;
         for (int i=0; i<0x10000; i++) {
-            lc = Character.toLowerCase(i);
-            uc = Character.toUpperCase(i);
+            lc = Character.toLowerCase((char) i);
+            uc = Character.toUpperCase((char) i);
 
             // lower/upper case value is not the same as code point
             if (lc != uc || lc != i) {
                 int[] map = new int[2];
                 int index = 0;
 
                 if (lc != i) {
                     map[index++] = lc;
                     map[index++] = LOWER_CASE_MATCH;
                     int[] lcMap = getMapping(lc);
                     if (lcMap != null) {
                         map = updateMap(i, map, lc, lcMap, LOWER_CASE_MATCH);
                     }
                 }
                 
                 if (uc != i) {
                     if (index == map.length) {
                         map = expandMap(map, 2);
                     }
                     map[index++] = uc;
                     map[index++] = UPPER_CASE_MATCH;
                     int[] ucMap = getMapping(uc);
                     if (ucMap != null) {
                         map = updateMap(i, map, uc, ucMap, UPPER_CASE_MATCH);
                     }
                 }
                 
                 set(i, map);
             }
         }
 
         mapBuilt = Boolean.TRUE;
     }",Buggy,"Fixing compilation errors on Java SE 1.4 and below. Character.toLowerCase(int) and Character.toUpperCase(int) didn't exist until Java SE 5.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@834593 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for compilation errors on older Java versions (1.4 and below) due to the use of `Character.toLowerCase(int)` and `Character.toUpperCase(int)`, which were introduced in Java SE 5. The code diff replaces these methods with `Character.toLowerCase((char) i)` and `Character.toUpperCase((char) i)`, casting the integer `i` to a character. This change ensures compatibility with older Java versions, as the `Character.toLowerCase(char)` and `Character.toUpperCase(char)` methods were available in earlier versions. This directly addresses the compilation errors mentioned in the commit message.

**Buggy**
"
xerces2-j,7513.json,13892ec89f2b5203650caf1d7b5355df433dacfd,"@@ -1,74 +1,74 @@
         private void mergeSchemaGrammars(SchemaGrammar cachedGrammar, SchemaGrammar newGrammar) {
 
             /** Add new top-level element declarations. **/
             XSNamedMap map = newGrammar.getComponents(XSConstants.ELEMENT_DECLARATION);
             int length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSElementDecl decl = (XSElementDecl) map.item(i);
-                if (cachedGrammar.getElementDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalElementDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalElementDecl(decl);
                 }
             }
             
             /** Add new top-level attribute declarations. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeDecl decl = (XSAttributeDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeDecl(decl);
                 }
             }
             
             /** Add new top-level type definitions. **/
             map = newGrammar.getComponents(XSConstants.TYPE_DEFINITION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSTypeDefinition decl = (XSTypeDefinition) map.item(i);
                 if (cachedGrammar.getGlobalTypeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalTypeDecl(decl);
                 }
             }
             
             /** Add new top-level attribute group definitions. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeGroupDecl decl = (XSAttributeGroupDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeGroupDecl(decl);
                 }
             }
             
             /** Add new top-level model group definitions. **/
             map = newGrammar.getComponents(XSConstants.MODEL_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSGroupDecl decl = (XSGroupDecl) map.item(i);
                 if (cachedGrammar.getGlobalGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalGroupDecl(decl);
                 }
             }
             
             /** Add new top-level notation declarations. **/
             map = newGrammar.getComponents(XSConstants.NOTATION_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSNotationDecl decl = (XSNotationDecl) map.item(i);
                 if (cachedGrammar.getGlobalNotationDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalNotationDecl(decl);
                 }
             }
             
             /** 
              * Add all annotations. Since these components are not named it's
              * possible we'll add duplicate components. There isn't much we can
              * do. It's no worse than XMLSchemaLoader when used as an XSLoader.
              */
             XSObjectList annotations = newGrammar.getAnnotations();
             length = annotations.getLength();
             for (int i = 0; i < length; ++i) {
                 cachedGrammar.addAnnotation((XSAnnotationImpl) annotations.item(i));
             }
             
         }",Buggy,"Fixing a few minor bugs.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fixing a few minor bugs"" suggests that the changes in the diff are related to bug fixes.

The diff involves changes in the `mergeSchemaGrammars` method. Specifically, it modifies the conditions under which global declarations (element, attribute, attribute group) are added to the `cachedGrammar`. The original code used methods like `getElementDeclaration`, `getAttributeDeclaration` to check if a declaration already exists in the `cachedGrammar`. These have been replaced with `getGlobalElementDecl`, `getGlobalAttributeDecl`, `getGlobalAttributeGroupDecl`. This suggests that the original methods were not correctly checking for global declarations, potentially leading to duplicate declarations or other issues when merging schema grammars. The change ensures that only truly new global declarations are added, which aligns with the intent of fixing bugs related to schema merging.

Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,2176.json,b16dd097ce2ebce1533edaab4e6743b08da63bfe,"@@ -1,26 +1,26 @@
     public int createDeferredDocumentType(String rootElementName,
                                           String publicId, String systemId) {
 
         // create node
         int nodeIndex = createNode(Node.DOCUMENT_TYPE_NODE);
         int chunk     = nodeIndex >> CHUNK_SHIFT;
         int index     = nodeIndex & CHUNK_MASK;
 
         // added for DOM2: createDoctype factory method includes
         // name, publicID, systemID
 
         // create extra data node
         int extraDataIndex = createNode((short)0); // node type unimportant
         int echunk = extraDataIndex >> CHUNK_SHIFT;
         int eindex = extraDataIndex & CHUNK_MASK;
 
         // save name, public id, system id
         setChunkValue(fNodeName, rootElementName, chunk, index);
-        setChunkValue(fNodeValue, publicId, chunk, eindex);
-        setChunkValue(fNodeURI, systemId, chunk, eindex);
+        setChunkValue(fNodeValue, publicId, chunk, index);
+        setChunkValue(fNodeURI, systemId, chunk, index);
         setChunkIndex(fNodeExtra, extraDataIndex, chunk, index);
 
         // return node index
         return nodeIndex;
 
     } // createDeferredDocumentType(String,String,String):int",Buggy,"Fixed bug that caused namespace information to be lost
in the deferred DOM.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317632 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to namespace information loss in the deferred DOM. The code diff modifies the `createDeferredDocumentType` method. Specifically, it changes where the `publicId` and `systemId` are stored. Originally, they were stored using `eindex` (index of the extra data node), but now they are stored using `index` (index of the main node). This suggests that the original code was incorrectly storing the public and system IDs in the extra data node's value and URI fields, instead of the main node's value and URI fields. This could lead to the loss of this information when the DOM is processed later. The change corrects this by storing the information in the correct place.

**Buggy**"
xerces2-j,5971.json,b3530fffbb47d1dc2dab5e538c013f388b4e142f,"@@ -1,26 +1,31 @@
         protected void resolveExternalSubsetAndRead()
             throws IOException, XNIException {
             
             fDTDDescription.setValues(null, null, fEntityManager.getCurrentResourceIdentifier().getExpandedSystemId(), null);
             fDTDDescription.setRootName(fElementQName.rawname);
             XMLInputSource src = fExternalSubsetResolver.getExternalSubset(fDTDDescription);
             
             if (src != null) {
                 fDoctypeName = fElementQName.rawname;
                 fDoctypePublicId = src.getPublicId();
                 fDoctypeSystemId = src.getSystemId();
                 // call document handler
                 if (fDocumentHandler != null) {
                     // This inserts a doctypeDecl event into the stream though no 
                     // DOCTYPE existed in the instance document.
                     fDocumentHandler.doctypeDecl(fDoctypeName, fDoctypePublicId, fDoctypeSystemId, null);
                 }
                 try {
-                    fDTDScanner.setInputSource(src);
-                    while (fDTDScanner.scanDTDExternalSubset(true));
+                    if (fValidationManager == null || !fValidationManager.isCachedDTD()) {
+                        fDTDScanner.setInputSource(src);
+                        while (fDTDScanner.scanDTDExternalSubset(true));
+                    }
+                    else {
+                        fDTDScanner.setInputSource(null);
+                    }
                 }
                 finally {
                     fEntityManager.setEntityHandler(XMLDocumentScannerImpl.this);
                 }
             }
         } // resolveExternalSubsetAndRead()",Buggy,"Fixing a bug reported by Ritu Raj Tiwari on xerces-j-dev. When resolving
an external subset if there's already a DTD in the grammar pool do not
read the input source.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320234 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to resolving external subsets when a DTD already exists in the grammar pool. The code diff adds a condition `fValidationManager == null || !fValidationManager.isCachedDTD()` before setting the input source and scanning the external subset. If `fValidationManager` is not null and a cached DTD exists, the code now skips reading the input source. This aligns with the commit message's intention to avoid reading the input source when a DTD is already present, suggesting a bug where the input source was unnecessarily read, potentially causing issues.

**Buggy**
"
xerces2-j,2755.json,2b2593eef2e2d2c5352fbc9b4e356053a93e34ab,"@@ -1,6 +1,12 @@
     Node matchNodeOrParent(Node node) {
+        // Additions and removals in the underlying data structure may occur
+        // before any iterations, and in this case the reference_node is null.
+        if (fCurrentNode == null) return null;
+        
+        // check if the removed node is an _ancestor_ of the 
+        // reference node
         for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=13062


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318813 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to a specific issue (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=13062). The code diff introduces a null check for `fCurrentNode` at the beginning of the `matchNodeOrParent` method. The comment explains that additions and removals in the underlying data structure may occur before any iterations, leading to `fCurrentNode` being null. This null check prevents a potential NullPointerException if `fCurrentNode` is null, which aligns with the bug fix indicated in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**
"
xerces2-j,2755.json,7f856ddb5537fc5e98c71ff23700a48bac7f652d,"@@ -1,6 +1,6 @@
     Node matchNodeOrParent(Node node) {
-        for (Node n = node; n != fRoot; n = n.getParentNode()) {
+        for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Applying patch suggested by Joe Kesselman. This fixes the following bug:
http://nagoya.apache.org/bugzilla/show_bug.cgi?id=6888


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318267 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix based on a patch suggested by Joe Kesselman, referencing bug ID 6888. The code change modifies the `matchNodeOrParent` method, specifically altering the initialization of the loop variable `n` from `node` to `fCurrentNode`. This suggests that the original implementation might have had an issue where it wasn't correctly traversing the parent nodes from the intended starting point, potentially leading to incorrect matching or unexpected behavior. The change to `fCurrentNode` likely addresses this issue, aligning with the bug fix intention stated in the commit message.

**Buggy**"
xerces2-j,4545.json,3893a210786fea6e8236c54214287515c5333708,"@@ -1,8 +1,9 @@
     public void setFeature(String featureId, boolean state)
         throws XMLConfigurationException {
         if (featureId.equals(XINCLUDE_FEATURE)) {
             fXIncludeEnabled = state;
+            fConfigUpdated = true;
             return;
         }
         super.setFeature(featureId,state);
     }",Buggy,"Fixing a bug. It was possible that the XIncludeHandler could be used without being properly reset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320336 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `XIncludeHandler` not being properly reset. The code diff shows that when the `XINCLUDE_FEATURE` is set, a flag `fConfigUpdated` is set to true. This flag likely indicates that the configuration has been updated and that the `XIncludeHandler` needs to be reset or reinitialized. The change aligns with the commit message, suggesting a fix for a potential issue where the handler was not being reset after a configuration change.

**Buggy**"
xerces2-j,376.json,9768ff6cd6cd57357deb3c4be70cb29bb476e3d2,"@@ -1,15 +1,16 @@
     public String getJavaEncoding() {
         String javaEncoding = null;
         String mimeEncoding = getMimeEncoding();
 
         if (mimeEncoding != null) {
             if (mimeEncoding.equals( ""DEFAULT"" ))
                 javaEncoding =  ""UTF8"";
             else if (mimeEncoding.equalsIgnoreCase( ""UTF-16"" ))
                 javaEncoding = ""Unicode"";
             else
                 javaEncoding = EncodingMap.getIANA2JavaMapping( mimeEncoding );    
-        } else   // Should never return null
+        } 
+        if(javaEncoding == null)   // Should never return null
             javaEncoding = ""UTF8"";
         return(javaEncoding);
     }",Buggy,"fix for bug #6008


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318133 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix for bug #6008"" strongly suggests that the commit is intended to resolve a specific bug identified as #6008. This is a clear indicator that the changes are likely related to correcting incorrect behavior or addressing a defect.

**Code Diff Analysis:**

The code diff focuses on the `getJavaEncoding()` method. Here's a breakdown:

1.  **Original Code:**
    ```java
    if (mimeEncoding != null) {
        if (mimeEncoding.equals( ""DEFAULT"" ))
            javaEncoding =  ""UTF8"";
        else if (mimeEncoding.equalsIgnoreCase( ""UTF-16"" ))
            javaEncoding = ""Unicode"";
        else
            javaEncoding = EncodingMap.getIANA2JavaMapping( mimeEncoding );    
    } else   // Should never return null
        javaEncoding = ""UTF8"";
    return(javaEncoding);
    ```

2.  **Modified Code:**
    ```java
    if (mimeEncoding != null) {
        if (mimeEncoding.equals( ""DEFAULT"" ))
            javaEncoding =  ""UTF8"";
        else if (mimeEncoding.equalsIgnoreCase( ""UTF-16"" ))
            javaEncoding = ""Unicode"";
        else
            javaEncoding = EncodingMap.getIANA2JavaMapping( mimeEncoding );    
    } 
    if(javaEncoding == null)   // Should never return null
        javaEncoding = ""UTF8"";
    return(javaEncoding);
    ```

The key change is moving the `javaEncoding = ""UTF8"";` assignment outside the `else` block of the `if (mimeEncoding != null)` condition and placing it within a new `if (javaEncoding == null)` condition.

**Reasoning:**

*   The original code assumed that if `mimeEncoding` was `null`, `javaEncoding` would always be set to ""UTF8"". However, if `mimeEncoding` was not null, and none of the `if` or `else if` conditions were met, and `EncodingMap.getIANA2JavaMapping( mimeEncoding )` returned null, then `javaEncoding` would remain null.
*   The modified code explicitly checks if `javaEncoding` is `null` after the initial `if (mimeEncoding != null)` block. If it is, it assigns ""UTF8"" to it. This ensures that `javaEncoding` always has a value before being returned, even in the case where `mimeEncoding` is not null, but no suitable mapping is found.
*   The comment ""// Should never return null"" suggests that the original intention was to always return a non-null value. The modified code enforces this intention more robustly.

**Conclusion:**

The code change addresses a potential scenario where `javaEncoding` could be `null`, which was likely causing issues (hence, the bug #6008). The modification ensures that a default value (""UTF8"") is assigned when no other encoding can be determined. This is a clear indication of a bug fix.

**Buggy**"
xerces2-j,5806.json,f6fbb6219a5d2f0cfd71f23812b4571049b5613b,"@@ -1,93 +1,97 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
             if (this.chardata == '{' && this.offset < this.regexlen) {
 
                 int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
 
                 if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
 
                     min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
+                        if (min < 0)
+                            throw ex(""parser.quantifier.5"", this.offset);
                     }
                 }
                 else {
                     throw ex(""parser.quantifier.1"", this.offset);
                 }
 
                 max = min;
                 if (ch == ',') {
 
                    if (off >= this.regexlen) {
                        throw ex(""parser.quantifier.3"", this.offset);
                    }
                    else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
 
                         max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
+                            if (max < 0)
+                                throw ex(""parser.quantifier.5"", this.offset);
                         }
 
                         if (min > max)
                             throw ex(""parser.quantifier.4"", this.offset);
                    }
                    else { // assume {min,}
                         max = -1;           
                     }
                 }
 
                if (ch != '}')
                    throw ex(""parser.quantifier.2"", this.offset);
 
                if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
 
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17415: Regexes with large min/max not handled correctly.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug related to handling regexes with large min/max values in quantifiers. The diff shows that the code was modified to add checks for integer overflow when parsing the min and max values within a quantifier (e.g., `{min,max}`). Specifically, the code now throws an exception if `min` or `max` becomes negative during the parsing process, which would indicate an integer overflow. This aligns with the commit message's description of the bug.

Therefore, the changes indicate a bug fix.

**Buggy**"
xerces2-j,5806.json,218c54d8d80c22fd4d9f45f8013e14eb23838699,"@@ -1,89 +1,93 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
-            if (this.chardata == '{') {
-                                                // this.offset -> next of '{'
-                int off = this.offset;
+            if (this.chardata == '{' && this.offset < this.regexlen) {
+
+                int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
-                if (off >= this.regexlen)  break;
-                ch = this.regex.charAt(off++);
-                if (ch != ',' && (ch < '0' || ch > '9'))  break;
-                if (ch != ',') {                // 0-9
-                    min = ch-'0';
+
+                if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
+
+                    min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
-                        ch = -1;
                     }
-                    if (ch < 0)  break;
                 }
-                //if (off >= this.regexlen)  break;
+                else {
+                    throw ex(""parser.quantifier.1"", this.offset);
+                }
+
                 max = min;
                 if (ch == ',') {
-                    if (off >= this.regexlen
-                        || ((ch = this.regex.charAt(off++)) < '0' || ch > '9')
-                        && ch != '}')
-                        break;
-                    if (ch == '}') {
-                        max = -1;           // {min,}
-                    } else {
-                        max = ch-'0';       // {min,max}
+
+                   if (off >= this.regexlen) {
+                       throw ex(""parser.quantifier.3"", this.offset);
+                   }
+                   else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
+
+                        max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
-                            ch = -1;
                         }
-                        if (ch < 0)  break;
-                        //if (min > max)
-                        //    throw new ParseException(""parseFactor(): min > max: ""+min+"", ""+max);
+
+                        if (min > max)
+                            throw ex(""parser.quantifier.4"", this.offset);
+                   }
+                   else { // assume {min,}
+                        max = -1;           
                     }
                 }
-                if (ch != '}')  break;
-                                                // off -> next of '}'
-                if (this.checkQuestion(off)) {
+
+               if (ch != '}')
+                   throw ex(""parser.quantifier.2"", this.offset);
+
+               if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
+
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17417: Regex {min,max} with min > max not rejected.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319159 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug where the regex parser was not rejecting quantifiers of the form `{min,max}` when `min > max`. The diff shows changes within the `parseFactor` method, specifically when handling the `{` character that indicates a quantifier.

Here's a breakdown of the changes:

1.  **Boundary Check**: A check `this.offset < this.regexlen` is added to ensure that the offset is within the bounds of the regex string before proceeding with quantifier parsing. This prevents potential `IndexOutOfBoundsException`.
2.  **Quantifier Parsing**: The code now explicitly checks if the characters following `{` are digits. If not, it throws a `ParseException` with the message ""parser.quantifier.1"". This enforces the correct syntax for quantifiers.
3.  **Error Handling for min > max**: The code now includes a check `if (min > max)` and throws a `ParseException` with the message ""parser.quantifier.4"" if the minimum value is greater than the maximum value in the quantifier. This directly addresses the bug described in the commit message.
4.  **Error Handling for missing '}'**: The code now throws a `ParseException` with the message ""parser.quantifier.2"" if the closing `}` is missing.
5.  **Exception throwing**: The code now throws exceptions instead of breaking out of the parsing logic.

These changes clearly indicate a bug fix related to incorrect parsing of regex quantifiers, specifically addressing the case where `min > max` and other syntax errors. The added error handling and validation directly align with the commit message.

**Buggy**
"
xerces2-j,7034.json,ba7898b39060234e9c88e9e06c0dd5fbed52a9fb,"@@ -1,12 +1,13 @@
     public void startDocument() throws SAXException {
         fNeedPushNSContext = true;
+        fNamespaceContext.reset();
         try {
             fSchemaDOMParser.startDocument(fSAXLocatorWrapper, null, fNamespaceContext, null);
         }
         catch (XMLParseException e) {
             convertToSAXParseException(e);
         }
         catch (XNIException e) {
             convertToSAXException(e);
         }
     }",Buggy,"Fixing a bug. We weren't reseting the NamespaceContext which can cause all sorts
of problems including the loss of some namespace decls on XSAnnotation components.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@644489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `NamespaceContext` not being reset, potentially leading to loss of namespace declarations. The diff shows that `fNamespaceContext.reset()` is added at the beginning of the `startDocument()` method. This aligns perfectly with the commit message, suggesting that the added line is intended to fix the described bug by ensuring the `NamespaceContext` is properly reset before processing a new document.

**Buggy**"
xerces2-j,3375.json,883dc18e1cd70be3f4dc6fcaf48ddcdfe4c2a99d,"@@ -1,19 +1,19 @@
         private int parseInt(int minDigits, int maxDigits)
             throws IllegalArgumentException {
             int vstart = vidx;
-            while (isDigit(peek()) && (vidx - vstart) <= maxDigits) {
+            while (isDigit(peek()) && (vidx - vstart) < maxDigits) {
                 vidx++;
             }
             if ((vidx - vstart) < minDigits) {
                 // we are expecting more digits
                 throw new IllegalArgumentException(value); //,vidx);
             }
 
             // NumberFormatException is IllegalArgumentException            
             //           try {
             return Integer.parseInt(value.substring(vstart, vidx));
             //            } catch( NumberFormatException e ) {
             //                // if the value is too long for int, NumberFormatException is thrown
             //                throw new IllegalArgumentException(value,vstart);
             //            }
         }",Buggy,"Fixing a bug. parseInt() was allowing maxDigits + 1 which caused 
XMLGregorianCalendar to accept bogus dates like ""2007-008-003"".

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@565088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `parseInt()` method. The method was incorrectly allowing `maxDigits + 1` digits, leading to invalid dates. The diff shows a change in the `while` loop condition from `(vidx - vstart) <= maxDigits` to `(vidx - vstart) < maxDigits`. This change ensures that the method now correctly enforces the maximum number of digits, preventing the acceptance of bogus dates. The change directly addresses the bug described in the commit message.

**Buggy**"
xerces2-j,6542.json,3574857d4a4ddaf8aabec99216c178a5a6089a19,"@@ -1,46 +1,46 @@
     private void setOrdered(){
 
-        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all ﾷprimitiveﾷ types {value} is as specified in the table in Fundamental Facets (C.1).
+        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all ""primitive"" types {value} is as specified in the table in Fundamental Facets (C.1).
         if(fVariety == VARIETY_ATOMIC){
             this.fOrdered = fBase.fOrdered;
         }
 
         // When {variety} is list, {value} is false.
         else if(fVariety == VARIETY_LIST){
             this.fOrdered = ORDERED_FALSE;
         }
 
         // When {variety} is union, the {value} is partial unless one of the following:
         // 1. If every member of {member type definitions} is derived from a common ancestor other than the simple ur-type, then {value} is the same as that ancestor's ordered facet.
         // 2. If every member of {member type definitions} has a {value} of false for the ordered facet, then {value} is false.
         else if(fVariety == VARIETY_UNION){
             int length = fMemberTypes.length;
             // REVISIT: is the length possible to be 0?
             if (length == 0) {
                 this.fOrdered = ORDERED_PARTIAL;
                 return;
             }
             // we need to process the first member type before entering the loop
             short ancestorId = getPrimitiveDV(fMemberTypes[0].fValidationDV);
             boolean commonAnc = ancestorId != DV_ANYSIMPLETYPE;
             boolean allFalse = fMemberTypes[0].fOrdered == ORDERED_FALSE;
             // for the other member types, check whether the value is false
             // and whether they have the same ancestor as the first one
             for (int i = 1; i < fMemberTypes.length && (commonAnc || allFalse); i++) {
                 if (commonAnc)
                     commonAnc = ancestorId == getPrimitiveDV(fMemberTypes[i].fValidationDV);
                 if (allFalse)
                     allFalse = fMemberTypes[i].fOrdered == ORDERED_FALSE;
             }
             if (commonAnc) {
                 // REVISIT: all member types should have the same ordered value
                 //          just use the first one. Can we assume this?
                 this.fOrdered = fMemberTypes[0].fOrdered;
             } else if (allFalse) {
                 this.fOrdered = ORDERED_FALSE;
             } else {
                 this.fOrdered = ORDERED_PARTIAL;
             }
         }
 
     }//setOrdered",Buggy,"fixing bug 16714


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code modifies the `setOrdered` method, specifically addressing the logic for `VARIETY_UNION`. The original code had a comment ""REVISIT: is the length possible to be 0?"". The code now handles the case where `fMemberTypes.length` is 0, setting `fOrdered` to `ORDERED_PARTIAL` and returning. The code also contains comments like ""REVISIT: all member types should have the same ordered value just use the first one. Can we assume this?"". These comments and the added null check suggest the original code had potential issues or edge cases that were not properly handled. The commit message ""fixing bug 16714"" further reinforces the idea that this change is a bug fix.

**Buggy**
"
xerces2-j,7850.json,d6e418eec78c8b253b3f353be05259d353faa58a,"@@ -1,12 +1,12 @@
     private CMNode multiNodes(CMNode node, int num, boolean copyFirst) {
         if (num == 0) {
             return null;
         }
         if (num == 1) {
             return copyFirst ? copyNode(node) : node;
         }
         int num1 = num/2;
         return fNodeFactory.getCMBinOpNode(XSModelGroupImpl.MODELGROUP_SEQUENCE,
                                            multiNodes(node, num1, copyFirst),
-                                           multiNodes(node, num-num1, false));
+                                           multiNodes(node, num-num1, true));
     }",Buggy,"Fixing a bug introduced by a previous commit when trying to balance the
syntax tree.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319270 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to balancing the syntax tree. The code diff shows a change in the `multiNodes` method, specifically in the last argument of the recursive call `multiNodes(node, num-num1, true)`. The argument was changed from `false` to `true`. This suggests that the previous value `false` was causing an issue in the tree balancing logic, and changing it to `true` corrects this. This aligns with the commit message indicating a bug fix.

**Buggy**"
xerces2-j,8612.json,841a543458d7b19286dbcc32207d8978ccf195f5,"@@ -1,19 +1,19 @@
     protected void init() {
         if(fValidation || fDynamicValidation) {
             super.init();
             // now overwrite some entries in parent:
 
             try {
                 fValID       = fDatatypeValidatorFactory.getBuiltInDV(""XML11ID"");
-                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
+                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREF"");
                 fValIDRefs   = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
-                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENSymbol"");
+                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKEN"");
                 fValNMTOKENS = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENS"");
 
             }
             catch (Exception e) {
                 // should never happen
                 e.printStackTrace(System.err);
             }
         }
     } // init()",Buggy,"Fixing 2 bugs:
1) Bugzilla# 18429, NPE thrown validating NMTOKEN. Thanks to Neil Delima for the patch.
2) S production is unchanged in XML 1.1, so shouldn't override isSpace()


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319269 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates two bug fixes. Let's analyze the diff to see if it aligns with the description.

The first bug fix mentioned is a NullPointerException (NPE) thrown while validating NMTOKEN, attributed to Bugzilla# 18429. The diff changes `fValNMTOKEN = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENSymbol"");` to `fValNMTOKEN = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKEN"");`. This suggests that the original code was using an incorrect validator (""XML11NMTOKENSymbol"") which might have caused an NPE under certain circumstances, and it has been corrected to use the correct validator (""XML11NMTOKEN"").

The second bug fix addresses the `S` production in XML 1.1, stating that it is unchanged and `isSpace()` shouldn't be overridden. However, there is no code change related to `isSpace()` in the provided diff. It's possible that the `isSpace()` fix is in a different part of the codebase and not included in this diff.

The change related to NMTOKEN validation clearly indicates a bug fix, as it corrects the validator being used. Therefore, the diff provides evidence of a bug fix.

**Buggy**"
xerces2-j,27.json,874363b725dcbddf23ed1f1d07d9a6fcd6e3ca4c,"@@ -1,46 +1,47 @@
     public void attributeDecl(String elementName, String attributeName,
                               String type, String[] enumeration,
                               String defaultType, XMLString defaultValue, 
+                              XMLString nonNormalizedDefaultValue,
                               Augmentations augs) throws XNIException {
 
         printIndent();
         fOut.print(""attributeDecl("");
         fOut.print(""elementName="");
         printQuotedString(elementName);
         fOut.print(',');
         fOut.print(""attributeName="");
         printQuotedString(attributeName);
         fOut.print(',');
         fOut.print(""type="");
         printQuotedString(type);
         fOut.print(',');
         fOut.print(""enumeration="");
         if (enumeration == null) {
             fOut.print(""null"");
         }
         else {
             fOut.print('{');
             for (int i = 0; i < enumeration.length; i++) {
                 printQuotedString(enumeration[i]);
                 if (i < enumeration.length - 1) {
                     fOut.print(',');
                 }
             }
             fOut.print('}');
         }
         fOut.print(',');
         fOut.print(""defaultType="");
         printQuotedString(defaultType);
         fOut.print(',');
         fOut.print(""defaultValue="");
         if (defaultValue == null) {
             fOut.print(""null"");
         }
         else {
             printQuotedString(defaultValue.ch, defaultValue.offset,
                               defaultValue.length);
         }
         fOut.println(')');
         fOut.flush();
 
     } // attributeDecl(String,String,String,String[],String,XMLString)",Buggy,"corrected attributeDecl signature in samples.
Thanks to John Spitzer for pointing that out (bug#6447).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318206 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a correction to the `attributeDecl` signature in the samples, attributed to a bug report (bug#6447). The diff shows the addition of the `XMLString nonNormalizedDefaultValue` parameter to the `attributeDecl` method. This change directly addresses the reported bug by correcting the method's signature. Therefore, the commit represents a bug fix.

**Buggy**"
xerces2-j,7660.json,c034fce51f8f78ab929b74f4a902deb10034459a,"@@ -1,5 +1,5 @@
     static final Field[] resize(Field[] oldArray, int newSize) {
         Field[] newArray = new Field[newSize];
-        System.arraycopy(oldArray, 0, newArray, 0, newSize);
+        System.arraycopy(oldArray, 0, newArray, 0, oldArray.length);
         return newArray;
     }",Buggy,"Fixing bug 9022: arrayCopy should take oldArray.length, instead of newSize.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318392 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 9022, specifically addressing an issue in the `arrayCopy` method where the wrong size was being used. The diff shows that the `System.arraycopy` method's length argument was changed from `newSize` to `oldArray.length`. This change aligns perfectly with the commit message, suggesting that the original code was incorrectly using the new array's size instead of the old array's size, potentially leading to data truncation or `ArrayIndexOutOfBoundsException` if `newSize` was smaller or larger than `oldArray.length` respectively. This is a clear indication of a bug fix.

**Buggy**"
xerces2-j,938.json,0dde63af37b11224e5bc46928596405440cd0107,"@@ -1,15 +1,8 @@
     public Node cloneNode( boolean deep )
     {
         HTMLDocumentImpl    clone;
-        NodeImpl            node;
 
         clone = new HTMLDocumentImpl();
-        if ( deep ) {
-            node = (NodeImpl) getFirstChild();
-            while ( node != null ) {
-                clone.appendChild( clone.importNode( node, true ) );
-                node = (NodeImpl) node.getNextSibling();
-            }
-        }
+        cloneNode(clone, deep);
         return clone;
     }",Buggy,"Fixing JIRA Bug #1021:
http://issues.apache.org/jira/browse/XERCESJ-1021

DocumentType nodes cannot be imported. Instead of attempting to loop
over the children of the Document node we should delegate to the
cloneNode(CoreDocumentImpl,boolean) method on DocumentImpl which
knows how to do the right thing. This should also fix ID assignment
and possibly other things.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@418366 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug (XERCESJ-1021) where `DocumentType` nodes could not be imported correctly. The message explains that the original code attempted to loop over the children of the `Document` node, which was incorrect. The fix involves delegating the cloning process to the `cloneNode(CoreDocumentImpl, boolean)` method on `DocumentImpl`, which is designed to handle the cloning correctly, including ID assignment.

The diff shows that the original implementation iterated through the children of the document and imported each node. The modified code replaces this loop with a call to `cloneNode(clone, deep)`. This change aligns with the commit message's description of delegating the cloning process to a more appropriate method. The commit message explicitly mentions that the previous implementation had issues with `DocumentType` nodes and ID assignment, which suggests a bug in the original code. The change in the code directly addresses the bug described in the commit message.

**Buggy**"
xerces2-j,8262.json,ef4f12df6ceef58d0976e59c42d72c11836172c8,"@@ -1,4 +1,10 @@
     public void setInputSource(XMLInputSource inputSource) throws IOException {
+        if (inputSource == null) {
+            // no system id was available
+            fDTDHandler.startDTD(null, null);
+            fDTDHandler.endDTD(null);
+            return;
+        }
         fEntityManager.setEntityHandler(this);
         fEntityManager.startDTDEntity(inputSource);
     } // setInputSource(XMLInputSource)",Buggy,"Fixing bug http://nagoya.apache.org/bugzilla/show_bug.cgi?id=11176


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318583 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to bugzilla issue 11176. The code diff introduces a null check for `inputSource` in the `setInputSource` method. If `inputSource` is null, it calls `startDTD` and `endDTD` on `fDTDHandler` with null arguments and returns. This suggests that the previous code might have thrown a `NullPointerException` or caused other issues when `inputSource` was null. The added null check and the subsequent handling of the null case indicate a bug fix.

**Buggy**"
xerces2-j,1373.json,51be532c652cf9a20ca7e950b34b2f00f7dafdd3,"@@ -1,4 +1,4 @@
     public void printText( StringBuffer text )
     {
-        _text.append( text );
+        _text.append( text.toString() );
     }",Buggy,"fixing bug 15768.  According to the javadoc for StringBuffer and String, this is what the original code would have compiled to; this fix simply makes sure that, if compiled under JDK 1.4, Xerces will work with previous JDKs


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318938 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `StringBuffer` and `String` classes, specifically addressing compatibility issues between JDK 1.4 and previous JDK versions. The code change involves modifying the `printText` method to explicitly convert the `StringBuffer` to a `String` before appending it to the `_text` field. This suggests that the original code might have had issues when compiled under JDK 1.4 and used with older JDKs, potentially due to differences in how `StringBuffer` was handled. The explicit conversion to `String` ensures consistent behavior across different JDK versions, which aligns with the bug fix described in the commit message.

**Buggy**"
xerces2-j,5333.json,5ba592aa9007de9e5392ebab7095bfd0cfa63824,"@@ -1,13 +1,13 @@
     protected boolean sameBaseURIAsIncludeParent() {
         String parentBaseURI = getIncludeParentBaseURI();
         String baseURI = fCurrentBaseURI.getExpandedSystemId();
         // REVISIT: should we use File#sameFile() ?
         //          I think the benefit of using it is that it resolves host names
         //          instead of just doing a string comparison.
         // TODO: [base URI] is still an open issue with the working group.
         //       They're deciding if xml:base should be added if the [base URI] is different in terms
         //       of resolving relative references, or if it should be added if they are different at all.
         //       Revisit this after a final decision has been made.
         //       The decision also affects whether we output the file name of the URI, or just the path.
-        return parentBaseURI.equals(baseURI);
+        return parentBaseURI != null && parentBaseURI.equals(baseURI);
     }",Buggy,"Partial fix for Bug #24992. Fix NPE which would occur if an 
include parent has no base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319611 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when an include parent has no base URI. The code diff modifies the `sameBaseURIAsIncludeParent()` method by adding a null check for `parentBaseURI` before comparing it with `baseURI`. This change directly addresses the reported NPE by preventing the `equals()` method from being called on a null `parentBaseURI`. Therefore, the code change aligns with the commit message and fixes a bug.

**Buggy**"
hibernate-orm,14257.json,8463057b85db928fe8d3aa0b4379bc5b0f255c71,"@@ -1,110 +1,123 @@
 	private void addSetter(ClassFile classfile, final Method[] setters) throws CannotCompileException {
 		ConstPool cp = classfile.getConstPool();
 		int target_type_index = cp.addClassInfo( this.targetBean.getName() );
 		String desc = GET_SETTER_DESC;
 		MethodInfo mi = new MethodInfo( cp, GENERATED_SETTER_NAME, desc );
 
 		Bytecode code = new Bytecode( cp, 4, 6 );
+		StackMapTable stackmap = null;
 		/* | this | bean | args | i | raw bean | exception | */
 		if ( setters.length > 0 ) {
 			int start, end; // required to exception table
 			// iconst_0 // i
 			code.addIconst( 0 );
 			// istore_3 // store i
 			code.addIstore( 3 );
 			// aload_1 // load the bean
 			code.addAload( 1 );
 			// checkcast // cast the bean into a raw bean
 			code.addCheckcast( this.targetBean.getName() );
 			// astore 4 // store the raw bean
 			code.addAstore( 4 );
 			/* current stack len = 0 */
 			// start region to handling exception (BulkAccessorException)
 			start = code.currentPc();
 			int lastIndex = 0;
 			for ( int i = 0; i < setters.length; ++i ) {
 				if ( setters[i] != null ) {
 					int diff = i - lastIndex;
 					if ( diff > 0 ) {
 						// iinc 3, 1
 						code.addOpcode( Opcode.IINC );
 						code.add( 3 );
 						code.add( diff );
 						lastIndex = i;
 					}
 				}
 				/* current stack len = 0 */
 				// aload 4 // load the raw bean
 				code.addAload( 4 );
 				// aload_2 // load the args
 				code.addAload( 2 );
 				// iconst_i
 				code.addIconst( i );
 				// aaload
 				code.addOpcode( Opcode.AALOAD );
 				// checkcast
 				Class[] setterParamTypes = setters[i].getParameterTypes();
 				Class setterParamType = setterParamTypes[0];
 				if ( setterParamType.isPrimitive() ) {
 					// checkcast (case of primitive type)
 					// invokevirtual (case of primitive type)
 					this.addUnwrapper( classfile, code, setterParamType );
 				}
 				else {
 					// checkcast (case of reference type)
 					code.addCheckcast( setterParamType.getName() );
 				}
 				/* current stack len = 2 */
 				String rawSetterMethod_desc = RuntimeSupport.makeDescriptor( setters[i] );
 				if ( !this.targetBean.isInterface() ) {
 					// invokevirtual
 					code.addInvokevirtual( target_type_index, setters[i].getName(), rawSetterMethod_desc );
 				}
 				else {
 					// invokeinterface
 					Class[] params = setters[i].getParameterTypes();
 					int size;
 					if ( params[0].equals( Double.TYPE ) || params[0].equals( Long.TYPE ) ) {
 						size = 3;
 					}
 					else {
 						size = 2;
 					}
 
 					code.addInvokeinterface( target_type_index, setters[i].getName(), rawSetterMethod_desc, size );
 				}
 			}
 
 			// end region to handling exception (BulkAccessorException)
 			end = code.currentPc();
 			// return
 			code.addOpcode( Opcode.RETURN );
 			/* current stack len = 0 */
 			// register in exception table
 			int throwableType_index = cp.addClassInfo( THROWABLE_CLASS_NAME );
-			code.addExceptionHandler( start, end, code.currentPc(), throwableType_index );
+			int handler_pc = code.currentPc();
+			code.addExceptionHandler( start, end, handler_pc, throwableType_index );
 			// astore 5 // store exception
 			code.addAstore( 5 );
 			// new // BulkAccessorException
 			code.addNew( BULKEXCEPTION_CLASS_NAME );
 			// dup
 			code.addOpcode( Opcode.DUP );
 			// aload 5 // load exception
 			code.addAload( 5 );
 			// iload_3 // i
 			code.addIload( 3 );
 			// invokespecial // BulkAccessorException.<init>
 			String cons_desc = ""(Ljava/lang/Throwable;I)V"";
 			code.addInvokespecial( BULKEXCEPTION_CLASS_NAME, MethodInfo.nameInit, cons_desc );
 			// athrow
 			code.addOpcode( Opcode.ATHROW );
+			StackMapTable.Writer writer = new StackMapTable.Writer(32);
+			int[] localTags = { StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.INTEGER };
+			int[] localData = { cp.getThisClassInfo(), cp.addClassInfo(""java/lang/Object""),
+                        	cp.addClassInfo(""[Ljava/lang/Object;""), 0};
+			int[] stackTags = { StackMapTable.OBJECT };
+			int[] stackData = { throwableType_index };
+			writer.fullFrame(handler_pc, localTags, localData, stackTags, stackData);
+			stackmap = writer.toStackMapTable(cp);
 		}
 		else {
 			// return
 			code.addOpcode( Opcode.RETURN );
 		}
-
-		mi.setCodeAttribute( code.toCodeAttribute() );
+		CodeAttribute ca = code.toCodeAttribute();
+		if (stackmap != null) {
+			ca.setAttribute(stackmap);
+		}
+		mi.setCodeAttribute( ca );
 		mi.setAccessFlags( AccessFlag.PUBLIC );
 		classfile.addMethod( mi );
 	}",Buggy,"HHH-7850 BulkAccessorFactory Java 7 verify error resolved by JASSIST-163 BulkAccessorFactory.java.diff2 patch from Shigeru Chiba.
",Buggy,"The commit message indicates that the change resolves a ""Java 7 verify error"" related to `BulkAccessorFactory` and mentions ""JASSIST-163"". This strongly suggests a bug fix.

The diff includes changes to exception handling within the `addSetter` method. Specifically, a `StackMapTable` is added to the generated code. StackMapTable is used for bytecode verification, and its absence or incorrectness can lead to `VerifyError` exceptions, especially in Java 7 and later. The added stack map frame provides information about the types on the stack and in local variables at the exception handler, which is crucial for the bytecode verifier.

The addition of the `StackMapTable` directly addresses the ""Java 7 verify error"" mentioned in the commit message. This confirms that the commit is indeed a bug fix.

**Buggy**"
hibernate-orm,15873.json,8fb35947ff6525a8286295f2959cd5a71502b2f9,"@@ -1,3 +1,3 @@
 	public String getQuerySequencesString() {
-		return ""select table_name from information_schema.TABLES where table_type='SEQUENCE'"";
+		return ""select table_name from information_schema.TABLES where table_schema = database() and table_type = 'SEQUENCE'"";
 	}",Buggy,"HHH-13373 fix the bug that 'different sequence names in differnt dbs' breaks SequenceInformationExtractorMariaDBDatabaseImpl
",Buggy,"The commit message indicates a bug fix related to sequence name handling in MariaDB when different databases have different sequence names. The code change modifies the SQL query used to retrieve sequence names.

The original query `select table_name from information_schema.TABLES where table_type='SEQUENCE'` retrieves all tables of type 'SEQUENCE' from the `information_schema`. This query doesn't restrict the results to the current database.

The modified query `select table_name from information_schema.TABLES where table_schema = database() and table_type = 'SEQUENCE'` adds a condition `table_schema = database()` to filter the results to only include sequences within the current database.

This change directly addresses the issue described in the commit message, where different databases having the same sequence names could lead to incorrect behavior. By restricting the query to the current database, the bug is resolved.

Therefore, the changes indicate a bug fix.

**Buggy**
"
hibernate-orm,15816.json,fc6312a73b99c8edf2af32122165cc8b31b79aa7,"@@ -1,3 +1,8 @@
 	public String getQuerySequencesString() {
-		return null ;
+		if ( supportsSequences() ) {
+			return ""select SEQUENCENAME from SYS.SYSSEQUENCES"";
+		}
+		else {
+			throw new MappingException( ""Derby does not support sequence prior to release 10.6.1.0"" );
+		}
 	}",Buggy,"HHH-10110 - Fix DerbyDialect#getQuerySequencesString() causing error during schema update
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HHH-10110 - Fix DerbyDialect#getQuerySequencesString() causing error during schema update"" clearly states that the change is intended to fix an error in the `DerbyDialect#getQuerySequencesString()` method that was causing issues during schema updates. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows that the original implementation of `getQuerySequencesString()` simply returned `null`. The new implementation checks if the Derby database supports sequences using `supportsSequences()`. If sequences are supported, it returns a SQL query to retrieve sequence names. If sequences are not supported (older Derby versions), it throws a `MappingException` indicating that sequences are not supported prior to version 10.6.1.0.

**Reasoning:**

1.  **Error Indication:** The original `null` return likely caused a `NullPointerException` or other unexpected behavior during schema updates when the Hibernate code expected a valid SQL query string. The commit message confirms this.
2.  **Conditional Logic:** The new implementation introduces conditional logic based on `supportsSequences()`. This suggests that the previous implementation was not handling different Derby versions correctly, leading to the error.
3.  **Exception Handling:** The addition of the `MappingException` for older Derby versions further supports the idea of a bug fix. It provides a more informative error message when sequences are not supported, preventing the application from crashing or behaving unexpectedly.
4.  **Schema Update Context:** The commit message explicitly mentions that the error occurred during schema updates. This is a critical operation, and fixing errors in this area is typically considered a bug fix.

**Conclusion:**

The commit message and the code changes strongly indicate that this commit is a bug fix. The original implementation was flawed, leading to errors during schema updates. The new implementation addresses this by providing a correct SQL query for supported Derby versions and throwing an exception for older versions.

**Buggy**"
hibernate-orm,22027.json,b33c6d435780a96d6f911fbf8a9d796f10c7af0d,"@@ -1,15 +1,17 @@
-	public void onLoad(final LoadEvent event,
-					   final LoadEventListener.LoadType loadType) throws HibernateException {
+	public void onLoad(
+			final LoadEvent event,
+			final LoadEventListener.LoadType loadType) throws HibernateException {
 
 		final EntityPersister persister = getPersister( event );
 
 		if ( persister == null ) {
 			throw new HibernateException( ""Unable to locate persister: "" + event.getEntityClassName() );
 		}
 
 		final Class idClass = persister.getIdentifierType().getReturnedClass();
-		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) )
+		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) ) {
 			checkIdClass( persister, event, loadType, idClass );
+		}
 
 		doOnLoad( persister, event, loadType );
 	}",Buggy,"HHH-10366 - Fix checkstyle error
",NotBuggy,"The commit message ""HHH-10366 - Fix checkstyle error"" indicates that the purpose of this commit is to address a checkstyle violation. The diff shows changes in whitespace and line breaks within the `onLoad` method. These changes do not introduce any new logic, alter existing functionality, or address any bug. They simply reformat the code to adhere to the checkstyle rules.

**NotBuggy**"
hibernate-orm,15530.json,220365600cd7692ede1977d366966a5828dc749c,"@@ -1,7 +1,14 @@
 	public String getProcessedSql() {
-		boolean useLimitOffset = supportsLimit() && supportsLimitOffset()
-				&& LimitHelper.hasFirstRow( selection ) && LimitHelper.hasMaxRows( selection );
-		return dialect.getLimitString(
-				sql, useLimitOffset ? LimitHelper.getFirstRow( selection ) : 0, getMaxOrLimit()
-		);
+		if (LimitHelper.useLimit(this, selection)) {
+			// useLimitOffset: whether ""offset"" is set or not;
+			// if set, use ""LIMIT offset, row_count"" syntax;
+			// if not, use ""LIMIT row_count""
+			boolean useLimitOffset = LimitHelper.hasFirstRow(selection);
+
+			return new StringBuilder(sql.length() + 20).append(sql)
+							.append(useLimitOffset ? "" limit ?, ?"" : "" limit ?"").toString();
+		}
+		else {
+			return sql; // or return unaltered SQL
+		}
 	}",Buggy,"HHH-7716 Fixed a bug in CUBRIDLimitHandler. Now correctly processes
LIMIT
clause
in SQL.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HHH-7716 Fixed a bug in CUBRIDLimitHandler. Now correctly processes LIMIT clause in SQL"" explicitly states that a bug related to the handling of the `LIMIT` clause in SQL queries within the `CUBRIDLimitHandler` has been fixed. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `getProcessedSql()` method. Let's break down the changes:

1.  **Original Code:** The original code used `supportsLimit()`, `supportsLimitOffset()`, `LimitHelper.hasFirstRow(selection)`, and `LimitHelper.hasMaxRows(selection)` to determine whether to apply the limit. It then used `dialect.getLimitString()` to construct the SQL with the limit.

2.  **Modified Code:** The modified code introduces a conditional block based on `LimitHelper.useLimit(this, selection)`.  If `useLimit` is true, it constructs the `LIMIT` clause directly using string concatenation. It also checks `LimitHelper.hasFirstRow(selection)` to decide between `LIMIT ?, ?` (offset and row count) and `LIMIT ?` (row count only). If `useLimit` is false, it returns the original SQL.

**Reasoning:**

*   The commit message explicitly mentions a bug fix related to the `LIMIT` clause.
*   The code changes directly affect how the `LIMIT` clause is processed. The original code relied on the dialect to handle the limit, while the modified code constructs the `LIMIT` clause directly. This suggests that the dialect-specific handling might have been incorrect, leading to the bug.
*   The introduction of the `LimitHelper.useLimit` condition suggests that the previous logic for determining whether to apply the limit was flawed.
*   The change from using `dialect.getLimitString` to directly appending ""limit"" indicates a more direct and potentially corrected handling of the LIMIT clause.
*   The handling of `offset` via `useLimitOffset` suggests a fix for cases where the offset was not being handled correctly.

**Conclusion:**

Based on the commit message and the code changes, it's clear that the commit addresses a bug in how the `LIMIT` clause was being processed. The code was modified to correctly handle the `LIMIT` clause, including the offset.

**Buggy**
"
hibernate-orm,26062.json,6a14aeeba79d3536a893f63154c0b697db3f3711,"@@ -1,13 +1,15 @@
 	private void processExpression(ReadWriteExpression annotation) {
 		String nonNullLogicalColumnName = logicalColumnName != null ? logicalColumnName : """"; //use the default for annotations 
-		if ( annotation != null && annotation.forColumn().equals( nonNullLogicalColumnName ) ) {
+		if ( annotation != null &&
+				( StringHelper.isEmpty( annotation.forColumn() )
+						|| annotation.forColumn().equals( nonNullLogicalColumnName ) ) ) {
 			readExpression = annotation.read();
 			if ( StringHelper.isEmpty( readExpression ) ) {
 				readExpression = null;
 			}
 			writeExpression = annotation.write();
 			if ( StringHelper.isEmpty( writeExpression ) ) {
 				writeExpression = null;
 			}
 		}
 	}",Buggy,"HHH-4510 Fix bug where explicit @Column would not match @ReadWriteExpresion with empty forColumn

git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@20747 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message indicates a bug fix related to `@Column` not matching `@ReadWriteExpression` when `forColumn` is empty.

The code diff modifies the condition that checks if a `ReadWriteExpression` annotation should be processed. The original condition only checked for equality between `annotation.forColumn()` and `nonNullLogicalColumnName`. The modified condition now includes an additional check: `StringHelper.isEmpty( annotation.forColumn() )`. This means the expression will be processed if `annotation.forColumn()` is empty, which aligns with the bug described in the commit message.

The change addresses the scenario where an explicit `@Column` with an empty `forColumn` attribute in `@ReadWriteExpression` would not be matched, thus fixing a bug.

**Buggy**
"
hibernate-orm,4331.json,f5e10c29ebf103df4de7ea7b528702f0d2d9fc2a,"@@ -1,9 +1,18 @@
 	public void destroy() {
 		if ( !active ) {
 			return;
 		}
 		active = false;
 		destroy( classLoaderServiceBinding );
 		destroy( strategySelectorBinding );
 		destroy( integratorServiceBinding );
+
+		if ( childRegistries != null ) {
+			for(ServiceRegistry serviceRegistry : childRegistries) {
+				if(serviceRegistry instanceof ServiceRegistryImplementor) {
+					ServiceRegistryImplementor serviceRegistryImplementor = (ServiceRegistryImplementor) serviceRegistry;
+					serviceRegistryImplementor.destroy();
+				}
+			}
+		}
 	}",Buggy,"HHH-10907 - Fix connection leak problem in hibernate-core tests
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Understanding the Commit Message:**

The commit message ""HHH-10907 - Fix connection leak problem in hibernate-core tests"" explicitly states that the purpose of the commit is to address a connection leak issue within the hibernate-core tests. This strongly suggests a bug fix.

**2. Analyzing the Code Diff:**

The code diff shows a modification to the `destroy()` method.  Specifically, it adds a loop that iterates through `childRegistries`. Inside the loop, it checks if each `serviceRegistry` is an instance of `ServiceRegistryImplementor`. If it is, it casts the `serviceRegistry` to `ServiceRegistryImplementor` and calls its `destroy()` method.

**3. Reasoning:**

*   **Connection Leak Indication:** The commit message mentions a ""connection leak."" Connection leaks often occur when resources (like database connections) are not properly released or closed after use.
*   **Resource Management:** The added code iterates through `childRegistries` and calls `destroy()` on each `ServiceRegistryImplementor`. The `destroy()` method is typically responsible for releasing resources held by the service registry.
*   **Potential Cause:** The original code might have been failing to properly destroy child service registries, leading to unreleased connections and a connection leak. The added code ensures that these child registries are explicitly destroyed, which would close any associated connections.
*   **Test Context:** The commit message indicates that this fix is within the ""hibernate-core tests"". Tests often create and destroy resources rapidly, making connection leaks more apparent.

**4. Conclusion:**

The commit message clearly states a bug fix related to connection leaks. The code changes involve explicitly destroying child service registries, which likely resolves the issue of unreleased connections. Therefore, the changes indicate a bug fix.

**Buggy**"
hibernate-orm,3407.json,8b9f171a034f7604853d4c4bc1ffa78a8e2991fe,"@@ -1,54 +1,55 @@
 	private void cleanseUniqueKeyMap() {
 		// We need to account for a few conditions here...
 		// 	1) If there are multiple unique keys contained in the uniqueKeys Map, we need to deduplicate
 		// 		any sharing the same columns as other defined unique keys; this is needed for the annotation
 		// 		processor since it creates unique constraints automagically for the user
 		//	2) Remove any unique keys that share the same columns as the primary key; again, this is
 		//		needed for the annotation processor to handle @Id @OneToOne cases.  In such cases the
 		//		unique key is unnecessary because a primary key is already unique by definition.  We handle
 		//		this case specifically because some databases fail if you try to apply a unique key to
 		//		the primary key columns which causes schema export to fail in these cases.
 		if ( uniqueKeys.isEmpty() ) {
 			// nothing to do
 			return;
 		}
 		else if ( uniqueKeys.size() == 1 ) {
 			// we have to worry about condition 2 above, but not condition 1
 			final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeys.entrySet().iterator().next();
 			if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 				uniqueKeys.remove( uniqueKeyEntry.getKey() );
 			}
 		}
 		else {
 			// we have to check both conditions 1 and 2
 			final Iterator<Map.Entry<String,UniqueKey>> uniqueKeyEntries = uniqueKeys.entrySet().iterator();
 			while ( uniqueKeyEntries.hasNext() ) {
 				final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeyEntries.next();
 				final UniqueKey uniqueKey = uniqueKeyEntry.getValue();
 				boolean removeIt = false;
 
 				// condition 1 : check against other unique keys
 				for ( UniqueKey otherUniqueKey : uniqueKeys.values() ) {
 					// make sure its not the same unique key
 					if ( uniqueKeyEntry.getValue() == otherUniqueKey ) {
 						continue;
 					}
 					if ( otherUniqueKey.getColumns().containsAll( uniqueKey.getColumns() )
 							&& uniqueKey.getColumns().containsAll( otherUniqueKey.getColumns() ) ) {
 						removeIt = true;
 						break;
 					}
 				}
 
 				// condition 2 : check against pk
 				if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 					removeIt = true;
 				}
 
 				if ( removeIt ) {
-					uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					//uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					uniqueKeyEntries.remove();
 				}
 			}
 
 		}
 	}",Buggy,"HHH-7446 bug-fix
",Buggy,"The commit message indicates a bug fix with ""HHH-7446 bug-fix"". The code changes involve the `cleanseUniqueKeyMap` method, which processes unique keys associated with database columns. The original code used `uniqueKeys.remove(uniqueKeyEntry.getKey())` inside a loop while iterating over `uniqueKeys.entrySet()`. This is a known anti-pattern in Java, as modifying a collection during iteration using the collection's `remove()` method can lead to `ConcurrentModificationException` or incorrect behavior. The corrected code replaces `uniqueKeys.remove(uniqueKeyEntry.getKey())` with `uniqueKeyEntries.remove()`, which is the correct way to remove an element from a map while iterating over its entry set using an iterator. This change directly addresses a potential bug related to concurrent modification during iteration.

**Buggy**"
hibernate-orm,29945.json,8e6fcce523698018a1e9952a8cf3a78485458ac7,"@@ -1,25 +1,25 @@
 	private void internalSetValue(T value) {
 		if ( procedureParameter.getMode() != ParameterMode.IN && procedureParameter.getMode() != ParameterMode.INOUT ) {
 			throw new IllegalStateException( ""Can only bind values for IN/INOUT parameters : "" + procedureParameter );
 		}
 
 		if ( procedureParameter.getParameterType() != null ) {
 			if ( value == null ) {
 				if ( !procedureParameter.isPassNullsEnabled() ) {
-					throw new IllegalArgumentException( ""The parameter with the ["" +
+					throw new IllegalArgumentException( ""The parameter "" +
 							( procedureParameter.getName() != null
-									? procedureParameter.getName() + ""] name""
-									: procedureParameter.getPosition() + ""] position"" )
+									? ""named ["" + procedureParameter.getName() + ""]""
+									: ""at position ["" + procedureParameter.getPosition() + ""]"" )
 							+ "" was null. You need to call ParameterRegistration#enablePassingNulls(true) in order to pass null parameters."" );
 				}
 			}
 			else if ( !procedureParameter.getParameterType().isInstance( value ) &&
 					!procedureParameter.getHibernateType().getReturnedClass().isInstance( value ) ) {
 				throw new IllegalArgumentException( ""Bind value ["" + value + ""] was not of specified type ["" + procedureParameter
 						.getParameterType() );
 			}
 		}
 
 		this.value = value;
 		this.isBound = true;
 	}",Buggy,"HHH-12905 Improve the error message and update the tests accordingly

Also fix a loose end in the MySQL test: at least with MariaDB, using
a bit(1) as datatype for boolean does not work: it always return true
even if you set it to 0. Using either boolean or tinyint(1) solves
the issue.

As I'm not sure older versions of MySQL supports a real boolean type I
used a tinyint(1).
",Buggy,"The commit message indicates two main changes: improving the error message and fixing a MySQL-related issue.

The code diff modifies the error message thrown when a null value is passed to a parameter that doesn't allow nulls. The original message included ""with the"" and used different phrasing for named and positional parameters. The updated message is more concise and consistent, using ""named"" or ""at position"" to identify the parameter. This change directly addresses the ""improve the error message"" part of the commit message.

The commit message also mentions a fix for a MySQL issue where `bit(1)` doesn't reliably represent boolean values. While this part isn't reflected in the provided code diff, the message clearly states that a fix was implemented, likely in the test suite as indicated by ""update the tests accordingly"". The change involves using `tinyint(1)` instead of `bit(1)` to represent boolean values in MySQL tests, suggesting a workaround for a database-specific behavior that could lead to incorrect test results. This indicates a bug fix related to database compatibility.

Given the error message improvement and the MySQL-specific fix described in the commit message, the changes indicate a bug fix.
**Buggy**
"
hibernate-orm,9074.json,50b788266339ac7f22f2f43dc011378e7b0b0aad,"@@ -1,33 +1,33 @@
 	private EnumValueMapper interpretParameters(Properties parameters) {
 		if ( parameters.containsKey( NAMED ) ) {
 			final boolean useNamed = ConfigurationHelper.getBoolean( NAMED, parameters );
 			if ( useNamed ) {
 				return new NamedEnumValueMapper();
 			}
 			else {
 				return new OrdinalEnumValueMapper();
 			}
 		}
 
 		if ( parameters.containsKey( TYPE ) ) {
 			final int type = Integer.decode( (String) parameters.get( TYPE ) );
 			if ( isNumericType( type ) ) {
 				return new OrdinalEnumValueMapper();
 			}
 			else if ( isCharacterType( type ) ) {
-				return new OrdinalEnumValueMapper();
+				return new NamedEnumValueMapper();
 			}
 			else {
 				throw new HibernateException(
 						String.format(
 								Locale.ENGLISH,
 								""Passed JDBC type code [%s] not recognized as numeric nor character"",
 								type
 						)
 				);
 			}
 		}
 
 		// the fallback
 		return new OrdinalEnumValueMapper();
 	}",Buggy,"HHH-10766 Resolve mapping 'type' parameter error
",Buggy,"The commit message ""HHH-10766 Resolve mapping 'type' parameter error"" suggests that the commit fixes an error related to the 'type' parameter mapping.

The code diff focuses on the `interpretParameters` method, which seems to handle the interpretation of parameters related to enum value mapping. Specifically, it checks for the presence of `NAMED` and `TYPE` parameters.

The key change is within the `if ( isCharacterType( type ) )` block. Originally, it returned `new OrdinalEnumValueMapper()`, but it has been changed to return `new NamedEnumValueMapper()`. This indicates a correction in the logic: when the type is a character type, it should use the `NamedEnumValueMapper` instead of the `OrdinalEnumValueMapper`. The original code likely had an incorrect mapping for character types, which is now resolved. The exception handling remains unchanged.

Based on the analysis, the change corrects a logical error in the mapping of character types, which aligns with the commit message indicating a resolution of a 'type' parameter error.

**Buggy**"
presto,26733.json,d9c1e2084c70835d5feecf645ed8836064809fc2,"@@ -1,18 +1,18 @@
     private void startDiscovery()
     {
         discoveryService.scheduleWithFixedDelay(() -> {
             try {
                 // jitter to avoid overloading database and overloading the backup store
-                SECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
+                MILLISECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
 
                 log.info(""Running shard organizer..."");
                 submitJobs(discoverAndInitializeTablesToOrganize());
             }
             catch (InterruptedException e) {
                 Thread.currentThread().interrupt();
             }
             catch (Throwable t) {
                 log.error(t, ""Error running shard organizer"");
             }
         }, 0, organizationDiscoveryIntervalMillis, TimeUnit.MILLISECONDS);
     }",Buggy,"fix organizationDiscoveryIntervalMillis bug
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix organizationDiscoveryIntervalMillis bug"" clearly states that the commit intends to fix a bug related to `organizationDiscoveryIntervalMillis`. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change within the `startDiscovery` method. Specifically, the line:

`-               SECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));`
`+               MILLISECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));`

has been modified. The change replaces `SECONDS.sleep` with `MILLISECONDS.sleep`.

**Reasoning:**

The original code was likely intended to sleep for a random duration between 1 second and `organizationDiscoveryIntervalMillis` seconds. However, the `discoveryService.scheduleWithFixedDelay` method is configured to run with a `TimeUnit.MILLISECONDS`. This means `organizationDiscoveryIntervalMillis` is interpreted as milliseconds.

If `organizationDiscoveryIntervalMillis` is a relatively small number (e.g., less than 1000), the original code would sleep for a very short duration (potentially less than 1 second), which might not be the intended behavior.  If `organizationDiscoveryIntervalMillis` was intended to represent milliseconds, the original code was incorrectly scaling it by using `SECONDS.sleep`.

The corrected code uses `MILLISECONDS.sleep`, ensuring that the random sleep duration is in milliseconds, consistent with the `TimeUnit.MILLISECONDS` used in the `scheduleWithFixedDelay` method. This correction aligns with the commit message indicating a fix for a bug related to `organizationDiscoveryIntervalMillis`.

**Conclusion:**

The code change corrects a likely unintended scaling issue with the sleep duration, aligning with the commit message indicating a bug fix.

**Buggy**
"
presto,16973.json,83ee1b402bab99302d7c271fcd5dc8a560c027dc,"@@ -1,14 +1,14 @@
     private void loadCatalog(File file)
             throws Exception
     {
         log.info(""-- Loading catalog %s --"", file);
         Map<String, String> properties = new HashMap<>(loadProperties(file));
 
         String connectorName = properties.remove(""connector.name"");
-        checkState(connectorName != null, ""Catalog configuration %s does not contain conector.name"", file.getAbsoluteFile());
+        checkState(connectorName != null, ""Catalog configuration %s does not contain connector.name"", file.getAbsoluteFile());
 
         String catalogName = Files.getNameWithoutExtension(file.getName());
 
         connectorManager.createConnection(catalogName, connectorName, ImmutableMap.copyOf(properties));
         log.info(""-- Added catalog %s using connector %s --"", catalogName, connectorName);
     }",Buggy,"Fix typo in error message about connector.name
",Buggy,"The commit message indicates a fix for a typo in an error message. The diff shows a change from ""conector.name"" to ""connector.name"" within the `checkState` method call. This aligns perfectly with the commit message, as it corrects a spelling mistake in the error message displayed when the ""connector.name"" property is missing in the catalog configuration file. This is a bug fix because the original typo could lead to confusion or difficulty in debugging configuration issues.

**Buggy**
"
presto,14423.json,8c469bda0657f41c146796ea2ccaaea6dfa552e6,"@@ -1,4 +1,4 @@
     public static boolean isPrefixSubnetOf(@SqlType(StandardTypes.IPPREFIX) Slice first, @SqlType(StandardTypes.IPPREFIX) Slice second)
     {
-        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first));
+        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first)) && between(ipSubnetMax(second), ipSubnetMin(first), ipSubnetMax(first));
     }",Buggy,"Fix bug in IS_SUBNET_OF function
",Buggy,"The commit message states ""Fix bug in IS_SUBNET_OF function"". The code modifies the `isPrefixSubnetOf` function by adding a condition to the return statement. The original code only checked if the minimum IP address of the first subnet falls within the range of the second subnet. The corrected code now also checks if the maximum IP address of the first subnet falls within the range of the second subnet. This suggests a bug where the original implementation incorrectly determined subnet relationships. The addition of the `&& between(ipSubnetMax(second), ipSubnetMin(first), ipSubnetMax(first))` condition fixes this bug.

**Buggy**
"
presto,8688.json,97566626dd63db55ee08b48b3e366d6d9453badb,"@@ -1,39 +1,39 @@
     private SymbolStatsEstimate normalizeSymbolStats(Symbol symbol, SymbolStatsEstimate symbolStats, PlanNodeStatsEstimate stats, TypeProvider types)
     {
         if (symbolStats.isUnknown()) {
             return SymbolStatsEstimate.unknown();
         }
 
         double outputRowCount = stats.getOutputRowCount();
         double distinctValuesCount = symbolStats.getDistinctValuesCount();
         double nullsFraction = symbolStats.getNullsFraction();
 
         if (!isNaN(distinctValuesCount)) {
-            Type type = requireNonNull(types.get(symbol), () -> ""No stats for symbol "" + symbol);
+            Type type = requireNonNull(types.get(symbol), () -> ""type is missing for symbol "" + symbol);
             double maxDistinctValuesByLowHigh = maxDistinctValuesByLowHigh(symbolStats, type);
             if (distinctValuesCount > maxDistinctValuesByLowHigh) {
                 distinctValuesCount = maxDistinctValuesByLowHigh;
             }
 
             if (distinctValuesCount > outputRowCount) {
                 distinctValuesCount = outputRowCount;
             }
 
             double nonNullValues = outputRowCount * (1 - nullsFraction);
             if (distinctValuesCount > nonNullValues) {
                 double difference = distinctValuesCount - nonNullValues;
                 distinctValuesCount -= difference / 2;
                 nonNullValues += difference / 2;
                 nullsFraction = 1 - nonNullValues / outputRowCount;
             }
         }
 
         if (distinctValuesCount == 0.0) {
             return SymbolStatsEstimate.zero();
         }
 
         return SymbolStatsEstimate.buildFrom(symbolStats)
                 .setDistinctValuesCount(distinctValuesCount)
                 .setNullsFraction(nullsFraction)
                 .build();
     }",Buggy,"Fix error message in StatsNormalizer
",Buggy,"The commit message ""Fix error message in StatsNormalizer"" indicates that the purpose of the commit is to correct an error message within the `StatsNormalizer` class.

The provided code diff shows a change in the `normalizeSymbolStats` method. Specifically, the error message within the `requireNonNull` method's lambda expression has been modified.

Original error message: `""No stats for symbol "" + symbol`
New error message: `""type is missing for symbol "" + symbol`

The change directly addresses the commit message's intention to fix an error message. The original message was misleading as it referred to missing stats, while the actual issue was a missing type. The updated message provides a more accurate description of the problem. This correction is likely a bug fix, as the original message could have led to confusion during debugging.

**Buggy**"
presto,26619.json,3325ab0064b5fd93af18c1d5e87170f5d66bae28,"@@ -1,10 +1,7 @@
     public void deleteRows(Block rowIds)
     {
-        if (rowsToDelete == null) {
-            rowsToDelete = new BitSet(Ints.checkedCast(recordReader.getFileRowCount()));
-        }
         for (int i = 0; i < rowIds.getPositionCount(); i++) {
             long rowId = BIGINT.getLong(rowIds, i);
             rowsToDelete.set(Ints.checkedCast(rowId));
         }
     }",Buggy,"Fix bug in delete for Raptor

When no rows match for delete, rowsToDelete is not set causing a NPE.
Set it in the constructor because we already know the maximum size.
We might over allocate for cases where no rows are selected for
deletion, but this is a short lived bit vector so it should be low
impact.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message clearly states that it's fixing a bug in the `deleteRows` method for Raptor. The bug is described as a `NullPointerException` (NPE) occurring when no rows match the delete criteria. The explanation is that `rowsToDelete` is not initialized in such cases, leading to the NPE when `rowsToDelete.set()` is called. The proposed solution is to initialize `rowsToDelete` in the constructor, ensuring it's always available. The message also acknowledges a potential over-allocation issue but deems it low impact.

**Git Diff Analysis:**

The diff shows that the code block:

```java
if (rowsToDelete == null) {
    rowsToDelete = new BitSet(Ints.checkedCast(recordReader.getFileRowCount()));
}
```

has been removed from the `deleteRows` method. This confirms that the initialization of `rowsToDelete` is no longer happening within the `deleteRows` method itself.

**Reasoning:**

1.  **Bug Identification:** The commit message explicitly mentions a bug (NPE) related to the `rowsToDelete` variable.
2.  **Root Cause:** The message identifies the root cause as the delayed initialization of `rowsToDelete`, which only happens if there are rows to delete.
3.  **Solution:** The proposed solution is to initialize `rowsToDelete` earlier, presumably in the constructor (though the diff doesn't show the constructor change directly, the message implies it).
4.  **Code Change Alignment:** The diff confirms that the conditional initialization within `deleteRows` has been removed, supporting the idea that the initialization is now handled elsewhere.
5.  **Error Handling:** The change addresses a potential runtime error (NPE) by ensuring that `rowsToDelete` is always initialized.

**Conclusion:**

Based on the commit message and the code diff, the changes are clearly aimed at fixing a bug (NPE) by ensuring the `rowsToDelete` variable is properly initialized. Therefore, the answer is **Buggy**."
presto,14862.json,9adc687000043241a0c0ecb1b56ef6b810c2a430,"@@ -1,23 +1,26 @@
     public static Block bigintDistinct(@SqlType(""array(bigint)"") Block array)
     {
         if (array.getPositionCount() == 0) {
             return array;
         }
 
         boolean containsNull = false;
         LongSet set = new LongOpenHashSet(array.getPositionCount());
         BlockBuilder distinctElementBlockBuilder = BIGINT.createBlockBuilder(new BlockBuilderStatus(), array.getPositionCount());
         for (int i = 0; i < array.getPositionCount(); i++) {
-            if (!containsNull && array.isNull(i)) {
-                containsNull = true;
-                distinctElementBlockBuilder.appendNull();
+            if (array.isNull(i)) {
+                if (!containsNull) {
+                    containsNull = true;
+                    distinctElementBlockBuilder.appendNull();
+                }
                 continue;
             }
+
             long value = BIGINT.getLong(array, i);
             if (set.add(value)) {
                 BIGINT.writeLong(distinctElementBlockBuilder, value);
             }
         }
 
         return distinctElementBlockBuilder.build();
     }",Buggy,"Fix null-handling bug in ArrayDistinctFunction
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix null-handling bug in ArrayDistinctFunction"" clearly states that the commit addresses a bug related to how the `ArrayDistinctFunction` handles null values. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff focuses on the `bigintDistinct` function, which operates on an array of big integers. Let's break down the changes:

1.  **Null Handling Logic:**
    *   The original code had a condition `!containsNull && array.isNull(i)` to check if the current element is null and if a null has not been encountered before. If both are true, it sets `containsNull` to true and appends null to the `distinctElementBlockBuilder`.
    *   The modified code changes this to `if (array.isNull(i)) { if (!containsNull) { ... } continue; }`. This means that if the current element is null, it checks if `containsNull` is false. If it is, it sets `containsNull` to true and appends null to the `distinctElementBlockBuilder`. Regardless, it continues to the next element.

2.  **Purpose of the Change:**
    *   The original code would only add *one* null value to the distinct array, even if the input array contained multiple nulls.
    *   The corrected code ensures that only *one* null value is added to the distinct array, even if the input array contains multiple nulls. The `continue` statement ensures that after encountering the first null, subsequent nulls are skipped from being added to the `set` and the `distinctElementBlockBuilder` after the first null is added.

**Reasoning:**

The commit message and the code changes are highly aligned. The original code had a flaw in how it handled multiple null values within the input array. The corrected code ensures that only a single null value is present in the output when the input array contains one or more null values. This is a clear indication of a bug fix related to null handling.

**Conclusion:**

**Buggy**
"
presto,20594.json,2d9e768a03b5c9d804825c9a489383465f373801,"@@ -1,8 +1,10 @@
         public PlanWithProperties visitFilter(FilterNode node, PreferredProperties preferredProperties)
         {
-            if (node.getSource() instanceof TableScanNode) {
+            if (node.getSource() instanceof TableScanNode && metadata.isLegacyGetLayoutSupported(session, ((TableScanNode) node.getSource()).getTable())) {
+                // If isLegacyGetLayoutSupported, then we can continue with legacy predicate pushdown logic.
+                // Otherwise, we leave the filter as is in the plan as it will be pushed into the TableScan by filter pushdown logic in the connector.
                 return planTableScan((TableScanNode) node.getSource(), node.getPredicate());
             }
 
             return rebaseAndDeriveProperties(node, planChild(node, preferredProperties));
         }",Buggy,"Fix bug AddExchanges dropping filter when pushdown is enabled
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix bug AddExchanges dropping filter when pushdown is enabled"" clearly states that the commit addresses a bug. The bug involves the `AddExchanges` component (likely related to query planning and distribution) incorrectly dropping a filter when predicate pushdown is enabled. Predicate pushdown is an optimization technique where filters are applied closer to the data source, reducing the amount of data that needs to be processed.

**Git Diff Analysis:**

The diff modifies the `visitFilter` method, which is part of a visitor pattern implementation for processing filter nodes in a query plan. The key change is the addition of a condition:

```java
metadata.isLegacyGetLayoutSupported(session, ((TableScanNode) node.getSource()).getTable())
```

This condition checks if the underlying table supports a ""legacy"" method for getting the table layout.  The code then proceeds with the original logic (presumably the one causing the bug) only if this legacy method is supported. Otherwise, the filter is left in the plan.

**Reasoning:**

1.  **Bug Indication:** The commit message explicitly mentions a bug fix.
2.  **Filter Dropping:** The original code likely dropped the filter because it assumed a certain layout retrieval method was always available.
3.  **Conditional Logic:** The added condition `metadata.isLegacyGetLayoutSupported` suggests that the original logic was only valid for tables supporting the legacy layout retrieval. When predicate pushdown was enabled for tables *not* supporting the legacy method, the filter was incorrectly dropped.
4.  **Error Handling:** The new code avoids dropping the filter when the legacy method is not supported, effectively preventing the bug. The comment clarifies that the filter will be handled by the connector's filter pushdown logic in such cases.

**Conclusion:**

The commit message and the code changes strongly indicate a bug fix. The code now handles a case where a filter was being incorrectly dropped due to an assumption about table layout retrieval methods.

**Buggy**
"
presto,30019.json,9e755c76e9d8e3a5e0a486002b3898be9d6d5d25,"@@ -1,14 +1,13 @@
     public void close()
     {
         closed = true;
-
         // use try with resources to close everything properly
-        try (ResultSet resultSet = this.resultSet;
+        try (Connection connection = this.connection;
                 Statement statement = this.statement;
-                Connection connection = this.connection) {
+                ResultSet resultSet = this.resultSet) {
             // do nothing
         }
         catch (SQLException e) {
             throw Throwables.propagate(e);
         }
     }",Buggy,"Fix ""Connection already closed"" error in JdbcRecordCursor

The resultset, statement and connection were being closed in
the wrong order.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states that it fixes a ""Connection already closed"" error. This strongly suggests a bug fix related to resource management (specifically, database connections). The message also indicates that the order of closing resources (ResultSet, Statement, and Connection) was incorrect.

2.  **Diff Analysis:** The code diff shows a change in the order of resource declaration within the `try-with-resources` block. The original order was `ResultSet`, `Statement`, `Connection`. The modified order is `Connection`, `Statement`, `ResultSet`.

3.  **Relationship between Commit Message and Diff:** The change in resource closing order directly addresses the problem described in the commit message. The ""Connection already closed"" error likely occurred because the Connection was being closed *before* the ResultSet or Statement, which might still be using the connection. By closing the Connection last, the code ensures that the ResultSet and Statement are closed while the Connection is still open.

4.  **Bug Fix Indicators:** The commit message explicitly mentions fixing an error. The code change directly relates to resource management, which is a common source of bugs, especially in database interactions. The change in closing order is a logical correction to prevent the ""Connection already closed"" error.

**Conclusion:**

The commit message and code diff clearly indicate a bug fix related to incorrect resource closing order, which was causing a ""Connection already closed"" error.

**Buggy**"
presto,32288.json,1eb20ea3faeb66b18c4c9300b4a6c8c3190ca6ce,"@@ -1,8 +1,9 @@
     public List<SchemaTableName> listTables(ConnectorSession session, String schemaNameOrNull)
     {
-        checkArgument(schemaNameOrNull == null || schemaNameOrNull.equals(SCHEMA_NAME),
-                ""Only '%s' schema is supported"", SCHEMA_NAME);
+        if (schemaNameOrNull != null && !schemaNameOrNull.equals(SCHEMA_NAME)) {
+            return ImmutableList.of();
+        }
         return tables.values().stream()
                 .map(BlackHoleTableHandle::toSchemaTableName)
                 .collect(toList());
     }",Buggy,"Fix listing tables in blackhole connector

Listing tables in a non-existent schema should not be an error.
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states that listing tables in a non-existent schema should not result in an error. This implies that previously, attempting to list tables in a non-existent schema *did* cause an error, which is undesirable behavior.

2.  **Code Diff:**
    *   The original code used `checkArgument` to verify if the provided `schemaNameOrNull` was either `null` or equal to `SCHEMA_NAME`. If it wasn't, `checkArgument` would throw an `IllegalArgumentException`, effectively causing an error when trying to list tables in a non-existent schema.
    *   The modified code replaces the `checkArgument` with an `if` condition. If the `schemaNameOrNull` is not `null` and not equal to `SCHEMA_NAME`, the method now returns an empty list (`ImmutableList.of()`). This means that instead of throwing an error, the method gracefully handles the case of a non-existent schema by returning an empty list of tables.

3.  **Alignment:** The code change directly addresses the issue described in the commit message. The previous code threw an error for non-existent schemas, while the new code returns an empty list, thus fixing the erroneous behavior.

**Conclusion:**

The commit message and code diff clearly indicate that a bug (an error being thrown when it shouldn't) was present and has been fixed.

**Buggy**"
presto,32344.json,5bc9087c740ce60796c3b92131fe7c2a09bc04f7,"@@ -1,4 +1,10 @@
     public List<File> getFiles(SchemaTableName table)
     {
-        return cachedFiles.getUnchecked(table);
+        try {
+            return cachedFiles.getUnchecked(table);
+        }
+        catch (UncheckedExecutionException e) {
+            throwIfInstanceOf(e.getCause(), PrestoException.class);
+            throw e;
+        }
     }",Buggy,"Fix error categorization in local file connector

Exceptions are wrapped by LoadingCache and must be unwrapped.
",Buggy,"The commit message indicates a fix related to error categorization within the local file connector. It specifically mentions that exceptions are wrapped by LoadingCache and need to be unwrapped.

The code diff shows the addition of a try-catch block around the `cachedFiles.getUnchecked(table)` call. Inside the catch block, it checks if the cause of the `UncheckedExecutionException` is a `PrestoException`. If it is, it rethrows the `PrestoException`. Otherwise, it rethrows the original `UncheckedExecutionException`.

This change aligns perfectly with the commit message. The `LoadingCache` wraps exceptions, and the code is now unwrapping them to properly categorize errors, specifically looking for and rethrowing `PrestoException`. This indicates a bug fix where exceptions were not being correctly handled or categorized before.

**Buggy**
"
presto,28248.json,dce842fd5d590740f0f7ca26ec5c7eb192e64640,"@@ -1,11 +1,16 @@
         protected Node visitNegativeExpression(NegativeExpression node, Context<C> context)
         {
             if (!context.isDefaultRewrite()) {
                 Node result = nodeRewriter.rewriteNegativeExpression(node, context.get(), TreeRewriter.this);
                 if (result != null) {
                     return result;
                 }
             }
 
+            Expression child = rewrite(node.getValue(), context.get());
+            if (child != node.getValue()) {
+                return new NegativeExpression(child);
+            }
+
             return node;
         }",Buggy,"Fix bug when rewriting NegativeExpression

It was discarding the rewritten subexpression of an arithmetic negation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix bug when rewriting NegativeExpression"" clearly states that the commit addresses a bug. The message further clarifies that the bug was related to discarding the rewritten subexpression of an arithmetic negation. This suggests that the previous implementation was not correctly handling the rewritten subexpression within a `NegativeExpression`.

**Git Diff Analysis:**

The diff shows changes within the `visitNegativeExpression` method. Let's break down the changes:

1.  **Existing Code:** The original code checks if a default rewrite is not requested. If not, it attempts to rewrite the `NegativeExpression` using `nodeRewriter.rewriteNegativeExpression`. If a rewritten result is obtained, it returns the result. Otherwise, it proceeds to return the original node.

2.  **Added Code:** The added code performs the following:
    *   `Expression child = rewrite(node.getValue(), context.get());`: This line rewrites the child expression (the value being negated) of the `NegativeExpression`.
    *   `if (child != node.getValue()) { return new NegativeExpression(child); }`: This condition checks if the child expression was actually rewritten. If it was (i.e., `child` is different from the original `node.getValue()`), it creates a *new* `NegativeExpression` with the rewritten child and returns it.

**Reasoning:**

The commit message and the code changes align perfectly. The original code was missing a crucial step: rewriting the child expression of the `NegativeExpression` and creating a new `NegativeExpression` with the rewritten child. The added code addresses this omission. If the child expression was rewritten, the original code would simply return the original `NegativeExpression`, effectively discarding the rewritten child. The added code ensures that the rewritten child is incorporated into a new `NegativeExpression`. This confirms the bug described in the commit message: ""discarding the rewritten subexpression of an arithmetic negation.""

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes directly address the issue described in the message. The added code corrects the logic to properly handle rewritten subexpressions within `NegativeExpression` nodes.

**Buggy**"
presto,8271.json,d560b6e644e49cab94fa9076ec62406cd62afa68,"@@ -1,12 +1,16 @@
         public InputStream handle(Request request, com.facebook.airlift.http.client.Response response)
         {
             try {
+                if (response.getStatusCode() != HTTP_OK) {
+                    String result = new BufferedReader(new InputStreamReader(response.getInputStream())).lines().collect(Collectors.joining(""\n""));
+                    throw new PrestoException(DRUID_BROKER_RESULT_ERROR, result);
+                }
                 if (APPLICATION_JSON.equals(response.getHeader(CONTENT_TYPE))) {
                     return response.getInputStream();
                 }
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Response received was not of type "" + APPLICATION_JSON);
             }
             catch (IOException e) {
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Unable to read response from worker"", e);
             }
         }",Buggy,"Fix unhandled HTTP response error for druid client
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""Fix unhandled HTTP response error for druid client"" clearly indicates that the commit aims to address an issue where HTTP response errors from a Druid client were not being properly handled. This suggests a bug fix.

**Code Diff Analysis:**

The code diff shows a modification within the `handle` method, which appears to be responsible for processing HTTP responses.

Specifically, the following code block has been added:

```java
if (response.getStatusCode() != HTTP_OK) {
    String result = new BufferedReader(new InputStreamReader(response.getInputStream())).lines().collect(Collectors.joining(""\n""));
    throw new PrestoException(DRUID_BROKER_RESULT_ERROR, result);
}
```

This code checks if the HTTP response status code is not equal to `HTTP_OK` (presumably 200). If it's not, it reads the response body, converts it to a string, and throws a `PrestoException` with the error message from the response.

**Reasoning:**

1.  **Alignment:** The code change directly aligns with the commit message. The added code explicitly handles the case where the HTTP response status code indicates an error.
2.  **Error Handling:** The added code introduces error handling for non-OK HTTP responses. Previously, the code only checked the `Content-Type` header. Now, it also checks the status code and throws an exception if it's not successful.
3.  **Bug Fix Indication:** The original code likely failed to handle cases where the Druid broker returned an error status code (e.g., 500, 404). This could have led to unexpected behavior or incorrect results. The added code fixes this by explicitly checking for error status codes and throwing an exception, providing more informative error messages.

**Conclusion:**

The commit message and code diff strongly suggest that this commit is a bug fix. The code adds error handling for previously unhandled HTTP response errors, making the system more robust.

**Buggy**"
presto,22102.json,37377fdfa6f208809b77185abca3b1d0bdcb2f92,"@@ -1,72 +1,73 @@
     public static Map<List<RowExpression>, Boolean> getExpressionsPartitionedByCSE(Collection<? extends RowExpression> expressions)
     {
         if (expressions.isEmpty()) {
             return ImmutableMap.of();
         }
 
         CommonSubExpressionCollector expressionCollector = new CommonSubExpressionCollector();
         expressions.forEach(expression -> expression.accept(expressionCollector, null));
         Set<RowExpression> cse = expressionCollector.cseByLevel.values().stream().flatMap(Set::stream).collect(toImmutableSet());
 
         if (cse.isEmpty()) {
             return expressions.stream().collect(toImmutableMap(ImmutableList::of, m -> false));
         }
 
         ImmutableMap.Builder<List<RowExpression>, Boolean> expressionsPartitionedByCse = ImmutableMap.builder();
         SubExpressionChecker subExpressionChecker = new SubExpressionChecker(cse);
         Map<Boolean, List<RowExpression>> expressionsWithCseFlag = expressions.stream().collect(Collectors.partitioningBy(expression -> expression.accept(subExpressionChecker, null)));
         expressionsWithCseFlag.get(false).forEach(expression -> expressionsPartitionedByCse.put(ImmutableList.of(expression), false));
 
         List<RowExpression> expressionsWithCse = expressionsWithCseFlag.get(true);
         if (expressionsWithCse.size() == 1) {
             RowExpression expression = expressionsWithCse.get(0);
             expressionsPartitionedByCse.put(ImmutableList.of(expression), true);
             return expressionsPartitionedByCse.build();
         }
 
         List<Set<RowExpression>> cseDependency = expressionsWithCse.stream()
                 .map(expression -> subExpressions(expression).stream()
                         .filter(cse::contains)
                         .collect(toImmutableSet()))
                 .collect(toImmutableList());
 
         boolean[] merged = new boolean[expressionsWithCse.size()];
 
         int i = 0;
         while (i < merged.length) {
             while (i < merged.length && merged[i]) {
                 i++;
             }
             if (i >= merged.length) {
                 break;
             }
             merged[i] = true;
             ImmutableList.Builder<RowExpression> newList = ImmutableList.builder();
             newList.add(expressionsWithCse.get(i));
             Set<RowExpression> dependencies = new HashSet<>();
             Set<RowExpression> first = cseDependency.get(i);
             dependencies.addAll(first);
             int j = i + 1;
             while (j < merged.length) {
                 while (j < merged.length && merged[j]) {
                     j++;
                 }
                 if (j >= merged.length) {
                     break;
                 }
                 Set<RowExpression> second = cseDependency.get(j);
                 if (!Sets.intersection(dependencies, second).isEmpty()) {
                     RowExpression expression = expressionsWithCse.get(j);
                     newList.add(expression);
                     dependencies.addAll(second);
                     merged[j] = true;
+                    j = i + 1;
                 }
                 else {
                     j++;
                 }
             }
             expressionsPartitionedByCse.put(newList.build(), true);
         }
 
         return expressionsPartitionedByCse.build();
     }",Buggy,"Fixing a bug in common sub expression partitioning

The previous implementation could group expressions differently with different order or exprsesions.
",Buggy,"The commit message indicates a fix for a bug related to common subexpression partitioning, where the grouping of expressions could vary based on their order.

The code diff focuses on the `getExpressionsPartitionedByCSE` method. The core logic involves identifying common subexpressions (CSEs) within a collection of `RowExpression` objects and then partitioning these expressions based on their CSE dependencies.

The key change appears to be the addition of `j = i + 1;` inside the `if (!Sets.intersection(dependencies, second).isEmpty())` block. This line resets the inner loop's index `j` to `i + 1` whenever a merge occurs. Without this reset, the algorithm might miss potential merges later in the `expressionsWithCse` list because `j` continues to increment from its previous position, potentially skipping over expressions that could now be merged due to the updated `dependencies`.

The original logic, without the reset, would only compare each expression once after a merge, potentially leading to different groupings depending on the initial order of expressions. The added line ensures that after a merge, the algorithm reconsiders all remaining expressions for potential inclusion in the current group, addressing the bug described in the commit message.

Therefore, the code change directly addresses the bug described in the commit message by ensuring consistent grouping of expressions regardless of their order.

**Buggy**"
presto,10913.json,e6f650722f4041eade54638d355de2b9f7a9e280,"@@ -1,4 +1,4 @@
     public boolean isFinalQueryInfo()
     {
-        return state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
+        return state.equals(QueryState.FAILED) || state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
     }",Buggy,"fix the client hang problem after query failed
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix the client hang problem after query failed"" clearly states that the change addresses a bug where the client hangs after a query fails. This suggests that the previous behavior was incorrect and led to a client-side issue.

**Code Diff Analysis:**

The code diff modifies the `isFinalQueryInfo()` method.  The original code only returned `true` if the query `state` was `done` *and* all stages were in a final state. The modified code adds a condition: `state.equals(QueryState.FAILED)`.  Now, the method returns `true` if the query `state` is `FAILED` *or* if the query is `done` and all stages are final.

**Reasoning:**

The original code likely had a bug where, if a query failed, the `isFinalQueryInfo()` method would not return `true`. This could cause a client to hang, waiting for a final query state that would never be reached under the failure condition. The added condition `state.equals(QueryState.FAILED)` directly addresses this issue by ensuring that `isFinalQueryInfo()` returns `true` when the query has failed, allowing the client to proceed and avoid the hang. This aligns perfectly with the commit message.

**Conclusion:**

The code change directly addresses the problem described in the commit message, which is a client hang after a query failure. The modification ensures that the `isFinalQueryInfo()` method returns `true` when the query has failed, preventing the client from indefinitely waiting for a final state. This is a clear indication of a bug fix.

**Buggy**
"
presto,7159.json,130527265fd8aebee395ed2412f4058d0e269f51,"@@ -1,84 +1,83 @@
         public PlanNode visitProject(ProjectNode project, Void context)
         {
             if (!(project.getSource() instanceof TableScanNode)) {
                 return visitPlan(project, context);
             }
 
             TableScanNode tableScan = (TableScanNode) project.getSource();
             if (!isParquetDereferenceEnabled(session, tableScan.getTable())) {
                 return visitPlan(project, context);
             }
 
             Map<RowExpression, Subfield> dereferenceToNestedColumnMap = extractDereferences(
                     session,
                     rowExpressionService.getExpressionOptimizer(),
                     new HashSet<>(project.getAssignments().getExpressions()));
             if (dereferenceToNestedColumnMap.isEmpty()) {
                 return visitPlan(project, context);
             }
 
-            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().values().stream()
-                    .map(columnHandle -> (HiveColumnHandle) columnHandle)
-                    .collect(toMap(HiveColumnHandle::getName, identity()));
+            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().entrySet().stream()
+                    .collect(toMap(e -> e.getKey().getName(), e -> (HiveColumnHandle) e.getValue()));
 
             List<VariableReferenceExpression> newOutputVariables = new ArrayList<>(tableScan.getOutputVariables());
             Map<VariableReferenceExpression, ColumnHandle> newAssignments = new HashMap<>(tableScan.getAssignments());
 
             Map<RowExpression, VariableReferenceExpression> dereferenceToVariableMap = new HashMap<>();
 
             for (Map.Entry<RowExpression, Subfield> dereference : dereferenceToNestedColumnMap.entrySet()) {
                 Subfield nestedColumn = dereference.getValue();
                 RowExpression dereferenceExpression = dereference.getKey();
 
                 // Find the nested column Hive Type
                 HiveColumnHandle regularColumnHandle = regularHiveColumnHandles.get(nestedColumn.getRootName());
                 if (regularColumnHandle == null) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""]'s base column "" + nestedColumn.getRootName() + "" is not present in table scan output"");
                 }
 
                 Optional<HiveType> nestedColumnHiveType = regularHiveColumnHandles.get(nestedColumn.getRootName())
                         .getHiveType()
                         .findChildType(
                                 nestedColumn.getPath().stream()
                                         .map(p -> ((Subfield.NestedField) p).getName())
                                         .collect(Collectors.toList()));
 
                 if (!nestedColumnHiveType.isPresent()) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""] type is not present in Hive column type"");
                 }
 
                 String pushdownColumnName = pushdownColumnNameForSubfield(nestedColumn);
                 // Create column handle for nested column
                 HiveColumnHandle nestedColumnHandle = new HiveColumnHandle(
                         pushdownColumnName,
                         nestedColumnHiveType.get(),
                         dereferenceExpression.getType().getTypeSignature(),
                         -1,
                         SYNTHESIZED,
                         Optional.of(""nested column pushdown""),
                         ImmutableList.of(nestedColumn),
                         Optional.empty());
 
                 VariableReferenceExpression newOutputVariable = variableAllocator.newVariable(pushdownColumnName, dereferenceExpression.getType());
                 newOutputVariables.add(newOutputVariable);
                 newAssignments.put(newOutputVariable, nestedColumnHandle);
 
                 dereferenceToVariableMap.put(dereferenceExpression, newOutputVariable);
             }
 
             TableScanNode newTableScan = new TableScanNode(
                     idAllocator.getNextId(),
                     tableScan.getTable(),
                     newOutputVariables,
                     newAssignments,
                     tableScan.getCurrentConstraint(),
                     tableScan.getEnforcedConstraint());
 
             Assignments.Builder newProjectAssignmentBuilder = Assignments.builder();
             for (Map.Entry<VariableReferenceExpression, RowExpression> entry : project.getAssignments().entrySet()) {
                 RowExpression newExpression = rewriteWith(new DereferenceExpressionRewriter(dereferenceToVariableMap), entry.getValue());
                 newProjectAssignmentBuilder.put(entry.getKey(), newExpression);
             }
 
             return new ProjectNode(idAllocator.getNextId(), newTableScan, newProjectAssignmentBuilder.build(), project.getLocality());
         }",Buggy,"Fix base column name not present error when dereference pushdown enabled
",Buggy,"The commit message indicates a fix for an error that occurs when the base column name is not present, specifically when dereference pushdown is enabled.

The code diff modifies how `regularHiveColumnHandles` are populated. The original code extracts the column name directly from the `HiveColumnHandle` but doesn't account for the variable associated with the column handle. The corrected code uses `tableScan.getAssignments().entrySet()` to correctly map the variable name (which represents the base column name) to the corresponding `HiveColumnHandle`.

The change directly addresses the issue described in the commit message. The original code would fail if the base column name, as represented by the variable name, was not present in the stream of `HiveColumnHandle` names. The corrected code ensures that the base column name is correctly retrieved from the `tableScan.getAssignments()` map. This resolves the error and enables dereference pushdown to function correctly when the base column name is not directly available from the column handle.

Therefore, the changes indicate a bug fix.

**Buggy**"
presto,22204.json,b88a9a91b24500d3c46df8cd613783effdc9d70c,"@@ -1,34 +1,34 @@
     private static Map<LambdaDefinitionExpression, CompiledLambda> generateMethodsForLambda(
             ClassDefinition containerClassDefinition,
             CallSiteBinder callSiteBinder,
             CachedInstanceBinder cachedInstanceBinder,
             List<RowExpression> expressions,
             Metadata metadata,
             SqlFunctionProperties sqlFunctionProperties,
             String methodNamePrefix,
             Set<LambdaDefinitionExpression> existingCompiledLambdas)
     {
         Set<LambdaDefinitionExpression> lambdaExpressions = expressions.stream()
                 .map(LambdaExpressionExtractor::extractLambdaExpressions)
                 .flatMap(List::stream)
                 .filter(lambda -> !existingCompiledLambdas.contains(lambda))
                 .collect(toImmutableSet());
         ImmutableMap.Builder<LambdaDefinitionExpression, CompiledLambda> compiledLambdaMap = ImmutableMap.builder();
 
-        int counter = 0;
+        int counter = existingCompiledLambdas.size();
         for (LambdaDefinitionExpression lambdaExpression : lambdaExpressions) {
             CompiledLambda compiledLambda = LambdaBytecodeGenerator.preGenerateLambdaExpression(
                     lambdaExpression,
                     methodNamePrefix + ""lambda_"" + counter,
                     containerClassDefinition,
                     compiledLambdaMap.build(),
                     callSiteBinder,
                     cachedInstanceBinder,
                     metadata,
                     sqlFunctionProperties);
             compiledLambdaMap.put(lambdaExpression, compiledLambda);
             counter++;
         }
 
         return compiledLambdaMap.build();
     }",Buggy,"Fix compiler error in LambdaBytecodeGenerator

When there are lambda expressions from different SQL functions, and they are
compiled into the same class due to CSE, we need to make sure the generated
function names are always unique.
",Buggy,"The commit message indicates a fix for a compiler error that occurs when lambda expressions from different SQL functions are compiled into the same class due to common subexpression elimination (CSE). The issue arises because the generated function names for these lambdas need to be unique.

The code diff modifies the `generateMethodsForLambda` method. Specifically, the initialization of the `counter` variable is changed from `0` to `existingCompiledLambdas.size()`. This change ensures that the generated lambda function names are unique across different SQL functions, even when CSE causes them to be compiled into the same class. The original implementation would have started the counter at 0 for each set of lambda expressions, potentially leading to name collisions if lambdas from different SQL functions were processed together.

The change directly addresses the problem described in the commit message by ensuring unique function names, thus resolving the compiler error.

**Buggy**"
presto,1269.json,4cb4d192f1cc8e0da404521946d7566738d45c00,"@@ -1,6 +1,6 @@
         private void prepareDropPartition(SchemaTableName schemaTableName, List<String> partitionValues)
         {
             metastoreDeleteOperations.add(new IrreversibleMetastoreOperation(
-                    format(""drop partition %s.%s %s"", schemaTableName, schemaTableName.getTableName(), partitionValues),
+                    format(""drop partition %s.%s %s"", schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues),
                     () -> delegate.dropPartition(schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues, true)));
         }",Buggy,"Fix SemiTransactionalHiveMetastore error message
",Buggy,"The commit message ""Fix SemiTransactionalHiveMetastore error message"" suggests that the commit aims to correct an error in an error message within the `SemiTransactionalHiveMetastore`.

The provided diff modifies the `prepareDropPartition` method. Specifically, it changes the format string used to construct the error message. The original format string used `schemaTableName` twice: `format(""drop partition %s.%s %s"", schemaTableName, schemaTableName.getTableName(), partitionValues)`. This would likely result in an incorrect or confusing error message because the `schemaTableName` object itself (toString representation) would be printed instead of the schema name. The corrected format string `format(""drop partition %s.%s %s"", schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues)` now correctly uses `schemaTableName.getSchemaName()` to include the schema name in the error message.

This change directly addresses the issue described in the commit message by fixing the error message generation. The original code had a bug where the schema name was not correctly included in the error message.

**Buggy**"
presto,17974.json,0498c0df5aff14c9f7f88db1fe2e43ed3470aa5b,"@@ -1,12 +1,16 @@
     private static Optional<TaskInfo> findFailedTask(StageInfo stageInfo)
     {
+        if (stageInfo == null) {
+            return Optional.empty();
+        }
+
         for (StageInfo subStage : stageInfo.getSubStages()) {
             Optional<TaskInfo> task = findFailedTask(subStage);
             if (task.isPresent()) {
                 return task;
             }
         }
         return stageInfo.getTasks().stream()
                 .filter(taskInfo -> taskInfo.getState() == TaskState.FAILED)
                 .findFirst();
     }",Buggy,"Fix NPE when query fails during parsing/planning

Commit 135626b793b42841c77f6b8c8b7fe80fac8ffc46 introduced a bug
where queries that fail during parsing/analysis/planning
throw an NPE when the completion event is processed.

This is due to an incorrect assumption that QueryInfos always
have an output stage, which is not true for queries that fail
before execution starts.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when queries fail during parsing, analysis, or planning. The NPE arises because the code incorrectly assumes that `QueryInfos` always have an output stage, which is not the case for queries that fail early in the process.

The provided code diff introduces a null check for `stageInfo` within the `findFailedTask` method. This check prevents the code from attempting to dereference a null `stageInfo` object, which would indeed cause an NPE. The added null check aligns perfectly with the commit message's description of the bug and its cause. The fix addresses the scenario where a query fails before execution starts, resulting in a missing output stage and a potential NPE when processing the completion event.

Therefore, the changes clearly indicate a bug fix.

**Buggy**
"
presto,22343.json,913c3f81cdc1a8dbc3201382b38d5e429f43fa7f,"@@ -1,36 +1,37 @@
     public BytecodeNode generateExpression(Signature signature, BytecodeGeneratorContext context, Type rowType, List<RowExpression> arguments)
     {
         BytecodeBlock block = new BytecodeBlock().setDescription(""Constructor for "" + rowType.toString());
         CallSiteBinder binder = context.getCallSiteBinder();
         Scope scope = context.getScope();
         List<Type> types = rowType.getTypeParameters();
 
         block.comment(""BlockBuilder blockBuilder = new InterleavedBlockBuilder(types, new BlockBuilderStatus(), 1);"");
         Variable blockBuilder = scope.createTempVariable(BlockBuilder.class);
         Binding typesBinding = binder.bind(types, List.class);
         block.append(blockBuilder.set(
                 newInstance(InterleavedBlockBuilder.class, loadConstant(typesBinding), newInstance(BlockBuilderStatus.class), constantInt(1))));
 
         for (int i = 0; i < arguments.size(); ++i) {
             Type fieldType = types.get(i);
             Class<?> javaType = fieldType.getJavaType();
             if (javaType == void.class) {
                 block.comment(i + ""-th field type of row is undefined"");
                 block.append(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop());
             }
             else {
                 Variable field = scope.createTempVariable(javaType);
-                block.comment(""Generate + "" + i + ""-th field of row"");
+                block.comment(""Clean wasNull and Generate + "" + i + ""-th field of row"");
+                block.append(context.wasNull().set(constantFalse()));
                 block.append(context.generate(arguments.get(i)));
                 block.putVariable(field);
                 block.append(new IfStatement()
                         .condition(context.wasNull())
                         .ifTrue(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop())
                         .ifFalse(constantType(binder, fieldType).writeValue(blockBuilder, field).pop()));
             }
         }
         block.comment(""put (Block) blockBuilder.build(); wasNull = false;"");
         block.append(blockBuilder.invoke(""build"", Block.class));
         block.append(context.wasNull().set(constantFalse()));
         return block;
     }",Buggy,"Fix bug when constructing ROW with null values

Not all generators set wasNull, causing following values
(generated from one generator that also doesn't set wasNull) to be null.
",Buggy,"The commit message indicates a bug fix related to the construction of rows with null values. Specifically, it mentions that not all generators were setting the `wasNull` flag, leading to subsequent values being incorrectly set to null.

The code diff introduces a change within the loop that iterates through the arguments used to construct the row. Before generating each field, the code now explicitly sets `context.wasNull()` to `constantFalse()`. This ensures that the `wasNull` flag is properly reset before each field is processed, addressing the issue described in the commit message.

The `IfStatement` checks `context.wasNull()` and appends null if it's true, otherwise writes the value. The explicit setting of `context.wasNull()` to `constantFalse()` before generating each field ensures the correct behavior of this conditional logic.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
presto,1944.json,d439d475c1eb6674ff8a9fb75ee08565217bc39c,"@@ -1,8 +1,8 @@
     public boolean isSingleValue()
     {
         return !low.isLowerUnbounded() &&
                 !high.isUpperUnbounded() &&
                 low.getBound() == Marker.Bound.EXACTLY &&
                 high.getBound() == Marker.Bound.EXACTLY &&
-                low.getValue() == high.getValue();
+                low.getValue().equals(high.getValue());
     }",Buggy,"Fix Range bug in SPI
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix Range bug in SPI"" explicitly states that the commit addresses a bug related to the ""Range"" functionality within the ""SPI"" (presumably, Service Provider Interface) context.

2.  **Code Diff:** The code diff modifies the `isSingleValue()` method. The change replaces `low.getValue() == high.getValue()` with `low.getValue().equals(high.getValue())`.

3.  **Analysis:**
    *   The original code `low.getValue() == high.getValue()` uses the `==` operator for comparison. In Java, `==` compares object references, meaning it checks if `low.getValue()` and `high.getValue()` point to the same object in memory. This can lead to incorrect results if two distinct objects have the same value but are not the same object.
    *   The corrected code `low.getValue().equals(high.getValue())` uses the `equals()` method. The `equals()` method, when properly implemented, compares the *values* of the objects, which is the correct way to determine if two objects represent the same value.
    *   The change from `==` to `.equals()` directly addresses a potential bug where the `isSingleValue()` method would incorrectly return `false` even if the low and high values of the range were logically the same. This is because the `==` operator would return `false` if the two values were different objects, even if they had the same value.

**Conclusion:**

The commit message indicates a bug fix, and the code change corrects a logical error in the `isSingleValue()` method by using `.equals()` for value comparison instead of `==` for reference comparison. This resolves a potential issue where the method would incorrectly identify ranges as not single-valued.

**Buggy**"
presto,17855.json,18088bed1462b2cf8f46ab44293b3791136588d3,"@@ -1,8 +1,7 @@
     public String toString()
     {
         return Objects.toStringHelper(this)
                 .add(""tableHandle"", tableHandle)
-                .add(""filters"", filters)
                 .add(""addresses"", addresses)
                 .toString();
     }",Buggy,"System connector predicate pushdown bug fix
",NotBuggy,"The commit message ""System connector predicate pushdown bug fix"" suggests that the commit aims to correct an issue related to predicate pushdown within a system connector. Predicate pushdown is an optimization technique where filtering conditions (predicates) are applied as early as possible in the data processing pipeline, ideally at the data source level.

The provided diff removes `filters` from the `toString()` method. This change, in isolation, doesn't directly indicate a bug fix related to predicate pushdown. It's more likely a change to the logging or debugging information.  The removal of `filters` from the `toString()` method doesn't inherently fix any logical errors, exception handling, or data inconsistencies related to predicate pushdown. It's possible that the actual bug fix was done elsewhere, and this change is just a side effect or a cleanup.

Therefore, based solely on this diff, it's difficult to confirm that this commit addresses a bug.

**NotBuggy**
"
presto,3309.json,c798add760c213bafe85a1f7934e54b5fe0b8728,"@@ -1,4 +1,4 @@
     public int getPositionCount()
     {
-        return blocks[0].getPositionCount();
+        return positionCount;
     }",Buggy,"Fix bug in count aggregation from inline view

This query now works:

SELECT COUNT(*) FROM (SELECT ... ) x
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Fix bug in count aggregation from inline view"" clearly states that the commit aims to resolve a bug related to `COUNT(*)` queries performed on inline views (subqueries in the `FROM` clause).
    *   The example `SELECT COUNT(*) FROM (SELECT ... ) x` illustrates the specific query type that was failing.

2.  **Code Diff Analysis:**
    *   The code diff shows a change within the `getPositionCount()` method.
    *   The original implementation `return blocks[0].getPositionCount();` retrieves the position count from the first element of the `blocks` array.
    *   The modified implementation `return positionCount;` directly returns the value of the `positionCount` field.

3.  **Relevance Assessment:**
    *   The change in `getPositionCount()` suggests that the original implementation might have been incorrect or unreliable in certain scenarios, potentially leading to an inaccurate count when used in the context of the inline view aggregation.
    *   The original code assumes that the position count is always available from the first block. This assumption could be flawed if the data is distributed across multiple blocks or if the first block is empty or incomplete.
    *   The modified code directly returns the `positionCount` field, which is likely a more reliable and accurate representation of the total number of positions.
    *   The commit message indicates that the bug was related to `COUNT(*)` aggregation, and the code change directly impacts the `getPositionCount()` method, which is likely used to determine the size of the data being aggregated. Therefore, the code change is directly relevant to the bug described in the commit message.

**Conclusion:**

The commit message explicitly states a bug fix related to count aggregation, and the code change modifies a method that is likely used to determine the size of the data being aggregated. The original code had a potential flaw in how it retrieved the position count, which could have led to incorrect aggregation results. The modified code provides a more reliable way to obtain the position count. Therefore, the changes indicate a bug fix.

**Buggy**"
presto,12870.json,f20d677b8203f10728649c8f4b038f68a08dd909,"@@ -1,4 +1,4 @@
     private static List<ParameterMetadata> createInputParameterMetadata(Type value, Type key)
     {
-        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
+        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
     }",Buggy,"Fix null-handling bug in min_by/max_by

Null keys were previously incorrectly handled. They were effectively treated
as default value of the type for comparison in the input function.
",Buggy,"The commit message indicates a fix for a null-handling bug in `min_by` and `max_by` functions, specifically related to how null keys are treated during comparison. The diff shows a change in the `createInputParameterMetadata` method, where `NULLABLE_BLOCK_INPUT_CHANNEL` for the `key` parameter is replaced with `BLOCK_INPUT_CHANNEL`.

Here's the reasoning:

1.  **Commit Message Analysis:** The commit message explicitly mentions a ""null-handling bug"" and states that null keys were incorrectly handled. This strongly suggests a bug fix.
2.  **Diff Analysis:** The change from `NULLABLE_BLOCK_INPUT_CHANNEL` to `BLOCK_INPUT_CHANNEL` for the `key` parameter implies that the code was previously treating the key as nullable when it should not have been. By changing it to `BLOCK_INPUT_CHANNEL`, the code now expects a non-nullable block input channel for the key. This aligns with the commit message's description of incorrect null handling.
3.  **Relevance:** The code change directly addresses the issue described in the commit message. The change ensures that the key is treated as non-nullable, which likely resolves the bug where null keys were being mishandled during comparison.

Based on the commit message and the code diff, the change indicates a bug fix related to null handling.

**Buggy**"
presto,16287.json,c5b89795521472323d392daa860240d64d3876f0,"@@ -1,35 +1,59 @@
     static DecodedBlockNode decodeBlock(BlockFlattener flattener, Closer blockLeaseCloser, Block block)
     {
         BlockLease lease = flattener.flatten(block);
         blockLeaseCloser.register(lease::close);
         Block decodedBlock = lease.get();
 
+        long estimatedSizeInBytes = decodedBlock.getLogicalSizeInBytes();
+
         if (decodedBlock instanceof ArrayBlock) {
             ColumnarArray columnarArray = ColumnarArray.toColumnarArray(decodedBlock);
-            return new DecodedBlockNode(columnarArray, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarArray.getElementsBlock())));
+            Block childBlock = columnarArray.getElementsBlock();
+            return new DecodedBlockNode(
+                    columnarArray,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    columnarArray.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof MapBlock) {
             ColumnarMap columnarMap = ColumnarMap.toColumnarMap(decodedBlock);
-            return new DecodedBlockNode(columnarMap, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarMap.getKeysBlock()), decodeBlock(flattener, blockLeaseCloser, columnarMap.getValuesBlock())));
+            Block keyBlock = columnarMap.getKeysBlock();
+            Block valueBlock = columnarMap.getValuesBlock();
+            return new DecodedBlockNode(
+                    columnarMap,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, keyBlock), decodeBlock(flattener, blockLeaseCloser, valueBlock)),
+                    columnarMap.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RowBlock) {
             ColumnarRow columnarRow = ColumnarRow.toColumnarRow(decodedBlock);
             ImmutableList.Builder<DecodedBlockNode> children = ImmutableList.builder();
             for (int i = 0; i < columnarRow.getFieldCount(); i++) {
-                children.add(decodeBlock(flattener, blockLeaseCloser, columnarRow.getField(i)));
+                Block childBlock = columnarRow.getField(i);
+                children.add(decodeBlock(flattener, blockLeaseCloser, childBlock));
             }
-            return new DecodedBlockNode(columnarRow, children.build());
+            return new DecodedBlockNode(columnarRow, children.build(), columnarRow.getRetainedSizeInBytes(), estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof DictionaryBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((DictionaryBlock) decodedBlock).getDictionary())));
+            Block dictionary = ((DictionaryBlock) decodedBlock).getDictionary();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, dictionary)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RunLengthEncodedBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((RunLengthEncodedBlock) decodedBlock).getValue())));
+            Block childBlock = ((RunLengthEncodedBlock) decodedBlock).getValue();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
-        return new DecodedBlockNode(decodedBlock, ImmutableList.of());
+        return new DecodedBlockNode(decodedBlock, ImmutableList.of(), block.getRetainedSizeInBytes(), estimatedSizeInBytes);
     }",Buggy,"Fix estimatedSerializedSizeInBytes for RLE and Dictionary Blocks

When a block passed to OptimizedPartitionedOutputOperator is a RLE or
Dictionary block, we used to estimated the serialized size using
getLogicalSize() which returns the size of the block after inflation.
However the child block of the RLE or Dictionary Block was using plain
sizeInBytes without considering it is going to be expanded. This
commit fixes this problem by adding a scale factor to estimate how many
times the child blocks are going to be expanded.
",Buggy,"The commit message describes a fix for the `estimatedSerializedSizeInBytes` calculation in `OptimizedPartitionedOutputOperator` when dealing with `RLE` and `Dictionary` blocks. The issue was that the logical size (inflated size) of the parent block was being used, while the child block's plain `sizeInBytes` was used without considering the potential expansion.

The code diff introduces `estimatedSizeInBytes` and passes it to the `DecodedBlockNode` constructor. The `estimatedSizeInBytes` is calculated using `decodedBlock.getLogicalSizeInBytes()`. The diff also adds `retainedSizeInBytes` to the `DecodedBlockNode` constructor.

The changes align with the commit message. The code now explicitly uses the logical size for estimating the serialized size, addressing the problem described in the commit message. The addition of `retainedSizeInBytes` to the `DecodedBlockNode` constructor might be related to the fix, as it could be used in the size estimation logic.

Therefore, the changes indicate a bug fix.

**Buggy**
"
presto,21104.json,58e78de1c0cde073cc472bb7b2d4cf52155e15e3,"@@ -1,4 +1,8 @@
         public StreamProperties visitTopN(TopNNode node, List<StreamProperties> inputProperties)
         {
+            // Partial TopN doesn't guarantee that stream is ordered
+            if (node.getStep().equals(TopNNode.Step.PARTIAL)) {
+                return Iterables.getOnlyElement(inputProperties);
+            }
             return StreamProperties.ordered();
         }",Buggy,"Fix stream properties for partial TopN

This is a bug fix, that seems to could never occur.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix stream properties for partial TopN"" explicitly states that the commit is a fix. The mention of ""partial TopN"" suggests that the fix is related to how stream properties are handled in the context of a partial TopN operation. The additional comment ""This is a bug fix, that seems to could never occur."" is interesting. It suggests the bug might be rare or difficult to trigger, but it's still acknowledged as a bug fix.

**Git Diff Analysis:**

The diff modifies the `visitTopN` method, specifically addressing the `TopNNode.Step.PARTIAL` case.

-   **Original Behavior (Implicit):**  The original code always returned `StreamProperties.ordered()`. This implies that the TopN operation was always assumed to produce an ordered stream, regardless of whether it was a full or partial TopN.
-   **New Behavior (Explicit):** The modified code now includes a conditional check: `if (node.getStep().equals(TopNNode.Step.PARTIAL))`.  If the TopN node represents a partial TopN, the code now returns `Iterables.getOnlyElement(inputProperties)`. This means that the stream properties of the *input* to the partial TopN are propagated, instead of assuming the output is ordered.

**Reasoning:**

The change directly addresses the stream properties of partial TopN operations. The original code incorrectly assumed that even a partial TopN would result in an ordered stream. The fix acknowledges that a partial TopN doesn't guarantee ordering and therefore propagates the input stream properties. This is a logical correction, as a partial TopN might not have processed enough data to establish a global ordering.

The commit message and the code change are clearly aligned. The code change directly implements the fix described in the commit message. The original code's incorrect assumption about the output stream properties of a partial TopN constitutes a bug.

**Conclusion:**

**Buggy**
"
presto,18169.json,86913bdc9e40dd112bda300f21ae0f95e1330515,"@@ -1,4 +1,4 @@
         public Type getType(int field)
         {
-            return delegate.getType(field);
+            return delegate.getType(toDelegateField(field));
         }",Buggy,"Fix MappedRecordCursor bug
",Buggy,"The commit message ""Fix MappedRecordCursor bug"" suggests a bug fix. The code change modifies the `getType` method in a class related to record processing. Specifically, it changes `delegate.getType(field)` to `delegate.getType(toDelegateField(field))`. This indicates that the original code was likely passing the wrong field index to the `delegate`, leading to incorrect type information. The `toDelegateField` method is presumably responsible for mapping the field index to the correct index for the delegate. This correction strongly suggests a bug fix.

**Buggy**
"
presto,26959.json,90e7efd07b37da0ff012bb99798ed37b4173661b,"@@ -1,85 +1,83 @@
     private String toPredicate(int columnIndex, Domain domain)
     {
         String columnName = columnsNames.get(columnIndex);
         Type type = types.get(columnIndex);
 
         if (domain.getRanges().isNone() && domain.isNullAllowed()) {
             return columnName + "" IS NULL"";
         }
 
         if (domain.getRanges().isAll() && !domain.isNullAllowed()) {
             return columnName + "" IS NOT NULL"";
         }
 
         // Add disjuncts for ranges
         List<String> disjuncts = new ArrayList<>();
         List<Comparable<?>> singleValues = new ArrayList<>();
         for (Range range : domain.getRanges()) {
             checkState(!range.isAll()); // Already checked
-            Comparable<?> lowValue = range.getLow().getValue();
             if (range.isSingleValue()) {
-                singleValues.add(lowValue);
+                singleValues.add(range.getLow().getValue());
             }
             else {
                 List<String> rangeConjuncts = new ArrayList<>();
                 if (!range.getLow().isLowerUnbounded()) {
-                    Object bindValue = getBindValue(columnIndex, lowValue);
+                    Object bindValue = getBindValue(columnIndex, range.getLow().getValue());
                     switch (range.getLow().getBound()) {
                         case ABOVE:
                             rangeConjuncts.add(toBindPredicate(columnName, "">""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, "">=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             throw new IllegalStateException(""Low Marker should never use BELOW bound: "" + range);
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getLow().getBound());
                     }
                 }
                 if (!range.getHigh().isUpperUnbounded()) {
-                    Comparable<?> highValue = range.getHigh().getValue();
-                    Object bindValue = getBindValue(columnIndex, highValue);
+                    Object bindValue = getBindValue(columnIndex, range.getHigh().getValue());
                     switch (range.getHigh().getBound()) {
                         case ABOVE:
                             throw new IllegalStateException(""High Marker should never use ABOVE bound: "" + range);
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getHigh().getBound());
                     }
                 }
                 // If rangeConjuncts is null, then the range was ALL, which should already have been checked for
                 checkState(!rangeConjuncts.isEmpty());
                 disjuncts.add(""("" + Joiner.on("" AND "").join(rangeConjuncts) + "")"");
             }
         }
 
         // Add back all of the possible single values either as an equality or an IN predicate
         if (singleValues.size() == 1) {
             disjuncts.add(toBindPredicate(columnName, ""=""));
             bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, getOnlyElement(singleValues))));
         }
         else if (singleValues.size() > 1) {
             disjuncts.add(columnName + "" IN ("" + Joiner.on("","").join(limit(cycle(""?""), singleValues.size())) + "")"");
             for (Comparable<?> singleValue : singleValues) {
                 bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, singleValue)));
             }
         }
 
         // Add nullability disjuncts
         checkState(!disjuncts.isEmpty());
         if (domain.isNullAllowed()) {
             disjuncts.add(columnName + "" IS NULL"");
         }
 
         return ""("" + Joiner.on("" OR "").join(disjuncts) + "")"";
     }",Buggy,"Fix bug for domain with lower unbounded range
",Buggy,"The commit message ""Fix bug for domain with lower unbounded range"" suggests a bug fix related to how the code handles domains with lower unbounded ranges.

The code diff modifies the `toPredicate` method, which converts a domain (representing a set of possible values for a column) into a SQL predicate. The changes involve how the low and high values of ranges within the domain are handled when constructing the predicate. Specifically, the code now directly uses `range.getLow().getValue()` and `range.getHigh().getValue()` instead of assigning them to local variables before use. This change is subtle but important. It suggests that the original code might have been using an incorrect or stale value for the low or high bound in some cases, potentially leading to incorrect SQL predicates when dealing with unbounded ranges.

The fact that the code now directly retrieves the values from the `range` object each time it's needed implies that there was a potential issue with how these values were being cached or accessed previously. This is a strong indicator of a bug fix.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
presto,1063.json,097536e42a4f29b25497ec5c782ef60cabdf71ef,"@@ -1,26 +1,31 @@
     public <V> Callable<V> wrap(Callable<V> callable)
     {
         return () -> {
             try (TimeStat.BlockTimer ignored = time.time()) {
                 return callable.call();
             }
             catch (Exception e) {
                 if (e instanceof MetaException) {
                     metastoreExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 if (e instanceof TException) {
+                    if (e instanceof TBase) {
+                        // This exception is an API response and not a server error
+                        throw e;
+                    }
+
                     thriftExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 totalFailures.update(1);
                 throw e;
             }
         };
     }",Buggy,"Fix Hive metastore client API stats

The stats were counting API response exceptions as server errors.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Hive metastore client API stats"" suggests that there was an issue with how Hive metastore client API statistics were being calculated. Specifically, it mentions that ""API response exceptions were being counted as server errors."" This clearly points to a bug in the original implementation.

**Git Diff Analysis:**

The code diff focuses on the `wrap` method, which seems to be a wrapper around Callable objects used for interacting with the Hive metastore. The key change is within the `catch` block that handles exceptions.

Here's a breakdown of the changes:

1.  **`if (e instanceof TException)` block:** This block handles exceptions related to Thrift (TException), which is a common framework for communication with Hive metastore.
2.  **New `if (e instanceof TBase)` condition:** Inside the `TException` block, a new condition `if (e instanceof TBase)` has been added. `TBase` is a base class for Thrift-generated classes. This condition checks if the `TException` is also an instance of `TBase`.
3.  **`throw e;` inside the new condition:** If the exception is both a `TException` and a `TBase`, the code now immediately re-throws the exception.

**Reasoning:**

The added `if (e instanceof TBase)` condition is crucial. It appears that exceptions that are both `TException` and `TBase` represent API responses from the Hive metastore. The original code was incorrectly counting these API response exceptions as server errors, incrementing `thriftExceptions` and `totalFailures`.

The fix is to identify these API response exceptions (using the `TBase` check) and re-throw them without updating the error counters. This ensures that only actual server errors are counted as such, correcting the statistics.

**Conclusion:**

The commit message and code diff clearly indicate a bug fix. The original code had a flaw in how it classified exceptions, leading to incorrect statistics. The changes address this flaw by correctly identifying and handling API response exceptions.

**Buggy**"
presto,2919.json,54dd54cb681af311e9b6fb6910ae004dfa71988b,"@@ -1,8 +1,8 @@
     public SingleArrayBlockWriter beginBlockEntry()
     {
         if (currentEntryOpened) {
-            throw new IllegalStateException(""Expected current entry to be closed but was closed"");
+            throw new IllegalStateException(""Expected current entry to be closed but was opened"");
         }
         currentEntryOpened = true;
         return new SingleArrayBlockWriter(values, values.getPositionCount());
     }",Buggy,"Minor exception message fix for ArrayBlockBuilder

When attempting to begin an entry on an already opened block writer,
the error message does not accurately reflect the current state.
",Buggy,"The commit message indicates a fix to an exception message in the `ArrayBlockBuilder` class. The diff shows a change in the `IllegalStateException` message thrown when attempting to begin an entry on an already opened block writer. The original message ""Expected current entry to be closed but was closed"" is grammatically incorrect and doesn't accurately reflect the state. The corrected message ""Expected current entry to be closed but was opened"" is grammatically correct and provides a more accurate description of the error condition. This change directly addresses the issue described in the commit message.

**Buggy**"
presto,37880.json,954e4714ac1aac177bbf00d5641088cc7eb15289,"@@ -1,8 +1,8 @@
     public int getPartition(Object key)
     {
         int partition = requireNonNull((Integer) key, ""key is null"");
         if (!(partition >= 0 && partition < numPartitions)) {
-            throw new IllegalArgumentException(""invalid partition: %s"" + partition);
+            throw new IllegalArgumentException(format(""Unexpected partition: %s. Total number of partitions: %s."", partition, numPartitions));
         }
         return partition;
     }",Buggy,"Fix error message in IntegerIdentityPartitioner
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix error message in IntegerIdentityPartitioner"" clearly states the intention is to correct an error message within the `IntegerIdentityPartitioner` class.

2.  **Code Diff:**
    *   The code diff modifies the `IllegalArgumentException` thrown when the partition value is outside the expected range.
    *   The original error message was `""invalid partition: %s"" + partition`. This is problematic because:
        *   It doesn't use `String.format` correctly, so the `%s` placeholder isn't replaced.
        *   It doesn't include the total number of partitions, making debugging harder.
    *   The corrected error message is `format(""Unexpected partition: %s. Total number of partitions: %s."", partition, numPartitions)`. This is a significant improvement because:
        *   It uses `String.format` correctly, inserting the `partition` value into the message.
        *   It includes the total number of partitions, providing more context for the error.

3.  **Alignment:** The code change directly addresses the issue described in the commit message. The original error message was indeed flawed, and the change fixes it by providing a more informative and correctly formatted message.

4.  **Bug Fix Indication:** The original error message could be considered a bug because it was unhelpful and potentially misleading. The corrected message makes debugging easier and provides crucial information. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
presto,5480.json,3d776ab3a9b34d155caa2097ed5e0a82c3f9ca39,"@@ -1,6 +1,7 @@
     private static Class<? extends InputFormat<?, ?>> getInputFormatClass(JobConf conf, String inputFormatName)
             throws ClassNotFoundException
     {
         Class<?> clazz = conf.getClassByName(inputFormatName);
-        return (Class<? extends InputFormat<?, ?>>) clazz.asSubclass(InputFormat.class);
+        // TODO: remove redundant cast to Object after IDEA-118533 is fixed
+        return (Class<? extends InputFormat<?, ?>>) (Object) clazz.asSubclass(InputFormat.class);
     }",Buggy,"Fix bogus error message in IDEA 13
",Buggy,"The commit message indicates a fix for a bogus error message in IDEA 13. The code change involves adding a redundant cast to `Object` before casting to `Class<? extends InputFormat<?, ?>>`. The comment ""// TODO: remove redundant cast to Object after IDEA-118533 is fixed"" suggests that the original code was causing an error in IDEA 13, and this change is a workaround to resolve that error. This aligns with the commit message indicating a bug fix.

**Buggy**"
presto,14661.json,a5a04929e51756cac167c14cf73399a0df74f4c5,"@@ -1,4 +1,5 @@
     public static long valueAtQuantileBigint(@SqlType(""qdigest(bigint)"") Slice input, @SqlType(StandardTypes.DOUBLE) double quantile)
     {
+        checkCondition(quantile >= 0 && quantile <= 1, INVALID_FUNCTION_ARGUMENT, ""Quantile should be within bounds [0, 1], was: "" + quantile);
         return new QuantileDigest(input).getQuantile(quantile);
     }",Buggy,"Fix invalid quantile to be a user error
",Buggy,"The commit message ""Fix invalid quantile to be a user error"" suggests that the change addresses how invalid quantile values are handled. The diff adds a check to ensure the quantile is within the valid range [0, 1]. If the quantile is outside this range, an `INVALID_FUNCTION_ARGUMENT` error is thrown. This aligns perfectly with the commit message, indicating a bug fix where invalid quantile values were not properly handled, and the change makes it a user error.

**Buggy**"
presto,2512.json,ff82d4c25dcaeb742f1eb3470a04e89a8d2ede7b,"@@ -1,5 +1,5 @@
     public Block getRegion(int positionOffset, int length)
     {
         checkValidRegion(positionCount, positionOffset, length);
-        return new DictionaryBlock(idsOffset + positionOffset, length, dictionary, ids, false, randomDictionaryId());
+        return new DictionaryBlock(idsOffset + positionOffset, length, getDictionary(), ids, false, getDictionarySourceId());
     }",Buggy,"Fix bug in DictionaryBlock getRegion
",Buggy,"The commit message ""Fix bug in DictionaryBlock getRegion"" suggests that the changes in the diff are intended to correct a bug within the `getRegion` method of the `DictionaryBlock` class.

The diff shows that the `DictionaryBlock` constructor is being called with `getDictionary()` and `getDictionarySourceId()` instead of `dictionary` and `randomDictionaryId()`. This change indicates a bug fix because the original code was likely using an incorrect or outdated dictionary or dictionary source ID, leading to unexpected behavior or incorrect results when accessing a region of the `DictionaryBlock`. The updated code uses the correct methods to retrieve the dictionary and its source ID, ensuring that the region is accessed with the appropriate context.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**
"
presto,22974.json,eb4e582942793631a8de8cf6e6e1c06ce3053d75,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = factories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(delegate.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in SessionPropertyDefaults
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Fix error message in SessionPropertyDefaults"" clearly indicates that the purpose of the commit is to correct an error message.

2.  **Git Diff:** The diff shows a change in the `checkState` method call within the `setConfigurationManager` method. Specifically, the error message string has been modified to include the value of the `name` variable using string formatting (`'%s'`).

3.  **Relevance:** The diff directly relates to the commit message. The original error message likely lacked context, making it difficult to diagnose the issue when a session property configuration manager was not registered. By including the `name` of the unregistered manager in the error message, the fix provides more helpful information for debugging. This makes it easier to identify which configuration manager is missing.

4.  **Bug Fix Indication:** The change improves the error reporting, which is a common characteristic of bug fixes. The original error message was not informative enough, hindering the debugging process. The corrected message provides specific details, resolving this issue.

**Conclusion:**

The commit message and the code changes are directly related. The change improves error reporting, which is a bug fix.

**Buggy**
"
presto,22974.json,59d297913ff6641b852bd236161f237a3f9e3af7,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = sessionPropertyConfigurationManagerFactories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(sessionPropertyConfigurationManager.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in QuerySessionSupplier
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix error message in QuerySessionSupplier"" clearly indicates that the primary goal of this commit is to correct an error message within the `QuerySessionSupplier` (or related) code.

2.  **Git Diff:** The diff shows a change to the `checkState` method call within the `setConfigurationManager` method. The original code had a placeholder `%s` without providing the argument to be formatted into the error message. The corrected code now includes the `name` variable as an argument to `checkState`, which will be used to format the error message, making it more informative.

3.  **Alignment:** The commit message and the code change are perfectly aligned. The code change directly addresses the issue described in the commit message by improving the error message.

4.  **Bug Fix Indication:** The original error message was likely unhelpful because it didn't include the name of the unregistered session property configuration manager. This lack of information would make debugging more difficult. The corrected code provides a more specific and helpful error message, which is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
presto,31633.json,318f9bf12b4402074f5f8caf0845a121c026f0df,"@@ -1,8 +1,8 @@
     public Object getObjectValue(ConnectorSession session, Block block, int position)
     {
         if (block.isNull(position)) {
             return null;
         }
 
-        return BingTile.decode(block.getLong(0, 0));
+        return BingTile.decode(block.getLong(position, 0));
     }",Buggy,"Fix getObjectValue in BingTileType

This change fixes a bug in decoding arrays of bing tiles.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Fix getObjectValue in BingTileType"" clearly states that the commit intends to fix a bug within the `getObjectValue` method of the `BingTileType` class.
    *   The message ""This change fixes a bug in decoding arrays of bing tiles"" provides more context, indicating the bug is related to decoding arrays of Bing tiles.

2.  **Git Diff Analysis:**
    *   The diff shows a change within the `getObjectValue` method.
    *   The original code `BingTile.decode(block.getLong(0, 0))` always used the value at position `0` of the block, regardless of the `position` argument passed to `getObjectValue`. This would lead to incorrect decoding when processing arrays of Bing tiles, as it would always decode the first tile in the array.
    *   The corrected code `BingTile.decode(block.getLong(position, 0))` now uses the `position` argument to retrieve the correct value from the block, fixing the bug in decoding arrays of Bing tiles.

3.  **Relevance and Bug Fix Indication:**
    *   The commit message and the code change are highly relevant. The commit message describes a bug in decoding Bing tile arrays, and the code change directly addresses this issue by using the correct position when retrieving the tile data from the block.
    *   The original code's behavior of always using position `0` is a clear indication of a bug, as it would prevent correct decoding of arrays of Bing tiles. The corrected code fixes this logical error.

**Conclusion:**

The commit message and the code change clearly indicate a bug fix. The original code had a logical error that prevented correct decoding of Bing tile arrays, and the corrected code fixes this error.

**Buggy**"
netty,5669.json,b39ffed042844adecaf0a4fc4e9a2f53edaa111d,"@@ -1,15 +1,15 @@
         private void record(int actualReadBytes) {
-            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) {
+            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT)]) {
                 if (decreaseNow) {
                     index = max(index - INDEX_DECREMENT, minIndex);
                     nextReceiveBufferSize = SIZE_TABLE[index];
                     decreaseNow = false;
                 } else {
                     decreaseNow = true;
                 }
             } else if (actualReadBytes >= nextReceiveBufferSize) {
                 index = min(index + INDEX_INCREMENT, maxIndex);
                 nextReceiveBufferSize = SIZE_TABLE[index];
                 decreaseNow = false;
             }
         }",Buggy,"Fix incorrect calculation of next buffer size in AdaptiveRecvByteBufAllocator (#9555)

Motivation:

Due a bug we did not always correctly calculate the next buffer size in AdaptiveRecvByteBufAllocator.

Modification:

Fix calculation and add unit test

Result:

Correct calculation is always used. 
",Buggy,"The commit message indicates a bug fix related to the incorrect calculation of the next buffer size in `AdaptiveRecvByteBufAllocator`. The provided diff shows a modification in the `record` method, specifically in the condition that checks if `actualReadBytes` is less than or equal to a value derived from `SIZE_TABLE` and `index`.

The original code had `index - INDEX_DECREMENT - 1`, while the corrected code has `index - INDEX_DECREMENT`. This change suggests that the original calculation was off by one, potentially leading to an incorrect buffer size being selected. The commit message explicitly states that the modification fixes the calculation. The addition of a unit test (mentioned in the commit message but not shown in the diff) further supports the claim of a bug fix. The change in the condition directly affects the logic for adjusting the buffer size, which aligns with the commit message's description of the problem.

Therefore, the changes indicate a bug fix.

**Buggy**
"
netty,10635.json,799350c369e68462b61c6aef97db2a33ea937434,"@@ -1,63 +1,67 @@
     public void updateDependencyTree(int childStreamId, int parentStreamId, short weight, boolean exclusive) {
         if (weight < MIN_WEIGHT || weight > MAX_WEIGHT) {
             throw new IllegalArgumentException(String.format(
                     ""Invalid weight: %d. Must be between %d and %d (inclusive)."", weight, MIN_WEIGHT, MAX_WEIGHT));
         }
         if (childStreamId == parentStreamId) {
             throw new IllegalArgumentException(""A stream cannot depend on itself"");
         }
 
         State state = state(childStreamId);
         if (state == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             state = new State(childStreamId);
             stateOnlyRemovalQueue.add(state);
             stateOnlyMap.put(childStreamId, state);
         }
 
         State newParent = state(parentStreamId);
         if (newParent == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             newParent = new State(parentStreamId);
             stateOnlyRemovalQueue.add(newParent);
             stateOnlyMap.put(parentStreamId, newParent);
+            // Only the stream which was just added will change parents. So we only need an array of size 1.
+            List<ParentChangedEvent> events = new ArrayList<ParentChangedEvent>(1);
+            connectionState.takeChild(newParent, false, events);
+            notifyParentChanged(events);
         }
 
         // if activeCountForTree == 0 then it will not be in its parent's pseudoTimeQueue and thus should not be counted
         // toward parent.totalQueuedWeights.
         if (state.activeCountForTree != 0 && state.parent != null) {
             state.parent.totalQueuedWeights += weight - state.weight;
         }
         state.weight = weight;
 
         if (newParent != state.parent || (exclusive && newParent.children.size() != 1)) {
             final List<ParentChangedEvent> events;
             if (newParent.isDescendantOf(state)) {
                 events = new ArrayList<ParentChangedEvent>(2 + (exclusive ? newParent.children.size() : 0));
                 state.parent.takeChild(newParent, false, events);
             } else {
                 events = new ArrayList<ParentChangedEvent>(1 + (exclusive ? newParent.children.size() : 0));
             }
             newParent.takeChild(state, exclusive, events);
             notifyParentChanged(events);
         }
 
         // The location in the dependency tree impacts the priority in the stateOnlyRemovalQueue map. If we created new
         // State objects we must check if we exceeded the limit after we insert into the dependency tree to ensure the
         // stateOnlyRemovalQueue has been updated.
         while (stateOnlyRemovalQueue.size() > maxStateOnlySize) {
             State stateToRemove = stateOnlyRemovalQueue.poll();
             stateToRemove.parent.removeChild(stateToRemove);
             stateOnlyMap.remove(stateToRemove.streamId);
         }
     }",Buggy,"Fix HTTP/2 dependency tree corruption

Motivation:

Chrome was randomly getting stuck loading the tiles examples.
Investigation showed that the Netty flow controller thought it had
nothing to send for the connection even though some streams has queued
data and window available.

Modifications:

Fixed an accounting error where an implicitly created parent was not
being added to the dependency tree, thus it and all of its children were
orphaned from the connection's tree and would never have data written.

Result:

Fixes #6621
",Buggy,"The commit message indicates a fix for a corruption issue in the HTTP/2 dependency tree, leading to streams being orphaned and data not being written. The code diff shows modifications within the `updateDependencyTree` method. Specifically, when a new parent stream is created (i.e., `newParent == null`), the code now calls `connectionState.takeChild(newParent, false, events)` and `notifyParentChanged(events)`. This suggests that the original code was missing a step to properly integrate the newly created parent into the dependency tree, which aligns with the commit message's description of the problem. The addition of these lines ensures that the newly created parent is correctly added to the connection's dependency tree, preventing it and its children from being orphaned.

Therefore, the changes indicate a bug fix.

**Buggy**
"
netty,536.json,6fc7c589f023b069ac5f4df8a6766f0b501487fb,"@@ -1,10 +1,11 @@
         DatagramPacket newDatagramPacket(ByteBuf buffer, InetSocketAddress localAddress) throws UnknownHostException {
             final InetAddress address;
-            if (scopeId != 0) {
-                address = Inet6Address.getByAddress(null, addr, scopeId);
+            if (addrLen == ipv4Bytes.length) {
+                System.arraycopy(addr, 0, ipv4Bytes, 0, addrLen);
+                address = InetAddress.getByAddress(ipv4Bytes);
             } else {
-                address = InetAddress.getByAddress(addr);
+                address = Inet6Address.getByAddress(null, addr, scopeId);
             }
             return new DatagramPacket(buffer.writerIndex(count),
                     localAddress, new InetSocketAddress(address, port));
         }",Buggy,"Correctly handle ipv6 mapped ipv4 addresses when using recvmmsg (#9541)


Motivation:

394a1b3485000c211595aff7495c4f863972af29 introduced the possibility to use recvmmsg(...) but did not correctly handle ipv6 mapped ip4 addresses to make it consistent with other transports.

Modifications:

- Correctly handle ipv6 mapped ipv4 addresses by only copy over the relevant bytes
- Small improvement on how to detect ipv6 mapped ipv4 addresses by using memcmp and not byte by byte compare
- Adjust test to cover this bug

Result:

Correctly handle ipv6 mapped ipv4 addresses
",Buggy,"The commit message indicates a bug fix related to handling IPv6 mapped IPv4 addresses when using `recvmmsg`. The motivation explains that the previous implementation introduced in commit `394a1b3485000c211595aff7495c4f863972af29` did not correctly handle these addresses, leading to inconsistency with other transports.

The modifications include:

1.  Correctly handling IPv6 mapped IPv4 addresses by copying only the relevant bytes.
2.  Improving the detection of IPv6 mapped IPv4 addresses using `memcmp` instead of byte-by-byte comparison.
3.  Adjusting the test to cover this bug.

The diff shows that the code now checks `addrLen` to determine if it's an IPv4 address. If it is, it copies the address bytes into `ipv4Bytes` and creates an `InetAddress` from it. Otherwise, it proceeds as before, creating an `Inet6Address`. This change directly addresses the issue of incorrectly handling IPv6 mapped IPv4 addresses.

The commit message clearly states that the changes are intended to fix a bug related to IPv6 mapped IPv4 addresses. The code modifications align with this intention by correctly handling these addresses and improving the detection mechanism. The test adjustment further confirms that this is a bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**"
netty,15758.json,f0a3f849f7d0e028eb97e672a3cbf52648b98617,"@@ -1,26 +1,27 @@
     public ByteBuf writeZero(int length) {
         if (length == 0) {
             return this;
         }
 
+        ensureWritable(length);
         checkIndex(writerIndex, length);
 
         int nLong = length >>> 3;
         int nBytes = length & 7;
         for (int i = nLong; i > 0; i --) {
             writeLong(0);
         }
         if (nBytes == 4) {
             writeInt(0);
         } else if (nBytes < 4) {
             for (int i = nBytes; i > 0; i --) {
                 writeByte((byte) 0);
             }
         } else {
             writeInt(0);
             for (int i = nBytes - 4; i > 0; i --) {
                 writeByte((byte) 0);
             }
         }
         return this;
     }",Buggy,"Fix a bug in AbstractByteBuf.writeZero() where the capacity is not auto-expanded
",Buggy,"The commit message indicates a bug fix in `AbstractByteBuf.writeZero()` related to auto-expansion of capacity. The diff shows the addition of `ensureWritable(length)` at the beginning of the method. This line ensures that the buffer has enough capacity to write the specified number of zero bytes. Without this, if the buffer's current capacity is less than the required length, the write operation would fail or lead to unexpected behavior. Thus, the added line fixes the bug described in the commit message.

**Buggy**"
netty,14984.json,dcc39e5b21eda7bdd9563ed86253a693923f6432,"@@ -1,13 +1,13 @@
     protected String format(ChannelHandlerContext ctx, String eventName, Object firstArg, Object secondArg) {
         if (secondArg == null) {
             return formatSimple(ctx, eventName, firstArg);
         }
 
         String chStr = ctx.channel().toString();
         String arg1Str = String.valueOf(firstArg);
         String arg2Str = secondArg.toString();
         StringBuilder buf = new StringBuilder(
-                chStr.length() + 1 + eventName + 2 + arg1Str.length() + 2 + arg2Str.length());
+                chStr.length() + 1 + eventName.length() + 2 + arg1Str.length() + 2 + arg2Str.length());
         buf.append(chStr).append(' ').append(eventName).append("": "").append(arg1Str).append("", "").append(arg2Str);
         return buf.toString();
     }",Buggy,"Fixes a LoggingHandler#format method with two arguments

Motivation:
Bug in capacity calculation: occurs auto convert to string instead of sum up.

Modifications:
Use `eventName.length()` in sum.

Result:
Less trash in logs.
",Buggy,"The commit message indicates a bug fix in the `LoggingHandler#format` method related to capacity calculation. The motivation explains that the bug occurs because of an incorrect calculation when converting to a string instead of summing up the lengths. The modification involves using `eventName.length()` in the sum, which suggests a correction to the initial capacity calculation of the `StringBuilder`. The result is less trash in the logs, implying that the fix reduces unnecessary memory allocation or string concatenation.

The diff shows that the length calculation of the `StringBuilder` was previously using `eventName` directly, which would convert `eventName` to a string and use the length of that string representation (likely the object's class name). The change replaces `eventName` with `eventName.length()`, which correctly calculates the length of the event name string. This confirms the bug fix described in the commit message.

**Buggy**"
netty,2881.json,338e1a991c5d15695bd654a7b8d2bd4008e868ae,"@@ -1,51 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
                         // After setting the state to NONE, look at the tasks queue one more time.
                         // If it is empty, then we can return from this method.
                         // Otherwise, it means the producer thread has called execute(Runnable)
                         // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
                         // There are two possible scenarios when this happen
                         //
                         // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
                         //    is successfully setting the state to SUBMITTED. This mean the producer
                         //    will call / has called executor.execute(this). In this case, we can just return.
                         // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
                         //    returns false. In this case, the producer thread won't call executor.execute.
                         //    In this case, we need to change the state to RUNNING and keeps running.
                         //
                         // The above cases can be distinguished by performing a
                         // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
-                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                        if (tasks.isEmpty() || !state.compareAndSet(NONE, RUNNING)) {
                             return; // done
                         }
                     }
                 }
             }
         }",Buggy,"Fix a bug introduced by 79706357c73ded02615d0445db7503b646ff9547 which can cause thread to spin in an infinite loop. (#9579)

Motivation:
peek() is implemented in a similar way to poll() for the mpsc queue, thus it is more like a consumer call.
It is possible that we could have multiple thread call peek() and possibly one thread calls poll() at at the same time.
This lead to multiple consumer scenario, which violates the multiple producer single consumer condition and could lead to spin in an infinite loop in peek()

Modification:
Use isEmpty() instead of peek() to check if task queue is empty

Result:
Dont violate the mpsc semantics.
",Buggy,"The commit message clearly states that the change fixes a bug introduced by a previous commit. The bug causes a thread to spin in an infinite loop due to incorrect usage of `peek()` in conjunction with `poll()` in a multi-threaded environment. The commit message explains that `peek()` was being used in a way that violated the multiple-producer single-consumer (MPSC) semantics of the queue, potentially leading to multiple consumers and the infinite loop.

The diff shows that `tasks.peek() == null` has been replaced with `tasks.isEmpty()`. This change aligns with the commit message's intention to avoid the consumer-like behavior of `peek()` and instead use `isEmpty()` to check the queue's state without potentially interfering with the MPSC semantics. This replacement directly addresses the described bug by preventing the race condition that caused the infinite loop.

Therefore, the changes indicate a bug fix.
**Buggy**
"
netty,2881.json,79706357c73ded02615d0445db7503b646ff9547,"@@ -1,34 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
-                        return; // done
+                        // After setting the state to NONE, look at the tasks queue one more time.
+                        // If it is empty, then we can return from this method.
+                        // Otherwise, it means the producer thread has called execute(Runnable)
+                        // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
+                        // There are two possible scenarios when this happen
+                        //
+                        // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
+                        //    is successfully setting the state to SUBMITTED. This mean the producer
+                        //    will call / has called executor.execute(this). In this case, we can just return.
+                        // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
+                        //    returns false. In this case, the producer thread won't call executor.execute.
+                        //    In this case, we need to change the state to RUNNING and keeps running.
+                        //
+                        // The above cases can be distinguished by performing a
+                        // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
+                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                            return; // done
+                        }
                     }
                 }
             }
         }",Buggy,"Fix race condition in the NonStickyEventExecutorGroup (#8232)

Motivation:

There was a race condition between the task submitter and task executor threads such that the last Runnable submitted may not get executed. 

Modifications:

The bug was fixed by checking the task queue and state in the task executor thread after it saw the task queue was empty.

Result:

Fixes #8230",Buggy,"The commit message indicates a fix for a race condition in `NonStickyEventExecutorGroup`. The provided code diff modifies the `run` method of an inner class, likely the task executor. The original code sets the state to `NONE` and returns when the task queue is empty. The modified code adds a check after setting the state to `NONE` to see if any new tasks have been added to the queue in the interim. It uses `tasks.peek() == null` to check if the queue is empty and `state.compareAndSet(NONE, RUNNING)` to handle the race condition described in the comments. This logic ensures that the executor continues running if new tasks arrive while it's transitioning to an idle state. The added comments explain the reasoning behind the changes and the potential race conditions. The changes clearly address a concurrency issue and aim to ensure all submitted tasks are executed, aligning with the commit message's claim of fixing a race condition.

**Buggy**
"
netty,17549.json,20894bc99e28191cc3926ae95a23f5c7d3969a33,"@@ -1,4 +1,3 @@
     public ByteBuffer internalNioBuffer(int index, int length) {
-        // Do not mess with the internal buffer's byte order.
-        return buf.nioBuffer(index, length).order(order);
+        return buf.internalNioBuffer(index, length).duplicate().order(order);
     }",Buggy,"Fix a bug in internalNioBuffer() implementations of derived buffers

- A user can create multiple duplicates of a buffer and access their internal NIO buffers. (e.g. write multiple duplicates to multiple channels assigned to different event loop.)  Because the derived buffers' internalNioBuffer() simply delegates the call to the original buffer, all derived buffers and the original buffer's internalNioBuffer() will return the same buffer, which will lead to a race condition.
- Fixes #1739
",Buggy,"The commit message describes a race condition that occurs when multiple duplicate buffers access the same internal NIO buffer. The `internalNioBuffer()` method in derived buffers was delegating the call to the original buffer, causing all derived buffers and the original buffer to return the same buffer. This could lead to race conditions when multiple threads or event loops access these buffers concurrently.

The code diff modifies the `internalNioBuffer()` method to return a duplicate of the internal NIO buffer before setting the byte order. This ensures that each derived buffer gets its own independent NIO buffer, preventing the race condition described in the commit message.

The changes directly address the bug described in the commit message by ensuring that each derived buffer has its own independent NIO buffer. This eliminates the race condition and fixes the bug.

**Buggy**
"
netty,2272.json,cd3254df88b60476dc04b39915d3d70c200eb6f4,"@@ -1,6 +1,7 @@
         private static void reachabilityFence0(Object ref) {
             if (ref != null) {
-                // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
-                synchronized (ref) { }
+                synchronized (ref) {
+                    // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
+                }
             }
         }",Buggy,"Update to new checkstyle plugin (#8777) (#8780)

Motivation:

We need to update to a new checkstyle plugin to allow the usage of lambdas.

Modifications:

- Update to new plugin version.
- Fix checkstyle problems.

Result:

Be able to use checkstyle plugin which supports new Java syntax.",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

*   **Purpose:** The primary goal is to update the Checkstyle plugin to a newer version that supports lambda expressions.
*   **Modifications:** The commit involves updating the plugin version and fixing Checkstyle-reported issues.
*   **Result:** The desired outcome is to enable the use of the Checkstyle plugin with modern Java syntax.

**Git Diff Analysis:**

*   The diff shows a change within the `reachabilityFence0` method. Specifically, the comment has been moved inside the synchronized block.
*   The code itself doesn't seem to have any functional changes. The `synchronized` block remains empty, and the `ref != null` check is still present.

**Reasoning:**

The commit message indicates a maintenance task: updating the Checkstyle plugin. The diff shows a change related to code style or formatting, likely to satisfy the new Checkstyle rules. The comment was probably flagged by the updated Checkstyle plugin because of its location outside the synchronized block. Moving the comment inside the block resolves the Checkstyle issue without altering the code's functionality. This doesn't seem to be related to fixing a bug in the original code.

**Conclusion:**

NotBuggy
"
netty,1691.json,32746c53c1089550f79bd2d6eeba4d65dab1f2d3,"@@ -1,24 +1,24 @@
-    static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
+    private static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
         switch (lowOrderBitsToPreserve) {
         case 0:
             return 0;
         case 1:
-            return (byte) (0x01 & b);
+            return (byte) (0x80 & b);
         case 2:
-            return (byte) (0x03 & b);
+            return (byte) (0xC0 & b);
         case 3:
-            return (byte) (0x07 & b);
+            return (byte) (0xE0 & b);
         case 4:
-            return (byte) (0x0F & b);
+            return (byte) (0xF0 & b);
         case 5:
-            return (byte) (0x1F & b);
+            return (byte) (0xF8 & b);
         case 6:
-            return (byte) (0x3F & b);
+            return (byte) (0xFC & b);
         case 7:
-            return (byte) (0x7F & b);
+            return (byte) (0xFE & b);
         case 8:
             return b;
         default:
             throw new IllegalArgumentException(""lowOrderBitsToPreserve: "" + lowOrderBitsToPreserve);
         }
     }",Buggy,"EDNS Client Subnet is not encoded correctly when source prefix length is not a multiple of 8.

Motivation:
When the ECS source prefix length is not a mutiple of 8, the last byte the address inside the
ECS OPT record is not padded properly.

Modifications:
DefaultDnsRecordEncoder.padWithZeros(...) was modified to add padding from the least
significant bits.

Result:
ECS encoding bug fixed.
",Buggy,"The commit message indicates a bug fix related to the encoding of the EDNS Client Subnet (ECS) option in DNS queries. Specifically, the issue arises when the source prefix length is not a multiple of 8, leading to incorrect padding of the last byte of the address in the ECS OPT record.

The code modification involves the `padWithZeros` method in `DefaultDnsRecordEncoder`. The original implementation used bitwise AND operations with masks like `0x01`, `0x03`, `0x07`, etc., which preserve the least significant bits. The corrected implementation uses masks like `0x80`, `0xC0`, `0xE0`, etc., which preserve the most significant bits. This change aligns with the commit message's description of the bug and its fix. The original code was masking out the most significant bits instead of padding the least significant bits with zeros.

Therefore, the changes indicate a bug fix.

**Buggy**"
netty,13070.json,a29532df43c9db08db5dbc83f30bb7bd2e55a596,"@@ -1,12 +1,12 @@
     private static Method updateByteBuffer(Checksum checksum) {
         if (PlatformDependent.javaVersion() >= 8) {
             try {
                 Method method = checksum.getClass().getDeclaredMethod(""update"", ByteBuffer.class);
-                method.invoke(method, ByteBuffer.allocate(1));
+                method.invoke(checksum, ByteBuffer.allocate(1));
                 return method;
             } catch (Throwable ignore) {
                 return null;
             }
         }
         return null;
     }",Buggy,"Fix ByteBufChecksum optimisation for CRC32 and Adler32 (#9242)

Motivation:

Because of a simple bug in ByteBufChecksum#updateByteBuffer(Checksum),
ReflectiveByteBufChecksum is never used for CRC32 and Adler32, resulting
in direct ByteBuffers being checksummed byte by byte, which is
undesriable.

Modification:

Fix ByteBufChecksum#updateByteBuffer(Checksum) method to pass the
correct argument to Method#invoke(Checksum, ByteBuffer).

Result:

ReflectiveByteBufChecksum will now be used for Adler32 and CRC32 on
Java8+ and direct ByteBuffers will no longer be checksummed on slow
byte-by-byte basis.",Buggy,"The commit message indicates a bug fix related to the `ByteBufChecksum` class. Specifically, it mentions that `ReflectiveByteBufChecksum` was not being used for `CRC32` and `Adler32` due to a bug in `ByteBufChecksum#updateByteBuffer(Checksum)`. This resulted in inefficient byte-by-byte checksumming for direct ByteBuffers.

The provided diff shows a change in the `updateByteBuffer` method. The original code had `method.invoke(method, ByteBuffer.allocate(1));`, which is incorrect because it's trying to invoke the method on itself instead of the checksum object. The corrected code is `method.invoke(checksum, ByteBuffer.allocate(1));`, which correctly invokes the `update` method on the `checksum` object with a ByteBuffer.

This change directly addresses the bug described in the commit message. By correcting the invocation of the `update` method, the `ReflectiveByteBufChecksum` can now be used for `CRC32` and `Adler32`, resolving the performance issue with direct ByteBuffers. The commit message clearly states the bug and the fix, and the diff confirms that the code change aligns with the described fix.

**Buggy**"
netty,239.json,1cc104e1c02f5ee54979d8dfcba5aa4fb20d3503,"@@ -1,3 +1,3 @@
-    public static SctpServerChannel open() {
+    public static SctpServerChannel open() throws IOException {
         return null;
     }",Buggy,"Fix a compilation error
",Buggy,"The commit message indicates a fix for a compilation error. The code change adds `throws IOException` to the method signature of `open()`. This suggests that the original code was missing a declaration for a checked exception that the method might throw, leading to a compilation error. The addition of `throws IOException` resolves this issue.

**Buggy**
"
netty,1341.json,a4c96483d1e83981a1e0860d3a6f71fbe21d500c,"@@ -1,18 +1,20 @@
     private static boolean anyInterfaceSupportsIpV6() {
         try {
             Enumeration<NetworkInterface> interfaces = NetworkInterface.getNetworkInterfaces();
             while (interfaces.hasMoreElements()) {
                 NetworkInterface iface = interfaces.nextElement();
                 Enumeration<InetAddress> addresses = iface.getInetAddresses();
                 while (addresses.hasMoreElements()) {
-                    if (addresses.nextElement() instanceof Inet6Address) {
+                    InetAddress inetAddress = addresses.nextElement();
+                    if (inetAddress instanceof Inet6Address && !inetAddress.isAnyLocalAddress() &&
+                        !inetAddress.isLoopbackAddress() && !inetAddress.isLinkLocalAddress()) {
                         return true;
                     }
                 }
             }
         } catch (SocketException e) {
             logger.debug(""Unable to detect if any interface supports IPv6, assuming IPv4-only"", e);
             // ignore
         }
         return false;
     }",Buggy,"Fix a bug where making IPv6 DnsQuestion when it's not supported (#10170)

Motivation:
Related https://github.com/line/armeria/issues/2463
Here is an example that an NIC has only link local address for IPv6.
```
$ ipaddr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if18692: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1460 qdisc noqueue
    link/ether 1a:5e:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff
    inet 10.xxx.xxx.xxx/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::xxxx:xxxx:xxxx:xxxx/64 scope link
       valid_lft forever preferred_lft forever
```
If the NICs have only local or link local addresses, We should not send IPv6 DNS queris.

Modification:
- Ignore link-local IPv6 addresses which may exist even on a machine without IPv6 network.

Result:
- `DnsNameResolver` does not send DNS queries for AAAA when IPv6 is not available.",Buggy,"The commit message indicates a bug fix related to IPv6 DNS queries being sent even when IPv6 is not properly supported. The provided `ipaddr` output demonstrates a scenario where a network interface has only a link-local IPv6 address, which should not trigger IPv6 DNS queries.

The code modification focuses on the `anyInterfaceSupportsIpV6()` method. The original code simply checked if any interface had an `Inet6Address`. The modified code adds additional checks:

1.  `!inetAddress.isAnyLocalAddress()`: Excludes the ""any local address"" (::).
2.  `!inetAddress.isLoopbackAddress()`: Excludes the loopback address (::1).
3.  `!inetAddress.isLinkLocalAddress()`: Excludes link-local addresses (fe80::/10).

These checks ensure that only global or site-local IPv6 addresses are considered as indicators of IPv6 support. The `SocketException` is still caught and logged at the debug level, preventing the application from crashing if there are issues accessing network interfaces.

The changes directly address the problem described in the commit message by preventing IPv6 DNS queries when only link-local or other non-routable IPv6 addresses are present. This aligns with the stated goal of the commit.

**Buggy**
"
netty,17654.json,c0bbde48b78163a394ae3fd63f98b6b6c56acb25,"@@ -1,3 +1,3 @@
     public ByteBuffer nioBuffer(int index, int length) {
-        return (ByteBuffer) buffer.duplicate().position(index).position(index + length);
+        return (ByteBuffer) buffer.duplicate().position(index).limit(index + length);
     }",Buggy,"[#1852] Fix bug in UnpooledDirectByteBuf.nioBuffer(...) implementation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message `[#1852] Fix bug in UnpooledDirectByteBuf.nioBuffer(...) implementation` explicitly states that the commit is intended to fix a bug in the `nioBuffer(...)` method of the `UnpooledDirectByteBuf` class. This is a strong indicator that the changes are bug-related.

2.  **Git Diff Analysis:**
    *   The diff shows a single line change within the `nioBuffer(int index, int length)` method.
    *   The original code used `.position(index + length)` twice, which is incorrect. The second `.position()` should have been `.limit()` to correctly set the limit of the duplicated ByteBuffer.
    *   The corrected code replaces the second `.position()` call with `.limit()`. This ensures that the returned `ByteBuffer` has the correct position and limit, representing the desired slice of the buffer.

3.  **Alignment and Bug Fix Indication:**
    *   The commit message and the code change are perfectly aligned. The commit message claims to fix a bug in `nioBuffer(...)`, and the code change modifies the `.nioBuffer()` method to correctly set the limit of the duplicated `ByteBuffer`.
    *   The original code had a logical error that would have resulted in an incorrect `ByteBuffer` being returned. The corrected code fixes this error.

**Conclusion:**

The commit message clearly states a bug fix, and the code change directly addresses a logical error in the `nioBuffer(...)` method. Therefore, the changes indicate a bug fix.

**Buggy**"
netty,12409.json,6983f704c0882dd322bc1cf0c7c1c635e4b05ea5,"@@ -1,3 +1,5 @@
     public ByteBuf readBytes(ByteBuf dst) {
-        throw new UnreplayableOperationException();
+        checkReadableBytes(dst.writableBytes());
+        buffer.readBytes(dst);
+        return this;
     }",Buggy,"Fix unreplayble error
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix unreplayble error"" strongly suggests that the commit addresses a bug related to an ""unreplayble error"". This implies that the previous code had a problem that prevented some operation from being replayed or retried, and the current commit aims to resolve it.

2.  **Code Diff:**
    *   The original code `throw new UnreplayableOperationException();` unconditionally threw an `UnreplayableOperationException`. This means that whenever the `readBytes(ByteBuf dst)` method was called, it would always throw this exception, preventing any actual reading of bytes.
    *   The modified code replaces the unconditional exception throw with the following:
        *   `checkReadableBytes(dst.writableBytes());`: This line likely checks if there are enough readable bytes in the source buffer and enough writable space in the destination buffer before proceeding. This is a crucial check to prevent potential errors during the read operation.
        *   `buffer.readBytes(dst);`: This line performs the actual reading of bytes from the internal buffer (`buffer`) into the destination buffer (`dst`).
        *   `return this;`: This line returns the current object, allowing for method chaining.

3.  **Alignment and Bug Fix Indication:**
    *   The original code's unconditional `UnreplayableOperationException` throw was clearly a bug. It prevented the `readBytes` method from ever functioning correctly.
    *   The modified code introduces a check for readable and writable bytes and then performs the actual read operation. This fixes the bug by allowing the method to read bytes when appropriate conditions are met.
    *   The commit message and the code changes are perfectly aligned. The commit message states that it fixes an ""unreplayble error"", and the code changes remove the unconditional exception that caused this error.

**Conclusion:**

The code diff clearly shows a bug fix. The original code always threw an exception, preventing the intended functionality. The modified code corrects this by performing the read operation after checking for sufficient readable and writable bytes. Therefore, the commit message accurately reflects the bug fix.

**Buggy**
"
netty,9557.json,97d07253e2933dbca0d08cd19072503506bda863,"@@ -1,12 +1,12 @@
     public static SpdySessionStatus valueOf(int code) {
         switch (code) {
         case 0:
             return OK;
         case 1:
             return PROTOCOL_ERROR;
-        case 11:
+        case 2:
             return INTERNAL_ERROR;
         }
 
         return new SpdySessionStatus(code, ""UNKNOWN ("" + code + ')');
     }",Buggy,"SPDY: fixed session status (internal error misdocumented)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""SPDY: fixed session status (internal error misdocumented)"" clearly states that the change is related to correcting a misdocumentation issue with the `INTERNAL_ERROR` session status in the SPDY protocol.

2.  **Git Diff:** The diff shows a change in the `valueOf(int code)` method of the `SpdySessionStatus` enum. Specifically, the case for code `11` is changed to code `2` for the `INTERNAL_ERROR` status.

3.  **Alignment:** The commit message and the code change are perfectly aligned. The code change directly addresses the misdocumentation mentioned in the commit message. It appears the internal error code was incorrectly documented as 11, and this commit corrects it to 2.

4.  **Bug Fix Indication:** This change is a bug fix because it corrects an incorrect value that could lead to incorrect interpretation of SPDY session status codes. Misdocumenting an error code is a bug.

**Conclusion:**

**Buggy**
"
argouml,1941.json,045f33e22637950982cc1193584f80301860683e,"@@ -1,7 +1,7 @@
     public void notationTextChanged(NotationTextEvent event) {
         this.setText(event.getText());
         this.setUnderline(event.isUnderlined());
-        this.setUnderline(event.isBold());
-        this.setItalic(event.isBold());
+        this.setBold(event.isBold());
+        this.setItalic(event.isItalic());
         this.damage();
     }",Buggy,"Fix some cut and paste errors

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19262 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message ""Fix some cut and paste errors"" suggests that the commit aims to correct mistakes likely introduced during copy-pasting code.

The diff shows changes in the `notationTextChanged` method. Specifically, the lines `this.setUnderline(event.isBold());` and `this.setItalic(event.isBold());` are modified. The first line is changed to `this.setBold(event.isBold());`, and the second line is changed to `this.setItalic(event.isItalic());`.

The original code incorrectly used `event.isBold()` to set both underline and italic properties. The corrected code now uses `event.isBold()` for setting the bold property and `event.isItalic()` for setting the italic property, which is the intended behavior. This aligns with the commit message indicating a correction of copy-paste errors. The original code's incorrect assignment of bold status to underline and italic properties represents a bug.

**Buggy**"
argouml,1718.json,126f591fd6961fb320d035184050b65257130479,"@@ -1,6 +1,8 @@
     public boolean doesAccept(Object objectToAccept) {
         if (Model.getFacade().isAClassifier(objectToAccept)) {
             return true;
+        } else if (Model.getFacade().isAComment(objectToAccept)) {
+        	return true;
         }
         return false;
     }",Buggy,"Fixed a bug where comments could not be added in Seq2 diagrams.

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15998 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a fix for a bug where comments could not be added in Seq2 diagrams. The code diff shows a modification to the `doesAccept` method, specifically adding a condition to accept objects that are instances of `Comment`. This aligns perfectly with the commit message, as it directly addresses the reported bug by allowing comments to be accepted in the context where this method is used (presumably related to Seq2 diagrams). The addition of the `else if (Model.getFacade().isAComment(objectToAccept)) { return true; }` block clearly fixes the issue of comments not being accepted.

**Buggy**"
docx4j,8366.json,9f08fd2eac1d042d1fe3b2e70b2918f551ea677e,"@@ -1,50 +1,50 @@
     public static Document CxnSpToSVG(CxnSp cxnSp) {
     	
     	// Geometrical transforms
     	CTTransform2D xfrm = cxnSp.getSpPr().getXfrm();
     	Box b = new Box(xfrm.getOff().getX(), xfrm.getOff().getY(),
-    			xfrm.getExt().getCx(), xfrm.getExt().getCx() );
+    			xfrm.getExt().getCx(), xfrm.getExt().getCy() );
     	
     	if (xfrm.getRot()!=0) {
     		b.rotate(xfrm.getRot());
     	}
     	if (xfrm.isFlipH() ) {
     		b.flipH();
     	}
     	if (xfrm.isFlipV() ) {
     		b.flipV();
     	}
     	
     	// Convert from EMU to pixels
     	b.toPixels();
 
     	// Wrap in a div positioning it on the page
     	Document document = createDocument();
 		Element xhtmlDiv = document.createElement(""div"");
 		// Firefox needs the following; Chrome doesn't
 		xhtmlDiv.setAttribute(""style"", 
 				""position: absolute; width:100%; height:100%; left:0px; top:0px;"");		
 		Node n = document.appendChild(xhtmlDiv);
     	
     	// Convert the object itself to SVG
 		Svg svg = oFactory.createSvg();
     	Line line = oFactory.createLine();
     	svg.getSVGDescriptionClassOrSVGAnimationClassOrSVGStructureClass().add(line);
     	
     	line.setX1(b.getOffset().getXAsString() );
     	line.setY1(b.getOffset().getYAsString() );
     	
     	Point otherEnd = b.getOtherCorner();
     	
     	line.setX2( otherEnd.getXAsString() );
     	line.setY2( otherEnd.getYAsString() );
 
     	line.setStyle(""stroke:rgb(99,99,99)"");
     	// You can't see the line in Midori, unless you specify the color.
     	// width eg stroke-width:2 is optional
     	
     	Document d2 = XmlUtils.marshaltoW3CDomDocument(svg, jcSVG);   
     	XmlUtils.treeCopy(d2, n);
     	return document;
     	
     }",Buggy,"Fix typo bug.
",Buggy,"The commit message ""Fix typo bug"" suggests a correction of a typographical error that was causing a bug.

The code diff shows a change in the line:

```java
xfrm.getExt().getCx(), xfrm.getExt().getCx() );
```

to

```java
xfrm.getExt().getCx(), xfrm.getExt().getCy() );
```

It appears that `xfrm.getExt().getCx()` was mistakenly used twice, and it has been corrected to `xfrm.getExt().getCy()`. This is likely a bug fix, as using the same value for both width and height would lead to incorrect geometrical transformations. The commit message aligns with the code change, indicating a typo that resulted in a functional error.

**Buggy**"
docx4j,5289.json,e0638645ae07b75593d031844af2c84f5b7a4960,"@@ -1,50 +1,50 @@
 	public OpcPackage get() throws Docx4JException {
 		
 		long startTime = System.currentTimeMillis();				
 
 		// 1. Get [Content_Types].xml
 		ContentTypeManager ctm = new ContentTypeManager();
 		try {
 			InputStream is = partStore.loadPart(""[Content_Types].xml"");		
 			ctm.parseContentTypesFile(is);
 		} catch (Docx4JException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		} catch (NullPointerException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		}
 		
 		// .. now find the name of the main part
 		String partName = ""_rels/.rels"";
 		RelationshipsPart rp = getRelationshipsPartFromZip(null, partName);
 		if (rp==null) {
 			throw new Docx4JException(""_rels/.rels appears to be missing from this package!"");
 		}
 		
 		String mainPartName = PackageRelsUtil.getNameOfMainPart(rp);
 		String pkgContentType = ctm.getContentType(new PartName(""/"" + mainPartName));
 
 		// 2. Create a new Package; this'll return the appropriate subclass
 		OpcPackage p = ctm.createPackage(pkgContentType);
 		log.info(""Instantiated package of type "" + p.getClass().getName() );
 		p.setPartStore(partStore);
 
 		p.setRelationships(rp);
 		rp.setSourceP(p); //
 		
 		// 5. Now recursively 
 //		(i) create new Parts for each thing listed
 //		in the relationships
 //		(ii) add the new Part to the package
 //		(iii) cross the PartName off unusedZipEntries
 		addPartsFromRelationships(p, rp, ctm );
 
 		// 6.
 		registerCustomXmlDataStorageParts(p);
 		
-		partStore.finishLoad();
+		partStore.finishSave();
 		
 		long endTime = System.currentTimeMillis();
 		log.info(""package read;  elapsed time: "" + Math.round((endTime-startTime)) + "" ms"" );
 		 
 		 return p;
 	}",Buggy,"Fix compilation errors in some classes
",Buggy,"The commit message ""Fix compilation errors in some classes"" suggests that the changes are aimed at resolving compilation issues.

The provided diff shows a change from `partStore.finishLoad();` to `partStore.finishSave();`. This change doesn't seem directly related to fixing compilation errors. Compilation errors usually involve syntax errors, type mismatches, or unresolved references. The change in the diff appears to be a method name correction or a change in functionality related to how the `partStore` is handled. It's possible that the original `finishLoad()` method was causing a compilation error (e.g., if it didn't exist or had incorrect parameters), but without more context, it's difficult to confirm. However, the change does look like a bug fix.

Reasoning:
1. The commit message indicates a fix for compilation errors.
2. The diff shows a change in a method call, which could potentially resolve a compilation error if the original method was incorrect or missing.
3. The change from `finishLoad` to `finishSave` suggests a correction in the intended operation, implying a bug fix.

**Buggy**
"
docx4j,5988.json,e43c7ce6875ed0292eea114d1f12f5b9bed3e549,"@@ -1,30 +1,31 @@
 	public static Part newPartForContentType(String contentType, String partName)
 	throws InvalidFormatException, PartUnrecognisedException {
 		
-		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)) {
+		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)
+				|| contentType.equals(ContentTypes.PRESENTATIONML_TEMPLATE) ) {
 			return new MainPresentationPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE)) {
 			return new SlidePart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_MASTER)) {
 			return new SlideMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_LAYOUT)) {
 			return new SlideLayoutPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TABLE_STYLES)) {
 			return new TableStylesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_PRES_PROPS)) {
 			return new PresentationPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_VIEW_PROPS)) {
 			return new ViewPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TAGS)) {
 			return new TagsPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_HANDOUT_MASTER)) {
 			return new HandoutMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_MASTER)) {
 			return new NotesMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_SLIDE)) {
 			return new NotesSlidePart(new PartName(partName));
 		} else {
 			throw new PartUnrecognisedException(""No subclass found for "" 
 					+ partName + "" (content type '"" + contentType + ""')"");					
 		}
 	}",Buggy,"Arnaud Kleinpeter's patch of 10 April, fixing error when trying to open a .potx file (powerpoint
template).
",Buggy,"The commit message indicates a fix for an error encountered when opening a `.potx` file, which is a PowerPoint template. The code diff modifies the `newPartForContentType` method to include `ContentTypes.PRESENTATIONML_TEMPLATE` along with `ContentTypes.PRESENTATIONML_MAIN` when creating a `MainPresentationPart`. This suggests that the original code was not correctly handling `.potx` files, leading to an error. The addition of the template content type resolves this issue.

Therefore, the changes indicate a bug fix.

**Buggy**"
docx4j,6677.json,e7a3747180c1121e45343a31225052ae7dc68b01,"@@ -1,7 +1,6 @@
 	public Node toNode(AbstractWmlConversionContext context, Model model, TransformState state, Document doc) throws TransformerException {
 	HyperlinkModel hyperlinkModel = (HyperlinkModel)model;
 	Node ret = null;
 		ret = toNode(context, hyperlinkModel, doc);
-		XmlUtils.treeCopy(hyperlinkModel.getContent().getChildNodes(), ret);
 		return ret;
 	}",Buggy,"Fixed several problems converting docx to pdf

Fixes:
- removed treeCopy from AbstractHyperlinkWriter.java to prevent double
links creation;
- added creation of TblPr to prevent NPE and moving of P created for
<caption> tag in nestedTableHierarchyFix() method in XHTMLimporter.java.
So if one generates docx from html and then pdf from this docx
everything will work.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message clearly states ""Fixed several problems converting docx to pdf."" It then lists two specific fixes:

1.  **Double links creation:**  This indicates a bug where hyperlinks were being created twice. The fix involves removing `treeCopy` from `AbstractHyperlinkWriter.java`.
2.  **NPE and moving of P tag:** This points to a bug in `XHTMLimporter.java` related to handling table properties (`TblPr`) and caption tags within nested tables. The fix involves adding the creation of `TblPr` to prevent a NullPointerException (NPE) and incorrect placement of paragraph tags.

The commit message strongly suggests that these changes are bug fixes.

**Git Diff Analysis:**

The provided Git diff shows a change in `AbstractHyperlinkWriter.java`. Specifically, the line `XmlUtils.treeCopy(hyperlinkModel.getContent().getChildNodes(), ret);` has been removed.

**Reasoning:**

The commit message states that removing `treeCopy` from `AbstractHyperlinkWriter.java` prevents double link creation. The Git diff confirms that this line has indeed been removed. This aligns perfectly with the commit message's description of the fix. The removal of `treeCopy` likely resolves a bug where the content of the hyperlink was being copied twice, leading to duplicate links.

**Conclusion:**

The commit message explicitly states that the changes are bug fixes, and the provided Git diff aligns with the description of one of the fixes. Therefore, the changes indicate a bug fix.

**Buggy**
"
docx4j,5744.json,ec6c723c0edc1ad95aa484b4da21adf68c9e5f49,"@@ -1,27 +1,27 @@
 	public void registerInContentTypeManager() {
 		
 		ContentTypeManager ctm = this.getPackage().getContentTypeManager(); 
 		if (type.equals(AltChunkType.Xhtml) ) {
 			ctm.addDefaultContentType(""xhtml"", ""application/xhtml+xml"");
 		} else if (type.equals(AltChunkType.Mht) ) {
-			ctm.addDefaultContentType(""mht"", ""multipart/related"");
+			ctm.addDefaultContentType(""mht"", ""message/rfc822"");
 		} else if (type.equals(AltChunkType.Xml) ) {
 			ctm.addDefaultContentType(""xml"", ""application/xml"");
 		} else if (type.equals(AltChunkType.TextPlain) ) {
 			ctm.addDefaultContentType(""txt"", ""text/plain"");
 		} else if (type.equals(AltChunkType.WordprocessingML) ) { //Docx
 			// In case we're being added to a docm/dotx/dotm
 			ctm.addDefaultContentType(""docx"", ContentTypes.WORDPROCESSINGML_DOCUMENT);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabled) ) {
 			ctm.addDefaultContentType(""docm"", ContentTypes.WORDPROCESSINGML_DOCUMENT_MACROENABLED);
 		} else if (type.equals(AltChunkType.OfficeWordTemplate) ) {
 			ctm.addDefaultContentType(""dotx"", ContentTypes.WORDPROCESSINGML_TEMPLATE);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabledTemplate) ) {
 			ctm.addDefaultContentType(""dotm"", ContentTypes.WORDPROCESSINGML_TEMPLATE_MACROENABLED);
 		} else if (type.equals(AltChunkType.Rtf) ) {
 			ctm.addDefaultContentType(""rtf"", ""text/rtf"");
 		} else if (type.equals(AltChunkType.Html) ) {
 			ctm.addDefaultContentType(""html"", ""text/html"");
 		}
 		
 	}",Buggy,"Bug fix: Change content type for mht to ""message/rfc822"" (Zoltan Luspai)
",Buggy,"The commit message indicates a bug fix related to the content type registration for MHT files. The diff shows a change in the `addDefaultContentType` method where the content type for ""mht"" is changed from ""multipart/related"" to ""message/rfc822"". This change directly addresses the bug described in the commit message. It's a correction of an incorrect content type, which is a bug fix.

**Buggy**"
docx4j,6106.json,e89bdbce6981005eb9b5b3c2c5893d0f1561ac1f,"@@ -1,20 +1,25 @@
 	public static String getNameOfMainPart(RelationshipsPart packageRels) throws Docx4JException  {
 		
 		// find rel of type officeDocument
 		for (Relationship rel : packageRels.getRelationships().getRelationship() ) {
 			
 			if (rel.getType().equals(
 					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument"") ) {
 				return rel.getTarget();
 			} 
 			else if (rel.getType().equals(
 					""http://schemas.microsoft.com/office/2006/relationships/graphicFrameDoc"") ) {
 				// v:shape/@o:gfxdata
 				return rel.getTarget();
 			} 
+			else if (rel.getType().equals(
+					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/diagramLayout"") ) {
+				// Glox
+				return rel.getTarget();
+			} 
 //			else {
 //				System.out.println(rel.getType());
 //			}
 		}
 		throw new Docx4JException(""No relationship of type officeDocument"");
 	}",Buggy,"Bug fix: make ParseGlox sample work.
",Buggy,"The commit message indicates a bug fix to make the ParseGlox sample work. The diff adds a new condition to the `getNameOfMainPart` method to handle relationships of type ""http://schemas.openxmlformats.org/officeDocument/2006/relationships/diagramLayout"". This suggests that the original code was not correctly processing Glox files, and the added condition fixes this issue. The comment ""// Glox"" further confirms that this change is related to handling Glox files. This indicates that the code was not handling a specific type of relationship, which caused the ParseGlox sample to fail. The added `else if` condition addresses this issue, which aligns with the commit message indicating a bug fix.

**Buggy**
"
docx4j,5455.json,8d2fa40ea092e0c98a6e8648a80184c0dfe88ed7,"@@ -1,62 +1,68 @@
 	public void removePart(PartName partName) {
 		
 		log.info(""trying to removePart "" + partName.getName() );
 		
 		if (partName == null)
 			throw new IllegalArgumentException(""partName was null"");
 		
 		Part part = getPackage().getParts().get(partName);
 		
 		if (part!=null) {
 
 			// Remove the relationship for which it is a target from here
 			// Throw an error if this can't be found!
 			Relationship relToBeRemoved = null;
 //			for (Relationship rel : relationshipsByID.values() ) {
 			for (Relationship rel : relationships.getRelationship() ) {
+				
+				if (rel.getTargetMode() !=null
+						&& rel.getTargetMode().equals(""External"") ) {
+					// This method can't be used to remove external resources
+					continue;
+				}
 								
 				URI resolvedTargetURI = null;
 
 				try {
 					resolvedTargetURI = org.docx4j.openpackaging.URIHelper
 							.resolvePartUri(sourceP.partName.getURI(), new URI(
 									rel.getTarget()));
 				} catch (URISyntaxException e) {
 					log.error(""Cannot convert "" + rel.getTarget()
 							+ "" in a valid relationship URI-> ignored"", e);
 				}		
 
 				log.debug(""Comparing "" + resolvedTargetURI + "" == "" + partName.getName());
 				
 				if (partName.getName().equals(resolvedTargetURI.toString()) ) { // was rel.getTargetURI()
 					
 					log.info(""True - will delete relationship with target "" + rel.getTarget());
 					relToBeRemoved = rel; // Avoid java.util.ConcurrentModificationException
 					break;
 				}
 				
 			}
 			if (relToBeRemoved==null) {
 				// The Part may be in the package somewhere, but its not
 				// a target of this relationships part!
 				throw new IllegalArgumentException(partName + "" is not a target of "" + this.partName );
 			} else {
 				removeRelationship(relToBeRemoved);				
 			}
 						
 			// Remove parts it references
 			if (part.getRelationshipsPart()!=null) {
 				part.getRelationshipsPart().removeParts();
 				
 				// part.setRelationships(null);  // Unnecessary
 			}			
 
 			// Remove from Content Type Manager
 				// TODO			
 			
 			// Delete the specified part from the package.
 			getPackage().getParts().remove(partName);						
 		}
 
 //		this.isDirty = true;
 	}",Buggy,"Bug fix in removePart - ignore external resources.
",Buggy,"The commit message indicates a bug fix related to the `removePart` method, specifically mentioning that external resources should be ignored during the removal process.

The code diff introduces a check within the loop that iterates through relationships. It checks if the target mode of a relationship is ""External"". If it is, the code skips to the next relationship using `continue`. This prevents the method from attempting to remove external resources, aligning with the commit message.

The addition of the `if (rel.getTargetMode() !=null && rel.getTargetMode().equals(""External"") )` block suggests that the original code was incorrectly attempting to remove external resources, which is a bug. The added condition fixes this by skipping external resources.

Therefore, the changes indicate a bug fix.

**Buggy**"
docx4j,3230.json,71277942a6f5c967ca96c8454953e8c6697dffbb,"@@ -1,9 +1,14 @@
 	public void setXslFO(Element foElement) {
 		
 		if (((Color)this.getObject()).getVal()!=null ) {
-			foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			if (((Color)this.getObject()).getVal().equals(""auto"")) {
+				// set it to black
+				foElement.setAttribute(FO_NAME, ""black"");				
+			} else {
+				foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			}
 		} else {
 			//
 		}
 
 	}",Buggy,"bug fix: XSL FO doesn't like font color #auto
",Buggy,"The commit message indicates a bug fix related to how XSL FO handles the font color `#auto`. The code diff shows that when the color value is ""auto"", it's now explicitly set to ""black"" instead of passing `#auto` directly to the XSL FO. This suggests that the original behavior of using `#auto` was causing an issue, which is now being addressed by substituting it with ""black"".

Reasoning:

1.  **Commit Message Analysis:** The phrase ""bug fix"" clearly indicates that the commit aims to resolve a bug. The message specifies that XSL FO doesn't handle the font color `#auto` correctly.
2.  **Code Diff Analysis:**
    *   The code adds a conditional check: `if (((Color)this.getObject()).getVal().equals(""auto""))`.
    *   If the color value is ""auto"", the code now sets the `FO_NAME` attribute to ""black"": `foElement.setAttribute(FO_NAME, ""black"");`.
    *   Otherwise, the original behavior of setting the attribute to `""#""+((Color)this.getObject()).getVal()` is preserved.
3.  **Relevance:** The code change directly addresses the issue described in the commit message. It handles the specific case where the color value is ""auto"", implying that XSL FO had problems with this value.

Conclusion:
**Buggy**"
docx4j,18108.json,b7d10c89810f8fb9ba39216b7630f82f493bfdee,"@@ -1,126 +1,125 @@
 	public static void main(String[] args) throws Docx4JException {
 		
 		// Input file
 		String inputfilepath = System.getProperty(""user.dir"") + ""/sample-docs/pptx/pptx-chart.pptx"";
 		
 		// The names of the parts which will be edited
 		// Alter these to match what is in your input pptx
 		// .. the chart
 		String chartPartName = ""/ppt/charts/chart1.xml"";
 		// .. the xlsx
 		String xlsPartName = ""/ppt/embeddings/Microsoft_Excel_Sheet1.xlsx"";
-//		String xlsPartName = ""/ppt/embeddings/Microsoft_Office_Excel_Worksheet1.xlsx"";
 		
 		// Output file
 		String outputfilepath = System.getProperty(""user.dir"") 
 				+ ""/OUT_EditEmbeddedCharts-"" 
 				+ System.currentTimeMillis() + "".pptx"";
 		
 		// Values to change
 		Random rand = new Random();
 
 		String firstValue  = String.valueOf(rand.nextInt(99));
 		String secondValue = String.valueOf(rand.nextInt(99));
 		
 		// Open the PPT template file
 		PresentationMLPackage ppt = (PresentationMLPackage) OpcPackage
 			.load(new java.io.File(inputfilepath));
 
 		/*
 		 * Get the Chart object and update the values. Afterwards, we'll update 
 		 * the associated spreadsheet so that the data is synchronized.
 		 */
 		Chart chart = (Chart) ppt.getParts().get(new PartName(chartPartName));
 		
 		List<Object> objects = chart.getJaxbElement().getChart().getPlotArea()
 				.getAreaChartOrArea3DChartOrLineChart();
 		
 		for (Object object : objects) {
 			
 			if (object instanceof CTBarChart) {
 
 				List<CTBarSer> ctBarSers = ((CTBarChart) object).getSer();
 				
 				for (CTBarSer ctBarSer : ctBarSers)
 				{
 					List<CTNumVal> ctNumVals = ctBarSer.getVal().getNumRef().getNumCache().getPt();
 					for (CTNumVal ctNumVal : ctNumVals)
 					{
 						System.out.println(""ctNumVal Val BEFORE: "" + ctNumVal.getV());
 						if (ctNumVal.getIdx() == 0) {
 							ctNumVal.setV(firstValue);
 						}
 						else if (ctNumVal.getIdx() == 1) {
 							ctNumVal.setV(secondValue);	
 						}
 						System.out.println(""ctNumVal Val AFTER: "" + ctNumVal.getV());
 					}
 				}
 			}
 		}
 				
 		/*
 		 * Get the spreadsheet and find the cell values that need to be updated
 		 */
 		
 		EmbeddedPackagePart epp  = (EmbeddedPackagePart) ppt
 			.getParts().get(new PartName(xlsPartName));
 		
 		if (epp==null) {
 			throw new Docx4JException(""Could find EmbeddedPackagePart: "" + xlsPartName);
 		}
 		
 		InputStream is = BufferUtil.newInputStream(epp.getBuffer());
 		
 		SpreadsheetMLPackage spreadSheet = (SpreadsheetMLPackage) SpreadsheetMLPackage.load(is);
 
 		Map<PartName,Part> partsMap = spreadSheet.getParts().getParts();		 
 		Iterator<Entry<PartName, Part>> it = partsMap.entrySet().iterator();
 
 		while(it.hasNext()) {
 			Map.Entry<PartName, Part> pairs = it.next();
 			
 			if (partsMap.get(pairs.getKey()) instanceof WorksheetPart) {
 				
 				WorksheetPart wsp = (WorksheetPart) partsMap.get(pairs.getKey()) ;
 				
 				List<Row> rows = wsp.getJaxbElement().getSheetData().getRow();
 
 				for (Row row : rows) {
 					List<Cell> cells = row.getC();
 					for (Cell cell : cells)
 					{
 						if (cell.getR().equals(""B2"") && cell.getV() != null) {
 							System.out.println(""B2 CELL VAL: "" + cell.getV());
 							// change the B2 cell value
+							cell.setT(STCellType.STR);
 							cell.setV(firstValue);
 						}
 						else if (cell.getR().equals(""B3"") && cell.getV() != null) {
 							System.out.println(""B3 CELL VAL: "" + cell.getV());
 							// Change the B3 cell value
-							cell.setV(secondValue); 
+							cell.setT(STCellType.STR);
+							cell.setV(secondValue);
 						}
 					}					
 				}
 			}
 		}
 
 		/*
 		 * Convert the Spreadsheet to a binary format, set it on the 
 		 * EmbeddedPackagePart, add it back onto the deck and save to a file.
 		 *  
 		 */		
 		ByteArrayOutputStream baos = new ByteArrayOutputStream();
 		
 		SaveToZipFile saver = new SaveToZipFile(spreadSheet);
 
 		saver.save(baos);
 		epp.setBinaryData(baos.toByteArray());
 
-		ppt.addTargetPart(epp);
-
 		// Write the new file to disk
 		ppt.save(new java.io.File(outputfilepath));
 
 		System.out.println(""\n\n done .. saved "" + outputfilepath);
 	}",Buggy,EditEmbeddedCharts - bug fixes,Buggy,"The commit message ""EditEmbeddedCharts - bug fixes"" suggests that the changes in the diff are related to fixing bugs in the `EditEmbeddedCharts` functionality.

The diff modifies the `EditEmbeddedCharts` class, specifically focusing on updating values within an embedded chart in a PowerPoint presentation. The code retrieves a chart and its associated spreadsheet, updates specific cell values in the spreadsheet, and synchronizes these changes with the chart.

The key changes are the addition of `cell.setT(STCellType.STR);` before setting the cell value. This suggests that the cell type was not being correctly set, leading to potential issues when updating the cell values. This correction directly addresses a potential bug related to data type handling.

Therefore, the changes indicate a bug fix.

**Buggy**"
docx4j,5328.json,2b240431a940e8529960d06423db48c7122090ec,"@@ -1,67 +1,70 @@
 	public void addPartsFromRelationships(ZipOutputStream out,  RelationshipsPart rp )
 	 throws Docx4JException {
 		
 //		for (Iterator it = rp.iterator(); it.hasNext(); ) {
 //			Relationship r = (Relationship)it.next();
 //			log.info(""For Relationship Id="" + r.getId() + "" Source is "" + r.getSource().getPartName() + "", Target is "" + r.getTargetURI() );
 		for ( Relationship r : rp.getRelationships().getRelationship() ) {
 			
 			log.debug(""For Relationship Id="" + r.getId() 
 					+ "" Source is "" + rp.getSourceP().getPartName() 
 					+ "", Target is "" + r.getTarget() );
 			
-//			if (!r.getTargetMode().equals(TargetMode.INTERNAL) ) {
+			if (r.getType().equals(Namespaces.HYPERLINK)) {				
+				continue;  // whether internal or external								
+			}
+			
 			if (r.getTargetMode() != null
 					&& r.getTargetMode().equals(""External"") ) {
 				
 				// ie its EXTERNAL
 				// As at 1 May 2008, we don't have a Part for these;
 				// there is just the relationship.
 
 				log.warn(""Encountered external resource "" + r.getTarget() 
 						   + "" of type "" + r.getType() );
 				
 				// So
 				continue;				
 			}
 			
 			try {
 				//String resolvedPartUri = URIHelper.resolvePartUri(r.getSourceURI(), r.getTargetURI() ).toString();
 
 				String resolvedPartUri = URIHelper.resolvePartUri(rp.getSourceURI(), new URI(r.getTarget() ) ).toString();		
 				
 				// Now drop leading ""/'
 				resolvedPartUri = resolvedPartUri.substring(1);				
 				
 				// Now normalise it .. ie abc/def/../ghi
 				// becomes abc/ghi
 				// Maybe this isn't necessary with a zip file,
 				// - ZipFile class may be smart enough to do it.
 				// But it is certainly necessary in the JCR case.
 //				target = (new java.net.URI(target)).normalize().toString();
 //				log.info(""Normalised, it is "" + target );				
 				
 //				Document contents = getDocumentFromZippedPart( zf,  target);
 				
 				if (!false) {
 					log.debug(""Getting part /"" + resolvedPartUri );
 					
 					Part part = p.getParts().get(new PartName(""/"" + resolvedPartUri));
 					
 					if (part==null) {
 						log.error(""Part "" + resolvedPartUri + "" not found!"");
 					} else {
 						log.debug(part.getClass().getName() );
 					}
 					
 					savePart(out, part);
 					
 				}
 					
 			} catch (Exception e) {
 				throw new Docx4JException(""Failed to add parts from relationships"", e);				
 			}
 		}
 		
 		
 	}",Buggy,"Bug fix: save file containing internal hyperlink rel
",Buggy,"The code diff focuses on handling relationships within a document, specifically how parts are added based on these relationships. The commit message indicates a bug fix related to saving files containing internal hyperlink relationships.

Here's a breakdown of the code changes and their relation to the commit message:

1.  **Hyperlink Handling:** The added code `if (r.getType().equals(Namespaces.HYPERLINK)) { continue; }` suggests that the previous code was incorrectly processing or failing to process hyperlink relationships. This change skips hyperlink relationships, implying a fix for a bug where these relationships were causing issues. The comment `// whether internal or external` clarifies that both internal and external hyperlinks are now skipped.

2.  **External Resource Handling:** The code already had logic to handle external resources, logging a warning and continuing to the next relationship.

3.  **Part URI Resolution:** The code resolves the target URI of a relationship to a part URI.

4.  **Part Retrieval and Saving:** The code retrieves a `Part` based on the resolved URI and saves it. Error logging is present if the part is not found.

Reasoning:

The key change is the addition of the hyperlink handling logic. The commit message explicitly mentions ""internal hyperlink rel,"" and the code now explicitly skips hyperlink relationships. This strongly suggests that the previous code had a bug related to how it processed (or failed to process) internal hyperlink relationships, and the current change is a fix for that bug. Without this change, the application might have crashed or produced corrupted files when encountering internal hyperlinks.

Conclusion:

**Buggy**
"
docx4j,6285.json,712217fb5f02ee12b18281bf0abe099e8e2cfe55,"@@ -1,32 +1,29 @@
 	private RelationshipsPart getRelationshipsPartFromXmlPackage(Base p, String partName) 
 			throws Docx4JException {
 		
-		RelationshipsPart thePart = null;
+		RelationshipsPart rp = null;
 		
 		try {
 			
 			org.docx4j.xmlPackage.Part part = parts.get(partName);
 			
 			if (part == null) {
 				return null;
 			}
 			
 			org.w3c.dom.Element el = part.getXmlData().getAny();
 			
-			RelationshipsPart rp = new RelationshipsPart(new PartName(partName) );
+			rp = new RelationshipsPart(new PartName(partName) );
 			// PartName already starts with a '/', so no need to add it
 			rp.setSourceP(p);
-			rp.unmarshal(el);
 			
-//			// Convert it to a Dom4J element
-//			thePart = new RelationshipsPart( p, new PartName( partName), convertW3CtoDom4J(el) );
-//				// PartName already starts with a '/', so no need to add it
-			
+			rp.setRelationships( (Relationships)rp.unmarshal(el) );
+						
 		} catch (Exception e) {
 			e.printStackTrace();
 			throw new Docx4JException(""Error getting document from XmlPackage:"" + partName, e);
 			
 		} 
 		
-		return thePart;
+		return rp;
 	}",Buggy,"bug fix in getRelationshipsPartFromXmlPackage
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""bug fix in getRelationshipsPartFromXmlPackage"" explicitly states that the change is intended to fix a bug within the `getRelationshipsPartFromXmlPackage` method.

2.  **Code Diff Analysis:**
    *   The code initializes `rp` to null.
    *   The code retrieves a part from `parts` using `partName`. If the part is null, the method returns null.
    *   The code retrieves the XML data from the part.
    *   The code creates a new `RelationshipsPart` object.
    *   The code sets the source part.
    *   The original code had commented-out lines related to Dom4J conversion, which are now removed.
    *   The code now unmarshals the XML element into the `RelationshipsPart` and sets the relationships.
    *   The code includes a try-catch block to handle exceptions during the process, throwing a `Docx4JException` if an error occurs.
    *   Finally, the method returns the `RelationshipsPart` object.

3.  **Bug Fix Indicators:**
    *   The original code had commented-out lines, suggesting a previous attempt to handle the XML data. The new code directly unmarshals the XML element into the `RelationshipsPart` and sets the relationships. This indicates a correction in how the XML data is processed.
    *   The change `rp.setRelationships( (Relationships)rp.unmarshal(el) );` suggests that the unmarshalled data was not being properly assigned to the `RelationshipsPart` object before. This could lead to incorrect or incomplete data being returned, which constitutes a bug.
    *   The try-catch block was already present, so no changes were made to the error handling.

**Conclusion:**

The commit message explicitly states a bug fix, and the code diff shows a correction in how the XML data is processed and assigned to the `RelationshipsPart` object. The change ensures that the unmarshalled data is properly associated with the `RelationshipsPart`, which likely resolves an issue where the relationships were not being correctly populated.

**Buggy**
"
mockito,494.json,881c1d9cace557e64641d9b104c59b2b0c86ee4c,"@@ -1,8 +1,7 @@
     public static IMockitoConfiguration getConfig() {
         if (!initialized) {
-            //TODO check email of mockito group
             throw new IllegalStateException(""Something went wrong. GlobalConfiguration should be initialised by now.\n"" +
                 ""Please report issue at http://mockito.org or write an email to mockito@googlegroups.com"");
         }
         return globalConfiguration;
     }",Buggy,"removed TODOs,
fixed the minor problem of breaking the invocation string when no args given

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401161
",Buggy,"The commit message indicates the removal of TODOs and a fix for a minor problem related to invocation strings when no arguments are given.

The provided code diff shows the removal of a TODO comment within the `getConfig()` method. This aligns with the ""removed TODOs"" part of the commit message. The diff doesn't show any changes related to invocation strings or argument handling. However, the commit message mentions fixing a ""minor problem of breaking the invocation string when no args given"". Since the provided diff only shows the removal of a TODO, it's possible that the fix for the invocation string issue is in a different part of the codebase not shown in the diff.

The removal of the TODO is a cleanup task, and the commit message also mentions a bug fix.

**Buggy**
"
mongo-java-driver,3587.json,e53bde1953e3e4d12809ce6c27c69a1eb8a4b8dd,"@@ -1,3 +1,4 @@
     public Binary readBinaryData(final String name) {
-        return readBinaryData(name);
+        verifyName(name);
+        return readBinaryData();
     }",Buggy,"fixed infinite recursion error: test forthcoming
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fixed infinite recursion error: test forthcoming"" clearly states that the commit addresses an infinite recursion error. The ""test forthcoming"" part suggests that a test case will be added to prevent regressions of this bug.

**Git Diff Analysis:**

The diff shows a change in the `readBinaryData` method.  The original code had:

```java
return readBinaryData(name);
```

This line is calling the same method `readBinaryData(name)` with the same argument `name`, which will indeed lead to infinite recursion.

The corrected code is:

```java
verifyName(name);
return readBinaryData();
```

Here's the breakdown of the changes:

1.  `verifyName(name);`: This line likely performs some validation on the input `name`. This is a good practice to prevent unexpected behavior or errors.
2.  `return readBinaryData();`: This line calls a different `readBinaryData` method, presumably one that takes no arguments. This resolves the infinite recursion issue.

**Reasoning:**

The commit message explicitly mentions fixing an infinite recursion error. The code diff directly addresses this issue by changing the recursive call to a different method call. The addition of `verifyName(name)` suggests an attempt to prevent similar issues in the future by validating input.

**Conclusion:**

The commit message and the code diff are perfectly aligned. The code change clearly fixes the infinite recursion error described in the commit message.

**Buggy**
"
mongo-java-driver,2935.json,6fa5b4b1073a1df6d2a70d2fef360622bae6fbea,"@@ -1,14 +1,15 @@
     public void apply(final ClassModelBuilder<?> classModelBuilder) {
         for (PropertyModelBuilder<?> propertyModelBuilder : classModelBuilder.getPropertyModelBuilders()) {
             if (!(propertyModelBuilder.getPropertyAccessor() instanceof PropertyAccessorImpl)) {
                 throw new CodecConfigurationException(format(""The SET_PRIVATE_FIELDS_CONVENTION is not compatible with ""
                         + ""propertyModelBuilder instance that have custom implementations of org.bson.codecs.pojo.PropertyAccessor: %s"",
                         propertyModelBuilder.getPropertyAccessor().getClass().getName()));
             }
             PropertyAccessorImpl<?> defaultAccessor = (PropertyAccessorImpl<?>) propertyModelBuilder.getPropertyAccessor();
             PropertyMetadata<?> propertyMetaData = defaultAccessor.getPropertyMetadata();
-            if (!propertyMetaData.isDeserializable() && isPrivate(propertyMetaData.getField().getModifiers())) {
+            if (!propertyMetaData.isDeserializable() && propertyMetaData.getField() != null
+                    && isPrivate(propertyMetaData.getField().getModifiers())) {
                 setPropertyAccessor(propertyModelBuilder);
             }
         }
     }",Buggy,"Fix NPE error with ConventionSetPrivateField

JAVA-2951
",Buggy,"Okay, let's analyze the provided commit message and Java diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Fix NPE error with ConventionSetPrivateField"" strongly suggests that the commit addresses a NullPointerException (NPE). The ""JAVA-2951"" likely refers to a specific issue tracker ticket.

2.  **Code Diff:**
    *   The core change is the addition of a null check: `propertyMetaData.getField() != null` within the `if` condition.
    *   The original code directly accessed `propertyMetaData.getField().getModifiers()` without ensuring that `propertyMetaData.getField()` was not null. This could lead to an NPE if `propertyMetaData.getField()` was indeed null.
    *   The added null check prevents the code from attempting to dereference a null `getField()`, thus preventing the NPE.

3.  **Relevance:** The code change directly addresses the issue described in the commit message. The added null check is a standard way to prevent NPEs. The commit message and code modification are highly relevant to each other.

4.  **Bug Fix Indicators:**
    *   The commit message explicitly states ""Fix NPE error"".
    *   The code change introduces a null check to prevent a potential NPE.

**Conclusion:**

The commit message and code diff clearly indicate a bug fix related to a potential NullPointerException. The added null check is a direct and common way to address such issues.

**Buggy**
"
mongo-java-driver,3321.json,9d17beae8f95df45c855e8284e96a6ddbdca07b7,"@@ -1,26 +1,28 @@
     public static UUID decodeBinaryToUuid(final byte[] data, final byte type, final UuidRepresentation uuidRepresentation) {
         if (data.length != 16) {
             throw new BsonSerializationException(String.format(""Expected length to be 16, not %d."", data.length));
         }
 
         if (type == BsonBinarySubType.UUID_LEGACY.getValue()) {
             switch(uuidRepresentation) {
                 case C_SHARP_LEGACY:
                     reverseByteArray(data, 0, 4);
                     reverseByteArray(data, 4, 2);
                     reverseByteArray(data, 6, 2);
                     break;
                 case JAVA_LEGACY:
                     reverseByteArray(data, 0, 8);
                     reverseByteArray(data, 8, 8);
                     break;
                 case PYTHON_LEGACY:
-                case STANDARD:
                     break;
+                case STANDARD:
+                    throw new BSONException(""Can not decode a subtype 3 (UUID legacy) BSON binary when the decoder is configured to use "" +
+                            ""the standard UUID representation"");
                 default:
                     throw new BSONException(""Unexpected UUID representation"");
             }
         }
 
         return new UUID(readLongFromArrayBigEndian(data, 0), readLongFromArrayBigEndian(data, 8));
     }",Buggy,"Fix UUID decoder bug

The UUID decoder now does NOT allow decoding of a subtype 3 (legacy UUID) BSON Binary value
when the UUID decoder's UUID representation is STANDARD.
",Buggy,"The commit message indicates a bug fix related to UUID decoding, specifically addressing the handling of subtype 3 (legacy UUID) BSON Binary values when the UUID decoder is set to use the STANDARD representation.

The code diff shows that when the `type` is `BsonBinarySubType.UUID_LEGACY` and the `uuidRepresentation` is `STANDARD`, a `BSONException` is now thrown. This confirms that the previous code allowed this scenario, which was incorrect and is now being fixed by throwing an exception, thus preventing the incorrect decoding.

Therefore, the changes indicate a bug fix.

**Buggy**"
mongo-java-driver,2112.json,381a79da3eab8c3d819826af40b0ae39dd08ae08,"@@ -1,3 +1,3 @@
-    public void pipe(BSONReader reader) {
+    public void pipe(final BSONReader reader) {
         pipeDocument(reader);
     }",Buggy,"Fixed checkstyle errors
",NotBuggy,"The commit message ""Fixed checkstyle errors"" indicates a change to address code style issues. The provided diff shows a minor change: adding the `final` keyword to the `reader` parameter in the `pipe` method. This aligns with checkstyle rules that often encourage or require the use of `final` for method parameters when they are not modified within the method. This change does not indicate a bug fix, but rather a code style improvement.

**NotBuggy**"
mongo-java-driver,685.json,0a759943e1d5a8b971798db82e8a89330545e558,"@@ -1,8 +1,3 @@
     public GridFSInputFile createFile(final File file) throws IOException {
-        FileInputStream fileInputStream = new FileInputStream(file);
-        try {
-            return createFile(fileInputStream, file.getName(), true);
-        } finally {
-            fileInputStream.close();
-        }
+        return createFile(new FileInputStream(file), file.getName(), true);
     }",Buggy,"Fixed GridFS.createFile bug

The underlying call handles the closure of the stream correctly

JAVA-1813
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed GridFS.createFile bug"" clearly states that the commit is intended to fix a bug in the `GridFS.createFile` method. It also mentions ""The underlying call handles the closure of the stream correctly"" and a JIRA ticket ""JAVA-1813"", which suggests a resource management issue was present.

2.  **Code Diff:** The code diff shows a change in how the `FileInputStream` is handled within the `createFile` method.
    *   **Original Code:** The original code explicitly creates a `FileInputStream`, wraps the call to `createFile` in a `try...finally` block to ensure the stream is closed, even if an exception occurs.
    *   **Modified Code:** The modified code simplifies this by directly passing the `FileInputStream` to the `createFile` method. The `try...finally` block is removed.

3.  **Bug Fix Inference:** The commit message indicates that the underlying `createFile` method now correctly handles closing the stream. The original code's `try...finally` block was likely a workaround for a bug where the stream was not being closed properly in the underlying call. The removal of the `try...finally` block suggests that the underlying method has been updated to handle stream closure, thus fixing the bug.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes support this by simplifying stream handling, implying that the underlying method now correctly manages the stream lifecycle. Therefore, the changes indicate a bug fix.

**Buggy**"
junit5,2294.json,01eb2e4e3ba37aedc2c8f578fcc77a55ef305cd5,"@@ -1,3 +1,9 @@
 	private static String toString(Object obj) {
-		return (obj instanceof Class ? getCanonicalName((Class<?>) obj) : String.valueOf(obj));
+		if (obj instanceof Class) {
+			return getCanonicalName((Class<?>) obj);
+		}
+		if (obj instanceof Object[]) {
+			return Arrays.toString((Object[]) obj);
+		}
+		return StringUtils.nullSafeToString(obj);
 	}",Buggy,"Fix bug in AssertionUtils.toString(Object)

Prior to this commit, AssertionUtils.toString(Object) printed an array via
the array's toString() method which resulted in non-user-friendly output.

This commit addresses this issue by printing arrays using Arrays.toString(),
which produces human readable output.

This is a prerequisite for #961.

Issue: #1030
",Buggy,"Okay, let's analyze this Git commit.

The commit message clearly states that a bug existed in `AssertionUtils.toString(Object)` where arrays were not being printed in a user-friendly format. The original implementation used the `toString()` method of the array object itself, which doesn't produce human-readable output.

The commit message also mentions that the fix involves using `Arrays.toString()` to print arrays, which generates a more readable representation.

The provided diff confirms this. The code now includes a check to see if the object is an instance of `Object[]`. If it is, `Arrays.toString((Object[]) obj)` is used to convert the array to a string. Otherwise, `StringUtils.nullSafeToString(obj)` is used.

The changes directly address the bug described in the commit message. The code modification ensures that arrays are printed in a human-readable format, resolving the original issue.

Therefore, the changes indicate a bug fix.

**Buggy**"
Essentials,1046.json,880ec1b3d953f1aef72ac925fe524799e4df8898,"@@ -1,13 +1,13 @@
 	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
 	{
 		charge(sender);
 		sender.sendMessage(Util.format(""gcmax"", (Runtime.getRuntime().maxMemory() / 1024 / 1024)));
 		sender.sendMessage(Util.format(""gcmin"", (Runtime.getRuntime().freeMemory() / 1024 / 1024)));
 		for (World w : server.getWorlds())
 		{
 			sender.sendMessage(
 					(w.getEnvironment() == World.Environment.NETHER ? ""Nether"" : ""World"") + "" \"""" + w.getName() + ""\"": ""
 					+ w.getLoadedChunks().length + Util.i18n(""gcchunks"")
-					+ w.getEntities().size() + Util.i18n(""entities""));
+					+ w.getEntities().size() + Util.i18n(""gcentities""));
 		}
 	}",Buggy,"Fix translation error in gc

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1465 e251c2fe-e539-e718-e476-b85c1f46cddb
",Buggy,"The commit message ""Fix translation error in gc"" suggests that the commit aims to correct a translation issue within the 'gc' command or related functionality.

The provided diff shows a change in the `run` method, specifically within the loop that iterates through the server's worlds. The line:

```java
+					+ w.getEntities().size() + Util.i18n(""gcentities""));
```

is modified from:

```java
-					+ w.getEntities().size() + Util.i18n(""entities""));
```

The change replaces `Util.i18n(""entities"")` with `Util.i18n(""gcentities"")`. This indicates that the internationalization key used for displaying the number of entities was incorrect and has been corrected to a more specific key related to the 'gc' command. This aligns perfectly with the commit message's intention to fix a translation error.

Therefore, the changes indicate a bug fix.

**Buggy**"
Essentials,2593.json,4c78ab4f2a3eb4d598564069aa5a21e0380de29d,"@@ -1,28 +1,29 @@
 	private List<String> sort(List<String> permList) {
 		
 		List<String> result = new ArrayList<String>();
 		
 		for (String key : permList) {
 			String a = key.charAt(0) == '-'? key.substring(1):key;
 			Map<String, Boolean> allchildren = GroupManager.BukkitPermissions.getAllChildren(a, new HashSet<String>());
 			if (allchildren != null) {
 
 				ListIterator<String> itr = result.listIterator();
 				
 				while (itr.hasNext()){
 					String node = (String) itr.next();
 					String b = node.charAt(0) == '-'? node.substring(1):node;
 					
+					// Insert the parent node before the child
 					if (allchildren.containsKey(b)) {
-						itr.previous();
-						itr.add(key);
+						itr.set(key);
+						itr.add(node);
 						break;
 					}
 				}
 			}
 			if (!result.contains(key))
 				result.add(key);
 		}
 		
 		return result;
 	}",Buggy,fix for an iterator error if there is only one element in the array.,Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix for an iterator error if there is only one element in the array"" clearly suggests that the change addresses a bug related to iterator behavior when dealing with a single-element array or list. This strongly implies a bug fix.

**Code Diff Analysis:**

The code diff focuses on the `sort` method, which appears to be sorting a list of permission strings (`permList`). The core logic involves iterating through the `result` list (using a `ListIterator`) and comparing each element (`node`) with the children of the current permission string (`key`).

The key changes are within the `while (itr.hasNext())` loop:

1.  **Original Logic:**
    *   `itr.previous();`
    *   `itr.add(key);`

2.  **New Logic:**
    *   `itr.set(key);`
    *   `itr.add(node);`

The original logic attempted to move the iterator back one position (`itr.previous()`) and then insert the `key` at that position (`itr.add(key)`). This could lead to issues, especially if the iterator was already at the beginning of the list (i.e., when the list has only one element). Calling `previous()` on an iterator at the start of the list will cause an exception.

The new logic replaces the current element (`node`) with `key` (`itr.set(key)`) and then adds the original `node` after the newly inserted `key` (`itr.add(node)`). This approach avoids the potential `previous()` call at the beginning of the list and seems more robust.

**Reasoning:**

The commit message and the code diff are strongly aligned. The change in iterator logic directly addresses a potential issue when the `result` list has only one element, which could cause an iterator error (likely a `NoSuchElementException` or similar). The original code's attempt to move the iterator backward before adding a new element is a common source of errors when dealing with iterators, especially at the boundaries of the list. The modified code replaces the current element and then adds the original element, which is a safer and more logical approach.

**Conclusion:**

The changes clearly indicate a bug fix related to iterator handling, specifically when dealing with a list with a single element. The commit message accurately describes the problem being addressed.

**Buggy**
"
Essentials,1458.json,4823712f47bfb7f64e22b8633b05e3f6801919c5,"@@ -1,13 +1,16 @@
 		public void onEntityDamageByEntity(final EntityDamageByEntityEvent event)
 		{
 			if (event.getCause() != DamageCause.ENTITY_ATTACK || event.getEntity().getType() != EntityType.PLAYER)
 			{
 				return;
 			}
 			final Entity damager = event.getDamager();
-			final User user = ess.getUser(damager);
-			if (user.isJailed())
+			if (damager.getType() == EntityType.PLAYER)
 			{
-				event.setCancelled(true);
+				final User user = ess.getUser(damager);
+				if (user != null && user.isJailed())
+				{
+					event.setCancelled(true);
+				}
 			}
 		}",Buggy,"Fixing up NPE bug in jails (implemented in Dev2.9.163)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixing up NPE bug in jails (implemented in Dev2.9.163)"" explicitly states that the commit addresses a NullPointerException (NPE) related to the jail functionality. This strongly suggests a bug fix.

**Git Diff Analysis:**

The code diff focuses on the `onEntityDamageByEntity` method, which handles entity damage events. The original code directly retrieves a `User` object using `ess.getUser(damager)` without checking if `damager` is a player. If `damager` is not a player, `ess.getUser(damager)` could potentially return `null`, leading to an NPE when `user.isJailed()` is called.

The modified code adds a check to ensure that the damager is a player (`damager.getType() == EntityType.PLAYER`). It also adds a null check `user != null` before calling `user.isJailed()`. These changes prevent the potential NPE by ensuring that the `user` variable is not null before accessing its `isJailed()` method.

**Reasoning:**

1.  **Commit Message and Code Alignment:** The commit message indicates an NPE fix, and the code changes introduce null checks to prevent a potential NPE when accessing the `user` object.
2.  **Error Handling:** The code adds a null check, which is a common technique for handling potential null values and preventing NPEs.
3.  **Logical Correction:** The original code assumed that the damager was always a player, which could lead to an NPE if the damager was a different type of entity. The modified code corrects this assumption by adding a type check.
4.  **Exception Handling Improvement:** While the code doesn't explicitly use try-catch blocks, the null check serves as a form of preventative exception handling by avoiding the conditions that would lead to an NPE.

**Conclusion:**

The commit message and code changes are consistent with a bug fix for a potential NullPointerException. The code adds null checks and type checks to prevent the NPE from occurring.

**Buggy**
"
Essentials,832.json,67b5b4e06b5a5952d86afe241001c0f5ba589ed4,"@@ -1,4 +1,5 @@
 	private boolean isAuthor(BookMeta bmeta, String player)
 	{
-		return bmeta.getAuthor().equalsIgnoreCase(player);
+		String author = bmeta.getAuthor();
+		return author != null && author.equalsIgnoreCase(player);
 	}",Buggy,"Fix minor /book bug (Null author)
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message ""Fix minor /book bug (Null author)"" clearly states that the commit addresses a bug related to a `/book` command or feature, specifically when the book's author is null.

**2. Analyzing the Code Diff:**

The code diff modifies the `isAuthor` method, which likely checks if a given player is the author of a book. The original code directly called `bmeta.getAuthor().equalsIgnoreCase(player)`. This would throw a `NullPointerException` if `bmeta.getAuthor()` returned `null`.

The modified code introduces a null check: `String author = bmeta.getAuthor(); return author != null && author.equalsIgnoreCase(player);`. This ensures that `equalsIgnoreCase` is only called if `author` is not null, preventing the `NullPointerException`.

**3. Reasoning:**

The commit message and the code diff are perfectly aligned. The commit message indicates a bug related to a null author, and the code diff introduces a null check to prevent a `NullPointerException` when the author is null. This is a classic bug fix scenario. The original code was vulnerable to crashing if a book had no author set. The modified code handles this case gracefully.

**Conclusion:**

**Buggy**
"
wicket,4568.json,89c5acbf41b98ab890ed489bbe15a5702da4595c,"@@ -1,4 +1,4 @@
 	public IModel<T> getModel()
 	{
-		return null;
+		return model;
 	}",Buggy,"fix small bug

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1159660 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""fix small bug"" indicates a bug fix. The code change replaces `return null;` with `return model;` in the `getModel()` method. Returning `null` would likely cause a `NullPointerException` when the model is accessed, which is a bug. The change fixes this by returning the actual model.

**Buggy**"
wicket,10114.json,f78f8eb142782d7931d91aec9a77037a8582f6cd,"@@ -1,5 +1,6 @@
 	public void renderHead(IHeaderResponse response)
 	{
 		super.renderHead(response);
-		response.render(CssHeaderItem.forReference(PageViewCSSResourceReference.get()));
+		response.render(
+			CssHeaderItem.forReference(new CssResourceReference(PageView.class, ""pageview.css"")));
 	}",Buggy,"WICKET-6737: fixed compilation error after deleting class
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""WICKET-6737: fixed compilation error after deleting class"" clearly states that the change is intended to resolve a compilation error that arose after a class was deleted. This strongly suggests a bug fix scenario.

**Git Diff Analysis:**

The diff shows a change in the `renderHead` method of a component. Specifically, it replaces the usage of `PageViewCSSResourceReference.get()` with `new CssResourceReference(PageView.class, ""pageview.css"")`.

**Reasoning:**

1.  **Class Deletion Implication:** The commit message mentions a class deletion. The original code likely relied on the deleted class `PageViewCSSResourceReference`. After the class was removed, any code referencing it would cause a compilation error.
2.  **Replacement with Direct Reference:** The diff replaces the reference to the deleted class with a direct reference to a CSS resource using `CssResourceReference`. This is a common way to resolve compilation errors caused by missing classes. The new code directly specifies the class (`PageView.class`) and the resource name (""pageview.css"").
3.  **Alignment:** The code change directly addresses the problem described in the commit message. The original code was broken due to a missing class, and the change provides a working alternative.

**Conclusion:**

The commit message explicitly states that the change fixes a compilation error caused by a class deletion. The code diff confirms that the change replaces a reference to the deleted class with a direct resource reference, resolving the compilation issue. Therefore, this is a bug fix.

**Buggy**"
wicket,7162.json,8c62ff32ea564e1efb3b9fc84b75fe783508732e,"@@ -1,39 +1,41 @@
 	public boolean equalTo(final MarkupStream that)
 	{
 		// While a has more markup elements
 		while (this.hasMore())
 		{
 			// Get an element from each
-			final MarkupElement thisElement = this.next();
-			final MarkupElement thatElement = that.next();
+			final MarkupElement thisElement = this.get();
+			final MarkupElement thatElement = that.get();
 			
 			// and if the elements are not equal
 			if (thisElement != null && thatElement != null)
 			{
 				if (!thisElement.equalTo(thatElement))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
 			else
 			{
 				// If one element is null,
 				if (!(thisElement == null && thatElement == null))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
+			this.next();
+			that.next();
 		}
 
 		// If we've run out of markup elements in b
 		if (!that.hasMore())
 		{
 			// then the two streams match perfectly
 			return true;
 		}
 
 		// Stream b had extra elements
 		return false;
 	}",Buggy,"fixed error in DiffUtil (MarkupStream.equalsTo). It failed to compare markup streams with just one MarkupElement.

git-svn-id: https://svn.apache.org/repos/asf/incubator/wicket/trunk@552187 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for an error in `DiffUtil`'s `MarkupStream.equalsTo` method, specifically when comparing markup streams with only one `MarkupElement`. The diff shows changes to the `equalTo` method.

Here's a breakdown of the changes:

1.  **`this.next()` and `that.next()` moved:** The calls to `this.next()` and `that.next()` were moved from inside the `while` loop to after the `if (thisElement != null && thatElement != null)` block.  This is significant. Previously, `next()` was called *unconditionally* at the top of the loop, advancing the stream's pointer regardless of whether the elements were actually compared. This would cause issues when comparing streams with a single element, as the first `next()` would advance the pointer past the element, and subsequent calls would return `null`.

2.  **`this.get()` and `that.get()` used:** The code now uses `this.get()` and `that.get()` instead of `this.next()` and `that.next()` to retrieve the current `MarkupElement` for comparison. This is important because `get()` retrieves the current element without advancing the stream's pointer, which is necessary for correct comparison.

The combination of these changes directly addresses the issue described in the commit message. The original code incorrectly advanced the stream pointers, leading to incorrect comparisons, especially when dealing with streams containing only one element. The corrected code ensures that the stream pointers are advanced only after the elements have been compared, resolving the bug.

**Buggy**
"
wicket,2888.json,e72ed7e22a7d002070ffe4a1d3e7daf514351c20,"@@ -1,14 +1,19 @@
 	public Serializable getCacheKey()
 	{
 		Class<?> scope = getScope();
 		String currentStyle = getCurrentStyle();
 		Locale currentLocale = getCurrentLocale();
 		
+		 IResourceStream packageResource = Application.get()
+			.getResourceSettings()
+			.getResourceStreamLocator()
+			.locate(scope, absolutePath, currentStyle, variation, currentLocale, null, false);
+		 
 		// if resource stream can not be found do not cache
-		if (exists(scope, absolutePath, currentLocale, currentStyle, variation))
+		if (packageResource != null)
 		{
 			return new CacheKey(scopeName, absolutePath, currentLocale, currentStyle, variation);
 		}
 
 		return null;
 	}",Buggy,"WICKET-6061 improved getChacheKey to fix problems with absolute path of package resources
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""WICKET-6061 improved getChacheKey to fix problems with absolute path of package resources"" explicitly states that the change is intended to *fix problems* related to the absolute path of package resources within the `getCacheKey` method. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `getCacheKey` method. Here's a breakdown:

1.  **Resource Stream Location:** A new `IResourceStream` is located using `Application.get().getResourceSettings().getResourceStreamLocator().locate(...)`. This suggests that the code is now explicitly trying to find the resource stream. The `absolutePath` is used in the `locate` method.
2.  **Conditional Caching:** The `if` condition has changed. Previously, it used an `exists` method (presumably to check if the resource exists). Now, it checks if the `packageResource` (the resource stream located in the previous step) is not null. This indicates that the existence check is now based on whether the resource stream can be located.
3.  **Cache Key Creation:** The `CacheKey` is created only if `packageResource` is not null.

**Reasoning:**

The original code likely had an issue where the `getCacheKey` method was not correctly handling absolute paths for package resources. This could lead to incorrect caching behavior, such as:

*   Caching resources that don't exist.
*   Not caching resources that do exist.
*   Using the wrong cache key for resources with absolute paths.

The changes in the diff address this issue by:

*   Explicitly locating the resource stream using the `absolutePath`.
*   Using the result of the resource stream location to determine whether to create a cache key.

By locating the resource stream, the code can now accurately determine if the resource exists and create a valid cache key. The commit message and code changes are highly relevant. The code is modified to handle the absolute path correctly, which was previously causing issues.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes align with the described problem. The code now correctly locates the resource stream before creating the cache key, addressing the issue with absolute paths.

**Buggy**
"
wicket,6423.json,835db0bf267a6c1b713f9801237c68cba043e9af,"@@ -1,5 +1,15 @@
 	public Border remove(final Component component)
 	{
-		getBodyContainer().remove(component);
+		if (component == body)
+		{
+			// when the user calls foo.add(getBodyContainer()) this method will be called with it to
+			// clear body container's old parent, in which case we do not want to redirect to body
+			// container but to border's old remove.
+			super.remove(body);
+		}
+		else
+		{
+			getBodyContainer().remove(component);
+		}
 		return this;
 	}",Buggy,"fixes a bug where border's getbodycontainer is left as a child of border even if it was added to another container
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message clearly states that it ""fixes a bug"" related to how a `Border` component handles its `body` container when the `body` container is added to another container. Specifically, it addresses a scenario where the `body` container remains a child of the `Border` even after being added elsewhere.

2.  **Code Diff:**
    *   The `remove(Component component)` method of the `Border` class is being modified.
    *   A conditional check `if (component == body)` is introduced. This suggests a special handling for the `body` container.
    *   Inside the `if` block, `super.remove(body)` is called. This indicates that the `Border` class's parent class's `remove` method is being used to remove the `body` container, likely to detach it from the `Border` properly.
    *   The `else` block retains the original behavior of removing the component from the `body` container.

3.  **Alignment and Bug Fix Indication:**
    *   The code change directly addresses the issue described in the commit message. The `if (component == body)` condition checks if the component to be removed is the `body` container itself. This is precisely the scenario described in the commit message where the `body` container might need to be detached from the `Border` due to being added elsewhere.
    *   The use of `super.remove(body)` suggests that the original `getBodyContainer().remove(component)` call was insufficient to fully detach the `body` container from the `Border` in the specific scenario outlined in the commit message. This indicates a bug in the original implementation.
    *   The added logic handles the edge case where the `body` container has been added to another container, preventing it from remaining a child of the `Border`. This correction strongly suggests a bug fix.

**Conclusion:**

The commit message and code diff are highly aligned. The code changes introduce a specific condition and handling to address the scenario described in the commit message, indicating a correction to a previously existing bug.

**Buggy**"
wicket,9455.json,7480012fafc537e405b6c0a624c1b76f39d56f41,"@@ -1,29 +1,29 @@
 	public static Number getMinValue(Class<? extends Number> numberType)
 	{
 		Number result;
 		if (Integer.class == numberType || int.class == numberType)
 		{
 			result = Integer.MIN_VALUE;
 		}
 		else if (Long.class == numberType || long.class == numberType) {
 			result = Long.MIN_VALUE;
 		}
 		else if (Float.class == numberType || float.class == numberType) {
 			result = Float.MIN_VALUE;
 		}
 		else if (Double.class == numberType || double.class == numberType) {
 			result = Double.MIN_VALUE;
 		}
 		else if (Byte.class == numberType || byte.class == numberType) {
 			result = Byte.MIN_VALUE;
 		}
 		else if (Short.class == numberType || short.class == numberType) {
 			result = Short.MIN_VALUE;
 		}
 		else { // null of any other Number
-			LOG.debug(""'{}' has no maximum value. Falling back to Double.MIN_VALUE."", numberType);
+			LOG.debug(""'{}' has no minimum value. Falling back to Double.MIN_VALUE."", numberType);
 			result = Double.MIN_VALUE;
 		}
 
 		return result;
 	}",Buggy,"Fix a copy/paste error
",Buggy,"The commit message ""Fix a copy/paste error"" suggests that the code change involves correcting a mistake that was likely introduced by copying and pasting code.

The diff shows a change in the `getMinValue` method. Specifically, the debug message within the `else` block has been modified. The original message was:

```java
LOG.debug(""'{}' has no maximum value. Falling back to Double.MIN_VALUE."", numberType);
```

The corrected message is:

```java
LOG.debug(""'{}' has no minimum value. Falling back to Double.MIN_VALUE."", numberType);
```

The word ""maximum"" has been changed to ""minimum"". This aligns perfectly with the commit message, as it indicates a copy/paste error where the word ""maximum"" was incorrectly used instead of ""minimum"" in the context of finding the minimum value. This correction directly addresses a bug in the logging message.

**Buggy**"
vraptor4,95.json,d57d1be4185c180ec3a99dab938a0cfaec66516c,"@@ -1,33 +1,33 @@
 	public <T> T forwardTo(final Class<T> type) {
 		return proxifier.proxify(type, new MethodInvocation<T>() {
 
 			@Override
 			public Object intercept(T proxy, Method method, Object[] args, SuperMethod superMethod) {
 				try {
 					logger.debug(""Executing {}"", method);
 					ControllerMethod old = methodInfo.getControllerMethod();
 					methodInfo.setControllerMethod(DefaultControllerMethod.instanceFor(type, method));
 					Object methodResult = method.invoke(container.instanceFor(type), args);
 					methodInfo.setControllerMethod(old);
 
 					Type returnType = method.getGenericReturnType();
 					if (!(returnType == void.class)) {
 						request.setAttribute(extractor.nameFor(returnType), methodResult);
 					}
-					if (response.isCommitted() || result.used()) {
+					if (response.isCommitted()) {
 						logger.debug(""Response already commited, not forwarding."");
 						return null;
 					}
 					String path = resolver.pathFor(DefaultControllerMethod.instanceFor(type, method));
 					logger.debug(""Forwarding to {}"", path);
 					request.getRequestDispatcher(path).forward(request, response);
 					return null;
 				} catch (InvocationTargetException e) {
 					propagateIfPossible(e.getCause());
 					throw new ProxyInvocationException(e);
 				} catch (Exception e) {
 					throw new ProxyInvocationException(e);
 				}
 			}
 		});
 	}",Buggy,"Fixing problem with forward.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixing problem with forward"" is quite generic but strongly suggests that the change addresses a bug related to the `forwardTo` method's forwarding mechanism.

**Code Diff Analysis:**

The code diff focuses on the `forwardTo` method. The key change is within the `intercept` method, specifically this line:

`-					if (response.isCommitted() || result.used()) {`
`+					if (response.isCommitted()) {`

The `|| result.used()` condition has been removed. This suggests that the original code had a condition where forwarding would be skipped if `result.used()` was true, even if the response wasn't committed. The removal of this condition implies that there was a scenario where forwarding was incorrectly being prevented.

**Reasoning:**

1.  **Commit Message Relevance:** The commit message directly mentions a problem with forwarding, aligning with the code changes.
2.  **Logical Correction:** The removal of `result.used()` from the conditional statement suggests a correction in the forwarding logic. It implies that the `result.used()` check was erroneously preventing forwarding in some cases.
3.  **Bug Indication:** The removal of a condition that was incorrectly preventing forwarding strongly indicates a bug fix. The original code was likely not forwarding as intended under certain circumstances.

**Conclusion:**

Based on the commit message and the code diff, the change appears to be a bug fix. The removal of the `result.used()` condition suggests that the original code had a flaw that prevented forwarding when it should have occurred.

**Buggy**"
vraptor4,1079.json,cff51ec6e04f0bcb011c05510d72b8c4c22b1b66,"@@ -1,4 +1,13 @@
 	public <T extends View> T use(Class<T> view) {
+		if(view.isAssignableFrom(Results.json()) && validator.hasErrors()) {
+	        throw new IllegalStateException(
+	                ""There are validation errors and you forgot to specify where to go. Please add in your method ""
+	                        + ""something like:\n""
+	                        + ""validator.onErrorUse(page()).of(AnyController.class).anyMethod();\n""
+	                        + ""or any view that you like.\n""
+	                        + ""If you didn't add any validation error, it is possible that a conversion error had happened."");
+	    }
+		
 		responseCommitted = true;
 		return container.instanceFor(view);
 	}",Buggy,"Fixing problem: validation ignored on redirects.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixing problem: validation ignored on redirects"" clearly indicates that the commit is intended to address a bug where validation was not being properly handled during redirects.

2.  **Code Diff:** The code diff introduces a check for validation errors specifically when the requested view is a JSON result (`Results.json()`). If validation errors exist and the view is JSON, an `IllegalStateException` is thrown with a helpful message guiding the developer on how to handle validation errors.

3.  **Alignment:** The code change aligns perfectly with the commit message. The added code directly addresses the issue of validation being ignored, at least in the context of JSON results. The exception thrown when validation errors exist and a JSON result is requested suggests that the previous behavior was incorrect (i.e., a bug). The previous behavior likely allowed the redirect to occur without considering validation errors, potentially leading to unexpected or incorrect application states.

4.  **Bug Fix Indicators:**
    *   The commit message explicitly states ""Fixing problem"".
    *   The code adds error handling (throwing an exception) where it was previously missing.
    *   The exception message guides the developer on how to correct the usage, implying that the previous usage was leading to errors.

**Conclusion:**

Based on the commit message and the code changes, this commit clearly addresses a bug related to validation being ignored on redirects (specifically when returning JSON). The added error handling and informative exception message further support this conclusion.

**Buggy**
"
flink,4079.json,1863bb88e4f2388a2f91b8000bcb3ca1e44d352b,"@@ -1,9 +1,8 @@
 	static SupplierWithException<BufferPool, IOException> createBufferPoolFactory(
 			BufferPoolFactory bufferPoolFactory,
 			int networkBuffersPerChannel,
 			int floatingNetworkBuffersPerGate,
 			int size,
 			ResultPartitionType type) {
-		int maxNumberOfMemorySegments = type.isBounded() ? floatingNetworkBuffersPerGate : Integer.MAX_VALUE;
-		return () -> bufferPoolFactory.createBufferPool(0, maxNumberOfMemorySegments);
+		return () -> bufferPoolFactory.createBufferPool(0, floatingNetworkBuffersPerGate);
 	}",Buggy,"[FLINK-14872][runtime] Temporary fix for potential deadlock problem when tasks read from blocking ResultPartitions. (#10472)

This commit implements a temporary fix for the potential deadlock problem reported in FLINK-14872. The problem itself is not solved completely, however the possibility of deadlock is largely reduced. We leave the proper fix of this problem to the future version.",Buggy,"The commit message indicates a temporary fix for a potential deadlock problem when tasks read from blocking ResultPartitions. The message explicitly states that the problem is not completely solved but the possibility of deadlock is largely reduced.

The code diff modifies the `createBufferPoolFactory` method. The original code calculates `maxNumberOfMemorySegments` based on `ResultPartitionType`. If the type is bounded, it uses `floatingNetworkBuffersPerGate`; otherwise, it uses `Integer.MAX_VALUE`. The modified code always uses `floatingNetworkBuffersPerGate` regardless of the `ResultPartitionType`.

The change directly relates to buffer pool creation, which is a critical resource for data exchange between tasks. Deadlocks can occur when tasks are waiting for resources that are held by other tasks, leading to a standstill. By limiting the `maxNumberOfMemorySegments` to `floatingNetworkBuffersPerGate` regardless of the partition type, the code reduces the possibility of a task requesting an unbounded number of memory segments, which could potentially lead to resource exhaustion and deadlocks.

The commit message and the code change are consistent. The change aims to mitigate a potential deadlock situation, indicating a bug fix.

**Buggy**"
flink,18562.json,d8c74d20970e6cf379165ffafe8550a5b98d7c4d,"@@ -1,16 +1,19 @@
 		public boolean equals(final Object obj) {
+			if (obj == this) {
+				return true;
+			}
 
 			if (obj instanceof FSKey) {
 				final FSKey key = (FSKey) obj;
 
 				if (!this.scheme.equals(key.scheme)) {
 					return false;
 				}
 
 				if ((this.authority == null) || (key.authority == null)) {
 					return this.authority == null && key.authority == null;
 				}
 				return this.authority.equals(key.authority);
 			}
 			return false;
 		}",Buggy,"[FLINK-1766]Fix the bug of equals function of FSKey

The equals function in org.apache.flink.core.fs.FileSystem.FSKey should first confirm whether obj == this, if obj is the same object.It should return true

Author: hongsibao <hongsibao@huawei.com>

Closes #511 from matadorhong/Bug_Flink_1776 and squashes the following commits:

1ad06d7 [hongsibao] Fix the code format problem
431fc4b [hongsibao] Fix the code format problem
ab0ae5e [hongsibao] [FLINK-1766]Fix the bug of equals function of FSKey
",Buggy,"The commit message indicates a bug fix in the `equals` function of `FSKey`. The bug is that the `equals` method doesn't check for object equality using `obj == this` before proceeding with other checks. The provided diff adds this check at the beginning of the `equals` method. This aligns with the commit message, suggesting a bug fix related to object comparison.

The added code `if (obj == this) { return true; }` directly addresses the issue described in the commit message. This is a standard optimization and correctness check in `equals` methods.

Therefore, the changes indicate a bug fix.
**Buggy**
"
flink,2817.json,cbfb807d65b68b2b6157e1b1d42606123ea499ad,"@@ -1,8 +1,7 @@
 	public void stop() throws Exception {
 		LOG.info(""Stopping ZooKeeperLeaderRetrievalService."");
 
 		client.getConnectionStateListenable().removeListener(connectionStateListener);
 
 		cache.close();
-		client.close();
 	}",Buggy,"[FLINK-5190] [runtime] fix ZooKeeperLeaderRetrievalService close the zk client when stopping bug
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message `[FLINK-5190] [runtime] fix ZooKeeperLeaderRetrievalService close the zk client when stopping bug` explicitly states that the commit fixes a bug related to closing the ZooKeeper client within the `ZooKeeperLeaderRetrievalService` during the stopping process.

2.  **Diff Analysis:** The diff shows that the line `client.close();` has been removed from the `stop()` method of the `ZooKeeperLeaderRetrievalService`.

3.  **Alignment and Bug Fix Indication:** The commit message and the diff are strongly aligned. The commit message says there's a bug related to closing the ZooKeeper client, and the diff removes the client closing logic. This suggests that the original code was incorrectly or prematurely closing the ZooKeeper client during the `stop()` method, which could lead to issues in other parts of the application that still rely on the client. Removing the `client.close()` call likely resolves this issue. The removal of the `client.close()` call indicates that the previous behavior was incorrect and potentially caused problems, thus qualifying as a bug fix.

**Conclusion:**

**Buggy**
"
flink,43722.json,f0ed58c6a580db0966104b81491d08d25d1ff57e,"@@ -1,32 +1,33 @@
 		public void reduce(Iterable<T> records, Collector<T> out) {
 			final AggregationFunction<Object>[] aggFunctions = this.aggFunctions;
 			final int[] fieldPositions = this.fieldPositions;
 
 			// aggregators are initialized from before
 
 			T current = null;
 			final Iterator<T> values = records.iterator();
 			while (values.hasNext()) {
 				current = values.next();
 
 				for (int i = 0; i < fieldPositions.length; i++) {
 					Object val = current.productElement(fieldPositions[i]);
 					aggFunctions[i].aggregate(val);
 				}
 			}
 
 			Object[] fields = new Object[serializer.getArity()];
+			int length = serializer.getArity();
 			// First copy all tuple fields, then overwrite the aggregated ones
-			for (int i = 0; i < fieldPositions.length; i++) {
-				fields[0] = current.productElement(i);
+			for (int i = 0; i < length; i++) {
+				fields[i] = current.productElement(i);
 			}
 			for (int i = 0; i < fieldPositions.length; i++) {
 				Object aggVal = aggFunctions[i].getAggregate();
 				fields[fieldPositions[i]] = aggVal;
 				aggFunctions[i].initializeAggregate();
 			}
 
 			T result = serializer.createInstance(fields);
 
 			out.collect(result);
 		}",Buggy,"Fix Bug in ScalaAggregate Operator and add ITCase
",Buggy,"The commit message indicates a bug fix in the `ScalaAggregate` operator. The code diff shows a change in the `reduce` method. Specifically, the loop that copies tuple fields has been modified. The original code iterated up to `fieldPositions.length`, which is the number of aggregated fields, which is incorrect. The corrected code iterates up to `serializer.getArity()`, which represents the total number of fields in the tuple. This change suggests that the original code was not correctly copying all the tuple fields, leading to a bug. The addition of an ITCase (Integration Test Case) in the commit message further supports the presence of a bug fix.

**Buggy**"
flink,25849.json,0081fb2ef2bd03d06a786dd8988865d2ff6168c2,"@@ -1,5 +1,5 @@
 		public void combine(Iterable<Tuple3<K1, K2, IN>> values, Collector<Tuple3<K1, K2, IN>> out) throws Exception {
 			iter.set(values.iterator());
 			coll.set(out);
-			this.wrappedFunction.combine(iter, coll);
+			((GroupCombineFunction)this.wrappedFunction).combine(iter, coll);
 		}",Buggy,"[FLINK-2135] Fix faulty cast to GroupReduceFunction

This closes #769
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""[FLINK-2135] Fix faulty cast to GroupReduceFunction"" clearly states that the commit intends to fix a faulty cast. This strongly suggests a bug fix.

2.  **Code Diff:** The code diff shows a change in the `combine` method. Specifically, the line `this.wrappedFunction.combine(iter, coll);` is being replaced with `((GroupCombineFunction)this.wrappedFunction).combine(iter, coll);`. This indicates that the code was previously calling the `combine` method on `wrappedFunction` without explicitly casting it to `GroupCombineFunction`. The change adds an explicit cast to `GroupCombineFunction` before calling the `combine` method.

3.  **Bug Fix Indication:** The explicit cast suggests that the original code might have been assuming that `wrappedFunction` was always a `GroupCombineFunction`, which might not always be the case. If `wrappedFunction` was not a `GroupCombineFunction`, the original code would have thrown a `ClassCastException` at runtime. The explicit cast fixes this potential issue by ensuring that the `combine` method is called on an object of the correct type. This aligns with the commit message indicating a bug fix related to a faulty cast.

**Conclusion:**

The commit message and the code diff both strongly suggest that this commit is fixing a bug related to an incorrect cast. Therefore, the answer is:

**Buggy**"
flink,17423.json,6624562982c9d57bebba8cb4b574b8ed28640a0d,"@@ -1,15 +1,17 @@
 	public static DeweyNumber fromString(final String deweyNumberString) {
 		String[] splits = deweyNumberString.split(""\\."");
 
-		if (splits.length == 0) {
+		if (splits.length == 1) {
 			return new DeweyNumber(Integer.parseInt(deweyNumberString));
-		} else {
+		} else if (splits.length > 0) {
 			int[] deweyNumber = new int[splits.length];
 
 			for (int i = 0; i < splits.length; i++) {
 				deweyNumber[i] = Integer.parseInt(splits[i]);
 			}
 
 			return new DeweyNumber(deweyNumber);
+		} else {
+			throw new IllegalArgumentException(""Failed to parse "" + deweyNumberString + "" as a Dewey number"");
 		}
 	}",Buggy,"[FLINK-13043] [Library / CEP] Fix the bug of parsing Dewey number from string

This closes #8936
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message `[FLINK-13043] [Library / CEP] Fix the bug of parsing Dewey number from string` clearly states that the commit is intended to fix a bug related to parsing Dewey numbers from strings. This strongly suggests that the code changes are bug-related.

**Code Diff Analysis:**

The code diff modifies the `fromString` method, which is responsible for converting a string representation into a `DeweyNumber` object. Let's break down the changes:

1.  **`if (splits.length == 0)` changed to `if (splits.length == 1)`:** This is a significant change. The original code had a condition that if the split array had length 0, it would parse the original string. This is likely incorrect. A Dewey number string like ""1"" should result in a split array of length 1. The change to `splits.length == 1` seems more appropriate for handling single-number Dewey numbers.

2.  **`else { ... }` changed to `else if (splits.length > 0) { ... }`:** This change is related to the previous one. It makes the condition more explicit and correct.

3.  **Added `else { throw new IllegalArgumentException(...) }`:** This is a crucial addition. It introduces error handling for cases where the input string cannot be parsed into a valid Dewey number. This indicates that the original code lacked proper error handling and could potentially lead to unexpected behavior or crashes.

**Reasoning:**

The commit message explicitly states a bug fix. The code changes involve correcting the logic for parsing Dewey numbers and adding error handling. The original code likely had a flaw in how it handled single-number Dewey numbers and lacked proper error handling for invalid input strings. The added `IllegalArgumentException` clearly suggests that the original code didn't handle invalid input gracefully, which is a common characteristic of buggy code. The changes address these issues, indicating a bug fix.

**Conclusion:**

**Buggy**
"
flink,18090.json,624cb64a462765419bb0f81e2a50b3cec1c877f6,"@@ -1,25 +1,22 @@
 	public void copyNormalizedKey(MemorySegment target, int offset, int len) {
+		// note that the char is an unsigned data type in java and consequently needs
+		// no code that transforms the signed representation to an offsetted representation
+		// that is equivalent to unsigned, when compared byte by byte
 		if (len == 2) {
 			// default case, full normalized key
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 		}
 		else if (len <= 0) {
 		}
 		else if (len == 1) {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
 		}
 		else {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 			for (int i = 2; i < len; i++) {
 				target.put(offset + i, (byte) 0);
 			}
 		}
 	}",Buggy,"Fixed erroneous normalized key generation in PactCharacter.
",Buggy,"The commit message indicates a fix to the normalized key generation in `PactCharacter`. The code diff shows the removal of the line `highByte -= Byte.MIN_VALUE;` in several places within the `copyNormalizedKey` method. This line appears to be an attempt to treat the `highByte` as an unsigned byte, but it's unnecessary since Java's `char` type is already unsigned. Removing this line corrects the normalized key generation. The comment added also confirms that the char is an unsigned data type in java.

Therefore, the changes indicate a bug fix.
**Buggy**
"
flink,18559.json,f04b32b02900181db82c6c518ffbd6430a0a369a,"@@ -1,40 +1,40 @@
 	private static HashMap<String, FileSystemFactory> loadFileSystems() {
 		final HashMap<String, FileSystemFactory> map = new HashMap<>();
 
 		// by default, we always have the local file system factory
 		map.put(""file"", new LocalFileSystemFactory());
 
 		LOG.debug(""Loading extension file systems via services"");
 
 		try {
 			ServiceLoader<FileSystemFactory> serviceLoader = ServiceLoader.load(FileSystemFactory.class);
 			Iterator<FileSystemFactory> iter = serviceLoader.iterator();
 
 			// we explicitly use an iterator here (rather than for-each) because that way
 			// we can catch errors in individual service instantiations
 
 			//noinspection WhileLoopReplaceableByForEach
 			while (iter.hasNext()) {
 				try {
 					FileSystemFactory factory = iter.next();
 					String scheme = factory.getScheme();
 					map.put(scheme, factory);
 					LOG.debug(""Added file system {}:{}"", scheme, factory.getClass().getName());
 				}
 				catch (Throwable t) {
 					// catching Throwable here to handle various forms of class loading
 					// and initialization errors
 					ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
-					LOG.error(""Failed to load a file systems via services"", t);
+					LOG.error(""Failed to load a file system via services"", t);
 				}
 			}
 		}
 		catch (Throwable t) {
 			// catching Throwable here to handle various forms of class loading
 			// and initialization errors
 			ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
 			LOG.error(""Failed to load additional file systems via services"", t);
 		}
 
 		return map;
 	}",Buggy,"[hotfix] [core] Fix lots of checkstyle errors in core.fs
",NotBuggy,"The commit message indicates a hotfix for checkstyle errors in the core.fs module. The diff shows a change in a log message: ""Failed to load a file systems via services"" was changed to ""Failed to load a file system via services"". This change corrects a grammatical error (pluralizing ""system"" when it should be singular). While this is a fix, it is not a bug fix in the sense of correcting a functional error or logical flaw. It's more of a code style or quality improvement.

Therefore, the change does not indicate a bug fix.

**NotBuggy**"
flink,11393.json,84b39dcb50167368d45882a88842760237fa3560,"@@ -1,19 +1,23 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
 		}
+		catch (LinkageError e) {
+			// hadoop classes are not in the classpath
+			LOG.debug(""Cannot determine user/group information using Hadoop utils. "" +
+					""Hadoop classes not loaded or compatible"", e);
+		}
 		catch (Throwable t) {
-			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
-				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
-			}
+			// some other error occurred that we should log and make known
+			LOG.warn(""Error while accessing user/group information via Hadoop utils."", t);
 		}
 		
 		String user = System.getProperty(""user.name"");
 		if (user == null) {
 			user = UNKNOWN;
 			if (LOG.isDebugEnabled()) {
 				LOG.debug(""Cannot determine user/group information for the current user."");
 			}
 		}
 		return user;
 	}",Buggy,"[hotfix] Fix error messages in EnvironmentInformation when accessing user information
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message `[hotfix] Fix error messages in EnvironmentInformation when accessing user information` strongly suggests a bug fix. The term ""hotfix"" implies an urgent correction to a problem. The phrase ""Fix error messages"" indicates that the existing error messages were either incorrect, misleading, or insufficient, which is a bug. The context ""when accessing user information"" narrows down the area where the bug existed.

**Git Diff Analysis:**

The diff focuses on the `getUserRunning()` method. Here's a breakdown of the changes:

1.  **Catching `LinkageError`:** A new `catch` block is added to handle `LinkageError`. This error typically occurs when there are class loading issues, meaning Hadoop classes are not available or compatible. The log message associated with this catch block provides a more informative message about the cause of the error.
2.  **Improved Error Logging:** The original `catch` block for `Throwable` was checking for `ClassNotFoundException` within a debug log level. This has been changed to a `warn` log level for all `Throwable` exceptions, and the log message is updated to indicate a general error while accessing user/group information.
3.  **No functional changes:** The core logic of retrieving the username remains the same. The changes are purely related to error handling and logging.

**Reasoning:**

The changes in the diff directly address the issue described in the commit message. The original code had inadequate error handling and logging when it failed to retrieve user information using Hadoop utils. The added `LinkageError` catch block and the improved logging in the `Throwable` catch block provide more informative error messages, making it easier to diagnose and resolve issues related to user information retrieval. The fact that a `LinkageError` is now explicitly handled suggests that this specific error case was previously not handled correctly, leading to potentially confusing or missing error messages.

**Conclusion:**

The commit message and the code changes are highly relevant. The changes improve error handling and provide more informative error messages, which aligns perfectly with the commit message's intention to ""Fix error messages."" The addition of the `LinkageError` catch block and the general improvement in error logging clearly indicate that the original code had a bug in its error handling.

**Buggy**
"
flink,11393.json,e45534cf2275f6918294c81de5377146c243904c,"@@ -1,12 +1,19 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
-		} catch (Throwable t) {
-			if (LOG.isDebugEnabled()) {
-				LOG.debug(""Cannot determine user/group information for the current user."", t);
-			} else {
-				LOG.info(""Cannot determine user/group information for the current user."");
-			}
-			return UNKNOWN;
 		}
+		catch (Throwable t) {
+			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
+				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
+			}
+		}
+		
+		String user = System.getProperty(""user.name"");
+		if (user == null) {
+			user = UNKNOWN;
+			if (LOG.isDebugEnabled()) {
+				LOG.debug(""Cannot determine user/group information for the current user."");
+			}
+		}
+		return user;
 	}",Buggy,"Fix problems with missing files/dependencies with cdh-4 distribution.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix problems with missing files/dependencies with cdh-4 distribution"" suggests that the commit addresses issues related to missing files or dependencies, specifically within the context of the CDH-4 distribution. This implies that the previous code might have been failing or behaving incorrectly due to these missing dependencies.

**Git Diff Analysis:**

The code modifies the `getUserRunning()` method. Here's a breakdown of the changes:

1.  **Original Code:** The original code attempts to get the user name using `UserGroupInformation.getCurrentUser().getShortUserName()`. If any `Throwable` (including `ClassNotFoundException`, `NoClassDefFoundError`, etc.) is caught, it logs a message and returns `UNKNOWN`.

2.  **Modified Code:**
    *   The `try-catch` block is modified. Now, it specifically checks if the caught exception is a `ClassNotFoundException`. If it is, the debug message is not logged.
    *   The code now attempts to get the username from the system property `user.name`.
    *   If the `user.name` property is null, it sets the user to `UNKNOWN` and logs a debug message.
    *   Finally, it returns the user name.

**Reasoning:**

The changes strongly suggest a bug fix:

*   **Dependency Issue:** The commit message mentions ""missing files/dependencies with cdh-4 distribution."" The code change attempts to address this by catching `Throwable` and then trying to get the username from `System.getProperty(""user.name"")`. This is a fallback mechanism in case the Hadoop-specific classes are not available (due to missing dependencies, which aligns with the commit message).
*   **Exception Handling:** The original code's catch-all `Throwable` block would mask `ClassNotFoundException` and other dependency-related exceptions, preventing the fallback mechanism from being triggered. The modified code specifically avoids logging debug messages for `ClassNotFoundException`, indicating that this exception is expected in some environments (like CDH-4 with missing dependencies).
*   **Fallback Mechanism:** The addition of `System.getProperty(""user.name"")` as a fallback suggests that the original method of getting the username was unreliable in certain environments, likely due to the aforementioned missing dependencies.

The changes address a scenario where the Hadoop-specific classes might not be available, likely due to missing dependencies in the CDH-4 distribution. The code now gracefully handles this situation by falling back to `System.getProperty(""user.name"")`. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
flink,3887.json,4d693c4fbc5e6f3ff34ccb3cb3a1d9f35d6bbd76,"@@ -1,25 +1,27 @@
 	private void writeAndCloseBufferConsumer(BufferConsumer bufferConsumer) throws IOException {
 		try {
 			final Buffer buffer = bufferConsumer.build();
 			try {
 				if (canBeCompressed(buffer)) {
 					final Buffer compressedBuffer = parent.bufferCompressor.compressToIntermediateBuffer(buffer);
 					data.writeBuffer(compressedBuffer);
-					compressedBuffer.recycleBuffer();
+					if (compressedBuffer != buffer) {
+						compressedBuffer.recycleBuffer();
+					}
 				} else {
 					data.writeBuffer(buffer);
 				}
 
 				numBuffersAndEventsWritten++;
 				if (buffer.isBuffer()) {
 					numDataBuffersWritten++;
 				}
 			}
 			finally {
 				buffer.recycleBuffer();
 			}
 		}
 		finally {
 			bufferConsumer.close();
 		}
 	}",Buggy,"[FLINK-15166][runtime] Fix the bug of wrongly recycling uncompressed buffer

For blocking shuffle data compression, the compressed intermediate buffer is recycled after it is written out. However when the data can not be compressed, the returned buffer is the original buffer which should not be recycled.

This commit fixes the bug of wrongly recycling uncompressed buffer by comparing the returned buffer with the original buffer.",Buggy,"The commit message indicates a bug fix related to the incorrect recycling of uncompressed buffers during blocking shuffle data compression. The description explains that when data cannot be compressed, the original buffer is returned, but it was being incorrectly recycled.

The code diff shows a modification within the `writeAndCloseBufferConsumer` method. Specifically, after attempting compression, the code now checks if the `compressedBuffer` is different from the original `buffer`. If they are different, it implies that compression occurred and the `compressedBuffer` should be recycled. However, if they are the same, it means the original buffer was returned (no compression), and thus it should not be recycled at this point. The original buffer is recycled in the finally block.

This change directly addresses the bug described in the commit message. The added conditional check prevents the premature recycling of the original buffer when compression is not applied.

**Buggy**"
flink,18028.json,0f28095f9f79b49d45905e61709ff4fab6fedd54,"@@ -1,8 +1,8 @@
 	public Appendable append(CharSequence csq, int start, int end) {
 		final int otherLen = end - start;
 		grow(this.len + otherLen);
-		for (int pos = start; pos < len; pos++)
+		for (int pos = start; pos < end; pos++)
 			this.value[this.len + pos] = csq.charAt(pos);
 		this.len += otherLen;
 		return this;
 	}",Buggy,"Fixed bug in PactString append.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed bug in PactString append"" explicitly states that the commit addresses a bug within the `append` method of the `PactString` class. This strongly suggests a bug fix.

2.  **Git Diff Analysis:**
    *   The diff modifies the `append` method.
    *   The original code had a loop condition `pos < len`, which is incorrect. It should have been `pos < end` to iterate through the provided `CharSequence` correctly. The corrected code changes the loop condition to `pos < end`.

3.  **Alignment and Bug Fix Indication:** The code change directly addresses the bug mentioned in the commit message. The original loop condition would have led to incorrect character appending, likely resulting in out-of-bounds access or incorrect string construction. The corrected code fixes this logical error.

**Conclusion:**

The commit message and the code diff are perfectly aligned. The code change fixes a clear logical error in the `append` method, which constitutes a bug fix.

**Buggy**"
flink,40862.json,6cf15c29b1bf3bb3ddcf69157079f9e5b185d9c8,"@@ -1,5 +1,4 @@
 	public SingleOutputStreamOperator<OUT, ?> reduce(ReduceFunction<OUT> reducer) {
 		return dataStream.addFunction(""groupReduce"", reducer, getTypeWrapper(reducer),
-				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition))
-				.partitionBy(keyPosition);
+				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition));
 	}",Buggy,"[streaming] Minor bug and license header fixes
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""[streaming] Minor bug and license header fixes"" suggests that the commit includes both a minor bug fix and updates to license headers. The presence of ""bug fix"" is a strong indicator.

**Git Diff Analysis:**

The diff shows a change in the `reduce` method. Specifically, the `.partitionBy(keyPosition)` call is removed.

**Reasoning:**

The removal of `.partitionBy(keyPosition)` suggests a potential correction in how the data is being processed after the `GroupReduceInvokable` is applied.  Partitioning can significantly affect the correctness of a streaming job, especially if the partitioning was incorrect or unnecessary. Removing it could be a fix for incorrect data distribution or processing. The commit message explicitly mentions a bug fix, and this code change aligns with that.  The removal of `partitionBy` could be addressing an issue where data was being incorrectly partitioned, leading to incorrect results.

**Conclusion:**

**Buggy**
"
flink,18045.json,6e9b2848d5fabace5c6ef491c87c562eed9b5f43,"@@ -1,27 +1,26 @@
 	public void copy(DataInputView in, DataOutputView target) throws IOException {
 		int len = in.readUnsignedByte();
 		target.writeByte(len);
 
 		if (len >= HIGH_BIT) {
 			int shift = 7;
 			int curr;
 			len = len & 0x7f;
 			while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
 				len |= (curr & 0x7f) << shift;
 				shift += 7;
 				target.writeByte(curr);
 			}
 			len |= curr << shift;
+			target.writeByte(curr);
 		}
 
 		for (int i = 0; i < len; i++) {
 			int c = in.readUnsignedByte();
 			target.writeByte(c);
-			if (c >= HIGH_BIT) {
-				int curr;
-				while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
-					target.writeByte(curr);
-				}
+			while (c >= HIGH_BIT) {
+				c = in.readUnsignedByte();
+				target.writeByte(c);
 			}
 		}
 	}",Buggy,"[FLINK-1336] [core] Fix bug in StringValue binary copy method
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message `[FLINK-1336] [core] Fix bug in StringValue binary copy method` explicitly states that the commit fixes a bug in the `StringValue`'s binary copy method. This strongly suggests that the code changes are related to correcting erroneous behavior.

**Code Diff Analysis:**

The code diff modifies the `copy` method, which appears to be responsible for copying a `StringValue` from an input stream (`DataInputView`) to an output stream (`DataOutputView`). Let's break down the changes:

1.  **Variable Length Encoding:** The code uses a variable-length encoding scheme to represent the length of the string. The first byte indicates the initial length or signals that more bytes follow. Bytes with the highest bit set (>= HIGH_BIT) indicate continuation.

2.  **First `if` Block:** The first `if` block handles the case where the initial length byte indicates that the length is encoded in multiple bytes. The loop reads subsequent bytes until a byte with the highest bit unset is encountered. The original code was missing writing the last byte `curr` to the target. The modified code adds `target.writeByte(curr);` to fix this.

3.  **Second `for` Loop:** This loop iterates `len` times, reading and writing each character of the string. The original code had an `if` statement inside the loop that checked if `c >= HIGH_BIT`. If it was, it would enter a `while` loop to read and write bytes until `c < HIGH_BIT`. However, after the `while` loop, `c` was not updated, leading to an infinite loop if the last byte read in the `while` loop was also >= `HIGH_BIT`. The modified code updates `c` after writing it to the target.

**Reasoning:**

The commit message clearly indicates a bug fix. The code diff shows modifications to the logic of the `copy` method, specifically in handling variable-length encoding and character copying. The original code had a potential infinite loop in the second `for` loop. The modified code fixes this by updating `c` after writing it to the target. The first `if` block was missing writing the last byte to the target. The modified code fixes this by adding `target.writeByte(curr);`. These changes suggest that the original implementation had errors that have now been corrected.

**Conclusion:**

**Buggy**
"
flink,26584.json,32440227eceb122008906878136156ffa839efa6,"@@ -1,3 +1,4 @@
 	public String toString() {
-		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism()) + "") : "" + getIdString();
+		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism())
+				+ ""Number task manager = "" + getNumTaskManager() + "") : "" + getIdString();
 	}",Buggy,"Fixed free memory calculation bug in case where multiple task manager run on the same jvm.
",Buggy,"The commit message indicates a fix for a bug in free memory calculation when multiple task managers run on the same JVM. The provided diff modifies the `toString()` method of a class, likely related to the local environment setup. The change adds the number of task managers to the string representation.

Reasoning:

1.  **Commit Message Analysis:** The commit message explicitly states a bug fix related to free memory calculation in a multi-task-manager scenario.
2.  **Diff Analysis:** The diff modifies the `toString()` method to include the number of task managers. While this change itself doesn't directly fix a memory calculation bug, it provides additional information about the environment, which could be useful for debugging or monitoring the memory usage in a multi-task-manager setup. The added information could be used to verify the fix.
3.  **Relevance:** The change is relevant to the commit message because displaying the number of task managers helps in understanding the context in which the free memory calculation is performed. This context is crucial for verifying the bug fix.

Conclusion: **Buggy**
"
flink,19363.json,7e10e2b7ba967332740e1058efa7310fe2834123,"@@ -1,22 +1,24 @@
 	public String toString()
 	{
 		if (this.indexes.size() == 0) {
 			return ""(none)"";
 		}
 		final StringBuffer buf = new StringBuffer();
 		for (int i = 0; i < indexes.size(); i++) {
 			if (buf.length() == 0) {
 				buf.append(""["");
 			}
 			else {
 				buf.append("","");
 			}
 			buf.append(this.indexes.get(i));
-			buf.append("":"");
-			buf.append(this.types.get(i).getName());
+			if (this.types.get(i) != null) {
+				buf.append("":"");
+				buf.append(this.types.get(i).getName());
+			}
 			buf.append("":"");
 			buf.append(this.orders.get(i).name());
 		}
 		buf.append(""]"");
 		return buf.toString();
 	}",Buggy,"Fixed miscellaneous minor bugs in compiler.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed miscellaneous minor bugs in compiler"" suggests that the commit addresses one or more bugs in the compiler component. The term ""minor bugs"" implies that the issues are likely small in scope and impact.

**Code Diff Analysis:**

The code diff is focused on the `toString()` method. The key change is the addition of a null check:

```java
if (this.types.get(i) != null) {
    buf.append("":"");
    buf.append(this.types.get(i).getName());
}
```

**Reasoning:**

1.  **NullPointerException Risk:** Without the null check, if `this.types.get(i)` is `null`, the code would throw a `NullPointerException` when trying to call `getName()` on it. This is a classic bug scenario.

2.  **Defensive Programming:** The added null check is a defensive programming technique. It anticipates a potential null value and prevents the program from crashing.

3.  **Bug Fix Indication:** The addition of a null check to prevent a `NullPointerException` strongly suggests that the original code had a bug where it didn't handle null values in the `types` list. The fix ensures that the `toString()` method can handle cases where `this.types.get(i)` is null without crashing.

4.  **Alignment with Commit Message:** The code change aligns with the commit message ""Fixed miscellaneous minor bugs in compiler."" A `NullPointerException` is a bug, and preventing it with a null check is a fix.

**Conclusion:**

The code diff introduces a null check to prevent a potential `NullPointerException`. This indicates that the original code had a bug related to handling null values. The commit message confirms that the commit addresses bug fixes. Therefore, the changes indicate a bug fix.

**Buggy**"
flink,4441.json,81a7837a942668c23c795e5bd8a68c4d17009f85,"@@ -1,32 +1,35 @@
 	public void close() throws IOException {
 		// atomically set the close flag
 		synchronized (this.closeLock) {
 			if (this.closed) {
 				return;
 			}
 			this.closed = true;
 			
 			try {
 				// wait until as many buffers have been returned as were written
 				// only then is everything guaranteed to be consistent.
 				while (this.requestsNotReturned.get() > 0) {
 					try {
 						// we add a timeout here, because it is not guaranteed that the
 						// decrementing during buffer return and the check here are deadlock free.
 						// the deadlock situation is however unlikely and caught by the timeout
 						this.closeLock.wait(1000);
 						checkErroneous();
 					}
 					catch (InterruptedException iex) {
 						throw new IOException(""Closing of asynchronous file channel was interrupted."");
 					}
 				}
+
+				// Additional check because we might have skipped the while loop
+				checkErroneous();
 			}
 			finally {
 				// close the file
 				if (this.fileChannel.isOpen()) {
 					this.fileChannel.close();
 				}
 			}
 		}
 	}",Buggy,"[FLINK-1545] [runtime][tests] Fixes AsynchronousFileIOChannelsTest.testExceptionForwardsToClose by introducing additional error check in AsynchronousFileIOChannel.close method

This closes #399
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""[FLINK-1545] [runtime][tests] Fixes AsynchronousFileIOChannelsTest.testExceptionForwardsToClose by introducing additional error check in AsynchronousFileIOChannel.close method"" clearly states that this commit is a bug fix. It specifically mentions a test case (`AsynchronousFileIOChannelsTest.testExceptionForwardsToClose`) that was failing and indicates that the fix involves adding an error check within the `close` method of `AsynchronousFileIOChannel`.

**Code Diff Analysis:**

The code diff shows a modification to the `close()` method of what appears to be an asynchronous file channel implementation. Let's break down the changes:

1.  **Synchronization and Closed Check:** The method uses a `closeLock` to ensure thread-safe closing and avoids redundant closing attempts via the `this.closed` flag.

2.  **Waiting for Buffer Returns:** The code waits until all outstanding buffer requests have been returned (`this.requestsNotReturned.get() > 0`). It includes a timeout within the `wait()` call and a `checkErroneous()` call within the loop. The timeout is in place to prevent potential deadlocks.

3.  **Additional Error Check:** The key change is the addition of `checkErroneous()` *after* the `while` loop. This suggests that there was a scenario where the loop could be skipped (perhaps if `this.requestsNotReturned.get()` was already 0), but an error condition still existed.

4.  **File Channel Closure:** Finally, the code closes the underlying `fileChannel` within a `finally` block to ensure it's closed even if exceptions occur.

**Reasoning:**

The commit message and code diff strongly suggest a bug fix. The commit message explicitly states it's a fix for a test case. The code diff introduces an additional error check (`checkErroneous()`) after the loop that waits for buffer returns. This implies that the original code could miss an error condition if the loop was skipped, leading to incorrect behavior or test failures. The added error check ensures that errors are always checked, even if the loop condition is not met. The timeout within the `while` loop also hints at potential concurrency issues that might have been contributing to the bug.

**Conclusion:**

**Buggy**
"
flink,25651.json,ce822bf7f5ec80df5d5a749b1439320af3fb8b18,"@@ -1,4 +1,7 @@
 	public <R> DeltaIteration<T, R> iterateDelta(DataSet<R> workset, int maxIterations, int... keyPositions) {
+		Preconditions.checkNotNull(workset);
+		Preconditions.checkNotNull(keyPositions);
+		
 		Keys.ExpressionKeys<T> keys = new Keys.ExpressionKeys<T>(keyPositions, getType(), false);
 		return new DeltaIteration<T, R>(getExecutionEnvironment(), getType(), this, workset, keys, maxIterations);
 	}",Buggy,"[FLINK-1254] [compiler] Fix compiler bug for pipeline breaker placement

This closes #216
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""[FLINK-1254] [compiler] Fix compiler bug for pipeline breaker placement"" explicitly states that the commit is intended to fix a bug in the Flink compiler related to pipeline breaker placement. This strongly suggests a bug fix.

2.  **Diff Analysis:** The diff shows the addition of `Preconditions.checkNotNull(workset)` and `Preconditions.checkNotNull(keyPositions)` at the beginning of the `iterateDelta` method. These checks ensure that the `workset` and `keyPositions` parameters are not null.

3.  **Relevance and Bug Fix Indication:** The added null checks are a defensive programming measure. Without these checks, if `workset` or `keyPositions` were null, a `NullPointerException` would occur at some later point in the code, potentially causing the compiler to fail or produce incorrect results related to pipeline breaker placement (as suggested by the commit message). The addition of these checks prevents this potential error, which aligns with the commit message's intention to fix a compiler bug.

**Conclusion:**

The commit message explicitly mentions a bug fix, and the code changes introduce null checks that prevent a potential `NullPointerException` which could cause the compiler to fail. This strongly suggests that the changes are indeed a bug fix.

**Buggy**"
flink,18032.json,1bfeeaead158051cb5f78f403c4878fc89596cc8,"@@ -1,23 +1,21 @@
 	public void write(final DataOutput out) throws IOException {
 		final int maxBit = 0x1 << 7;
 
 		int len = this.value.length();
 
 		while (len >= maxBit) {
 			out.write(len | maxBit);
 			len >>= 7;
 		}
 		out.write(len);
 
 		for (int i = 0; i < this.value.length(); i++) {
 			int c = this.value.charAt(i);
 
-			if (c < maxBit)
-				out.write(c);
-			else
-				while (c >= maxBit) {
-					out.write(c | maxBit);
-					c >>= 7;
-				}
+			while (c >= maxBit) {
+				out.write(c | maxBit);
+				c >>= 7;
+			}
+			out.write(c);
 		}
 	}",Buggy,"Fixed Serialization Bug in PactString
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed Serialization Bug in PactString"" clearly states that the commit addresses a bug related to the serialization of `PactString` objects. This strongly suggests that the code changes are intended to correct incorrect or faulty behavior during the serialization process.

**Code Diff Analysis:**

The code diff focuses on the `write` method, which is responsible for writing the `PactString`'s value to a `DataOutput` stream. The original code had a conditional statement `if (c < maxBit)` that seems to handle characters within a certain range differently. The `else` block handles characters outside that range.

The modified code removes the `if (c < maxBit)` condition and directly enters a `while` loop to handle all characters. This loop continues as long as the character `c` is greater than or equal to `maxBit`. Inside the loop, it writes the character's value with the `maxBit` set and then right-shifts the character by 7 bits. After the loop, it writes the remaining bits of the character.

The original code's logic appears flawed. It seems to assume that characters less than `maxBit` (128) can be written directly, while larger characters need to be split into multiple bytes. However, the splitting logic was only applied to characters *greater than or equal to* `maxBit`. This could lead to incorrect serialization of characters in the range [128, 255] if the intention was to encode all characters greater than or equal to 128 using a variable-length encoding scheme.

The modified code ensures that all characters are handled using the variable-length encoding scheme, regardless of their initial value. This is a more consistent and likely correct approach. The removal of the conditional statement and the universal application of the `while` loop suggest a correction of a logical error in the original serialization logic.

**Reasoning:**

1.  **Commit Message Alignment:** The commit message explicitly mentions a ""Serialization Bug,"" which aligns with the code changes in the `write` method, a core part of the serialization process.
2.  **Logical Correction:** The original code had a conditional statement that could lead to inconsistent serialization behavior. The modified code removes this condition and applies a consistent encoding scheme to all characters, indicating a correction of a logical error.
3.  **Bug Fix Indication:** The changes address a potential flaw in how characters were serialized, which could lead to data corruption or incorrect deserialization. This strongly suggests that the commit fixes a bug.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix related to the serialization of `PactString` objects. The original code had a logical flaw that could lead to incorrect serialization, and the modified code corrects this flaw by applying a consistent encoding scheme.

**Buggy**
"
flink,22768.json,3854552ceefd2b2b9c0e2a9b6152a7fcb69153fe,"@@ -1,26 +1,32 @@
 	public static String unresolvedHostToNormalizedString(String host) {
 		// Return loopback interface address if host is null
 		// This represents the behavior of {@code InetAddress.getByName } and RFC 3330
 		if (host == null) {
 			host = InetAddress.getLoopbackAddress().getHostAddress();
 		} else {
 			host = host.trim().toLowerCase();
+			if (host.startsWith(""["") && host.endsWith(""]"")) {
+				String address = host.substring(1, host.length() - 1);
+				if (IPAddressUtil.isIPv6LiteralAddress(address)) {
+					host = address;
+				}
+			}
 		}
 
 		// normalize and valid address
 		if (IPAddressUtil.isIPv6LiteralAddress(host)) {
 			byte[] ipV6Address = IPAddressUtil.textToNumericFormatV6(host);
 			host = getIPv6UrlRepresentation(ipV6Address);
 		} else if (!IPAddressUtil.isIPv4LiteralAddress(host)) {
 			try {
 				// We don't allow these in hostnames
 				Preconditions.checkArgument(!host.startsWith("".""));
 				Preconditions.checkArgument(!host.endsWith("".""));
 				Preconditions.checkArgument(!host.contains("":""));
 			} catch (Exception e) {
 				throw new IllegalConfigurationException(""The configured hostname is not valid"", e);
 			}
 		}
 
 		return host;
 	}",Buggy,"[FLINK-12840] [core] Fix network utils to work with ipv6 correctly

  - Fixes problems around akka configuration parsing with some IPv6 literals
  - Fixes an issue with address parsing and validation with some Ipv6 literals

This closes #8734
",Buggy,"The commit message indicates fixes related to IPv6 address handling in network utilities, specifically addressing issues with Akka configuration parsing and address validation. The code diff modifies the `unresolvedHostToNormalizedString` method to handle IPv6 literals correctly.

Here's a breakdown of the code changes:

1.  **IPv6 Literal Handling:** The code now checks if the host string is enclosed in square brackets (`[]`). If so, it extracts the address within the brackets and validates if it's a valid IPv6 literal address using `IPAddressUtil.isIPv6LiteralAddress`. If it is, the `host` variable is updated to contain the address without the brackets. This addresses the issue of parsing IPv6 literals enclosed in brackets, which is a common notation.

2.  **Normalization and Validation:** The code continues to normalize and validate the address. If the host is an IPv6 literal address, it converts it to a URL representation using `getIPv6UrlRepresentation`. It also checks if the host is a valid IPv4 literal address. If not, it performs additional checks to ensure the hostname doesn't start or end with a dot (`.`) or contain a colon (`:`).

The changes directly address the issues mentioned in the commit message. The code now correctly parses and validates IPv6 addresses, which likely caused problems with Akka configuration parsing and address validation. The addition of IPv6 literal handling and the subsequent normalization and validation steps suggest that the previous implementation had bugs related to IPv6 address processing.

Therefore, the changes indicate a bug fix.
**Buggy**"
flink,28054.json,6e5954e8a03ad5d440447a57098976b0250f4f72,"@@ -1,43 +1,45 @@
 	private RexNode convertOver(List<Expression> children) {
 		List<Expression> args = children;
 		Expression agg = args.get(0);
 		SqlAggFunction aggFunc = agg.accept(new SqlAggFunctionVisitor(typeFactory));
 		RelDataType aggResultType = typeFactory.createFieldTypeFromLogicalType(
 				fromDataTypeToLogicalType(((ResolvedExpression) agg).getOutputDataType()));
 
 		// assemble exprs by agg children
 		List<RexNode> aggExprs = agg.getChildren().stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 
 		// assemble order by key
 		Expression orderKeyExpr = args.get(1);
 		Set<SqlKind> kinds = new HashSet<>();
 		RexNode collationRexNode = createCollation(orderKeyExpr.accept(this), RelFieldCollation.Direction.ASCENDING,
 				null, kinds);
 		ImmutableList<RexFieldCollation> orderKey = ImmutableList
 				.of(new RexFieldCollation(collationRexNode, kinds));
 
 		// assemble partition by keys
 		List<RexNode> partitionKeys = args.subList(4, args.size()).stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 		// assemble bounds
 		Expression preceding = args.get(2);
-		boolean isPhysical = ((ResolvedExpression) preceding).getOutputDataType().equals(DataTypes.BIGINT());
+		boolean isPhysical = LogicalTypeChecks.hasRoot(
+				fromDataTypeToLogicalType(((ResolvedExpression) preceding).getOutputDataType()),
+				LogicalTypeRoot.BIGINT);
 		Expression following = args.get(3);
 		RexWindowBound lowerBound = createBound(preceding, SqlKind.PRECEDING);
 		RexWindowBound upperBound = createBound(following, SqlKind.FOLLOWING);
 
 		// build RexOver
 		return relBuilder.getRexBuilder().makeOver(
 				aggResultType,
 				aggFunc,
 				aggExprs,
 				partitionKeys,
 				orderKey,
 				lowerBound,
 				upperBound,
 				isPhysical,
 				true,
 				false,
 				false);
 	}",Buggy,"[FLINK-13107][table-planner-blink] Fix Bug to check whether OverCall is RowMode or RangeMode.
",Buggy,"The commit message indicates a bug fix related to checking whether an OverCall is in RowMode or RangeMode. The code diff modifies the `convertOver` method, specifically the logic for determining if the window is physical (RowMode) or logical (RangeMode).

The original code directly compared the output data type of the `preceding` expression with `DataTypes.BIGINT()`. The modified code uses `LogicalTypeChecks.hasRoot` to check if the root logical type of the `preceding` expression's output data type is `LogicalTypeRoot.BIGINT`. This change suggests that the original code was too strict and didn't handle cases where the data type was a more specific type derived from BIGINT. The new code is more robust and correctly identifies RowMode in a broader range of scenarios.

Therefore, the code change fixes a bug related to incorrect mode detection in OverCalls.

**Buggy**"
flink,22909.json,b01641bcc13631b0db82c54143670613babb7c0c,"@@ -1,79 +1,79 @@
 	public NumberSequenceIterator[] split(int numPartitions) {
 		if (numPartitions < 1) {
 			throw new IllegalArgumentException(""The number of partitions must be at least 1."");
 		}
 		
 		if (numPartitions == 1) {
 			return new NumberSequenceIterator[] { new NumberSequenceIterator(current, to) };
 		}
 		
 		// here, numPartitions >= 2 !!!
 		
 		long elementsPerSplit;
 		
-		if (to - current >= 0) {
-			elementsPerSplit = (to - current) / numPartitions;
+		if (to - current + 1 >= 0) {
+			elementsPerSplit = (to - current + 1) / numPartitions;
 		}
 		else {
 			// long overflow of the range.
 			// we compute based on half the distance, to prevent the overflow.
 			// in most cases it holds that: current < 0 and to > 0, except for: to == 0 and current == Long.MIN_VALUE
 			// the later needs a special case
 			final long halfDiff; // must be positive
 			
 			if (current == Long.MIN_VALUE) {
 				// this means to >= 0
 				halfDiff = (Long.MAX_VALUE/2+1) + to/2;
 			} else {
 				long posFrom = -current;
 				if (posFrom > to) {
 					halfDiff = to + ((posFrom - to) / 2);
 				} else {
 					halfDiff = posFrom + ((to - posFrom) / 2);
 				}
 			}
 			elementsPerSplit = halfDiff / numPartitions * 2;
 		}
 		
 		if (elementsPerSplit < Long.MAX_VALUE) {
 			// figure out how many get one in addition
-			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current;
+			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current + 1;
 			
 			// based on rounding errors, we may have lost one)
 			if (numWithExtra > numPartitions) {
 				elementsPerSplit++;
 				numWithExtra -= numPartitions;
 				
 				if (numWithExtra > numPartitions) {
 					throw new RuntimeException(""Bug in splitting logic. To much rounding loss."");
 				}
 			}
 			
 			NumberSequenceIterator[] iters = new NumberSequenceIterator[numPartitions];
 			long curr = current;
 			int i = 0;
 			for (; i < numWithExtra; i++) {
 				long next = curr + elementsPerSplit + 1;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1);
 				curr = next;
 			}
 			for (; i < numPartitions; i++) {
 				long next = curr + elementsPerSplit;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1, true);
 				curr = next;
 			}
 			
 			return iters;
 		}
 		else {
 			// this can only be the case when there are two partitions
 			if (numPartitions != 2) {
 				throw new RuntimeException(""Bug in splitting logic."");
 			}
 			
 			return new NumberSequenceIterator[] {
 				new NumberSequenceIterator(current, current + elementsPerSplit),
 				new NumberSequenceIterator(current + elementsPerSplit, to)
 			};
 		}
 	}",Buggy,"Fixed bug in splitting logic of number sequence iterator.
",Buggy,"The commit message indicates a bug fix in the splitting logic of a number sequence iterator. The diff shows changes in the calculation of `elementsPerSplit` and `numWithExtra`, as well as adjustments to the `next` value in the loops that create the iterators. Specifically, `to - current` is changed to `to - current + 1` in two places, and `next` is decremented by 1 when creating `NumberSequenceIterator`. These changes suggest an attempt to correct an off-by-one error in the splitting logic, which aligns with the commit message.

**Buggy**"
flink,36898.json,6dcf74f022f4a127260b0e8d52b772fb28fa2249,"@@ -1,5 +1,10 @@
 	public void setCosts(Costs nodeCosts) {
-		// do not account for any cost, regardless of what the estimator
-		// calculates for our shipping strategies
-		super.setCosts(new Costs());
+		// the plan enumeration logic works as for regular two-input-operators, which is important
+		// because of the branch handling logic. it does pick redistributing network channels
+		// between the sink and the sink joiner, because sinks joiner has a different DOP than the sink.
+		// we discard any cost and simply use the sum of the costs from the two children.
+		
+		Costs totalCosts = getInput1().getSource().getCumulativeCosts().clone();
+		totalCosts.addCosts(getInput2().getSource().getCumulativeCosts());
+		super.setCosts(totalCosts);
 	}",Buggy,"Fixed erroneous costs handling in sink joiner.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed erroneous costs handling in sink joiner"" explicitly states that the change addresses an error in how costs were being handled within the `sink joiner`. This strongly suggests a bug fix.

2.  **Code Diff Analysis:**
    *   The original code `super.setCosts(new Costs());` was unconditionally setting the costs to a new, empty `Costs` object. This means the `sink joiner` was ignoring any cost estimations from its inputs or shipping strategies.
    *   The modified code calculates `totalCosts` by cloning the cumulative costs from `getInput1().getSource()` and adding the cumulative costs from `getInput2().getSource()`. Then, it sets the `super.setCosts` to the `totalCosts`.
    *   The comment in the modified code explains why the costs are handled this way. It emphasizes that the plan enumeration logic relies on the standard two-input-operator behavior, especially for branch handling. It also acknowledges that network channels might be added between the sink and sink joiner due to different DOPs (degrees of parallelism). The code discards any cost and simply use the sum of the costs from the two children.

3.  **Alignment:** The code changes directly address the issue described in the commit message. The original code was discarding costs, which is clearly an error. The new code calculates and sets the costs based on the cumulative costs of the inputs, which is a more reasonable and likely correct approach.

4.  **Bug Fix Indicators:**
    *   The commit message uses the word ""Fixed,"" which is a strong indicator of a bug fix.
    *   The code change corrects an incorrect calculation (or rather, the complete omission) of costs.
    *   The added comment clarifies the rationale behind the cost calculation, suggesting a deeper understanding of the issue being addressed.

**Conclusion:**

Based on the commit message and the code diff, the changes clearly indicate a bug fix related to the erroneous handling of costs in the sink joiner.

**Buggy**"
flink,9963.json,750325e74c1cd91006b251f18f3ed8acf90c2a91,"@@ -1,3 +1,8 @@
 		public void jobLeaderLostLeadership(final JobID jobId, final UUID oldJobLeaderId) {
-			ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+			runAsync(new Runnable() {
+				@Override
+				public void run() {
+					ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+				}
+			});
 		}",Buggy,"[FLINK-5893] [RM] Fix the bug of race condition for removing previous JobManagerRegistration in ResourceManager

This closes #3399.
",Buggy,"The commit message indicates a fix for a race condition when removing a previous JobManagerRegistration in the ResourceManager. The code diff wraps the call to `ResourceManager.this.jobLeaderLostLeadership` within a `runAsync` block. This suggests that the original call might have been happening in a context where it could interfere with other operations, leading to the race condition. By making it asynchronous, the operation is deferred and executed in a different thread, potentially resolving the race condition. This change directly relates to addressing the bug described in the commit message.

**Buggy**"
flink,14028.json,0f88c392b6ad6f91aed33e157bdc9df6f613f09d,"@@ -1,3 +1,3 @@
 	public TableSource<Row> projectFields(int[] fields) {
-		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, null);
+		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, predicate);
 	}",Buggy,"[FLINK-15361][parquet] ParquetTableSource should pass predicate in projectFields

fix the problem, when after projectFields, ParquetTableSource will loose predicates.

this closes #10660.
",Buggy,"The commit message indicates a fix for a bug where `ParquetTableSource` loses predicates after projecting fields. The code diff shows that the `projectFields` method in `ParquetTableSource` was modified to pass the `predicate` to the new `ParquetTableSource` instance. Previously, the `predicate` was set to `null` after projection. This change aligns with the commit message and fixes the bug where predicates were lost after projecting fields.

Reasoning:
1. The commit message explicitly states that the change fixes a bug related to predicate loss in `ParquetTableSource` after projecting fields.
2. The code diff shows that the `projectFields` method was modified to pass the `predicate` to the new `ParquetTableSource` instance.
3. The previous implementation was setting the `predicate` to `null` after projection, which caused the bug.
4. The current change fixes the bug by passing the `predicate` to the new `ParquetTableSource` instance.

Conclusion: **Buggy**"
flink,27787.json,37f67b7d394ccb3355ebc995af1c3fee04ce060f,"@@ -1,8 +1,8 @@
 	public Expression[] accumulateExpressions() {
 		Expression[] accExpressions = new Expression[1 + operands().length];
-		// sequence = if (lastValues equalTo orderKeys) sequence else sequence + 1
-		accExpressions[0] = ifThenElse(orderKeyEqualsExpression(), sequence, plus(sequence, literal(1L)));
+		// sequence = if (lastValues equalTo orderKeys and sequence != 0) sequence else sequence + 1
+		accExpressions[0] = ifThenElse(and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L)))), sequence, plus(sequence, literal(1L)));
 		Expression[] operands = operands();
 		System.arraycopy(operands, 0, accExpressions, 1, operands.length);
 		return accExpressions;
 	}",Buggy,"[FLINK-14053][table-planner-blink] Fix DenseRankAggFunction first row bug.

We should consider the possibility that first row's order by key is equal to the initial last value.

This closes #9966
",Buggy,"The commit message indicates a bug fix in `DenseRankAggFunction` related to the first row's order by key. The provided code diff modifies the `accumulateExpressions` method, specifically the logic for calculating the sequence.

The original code:
```java
accExpressions[0] = ifThenElse(orderKeyEqualsExpression(), sequence, plus(sequence, literal(1L)));
```
This logic increments the sequence unless the current `orderKey` is equal to the `lastValues`.

The modified code:
```java
accExpressions[0] = ifThenElse(and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L)))), sequence, plus(sequence, literal(1L)));
```
The modified logic adds an additional condition: the sequence must not be equal to 0. This suggests that the original logic had a flaw where the sequence was not being incremented correctly for the first row when the order key was equal to the initial last value (likely due to initialization). The added condition `not(equalTo(sequence, literal(0L)))` ensures that the sequence is incremented for the first row even if the order key matches the initial last value, which aligns with the bug described in the commit message.

The change addresses a specific edge case related to the first row and its impact on the sequence calculation. This strongly indicates a bug fix.

**Buggy**"
flink,7316.json,c59f4836fecc4069b2cffeae8dd81e50ea5d5e73,"@@ -1,4 +1,4 @@
 	public String getDescription() {
-		return ""String value that specifies the termination mode. Supported values are: "" +
-			StringUtils.toQuotedListString(TerminationMode.values()) + '.';
+		return ""String value that specifies the termination mode. The only supported value is: \"""" +
+			TerminationMode.CANCEL.name().toLowerCase() + ""\""."";
 	}",Buggy,"[FLINK-13136][docs] Fix documentation error about stopping job with restful api

This closes #9013.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message `[FLINK-13136][docs] Fix documentation error about stopping job with restful api` clearly states that the purpose of the commit is to correct an error in the documentation related to stopping a Flink job using the RESTful API.
    *   The phrase ""Fix documentation error"" strongly suggests that there was an inaccuracy or mistake in the documentation that needed to be rectified.
    *   The reference to ""stopping job with restful api"" provides context, indicating that the error was specifically related to the documentation on how to stop a Flink job via the REST API.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `getDescription()` method.
    *   The original description stated that multiple `TerminationMode` values were supported.
    *   The modified description states that only `TerminationMode.CANCEL` is supported.
    *   This change directly addresses the documentation error mentioned in the commit message. It corrects the documentation to accurately reflect the supported termination mode.

3.  **Alignment and Bug Fix Indication:**
    *   The commit message and the diff are perfectly aligned. The commit message describes a documentation fix, and the diff shows the actual correction in the documentation.
    *   The original documentation was *incorrect* because it claimed that multiple termination modes were supported, while the code only supports `CANCEL`. This discrepancy constitutes a bug in the documentation.
    *   The change corrects this error, making the documentation accurate.

**Conclusion:**

The commit message and the code changes clearly indicate a bug fix in the documentation. The original documentation contained an error regarding the supported termination modes, and the commit corrects this error.

**Buggy**"
flink,13283.json,2cb7bb96001f9780a27a880245382958448151c4,"@@ -1,3 +1,5 @@
 	public void setItemCount(long itemCount) throws IOException {
-		writeVarLongCount(out, itemCount);
+		if (itemCount > 0) {
+			writeVarLongCount(out, itemCount);
+		}
 	}",Buggy,"Fix bug in avro serialization for empty collections.
Add additional test for avro.
",Buggy,"The commit message indicates a bug fix related to Avro serialization of empty collections and includes a new test case for Avro.

The code diff shows a modification in the `setItemCount` method. It adds a condition to check if `itemCount` is greater than 0 before calling `writeVarLongCount`. This suggests that the original code might have had an issue when `itemCount` was 0 or negative, potentially leading to incorrect serialization or errors. The added check prevents `writeVarLongCount` from being called with a non-positive `itemCount`, which aligns with fixing a bug related to empty collections.

Therefore, the changes indicate a bug fix.

**Buggy**
"
flink,34466.json,a86bce5a176144e06d0120b804f3af986c325ebf,"@@ -1,18 +1,18 @@
 	private static int getMillis(String dateStr) {
 		int length = dateStr.length();
 		if (length == 19) {
 			// ""1999-12-31 12:34:56"", no milli second left
 			return 0;
 		} else if (length == 21) {
 			// ""1999-12-31 12:34:56.7"", return 7
 			return Integer.parseInt(dateStr.substring(20)) * 100;
 		} else if (length == 22) {
 			// ""1999-12-31 12:34:56.78"", return 78
 			return Integer.parseInt(dateStr.substring(20)) * 10;
 		} else if (length >= 23 && length <= 26) {
 			// ""1999-12-31 12:34:56.123"" ~ ""1999-12-31 12:34:56.123456""
-			return Integer.parseInt(dateStr.substring(20, 23)) * 10;
+			return Integer.parseInt(dateStr.substring(20, 23));
 		} else {
 			return 0;
 		}
 	}",Buggy,"[FLINK-12553][table-runtime-blink] Fix bug that SqlDateTimeUtils#parseToTimeMillis doesn't parse millisecond correctly

This closes #8483
",Buggy,"The commit message indicates a bug fix in `SqlDateTimeUtils#parseToTimeMillis` related to parsing milliseconds. The code diff focuses on the `getMillis` method, which extracts milliseconds from a date string. The original code had a multiplication by 10 for the case where the length of the millisecond string was between 3 and 6 characters. This multiplication is removed in the corrected code. This change directly addresses the bug described in the commit message, ensuring that milliseconds are parsed correctly without unnecessary scaling.

**Buggy**"
flink,22688.json,7b6b5a2e019866bf8fdd993775eab410e22f0f5d,"@@ -1,11 +1,14 @@
 	public static Object deserializeObject(byte[] bytes, ClassLoader cl) throws IOException, ClassNotFoundException {
 		ObjectInputStream oois = null;
+		final ClassLoader old = Thread.currentThread().getContextClassLoader();
 		try {
+			Thread.currentThread().setContextClassLoader(cl);
 			oois = new ClassLoaderObjectInputStream(new ByteArrayInputStream(bytes), cl);
 			return oois.readObject();
 		} finally {
+			Thread.currentThread().setContextClassLoader(old);
 			if (oois != null) {
 				oois.close();
 			}
 		}
 	}",Buggy,"Fixes a bug where Thread.currentThread().getContextClassLoader() does not return the user code class loader within object deserialization.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message states: ""Fixes a bug where Thread.currentThread().getContextClassLoader() does not return the user code class loader within object deserialization."" This clearly indicates a bug fix related to class loader management during object deserialization. The problem is that the context class loader was not correctly set to the user's class loader, which could lead to class loading issues during deserialization.

**Git Diff Analysis:**

The diff shows the `deserializeObject` method being modified. Here's a breakdown of the changes:

1.  `final ClassLoader old = Thread.currentThread().getContextClassLoader();`: This line saves the current thread's context class loader before any changes are made. This is crucial for restoring the original class loader after the deserialization process is complete.

2.  `Thread.currentThread().setContextClassLoader(cl);`: This line sets the thread's context class loader to the provided `cl` (ClassLoader). This is the core of the fix, ensuring that the deserialization process uses the correct class loader.

3.  `Thread.currentThread().setContextClassLoader(old);`: This line, within the `finally` block, restores the original context class loader. This is essential to prevent unintended side effects on other parts of the application that might rely on the context class loader.

**Relevance and Bug Fix Indication:**

The code changes directly address the issue described in the commit message. The changes ensure that the correct class loader is used during deserialization and that the original class loader is restored afterward. This pattern of saving the old value, setting a new value, and restoring the old value in a `finally` block is a common pattern when working with thread context class loaders and other thread-local variables. The changes are clearly intended to fix the class loading issue during deserialization.

**Conclusion:**

The commit message and the code changes are highly relevant and indicate a bug fix. The code modifies the thread's context class loader to use the user-provided class loader during deserialization, which directly addresses the problem described in the commit message.

**Buggy**
"
flink,22189.json,e28b62e0e2973b01ad5b08ce319aaf0e7ce4c087,"@@ -1,11 +1,11 @@
 	public MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) {
-		return new MethodVisitor(Opcodes.ASM4) {
+		return new MethodVisitor(Opcodes.ASM5) {
 
 			@Override
 			public void visitFieldInsn(int op, String owner, String name, String desc) {
 				if (op == Opcodes.GETFIELD && name.equals(this0Name)) {
 					isThis0Accessed = true;
 				}
 			}
 		};
 	}",Buggy,"[FLINK-3143] update Closure Cleaner's ASM references to ASM5

- This solves errors with reflectasm using Scala 2.11 and Java 8

This closes #1445.
",Buggy,"The commit message indicates an update to the ASM (a Java bytecode manipulation and analysis framework) references from an older version to ASM5 within the Closure Cleaner. The stated reason is to resolve errors encountered when using reflectasm with Scala 2.11 and Java 8.

The provided code diff shows a change from `Opcodes.ASM4` to `Opcodes.ASM5` within the `visitMethod` method of a class related to bytecode manipulation. This aligns perfectly with the commit message's intention to update ASM references. The commit message explicitly mentions resolving errors, which strongly suggests that the previous ASM4 reference was causing issues, thus indicating a bug fix.

**Buggy**"
flink,36539.json,73b5b3dd81e2a146592d9623f44ceff3d8c035fa,"@@ -1,6 +1,6 @@
 	public void setDegreeOfParallelism(int degreeOfParallelism) {
 		if (degreeOfParallelism < 1) {
-			throw new IllegalArgumentException();
+			throw new IllegalArgumentException(""Degree of parallelism of "" + degreeOfParallelism + "" is invalid."");
 		}
 		this.degreeOfParallelism = degreeOfParallelism;
 	}",Buggy,"Fix error with invalid config values for degree of parallelism.
",Buggy,"The commit message indicates a fix related to invalid configuration values for the degree of parallelism. The code diff shows that an `IllegalArgumentException` is thrown when the `degreeOfParallelism` is less than 1. The exception message has been updated to provide more context about the invalid value. This change aligns with the commit message, as it addresses the handling of invalid configuration values. The original code likely threw a generic `IllegalArgumentException` without specifying the reason, which has been improved with a descriptive message. This indicates a bug fix related to error handling and input validation.

**Buggy**"
flink,1954.json,f28c28643df43633439b2c99c383e1ed01319ea3,"@@ -1,9 +1,9 @@
 	public <T extends Value> Class<? extends ConvergenceCriterion<T>> getConvergenceCriterion() {
 		@SuppressWarnings(""unchecked"")
-		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) 
+		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) (Class<?>) 
 							this.config.getClass(ITERATION_CONVERGENCE_CRITERION, null, ConvergenceCriterion.class);
 		if (clazz == null) {
 			throw new NullPointerException();
 		}
 		return clazz;
 	}",Buggy,"Fixed compiler error in maven for TaskConfig.java
",Buggy,"The commit message indicates a fix for a compiler error in `TaskConfig.java`. The diff shows a change in the type casting of `Class<? extends ConvergenceCriterion<T>>`. Specifically, it adds an intermediate cast to `(Class<?>)` before casting to `Class<? extends ConvergenceCriterion<T>>`. This pattern is often used to suppress unchecked cast warnings or to work around limitations in Java's type system, which can arise from type erasure and generics. The original code likely produced a compiler warning or error due to the direct cast. The added cast addresses this by effectively telling the compiler to ignore the type safety at that point, which is then followed by a cast to the desired type. This suggests the original code had a type-related issue that prevented compilation, which is indeed a bug.

**Buggy**"
flink,2537.json,5e498dc9e763a5daa3867456f657e19cd08fbb66,"@@ -1,13 +1,11 @@
-		private void handleCompletedFuture(T value, Throwable throwable) {
+		private void handleCompletedFuture(int index, T value, Throwable throwable) {
 			if (throwable != null) {
 				completeExceptionally(throwable);
 			} else {
-				int index = nextIndex.getAndIncrement();
-
 				results[index] = value;
 
 				if (numCompleted.incrementAndGet() == numTotal) {
 					complete(Arrays.asList(results));
 				}
 			}
 		}",Buggy,"[FLINK-12021] Deploy execution in topological sorted order

Due to changes how the slot futures are completed and due to the fact that the
ResultConjunctFuture does not maintain the order in which the futures were specified,
it could happen that executions were not deployed in topological order. This commit
fixes this problem by changing the ResultConjunctFuture so that it maintains the order
of the specified futures in its result collection.

This closes #8060.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""[FLINK-12021] Deploy execution in topological sorted order"" indicates a fix related to the order of execution deployment.
    *   It explicitly mentions that executions were not being deployed in topological order due to changes in how slot futures were completed and the behavior of `ResultConjunctFuture`.
    *   The message states that the `ResultConjunctFuture` was not maintaining the order of futures, and the commit aims to fix this by modifying `ResultConjunctFuture` to preserve the order.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `handleCompletedFuture` method. The original method `private void handleCompletedFuture(T value, Throwable throwable)` is modified to `private void handleCompletedFuture(int index, T value, Throwable throwable)`.
    *   The original method used `nextIndex.getAndIncrement()` to determine the index at which to store the result. This implies that the order of completion determined the index, not the original order.
    *   The modified method now receives an `index` as an argument, which suggests that the index is now being explicitly passed in, likely representing the original order of the futures.
    *   The `results[index] = value;` line confirms that the value is now being stored at the provided `index`.

3.  **Correlation and Bug Fix Indication:**
    *   The commit message and the code diff are strongly correlated. The commit message describes an ordering issue in execution deployment, and the code diff modifies the `ResultConjunctFuture` to maintain the order of futures.
    *   The change in `handleCompletedFuture` to accept an `index` and use it to store the result directly addresses the problem described in the commit message. This indicates a bug fix related to incorrect ordering.

**Conclusion:**

The commit message and the code diff are highly relevant to each other and indicate a bug fix related to the order of execution deployment. The modification ensures that the `ResultConjunctFuture` maintains the original order of futures, resolving the described ordering issue.

**Buggy**
"
eclipseJdt,3793.json,2f6e64cbbbc83dfd41e2988704a66b1a58193e8d,"@@ -1,3 +1,3 @@
 	public boolean isTypeAccess() {
-		return false;
+		return !this.haveReceiver;
 	}",Buggy,"Fixed Bug 424226 - [1.8] Cannot use static method from an interface in
static method reference ",Buggy,"The commit message indicates a bug fix related to using static methods from an interface in static method references in Java 8. The code diff changes the return value of the `isTypeAccess()` method from `false` to `!this.haveReceiver`. This suggests that the original implementation was incorrect and always returned `false`, which might have caused the bug described in the commit message. The change to `!this.haveReceiver` likely fixes the logic to correctly determine if it is a type access.

**Buggy**
"
eclipseJdt,651.json,969e112c287880aaa166b5c77a56bd62cba0db4b,"@@ -1,7 +1,11 @@
 private static IJavaProject getJavaProject(IPath path, IJavaModel model) {
-	IJavaProject project = model.getJavaProject(path.lastSegment());
+	String lastSeg = path.lastSegment();
+	if (lastSeg == null) {
+		lastSeg = path.toOSString();
+	}
+	IJavaProject project = model.getJavaProject(lastSeg);
 	if (project.exists()) {
 		return project;
 	}
 	return null;
 }",Buggy,"Fix for Bug 464339 When finding references, Java Search fails with
NullPointerException",Buggy,"Okay, let's analyze this Git diff and commit message.

**1. Understanding the Commit Message:**

The commit message ""Fix for Bug 464339 When finding references, Java Search fails with NullPointerException"" clearly indicates a bug fix. The bug is a `NullPointerException` occurring during Java Search when finding references. The commit aims to resolve this specific issue.

**2. Analyzing the Code Diff:**

The code diff modifies the `getJavaProject` method.  The original code directly used `path.lastSegment()` to get the project name.  The modified code adds a null check for `path.lastSegment()`. If `path.lastSegment()` returns `null`, the code now uses `path.toOSString()` as a fallback.

**3. Reasoning:**

*   **NullPointerException Cause:** The original code's direct use of `path.lastSegment()` without a null check was likely the cause of the `NullPointerException`. If `path.lastSegment()` returned `null` (which can happen in certain scenarios, such as a malformed or unexpected path), it would lead to a `NullPointerException` when passed to `model.getJavaProject()`.
*   **Bug Fix:** The added null check and the fallback to `path.toOSString()` directly address the potential `NullPointerException`. This is a classic bug fix scenario where a missing null check is added to prevent a crash.
*   **Relevance:** The code change is directly relevant to the commit message. The commit message describes a `NullPointerException`, and the code change adds a null check to prevent it.

**4. Conclusion:**

The commit message explicitly states a bug fix for a `NullPointerException`. The code diff introduces a null check that directly addresses the potential cause of the exception. Therefore, this is a bug fix.

**Buggy**
"
eclipseJdt,20760.json,5666cb170a5113b959f09f9a106548eceb94f3e9,"@@ -1,17 +1,23 @@
 	private boolean isPrimaryType(String name, IType type, boolean partialMatch) {
 		/*
 		 * Please have a look at: NameLookup#NameLookup
 		 * The HashTable this.typesInWorkingCopies contains values which are HashTables themselves.
 		 * The values of these HashTables are either of IType or IType[].
 		 * These values are types belonging to a compilation unit. Please check:
 		 * CompilationUnit#getTypes().
 		 * Therefore the parents of these types would be compilation units.
 		 */
 		ICompilationUnit cu = (ICompilationUnit) type.getParent();
 		String cuName = cu.getElementName().substring(0, cu.getElementName().lastIndexOf('.'));
+		/*
+		 * Secondary types along with primary types have their parent as the compilation unit.
+		 * The names of the primary type would match with their compilation unit.
+		 */
+		if (!cuName.equals(type.getElementName()))
+			return false;
 		if (partialMatch) {
 			return cuName.regionMatches(0, name, 0, name.length());
 		} else {
 			return cuName.equals(name);
 		}
 	}",Buggy,"Fixed Bug 431501 - NameLookup#findType API finds secondary types with
secondaryType and partial match set

Change-Id: Iad7904647729b2f282d039856b6ecee5325ebedb
Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed Bug 431501 - NameLookup#findType API finds secondary types with secondaryType and partial match set"" clearly states that this commit is intended to fix a bug. The bug involves the `NameLookup#findType` API incorrectly finding secondary types when specific conditions (secondaryType and partial match are set) are met. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff modifies the `isPrimaryType` method. Here's a breakdown:

1.  **Context:** The code is within a method that seems to determine if a given `IType` is a primary type within a compilation unit. The surrounding comments explain the structure of how types are stored and retrieved.

2.  **The Change:** The core change is the addition of this `if` statement:

    ```java
    if (!cuName.equals(type.getElementName()))
        return false;
    ```

    *   `cuName`: The name of the compilation unit (without the extension).
    *   `type.getElementName()`: The name of the type.

    This condition checks if the compilation unit name is equal to the type's element name. If they are *not* equal, the method immediately returns `false`.

3.  **Reasoning:** The comment added above the `if` statement explains the logic: ""Secondary types along with primary types have their parent as the compilation unit. The names of the primary type would match with their compilation unit."" This implies that secondary types have a different naming convention where their element name doesn't match the compilation unit name. The added check ensures that only primary types (where the names match) are considered when this method is called.

**Connecting the Dots:**

The commit message describes a bug where `NameLookup#findType` incorrectly identifies secondary types. The code change adds a check within `isPrimaryType` to specifically filter out secondary types based on their naming relationship with the compilation unit. This aligns perfectly with the bug description. The added check prevents the method from returning `true` for secondary types, which would have caused the bug described in the commit message.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes directly address the issue of incorrectly identifying secondary types. The added logic filters out secondary types, resolving the bug described in the commit message.

**Buggy**
"
eclipseJdt,1127.json,fe3ac1852503ee5f3d26919dd323775bfa197648,"@@ -1,14 +1,17 @@
 public PossibleMatch[] getPossibleMatches(IPackageFragmentRoot[] roots) {
 	PossibleMatch[] result = new PossibleMatch[this.elementCount];
 	int index = 0;
+	HashSet<IPath> processedHash = new HashSet<>();
 	for (int i = 0, length = roots.length; i < length; i++) {
-		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(roots[i].getPath());
-		if (possibleMatches != null) {
+		IPath path = roots[i].getPath();
+		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(path);
+		if (possibleMatches != null && !processedHash.contains(path)) {
 			possibleMatches.copyInto(result, index);
 			index += possibleMatches.size();
+			processedHash.add(path);
 		}
 	}
 	if (index < this.elementCount)
 		System.arraycopy(result, 0, result = new PossibleMatch[index], 0, index);
 	return result;
 }",Buggy,"Fix for bug 478360 - [1.9][search] AIOOBE  while searching for java.base
methods",Buggy,"The commit message indicates a fix for an `ArrayIndexOutOfBoundsException` (AIOOBE) that occurs during a search for `java.base` methods. The code diff introduces a `HashSet<IPath> processedHash` to keep track of the paths of the package fragment roots that have already been processed. Before copying the `possibleMatches` into the `result` array, the code now checks if the path associated with the current root has already been processed. If it has, the code skips processing the root. This prevents the same `possibleMatches` from being added multiple times, which could lead to an AIOOBE if the `index` exceeds the bounds of the `result` array. The addition of the `processedHash` and the check `!processedHash.contains(path)` strongly suggests a fix for a bug where the same root was being processed multiple times, leading to the AIOOBE.

**Buggy**"
eclipseJdt,3485.json,5df55b1d083f2bad66acc93a2ff019e72423c2c9,"@@ -1,12 +1,20 @@
 	public static IndexLocation createIndexLocation(URL url) {
 		URL localUrl;
 		try {
 			localUrl = FileLocator.resolve(url);
 		} catch (IOException e) {
 			return null;
 		}
 		if (localUrl.getProtocol().equals(""file"")) { //$NON-NLS-1$
-			return new FileIndexLocation(url, new File(localUrl.getPath()));
+			File localFile = null;
+			try {
+				URI localFileURI = new URI(localUrl.toExternalForm());
+				localFile = new File(localFileURI);
+			}
+			catch(Exception ex) {
+				localFile = new File(localUrl.getPath());
+			}
+			return new FileIndexLocation(url, localFile);
 		}
 		return new JarIndexLocation(url, localUrl);
 	}",Buggy,"Fix for bug 397818 - An IndexLocation is not created properly if the
pre-build indexes are located in a directory with a space
",Buggy,"The commit message indicates a fix for bug 397818, which is related to creating an `IndexLocation` when pre-built indexes are located in a directory with a space. The code changes involve modifying how a `FileIndexLocation` is created when the URL protocol is ""file"".

Here's a step-by-step analysis:

1. **Original Code:** The original code directly creates a `File` object using `localUrl.getPath()`. This approach can fail if the path contains spaces or special characters, as `getPath()` might not properly decode the URL.

2. **Modified Code:** The modified code introduces a `try-catch` block to handle potential exceptions when creating a `File` object. It first attempts to create a `File` using a `URI` constructed from `localUrl.toExternalForm()`. This is a more robust way to handle URLs with spaces and special characters. If this fails (e.g., due to an invalid URI), it falls back to the original approach of using `localUrl.getPath()`.

3. **Bug Fix Indication:** The introduction of the `try-catch` block and the use of `URI` to create the `File` object strongly suggest a bug fix. The original code was likely failing when dealing with file paths containing spaces, as indicated in the commit message. The modified code attempts to handle these cases more gracefully. The `catch` block provides a fallback mechanism, ensuring that the code doesn't completely fail if the URI-based approach doesn't work.

4. **Relevance:** The code change is directly relevant to the commit message. The bug described in the message is about handling file paths with spaces, and the code modification addresses this issue by using `URI` to create the `File` object, which is a standard way to handle spaces in file paths represented as URLs.

**Conclusion:**

**Buggy**
"
eclipseJdt,3849.json,e14a67b89e92e257eae6e75fc0c7b3046b4c9f63,"@@ -1,5 +1,6 @@
 public Object reusableJSRTarget() {
-	if (this.constant != Constant.NotAConstant)
+	if (this.constant != Constant.NotAConstant && (this.implicitConversion & TypeIds.BOXING) == 0) {
 		return this.constant;
+	}
 	return null;
 }",Buggy,Fix for bug 394718 - VerifyError: Inconsistent stackmap frames,Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the change indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix for bug 394718 - VerifyError: Inconsistent stackmap frames"" explicitly states that the commit addresses a bug (394718) that results in a `VerifyError` related to inconsistent stackmap frames. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `reusableJSRTarget()` method.  Specifically, a condition has been added to the `if` statement: `(this.implicitConversion & TypeIds.BOXING) == 0`.

**Reasoning:**

1.  **Bug Context:** The `VerifyError: Inconsistent stackmap frames` error typically arises when the JVM's bytecode verifier detects inconsistencies in the stack map frames of a method. Stack map frames are used to optimize bytecode verification and are essential for ensuring type safety. These errors often occur due to incorrect bytecode generation or transformations.

2.  **Code Change Implication:** The added condition `(this.implicitConversion & TypeIds.BOXING) == 0` suggests that the original code was incorrectly handling cases involving boxing conversions. Boxing conversions (e.g., `int` to `Integer`) can introduce complexities in bytecode generation, and if not handled correctly, can lead to stack map inconsistencies. The added condition likely prevents the `reusableJSRTarget()` method from returning a constant when an implicit boxing conversion is involved, which was causing the `VerifyError`.

3.  **Error Handling/Logical Correction:** The modification appears to be a logical correction to handle boxing conversions correctly, thus preventing the generation of bytecode that would trigger the `VerifyError`.

**Conclusion:**

Based on the commit message and the code changes, the commit clearly addresses a bug related to bytecode verification and stackmap frames. The added condition suggests a correction in how boxing conversions are handled, which was causing the `VerifyError`.

**Buggy**
"
eclipseJdt,2849.json,48e3ddc97a372ba17e3a8fb05bd8c143189dd168,"@@ -1,21 +1,21 @@
 	public static final char[] concat(char[] first, char[] second, char[] third, char[] fourth) {
 		if (first == null)
 			return concat(second, third, fourth);
 		if (second == null)
 			return concat(first, third, fourth);
 		if (third == null)
 			return concat(first, second, fourth);
 		if (fourth == null)
 			return concat(first, second, third);
 
 		int length1 = first.length;
 		int length2 = second.length;
 		int length3 = third.length;
 		int length4 = fourth.length;
 		char[] result = new char[length1 + length2 + length3 + length4];
 		System.arraycopy(first, 0, result, 0, length1);
 		System.arraycopy(second, 0, result, length1, length2);
 		System.arraycopy(third, 0, result, length1 + length2, length3);
-		System.arraycopy(third, 0, result, length1 + length2 + length3, length4);
+		System.arraycopy(fourth, 0, result, length1 + length2 + length3, length4);
 		return result;
 	}",Buggy,"Bug 481796 - Fix ArrayIndexOutOfBoundsException

Change-Id: Ib1cded7b613262929f0c906a04bd69a523311141
Signed-off-by: Stefan Xenos <sxenos@gmail.com>
",Buggy,"The commit message indicates a fix for an `ArrayIndexOutOfBoundsException`. The diff shows a change in the `concat` method where `System.arraycopy(third, 0, result, length1 + length2 + length3, length4);` is corrected to `System.arraycopy(fourth, 0, result, length1 + length2 + length3, length4);`. This correction suggests that the original code was copying the `third` array into the `result` array at an incorrect offset, potentially leading to an `ArrayIndexOutOfBoundsException` if the combined lengths exceeded the size of the `result` array. The change directly addresses the array indexing issue, aligning with the commit message's intent to fix the exception.

**Buggy**"
eclipseJdt,5117.json,8f577b9f934a073bf0b3684c3935f2cd08a1660a,"@@ -1,8 +1,8 @@
 	public StringBuffer printExpression(int indent, StringBuffer output) {
-		output.append(""NAryStringLiteral{""); //$NON-NLS-1$
+		output.append(""StringLiteralConcatenation{""); //$NON-NLS-1$
 		for (int i = 0, max = this.counter; i < max; i++) {
 			this.literals[i].printExpression(indent, output);
 			output.append(""+\n"");//$NON-NLS-1$
 		}
 		return output.append('}');
 	}",Buggy,"HEAD - Fix bug in formatting string literal concatenation
",Buggy,"The commit message indicates a bug fix related to formatting string literal concatenation. The diff shows a change in the string literal used when appending to the output buffer. Specifically, ""NAryStringLiteral{"" is changed to ""StringLiteralConcatenation{"". This change suggests that the original string literal was incorrect, and the commit fixes this inaccuracy. The change directly relates to formatting, as stated in the commit message.

**Buggy**"
eclipseJdt,20784.json,653036e82ad47971c36ae720862ebb4740be149f,"@@ -1,12 +1,13 @@
 char[][] fullExclusionPatternChars() {
 	try {
+		if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;
 		ClasspathEntry entry = (ClasspathEntry)getRawClasspathEntry();
 		if (entry == null) {
 			return null;
 		} else {
 			return entry.fullExclusionPatternChars();
 		}
 	} catch (JavaModelException e) { 
 		return null;
 	}
 }",Buggy,"Fix for bug 28489
",Buggy,"The commit message indicates a bug fix related to bug 28489. The code diff adds a condition `if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;` at the beginning of the `fullExclusionPatternChars()` method. This suggests that the method was previously being called in scenarios where `this.getKind()` was not equal to `IPackageFragmentRoot.K_SOURCE`, leading to an error or unexpected behavior. The added check prevents this by returning `null` in such cases. The try-catch block suggests that `JavaModelException` was possibly thrown, and the catch block returns null, which might be related to the bug. Therefore, the change appears to be a bug fix.

**Buggy**"
eclipseJdt,1595.json,0a908cd3dc3279066d94660987f6e5450b72486e,"@@ -1,54 +1,55 @@
 private MethodBinding getMethodBinding(MethodPattern methodPattern, TypeBinding declaringTypeBinding) {
 	MethodBinding result;
 	char[][] parameterTypes = methodPattern.parameterSimpleNames;
 	if (parameterTypes == null) return null;
 	int paramTypeslength = parameterTypes.length;
 	ReferenceBinding referenceBinding = (ReferenceBinding) declaringTypeBinding;
 	MethodBinding[] methods = referenceBinding.getMethods(methodPattern.selector);
 	int methodsLength = methods.length;
 	TypeVariableBinding[] refTypeVariables = referenceBinding.typeVariables();
 	int typeVarLength = refTypeVariables==null ? 0 : refTypeVariables.length;
 	List <MethodBinding> possibleMethods = new ArrayList<MethodBinding>(methodsLength);
 	for (int i=0; i<methodsLength; i++) {
 		TypeBinding[] methodParameters = methods[i].parameters;
 		int paramLength = methodParameters==null ? 0 : methodParameters.length;
 		TypeVariableBinding[] methodTypeVariables = methods[i].typeVariables;
 		int methTypeVarLength = methodTypeVariables==null ? 0 : methodTypeVariables.length;
 		boolean found = false;
 		if (methodParameters != null && paramLength == paramTypeslength) {
 			for (int p=0; p<paramLength; p++) {
-				if (CharOperation.equals(methodParameters[p].sourceName(), parameterTypes[p])) {
+				TypeBinding parameter = methodParameters[p];
+				if (matchParams(methodPattern, p, parameter)) {
 					// param erasure match
 					found = true;
 				} else {
 					// type variable
 					found = false;
 					if (refTypeVariables != null) {
 						for (int v=0; v<typeVarLength; v++) {
 							if (!CharOperation.equals(refTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found && methodTypeVariables != null) {
 						for (int v=0; v<methTypeVarLength; v++) {
 							if (!CharOperation.equals(methodTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found) break;
 				}
 			}
 		}
 		if (found) {
 			possibleMethods.add(methods[i]);
 		}
 	}
 	result =  getMostApplicableMethod(possibleMethods, methodPattern);
 	return result;
 }",Buggy,Fix for Bug 521240: Search ignores the qualifier of the parameter type,Buggy,"The commit message indicates a fix for a bug where the search functionality was ignoring the qualifier of the parameter type.

The code diff replaces the direct comparison of `methodParameters[p].sourceName()` and `parameterTypes[p]` with a call to a new method `matchParams(methodPattern, p, parameter)`. This suggests that the original comparison was too simplistic and didn't account for the qualifier of the parameter type. The new `matchParams` method likely contains the logic to correctly compare parameter types, including their qualifiers.

Based on the commit message and the code changes, it is clear that the changes are intended to fix a bug in the search functionality related to parameter type matching.

**Buggy**"
eclipseJdt,1134.json,b7bd88d9bc859369d4d17c37d109bd3331837ba7,"@@ -1,7 +1,8 @@
 public int match(MessageSend msgSend, MatchingNodeSet nodeSet)  {
 	if ((msgSend.bits & ASTNode.InsideJavadoc) == 0) return IMPOSSIBLE_MATCH;
+	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (this.pattern.declaringSimpleName == null || CharOperation.equals(msgSend.selector, this.pattern.declaringSimpleName)) {
 		return nodeSet.addMatch(msgSend, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 	}
 	return IMPOSSIBLE_MATCH;
 }",Buggy,"Fixed bug 381567: [search] Unexpected results from SearchEngine#search
Conflicts:
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fixed bug 381567: [search] Unexpected results from SearchEngine#search"" clearly states that the commit is intended to fix a bug. The bug is related to the search functionality, specifically within the `SearchEngine#search` method, and it results in unexpected search results.

**2. Code Diff Analysis:**

The code diff shows a modification to the `match` method.  A new condition `if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;` has been added at the beginning of the method.

**3. Reasoning:**

The added condition `if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;` suggests that the `match` method was being called even when the search pattern was not intended to find references. This could lead to incorrect or unexpected matches, which aligns with the bug description in the commit message. By adding this check, the method now avoids processing message sends when `findReferences` is false, potentially correcting the unexpected search results. The original code was likely matching more than it should have.

**Conclusion:**

The commit message explicitly states a bug fix, and the code modification introduces a condition that prevents the `match` method from running when it shouldn't, which likely resolves the issue of unexpected search results.

**Buggy**
"
eclipseJdt,5656.json,905dec82e916c38704735fea38fad8cba693cc45,"@@ -1,3 +1,3 @@
 public TypeBinding clone(TypeBinding outerType) {
-	throw new IllegalStateException(); // shouldn't get here.
+	return this; // shouldn't get here.
 }",Buggy,"Fixed Bug 427105 - [1.8][builder] Differences between incremental and
full builds in method contract verification in the presence of type
annotations",Buggy,"The commit message indicates a bug fix related to differences between incremental and full builds in method contract verification when type annotations are present. The code change replaces `throw new IllegalStateException()` with `return this`. This change suggests that the original code was throwing an exception in a situation where it shouldn't have, likely causing the discrepancy between incremental and full builds. The corrected code now returns the current object, which is a more appropriate action in this context. This aligns with the commit message's description of fixing a bug.

**Buggy**"
eclipseJdt,1327.json,84f1aab618cdc74f8c6aecebdc0fb845b80d6368,"@@ -1,4 +1,6 @@
 protected void consumeMemberValuePair() {
 	super.consumeMemberValuePair();
-	this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	if ((this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0) {
+		this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	}
 }",Buggy,"Fix for Bug 435480 [1.8][search] search in method reference expressions
finds annotation element name",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix for Bug 435480 [1.8][search] search in method reference expressions finds annotation element name"" clearly states that this commit is intended to fix a bug. The bug involves the search functionality incorrectly finding annotation element names within method reference expressions.

2.  **Code Diff:** The code diff modifies the `consumeMemberValuePair()` method. It adds a conditional check using `(this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0`. This condition seems to be specifically introduced to avoid processing `MemberValuePair` instances when the search is within a method reference expression. The original code unconditionally called `this.patternLocator.match()`, which was causing the incorrect matching of annotation element names.

3.  **Alignment:** The code change directly addresses the problem described in the commit message. The added condition prevents the `patternLocator.match()` method from being called when processing method reference expressions, thus fixing the bug where annotation element names were incorrectly found.

4.  **Bug Fix Indicators:** The commit message explicitly mentions a bug fix. The code change introduces a conditional statement to prevent incorrect matching in a specific scenario (method reference expressions), which is a typical pattern for bug fixes.

**Conclusion:**

The commit message and code diff are highly aligned and indicate a bug fix. The code change directly addresses the issue described in the commit message.

**Buggy**"
eclipseJdt,165.json,fe024fd7797ce73b0a83969f685073606863b79e,"@@ -1,23 +1,23 @@
 	public PrimitiveTypeImpl getPrimitiveType(TypeKind kind)
 	{
 		switch (kind) {
 		case BOOLEAN:
 			return PrimitiveTypeImpl.BOOLEAN;
 		case BYTE:
 			return PrimitiveTypeImpl.BYTE;
 		case CHAR:
 			return PrimitiveTypeImpl.CHAR;
 		case DOUBLE:
 			return PrimitiveTypeImpl.DOUBLE;
 		case FLOAT:
 			return PrimitiveTypeImpl.FLOAT;
 		case INT:
 			return PrimitiveTypeImpl.INT;
 		case LONG:
 			return PrimitiveTypeImpl.LONG;
 		case SHORT:
 			return PrimitiveTypeImpl.SHORT;
 		default:
-			throw new IllegalStateException();
+			throw new IllegalArgumentException();
 		}
 	}",Buggy,"Fixes Bug 427943 - The method
org.eclipse.jdt.internal.compiler.apt.model.Factory.getPrimitiveType
does not throw IllegalArgumentException

Change-Id: I652c422b80ef6192955ef508387f9fd0fac31a7e
Signed-off-by: Harry Terkelsen <het@google.com>",Buggy,"The commit message indicates a fix for Bug 427943, specifically addressing the exception type thrown by the `getPrimitiveType` method. The original code threw `IllegalStateException` in the default case of the switch statement, while the corrected code throws `IllegalArgumentException`. This change aligns with the commit message, indicating that the original exception type was incorrect or inappropriate for the given scenario. The change directly addresses the reported bug by correcting the exception type.

**Buggy**"
eclipseJdt,1253.json,025c833d41053b8599d25c53d05e92e37975873f,"@@ -1,18 +1,29 @@
 protected int resolveLevelForType(char[] qualifiedPattern, TypeBinding type) {
 	if (qualifiedPattern == null) return ACCURATE_MATCH;
 	if (type == null || !type.isValidBinding()) return INACCURATE_MATCH;
 
 	// Type variable cannot be specified through pattern => this kind of binding cannot match it (see bug 79803)
 	if (type.isTypeVariable()) return IMPOSSIBLE_MATCH;
 
+	if (type instanceof IntersectionTypeBinding18) {
+		int result = IMPOSSIBLE_MATCH, prev = IMPOSSIBLE_MATCH;
+		IntersectionTypeBinding18 i18 = (IntersectionTypeBinding18) type;
+		for (ReferenceBinding ref : i18.intersectingTypes) {
+			result = resolveLevelForType(qualifiedPattern, ref);
+			if (result == ACCURATE_MATCH) return result; 
+			if (result == IMPOSSIBLE_MATCH) continue;
+			if (prev == IMPOSSIBLE_MATCH) prev = result;
+		}
+		return prev;
+	}
 	// NOTE: if case insensitive search then qualifiedPattern is assumed to be lowercase
 
 	char[] qualifiedPackageName = type.qualifiedPackageName();
 	char[] qualifiedSourceName = qualifiedSourceName(type);
 	char[] fullyQualifiedTypeName = qualifiedPackageName.length == 0
 		? qualifiedSourceName
 		: CharOperation.concat(qualifiedPackageName, qualifiedSourceName, '.');
 	return CharOperation.match(qualifiedPattern, fullyQualifiedTypeName, this.isCaseSensitive)
 		? ACCURATE_MATCH
 		: IMPOSSIBLE_MATCH;
 }",Buggy,"Fix for Bug 485805 [1.8][search] Search engine throws
UnsupportedOperationException when searching for subclass implementors

Change-Id: I28b09ffc6e446e2967c4dbf077816ef2df83d2ab",Buggy,"The commit message indicates a fix for a bug where the search engine throws an `UnsupportedOperationException` when searching for subclass implementors in Java 1.8. The code diff introduces a new `if` block to handle `IntersectionTypeBinding18`, which is specific to Java 1.8 and later due to the introduction of intersection types. The code iterates through the intersecting types of the `IntersectionTypeBinding18` and recursively calls `resolveLevelForType` for each intersecting type. This suggests that the original code was not handling intersection types correctly, leading to the exception. The added code addresses this by properly resolving the type level for intersection types.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**"
eclipseJdt,2496.json,7d2b09ebfd4cb99d1f345eedcae879729e8aff7e,"@@ -1,51 +1,53 @@
 	public MemoryAccessLog getReportFor(long address, int size) {
 		List<Tag> tags = new ArrayList<>();
 		tags.addAll(this.operationStack);
-		int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
-		int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
-		long currentWrite = this.timer;
 
 		List<MemoryOperation> operations = new ArrayList<>();
-		do {
-			long nextAddress = this.buffer0[currentPosition];
-			int nextArgument = this.buffer1[currentPosition];
-			byte nextOp = this.operation[currentPosition];
-
-			switch (nextOp) {
-				case POP_OPERATION: {
-					tags.add(getTagForId(nextArgument));
-					break;
-				}
-				case PUSH_OPERATION: {
-					tags.remove(tags.size() - 1);
-					break;
-				}
-				default: {
-					boolean isMatch = false;
-					if (address < nextAddress) {
-						long diff = nextAddress - address;
-						if (diff < size) {
-							isMatch = true;
-						}
-					} else {
-						long diff = address - nextAddress;
-						if (diff < nextArgument) {
-							isMatch = true;
-						}
+		if (this.buffer0 != null) {
+			int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
+			int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
+			long currentWrite = this.timer;
+			do {
+				long nextAddress = this.buffer0[currentPosition];
+				int nextArgument = this.buffer1[currentPosition];
+				byte nextOp = this.operation[currentPosition];
+	
+				switch (nextOp) {
+					case POP_OPERATION: {
+						tags.add(getTagForId(nextArgument));
+						break;
 					}
-
-					if (isMatch) {
-						List<Tag> stack = new ArrayList<>();
-						stack.addAll(tags);
-						MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
-								nextArgument, stack);
-						operations.add(nextOperation);
+					case PUSH_OPERATION: {
+						tags.remove(tags.size() - 1);
+						break;
 					}
-
-					currentWrite--;
+					default: {
+						boolean isMatch = false;
+						if (address < nextAddress) {
+							long diff = nextAddress - address;
+							if (diff < size) {
+								isMatch = true;
+							}
+						} else {
+							long diff = address - nextAddress;
+							if (diff < nextArgument) {
+								isMatch = true;
+							}
+						}
+	
+						if (isMatch) {
+							List<Tag> stack = new ArrayList<>();
+							stack.addAll(tags);
+							MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
+									nextArgument, stack);
+							operations.add(nextOperation);
+						}
+	
+						currentWrite--;
+					}
 				}
-			}
-			currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
-		} while (currentPosition != pointerToStart);
+				currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
+			} while (currentPosition != pointerToStart);
+		}
 		return new MemoryAccessLog(operations);
 	}",Buggy,"Bug 514089 - Build a tool to help debug index corruption

Fix NPE when corruption is detected and the buffer size is 0.

Change-Id: I4163e9f294c96582b201bdf09593602ab1807d6e
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when corruption is detected and the buffer size is 0. The code diff adds a null check for `this.buffer0` before accessing its properties like `length`. If `this.buffer0` is null, the code within the `if` block is skipped, preventing the NPE. This aligns with the commit message's description of fixing an NPE.

**Buggy**"
eclipseJdt,15048.json,30961650a033c638f3a768401431bcd77f1a4058,"@@ -1,26 +1,29 @@
 	private void removeUnresolvedBindings(org.eclipse.jdt.internal.compiler.ast.TypeDeclaration type) {
 		final MemberTypeDeclaration[] memberTypes = type.memberTypes;
 		if (memberTypes != null) {
 			for (int i = 0, max = memberTypes.length; i < max; i++){
 				removeUnresolvedBindings(memberTypes[i]);
 			}
 		}
 		if (type.binding != null && (type.binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 			type.binding = null;
-			final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
-			if (fields != null) {
-				for (int i = 0, max = fields.length; i < max; i++){
+		}
+		
+		final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
+		if (fields != null) {
+			for (int i = 0, max = fields.length; i < max; i++){
+				if (fields[i].binding != null && (fields[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					fields[i].binding = null;
 				}
 			}
 		}
-
+	
 		final AbstractMethodDeclaration[] methods = type.methods;
 		if (methods != null) {
 			for (int i = 0, max = methods.length; i < max; i++){
 				if (methods[i].binding !=  null && (methods[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					methods[i].binding = null;
 				}
 			}
 		}
 	}",Buggy,"Fix for bug 40804
",Buggy,"The commit message ""Fix for bug 40804"" indicates a bug fix. The code changes involve modifying the `removeUnresolvedBindings` method to handle unresolved bindings in field declarations more carefully. Specifically, the code now checks if `fields[i].binding` is not null and if its modifiers include `CompilerModifiers.AccUnresolved` before setting it to null. This suggests that the previous code might have been incorrectly setting bindings to null, potentially leading to errors or unexpected behavior. The added null check and modifier check indicate a correction to a potentially faulty logic.

**Buggy**"
eclipseJdt,18985.json,4b7408bf1768e4fe08faa711e23ea5c6fe36d225,"@@ -1,6 +1,6 @@
 public String toString() {
-	return ""State for "" + this.javaProjectName
-		+ "" (#"" + this.buildNumber
+	return ""State for "" + this.javaProjectName //$NON-NLS-1$
+		+ "" (#"" + this.buildNumber //$NON-NLS-1$
 			+ "" @ "" + new Date(this.lastStructuralBuildTime) //$NON-NLS-1$
 				+ "")""; //$NON-NLS-1$
 }",Buggy,"HEAD - Fix Clean-up pass 1 problems
",Buggy,"The commit message ""Fix Clean-up pass 1 problems"" suggests that the commit addresses issues identified during a cleanup process.

The code diff introduces `//$NON-NLS-1$` comments to the `toString()` method. These comments are typically used to suppress warnings related to externalizing strings for internationalization (i18n). The addition of these comments indicates a cleanup activity, specifically addressing potential i18n concerns.

The changes align with the commit message, indicating a cleanup task. While not directly fixing a functional bug, it addresses a potential issue related to code maintainability and i18n.

**Buggy**
"
eclipseJdt,4719.json,6fe04df602475d9f13e955fcfd38124da359e84a,"@@ -1,28 +1,34 @@
 public void computeConversion(Scope scope, TypeBinding runtimeTimeType, TypeBinding compileTimeType) {
 	if (runtimeTimeType == null || compileTimeType == null)
 		return;
-	if ((this.bits & Binding.FIELD) != 0 && this.binding != null && this.binding.isValidBinding()) {
-		// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
-		FieldBinding field = (FieldBinding) this.binding;
-		FieldBinding originalBinding = field.original();
-		TypeBinding originalType = originalBinding.type;
-		// extra cast needed if field type is type variable
-		if (originalType.leafComponentType().isTypeVariable()) {
-	    	TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
-	    		? compileTimeType  // unboxing: checkcast before conversion
-	    		: runtimeTimeType;
-	        this.genericCast = originalType.genericCast(scope.boxing(targetType));
-	        if (this.genericCast instanceof ReferenceBinding) {
+	if (this.binding != null && this.binding.isValidBinding()) {
+		TypeBinding originalType = null;
+		if ((this.bits & Binding.FIELD) != 0) {
+			// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
+			FieldBinding field = (FieldBinding) this.binding;
+			FieldBinding originalBinding = field.original();
+			originalType = originalBinding.type;
+		} else if ((this.bits & Binding.LOCAL) != 0) {
+			LocalVariableBinding local = (LocalVariableBinding) this.binding;
+			originalType = local.type;
+		}
+		// extra cast needed if field/local type is type variable
+		if (originalType != null && originalType.leafComponentType().isTypeVariable()) {
+			TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
+					? compileTimeType  // unboxing: checkcast before conversion
+							: runtimeTimeType;
+			this.genericCast = originalType.genericCast(scope.boxing(targetType));
+			if (this.genericCast instanceof ReferenceBinding) {
 				ReferenceBinding referenceCast = (ReferenceBinding) this.genericCast;
 				if (!referenceCast.canBeSeenBy(scope)) {
-		        	scope.problemReporter().invalidType(this,
-		        			new ProblemReferenceBinding(
-								CharOperation.splitOn('.', referenceCast.shortReadableName()),
-								referenceCast,
-								ProblemReasons.NotVisible));
+					scope.problemReporter().invalidType(this,
+							new ProblemReferenceBinding(
+									CharOperation.splitOn('.', referenceCast.shortReadableName()),
+									referenceCast,
+									ProblemReasons.NotVisible));
 				}
-	        }
+			}
 		}
 	}
 	super.computeConversion(scope, runtimeTimeType, compileTimeType);
 }",Buggy,"Fixed Bug 416480 - Error in bytecode generated by ECJ compiler leads to
IncompatibleClassChangeError",Buggy,"The commit message indicates a fix for a bug (416480) that leads to an `IncompatibleClassChangeError` due to incorrect bytecode generated by the ECJ compiler.

The code diff modifies the `computeConversion` method. The original code only considered `FieldBinding` when determining if a generic cast was needed. The modified code now also considers `LocalVariableBinding`. This suggests that the bug was occurring when dealing with local variables, not just fields. The addition of the `else if ((this.bits & Binding.LOCAL) != 0)` block and the subsequent handling of `LocalVariableBinding` confirms this. The check `originalType != null` is also added to prevent NullPointerException.

The code change addresses a specific scenario related to type variables and generic casts, which aligns with the bug fix mentioned in the commit message. The addition of handling `LocalVariableBinding` suggests a broader fix than initially anticipated.

**Buggy**"
eclipseJdt,25728.json,ab7dabe7a80a22f7a1c8f6edce715e69ee26d4d5,"@@ -1,34 +1,36 @@
 	protected static boolean hasEmptyName(TypeReference reference, ASTNode assistNode) {
 		if (reference == null) return false;
 
-		if (reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
+		// https://bugs.eclipse.org/bugs/show_bug.cgi?id=397070
+		if (reference != assistNode &&
+				reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
 
 		if (reference instanceof CompletionOnSingleTypeReference ||
 				reference instanceof CompletionOnQualifiedTypeReference ||
 				reference instanceof CompletionOnParameterizedQualifiedTypeReference) {
 			char[][] typeName = reference.getTypeName();
 			if (typeName[typeName.length - 1].length == 0) return true;
 		}
 		if (reference instanceof ParameterizedSingleTypeReference) {
 			ParameterizedSingleTypeReference parameterizedReference = (ParameterizedSingleTypeReference) reference;
 			TypeReference[] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (hasEmptyName(typeArguments[i], assistNode)) return true;
 				}
 			}
 		} else if (reference instanceof ParameterizedQualifiedTypeReference) {
 			ParameterizedQualifiedTypeReference parameterizedReference = (ParameterizedQualifiedTypeReference) reference;
 			TypeReference[][] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (typeArguments[i] != null) {
 						for (int j = 0; j < typeArguments[i].length; j++) {
 							if (hasEmptyName(typeArguments[i][j], assistNode)) return true;
 						}
 					}
 				}
 			}
 		}
 		return false;
 	}",Buggy,"Fix for bug 397070 - On completion in implements/extends clause,
supertype/interface-signature(s) can not be retrieved from enclosing
type",Buggy,"The commit message indicates a fix for bug 397070, which relates to code completion in `implements/extends` clauses. The bug prevents supertype/interface signatures from being retrieved from the enclosing type.

The code diff modifies the `hasEmptyName` method. The original code had a condition:
```java
if (reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
```
The modified code adds a check `reference != assistNode`:
```java
if (reference != assistNode && reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
```
The added condition `reference != assistNode` suggests that the original code was incorrectly returning `false` when `reference` and `assistNode` were the same object. This would prevent the code completion logic from working correctly in certain scenarios, which aligns with the bug description. The added comment `// https://bugs.eclipse.org/bugs/show_bug.cgi?id=397070` further confirms that this change is related to the bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**"
eclipseJdt,20900.json,c79924988b83a4410dc68ac4bbc9ed2e5112cc61,"@@ -1,3 +1,3 @@
-	public static IClasspathContainer getUserLibrary(String name) {
-		return (IClasspathContainer) getLibraryMap().get(name);
+	public static UserLibrary getUserLibrary(String name) {
+		return (UserLibrary) getLibraryMap().get(name);
 	}",Buggy,"Fix bug: invalid getUserLibrary return
",Buggy,"The commit message indicates a bug fix related to the return type of the `getUserLibrary` method. The diff shows that the return type of the `getUserLibrary` method has been changed from `IClasspathContainer` to `UserLibrary`. This change suggests that the original return type was incorrect, leading to a bug. The commit message and the code change are consistent with each other.

**Buggy**"
eclipseJdt,14646.json,8bf6b68ac4b26f2f99e3e6e3e77b19496440917e,"@@ -1,12 +1,11 @@
 	public ITypeBinding getGenericTypeOfWildcardType() {
 		switch (this.binding.kind()) {
 			case Binding.WILDCARD_TYPE :
-			case Binding.INTERSECTION_TYPE :
 				WildcardBinding wildcardBinding = (WildcardBinding) this.binding;
 				if (wildcardBinding.genericType != null) {
 					return this.resolver.getTypeBinding(wildcardBinding.genericType);
 				}
 				break;
 		}
 		return null;
 	}",Buggy,"HEAD - Fixed bug 341759: NPE in ITypeBinding#getName() for intersection type
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in `ITypeBinding#getName()` when dealing with intersection types. The code diff removes `Binding.INTERSECTION_TYPE` from the `switch` statement in `getGenericTypeOfWildcardType()`.

Here's the reasoning:

1.  **Commit Message Focus:** The commit message explicitly mentions an NPE related to intersection types.
2.  **Code Change Analysis:** The original code had a `switch` statement that included both `Binding.WILDCARD_TYPE` and `Binding.INTERSECTION_TYPE`. The change removes `Binding.INTERSECTION_TYPE` from this `switch`.
3.  **Potential Bug:** The original code likely attempted to handle `Binding.INTERSECTION_TYPE` in the same way as `Binding.WILDCARD_TYPE`. However, the logic within the `if` statement (`wildcardBinding.genericType != null`) and the subsequent call to `this.resolver.getTypeBinding()` might not be valid or safe for intersection types, potentially leading to an NPE if `wildcardBinding.genericType` is null in the context of an intersection type.
4.  **Fix Implication:** By removing `Binding.INTERSECTION_TYPE` from the `switch`, the code avoids executing the potentially problematic logic for intersection types, thus preventing the NPE.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
eclipseJdt,19389.json,ae30fd6df4fe6e776883eda11e8cb81aefe2c34c,"@@ -1,29 +1,26 @@
 protected IBuffer openBuffer(IProgressMonitor pm, Object info) throws JavaModelException {
 
 	// create buffer
-	IBuffer buffer = this.owner.createBuffer(this);
-	if (buffer == null) return null;
+	IBuffer buffer = BufferManager.createBuffer(this);
 
 	// set the buffer source
-	if (buffer.getCharacters() == null) {
-		IBuffer classFileBuffer = this.classFile.getBuffer();
-		if (classFileBuffer != null) {
-			buffer.setContents(classFileBuffer.getCharacters());
-		} else {
-			// Disassemble
-			IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
-			Disassembler disassembler = new Disassembler();
-			String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
-			buffer.setContents(contents);
-		}
+	IBuffer classFileBuffer = this.classFile.getBuffer();
+	if (classFileBuffer != null) {
+		buffer.setContents(classFileBuffer.getCharacters());
+	} else {
+		// Disassemble
+		IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
+		Disassembler disassembler = new Disassembler();
+		String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
+		buffer.setContents(contents);
 	}
 
 	// add buffer to buffer cache
 	BufferManager bufManager = getBufferManager();
 	bufManager.addBuffer(buffer);
 
 	// listen to buffer changes
 	buffer.addBufferChangedListener(this);
 
 	return buffer;
 }",Buggy,"HEAD - Fixed bug 337935: Test failures when run as an IDE (org.eclipse.sdk.ide)
",Buggy,"The commit message indicates a fix for test failures when running as an IDE. The code diff modifies how a buffer is created and populated with content, specifically within the `openBuffer` method.

The original code had the following structure:
1. Create a buffer using `this.owner.createBuffer(this)`.
2. Check if the buffer is null and return if it is.
3. Check if the buffer's characters are null.
4. If the characters are null, populate the buffer with content from either `classFileBuffer` or by disassembling the class file.

The modified code has the following structure:
1. Create a buffer using `BufferManager.createBuffer(this)`.
2. Populate the buffer with content from either `classFileBuffer` or by disassembling the class file.

The key change is the removal of the null check for the created buffer and the change in how the buffer is created. The original code used `this.owner.createBuffer(this)`, while the modified code uses `BufferManager.createBuffer(this)`. The removal of the null check suggests that the original buffer creation process could sometimes return null, leading to errors. The change in buffer creation mechanism likely addresses the root cause of the null buffer issue. This change directly relates to fixing the test failures mentioned in the commit message, as a null buffer could cause unexpected behavior during testing, especially when running in an IDE environment.

Therefore, the changes indicate a bug fix.

**Buggy**"
eclipseJdt,25842.json,4c94b3d7ef7a5503607858476a6477496a59c95b,"@@ -1,5 +1,5 @@
-protected void consumeLambdaHeader() {
-	super.consumeLambdaHeader();
+protected void consumeNestedLambda() {
+	super.consumeNestedLambda();
 	LambdaExpression lexp = (LambdaExpression) this.astStack[this.astPtr];
 	pushOnElementStack(K_LAMBDA_EXPRESSION_DELIMITER, EXPRESSION_BODY, lexp);
 }",Buggy,"Fixed Bug 430026 - [1.8] Lambda parameter has wrong parent if it
declares its type",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed Bug 430026 - [1.8] Lambda parameter has wrong parent if it declares its type"" clearly states that the commit is intended to fix a bug. The bug is specifically related to lambda expressions in Java 1.8, where a lambda parameter might have an incorrect parent node in the Abstract Syntax Tree (AST) if the parameter's type is explicitly declared.

**Diff Analysis:**

The diff shows a change in the method name from `consumeLambdaHeader()` to `consumeNestedLambda()`.  The core logic within the method remains the same: it calls the superclass's method and then manipulates the `astStack` and `elementStack` to handle a `LambdaExpression`.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly mentions ""Fixed Bug,"" strongly suggesting a bug fix.
2.  **Relevance:** The change in method name from `consumeLambdaHeader` to `consumeNestedLambda` suggests a refinement in how lambda expressions are processed, potentially to address the issue of incorrect parent assignment in the AST. The change likely reflects a more accurate description of the method's purpose within the context of nested or typed lambda expressions.
3.  **AST Manipulation:** The code manipulates the `astStack`, which is directly related to building the AST. The bug description mentions an issue with the parent of a lambda parameter *within* the AST. This connection between the code and the bug description strengthens the likelihood of a bug fix.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes, while seemingly small, are related to AST manipulation and lambda expression processing, which aligns with the bug description. Therefore, the changes indicate a bug fix.

**Buggy**"
eclipseJdt,27032.json,b308074bc5b31f3a5857e47f87565296fbba4a15,"@@ -1,34 +1,39 @@
     public static EclipseDeclarationImpl createDeclaration(IBinding binding, BaseProcessorEnv env)
     {
         if(binding == null) return null;
        
         switch(binding.getKind())
         {
     	case IBinding.TYPE:
     		final ITypeBinding typeBinding = (ITypeBinding)binding;
         	if( typeBinding.isAnonymous() || typeBinding.isArray() || 
     			typeBinding.isWildcardType() || typeBinding.isPrimitive() )       
                 throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         	if( typeBinding.isTypeVariable() )
 	        	return new TypeParameterDeclarationImpl(typeBinding, env);
         	else
         		return createReferenceType(typeBinding, env);
         case IBinding.VARIABLE:
         	final IVariableBinding varBinding = (IVariableBinding)binding;            
             if(varBinding.isEnumConstant())
                 return new EnumConstantDeclarationImpl(varBinding, env);
             else
                 return new FieldDeclarationImpl(varBinding, env);
         case IBinding.METHOD:
             final IMethodBinding method = (IMethodBinding)binding;
             if( method.isConstructor() )
                 return new ConstructorDeclarationImpl(method, env);
             final ITypeBinding declaringType = method.getDeclaringClass();
             if( declaringType != null && declaringType.isAnnotation() )
                 return new AnnotationElementDeclarationImpl(method, env);
             else
-                return new MethodDeclarationImpl(method, env);             
+                return new MethodDeclarationImpl(method, env);
+        case IBinding.PACKAGE:
+            // https://bugs.eclipse.org/bugs/show_bug.cgi?id=352949
+            // Don't throw an exception, but just return null. 
+        	// apt also doesn't return a value
+        	return null;
         default:
             throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         }     
     }",Buggy,"Fix for bug 352949: Impossible to process package-level annotations with
an AnnotationProcessor",Buggy,"The commit message indicates a fix for bug 352949, which concerns the processing of package-level annotations with an annotation processor. The code diff introduces a new case in the `switch` statement to handle `IBinding.PACKAGE`. Instead of throwing an `IllegalStateException`, it now returns `null` when encountering a package binding. The comment explicitly mentions the bug number and explains the reasoning behind the change, aligning with the bug report. This change avoids an exception being thrown when package-level annotations are encountered, which directly addresses the bug described in the commit message.

**Buggy**"
eclipseJdt,14260.json,ea916b0462fec6996e619f84c30f348e2877686a,"@@ -1,15 +1,23 @@
 	protected int retrieveEndOfRightParenthesisPosition(int start, int end) {
 		this.scanner.resetTo(start, end);
 		try {
 			int token;
+			int count = 0;
 			while ((token = this.scanner.getNextToken()) != TerminalTokens.TokenNameEOF) {
 				switch(token) {
 					case TerminalTokens.TokenNameRPAREN:
-						return this.scanner.currentPosition;
+						count--;
+						if (count <= 0) return this.scanner.currentPosition;
+						 break;
+					case TerminalTokens.TokenNameLPAREN:
+						count++;
+						//$FALL-THROUGH$
+					default:
+						break;
 				}
 			}
 		} catch(InvalidInputException e) {
 			// ignore
 		}
 		return -1;
 	}",Buggy,"Fix for bug 399600 - ASTConverter#retrieveEndOfRightParenthesisPosition
fails for certain cases",Buggy,"The commit message indicates a fix for a bug where `ASTConverter#retrieveEndOfRightParenthesisPosition` fails in certain cases. The code diff shows that the method `retrieveEndOfRightParenthesisPosition` has been modified to handle nested parentheses. A counter `count` is introduced to keep track of the parenthesis nesting level. The code now correctly identifies the matching right parenthesis even when there are nested parentheses. This aligns with the commit message, indicating a bug fix.

Reasoning:
1. The commit message explicitly states that it's a bug fix for a specific issue.
2. The code changes introduce a counter to handle nested parentheses, which addresses the reported failure in certain cases.
3. The added logic ensures that the correct right parenthesis is identified, even with nested parentheses.

Conclusion: **Buggy**"
eclipseJdt,11577.json,3d11e595fd95f6b5ceb0fd10d1d7aa7d98828a7b,"@@ -1,56 +1,68 @@
 public void generateSyntheticEnclosingInstanceValues(BlockScope currentScope, ReferenceBinding targetType, Expression enclosingInstance, ASTNode invocationSite) {
 	// supplying enclosing instance for the anonymous type's superclass
 	ReferenceBinding checkedTargetType = targetType.isAnonymousType() ? (ReferenceBinding)targetType.superclass().erasure() : targetType;
 	boolean hasExtraEnclosingInstance = enclosingInstance != null;
 	if (hasExtraEnclosingInstance
 			&& (!checkedTargetType.isNestedType() || checkedTargetType.isStatic())) {
 		currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		return;
 	}
 
 	// perform some emulation work in case there is some and we are inside a local type only
 	ReferenceBinding[] syntheticArgumentTypes;
 	if ((syntheticArgumentTypes = targetType.syntheticEnclosingInstanceTypes()) != null) {
 
 		ReferenceBinding targetEnclosingType = checkedTargetType.enclosingType();
 		long compliance = currentScope.compilerOptions().complianceLevel;
 
 		// deny access to enclosing instance argument for allocation and super constructor call (if 1.4)
 		// always consider it if complying to 1.5
 		boolean denyEnclosingArgInConstructorCall;
 		if (compliance <= ClassFileConstants.JDK1_3) {
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression;
 		} else if (compliance == ClassFileConstants.JDK1_4){
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression
 				|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess();
-		} else {
+		} else if (compliance < ClassFileConstants.JDK1_7) {
 			//compliance >= JDK1_5
 			denyEnclosingArgInConstructorCall = (invocationSite instanceof AllocationExpression
 					|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess())
 				&& !targetType.isLocalType();
+		} else {
+			//compliance >= JDK1_7
+			if (invocationSite instanceof AllocationExpression) {
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType();
+			} else if (invocationSite instanceof ExplicitConstructorCall && 
+					((ExplicitConstructorCall)invocationSite).isSuperAccess()) {
+				MethodScope enclosingMethodScope = currentScope.enclosingMethodScope();
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType() && enclosingMethodScope != null
+						&& enclosingMethodScope.isConstructorCall; 
+			} else {
+				denyEnclosingArgInConstructorCall = false;
+			}
 		}
 
 		boolean complyTo14 = compliance >= ClassFileConstants.JDK1_4;
 		for (int i = 0, max = syntheticArgumentTypes.length; i < max; i++) {
 			ReferenceBinding syntheticArgType = syntheticArgumentTypes[i];
 			if (hasExtraEnclosingInstance && TypeBinding.equalsEquals(syntheticArgType, targetEnclosingType)) {
 				hasExtraEnclosingInstance = false;
 				enclosingInstance.generateCode(currentScope, this, true);
 				if (complyTo14){
 					dup();
 					invokeObjectGetClass(); // will perform null check
 					pop();
 				}
 			} else {
 				Object[] emulationPath = currentScope.getEmulationPath(
 						syntheticArgType,
 						false /*not only exact match (that is, allow compatible)*/,
 						denyEnclosingArgInConstructorCall);
 				generateOuterAccess(emulationPath, invocationSite, syntheticArgType, currentScope);
 			}
 		}
 		if (hasExtraEnclosingInstance){
 			currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		}
 	}
 }",Buggy,"Fixed bug 373371 - [compiler] JDT Compiler reports an error whereas
javac compiles without problem

Change-Id: I1ca89bd73228aa19ee351e12ef64c474cbc58175
",Buggy,"The commit message indicates a fix for a compiler error where JDT Compiler reports an error while `javac` does not. The code changes involve modifications to the `generateSyntheticEnclosingInstanceValues` method, specifically addressing how enclosing instance arguments are handled in constructor calls based on compliance level. The changes introduce different logic for handling enclosing instance arguments based on the compliance level (JDK 1.7 and above). The added condition `enclosingMethodScope != null && enclosingMethodScope.isConstructorCall` suggests a specific scenario related to constructor calls that was previously mishandled. These changes are likely addressing a specific bug related to how the JDT compiler handles enclosing instances in certain scenarios, aligning with the commit message's intent to resolve a discrepancy between JDT and `javac`.

**Buggy**"
eclipseJdt,877.json,48d4662ccc94304b71283abf02dc80106ec8ea24,"@@ -1,21 +1,22 @@
 private void readIndexMap() {
 	try {
 		char[] indexMaps = org.eclipse.jdt.internal.compiler.util.Util.getFileCharContent(this.indexNamesMapFile, null);
 		char[][] names = CharOperation.splitOn('\n', indexMaps);
 		if (names.length >= 3) {
 			// First line is DiskIndex signature (see writeIndexMapFile())
 			String savedSignature = DiskIndex.SIGNATURE;
 			if (savedSignature.equals(new String(names[0]))) {
 				for (int i = 1, l = names.length-1 ; i < l ; i+=2) {
 					IndexLocation indexPath = IndexLocation.createIndexLocation(new URL(new String(names[i])));
+					if (indexPath == null) continue;
 					this.indexLocations.put(new Path(new String(names[i+1])), indexPath );
 					this.indexStates.put(indexPath, REUSE_STATE);
 				}
 			}		
 		}
 	} catch (IOException ignored) {
 		if (VERBOSE)
 			Util.verbose(""Failed to read saved index file names""); //$NON-NLS-1$
 	}
 	return;
 }",Buggy,"Fix for Bug 420590
JavaIndexTests.testExistentIndexAfterRestart failed with error: NPE in
SimpleLookupTable.put
",Buggy,"The commit message indicates a fix for a bug (Bug 420590) that caused a NullPointerException (NPE) in `SimpleLookupTable.put` during the `JavaIndexTests.testExistentIndexAfterRestart` test.

The code diff introduces a null check for `indexPath` after it's created using `IndexLocation.createIndexLocation`. If `createIndexLocation` returns null, the code now skips the subsequent `put` operations on `this.indexLocations` and `this.indexStates`.

Reasoning:

1.  **Bug Context:** The commit message explicitly states a bug fix related to an NPE in `SimpleLookupTable.put`. This strongly suggests that the previous code was vulnerable to inserting null values into the `indexLocations` table, which would cause a crash later.
2.  **Code Change:** The added `if (indexPath == null) continue;` statement directly addresses the possibility of `indexPath` being null. This prevents the subsequent `put` operations from being called with a null key, which would cause the reported NPE.
3.  **Error Handling:** The code now gracefully handles the case where `IndexLocation.createIndexLocation` returns null, preventing the program from crashing.

Conclusion: **Buggy**"
eclipseJdt,1436.json,0f1f2b20ab9c02179f9ab0cd2826219f39372374,"@@ -1,6 +1,8 @@
 public int match(ReferenceExpression node, MatchingNodeSet nodeSet) {
 	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (!matchesName(this.pattern.selector, node.selector)) return IMPOSSIBLE_MATCH;
+	if (node.selector != null &&  Arrays.equals(node.selector, org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init))
+		return IMPOSSIBLE_MATCH; // :: new
 	nodeSet.mustResolve = true;
 	return nodeSet.addMatch(node, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 }",Buggy,"Fix for [Bug][458614][1.8][search] Constructor reference not found in
search",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

*   The commit message explicitly states ""Fix for [Bug][458614][1.8][search]"". This strongly suggests the commit is intended to resolve a bug.
*   The message also mentions ""Constructor reference not found in search,"" indicating the bug is related to searching for constructor references (using the `::new` syntax).

**2. Code Diff Analysis:**

*   The code change adds a condition to the `match` method that checks if the `node.selector` is equal to `org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init`.
*   If this condition is true, the method returns `IMPOSSIBLE_MATCH`.
*   The comment ""// :: new"" clarifies that this check is specifically for constructor references.

**3. Reasoning:**

The code diff directly addresses the issue described in the commit message. The added condition prevents the `match` method from incorrectly matching constructor references when searching. The `ConstantPool.Init` likely represents the internal representation of the `<init>` method (constructor) in the compiler's constant pool. By adding this check, the code ensures that constructor references are handled correctly during search operations. The original code likely failed to properly handle constructor references, leading to the bug reported in the commit message.

**Conclusion:**

The commit message clearly indicates a bug fix, and the code changes directly address the problem of incorrectly matching constructor references during search. The added condition prevents the incorrect matching of constructor references, aligning with the commit message's description.

**Buggy**"
eclipseJdt,23118.json,01fe9b211e794ff01834ebe0e40bd02cb1110d75,"@@ -1,30 +1,39 @@
 public static char[] getSignatureSimpleName(char[] typeSignature) {
 	if(typeSignature == null) return CharOperation.NO_CHAR;
 
 	char[] qualifiedType = Signature.toCharArray(typeSignature);
 
 	int dotCount = 0;
 	indexFound: for(int i = 0; i < typeSignature.length; i++) {
 		switch(typeSignature[i]) {
 			case C_DOT:
 				dotCount++;
 				break;
 			case C_GENERIC_START:
 				break indexFound;
 			case C_DOLLAR:
 				break indexFound;
 		}
 	}
 
 	if(dotCount > 0) {
+		int typeStart = 0;
 		for(int i = 0; i < qualifiedType.length; i++) {
-			if(qualifiedType[i] == '.') {
-				dotCount--;
+			switch (qualifiedType[i]) {
+				case '.':
+					dotCount--;
+					break;
+				case ' ':
+					typeStart = i+1;
+					break;
 			}
 			if(dotCount <= 0) {
-				return CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				char[] simpleName = CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				if (typeStart > 0 && typeStart < qualifiedType.length)
+					return CharOperation.concat(CharOperation.subarray(qualifiedType, 0, typeStart), simpleName);
+				return simpleName;
 			}
 		}
 	}
 	return qualifiedType;
 }",Buggy,"Bug 463533 - Signature.getSignatureSimpleName() returns different
results for resolved and unresolved extends

Change-Id: Iaa8b4273dab596b84734326ad99f775bbe74a330
Signed-off-by: Stephan Herrmann <stephan.herrmann@berlin.de>",Buggy,"The commit message indicates a bug fix related to the `Signature.getSignatureSimpleName()` method. The method was returning different results for resolved and unresolved extends clauses.

The code changes modify the logic within the `if(dotCount > 0)` block. The original code simply returned a subarray starting from the index after the last dot. The modified code now includes a `typeStart` variable to handle cases where there might be spaces in the qualified type name. It also concatenates the part before the space with the simple name if a space is found.

The addition of handling spaces and concatenating parts of the qualified name suggests that the original implementation was not correctly handling all possible cases, leading to incorrect results. This aligns with the bug description in the commit message. The changes address a specific issue where the method's output differed based on whether the extends clause was resolved or unresolved, implying a bug in the original logic.

Therefore, the changes indicate a bug fix.

**Buggy**
"
eclipseJdt,26368.json,04a3935ecc2f9e24622bcfddda7aa95a0643fdd7,"@@ -1 +1,10 @@
-    public String toString(){ return _arrayBinding.toString(); }

+    public String toString(){ 
+    	final ITypeBinding elementType = _arrayBinding.getElementType();
+    	final StringBuilder buffer = new StringBuilder();
+    	String name = elementType.getQualifiedName();
+    	buffer.append(name);
+		for( int i=0, dim = _arrayBinding.getDimensions(); i<dim; i++ )
+			buffer.append(""[]""); //$NON-NLS-1$
+		
+		return buffer.toString();
+    }",Buggy,"CR244832 | Annotation member that returns Class[] produces error in generated ControlBean

fixed ArrayType toString()

drt: apt.core
",Buggy,"The commit message indicates a fix related to how `Class[]` is handled in generated ControlBeans, specifically addressing an error that occurs when an annotation member returns `Class[]`. The code diff shows a change in the `toString()` method of what appears to be an `ArrayType` class. The original implementation simply delegated to `_arrayBinding.toString()`. The new implementation retrieves the element type of the array, gets its qualified name, and then appends ""[]"" for each dimension of the array. This suggests that the original `toString()` method was not correctly representing array types, particularly `Class[]`, which caused the error mentioned in the commit message. The new implementation constructs a string representation that includes the element type and the array dimensions, which is a more accurate representation of an array type. This change directly addresses the issue described in the commit message.

**Buggy**"
eclipseJdt,1082.json,41e3b1756b128250deb0f9b82843aa520278c691,"@@ -1,53 +1,53 @@
 void matchReportReference(Expression expr, int lastIndex, TypeBinding refBinding, MatchLocator locator) throws CoreException {
 
 	// Look if there's a need to special report for parameterized type
 	if (refBinding.isParameterizedType() || refBinding.isRawType()) {
 
 		// Try to refine accuracy
 		ParameterizedTypeBinding parameterizedBinding = (ParameterizedTypeBinding)refBinding;
 		updateMatch(parameterizedBinding, this.pattern.getTypeArguments(), this.pattern.hasTypeParameters(), 0, locator);
 		
 		// See whether it is necessary to report or not
 		if (match.getRule() == 0) return; // impossible match
 		boolean report = (this.isErasureMatch && match.isErasure()) || (this.isEquivalentMatch && match.isEquivalent()) || match.isExact();
 		if (!report) return;
 
 		// Make a special report for parameterized types if necessary
 		 if (refBinding.isParameterizedType() && this.pattern.hasTypeArguments())  {
 			TypeReference typeRef = null;
 			TypeReference[] typeArguments = null;
 			if (expr instanceof ParameterizedQualifiedTypeReference) {
 				typeRef = (ParameterizedQualifiedTypeReference) expr;
 				typeArguments = ((ParameterizedQualifiedTypeReference) expr).typeArguments[lastIndex];
 			}
 			else if (expr instanceof ParameterizedSingleTypeReference) {
 				typeRef = (ParameterizedSingleTypeReference) expr;
 				typeArguments = ((ParameterizedSingleTypeReference) expr).typeArguments;
 			}
 			if (typeRef != null) {
 				locator.reportAccurateParameterizedTypeReference(match, typeRef, lastIndex, typeArguments);
 				return;
 			}
 		}
 	} else if (this.pattern.hasTypeArguments()) { // binding has no type params, compatible erasure if pattern does
 		match.setRule(SearchPattern.R_ERASURE_MATCH);
 	}
 
 	// Report match
 	if (expr instanceof ArrayTypeReference) {
 		locator.reportAccurateTypeReference(match, expr, this.pattern.simpleName);
 		return;
 	}
 	if (refBinding.isLocalType()) {
-		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=84049
+		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=82673
 		LocalTypeBinding local = (LocalTypeBinding) refBinding;
 		IJavaElement focus = ((InternalSearchPattern)pattern).focus;
 		if (focus != null && local.enclosingMethod != null && focus.getParent().getElementType() == IJavaElement.METHOD) {
 			IMethod method = (IMethod) focus.getParent();
 			if (!CharOperation.equals(local.enclosingMethod.selector, method.getElementName().toCharArray())) {
 				return;
 			}
 		}
 	}
 	locator.report(match);
 }",Buggy,"Fix bug reference
",Buggy,"The commit message ""Fix bug reference"" indicates that the commit aims to correct a bug related to a reference. The diff includes a change in a comment referencing a bug ID. Specifically, the bug ID is changed from 84049 to 82673. This change suggests that the original bug reference was incorrect and has been updated to point to the correct bug report. Although the code itself remains unchanged, the correction of the bug reference implies that there was an issue with the original reference, indicating a bug fix.

**Buggy**"
eclipseJdt,25078.json,71d81627399572ca242308eb9b1f8436f445d7df,"@@ -1,9 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
 		TypeBinding type = super.resolveType(scope);
-		if (type instanceof PolyTypeBinding)
+		if (type == null || type instanceof ProblemReferenceBinding || type instanceof PolyTypeBinding)
 			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 440731 - [1.8][code select] Hover, F3 doesn't work for method
reference in method invocation of overloaded method

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"Okay, let's analyze the provided commit message and Java code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states ""Fixed Bug 440731"". This strongly suggests that the commit addresses a bug.
    *   The message also provides context: ""[1.8][code select] Hover, F3 doesn't work for method reference in method invocation of overloaded method"". This indicates the bug was related to code selection (hover/F3 functionality) in Java 1.8, specifically when dealing with method references within method invocations of overloaded methods.

2.  **Code Diff Analysis:**
    *   The code diff modifies the `resolveType` method.
    *   The original code had a check `if (type instanceof PolyTypeBinding)`. The modified code adds `type == null || type instanceof ProblemReferenceBinding ||` before the original check.
    *   The added condition `type == null` suggests that the original code was not handling cases where `type` could be null, potentially leading to a `NullPointerException` or incorrect behavior.
    *   The added condition `type instanceof ProblemReferenceBinding` suggests that the original code was not handling cases where `type` could be a `ProblemReferenceBinding`, potentially leading to incorrect behavior when resolving types that have errors.

3.  **Correlation:**
    *   The commit message describes a problem with code selection in a specific scenario (method references in overloaded method invocations). The code change addresses potential null values and problem bindings during type resolution. It is plausible that these unhandled cases in type resolution were causing the code selection (hover/F3) to fail in the scenario described in the commit message.

**Conclusion:**

The commit message explicitly indicates a bug fix. The code changes introduce null checks and handle `ProblemReferenceBinding`, which were likely causing the bug described in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
eclipseJdt,25078.json,a5e431ef5dd3e4ca66d16fb63b6f792ceeb0643b,"@@ -1,7 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
-		super.resolveType(scope);
+		TypeBinding type = super.resolveType(scope);
+		if (type instanceof PolyTypeBinding)
+			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 424110 - [1.8][hovering] Hover, F3 does not work for method
reference in method invocation",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed Bug 424110 - [1.8][hovering] Hover, F3 does not work for method reference in method invocation"" clearly states that the commit is intended to fix a bug. The bug is related to the ""hovering"" and ""F3"" functionality (likely related to code navigation/information display in an IDE) not working correctly for method references within method invocations, specifically in Java 8.

**Git Diff Analysis:**

The diff shows a change in the `resolveType` method.  Let's break down the changes:

1.  **`TypeBinding type = super.resolveType(scope);`**:  The result of the superclass's `resolveType` method is now stored in a local variable `type`.

2.  **`if (type instanceof PolyTypeBinding) return type;`**: This is the core of the change. It checks if the resolved type is a `PolyTypeBinding`. If it is, the method immediately returns the `type`.

**Reasoning:**

The original code directly called `super.resolveType(scope)` without checking the result. The added `if` condition suggests that there was a case where `super.resolveType(scope)` was returning a `PolyTypeBinding` that was causing the subsequent code to fail, leading to the bug described in the commit message.  The `PolyTypeBinding` likely represents a polymorphic type, and the original code was not handling it correctly, which would explain why ""hovering"" and ""F3"" were not working as expected. By explicitly returning the `PolyTypeBinding` in this case, the code now handles this specific type of result correctly, resolving the bug.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes introduce a conditional check and early return that addresses a specific type of binding (`PolyTypeBinding`) that was previously not handled correctly. This strongly suggests that the changes are indeed a bug fix.

**Buggy**
"
eclipseJdt,3790.json,293a53a19363893dc72bcd9c4bf117c4e143553f,"@@ -1,3 +1,3 @@
 	public TypeBinding[] genericTypeArguments() {
-		return null;
+		return this.resolvedTypeArguments;
 	}",Buggy,"Fixed Bug 420580 - [1.8][compiler] ReferenceExpression drops explicit
type arguments",Buggy,"The commit message indicates a fix for a bug where `ReferenceExpression` drops explicit type arguments. The code change replaces returning `null` with returning `this.resolvedTypeArguments` in the `genericTypeArguments()` method. This change directly addresses the bug described in the commit message by ensuring that the resolved type arguments are returned instead of being dropped (returning null). This is a clear indication of a bug fix.

**Buggy**
"
eclipseJdt,144.json,63326b7a3054b32190c64b8268487d927040f016,"@@ -1,4 +1,4 @@
 	public Element getEnclosingElement()
 	{
-		return null;
+		return getGenericElement();
 	}",Buggy,"HEAD - Fixed bug 342470: javax.lang.model.element.Element.getEnclosingElement() doesn't return null for type parameter
",Buggy,"The commit message indicates a fix for bug 342470, specifically addressing an issue where `javax.lang.model.element.Element.getEnclosingElement()` was incorrectly returning `null` for type parameters.

The diff shows that the method `getEnclosingElement()`'s return value has been changed from `null` to `getGenericElement()`. This change directly addresses the bug described in the commit message. The original implementation was clearly incorrect as it always returned `null`. The updated implementation now returns the enclosing generic element, which is the correct behavior for type parameters.

Therefore, the changes indicate a bug fix.

**Buggy**
"
eclipseJdt,11825.json,ee5e3843269e8af70318f7535175e87d19c752b4,"@@ -1,10 +1,11 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
 			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
+			: binding.isPrivate() ? MethodHandleRefKindInvokeSpecial
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 439889 - [1.8][compiler] [lambda] Deserializing lambda fails
with IllegalArgumentException: ""Invalid lambda deserialization""",Buggy,"The commit message indicates a bug fix related to lambda deserialization in the compiler for Java 1.8. The IllegalArgumentException ""Invalid lambda deserialization"" is mentioned, suggesting an issue during the process of reconstructing lambda expressions from a serialized form.

The code diff modifies the `literalIndexForMethodHandle` method. Specifically, it adds a condition to check if the method is private and, if so, sets the `referenceKind` to `MethodHandleRefKindInvokeSpecial`. This change seems to address a scenario where private methods within a lambda expression were not being correctly handled during deserialization. The original code might have been missing this check, leading to incorrect method handle resolution and the observed IllegalArgumentException.

The addition of the `binding.isPrivate()` check aligns with the bug fix described in the commit message. It suggests that the previous implementation had a flaw in handling private methods within lambda expressions during deserialization, which could lead to the ""Invalid lambda deserialization"" error.

**Buggy**
"
eclipseJdt,11825.json,6a493caae74ed7f294c44634abfb786fa2111d3d,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421797 - [1.8][compiler] ClassFormatError with default 
methods & I.super.foo() syntax ",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixed Bug 421797 - \[1.8][compiler] ClassFormatError with default methods & I.super.foo() syntax"" clearly states that this commit is intended to fix a bug. The bug is related to `ClassFormatError` in the compiler when dealing with default methods and the `I.super.foo()` syntax (which is used to invoke interface methods from within a class that implements the interface).

2.  **Code Diff:** The code diff modifies the `literalIndexForMethodHandle` method. Specifically, it changes how the `referenceKind` is determined for interface methods.

    *   **Original Code:** `isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface`
    *   **Modified Code:** `isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface`

    The change adds a check for `binding.isPrivate()`. If the interface method is private, the `referenceKind` is set to `MethodHandleRefKindInvokeSpecial`. Otherwise, it defaults to `MethodHandleRefKindInvokeInterface`.

3.  **Alignment and Bug Fix Indication:** The change directly addresses the scenario described in the commit message. The original code likely incorrectly handled private interface methods (default methods) called via `I.super.foo()`, leading to the `ClassFormatError`. By adding the check for `binding.isPrivate()` and using `MethodHandleRefKindInvokeSpecial` when appropriate, the code now correctly handles this case. This is a logical correction to handle a specific scenario that was previously causing an error.

**Conclusion:**

The commit message explicitly states that it fixes a bug, and the code changes align with the description of the bug. The code modification introduces a check for private interface methods, which likely resolves the `ClassFormatError` that was occurring when calling default methods using the `I.super.foo()` syntax. Therefore, this is a bug fix.

**Buggy**
"
eclipseJdt,11825.json,ffd96339198fcdfc38a0c44d4da9dfd0a59d21fc,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421712 - [1.8][compiler] java.lang.NoSuchMethodError with
lambda expression in interface default method.",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed Bug 421712 - [1.8][compiler] java.lang.NoSuchMethodError with lambda expression in interface default method"" clearly states that this commit is intended to fix a bug.  The bug involves a `java.lang.NoSuchMethodError` occurring when using lambda expressions within interface default methods in Java 8.

**Git Diff Analysis:**

The diff modifies the `literalIndexForMethodHandle` method.  Specifically, it changes how the `referenceKind` is determined when the method being referenced is in an interface.

*   **Original Code:** `isInterface ? MethodHandleRefKindInvokeInterface`
*   **Modified Code:** `isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface`

The original code always assumed `MethodHandleRefKindInvokeInterface` when dealing with interfaces. The modified code now checks if the method is static. If it's a static method in an interface, it uses `MethodHandleRefKindInvokeStatic`; otherwise, it defaults to `MethodHandleRefKindInvokeInterface`.

**Reasoning:**

The original code incorrectly assumed that all interface methods should be invoked using `MethodHandleRefKindInvokeInterface`. However, static methods in interfaces are invoked differently (using `MethodHandleRefKindInvokeStatic`). The change addresses this oversight. The `NoSuchMethodError` likely occurred because the wrong method handle kind was being used when invoking static methods defined in interfaces, especially when those methods were being accessed via a lambda expression. The fix correctly distinguishes between static and non-static interface methods when determining the method handle kind.

**Conclusion:**

The commit message explicitly states a bug fix, and the code change addresses a specific scenario that would lead to a `NoSuchMethodError`. The change involves a logical correction in how method handles are determined for interface methods, which directly relates to the bug described in the commit message.

**Buggy**
"
eclipseJdt,27787.json,6c76d390a016d2bc042688766f03adca637f8318,"@@ -1,4 +1,9 @@
 	public int run(InputStream in, OutputStream out, OutputStream err, String... arguments) {
-		boolean succeed = new Main(new PrintWriter(new OutputStreamWriter(out)), new PrintWriter(new OutputStreamWriter(err)), true/*systemExit*/, null/*options*/, null/*progress*/).compile(arguments);
+		boolean succeed = new Main(
+				new PrintWriter(new OutputStreamWriter(out != null ? out : System.out)),
+				new PrintWriter(new OutputStreamWriter(err != null ? err : System.err)),
+				true/* systemExit */,
+				null/* options */,
+				null/* progress */).compile(arguments);
 		return succeed ? 0 : -1;
 	}",Buggy,"Fixed Bug 426434 - EclipseCompiler#run() crashes when null out/err
passed by client",Buggy,"The commit message indicates a fix for a crash in `EclipseCompiler#run()` when `null` is passed as `out` or `err`.

The code diff modifies the `EclipseCompiler#run()` method to handle `null` values for the `out` and `err` `OutputStream` parameters. It now uses `System.out` and `System.err` as default streams if `out` or `err` are `null`, respectively. This prevents a `NullPointerException` from being thrown when a `PrintWriter` is constructed with a `null` `OutputStreamWriter`.

The code change directly addresses the issue described in the commit message. The original code would crash with a `NullPointerException` if `out` or `err` were `null`. The modified code provides a fallback to `System.out` and `System.err`, preventing the crash. This is a clear bug fix.

**Buggy**"
eclipseJdt,16316.json,f372eb4169324cad87b7cff6d25b5939cf1574c9,"@@ -1,4 +1,4 @@
 	public boolean isLocalTypeDeclaration() {
 		ASTNode parent = getParent();
-		return (parent instanceof Block);
+		return (parent instanceof TypeDeclarationStatement);
 	}",Buggy,"Fix TypeDeclaration.isLocalTypeDeclaration (bug #10468)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Fix TypeDeclaration.isLocalTypeDeclaration (bug #10468)"" explicitly states that this commit is intended to fix a bug in the `isLocalTypeDeclaration` method of the `TypeDeclaration` class. The presence of ""Fix"" and a bug number strongly suggests a bug fix.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `isLocalTypeDeclaration` method.
    *   The original code checked if the parent of the `TypeDeclaration` was a `Block`.
    *   The modified code checks if the parent is a `TypeDeclarationStatement`.
    *   The change suggests that the original logic was incorrect in identifying local type declarations. A local type declaration in Java is declared inside a block of code, but specifically within a `TypeDeclarationStatement`. The original code was too broad, incorrectly identifying type declarations within any block as local.

3.  **Synthesis:**
    *   The commit message clearly indicates a bug fix.
    *   The code change modifies the logic of `isLocalTypeDeclaration` to correctly identify local type declarations by checking for `TypeDeclarationStatement` instead of just `Block`. This correction aligns with the intent of fixing a bug related to the identification of local type declarations.

**Conclusion:**

**Buggy**
"
eclipseJdt,5374.json,8739fac6b0362a32fbe52c714ff661e202a84e1e,"@@ -1,31 +1,31 @@
 public void resolve(BlockScope upperScope) {
 	// special scope for secret locals optimization.
 	this.scope = new BlockScope(upperScope);
 	TypeBinding type = this.expression.resolveType(this.scope);
-	if (type == null)
-		return;
-	switch (type.id) {
-		case T_boolean :
-		case T_char :
-		case T_float :
-		case T_double :
-		case T_byte :
-		case T_short :
-		case T_int :
-		case T_long :
-			this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
-			break;
-		case T_void :
-			this.scope.problemReporter().illegalVoidExpression(this.expression);
-			break;
-		case T_null :
-			this.scope.problemReporter().invalidNullToSynchronize(this.expression);
-			break;
+	if (type != null) {
+		switch (type.id) {
+			case T_boolean :
+			case T_char :
+			case T_float :
+			case T_double :
+			case T_byte :
+			case T_short :
+			case T_int :
+			case T_long :
+				this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
+				break;
+			case T_void :
+				this.scope.problemReporter().illegalVoidExpression(this.expression);
+				break;
+			case T_null :
+				this.scope.problemReporter().invalidNullToSynchronize(this.expression);
+				break;
+			}
+			//continue even on errors in order to have the TC done into the statements
+			this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
+			this.scope.addLocalVariable(this.synchroVariable);
+			this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
+			this.expression.computeConversion(this.scope, type, type);
 	}
-	//continue even on errors in order to have the TC done into the statements
-	this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
-	this.scope.addLocalVariable(this.synchroVariable);
-	this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
-	this.expression.computeConversion(this.scope, type, type);
 	this.block.resolveUsing(this.scope);
 }",Buggy,"Bug 465048 - Bindings are null inside synchronized blocks

Resolve the block-part of a synchronized block, even if the synchronized
variable can't be resolved.

Change-Id: I3ed513fa21b10ae9c7fa39f1a94fe85fb98e5eb9
Signed-off-by: Carmi Grushko <carmi@google.com>
",Buggy,"The commit message indicates a fix for a bug where bindings were null inside synchronized blocks. The code change moves the block resolution logic inside an `if (type != null)` block. This ensures that the block is resolved even if the synchronized variable's type cannot be resolved, which aligns with the commit message's intention to ""Resolve the block-part of a synchronized block, even if the synchronized variable can't be resolved."" This change addresses a scenario where a null `type` would prevent the block from being resolved, leading to incorrect or incomplete compilation.

**Buggy**"
eclipseJdt,25918.json,8986600df161cd7584749fb6a28497a308fa862c,"@@ -1,5 +1,3 @@
 public boolean requireExtendedRecovery() {
-	if (this.assistNode instanceof TypeReference || this.assistNode instanceof CompletionOnKeyword2)
-		return false;
 	return lastIndexOfElement(K_LAMBDA_EXPRESSION_DELIMITER) >= 0;
 }",Buggy,"Fixed Bug 427463 - [1.8][content assist] No completions available in
throw statement within lambda body ",Buggy,"The commit message indicates a fix for a bug where code completion was not working within a lambda body's throw statement. The code diff removes a condition that checks if the assist node is a TypeReference or CompletionOnKeyword2 before returning false. This change likely enables code completion in more scenarios, including the throw statement within a lambda body, thus fixing the bug described in the commit message.

Reasoning:

1.  **Commit Message Analysis:** The commit message explicitly states a bug fix related to code completion within a lambda expression's throw statement.
2.  **Code Diff Analysis:** The code diff removes a condition that could potentially prevent code completion in certain scenarios.
3.  **Relevance:** The removal of the condition directly relates to enabling code completion in more contexts, aligning with the bug fix described in the commit message.

Conclusion: **Buggy**"
eclipseJdt,17024.json,c554caa948bbe051a5aeee635338c74832d8067f,"@@ -1,16 +1,15 @@
 	public static Token fromCurrent(Scanner scanner, int currentToken) {
 		int start = scanner.getCurrentTokenStartPosition();
 		int end = scanner.getCurrentTokenEndPosition();
 		if (currentToken == TokenNameCOMMENT_LINE) {
 			// don't include line separator
-			String source = scanner.getCurrentTokenString();
-			for (int i = source.length() - 1; i > 0; i--) {
-				char c = source.charAt(i);
+			while(end >= start) {
+				char c = scanner.source[end];
 				if (c != '\r' && c != '\n')
 					break;
 				end--;
 			}
 		}
 		Token token = new Token(start, end, currentToken);
 		return token;
 	}",Buggy,"Fixed bug 471090: Java Code Formatter breaks code if single line
comments contains unicode escape

Change-Id: Id2c901c7853e4062182163ef714afc0e43833baf
Signed-off-by: Till Brychcy <register.eclipse@brychcy.de>
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed bug 471090: Java Code Formatter breaks code if single line comments contains unicode escape"" clearly states that the commit is intended to fix a bug. The bug is specifically related to the Java Code Formatter incorrectly handling single-line comments containing Unicode escapes.

**Code Diff Analysis:**

The code diff focuses on the `fromCurrent` method within a `Scanner` class, likely used by the Java Code Formatter. The original code extracts the comment string using `scanner.getCurrentTokenString()` and then iterates from the end of the string to remove trailing line separators (`\r` or `\n`). The modified code directly accesses the `scanner.source` character array and iterates backwards from `end` to `start` to remove trailing line separators.

**Reasoning:**

The original code used `scanner.getCurrentTokenString()`, which might have issues when dealing with Unicode escapes within the comment string. The modified code directly accesses the underlying character array (`scanner.source`) to avoid potential encoding or interpretation issues that `getCurrentTokenString()` might introduce. This change suggests that the original method was not correctly handling Unicode escapes in single-line comments, leading to the code formatter breaking. By directly accessing the character array, the code avoids any potential misinterpretation of Unicode escapes during string extraction. The change directly addresses the problem described in the commit message.

**Conclusion:**

The commit message explicitly states a bug fix, and the code changes address a potential issue with Unicode escapes in single-line comments, aligning with the bug description. Therefore, the changes indicate a bug fix.

**Buggy**
"
eclipseJdt,1305.json,63969f0e42bcdfc0a055ac0f3d2f47fa48cf3e29,"@@ -1,7 +1,9 @@
 protected void consumeCastExpressionLL1WithBounds() {
 	super.consumeCastExpressionLL1WithBounds();
 	if ((this.patternFineGrain & IJavaSearchConstants.CAST_TYPE_REFERENCE) != 0) {
 		CastExpression castExpression = (CastExpression) this.expressionStack[this.expressionPtr];
-		this.patternLocator.match(castExpression.type, this.nodeSet);
+		TypeReference[] typeReferences = ((IntersectionCastTypeReference) castExpression.type).typeReferences;
+		for (int i = 0, length = typeReferences.length; i < length; i++)
+			this.patternLocator.match(typeReferences[i], this.nodeSet);
 	}
 }",Buggy,"Fixed Bug 427537 - [1.8][search] CCE with search match location set to
cast type and intersection casts",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message ""Fixed Bug 427537 - [1.8][search] CCE with search match location set to cast type and intersection casts"" clearly states that it's addressing a bug (427537). The bug is described as a `ClassCastException` (CCE) occurring during a search operation when the search match location is set to a cast type, specifically involving intersection casts (introduced in Java 8).

**2. Analyzing the Code Diff:**

The code diff modifies the `consumeCastExpressionLL1WithBounds` method.  Here's a breakdown:

*   **Original Code:**
    ```java
    CastExpression castExpression = (CastExpression) this.expressionStack[this.expressionPtr];
    this.patternLocator.match(castExpression.type, this.nodeSet);
    ```
    This code assumes that `castExpression.type` is a simple `TypeReference`.

*   **Modified Code:**
    ```java
    CastExpression castExpression = (CastExpression) this.expressionStack[this.expressionPtr];
    TypeReference[] typeReferences = ((IntersectionCastTypeReference) castExpression.type).typeReferences;
    for (int i = 0, length = typeReferences.length; i < length; i++)
    	this.patternLocator.match(typeReferences[i], this.nodeSet);
    ```
    This code now handles the case where `castExpression.type` is an `IntersectionCastTypeReference`. It retrieves the individual `TypeReference` elements within the intersection cast and iterates through them, matching each one.

**3. Reasoning and Conclusion:**

The original code directly casted `castExpression.type` to `TypeReference` and then called `patternLocator.match`. This would cause a `ClassCastException` if `castExpression.type` was actually an `IntersectionCastTypeReference`. The modified code specifically checks for `IntersectionCastTypeReference`, extracts the underlying `TypeReference` array, and iterates through it, calling `patternLocator.match` on each individual `TypeReference`. This resolves the `ClassCastException` described in the commit message. The code change directly addresses the bug described in the commit message by handling a specific type of cast expression that was previously causing a crash.

**Conclusion:** **Buggy**
"
eclipseJdt,17644.json,f7608e255b7a2e7cde81de854a6575f0fb055925,"@@ -1,20 +1,20 @@
 public char[] getMainTypeName() {
 	if (this.mainTypeName == null) {
 		int start = CharOperation.lastIndexOf('/', this.fileName) + 1;
 		if (start == 0 || start < CharOperation.lastIndexOf('\\', this.fileName))
 			start = CharOperation.lastIndexOf('\\', this.fileName) + 1;
-		int separator = CharOperation.indexOf('|', this.fileName) + 1;
+		int separator = CharOperation.lastIndexOf('|', this.fileName) + 1;
 		if (separator > start) // case of a .class file in a default package in a jar
 			start = separator;
 
 		int end = CharOperation.lastIndexOf('$', this.fileName);
 		if (end == -1 || !Util.isClassFileName(this.fileName)) {
 			end = CharOperation.lastIndexOf('.', this.fileName);
 			if (end == -1)
 				end = this.fileName.length;
 		}
 
 		this.mainTypeName = CharOperation.subarray(this.fileName, start, end);
 	}
 	return this.mainTypeName;
 }",Buggy,"Fix for bug 515484 [1.9] [dom ast] ast not generated for
module-info.class  ",Buggy,"The commit message indicates a fix for bug 515484, specifically related to the AST generation for `module-info.class` files. The code diff modifies the `getMainTypeName()` method, which extracts the main type name from a file name. The change involves replacing `CharOperation.indexOf('|', this.fileName)` with `CharOperation.lastIndexOf('|', this.fileName)`. This suggests that the original code was incorrectly identifying the start index when a '|' character was present in the filename, potentially leading to an incorrect type name being extracted, especially in cases involving jar files. The use of `lastIndexOf` is more appropriate to find the last occurrence of the separator. This correction directly relates to the bug described in the commit message.

**Buggy**
"
eclipseJdt,19098.json,302894c7bc98fd880776f5e317788c7b3da88ebe,"@@ -1,23 +1,23 @@
 protected void recordParticipantResult(CompilationParticipantResult result) {
 	// any added/changed/deleted generated files have already been taken care
 	// just record the problems and dependencies - do not expect there to be many
 	// must be called after we're finished with the compilation unit results but before incremental loop adds affected files
 	CategorizedProblem[] problems = result.problems;
 	if (problems != null && problems.length > 0) {
 		// existing problems have already been removed so just add these as new problems
 		this.notifier.updateProblemCounts(problems);
 		try {
 			storeProblemsFor(result.sourceFile, problems);
 		} catch (CoreException e) {
 			// must continue with compile loop so just log the CoreException
 			Util.log(e, ""JavaBuilder logging CompilationParticipant's CoreException to help debugging""); //$NON-NLS-1$
 		}
 	}
 
 	String[] dependencies = result.dependencies;
 	if (dependencies != null) {
-		ReferenceCollection refs = (ReferenceCollection) this.newState.references.get(result.sourceFile.typeLocator());
+		ReferenceCollection refs = this.newState.references.get(result.sourceFile.typeLocator());
 		if (refs != null)
 			refs.addDependencies(dependencies);
 	}
 }",Buggy,"Bug 563030 - builder.State: SimpleLookupTable performance does not
scale

- resolve new warning

Change-Id: I20cd500a7872eaa5cbda532f8f316d1a4168a3d9
",Buggy,"The commit message indicates a performance improvement related to `builder.State: SimpleLookupTable` and the resolution of a new warning. The code diff modifies the `recordParticipantResult` method in `CompilationParticipantResult`.

The change involves removing a cast `(ReferenceCollection)` when retrieving `refs` from `this.newState.references.get(result.sourceFile.typeLocator())`. This suggests that the type returned by `this.newState.references.get()` was already `ReferenceCollection`, and the cast was unnecessary. Removing the cast can improve performance by avoiding an unnecessary operation.

The commit message mentions a performance improvement related to `SimpleLookupTable`. While the code change itself doesn't directly involve `SimpleLookupTable`, it's possible that `this.newState.references` is a `SimpleLookupTable` or uses it internally. Removing the unnecessary cast could contribute to the overall performance improvement mentioned in the commit message.

The commit message also mentions resolving a new warning. The removal of the cast could be related to resolving a compiler warning about an unnecessary cast.

Given the context, the change is likely related to the performance improvement and warning resolution mentioned in the commit message. The removal of the unnecessary cast suggests a bug fix or optimization.

**Buggy**"
eclipseJdt,25968.json,15950deb058261ec048eb3ddbd1c2ef13df604e6,"@@ -1,29 +1,30 @@
 private CodeSnippetToCuMapper getMapper() {
 	if (this.mapper == null) {
 		char[] varClassName = null;
 		VariablesInfo installedVars = this.context.installedVars;
 		if (installedVars != null) {
 			char[] superPackageName = installedVars.packageName;
 			if (superPackageName != null && superPackageName.length != 0) {
 				varClassName = CharOperation.concat(superPackageName, installedVars.className, '.');
 			} else {
 				varClassName = installedVars.className;
 			}
 
 		}
 		this.mapper = new CodeSnippetToCuMapper(
 			this.codeSnippet,
 			this.context.packageName,
 			this.context.imports,
 			getClassName(),
 			varClassName,
 			this.context.localVariableNames,
 			this.context.localVariableTypeNames,
 			this.context.localVariableModifiers,
 			this.context.declaringTypeName,
-			this.context.lineSeparator
+			this.context.lineSeparator,
+			CompilerOptions.versionToJdkLevel(this.options.get(JavaCore.COMPILER_COMPLIANCE))
 		);
 
 	}
 	return this.mapper;
 }",Buggy,"HEAD - Fixed bug 345334: CodeSnippet's run method is missing @Override annotation
",NotBuggy,"The commit message states that the `@Override` annotation was missing in the `run` method of the `CodeSnippet` class. However, the provided diff does not show any changes related to the `run` method or the addition of the `@Override` annotation. The diff focuses on the `getMapper` method and its internal logic, specifically how the `CodeSnippetToCuMapper` is instantiated and configured. The changes include adding the compiler compliance level to the constructor of `CodeSnippetToCuMapper`. These changes do not seem to be related to the missing `@Override` annotation mentioned in the commit message. Therefore, based on the provided diff, the changes do not align with the commit message.

**NotBuggy**"
eclipseJdt,4105.json,35e2019a40c179f5dd02638e5bff541c41227721,"@@ -1,6 +1,9 @@
 	void recordResolution(LookupEnvironment env, TypeBinding typeFound) {
-		if (typeFound != null && typeFound.isValidBinding())
-			for (int i = 0; i < env.resolutionListeners.length; i++) {
-				env.resolutionListeners[i].recordResolution(this, typeFound);
+		if (typeFound != null && typeFound.isValidBinding()) {
+			synchronized (env.root) {
+				for (int i = 0; i < env.root.resolutionListeners.length; i++) {
+					env.root.resolutionListeners[i].recordResolution(this, typeFound);
+				}
 			}
+		}
 	}",Buggy,"Fix for bug 519980 [9][search] search for a type does not return the
matches in provides 

Change-Id: I0059fa02a1040f10a8f4a949c07806018dee73b5",Buggy,"The commit message indicates a fix for bug 519980, specifically addressing an issue where searching for a type doesn't return matches in ""provides"". The code diff shows a change in the `recordResolution` method where the loop iterating through `env.resolutionListeners` is now synchronized on `env.root`. This synchronization suggests that the original code might have had concurrency issues, potentially leading to missed matches or incorrect results when multiple threads were involved in the type search. The synchronization ensures that the resolution listeners are accessed and updated in a thread-safe manner, which aligns with the bug fix described in the commit message.

**Buggy**"
eclipseJdt,11564.json,266fb0d2bc58e2f4f179d2c1279dee095ed53e45,"@@ -1,6 +1,6 @@
 public void generateSyntheticBodyForArrayConstructor(SyntheticMethodBinding methodBinding) {
 	initializeMaxLocals(methodBinding);
 	iload_0();
-	anewarray(((ArrayBinding) methodBinding.returnType).elementsType());
+	newArray(null, null, (ArrayBinding) methodBinding.returnType);
 	areturn();
 }",Buggy,"Fixed Bug 424444 - [1.8] VerifyError when constructor reference used
with primitive array",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Fixed Bug 424444 - \[1.8] VerifyError when constructor reference used with primitive array"" clearly states that the commit is intended to fix a bug. The bug is specifically a `VerifyError` occurring in Java 1.8 when using constructor references with primitive arrays. This strongly suggests a bug fix.

2.  **Code Diff Analysis:**
    *   The code diff shows a change within the `generateSyntheticBodyForArrayConstructor` method.
    *   The original code used `anewarray(((ArrayBinding) methodBinding.returnType).elementsType())`.
    *   The modified code uses `newArray(null, null, (ArrayBinding) methodBinding.returnType)`.
    *   The change replaces `anewarray` with `newArray`. The `anewarray` instruction is specifically for creating arrays of reference types. The `newArray` instruction (presumably a method call within the same class or a utility method) likely handles array creation more generically, potentially including primitive arrays. This suggests that the original code was not correctly handling the case of primitive arrays, leading to the `VerifyError`.

3.  **Relevance and Bug Fix Indication:**
    *   The code change directly addresses the issue described in the commit message. The original code's use of `anewarray` was likely the cause of the `VerifyError` when dealing with primitive arrays. The replacement with `newArray` suggests a more robust array creation mechanism that correctly handles both reference and primitive arrays. This aligns perfectly with the commit message's claim of fixing a bug related to constructor references and primitive arrays.

**Conclusion:**

The commit message explicitly states that a bug is being fixed, and the code changes directly relate to the described bug (handling primitive arrays in constructor references). The change from `anewarray` to `newArray` strongly suggests a correction to how arrays are created, specifically to address the `VerifyError` mentioned in the commit message.

**Buggy**
"
eclipseJdt,6346.json,409121a5eb3d3ef99ff5c31121bd10011631e82f,"@@ -1,61 +1,65 @@
 	private void checkAndSetModifiersForField(FieldBinding fieldBinding, FieldDeclaration fieldDecl) {
 		int modifiers = fieldBinding.modifiers;
 		final ReferenceBinding declaringClass = fieldBinding.declaringClass;
 		if ((modifiers & ExtraCompilerModifiers.AccAlternateModifierProblem) != 0)
 			problemReporter().duplicateModifierForField(declaringClass, fieldDecl);
 
 		if (declaringClass.isInterface()) {
 			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal;
 			// set the modifiers
 			modifiers |= IMPLICIT_MODIFIERS;
 
 			// and then check that they are the only ones
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != IMPLICIT_MODIFIERS) {
 				if ((declaringClass.modifiers  & ClassFileConstants.AccAnnotation) != 0)
 					problemReporter().illegalModifierForAnnotationField(fieldDecl);
 				else
 					problemReporter().illegalModifierForInterfaceField(fieldDecl);
 			}
 			fieldBinding.modifiers = modifiers;
 			return;
 		} else if (fieldDecl.getKind() == AbstractVariableDeclaration.ENUM_CONSTANT) {
 			// check that they are not modifiers in source
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != 0)
 				problemReporter().illegalModifierForEnumConstant(declaringClass, fieldDecl);
 
 			// set the modifiers
-			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum;
+			// https://bugs.eclipse.org/bugs/show_bug.cgi?id=267670. Force all enumerators to be marked
+			// as used locally. We are unable to track the usage of these reliably as they could be used
+			// in non obvious ways via the synthesized methods values() and valueOf(String) or by using 
+			// Enum.valueOf(Class<T>, String).
+			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum | ExtraCompilerModifiers.AccLocallyUsed;
 			fieldBinding.modifiers|= IMPLICIT_MODIFIERS;
 			return;
 		}
 
 		// after this point, tests on the 16 bits reserved.
 		int realModifiers = modifiers & ExtraCompilerModifiers.AccJustFlag;
 		final int UNEXPECTED_MODIFIERS = ~(ClassFileConstants.AccPublic | ClassFileConstants.AccPrivate | ClassFileConstants.AccProtected | ClassFileConstants.AccFinal | ClassFileConstants.AccStatic | ClassFileConstants.AccTransient | ClassFileConstants.AccVolatile);
 		if ((realModifiers & UNEXPECTED_MODIFIERS) != 0) {
 			problemReporter().illegalModifierForField(declaringClass, fieldDecl);
 			modifiers &= ~ExtraCompilerModifiers.AccJustFlag | ~UNEXPECTED_MODIFIERS;
 		}
 
 		int accessorBits = realModifiers & (ClassFileConstants.AccPublic | ClassFileConstants.AccProtected | ClassFileConstants.AccPrivate);
 		if ((accessorBits & (accessorBits - 1)) > 1) {
 			problemReporter().illegalVisibilityModifierCombinationForField(declaringClass, fieldDecl);
 
 			// need to keep the less restrictive so disable Protected/Private as necessary
 			if ((accessorBits & ClassFileConstants.AccPublic) != 0) {
 				if ((accessorBits & ClassFileConstants.AccProtected) != 0)
 					modifiers &= ~ClassFileConstants.AccProtected;
 				if ((accessorBits & ClassFileConstants.AccPrivate) != 0)
 					modifiers &= ~ClassFileConstants.AccPrivate;
 			} else if ((accessorBits & ClassFileConstants.AccProtected) != 0 && (accessorBits & ClassFileConstants.AccPrivate) != 0) {
 				modifiers &= ~ClassFileConstants.AccPrivate;
 			}
 		}
 
 		if ((realModifiers & (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile)) == (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile))
 			problemReporter().illegalModifierCombinationFinalVolatileForField(declaringClass, fieldDecl);
 
 		if (fieldDecl.initialization == null && (modifiers & ClassFileConstants.AccFinal) != 0)
 			modifiers |= ExtraCompilerModifiers.AccBlankFinal;
 		fieldBinding.modifiers = modifiers;
 	}",Buggy,"Fix for bug #267670
",Buggy,"The commit message ""Fix for bug #267670"" indicates that this commit is intended to address a specific bug.

The diff shows a modification to the `checkAndSetModifiersForField` method, specifically when handling enum constants. The change adds `ExtraCompilerModifiers.AccLocallyUsed` to the implicit modifiers for enum constants. The comment explicitly mentions the bug number ""https://bugs.eclipse.org/bugs/show_bug.cgi?id=267670"" and explains the reasoning behind the change: to force all enumerators to be marked as used locally because their usage cannot be reliably tracked due to synthesized methods like `values()` and `valueOf(String)`, or by using `Enum.valueOf(Class<T>, String)`.

This change directly addresses a bug related to the incorrect tracking of enum constant usage.

**Buggy**"
eclipseJdt,4552.json,cc7009a12280dbb7a101bdbfbf1048948dc0093c,"@@ -1,66 +1,72 @@
 protected void verifyDuplicationAndOrder(int length, TypeBinding[] argumentTypes, boolean containsUnionTypes) {
 	// Verify that the catch clause are ordered in the right way:
 	// more specialized first.
 	if (containsUnionTypes) {
 		int totalCount = 0;
 		ReferenceBinding[][] allExceptionTypes = new ReferenceBinding[length][];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			ReferenceBinding currentExceptionType = (ReferenceBinding) argumentTypes[i];
 			TypeReference catchArgumentType = this.catchArguments[i].type;
 			if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 				TypeReference[] typeReferences = ((UnionTypeReference) catchArgumentType).typeReferences;
 				int typeReferencesLength = typeReferences.length;
 				ReferenceBinding[] unionExceptionTypes = new ReferenceBinding[typeReferencesLength];
 				for (int j = 0; j < typeReferencesLength; j++) {
 					unionExceptionTypes[j] = (ReferenceBinding) typeReferences[j].resolvedType;
 				}
 				totalCount += typeReferencesLength;
 				allExceptionTypes[i] = unionExceptionTypes;
 			} else {
 				allExceptionTypes[i] = new ReferenceBinding[] { currentExceptionType };
 				totalCount++;
 			}
 		}
 		this.caughtExceptionTypes = new ReferenceBinding[totalCount];
 		this.caughtExceptionsCatchBlocks  = new int[totalCount];
 		for (int i = 0, l = 0; i < length; i++) {
 			ReferenceBinding[] currentExceptions = allExceptionTypes[i];
+			if (currentExceptions == null) continue;
 			loop: for (int j = 0, max = currentExceptions.length; j < max; j++) {
 				ReferenceBinding exception = currentExceptions[j];
 				this.caughtExceptionTypes[l] = exception;
 				this.caughtExceptionsCatchBlocks[l++] = i;
 				// now iterate over all previous exceptions
 				for (int k = 0; k < i; k++) {
 					ReferenceBinding[] exceptions = allExceptionTypes[k];
+					if (exceptions == null) continue;
 					for (int n = 0, max2 = exceptions.length; n < max2; n++) {
 						ReferenceBinding currentException = exceptions[n];
 						if (exception.isCompatibleWith(currentException)) {
 							TypeReference catchArgumentType = this.catchArguments[i].type;
 							if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 								catchArgumentType = ((UnionTypeReference) catchArgumentType).typeReferences[j];
 							}
 							this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 								catchArgumentType,
 								exception,
 								currentException);
 							break loop;
 						}
 					}
 				}
 			}
 		}
 	} else {
 		this.caughtExceptionTypes = new ReferenceBinding[length];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			this.caughtExceptionTypes[i] = (ReferenceBinding) argumentTypes[i];
 			for (int j = 0; j < i; j++) {
 				if (this.caughtExceptionTypes[i].isCompatibleWith(argumentTypes[j])) {
 					this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 						this.catchArguments[i].type,
 						this.caughtExceptionTypes[i],
 						argumentTypes[j]);
 				}
 			}
 		}
 	}
 }",Buggy,"Fixed bug 433879 - ArrayBinding cannot be cast to ReferenceBinding

Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>",Buggy,"The commit message ""Fixed bug 433879 - ArrayBinding cannot be cast to ReferenceBinding"" indicates a bug fix where a type casting error was occurring. The code diff shows that `argumentTypes[i]` which is a `TypeBinding`, was being cast to `ReferenceBinding` without checking its actual type. The added `if (argumentTypes[i] instanceof ArrayBinding) continue;` checks prevent the casting when the type is an `ArrayBinding`, resolving the reported bug. The same check is added in two different loops. Also, a null check is added for `currentExceptions` and `exceptions` to prevent null pointer exceptions.

**Buggy**"
eclipseJdt,6311.json,c36a6a2b662267e56067d121b7f34ae48cbcb692,"@@ -1,16 +1,18 @@
 void checkInheritedMethods(MethodBinding inheritedMethod, MethodBinding otherInheritedMethod) {
 
 	// the 2 inherited methods clash because of a parameterized type overrides a raw type
 	//		interface I { void foo(A a); }
 	//		class Y { void foo(A<String> a) {} }
 	//		abstract class X extends Y implements I { }
 	//		class A<T> {}
 	// in this case the 2 inherited methods clash because of type variables
 	//		interface I { <T, S> void foo(T t); }
 	//		class Y { <T> void foo(T t) {} }
 	//		abstract class X extends Y implements I {}
 
-	if (inheritedMethod.declaringClass.isInterface() || inheritedMethod.isStatic()) return;
+	if (inheritedMethod.isStatic()) return;
+	if (this.environment.globalOptions.complianceLevel < ClassFileConstants.JDK1_7 && inheritedMethod.declaringClass.isInterface())
+		return;  // JDK7 checks for name clashes in interface inheritance, while JDK6 and below don't. See https://bugs.eclipse.org/bugs/show_bug.cgi?id=354229
 
 	detectInheritedNameClash(inheritedMethod.original(), otherInheritedMethod.original());
 }",Buggy,"Fixed bug 354229: [compiler][1.7] Name clash error not being reported by
ecj.",Buggy,"The commit message indicates a fix for bug 354229, specifically related to a name clash error not being reported by the Eclipse Compiler for Java (ecj) under Java 7.

The code diff modifies the `checkInheritedMethods` method. The original code had a condition `inheritedMethod.declaringClass.isInterface() || inheritedMethod.isStatic()`. The modified code removes the interface check for compliance levels greater than or equal to JDK 1.7. The added comment explicitly mentions bug 354229 and explains that JDK7 checks for name clashes in interface inheritance, while JDK6 and below do not. This aligns perfectly with the commit message. The change ensures that name clash errors are correctly reported in ecj for Java 7, which was previously not happening.

Therefore, the changes indicate a bug fix.

**Buggy**
"
eclipseJdt,4397.json,7089fcfd703b096f125599a900905c00e15535fa,"@@ -1,30 +1,36 @@
 	public void getAllAnnotationContexts(int targetType, int typeParameterIndex, List allAnnotationContexts) {
 		AnnotationCollector collector = new AnnotationCollector(this, targetType, typeParameterIndex, allAnnotationContexts);
 		if (this.annotations != null) {
 			int annotationsLength = this.annotations.length;
 			for (int i = 0; i < annotationsLength; i++)
 				this.annotations[i].traverse(collector, (BlockScope) null);
 		}
 		switch(collector.targetType) {
 			case AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER_BOUND;
 				break;
 			case AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER_BOUND;
 		}
-		if (this.type != null && ((this.type.bits & ASTNode.HasTypeAnnotations) != 0)) {
-			collector.info2 = 0;
-			this.type.traverse(collector, (BlockScope) null);
+		int boundIndex = 0;
+		if (this.type != null) {
+			// boundIndex 0 is always a class
+			if (this.type.resolvedType.isInterface())
+				boundIndex = 1;
+			if ((this.type.bits & ASTNode.HasTypeAnnotations) != 0) {
+				collector.info2 = boundIndex;
+				this.type.traverse(collector, (BlockScope) null);
+			}
 		}
 		if (this.bounds != null) {
 			int boundsLength = this.bounds.length;
 			for (int i = 0; i < boundsLength; i++) {
 				TypeReference bound = this.bounds[i];
 				if ((bound.bits & ASTNode.HasTypeAnnotations) == 0) {
 					continue;
 				}
-				collector.info2 = i + 1;
+				collector.info2 = ++boundIndex;
 				bound.traverse(collector, (BlockScope) null);
 			}
 		}
 	}",Buggy,"Fixed Bug 415543 - [1.8][compiler] Incorrect bound index in
RuntimeInvisibleTypeAnnotations attrribute

Signed-off-by: Andrew Clement <aclement@gopivotal.com>",Buggy,"The commit message indicates a fix for an incorrect bound index in the `RuntimeInvisibleTypeAnnotations` attribute. The code diff modifies the `getAllAnnotationContexts` method, specifically addressing how the `boundIndex` is calculated and used when traversing type bounds.

Here's a breakdown of the changes and their relevance to the bug fix:

1.  **`boundIndex` Initialization:** The code now initializes `boundIndex` to 0.
2.  **Interface Check:** It checks if the resolved type of `this.type` is an interface. If it is, `boundIndex` is set to 1. This suggests that interfaces are treated differently in the context of type parameter bounds.
3.  **`collector.info2` Assignment:** The `collector.info2` is now set to `boundIndex` before traversing `this.type`.
4.  **Bound Traversal:** Inside the loop that iterates through `this.bounds`, `boundIndex` is incremented *before* being assigned to `collector.info2`. This is a crucial change from the original code, where `i + 1` was directly assigned to `collector.info2`.

The original code directly used the loop index `i + 1` as the bound index, which could be incorrect if the class/interface bound was not properly accounted for. The updated code explicitly manages the `boundIndex`, incrementing it as it traverses the bounds and handling the interface case. This ensures that the correct bound index is used when creating the `RuntimeInvisibleTypeAnnotations` attribute.

The changes directly address the issue described in the commit message, indicating a bug fix related to the calculation of the bound index.

**Buggy**"
eclipseJdt,5447.json,3c8db8654fc8e2927c75863ec1232c9bc3800c9b,"@@ -1,31 +1,34 @@
 	public TypeBinding resolveType(BlockScope scope) {
 
 		if (this.compilerAnnotation != null)
 			return this.resolvedType;
 
 		this.constant = Constant.NotAConstant;
 
 		ReferenceBinding containerAnnotationType = (ReferenceBinding) this.resolvedType;
 		if (!containerAnnotationType.isValidBinding())
 			containerAnnotationType = (ReferenceBinding) containerAnnotationType.closestMatch();
 		Annotation repeatingAnnotation = this.containees[0];
 		ReferenceBinding repeatingAnnotationType = (ReferenceBinding) repeatingAnnotation.resolvedType;
+		if (!repeatingAnnotationType.isDeprecated() && isTypeUseDeprecated(containerAnnotationType, scope)) {
+			scope.problemReporter().deprecatedType(containerAnnotationType, repeatingAnnotation);
+		}
 		checkContainerAnnotationType(repeatingAnnotation, scope, containerAnnotationType, repeatingAnnotationType, true); // true => repeated *use* site error reporting requested.
 		this.resolvedType = containerAnnotationType = repeatingAnnotationType.containerAnnotationType();
 		if (!this.resolvedType.isValidBinding())
 			return this.resolvedType;
 		
 		// OK, the declaration site of the repeating annotation type as well as the use site where the annotations actually repeat pass muster. 
 		MethodBinding[] methods = containerAnnotationType.methods();
 		MemberValuePair pair = memberValuePairs()[0];
 		
 		for (int i = 0, length = methods.length; i < length; i++) {
 			MethodBinding method = methods[i];
 			if (CharOperation.equals(method.selector, TypeConstants.VALUE)) {
 				pair.binding = method;
 				pair.resolveTypeExpecting(scope, method.returnType);
 			}
 		}
 		this.compilerAnnotation = scope.environment().createAnnotation((ReferenceBinding) this.resolvedType, computeElementValuePairs());
 		return this.resolvedType;
 	}",Buggy,"Follow up fixes for Bug 412153 - [1.8][compiler] Check validity of
annotations which may be repeatable",Buggy,"The commit message indicates a follow-up fix for bug 412153, which involves checking the validity of repeatable annotations. The code diff introduces a check for deprecated types. Specifically, it checks if the container annotation type is deprecated when the repeating annotation type is not. If the container annotation type is deprecated, a deprecation warning is reported. This change addresses a potential issue where deprecated container annotations were not being properly flagged when used with repeatable annotations.

The added `if` condition `!repeatingAnnotationType.isDeprecated() && isTypeUseDeprecated(containerAnnotationType, scope)` suggests a bug fix. The code now correctly identifies and reports deprecated container annotation types in the context of repeatable annotations, which was likely missed before.

**Buggy**"
eclipseJdt,3852.json,d15e79b3439bbaf42d22da90dbcfabf7ceac0248,"@@ -1,3 +1,3 @@
 public boolean isCompatibleWith(TypeBinding left, Scope scope) {
-	throw new UnsupportedOperationException(""Unexpected control flow, should not have reached Expression.isCompatibleWith""); //$NON-NLS-1$
+	return this.resolvedType != null && this.resolvedType.isCompatibleWith(left,  scope);
 }",Buggy,"Fixed Bug 426315 - [1.8][compiler] UnsupportedOperationException with
conditional expression ",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states ""Fixed Bug 426315"" and describes the bug as an ""UnsupportedOperationException"" occurring with a conditional expression in the compiler (likely Java 1.8). This strongly suggests a bug fix.

2.  **Code Diff:** The code diff shows a change within the `isCompatibleWith` method.
    *   The original code threw an `UnsupportedOperationException` with the message ""Unexpected control flow, should not have reached Expression.isCompatibleWith"". This indicates that the method was intentionally left unimplemented, likely because the code path leading to it was not expected to be executed. The exception suggests a flaw in the program's logic that caused this unexpected code path to be taken.
    *   The modified code replaces the exception with a `return` statement that checks if `this.resolvedType` is not null and then delegates the compatibility check to `this.resolvedType.isCompatibleWith(left, scope)`. This suggests that the developer has now implemented the correct logic for this code path.

3.  **Alignment:** The commit message and code diff are perfectly aligned. The commit message describes an `UnsupportedOperationException`, and the code diff removes that exception and replaces it with a proper implementation. This strongly indicates that the change addresses the bug described in the commit message.

4.  **Bug Fix Indicators:**
    *   The original code threw an exception, which is a clear sign of a bug or incomplete implementation.
    *   The modified code replaces the exception with functional code, indicating a correction of the bug.
    *   The commit message explicitly states that it's a bug fix.

**Conclusion:**

The evidence strongly suggests that the provided Git diff and commit message represent a bug fix. The code was throwing an `UnsupportedOperationException` in an unexpected code path, and the change replaces the exception with a proper implementation.

**Buggy**"
eclipseJdt,22376.json,883d74a76cfedfa8b3e490bff988420af3184ead,"@@ -1,23 +1,24 @@
 	private void pushOnCommentsStack(int start, int end) {
 
 		for (int i=start; i<=end; i++) {
+			if (this.scanner.commentPtr < i) break;
 			// First see if comment hasn't been already stored
 			int scannerStart = this.scanner.commentStarts[i]<0 ? -this.scanner.commentStarts[i] : this.scanner.commentStarts[i];
 			int commentStart = this.commentPtr == -1 ? -1 : (this.commentStarts[this.commentPtr]<0 ? -this.commentStarts[this.commentPtr] : this.commentStarts[this.commentPtr]);
 			if (commentStart == -1 ||  scannerStart > commentStart) {
 				int stackLength = this.commentStarts.length;
 				if (++this.commentPtr >= stackLength) {
 					System.arraycopy(
 						this.commentStarts, 0,
 						this.commentStarts = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 					System.arraycopy(
 						this.commentStops, 0,
 						this.commentStops = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 				}
 				this.commentStarts[this.commentPtr] = this.scanner.commentStarts[i];
 				this.commentStops[this.commentPtr] = this.scanner.commentStops[i];
 			}
 		}
 	}",Buggy,"Fix for Bug 526996: CompilationUnit.setCommentTable(...) throws
IllegalStateException",Buggy,"The commit message indicates a fix for a bug where `CompilationUnit.setCommentTable(...)` throws an `IllegalStateException`. The code diff modifies the `pushOnCommentsStack` method. Specifically, it adds a check `if (this.scanner.commentPtr < i) break;` within the loop. This check likely prevents the code from accessing `this.scanner.commentStarts[i]` when `i` is out of bounds for the `scanner.commentStarts` array, which could lead to an `ArrayIndexOutOfBoundsException` and potentially an `IllegalStateException` being thrown during the comment table setting process. The added check acts as a guard to prevent this out-of-bounds access, aligning with the bug fix described in the commit message.

**Buggy**"
eclipseJdt,27085.json,1793b84adc002b2d6c63dad898df1777f10b28a0,"@@ -1,89 +1,89 @@
 	public boolean containsAnnotations() throws IOException {
 		State state = NORMAL;
 		
 		// for escaping quotes -- need to ignore the next single character
 		// Since this applies to all states it's handled separately
 		boolean seenBackslash = false;
 		
 		int c = getNext();
 		while (c != -1) {
 			
 			if (seenBackslash) {
 				// Skip one character
 				seenBackslash = false;
 			}
 			else if (c == '\\') {
 				// Skip the next character
 				seenBackslash = true;
 			}
 			else {
 				// Handle the character based on state
 				switch (state) {
 				
-				case (NORMAL) :
+				case NORMAL :
 					if (c == '@')
 						return true;
 					if (c == '/') {
 						state = SEEN_SLASH;
 					}
 					else if (c == '\'') {
 						state = IN_SINGLE_QUOTE;
 					}
 					else if (c == '\""') {
 						state = IN_DOUBLE_QUOTE;
 					}
 					break;
 					
-				case (SEEN_SLASH) :
+				case SEEN_SLASH :
 					if (c == '*') {
 						state = IN_COMMENT;
 					}
 					else if (c == '/') {
 						state = IN_SINGLE_LINE_COMMENT;
 					}
 					else {
 						state = NORMAL;
 					}
 					break;
 				
-				case (IN_COMMENT) :
+				case IN_COMMENT :
 					if (c == '*') {
 						state = IN_COMMENT_SEEN_STAR;
 					}
 					break;
 				
-				case (IN_COMMENT_SEEN_STAR) :
+				case IN_COMMENT_SEEN_STAR :
 					if (c == '/') {
 						state = NORMAL;
 					}
 					else {
 						state = IN_COMMENT;
 					}
 					break;
 					
-				case (IN_SINGLE_LINE_COMMENT) :
+				case IN_SINGLE_LINE_COMMENT :
 					if (c == '\n' || c == '\r') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_SINGLE_QUOTE) :
+				case IN_SINGLE_QUOTE :
 					if (c == '\'') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_DOUBLE_QUOTE) :
+				case IN_DOUBLE_QUOTE :
 					if (c == '\""') {
 						state = NORMAL;
 					}
 					break;
 					
 				default :
 					throw new IllegalStateException(""Unhandled state: "" + state);  //$NON-NLS-1$
 				}
 			}
 			c = getNext();
 		}
 		return false;
 	}",Buggy,"jgarms: fix syntax error in switch/case usage that is not caught by eclipse. This allows compiling by javac.
",Buggy,"The commit message indicates a fix for a syntax error in a switch/case statement. The diff shows changes from `case (NORMAL) :` to `case NORMAL :` and similar changes for other cases. This correction addresses a syntax error related to the switch/case statement, which prevents the code from compiling with `javac`. This is a bug fix.

**Buggy**"
eclipseJdt,6097.json,9a8f94b0b98db0b5d6b8eb4d27d568dc18fc1309,"@@ -1,18 +1,20 @@
 public ReferenceBinding anonymousOriginalSuperType() {
 	if (!isPrototype())
 		return ((LocalTypeBinding) this.prototype).anonymousOriginalSuperType();
+	if (this.superclass == null && this.scope != null)
+		return this.scope.getJavaLangObject();
 	
 	if (this.superInterfaces != Binding.NO_SUPERINTERFACES) {
 		return this.superInterfaces[0];
 	}
 	if ((this.tagBits & TagBits.HierarchyHasProblems) == 0) {
 		return this.superclass;
 	}
 	if (this.scope != null) {
 		TypeReference typeReference = this.scope.referenceContext.allocation.type;
 		if (typeReference != null) {
 			return (ReferenceBinding) typeReference.resolvedType;
 		}
 	}
 	return this.superclass; // default answer
 }",Buggy,"Fixed Bug 435348 - [1.8][compiler] NPE in JDT Core during AST creation

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Fixed Bug 435348 - [1.8][compiler] NPE in JDT Core during AST creation"" explicitly states that the commit fixes a bug (NullPointerException) in the JDT Core compiler related to AST creation. This strongly suggests a bug fix.

2.  **Code Diff:** The code diff modifies the `anonymousOriginalSuperType()` method. The added code is:

    ```java
    if (this.superclass == null && this.scope != null)
    	return this.scope.getJavaLangObject();
    ```

    This code checks if `this.superclass` is null and `this.scope` is not null. If both conditions are true, it returns `this.scope.getJavaLangObject()`. This pattern suggests a defensive check to prevent a NullPointerException. If `superclass` is null, accessing it later in the method (e.g., `return this.superclass;`) would cause an NPE. By returning `java.lang.Object` in this case, the code avoids the potential NPE. The fact that the code is checking for a null `superclass` and providing a default return value indicates that there was a case where `superclass` could be null, leading to an error.

**Reasoning:**

The commit message clearly indicates a bug fix related to a NullPointerException. The code diff introduces a null check for `superclass` and provides an alternative return value if it's null. This aligns perfectly with the commit message, suggesting that the added code is a fix for the reported NPE. The added check is a defensive programming technique to avoid a potential error condition.

**Conclusion:**

**Buggy**
"
eclipseJdt,20759.json,c71a2dd3276b1c054fbb64a586af2db8d142bd3f,"@@ -1,61 +1,66 @@
 	protected void seekTypesInSourcePackage(
 			String name,
 			IPackageFragment pkg,
 			int firstDot,
 			boolean partialMatch,
 			String topLevelTypeName,
 			int acceptFlags,
 			IJavaElementRequestor requestor) {
 
 		long start = -1;
 		if (VERBOSE)
 			start = System.currentTimeMillis();
 		try {
 			if (!partialMatch) {
 				try {
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						String cuName = cu.getElementName();
 						int lastDot = cuName.lastIndexOf('.');
 						if (lastDot != topLevelTypeName.length() || !topLevelTypeName.regionMatches(0, cuName, 0, lastDot))
 							continue;
+
+						// https://bugs.eclipse.org/bugs/show_bug.cgi?id=351697
+						// If we are looking at source location, just ignore binary types
+						if (!(cu instanceof ICompilationUnit))
+							continue;
 						IType type = ((ICompilationUnit) cu).getType(topLevelTypeName);
 						type = getMemberType(type, name, firstDot);
 						if (acceptType(type, acceptFlags, true/*a source type*/)) { // accept type checks for existence
 							requestor.acceptType(type);
 							break;  // since an exact match was requested, no other matching type can exist
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			} else {
 				try {
 					String cuPrefix = firstDot == -1 ? name : name.substring(0, firstDot);
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						if (!cu.getElementName().toLowerCase().startsWith(cuPrefix))
 							continue;
 						try {
 							IType[] types = ((ICompilationUnit) cu).getTypes();
 							for (int j = 0, typeLength = types.length; j < typeLength; j++)
 								seekTypesInTopLevelType(name, firstDot, types[j], requestor, acceptFlags);
 						} catch (JavaModelException e) {
 							// cu doesn't exist -> ignore
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			}
 		} finally {
 			if (VERBOSE)
 				this.timeSpentInSeekTypesInSourcePackage += System.currentTimeMillis()-start;
 		}
 	}",Buggy,"master - Fix for bug 351697: ClassCastException while copying a .class file
to wrong source package
",Buggy,"The commit message indicates a fix for a `ClassCastException` that occurs when copying a `.class` file to the wrong source package. The code diff adds a check `if (!(cu instanceof ICompilationUnit)) continue;` within the loop that iterates through the children of a package. This check ensures that only `ICompilationUnit` instances (source files) are processed, preventing the `ClassCastException` that would occur if a binary file (like a `.class` file) was encountered and cast to `ICompilationUnit`. This aligns perfectly with the commit message, confirming that the change addresses the reported bug.

**Buggy**"
eclipseJdt,17605.json,0aa34db9e617bf74b6393bf5508b95dcdaed80af,"@@ -1,10 +1,11 @@
 protected void consumeMethodHeaderName(boolean isAnnotationMethod) {
 	long selectorSourcePositions = this.identifierPositionStack[this.identifierPtr];
 	int selectorSourceEnd = (int) selectorSourcePositions;
 	int currentAstPtr = this.astPtr;
 	super.consumeMethodHeaderName(isAnnotationMethod);
 	if (this.astPtr > currentAstPtr) { // if ast node was pushed on the ast stack
 		this.sourceEnds.put(this.astStack[this.astPtr], selectorSourceEnd);
 		rememberCategories();
 	}
+	flushCommentsDefinedPriorTo(this.scanner.currentPosition);
 }",Buggy,"Fix for Bug 443942 Reconciler reports AST with wrong node range (with
comment after return type)",Buggy,"The commit message indicates a fix for a bug where the reconciler reports an AST with the wrong node range, specifically when there's a comment after the return type. The code change involves adding `flushCommentsDefinedPriorTo(this.scanner.currentPosition);` at the end of the `consumeMethodHeaderName` method. This suggests that the bug was related to how comments were being handled in relation to the AST node range. Flushing comments defined prior to the current position likely ensures that comments are correctly associated with the AST node, thus fixing the reported bug.

**Buggy**"
eclipseJdt,6321.json,93b912e3753e060958c1b01615e164e0662ca645,"@@ -1,10 +1,10 @@
 MethodBinding findReplacedMethod(MethodBinding specific, MethodBinding general) {
 	MethodBinding generalSubstitute = computeSubstituteMethod(general, specific);
 	if (generalSubstitute != null 
-			&& (!specific.isAbstract() || general.isAbstract())	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
+			&& (!specific.isAbstract() || general.isAbstract() || (general.isDefaultMethod() && specific.declaringClass.isClass()))	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
 			&& isSubstituteParameterSubsignature(specific, generalSubstitute)) 
 	{
 		return generalSubstitute;
 	} 
 	return null;
 }",Buggy,"Fixed Bug 426318 - [1.8][compiler] Bogus name clash error in the
presence of default methods and varargs",Buggy,"The commit message indicates a bug fix related to a name clash error in the compiler when default methods and varargs are involved. The code diff modifies the `findReplacedMethod` method, specifically the condition `(!specific.isAbstract() || general.isAbstract())`. The change adds `|| (general.isDefaultMethod() && specific.declaringClass.isClass())` to this condition. This addition suggests that the original condition was insufficient to handle cases involving default methods, leading to the reported name clash error. The added condition likely addresses a scenario where a default method in a superinterface could incorrectly trigger a name clash with a method in a class implementing the interface. Therefore, the code change directly relates to fixing the bug described in the commit message.

**Buggy**
"
commons-lang,3021.json,e582456625cc8a7056cc9354d2a75913f4ceb393,"@@ -1,46 +1,48 @@
     private void init() {
         thisYear= Calendar.getInstance(timeZone, locale).get(Calendar.YEAR);
         
         nameValues= new ConcurrentHashMap<Integer, KeyValue[]>();
         
         StringBuilder regex= new StringBuilder();
         List<Strategy> collector = new ArrayList<Strategy>();
         
         Matcher patternMatcher= formatPattern.matcher(pattern);
         if(!patternMatcher.lookingAt()) {
             throw new IllegalArgumentException(""Invalid pattern"");
         }
 
-        String localeName = locale.toString();
         // These locales don't use the Gregorian calendar
         // See http://docs.oracle.com/javase/6/docs/technotes/guides/intl/calendar.doc.html
-        if (localeName.equals(""ja_JP_JP"") || localeName.startsWith(""th_TH"")) {
+        // Also, the getEras() methods don't return the correct era names.
+        // N.B. Not safe to use toString() comparison because that changes between Java versions
+        if (locale.equals(JAPANESE_IMPERIAL)
+        || (locale.getLanguage().equals(""th"") && locale.getCountry().equals(""TH""))) {
             collector.add(new SimpleDateFormatStrategy());
             strategies= collector.toArray(new Strategy[collector.size()]);
             parsePattern= Pattern.compile(""(.*+)"");
             return;
         }
 
         currentFormatField= patternMatcher.group();
         Strategy currentStrategy= getStrategy(currentFormatField);
         for(;;) {
             patternMatcher.region(patternMatcher.end(), patternMatcher.regionEnd());
             if(!patternMatcher.lookingAt()) {
                 nextStrategy = null;
                 break;
             }
             String nextFormatField= patternMatcher.group();
             nextStrategy = getStrategy(nextFormatField);
             if(currentStrategy.addRegex(this, regex)) {
                 collector.add(currentStrategy);                
             }
             currentFormatField= nextFormatField;
             currentStrategy= nextStrategy;
         }
         if(currentStrategy.addRegex(this, regex)) {
             collector.add(currentStrategy);                
         }
         currentFormatField= null;
         strategies= collector.toArray(new Strategy[collector.size()]);
         parsePattern= Pattern.compile(regex.toString());
     }",Buggy,"LANG-828 FastDateParser does not handle non-Gregorian calendars properly
Fix bug in Java 7 (Locale.toString() format has changed)

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1390189 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for `FastDateParser` not handling non-Gregorian calendars properly, specifically mentioning a bug related to changes in `Locale.toString()` format in Java 7.

The code diff replaces the use of `locale.toString()` with `locale.equals(JAPANESE_IMPERIAL)` and checks for language and country codes separately (`locale.getLanguage().equals(""th"") && locale.getCountry().equals(""TH"")`). This change addresses the issue of relying on a specific string representation of the locale, which is subject to change across Java versions, as highlighted in the commit message. The original code used `localeName.equals(""ja_JP_JP"")` and `localeName.startsWith(""th_TH"")`, which are fragile because the exact string representation of the locale can vary. The updated code uses `locale.equals(JAPANESE_IMPERIAL)` which compares the locale object itself, and `locale.getLanguage().equals(""th"") && locale.getCountry().equals(""TH"")` which is more robust.

This change directly addresses the bug described in the commit message by providing a more reliable way to identify non-Gregorian calendars, avoiding the reliance on the `toString()` method of the `Locale` object.

**Buggy**
"
commons-lang,734.json,fbb0f7f88c84001e0a92dae6a71b7e43bda65a56,"@@ -1,54 +1,58 @@
     public static String wrap(final String str, int wrapLength, String newLineStr, final boolean wrapLongWords) {
         if (str == null) {
             return null;
         }
         if (newLineStr == null) {
             newLineStr = SystemUtils.LINE_SEPARATOR;
         }
         if (wrapLength < 1) {
             wrapLength = 1;
         }
         final int inputLineLength = str.length();
         int offset = 0;
         final StringBuilder wrappedLine = new StringBuilder(inputLineLength + 32);
         
-        while (inputLineLength - offset > wrapLength) {
+        while (offset < inputLineLength) {
             if (str.charAt(offset) == ' ') {
                 offset++;
                 continue;
             }
+            // only last line without leading spaces is left
+            if(inputLineLength - offset <= wrapLength) {
+                break;
+            }
             int spaceToWrapAt = str.lastIndexOf(' ', wrapLength + offset);
 
             if (spaceToWrapAt >= offset) {
                 // normal case
                 wrappedLine.append(str.substring(offset, spaceToWrapAt));
                 wrappedLine.append(newLineStr);
                 offset = spaceToWrapAt + 1;
                 
             } else {
                 // really long word or URL
                 if (wrapLongWords) {
                     // wrap really long word one line at a time
                     wrappedLine.append(str.substring(offset, wrapLength + offset));
                     wrappedLine.append(newLineStr);
                     offset += wrapLength;
                 } else {
                     // do not wrap really long word, just extend beyond limit
                     spaceToWrapAt = str.indexOf(' ', wrapLength + offset);
                     if (spaceToWrapAt >= 0) {
                         wrappedLine.append(str.substring(offset, spaceToWrapAt));
                         wrappedLine.append(newLineStr);
                         offset = spaceToWrapAt + 1;
                     } else {
                         wrappedLine.append(str.substring(offset));
                         offset = inputLineLength;
                     }
                 }
             }
         }
 
         // Whatever is left in line is short enough to just pass through
         wrappedLine.append(str.substring(offset));
 
         return wrappedLine.toString();
     }",Buggy,"LANG-995: Fix bug with stripping spaces on last line in WordUtils.wrap(). This fixes #18 from github. Thanks to Andrey Khobnya

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1586649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to stripping spaces on the last line in `WordUtils.wrap()`. The diff modifies the `while` loop condition and adds a check for the last line without leading spaces. The original code had a potential issue where trailing spaces on the last line might not be handled correctly. The added condition `if(inputLineLength - offset <= wrapLength)` specifically addresses this scenario, ensuring that the remaining part of the string is processed correctly when it's shorter than the wrap length. This aligns with the commit message's claim of fixing a bug related to spaces on the last line.

**Buggy**"
commons-lang,1056.json,cd9922ef8b2c57ed6aa1390f89f94c1c40cdc67c,"@@ -1,6 +1,6 @@
     protected void setArrayEnd(String arrayEnd) {
-        if (arrayStart == null) {
-            arrayStart = """";
+        if (arrayEnd == null) {
+            arrayEnd = """";
         }
         this.arrayEnd = arrayEnd;
     }",Buggy,"ToStringStyle setArrayEnd handled null incorrectly
bug 31933, fix from Masato Tezuka


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@138011 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to how `ToStringStyle` handles null values for `arrayEnd`. Specifically, it mentions that `setArrayEnd` was incorrectly handling null values.

The diff shows a change in the `setArrayEnd` method. The original code had a condition `if (arrayStart == null)`, which was likely a typo and should have been `if (arrayEnd == null)`. The corrected code now checks `if (arrayEnd == null)` and assigns an empty string to `arrayEnd` if it's null. This fixes the bug where a null `arrayEnd` could lead to unexpected behavior or errors.

The commit message and the code change are directly related, and the change addresses a bug in handling null values.

**Buggy**"
commons-lang,816.json,2d17989e282e67c9c37153adc4074082d3a38b1c,"@@ -1,7 +1,7 @@
     public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
-        comparison = superHashCode;
+        comparison = superCompareTo;
         return this;
     }",Buggy,"Fix stupid compile error....


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Fix stupid compile error...."" suggests that the commit aims to correct a compilation error in the code.

The diff shows a change in the `appendSuper` method of the `CompareToBuilder` class. Specifically, the line `comparison = superHashCode;` is changed to `comparison = superCompareTo;`.

Given the method name `appendSuper` and the parameter name `superCompareTo`, it's highly likely that the original code `comparison = superHashCode;` was indeed a mistake and a compilation error, as `superHashCode` is not defined in the context. The corrected code assigns the value of `superCompareTo` to the `comparison` field, which aligns with the intended logic of the method.

Therefore, the change directly addresses a bug that was causing a compilation error.

**Buggy**"
commons-lang,816.json,dbe97c09cad3ba2c7473c15778773fa4d071e7de,"@@ -1,7 +1,7 @@
-    public CompareToBuilder appendSuper(int superHashCode) {
+    public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
         comparison = superHashCode;
         return this;
     }",Buggy,"Javadoc fixes
bug 21758, from Pete Gieser


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137487 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""Javadoc fixes"" indicates that the commit is intended to fix Javadoc comments. However, the diff shows a change in the parameter name from `superHashCode` to `superCompareTo`. This suggests that the original parameter name was incorrect or misleading, and the change aims to improve the code's clarity and correctness. This is not strictly a Javadoc fix, but rather a code correction. Since the change addresses a potential misunderstanding or error in the code, it can be considered a bug fix.

**Buggy**"
commons-lang,183.json,8cafd87c83f4197f7e8d14de8cba453c844266c0,"@@ -1,20 +1,20 @@
     public static List<Method> getMethodsListWithAnnotation(final Class<?> cls,
                                                             final Class<? extends Annotation> annotationCls,
                                                             boolean searchSupers, boolean ignoreAccess) {
 
         Validate.isTrue(cls != null, ""The class must not be null"");
         Validate.isTrue(annotationCls != null, ""The annotation class must not be null"");
         List<Class<?>> classes = (searchSupers ? getAllSuperclassesAndInterfaces(cls)
                 : new ArrayList<Class<?>>());
-        classes.add(cls);
+        classes.add(0, cls);
         final List<Method> annotatedMethods = new ArrayList<>();
         for (Class<?> acls : classes) {
             final Method[] methods = (ignoreAccess ? acls.getDeclaredMethods() : acls.getMethods());
             for (final Method method : methods) {
                 if (method.getAnnotation(annotationCls) != null) {
                     annotatedMethods.add(method);
                 }
             }
         }
         return annotatedMethods;
     }",Buggy,"LANG-1317: Add MethodUtils#findAnnotation and extend MethodUtils#getMethodsWithAnnotation for non-public, super-class and interface methods

fix bug introduced by last commit
",Buggy,"The commit message ""LANG-1317: Add MethodUtils#findAnnotation and extend MethodUtils#getMethodsWithAnnotation for non-public, super-class and interface methods\n\nfix bug introduced by last commit"" indicates that the commit addresses a bug introduced in the previous commit.

The diff shows a change in the `getMethodsListWithAnnotation` method. Specifically, the line `classes.add(cls);` is changed to `classes.add(0, cls);`. This change affects the order in which classes are searched for methods with the specified annotation. By adding the class `cls` at the beginning of the list, the method now prioritizes methods declared in the class itself before searching in superclasses and interfaces. This change in order suggests a bug fix related to method resolution or precedence. The commit message explicitly states that this commit fixes a bug introduced by the last commit, which strongly supports the conclusion that this diff represents a bug fix.

**Buggy**"
commons-lang,2681.json,82c5dada6f7b369b69c383aa995f45578325e139,"@@ -1,15 +1,15 @@
-    private static void removeCommonFrames(List causeFrames, List wrapperFrames) {
+    public static void removeCommonFrames(List causeFrames, List wrapperFrames) {
         int causeFrameIndex = causeFrames.size() - 1;
         int wrapperFrameIndex = wrapperFrames.size() - 1;
         while (causeFrameIndex >= 0 && wrapperFrameIndex >= 0) {
             // Remove the frame from the cause trace if it is the same
             // as in the wrapper trace
             String causeFrame = (String) causeFrames.get(causeFrameIndex);
             String wrapperFrame = (String) wrapperFrames.get(wrapperFrameIndex);
             if (causeFrame.equals(wrapperFrame)) {
                 causeFrames.remove(causeFrameIndex);
             }
             causeFrameIndex--;
             wrapperFrameIndex--;
         }
     }",Buggy,"Bug #14357 fixed. Mohan's patch makes removeCommonFrames public, and adds an
isThrowableNested to ExceptionUtils.

It adds static attributes to decide if the stack trace should be topDown
and if the stack traces should be trimmed on repeat. If running 1.4 or higher,
it uses the default stack trace, and the functionality of NestableError,
NestableException and NestableRuntimeException getMessage()s all change.
Accompanying these changes are numerous tests.

Submitted by:	Mohan Kishore


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137314 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix (Bug #14357). The changes include making `removeCommonFrames` public and adding `isThrowableNested` to `ExceptionUtils`. It also mentions changes to stack trace handling based on the Java version and modifications to `getMessage()` in `NestableError`, `NestableException`, and `NestableRuntimeException`. The provided diff shows a change in the visibility of the `removeCommonFrames` method from private to public. This aligns with the commit message. Therefore, the changes indicate a bug fix.
**Buggy**"
commons-lang,2064.json,f1ba9c252d99b3adf2bb3750339b895b2c097bd2,"@@ -1,16 +1,17 @@
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
             String name = desc.getName();
             try {
                 return Class.forName(name, false, classLoader);
             } catch (ClassNotFoundException ex) {
                 try {
                     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
                 } catch (ClassNotFoundException cnfe) {
                     Class<?> cls = primitiveTypes.get(name);
-                    if (cls != null)
+                    if (cls != null) {
                         return cls;
-                    else
+                    } else {
                         throw cnfe;
+                    }
                 }
             }
         }",Buggy,"Applying Benedikt Ritter's patch to fix the Checkstyle error in SerializationUtils - LANG-793

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1301321 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a fix for a Checkstyle error and refers to LANG-793. The diff shows a change in the `resolveClass` method within `SerializationUtils`. Specifically, the `if-else` statement handling primitive types has been modified to include explicit curly braces. This change improves code readability and maintainability, aligning with Checkstyle guidelines. While it addresses a style issue, it doesn't directly fix a functional bug.

Therefore, the changes are related to code style and not a bug fix.

**NotBuggy**"
elasticsearch,60742.json,f2411253026503954da172ddd0b1bf77789dd476,"@@ -1,3 +1,3 @@
     public boolean snapshot() {
-        return snapshot != null && snapshot;
+        return snapshot;
     }",Buggy,"Internal: Change snapshot state for unreleased versions and add
validation tests for constants

Currently the snapshot flag for Version constants is only set to true
for CURRENT.  However, this means that the snapshot state changes from
branch to branch.  Instead, snapshot should be ""is this version
released?"".  This change also adds a validation test checking that
ID -> constant and vice versa are correct, and fixes one bug found there
(for an unreleased version).
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message indicates several things:

1.  **Snapshot State Change:** The primary goal is to modify the meaning of the `snapshot` flag in the `Version` constants. It's changing from ""is this version a snapshot build?"" to ""is this version released?"".
2.  **Inconsistency Issue:** The commit message highlights an issue where the `snapshot` flag's state changes between branches, implying an undesirable behavior.
3.  **Validation Test Addition:** The commit introduces a new validation test to ensure the correctness of ID-to-constant and constant-to-ID mappings.
4.  **Bug Fix:** The commit message explicitly states that the new validation test found and fixed a bug related to an unreleased version.

**Git Diff Analysis:**

The diff shows a simple change in the `snapshot()` method:

```diff
@@ -1,3 +1,3 @@
     public boolean snapshot() {
-        return snapshot != null && snapshot;
+        return snapshot;
     }
```

The change removes the null check `snapshot != null &&`. This suggests that the `snapshot` field is now guaranteed to be non-null. The original code implies that `snapshot` could be null, and the method would return `false` in that case. Now, it directly returns the boolean value of `snapshot`.

**Reasoning:**

The commit message explicitly mentions a bug fix found by the new validation test. The change in the `snapshot()` method, while seemingly small, is directly related to the intended change in the meaning of the `snapshot` flag. The removal of the null check suggests a tightening of the contract for the `snapshot` field, likely as a result of the bug fix. The combination of the explicit bug fix mention and the code modification strongly suggests that this commit addresses a bug.

**Conclusion:**

**Buggy**
"
elasticsearch,52357.json,1a915446037ab83ae292df8733e7ba7d63bb667e,"@@ -1,15 +1,16 @@
     public RestStatus status() {
         if (shardFailures.length == 0) {
-            // if no successful shards, it means no active shards, so just return SERVICE_UNAVAILABLE
-            return RestStatus.SERVICE_UNAVAILABLE;
+            // if no successful shards, the failure can be due to EsRejectedExecutionException during fetch phase
+            // on coordinator node. so get the status from cause instead of returning SERVICE_UNAVAILABLE blindly
+            return getCause() == null ? RestStatus.SERVICE_UNAVAILABLE : ExceptionsHelper.status(getCause());
         }
         RestStatus status = shardFailures[0].status();
         if (shardFailures.length > 1) {
             for (int i = 1; i < shardFailures.length; i++) {
                 if (shardFailures[i].status().getStatus() >= 500) {
                     status = shardFailures[i].status();
                 }
             }
         }
         return status;
     }",Buggy,"Fixing 503 Service Unavailable errors during fetch phase  (#39086)

When ESRejectedExecutionException gets thrown on the coordinating node while trying to fetch hits, the resulting exception will hold no shard failures, hence `503` is used as the response status code. In that case, `429` should be returned instead. Also, the status code should be taken from the cause if available whenever there are no shard failures instead of blindly returning `503` like we currently do.

Closes #38586",Buggy,"The commit message indicates a fix for `503 Service Unavailable` errors that occur during the fetch phase. The problem arises when `ESRejectedExecutionException` is thrown on the coordinating node, leading to an exception without shard failures and a resulting `503` status code. The commit message proposes returning `429` instead in such cases and suggests using the status code from the cause exception when no shard failures are present.

The code diff modifies the `status()` method.  Specifically, when `shardFailures.length == 0`, the code now checks the cause of the exception using `getCause()`. If the cause is null, it defaults to `SERVICE_UNAVAILABLE`. Otherwise, it retrieves the status from the cause using `ExceptionsHelper.status(getCause())`. This aligns with the commit message's intention to handle `ESRejectedExecutionException` and use the status code from the cause exception. The original code blindly returned `SERVICE_UNAVAILABLE` when there were no shard failures.

The changes directly address the problem described in the commit message, indicating a bug fix.

**Buggy**"
elasticsearch,47411.json,f0f2b261595e2f5ea48736eac7a4e4121dd20b2c,"@@ -1,21 +1,21 @@
     public static int getRoutingFactor(int sourceNumberOfShards, int targetNumberOfShards) {
         final int factor;
         if (sourceNumberOfShards < targetNumberOfShards) { // split
             factor = targetNumberOfShards / sourceNumberOfShards;
             if (factor * sourceNumberOfShards != targetNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""factor of [""
                     + targetNumberOfShards + ""]"");
             }
         } else if (sourceNumberOfShards > targetNumberOfShards) { // shrink
             factor = sourceNumberOfShards / targetNumberOfShards;
             if (factor * targetNumberOfShards != sourceNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""multiple of [""
                     + targetNumberOfShards + ""]"");
             }
         } else {
             factor = 1;
         }
         return factor;
     }",Buggy,"Fix duplicate phrase in shrink/split error message (#36734)

This commit removes a duplicate ""must be a"" from the shrink/split error
messages.",Buggy,"The commit message ""Fix duplicate phrase in shrink/split error message"" clearly indicates a bug fix related to the error message. The diff shows that the phrase ""must be a must be a"" was corrected to ""must be a"" in two different error messages within the `getRoutingFactor` method. This confirms that the commit addresses a bug in the error message generation.

Reasoning:
1. **Commit Message Analysis:** The commit message explicitly states that it fixes a duplicate phrase in an error message. This directly points to a bug fix.
2. **Diff Analysis:** The diff highlights the removal of the duplicate phrase ""must be a"" from the error messages in the `IllegalArgumentException` constructor calls. This confirms the bug fix described in the commit message.
3. **Relevance:** The code change is directly relevant to the commit message, as it fixes the error message as described.

Conclusion:
**Buggy**
"
elasticsearch,8045.json,92f24c967ae2ad22a728ff43de231cf351b92fd2,"@@ -1,39 +1,39 @@
         protected MultiSearchResponse shardOperation(Request request, ShardId shardId) throws IOException {
             final IndexService indexService = indicesService.indexService(shardId.getIndex());
             final IndexShard indexShard = indicesService.getShardOrNull(shardId);
             try (Engine.Searcher searcher = indexShard.acquireSearcher(""enrich_msearch"")) {
                 final FieldsVisitor visitor = new FieldsVisitor(true);
                 final QueryShardContext context = indexService.newQueryShardContext(shardId.id(),
-                    searcher.getIndexReader(), () -> {throw new UnsupportedOperationException();}, null);
+                    searcher, () -> {throw new UnsupportedOperationException();}, null);
                 final MapperService mapperService = context.getMapperService();
                 final Text typeText = mapperService.documentMapper().typeText();
 
                 final MultiSearchResponse.Item[] items = new MultiSearchResponse.Item[request.multiSearchRequest.requests().size()];
                 for (int i = 0; i < request.multiSearchRequest.requests().size(); i++) {
                     final SearchSourceBuilder searchSourceBuilder = request.multiSearchRequest.requests().get(i).source();
 
                     final QueryBuilder queryBuilder = searchSourceBuilder.query();
                     final int from = searchSourceBuilder.from();
                     final int size = searchSourceBuilder.size();
                     final FetchSourceContext fetchSourceContext = searchSourceBuilder.fetchSource();
 
                     final Query luceneQuery = queryBuilder.rewrite(context).toQuery(context);
                     final int n = from + size;
                     final TopDocs topDocs = searcher.search(luceneQuery, n, new Sort(SortField.FIELD_DOC));
 
                     final SearchHit[] hits = new SearchHit[topDocs.scoreDocs.length];
                     for (int j = 0; j < topDocs.scoreDocs.length; j++) {
                         final ScoreDoc scoreDoc = topDocs.scoreDocs[j];
 
                         visitor.reset();
                         searcher.doc(scoreDoc.doc, visitor);
                         visitor.postProcess(mapperService);
                         final SearchHit hit = new SearchHit(scoreDoc.doc, visitor.uid().id(), typeText, Map.of());
                         hit.sourceRef(filterSource(fetchSourceContext, visitor.source()));
                         hits[j] = hit;
                     }
                     items[i] = new MultiSearchResponse.Item(createSearchResponse(topDocs, hits), null);
                 }
                 return new MultiSearchResponse(items, 1L);
             }
         }",Buggy,"fixed compile errors after merging
",Buggy,"The commit message ""fixed compile errors after merging"" indicates that the changes are aimed at resolving compilation issues that arose after a merge operation. The diff shows a change in the `shardOperation` method, specifically in the instantiation of `QueryShardContext`. The original code passes `searcher.getIndexReader()` as an argument, while the modified code passes the `searcher` object itself. This suggests that the `QueryShardContext` constructor's signature or expected input has changed, necessitating this adjustment to align with the updated API and resolve the compilation error.

Given that the commit message explicitly mentions fixing compile errors and the code change directly addresses a type mismatch in a method call, it is highly likely that this commit is a bug fix.

**Buggy**"
elasticsearch,32648.json,fa3d365ee83a089d461eacf66f2118f68dc1a8ed,"@@ -1,19 +1,25 @@
     public BytesReference slice(int from, int length) {
+        FutureObjects.checkFromIndexSize(from, length, this.length);
+
+        if (length == 0) {
+            return BytesArray.EMPTY;
+        }
+
         // for slices we only need to find the start and the end reference
         // adjust them and pass on the references in between as they are fully contained
         final int to = from + length;
-        final int limit = getOffsetIndex(from + length);
+        final int limit = getOffsetIndex(to - 1);
         final int start = getOffsetIndex(from);
         final BytesReference[] inSlice = new BytesReference[1 + (limit - start)];
         for (int i = 0, j = start; i < inSlice.length; i++) {
             inSlice[i] = references[j++];
         }
         int inSliceOffset = from - offsets[start];
         if (inSlice.length == 1) {
             return inSlice[0].slice(inSliceOffset, length);
         }
         // now adjust slices in front and at the end
         inSlice[0] = inSlice[0].slice(inSliceOffset, inSlice[0].length() - inSliceOffset);
         inSlice[inSlice.length-1] = inSlice[inSlice.length-1].slice(0, to - offsets[limit]);
         return new CompositeBytesReference(inSlice);
     }",Buggy,"Fix CompositeBytesReference#slice to not throw AIOOBE with legal offsets. (#35955)

CompositeBytesReference#slice has two bugs:
 - One that makes it fail if the reference is empty and an empty slice is
   created, this is #35950 and is fixed by special-casing empty-slices.
 - One performance bug that makes it always create a composite slice when
   creating a slice that ends on a boundary, this is fixed by computing `limit`
   as the index of the sub reference that holds the last element rather than
   the next element after the slice.

Closes #35950
",Buggy,"Let's analyze the provided commit message and code diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message explicitly states that it fixes two bugs in `CompositeBytesReference#slice`. It mentions an `AIOOBE` (ArrayIndexOutOfBoundsException) that occurs under specific conditions (empty reference and empty slice creation) and a performance bug related to slice creation on boundaries. The message also refers to issue #35950, indicating a reported bug.

**Code Diff Analysis:**

1.  **`FutureObjects.checkFromIndexSize(from, length, this.length);`**: This line adds a check to ensure that the `from` and `length` parameters are within the valid bounds of the `BytesReference`. This is a standard practice for preventing `IndexOutOfBoundsException` and suggests a potential vulnerability related to out-of-bounds access.

2.  **`if (length == 0) { return BytesArray.EMPTY; }`**: This condition handles the case where an empty slice is requested. The commit message mentions a bug related to empty slices, and this code directly addresses it by returning an empty `BytesArray`.

3.  **`final int limit = getOffsetIndex(to - 1);`**: This is a key change. The original code calculated `limit` using `from + length`, which pointed to the index *after* the end of the slice. The corrected code calculates `limit` using `to - 1`, which points to the index of the *last element* within the slice. This change directly addresses the performance bug mentioned in the commit message, where a composite slice was unnecessarily created when the slice ended on a boundary.

4.  **`inSlice[inSlice.length-1] = inSlice[inSlice.length-1].slice(0, to - offsets[limit]);`**: This line uses the corrected `limit` to slice the last `BytesReference`.

**Reasoning:**

The commit message clearly states that the changes fix two bugs. The code diff confirms this by:

*   Adding a check for valid `from` and `length` parameters, preventing potential `IndexOutOfBoundsException`.
*   Handling the case of empty slices, resolving the `AIOOBE` mentioned in the commit message.
*   Correcting the `limit` calculation, addressing the performance bug related to slice creation on boundaries.

The changes directly align with the commit message and resolve the described bugs.

**Conclusion:**

**Buggy**
"
elasticsearch,53519.json,68c82cd113e0a79916ea9fce6546139de7753731,"@@ -1,15 +1,14 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
-        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"Revert ""fix bug in bulk replication for noop update operation""

This reverts commit 42bc2d15bedb0f3b457bbfe06247311b9667b7c9.
",Buggy,"The commit message ""Revert ""fix bug in bulk replication for noop update operation"""" indicates that a previous commit, which was intended to fix a bug, is being reverted. The diff shows a change in the `writeDocumentRequest` method. Specifically, the assertion `assert request != null : ""request must not be null"";` has been removed. Since the commit message explicitly states that it's reverting a bug fix, and the diff shows a change related to request handling, it's highly likely that the reverted commit was indeed a bug fix. Therefore, the current commit is undoing that bug fix.

**Buggy**
"
elasticsearch,53519.json,42bc2d15bedb0f3b457bbfe06247311b9667b7c9,"@@ -1,14 +1,15 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
+        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"fix bug in bulk replication for noop update operation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fix bug in bulk replication for noop update operation"" clearly states that the commit is intended to fix a bug. The bug is specifically related to ""bulk replication"" and ""noop update operation"". This suggests that under certain conditions involving bulk replication and update operations that don't actually modify data (noop updates), there was a problem.

**Git Diff Analysis:**

The diff adds an assertion `assert request != null : ""request must not be null"";` at the beginning of the `writeDocumentRequest` method. This assertion checks if the `request` object is null. If it is, the assertion will throw an `AssertionError`.

**Reasoning:**

1. **Alignment with Commit Message:** The commit message mentions a bug fix. The added assertion suggests a defensive programming approach to prevent a `NullPointerException` or other unexpected behavior when the `request` object is unexpectedly null. This aligns with the intention of fixing a bug.

2. **Bug Fix Indication:** The added assertion indicates a potential bug. If the `request` object could be null in the context of bulk replication with noop updates, it would lead to an error. The assertion acts as a safeguard to prevent this error or to provide an early indication of the problem. The commit message explicitly mentions a bug related to noop updates, which further strengthens the argument that this change is a bug fix.

3. **Error Handling:** The assertion can be considered a form of error handling. It detects an invalid state (null request) and prevents the code from proceeding further, potentially avoiding a more severe error later on.

**Conclusion:**

Based on the commit message and the code diff, the changes indicate a bug fix related to handling null `request` objects, possibly in the context of bulk replication and noop update operations.

**Buggy**
"
elasticsearch,9986.json,7e12d5a7958eb9f62fdb073863110dbd09b79747,"@@ -1,48 +1,48 @@
     private void buildUserFromClaims(JWTClaimsSet claims, ActionListener<AuthenticationResult> authResultListener) {
         final String principal = principalAttribute.getClaimValue(claims);
         if (Strings.isNullOrEmpty(principal)) {
             authResultListener.onResponse(AuthenticationResult.unsuccessful(
                 principalAttribute + ""not found in "" + claims.toJSONObject(), null));
             return;
         }
 
         final Map<String, Object> tokenMetadata = new HashMap<>();
         tokenMetadata.put(""id_token_hint"", claims.getClaim(""id_token_hint""));
         ActionListener<AuthenticationResult> wrappedAuthResultListener = ActionListener.wrap(auth -> {
             if (auth.isAuthenticated()) {
                 // Add the ID Token as metadata on the authentication, so that it can be used for logout requests
                 Map<String, Object> metadata = new HashMap<>(auth.getMetadata());
                 metadata.put(CONTEXT_TOKEN_DATA, tokenMetadata);
                 auth = AuthenticationResult.success(auth.getUser(), metadata);
             }
             authResultListener.onResponse(auth);
         }, authResultListener::onFailure);
 
         if (delegatedRealms.hasDelegation()) {
             delegatedRealms.resolve(principal, wrappedAuthResultListener);
             return;
         }
 
         final Map<String, Object> userMetadata;
         if (populateUserMetadata) {
             userMetadata = claims.getClaims().entrySet().stream().filter(entry -> {
                 /*
                  * We whitelist the Types that we want to parse as metadata from the Claims, explicitly filtering out {@link Date}s
                  */
                 Object v = entry.getValue();
-                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collections);
+                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collection);
             }).collect(Collectors.toUnmodifiableMap(entry -> ""oidc("" + entry.getKey() + "")"", Map.Entry::getValue));
         } else {
             userMetadata = Map.of();
         }
         final List<String> groups = groupsAttribute.getClaimValues(claims);
         final String dn = dnAttribute.getClaimValue(claims);
         final String mail = mailAttribute.getClaimValue(claims);
         final String name = nameAttribute.getClaimValue(claims);
         UserRoleMapper.UserData userData = new UserRoleMapper.UserData(principal, dn, groups, userMetadata, config);
         roleMapper.resolveRoles(userData, ActionListener.wrap(roles -> {
             final User user = new User(principal, roles.toArray(Strings.EMPTY_ARRAY), name, mail, userMetadata, true);
             wrappedAuthResultListener.onResponse(AuthenticationResult.success(user));
         }, wrappedAuthResultListener::onFailure));
 
     }",Buggy,"Populate OpenIDConnect metadata collections (#50521)

The OpenIdConnectRealm had a bug which would cause it not to populate
User metadata for collections contained in the user JWT claims.

This commit fixes that bug.

Resolves: #50250",Buggy,"The commit message indicates a bug fix in the `OpenIdConnectRealm` that prevented the population of user metadata for collections from JWT claims. The code diff modifies the `buildUserFromClaims` method, specifically the part where user metadata is extracted from the JWT claims.

The original code had a condition `v instanceof Collections`, which is incorrect. `Collections` is an interface, not a concrete class. The corrected code changes this to `v instanceof Collection`, which is the correct way to check if a value is a collection. This change directly addresses the bug described in the commit message, as it now correctly identifies and includes collections as part of the user metadata.

Therefore, the changes indicate a bug fix.

**Buggy**"
elasticsearch,31859.json,cea2d21c50c5a680cbbaefba254866601b5b4608,"@@ -1,20 +1,20 @@
     private int findPlaceholderEndIndex(CharSequence buf, int startIndex) {
         int index = startIndex + this.placeholderPrefix.length();
         int withinNestedPlaceholder = 0;
         while (index < buf.length()) {
             if (Strings.substringMatch(buf, index, this.placeholderSuffix)) {
                 if (withinNestedPlaceholder > 0) {
                     withinNestedPlaceholder--;
-                    index = index + this.placeholderPrefix.length() - 1;
+                    index = index + this.placeholderSuffix.length();
                 } else {
                     return index;
                 }
             } else if (Strings.substringMatch(buf, index, this.placeholderPrefix)) {
                 withinNestedPlaceholder++;
                 index = index + this.placeholderPrefix.length();
             } else {
                 index++;
             }
         }
         return -1;
     }",Buggy,"Fix bug in PropertyPlaceholder and add unit tests.

Close #6034
",Buggy,"The commit message indicates a bug fix in `PropertyPlaceholder` and the addition of unit tests (though the diff doesn't show the unit tests). The code change is within the `findPlaceholderEndIndex` method, which is responsible for finding the end index of a placeholder within a string. The bug fix seems to be related to handling nested placeholders.

The original code had `index = index + this.placeholderPrefix.length() - 1;` which seems incorrect. When a nested placeholder suffix is found, the index should advance by the length of the suffix, not the prefix minus 1. The corrected code `index = index + this.placeholderSuffix.length();` fixes this. This indicates a bug where nested placeholders were not being correctly parsed, leading to incorrect end index detection.

Therefore, the change aligns with the commit message indicating a bug fix.

**Buggy**"
elasticsearch,52107.json,267cd65506ab22ceb7606243ef58979bde5dbfe5,"@@ -1,28 +1,28 @@
         public void start() {
             if (shardIt == null) {
                 // just execute it on the local node
                 transportService.sendRequest(clusterService.localNode(), transportShardAction, internalRequest.request(), new BaseTransportResponseHandler<Response>() {
                     @Override
                     public Response newInstance() {
                         return newResponse();
                     }
 
                     @Override
                     public String executor() {
                         return ThreadPool.Names.SAME;
                     }
 
                     @Override
                     public void handleResponse(final Response response) {
                         listener.onResponse(response);
                     }
 
                     @Override
                     public void handleException(TransportException exp) {
-                        perform(exp);
+                        listener.onFailure(exp);
                     }
                 });
             } else {
                 perform(null);
             }
         }",Buggy,"Analysis : Fix no response from Analyze API without specified index

Fix error handling in TransportSingleShardAction without shardIt

Closes #15148
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Analysis : Fix no response from Analyze API without specified index"" and ""Fix error handling in TransportSingleShardAction without shardIt"" strongly suggests that the commit addresses a bug. Specifically, it mentions a scenario where the Analyze API fails to respond when no index is specified, and also indicates a problem with error handling in `TransportSingleShardAction` when `shardIt` is null. The ""Closes #15148"" further reinforces that this commit is intended to resolve a reported issue.

**Code Diff Analysis:**

The code diff shows a change within the `start()` method of an anonymous class, likely a transport action handler. The key change is in the `handleException` method.

*   **Original Code:** `perform(exp);`
*   **Modified Code:** `listener.onFailure(exp);`

The original code was calling `perform(exp)` within the `handleException` block. Based on the context (TransportAction, listener), it's highly probable that `perform` was intended to retry or handle the request in some way. However, in the case where `shardIt` is null (as indicated in the commit message), this might have led to incorrect behavior or infinite loops if `perform` didn't handle the exception correctly.

The modified code directly calls `listener.onFailure(exp)`, which is a more appropriate way to propagate the exception to the listener, indicating a failure in the transport action. This change directly addresses the error handling issue mentioned in the commit message. By calling `listener.onFailure(exp)` the code now correctly propagates the error to the listener, preventing the ""no response"" scenario described in the commit message.

**Reasoning:**

1.  The commit message explicitly states that it fixes a bug related to the Analyze API and error handling in `TransportSingleShardAction`.
2.  The code diff shows a change in the error handling logic within the `handleException` method.
3.  The change replaces a potentially problematic `perform(exp)` call with a more appropriate `listener.onFailure(exp)` call, which correctly propagates the error.
4.  The context (TransportAction, listener) suggests that the change improves the reliability and correctness of the transport action.

**Conclusion:**

The commit message and code diff are strongly aligned and indicate a bug fix related to error handling and the Analyze API. Therefore, the answer is:

**Buggy**
"
elasticsearch,11277.json,3b739b9fd5da2323a84b242708b64c16660113cc,"@@ -1,6 +1,6 @@
         protected ShardsIterator shards(ClusterState state, InternalRequest request) {
-            return state.routingTable()
-                    .index(request.concreteIndex())
-                    .shard(request.request().getShard().id())
+            return state
+                    .routingTable()
+                    .shardRoutingTable(request.concreteIndex(), request.request().getShard().id())
                     .activeInitializingShardsRandomIt();
         }",Buggy,"Avoid NPE on shard changes action (#32630)

If a leader index is deleted while there is an active follower, the
follower will send shard changes requests bound for the leader
index. Today this will result in a null pointer exception because there
will not be an index routing table for the index. A null pointer
exception looks like a bug to a user so this commit addresses this by
throwing an index not found exception instead.",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

*   **Problem:** The commit message clearly states that a `NullPointerException` (NPE) can occur when a leader index is deleted while a follower is still active and sending shard changes requests. This happens because the index routing table is missing.
*   **Solution:** Instead of an NPE, the commit aims to throw an `IndexNotFoundException`.
*   **Rationale:** The commit message explicitly mentions that an NPE ""looks like a bug to a user,"" indicating the original behavior was undesirable and considered a bug.

**Git Diff Analysis:**

*   The diff modifies the `shards()` method.
*   The original code directly accessed the index routing table using `state.routingTable().index(request.concreteIndex())`. If the index doesn't exist, this would likely return `null`, and subsequent calls on the result would cause an NPE.
*   The modified code uses `state.routingTable().shardRoutingTable(request.concreteIndex(), request.request().getShard().id())`. This method likely handles the case where the index is missing and throws an `IndexNotFoundException` (or returns null, which is then handled appropriately to throw the exception).

**Reasoning:**

The commit message describes a scenario leading to an NPE, which is a type of bug. The code change replaces a direct access that could result in a null value with a method that likely handles the missing index case more gracefully, preventing the NPE and throwing a more informative exception. This aligns perfectly with the commit message's intention to avoid the NPE and provide a better user experience. The change directly addresses the bug described in the commit message.

**Conclusion:**

**Buggy**
"
elasticsearch,50555.json,03e8734b04d841e31724b10cf0d9c41801699411,"@@ -1,24 +1,29 @@
     private void getMultipleReposSnapshotInfo(List<RepositoryMetaData> repos, String[] snapshots, boolean ignoreUnavailable,
                                               boolean verbose, ActionListener<GetSnapshotsResponse> listener) {
+        // short-circuit if there are no repos, because we can not create GroupedActionListener of size 0
+        if (repos.isEmpty()) {
+            listener.onResponse(new GetSnapshotsResponse(Collections.emptyList()));
+            return;
+        }
         final GroupedActionListener<GetSnapshotsResponse.Response> groupedActionListener =
                 new GroupedActionListener<>(
                         ActionListener.map(listener, responses -> {
                             assert repos.size() == responses.size();
                             return new GetSnapshotsResponse(responses);
                         }), repos.size());
 
         // run concurrently for all repos on GENERIC thread pool
         for (final RepositoryMetaData repo : repos) {
             threadPool.executor(ThreadPool.Names.GENERIC).execute(new ActionRunnable<>(groupedActionListener) {
                 @Override
                 protected void doRun() {
                     try {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.snapshots(
                                 repo.name(), getSingleRepoSnapshotInfo(repo.name(), snapshots, ignoreUnavailable, verbose)));
                     } catch (ElasticsearchException e) {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.error(repo.name(), e));
                     }
                 }
             });
         }
     }",Buggy,"Fix GET /_snapshot/_all/_all if there are no repos (#43558)

When there are no repositories, a request to GET /_snapshot/_all/_all
returns a 504 timeout error.
This happens because try to create GroupedActionListener with the
size of zero, which leads to an exception.
This commit short-circuits if there are no repos and adds a test to
verify the fix.

Closes #43547",Buggy,"The commit message indicates a bug fix related to the `GET /_snapshot/_all/_all` endpoint when no repositories exist. The original code would throw an exception when trying to create a `GroupedActionListener` with a size of zero, leading to a 504 timeout error.

The code diff introduces a short-circuit: if the list of repositories (`repos`) is empty, the method now immediately returns an empty `GetSnapshotsResponse`. This prevents the creation of the zero-sized `GroupedActionListener` and avoids the exception.

The commit message and code changes are consistent. The code change directly addresses the bug described in the commit message. The addition of the `if (repos.isEmpty())` block is a clear indication of a bug fix.

**Buggy**
"
elasticsearch,35892.json,ba8ad9c2b727f9609e16aac6ed2df17f44c6b683,"@@ -1,3 +1,3 @@
     public long getAgeInMillis() {
-        return Math.max(0, (System.nanoTime() - creationDate) / 1000);
+        return TimeUnit.MILLISECONDS.convert(relativeTimeProvider.getAsLong() - creationDate, TimeUnit.NANOSECONDS);
     }",Buggy,"Fix calculation of age of pending tasks

This commit addresses a time unit conversion bug in calculating the age
of a PrioritizedRunnable. The issue was an incorrect conversion from
nanoseconds to milliseconds as instead the conversion was to
microseconds. This leads to the timeInQueue metric for pending tasks to
be off by three orders of magnitude.
",Buggy,"The commit message clearly states a bug fix related to time unit conversion when calculating the age of pending tasks. It pinpoints an incorrect conversion from nanoseconds to microseconds instead of milliseconds, causing a significant error in the timeInQueue metric.

The code diff shows a change in the `getAgeInMillis()` method. The original code used a direct division by 1000, which, based on the context provided by the commit message, was intended to convert nanoseconds to microseconds, not milliseconds. The corrected code uses `TimeUnit.MILLISECONDS.convert()`, which explicitly converts nanoseconds to milliseconds.

The code change directly addresses the bug described in the commit message. The use of `TimeUnit.MILLISECONDS.convert()` ensures the correct conversion, fixing the inaccurate age calculation.

**Buggy**"
elasticsearch,55497.json,53f6dcfd375169c9847a6059253a1990fb7c99f5,"@@ -1,11 +1,11 @@
         private SpanQuery newSpanQuery(Term[] terms, boolean isPrefix) {
             if (terms.length == 1) {
                 return isPrefix ? fieldType.spanPrefixQuery(terms[0].text(), spanRewriteMethod, context) : new SpanTermQuery(terms[0]);
             }
             SpanQuery[] spanQueries = new SpanQuery[terms.length];
             for (int i = 0; i < terms.length; i++) {
-                spanQueries[i] = isPrefix ? new SpanTermQuery(terms[i]) :
-                    fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context);
+                spanQueries[i] = isPrefix ? fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context) :
+                    new SpanTermQuery(terms[i]);
             }
             return new SpanOrQuery(spanQueries);
         }",Buggy,"Fix wrong logic in `match_phrase` query with multi-word synonyms (#43941)

Disjunction over two individual terms in a phrase query with multi-word synonyms
wrongly applies a prefix query to each of these terms. This change fixes this bug
by inversing the logic to use prefixes on `phrase_prefix` queries only.

Closes #43308",Buggy,"The commit message indicates a bug fix related to how `match_phrase` queries handle multi-word synonyms, specifically when dealing with prefixes. The original logic incorrectly applied a prefix query to each term in a disjunction of individual terms, even when it shouldn't. The fix involves inverting the logic to apply prefixes only to `phrase_prefix` queries.

The code diff shows a change in the `newSpanQuery` method. The original code had the following logic:

```java
spanQueries[i] = isPrefix ? new SpanTermQuery(terms[i]) :
    fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context);
```

The corrected code inverts the logic:

```java
spanQueries[i] = isPrefix ? fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context) :
    new SpanTermQuery(terms[i]);
```

The `isPrefix` variable controls whether a prefix query should be used. The original code incorrectly used `new SpanTermQuery(terms[i])` when `isPrefix` was true and `fieldType.spanPrefixQuery` when `isPrefix` was false. The corrected code swaps these two, so that `fieldType.spanPrefixQuery` is used when `isPrefix` is true, and `new SpanTermQuery` is used when `isPrefix` is false. This aligns with the commit message's claim of inverting the logic to use prefixes only on `phrase_prefix` queries.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
elasticsearch,58217.json,a46d2f21c6504c8ff8cb98a2180293994b857a03,"@@ -1,10 +1,11 @@
     private static int expandCommonMappers(List<ObjectMapper> parentMappers, String[] nameParts, int i) {
         ObjectMapper last = parentMappers.get(parentMappers.size() - 1);
         while (i < nameParts.length - 1 && last.getMapper(nameParts[i]) != null) {
             Mapper newLast = last.getMapper(nameParts[i]);
             assert newLast instanceof ObjectMapper;
-            parentMappers.add((ObjectMapper)newLast);
+            last = (ObjectMapper) newLast;
+            parentMappers.add(last);
             ++i;
         }
         return i;
     }",Buggy,"Fix dynamic mapper bug with deeply nested fields.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix dynamic mapper bug with deeply nested fields"" clearly states that the commit is intended to resolve a bug related to dynamic mappers when dealing with deeply nested fields. This suggests that the previous implementation had issues correctly handling such scenarios.

**Git Diff Analysis:**

The code diff is focused on the `expandCommonMappers` method. Let's break down the changes:

```diff
-            parentMappers.add((ObjectMapper)newLast);
+            last = (ObjectMapper) newLast;
+            parentMappers.add(last);
```

The original code was adding `newLast` directly to the `parentMappers` list. The corrected code now updates the `last` variable to `newLast` *before* adding it to the `parentMappers` list.

**Reasoning:**

The original code was adding the *same* `newLast` object to the `parentMappers` list in each iteration of the `while` loop. This would result in the `parentMappers` list containing multiple references to the same mapper object, which would be incorrect when dealing with deeply nested fields.

The corrected code updates the `last` variable in each iteration to the *new* mapper, ensuring that the `parentMappers` list contains the correct sequence of mappers representing the nested structure. This is crucial for correctly resolving fields at different levels of nesting.

The change directly addresses the problem described in the commit message: a bug in how dynamic mappers were handled with deeply nested fields. The original code's logic was flawed, leading to incorrect mapper resolution. The corrected code fixes this by ensuring the `parentMappers` list accurately reflects the nested mapper structure.

**Conclusion:**

The code change directly addresses the bug described in the commit message. The original code had a logical flaw that caused incorrect mapper resolution in deeply nested fields. The corrected code fixes this flaw.

**Buggy**
"
elasticsearch,63178.json,c33f894846989694f6d6fdc250ae396a88545c55,"@@ -1,19 +1,4 @@
         public Set<Entry<String, List<String>>> entrySet() {
-            return httpHeaders.names().stream().map(k -> new Entry<String, List<String>>() {
-
-                @Override
-                public String getKey() {
-                    return k;
-                }
-
-                @Override
-                public List<String> getValue() {
-                    return httpHeaders.getAll(k);
-                }
-
-                @Override
-                public List<String> setValue(List<String> value) {
-                    throw new UnsupportedOperationException(""modifications are not supported"");
-                }
-            }).collect(Collectors.toSet());
+            return httpHeaders.names().stream().map(k -> new AbstractMap.SimpleImmutableEntry<>(k, httpHeaders.getAll(k)))
+                    .collect(Collectors.toSet());
         }",Buggy,"Fixing compilation problem in Eclipse (#22956)

",Buggy,"The commit message indicates a fix for a compilation problem in Eclipse. The code diff replaces an anonymous class implementation of `Entry<String, List<String>>` with `AbstractMap.SimpleImmutableEntry<String, List<String>>`. This change likely addresses a compilation issue related to the anonymous class, possibly due to limitations or bugs in the Eclipse compiler. The use of `AbstractMap.SimpleImmutableEntry` provides a more concise and potentially more compatible way to create immutable map entries.

**Buggy**
"
elasticsearch,10846.json,70d524c979f90759901fd81f70c56172c311bd86,"@@ -1,7 +1,7 @@
     protected boolean shouldCollect() {
-        if (licensee.collectionEnabled()) {
+        if (!licensee.collectionEnabled()) {
             logger.trace(""collector [{}] can not collect data due to invalid license"", name());
             return false;
         }
         return true;
     }",Buggy,"Marvel: Fix bug that prevent collectors to collect when license is enabled

Original commit: elastic/x-pack-elasticsearch@9f22baa2ee8f873e4123681d36bcbe759da619f1
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Marvel: Fix bug that prevent collectors to collect when license is enabled"" clearly states that the commit aims to resolve a bug. The bug description indicates that collectors were being prevented from collecting data even when the license was enabled, which is an incorrect behavior.

**Code Diff Analysis:**

The code diff shows a change in the `shouldCollect()` method:

`-       if (licensee.collectionEnabled()) {`
`+       if (!licensee.collectionEnabled()) {`

The change involves negating the condition `licensee.collectionEnabled()`.  Previously, the method would return `false` (preventing collection) if the license *was* enabled. Now, it returns `false` only if the license is *not* enabled.

**Reasoning:**

The original code had a logical error. It was checking if the license was enabled and, if so, incorrectly preventing data collection. The corrected code now correctly checks if the license is *not* enabled and prevents collection only in that case. This aligns perfectly with the commit message, which states that the bug prevented collectors from collecting when the license was enabled. The negation fixes the incorrect logic.

**Conclusion:**

The code change directly addresses the bug described in the commit message. The original code had a flaw in its logic that caused it to behave contrary to the intended functionality. The corrected code fixes this flaw.

**Buggy**
"
elasticsearch,15635.json,00cecac86e03195a28f4bec92ccd8cc182a16132,"@@ -1,13 +1,15 @@
     public List<ExecutorBuilder<?>> getExecutorBuilders(final Settings settings) {
         if (enabled) {
-            final FixedExecutorBuilder builder =
-                    new FixedExecutorBuilder(
-                            settings,
+            final ScalingExecutorBuilder builder =
+                    new ScalingExecutorBuilder(
                             InternalWatchExecutor.THREAD_POOL_NAME,
+                            0,
+                            // watcher threads can block on I/O for a long time, so we let this
+                            // pool be large so that execution of unblocked watches can proceed
                             5 * EsExecutors.boundedNumberOfProcessors(settings),
-                            1000,
+                            TimeValue.timeValueMinutes(5),
                             ""xpack.watcher.thread_pool"");
             return Collections.singletonList(builder);
         }
         return Collections.emptyList();
     }",Buggy,"Change Watcher thread pool to be scaling

Watcher uses a custom thread pool. This is because executing watches can
be long-running tasks that often block on I/O and it is best to not
consume the core thread pools with these tasks. Today this thread pool
is fixed, and sized at five times the bounded number of cores (so 160 on
a 32-core box). It makes sense for there to possibly be so many threads,
again because these tasks can block on I/O and having excess capacity
lets unblocked watches execute. It's the fixed size that can cause
problem, all these threads are always consuming resources even when
there are no or not that many watches running. This commit changes this
thread pool to be a scaling thread pool.

Relates elastic/elasticsearch#3660

Original commit: elastic/x-pack-elasticsearch@3cafab6e8360b17ecaf99d37bc1e024a9c7ff256
",NotBuggy,"The commit message describes a change to the Watcher thread pool, transitioning it from a fixed-size pool to a scaling pool. The reasoning is to reduce resource consumption when there are few or no watches running, as the fixed-size pool always consumes resources regardless of the workload. The provided diff shows the replacement of `FixedExecutorBuilder` with `ScalingExecutorBuilder` and adjusts the parameters accordingly. This change directly addresses the problem described in the commit message. There is no explicit mention of a bug being fixed, but the commit aims to improve resource utilization and prevent potential issues related to excessive thread usage. The change is an optimization rather than a bug fix.

**NotBuggy**"
elasticsearch,11653.json,bca4edcd56fa984b9e712f9298e6f91a19983710,"@@ -1,76 +1,77 @@
     static Tuple<String, String> overrideFormatToGrokAndRegex(String overrideFormat) {
 
         if (overrideFormat.indexOf('\n') >= 0 || overrideFormat.indexOf('\r') >= 0) {
             throw new IllegalArgumentException(""Multi-line timestamp formats ["" + overrideFormat + ""] not supported"");
         }
 
         if (overrideFormat.indexOf(INDETERMINATE_FIELD_PLACEHOLDER) >= 0) {
             throw new IllegalArgumentException(""Timestamp format ["" + overrideFormat + ""] not supported because it contains [""
                 + INDETERMINATE_FIELD_PLACEHOLDER + ""]"");
         }
 
         StringBuilder grokPatternBuilder = new StringBuilder();
         StringBuilder regexBuilder = new StringBuilder();
 
         boolean notQuoted = true;
         char prevChar = '\0';
         String prevLetterGroup = null;
         int pos = 0;
         while (pos < overrideFormat.length()) {
             char curChar = overrideFormat.charAt(pos);
 
             if (curChar == '\'') {
                 notQuoted = !notQuoted;
             } else if (notQuoted && Character.isLetter(curChar)) {
                 int startPos = pos;
                 int endPos = startPos + 1;
                 while (endPos < overrideFormat.length() && overrideFormat.charAt(endPos) == curChar) {
                     ++endPos;
                     ++pos;
                 }
                 String letterGroup = overrideFormat.substring(startPos, endPos);
                 Tuple<String, String> grokPatternAndRegexForGroup = VALID_LETTER_GROUPS.get(letterGroup);
                 if (grokPatternAndRegexForGroup == null) {
                     // Special case of fractional seconds
                     if (curChar != 'S' || FRACTIONAL_SECOND_SEPARATORS.indexOf(prevChar) == -1 ||
                         ""ss"".equals(prevLetterGroup) == false || endPos - startPos > 9) {
                         String msg = ""Letter group ["" + letterGroup + ""] in ["" + overrideFormat + ""] is not supported"";
                         if (curChar == 'S') {
                             msg += "" because it is not preceded by [ss] and a separator from ["" + FRACTIONAL_SECOND_SEPARATORS + ""]"";
                         }
                         throw new IllegalArgumentException(msg);
                     }
-                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional
-                    // fraction, but we need to remove the separator that's included in %{SECOND}
-                    grokPatternBuilder.deleteCharAt(grokPatternBuilder.length() - 1);
+                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional fraction,
+                    // but we need to remove the separator that's included in %{SECOND} (and that might be escaped)
+                    int numCharsToDelete = (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(prevChar) >= 0) ? 2 : 1;
+                    grokPatternBuilder.delete(grokPatternBuilder.length() - numCharsToDelete, grokPatternBuilder.length());
                     regexBuilder.append(""\\d{"").append(endPos - startPos).append('}');
                 } else {
                     grokPatternBuilder.append(grokPatternAndRegexForGroup.v1());
                     if (regexBuilder.length() == 0) {
                         regexBuilder.append(""\\b"");
                     }
                     regexBuilder.append(grokPatternAndRegexForGroup.v2());
                 }
                 if (pos + 1 == overrideFormat.length()) {
                     regexBuilder.append(""\\b"");
                 }
                 prevLetterGroup = letterGroup;
             } else {
                 if (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(curChar) >= 0) {
                     grokPatternBuilder.append('\\');
                     regexBuilder.append('\\');
                 }
                 grokPatternBuilder.append(curChar);
                 regexBuilder.append(curChar);
             }
 
             prevChar = curChar;
             ++pos;
         }
 
         if (prevLetterGroup == null) {
             throw new IllegalArgumentException(""No time format letter groups in override format ["" + overrideFormat + ""]"");
         }
 
         return new Tuple<>(grokPatternBuilder.toString(), regexBuilder.toString());
     }",Buggy,"[ML] Fix custom timestamp override with dot-separated fractional seconds (#44127)

Custom timestamp overrides provided to the find_file_structure
endpoint produced an invalid Grok pattern if the fractional
seconds separator was a dot rather than a comma or colon.
This commit fixes that problem and adds tests for this sort
of timestamp override.

Fixes #44110",Buggy,"The commit message indicates a bug fix related to custom timestamp overrides in the `find_file_structure` endpoint. Specifically, the issue arises when the fractional seconds separator is a dot instead of a comma or colon, leading to an invalid Grok pattern.

The code diff modifies the `overrideFormatToGrokAndRegex` method to handle dot separators correctly. It adjusts the logic for fractional seconds (indicated by 'S' characters) preceded by ""ss"" and a separator. The change involves deleting the separator character from the `grokPatternBuilder`. The original code assumed that the separator was always a single character, but the updated code checks if the separator needs escaping in regex, and deletes one or two characters accordingly.

The commit also mentions adding tests for this specific scenario, which further supports the claim that this commit addresses a bug.

Therefore, the changes indicate a bug fix.
**Buggy**"
elasticsearch,26537.json,61f5c188e0a577555db382dd0fe9a4222da9df1a,"@@ -1,12 +1,12 @@
-    static Request openJob(OpenJobRequest openJobRequest) {
+    static Request openJob(OpenJobRequest openJobRequest) throws IOException {
         String endpoint = new EndpointBuilder()
                 .addPathPartAsIs(""_xpack"")
                 .addPathPartAsIs(""ml"")
                 .addPathPartAsIs(""anomaly_detectors"")
                 .addPathPart(openJobRequest.getJobId())
                 .addPathPartAsIs(""_open"")
                 .build();
         Request request = new Request(HttpPost.METHOD_NAME, endpoint);
         request.setEntity(createEntity(openJobRequest, REQUEST_BODY_CONTENT_TYPE));
         return request;
     }",Buggy,"HLRC: Fix Compile Error From Missing Throws (#33083)

* 50441f97ae745814db96c262e99d0f465aca5b2c#diff-53a95fe7ded21313483f1b2f15977395L72 removed the throws breaking compilation here",Buggy,"The commit message indicates a fix for a compilation error caused by a missing `throws` declaration. The diff shows the addition of `throws IOException` to the `openJob` method signature. This aligns perfectly with the commit message, indicating that the change was made to resolve a compilation issue, likely due to an unhandled `IOException` that the method might throw.

**Buggy**
"
elasticsearch,54800.json,2637e499ac88ce0aa14db62b13be052e7ca78f98,"@@ -1,8 +1,8 @@
     protected final DirectoryReader wrapReader(DirectoryReader reader,
                                                     Function<DirectoryReader, DirectoryReader> readerWrapperFunction) throws IOException {
-        reader = ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
         if (engineConfig.getIndexSettings().isSoftDeleteEnabled()) {
             reader = new SoftDeletesDirectoryReaderWrapper(reader, Lucene.SOFT_DELETES_FIELD);
         }
-        return readerWrapperFunction.apply(reader);
+        reader = readerWrapperFunction.apply(reader);
+        return ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
     }",Buggy,"Fix assertion error when caching the result of a search in a read-only index (#41900)

The ReadOnlyEngine wraps its reader with a SoftDeletesDirectoryReaderWrapper if soft deletes
are enabled. However the wrapping is done on top of the ElasticsearchDirectoryReader and that
trips assertion later on since the cache key of these directories are different. This commit
changes the order of the wrapping to put the ElasticsearchDirectoryReader first in order to
ensure that it is always retrieved first when we unwrap the directory.

Closes #41795
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states ""Fix assertion error"". This strongly suggests a bug fix.
    *   The message describes a scenario where caching the result of a search in a read-only index leads to an assertion error.
    *   The root cause is identified as the order in which `DirectoryReader` instances are wrapped, specifically when soft deletes are enabled. The `ElasticsearchDirectoryReader` was wrapped *after* the `SoftDeletesDirectoryReaderWrapper`, causing issues with cache key retrieval.
    *   The proposed solution involves changing the wrapping order to ensure `ElasticsearchDirectoryReader` is always retrieved first.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `wrapReader` method.
    *   The original code wrapped the reader with `ElasticsearchDirectoryReader` *before* checking if soft deletes were enabled and applying the `SoftDeletesDirectoryReaderWrapper`. Then, it applied the `readerWrapperFunction`.
    *   The modified code now applies the `readerWrapperFunction` *before* wrapping with `ElasticsearchDirectoryReader`. The soft delete wrapping remains in the same order.
    *   This change directly addresses the issue described in the commit message by altering the wrapping order. The `ElasticsearchDirectoryReader` is now the outermost wrapper.

3.  **Synthesis:**
    *   The commit message clearly indicates a bug fix related to an assertion error.
    *   The code change directly addresses the root cause described in the commit message by modifying the order in which `DirectoryReader` instances are wrapped.
    *   The change in wrapping order is intended to resolve the cache key issue that triggers the assertion error.

**Conclusion:**

**Buggy**
"
elasticsearch,31501.json,0f93b7abdf65425344f5cedda3e6b04b31e910d4,"@@ -1,3 +1,3 @@
-    static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
+    public static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
         return (assignment.isAssigned() == false || nodes.nodeExists(assignment.getExecutorNode()) == false);
     }",Buggy,"Fix compilation errors in ML integration tests

After elastic/elasticsearch#29109, the `needsReassignment` method has
been moved to the PersistentTasksClusterService. This commit fixes
some compilation in tests I introduced.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states ""Fix compilation errors in ML integration tests"". This strongly suggests that the previous commit (elastic/elasticsearch#29109) introduced a breaking change, and this commit is intended to rectify that.
    *   The message also mentions that the `needsReassignment` method was moved to `PersistentTasksClusterService`. This implies that code relying on the old location or signature of this method would now fail to compile.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the visibility modifier of the `needsReassignment` method from `static boolean` to `public static boolean`.
    *   Compilation errors often arise when a method's visibility is changed (e.g., from public to private or package-private), or when a method is moved to a different class. In this case, the change from `static` to `public static` is less likely to cause a compilation error directly. However, the commit message tells us that the method was moved in a previous commit. The change to `public` here is likely to make the method accessible from the test classes that were previously able to access it due to being in the same package or due to the method being package-private.

3.  **Synthesis:**
    *   The commit message clearly indicates a fix for compilation errors.
    *   The code change, while small, aligns with the commit message's description of adapting to a change in the location of the `needsReassignment` method. The change to `public` is likely to make the method accessible from test classes.
    *   The fact that the tests were broken by a previous commit and this commit fixes them indicates a bug fix.

**Conclusion:**

**Buggy**
"
elasticsearch,63560.json,63f33e0f1e045f5c345b80bdbc4b6f367e72aaad,"@@ -1,16 +1,18 @@
     public void writeTo(StreamOutput out) throws IOException {
+        // marshall doc count
+        out.writeGenericValue(docCount);
         // marshall fieldSum
         out.writeGenericValue(fieldSum);
         // counts
         out.writeGenericValue(counts);
         // mean
         out.writeGenericValue(means);
         // variances
         out.writeGenericValue(variances);
         // skewness
         out.writeGenericValue(skewness);
         // kurtosis
         out.writeGenericValue(kurtosis);
         // covariances
         out.writeGenericValue(covariances);
     }",Buggy,"Serialize doc counts in Matrix-Stats module

This fixes a bug in the RunningStats class for the matrix stats aggregation module. doc counts were not being searlized which means they were only computed the first time the aggregation was computed. This was causing incorrect results when the aggregation was pulled from cache.
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message:** The commit message clearly states a bug fix related to the `RunningStats` class in the matrix-stats aggregation module. The bug was that `docCount` was not being serialized, leading to incorrect results when the aggregation was retrieved from the cache.

2.  **Git Diff:** The diff shows a modification to the `writeTo` method, which is responsible for serializing the `RunningStats` object. The line `out.writeGenericValue(docCount);` is added. This line serializes the `docCount` field, which aligns perfectly with the commit message's description of the bug fix.

3.  **Alignment:** The commit message and the code change are perfectly aligned. The code change directly addresses the bug described in the commit message. The commit message explains the consequences of the bug (incorrect results from cache), and the code change fixes the root cause (lack of serialization of `docCount`).

**Conclusion:**

**Buggy**
"
elasticsearch,62818.json,e6fb3a5d950c52ccba8cf1f0b0d6e819e70c243c,"@@ -1,15 +1,21 @@
     public void execute(IngestDocument document) {
         String oldVal = document.getFieldValue(field, String.class, ignoreMissing);
 
         if (oldVal == null && ignoreMissing) {
             return;
         } else if (oldVal == null) {
             throw new IllegalArgumentException(""field ["" + field + ""] is null, cannot extract key-value pairs."");
         }
 
         String fieldPathPrefix = (targetField == null) ? """" : targetField + ""."";
         Arrays.stream(oldVal.split(fieldSplit))
-            .map((f) -> f.split(valueSplit, 2))
+            .map((f) -> {
+                String[] kv = f.split(valueSplit, 2);
+                if (kv.length != 2) {
+                    throw new IllegalArgumentException(""field ["" + field + ""] does not contain value_split ["" + valueSplit + ""]"");
+                }
+                return kv;
+            })
             .filter((p) -> includeKeys == null || includeKeys.contains(p[0]))
             .forEach((p) -> append(document, fieldPathPrefix + p[0], p[1]));
     }",Buggy,"fix index out of bounds error in KV Processor (#22288)

- checks for index-out-of-bounds
- added unit tests for failed `field_split` and `value_split` scenarios

missed this test in #22272.",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states that it fixes an ""index out of bounds error"" in the KV Processor. It also mentions adding unit tests for failed `field_split` and `value_split` scenarios. This strongly suggests a bug fix.

2.  **Code Diff:**
    *   The code now includes a check `if (kv.length != 2)` after splitting each field using `valueSplit`.
    *   If the resulting array `kv` does not have a length of 2, an `IllegalArgumentException` is thrown, indicating that the `valueSplit` did not produce the expected key-value pair.
    *   The original code directly accessed `p[0]` and `p[1]` without validating the size of `p`, which could lead to an `ArrayIndexOutOfBoundsException` if `valueSplit` didn't produce two elements.

3.  **Alignment:** The code change directly addresses the potential ""index out of bounds error"" mentioned in the commit message. The added check ensures that the code only proceeds if the split operation results in a key-value pair, preventing the error. The commit message also mentions adding unit tests, which further supports the idea of a bug fix.

**Conclusion:**

The code diff introduces a check to prevent an `ArrayIndexOutOfBoundsException` when the `valueSplit` doesn't produce the expected number of elements. This aligns perfectly with the commit message indicating a fix for an ""index out of bounds error.""

**Buggy**
"
elasticsearch,31383.json,76cd7b1eb2cf7f87081c24ae6362ae1d9dec6100,"@@ -1,3 +1,3 @@
-        public Params getParams() {
+        public P getParams() {
             return params;
         }",Buggy,"Fixes compile errors in Eclipse due to generics

PersistentTasksCustomMetadata was using a generic param named `Params`. This conflicted with the imported interface `ToXContent.Params`. The java compiler was preferring the generic param over the interface so everything was fine but Eclipse apparently prefers the interface int his case which was screwing up the Hierarchy and causing compile errors in Eclipse. This changes fixes it by renaming the Generic param to `P`
",Buggy,"The commit message indicates a fix for a compile error in Eclipse caused by a naming conflict with a generic parameter. The generic parameter `Params` in `PersistentTasksCustomMetadata` conflicted with the imported interface `ToXContent.Params`. The Java compiler and Eclipse handled the conflict differently, leading to compile errors in Eclipse.

The code diff shows that the generic parameter `Params` has been renamed to `P`. This change directly addresses the naming conflict described in the commit message. The renaming resolves the ambiguity and ensures that Eclipse correctly identifies the intended type, thus fixing the compile errors.

Based on the commit message and the code diff, the change clearly indicates a bug fix related to a naming conflict that caused compile errors in Eclipse.

**Buggy**
"
elasticsearch,18621.json,6236b3aee4c4ccf9a06ca94af4c4082b7fcf5cde,"@@ -1,32 +1,45 @@
     public ResourcePrivilegesMap checkResourcePrivileges(Set<String> checkForIndexPatterns, boolean allowRestrictedIndices,
                                                          Set<String> checkForPrivileges) {
         final ResourcePrivilegesMap.Builder resourcePrivilegesMapBuilder = ResourcePrivilegesMap.builder();
         final Map<IndicesPermission.Group, Automaton> predicateCache = new HashMap<>();
         for (String forIndexPattern : checkForIndexPatterns) {
-            final Automaton checkIndexAutomaton = IndicesPermission.Group.buildIndexMatcherAutomaton(allowRestrictedIndices,
-                    forIndexPattern);
-            Automaton allowedIndexPrivilegesAutomaton = null;
-            for (Group group : groups) {
-                final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
-                        g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
-                if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
-                    if (allowedIndexPrivilegesAutomaton != null) {
-                        allowedIndexPrivilegesAutomaton = Automatons
-                                .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
-                    } else {
-                        allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+            Automaton checkIndexAutomaton = Automatons.patterns(forIndexPattern);
+            if (false == allowRestrictedIndices && false == RestrictedIndicesNames.RESTRICTED_NAMES.contains(forIndexPattern)) {
+                checkIndexAutomaton = Automatons.minusAndMinimize(checkIndexAutomaton, RestrictedIndicesNames.NAMES_AUTOMATON);
+            }
+            if (false == Operations.isEmpty(checkIndexAutomaton)) {
+                Automaton allowedIndexPrivilegesAutomaton = null;
+                for (Group group : groups) {
+                    final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
+                            g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
+                    if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
+                        if (allowedIndexPrivilegesAutomaton != null) {
+                            allowedIndexPrivilegesAutomaton = Automatons
+                                    .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
+                        } else {
+                            allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+                        }
                     }
                 }
-            }
-            for (String privilege : checkForPrivileges) {
-                IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
-                if (allowedIndexPrivilegesAutomaton != null
-                        && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
-                    resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
-                } else {
+                for (String privilege : checkForPrivileges) {
+                    IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
+                    if (allowedIndexPrivilegesAutomaton != null
+                            && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
+                    } else {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
+                    }
+                }
+            } else {
+                // the index pattern produced the empty automaton, presumably because the requested pattern expands exclusively inside the
+                // restricted indices namespace - a namespace of indices that are normally hidden when granting/checking privileges - and
+                // the pattern was not marked as `allowRestrictedIndices`. We try to anticipate this by considering _explicit_ restricted
+                // indices even if `allowRestrictedIndices` is false.
+                // TODO The `false` result is a _safe_ default but this is actually an error. Make it an error.
+                for (String privilege : checkForPrivileges) {
                     resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
                 }
             }
         }
         return resourcePrivilegesMapBuilder.build();
     }",Buggy,"Fix Has Privilege API check on restricted indices (#41226)

The Has Privileges API allows to tap into the authorization process, to validate
privileges without actually running the operations to be authorized. This commit
fixes a bug, in which the Has Privilege API returned spurious results when checking
for index privileges over restricted indices (currently .security, .security-6,
.security-7). The actual authorization process is not affected by the bug.",Buggy,"The commit message indicates a bug fix in the Has Privileges API, specifically related to checking index privileges over restricted indices. The code diff modifies the logic for building the `checkIndexAutomaton`.

Here's a breakdown of the changes and their relevance to the bug fix:

1.  **Automaton Creation:** The original code directly builds the `checkIndexAutomaton` using `IndicesPermission.Group.buildIndexMatcherAutomaton`. The modified code now uses `Automatons.patterns(forIndexPattern)` to create the initial automaton.

2.  **Restricted Indices Handling:** The new code explicitly checks if `allowRestrictedIndices` is false and if the index pattern is a restricted index. If both conditions are true, it subtracts the restricted indices namespace from the automaton using `Automatons.minusAndMinimize`. This is the core of the bug fix, ensuring that restricted indices are correctly handled when `allowRestrictedIndices` is false.

3.  **Empty Automaton Handling:** The code includes a check for an empty `checkIndexAutomaton`. If the automaton is empty, it means the index pattern expands exclusively inside the restricted indices namespace, and `allowRestrictedIndices` is false. In this case, it explicitly sets the privilege check to false. The comment indicates that this is a safe default but should ideally be an error in the future.

The changes directly address the issue described in the commit message. The bug fix ensures that the Has Privileges API returns correct results when checking for index privileges over restricted indices, especially when `allowRestrictedIndices` is false. The code modifications focus on refining the automaton used for privilege checks, taking into account restricted indices.

Therefore, the changes indicate a bug fix.

**Buggy**
"
elasticsearch,52742.json,a36543531b4f547bbb7be8156d4c1e55f0d53cf9,"@@ -1,8 +1,11 @@
     private void onFailedFreedContext(Throwable e, DiscoveryNode node) {
         logger.warn(() -> new ParameterizedMessage(""Clear SC failed on node[{}]"", node), e);
+        /*
+         * We have to set the failure marker before we count down otherwise we can expose the failure marker before we have set it to a
+         * racing thread successfully freeing a context. This would lead to that thread responding that the clear scroll succeeded.
+         */
+        hasFailed.set(true);
         if (expectedOps.countDown()) {
             listener.onResponse(new ClearScrollResponse(false, freedSearchContexts.get()));
-        } else {
-            hasFailed.set(true);
         }
     }",Buggy,"Fix race in clear scroll (#31259)

Here is the problem: if two threads are racing and one hits a failure
freeing a context and the other succeeded, we can expose the value of
the has failure marker to the succeeding thread before the failing
thread has had a chance to set the failure marker. This is a problem if
the failing thread counted down the expected number of operations, then
be put to sleep by a gentle lullaby from the OS, and then the other
thread could count down to zero. Since the failing thread did not get to
set the failure marker, the succeeding thread would respond that the
clear scroll succeeded and that makes that thread a liar. This commit
addresses by first setting the failure marker before we potentially
expose its value to another thread.",Buggy,"The commit message describes a race condition in the `clear scroll` operation where a thread succeeding in freeing a context could report success prematurely if another thread fails but hasn't yet set the failure marker. The commit message explicitly states the fix involves setting the failure marker before potentially exposing its value to another thread.

The code diff moves the `hasFailed.set(true)` call to the beginning of the `onFailedFreedContext` method, before the `countDown` operation. This aligns perfectly with the commit message's description of the fix. The original code had a race condition where `expectedOps.countDown()` could potentially trigger the `listener.onResponse` before `hasFailed.set(true)` was executed, leading to the incorrect success report.

Therefore, the changes clearly indicate a bug fix.

**Buggy**
"
elasticsearch,9348.json,308ae98988fb9d7c5374a5b16dac810a7bd9715c,"@@ -1,20 +1,22 @@
         private List<Realm> getRealmList(String principal) {
             final List<Realm> orderedRealmList = this.defaultOrderedRealmList;
             if (lastSuccessfulAuthCache != null) {
                 final Realm lastSuccess = lastSuccessfulAuthCache.get(principal);
                 if (lastSuccess != null) {
                     final int index = orderedRealmList.indexOf(lastSuccess);
                     if (index > 0) {
                         final List<Realm> smartOrder = new ArrayList<>(orderedRealmList.size());
                         smartOrder.add(lastSuccess);
-                        for (int i = 1; i < orderedRealmList.size(); i++) {
+                        for (int i = 0; i < orderedRealmList.size(); i++) {
                             if (i != index) {
                                 smartOrder.add(orderedRealmList.get(i));
                             }
                         }
+                        assert smartOrder.size() == orderedRealmList.size() && smartOrder.containsAll(orderedRealmList)
+                            : ""Element mismatch between SmartOrder="" + smartOrder + "" and DefaultOrder="" + orderedRealmList;
                         return Collections.unmodifiableList(smartOrder);
                     }
                 }
             }
             return orderedRealmList;
         }",Buggy,"Fix iterate-from-1 bug in smart realm order (#49473)

The AuthenticationService has a feature to ""smart order"" the realm
chain so that whicherver realm was the last one to successfully
authenticate a given user will be tried first when that user tries to
authenticate again.

There was a bug where the building of this realm order would
incorrectly drop the first realm from the default chain unless that
realm was the ""last successful"" realm.

In most cases this didn't cause problems because the first realm is
the reserved realm and so it is unusual for a user that authenticated
against a different realm to later need to authenticate against the
resevered realm.

This commit fixes that bug and adds relevant asserts and tests.",Buggy,"The commit message indicates a bug fix related to the ""smart order"" feature of realms in the `AuthenticationService`. The bug caused the first realm in the default chain to be dropped incorrectly under certain conditions.

The code diff modifies the loop in the `getRealmList` method.  The original loop started from `i = 1`, while the corrected loop starts from `i = 0`. This change addresses the bug described in the commit message, where the first realm was being dropped. The added assertion confirms that the `smartOrder` list contains all elements from the `defaultOrderedRealmList` and has the same size, ensuring no realms are dropped.

Based on the commit message and the code diff, the change fixes a bug in the realm ordering logic.

**Buggy**"
elasticsearch,38299.json,38085cf90af953cfcaf7807c2dfccc55b742cdff,"@@ -1,5 +1,4 @@
     public AggregationPath subPath(int offset, int length) {
-        PathElement[] subTokens = new PathElement[length];
-        System.arraycopy(pathElements, offset, subTokens, 0, length);
-        return new AggregationPath(pathElements);
+        List<PathElement> subTokens = new ArrayList<>(pathElements.subList(offset, offset + length));
+        return new AggregationPath(subTokens);
     }",Buggy,"Aggregation: Fix AggregationPath.subPath() to not throw ArrayStoreException

Aggregation.subPath() always threw an ArrayStoreException because we were trying to pass a List into System.arraycopy(). This change fixes that bug and adds a test to prevent regression
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states that `Aggregation.subPath()` was throwing an `ArrayStoreException`. It also mentions that the change fixes this bug and adds a test to prevent regression. This strongly suggests a bug fix.

2.  **Code Diff Analysis:**
    *   The original code used `System.arraycopy()` to copy elements from `pathElements` (presumably a `PathElement[]`) into `subTokens` (also a `PathElement[]`). However, the `AggregationPath` constructor was being called with the original `pathElements` instead of the newly created `subTokens`. This is a logical error, but not the cause of the `ArrayStoreException`.
    *   The corrected code uses `pathElements.subList()` to create a sublist of `PathElement` objects. This sublist is then used to create a new `ArrayList<PathElement>`. The `AggregationPath` is then constructed with this new list.
    *   The `ArrayStoreException` likely occurred because `System.arraycopy` expects an array as the destination, but was likely receiving a `List` or some other incompatible type. The change to use `ArrayList` resolves this.

3.  **Alignment:** The code change aligns perfectly with the commit message. The change directly addresses the reported `ArrayStoreException` by using a `List` instead of an array in the `AggregationPath` constructor.

**Conclusion:**

The commit message and code diff clearly indicate a bug fix. The original code was throwing an `ArrayStoreException`, and the change resolves this issue by using a `List` instead of an array.

**Buggy**"
elasticsearch,10639.json,8b201e64ffeffa8dfe8c5849a81decb2a62dbe06,"@@ -1,19 +1,20 @@
     public static String loadWatch(final ClusterService clusterService, final String watchId) {
         final String resource = String.format(Locale.ROOT, WATCH_FILE, watchId);
 
         try {
             final String clusterUuid = clusterService.state().metaData().clusterUUID();
             final String uniqueWatchId = createUniqueWatchId(clusterUuid, watchId);
 
             // load the resource as-is
             String source = loadResource(resource).utf8ToString();
 
             source = CLUSTER_UUID_PROPERTY.matcher(source).replaceAll(clusterUuid);
             source = WATCH_ID_PROPERTY.matcher(source).replaceAll(watchId);
             source = UNIQUE_WATCH_ID_PROPERTY.matcher(source).replaceAll(uniqueWatchId);
+            source = VERSION_CREATED_PROPERTY.matcher(source).replaceAll(Integer.toString(LAST_UPDATED_VERSION));
 
             return source;
         } catch (final IOException e) {
             throw new RuntimeException(""Unable to load Watch ["" + watchId + ""]"", e);
         }
     }",Buggy,"Fix cluster alert for watcher/monitoring IndexOutOfBoundsExcep… (#45308)

If a cluster sending monitoring data is unhealthy and triggers an
alert, then stops sending data the following exception [1] can occur.

This exception stops the current Watch and the behavior is actually
correct in part due to the exception. Simply fixing the exception
introduces some incorrect behavior. Now that the Watch does not
error in the this case, it will result in an incorrectly ""resolved""
alert.  The fix here is two parts a) fix the exception b) fix the
following incorrect behavior.

a) fixing the exception is as easy as checking the size of the
array before accessing it.

b) fixing the following incorrect behavior is a bit more intrusive

- Note - the UI depends on the success/met state for each condition
to determine an ""OK"" or ""FIRING""

In this scenario, where an unhealthy cluster triggers an alert and
then goes silent, it should keep ""FIRING"" until it hears back that
the cluster is green. To keep the Watch ""FIRING"" either the index
action or the email action needs to fire. Since the Watch is neither
a ""new"" alert or a ""resolved"" alert, we do not want to keep sending
an email (that would be non-passive too). Without completely changing
the logic of how an alert is resolved allowing the index action to
take place would result in the alert being resolved. Since we can
not keep ""FIRING"" either the email or index action (since we don't
want to resolve the alert nor re-write the logic for alert resolution),
we will introduce a 3rd action. A logging action that WILL fire when
the cluster is unhealthy. Specifically will fire when there is an
unresolved alert and it can not find the cluster state.
This logging action is logged at debug, so it should be noticed much.
This logging action serves as an 'anchor' for the UI to keep the state
in an a ""FIRING"" status until the alert is resolved.

This presents a possible scenario where a cluster starts firing,
then goes completely silent forever, the Watch will be ""FIRING""
forever. This is an edge case that already exists in some scenarios
and requires manual intervention to remove that Watch.

This changes changes to use a template-like method to populate the 
version_created for the default monitoring watches. The version is 
set to 7.5 since that is where this is first introduced.

Fixes #43184




",Buggy,"The commit message indicates a bug fix for an `IndexOutOfBoundsException` in the monitoring cluster alert system. It also describes a scenario where an unhealthy cluster stops sending data, leading to incorrect alert resolution. The fix involves checking the size of an array to prevent the exception and introducing a logging action to maintain the ""FIRING"" status of the alert in the UI when the cluster is silent. The commit message also mentions updating the `version_created` property for default monitoring watches.

The diff shows a modification to the `loadWatch` method, specifically adding a replacement for the `VERSION_CREATED_PROPERTY` using `LAST_UPDATED_VERSION`. This change aligns with the commit message's description of updating the version for default monitoring watches. While the diff itself doesn't directly show the fix for the `IndexOutOfBoundsException` or the introduction of the logging action, the commit message provides context for the change and confirms that it is part of a larger bug fix.

Therefore, based on the commit message and the provided diff, the changes indicate a bug fix.

**Buggy**"
elasticsearch,61886.json,aed30d6cc7761d12b929696774591ac0ccb989d0,"@@ -1,32 +1,32 @@
     public void checkInvalidPatterns() throws IOException {
         Pattern allPatterns = Pattern.compile(""("" + String.join("")|("", getPatterns().values()) + "")"");
         List<String> failures = new ArrayList<>();
         for (File f : files()) {
             List<String> lines;
             try(Stream<String> stream = Files.lines(f.toPath(), StandardCharsets.UTF_8)) {
                     lines = stream.collect(Collectors.toList());
             } catch (UncheckedIOException e) {
                 throw new IllegalArgumentException(""Failed to read "" + f + "" as UTF_8"", e);
             }
             List<Integer> invalidLines = IntStream.range(0, lines.size())
                 .filter(i -> allPatterns.matcher(lines.get(i)).find())
                 .boxed()
                 .collect(Collectors.toList());
 
             String path = getProject().getRootProject().getProjectDir().toURI().relativize(f.toURI()).toString();
-            failures = invalidLines.stream()
+            failures.addAll(invalidLines.stream()
                 .map(l -> new AbstractMap.SimpleEntry<>(l+1, lines.get(l)))
                 .flatMap(kv -> patterns.entrySet().stream()
                     .filter(p -> Pattern.compile(p.getValue()).matcher(kv.getValue()).find())
                     .map(p -> ""- "" + p.getKey() + "" on line "" + kv.getKey() + "" of "" + path)
                 )
-                .collect(Collectors.toList());
+                .collect(Collectors.toList()));
         }
         if (failures.isEmpty() == false) {
             throw new GradleException(""Found invalid patterns:\n"" + String.join(""\n"", failures));
         }
 
         File outputMarker = getOutputMarker();
         outputMarker.getParentFile().mkdirs();
         Files.write(outputMarker.toPath(), ""done"".getBytes(StandardCharsets.UTF_8));
     }",Buggy,"Don't replace forbidden pattern failures when found (#40710)

This commit fixes a bug in forbidden patterns where the failures for a
file replace the failures from the previous files instead of extending
them.
",Buggy,"The commit message indicates a bug fix related to how forbidden pattern failures are handled. Specifically, it mentions that failures from previous files were being replaced instead of extended.

The code diff shows a change in the `checkInvalidPatterns` method. The line `failures = invalidLines.stream()` is changed to `failures.addAll(invalidLines.stream())`. This change directly addresses the bug described in the commit message. Instead of reassigning the `failures` list in each iteration of the loop, the new failures are now added to the existing list. This ensures that failures from all files are accumulated correctly.

The change directly fixes the bug described in the commit message.

**Buggy**"
elasticsearch,39934.json,8260138e5975ebcb588933d792d22374168c48cf,"@@ -1,21 +1,21 @@
         void toXContent(XContentBuilder builder, Params params, boolean keyed, @Nullable ValueFormatter formatter) throws IOException {
-            if (formatter != null) {
+            if (formatter != null && formatter != ValueFormatter.RAW) {
                 Text keyTxt = new StringText(formatter.format(key));
                 if (keyed) {
                     builder.startObject(keyTxt.string());
                 } else {
                     builder.startObject();
                 }
                 builder.field(CommonFields.KEY_AS_STRING, keyTxt);
             } else {
                 if (keyed) {
                     builder.startObject(String.valueOf(getKeyAsNumber()));
                 } else {
                     builder.startObject();
                 }
             }
             builder.field(CommonFields.KEY, key);
             builder.field(CommonFields.DOC_COUNT, docCount);
             aggregations.toXContentInternal(builder, params);
             builder.endObject();
         }",Buggy,"Aggregations: Fixed Histogram key_as_string bug

The key as string field in the response for the histogram aggregation will now only show if format is specified on the request.

Closes #6655
",Buggy,"The commit message indicates a fix for a bug related to the `key_as_string` field in the histogram aggregation response. Specifically, the message states that the `key_as_string` field should only be present if a format is specified in the request.

The diff modifies the `toXContent` method, which is responsible for generating the XContent representation of the histogram bucket. The key change is the addition of `formatter != ValueFormatter.RAW` to the `if` condition: `if (formatter != null && formatter != ValueFormatter.RAW)`. This ensures that the code block responsible for adding the `key_as_string` field is only executed if a formatter is specified and it's not the raw formatter.

Before this change, the `key_as_string` field was always included if a formatter was present, regardless of whether it was the raw formatter. This could lead to unexpected behavior and inconsistencies in the response. The modification ensures that the `key_as_string` field is only added when a specific format is requested, aligning with the intended behavior.

The commit message and the code changes are consistent. The code change directly addresses the bug described in the commit message. The addition of the `formatter != ValueFormatter.RAW` condition ensures that the `key_as_string` field is only included when a format is explicitly specified, thus fixing the bug.

**Buggy**
"
elasticsearch,8075.json,ae4bfe99ecd6e298c56236c7abf9a53c6b490ab2,"@@ -1,30 +1,31 @@
     protected void masterOperation(Task task, DeleteEnrichPolicyAction.Request request, ClusterState state,
                                    ActionListener<AcknowledgedResponse> listener) throws Exception {
         List<PipelineConfiguration> pipelines = IngestService.getPipelines(state);
         EnrichPolicy policy = EnrichStore.getPolicy(request.getName(), state);
         List<String> pipelinesWithProcessors = new ArrayList<>();
 
         for (PipelineConfiguration pipelineConfiguration : pipelines) {
             List<AbstractEnrichProcessor> enrichProcessors =
                 ingestService.getProcessorsInPipeline(pipelineConfiguration.getId(), AbstractEnrichProcessor.class);
             for (AbstractEnrichProcessor processor: enrichProcessors) {
                 if (processor.getPolicyName().equals(request.getName())) {
                     pipelinesWithProcessors.add(pipelineConfiguration.getId());
                 }
             }
         }
 
         if (pipelinesWithProcessors.isEmpty() == false) {
             listener.onFailure(
                 new ElasticsearchStatusException(""Could not delete policy [{}] because a pipeline is referencing it {}"",
                     RestStatus.CONFLICT, request.getName(), pipelinesWithProcessors));
+            return;
         }
 
         EnrichStore.deletePolicy(request.getName(), clusterService, e -> {
            if (e == null) {
                listener.onResponse(new AcknowledgedResponse(true));
            } else {
                listener.onFailure(e);
            }
         });
     }",Buggy,"Fix policy removal bug in delete policy (#45573)

The delete policy had a subtle bug in that it would still delete the
policy if pipelines were accessing it, after giving the client back an
error. This commit fixes that and ensures it does not happen by adding
verification in the test.",Buggy,"The commit message indicates a bug fix related to the deletion of enrich policies. The bug was that the policy would still be deleted even if pipelines were referencing it, despite the client receiving an error.

The code diff shows that a `return` statement has been added after the `listener.onFailure` call within the `if (pipelinesWithProcessors.isEmpty() == false)` block. This ensures that the `EnrichStore.deletePolicy` method is not called if there are pipelines referencing the policy. This aligns perfectly with the commit message's description of the bug and its fix. The addition of the `return` statement prevents the erroneous deletion of the policy.

Therefore, the changes indicate a bug fix.
**Buggy**
"
elasticsearch,51449.json,63fe3c6ed604d2aee3ae5266b35e87f35ff09bee,"@@ -1,40 +1,44 @@
     private void verifyThenSubmitUpdate(ClusterRerouteRequest request, ActionListener<ClusterRerouteResponse> listener,
         Map<String, List<AbstractAllocateAllocationCommand>> stalePrimaryAllocations) {
         transportService.sendRequest(transportService.getLocalNode(), IndicesShardStoresAction.NAME,
             new IndicesShardStoresRequest().indices(stalePrimaryAllocations.keySet().toArray(Strings.EMPTY_ARRAY)),
             new ActionListenerResponseHandler<>(
                 ActionListener.wrap(
                     response -> {
                         ImmutableOpenMap<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> status =
                             response.getStoreStatuses();
                         Exception e = null;
                         for (Map.Entry<String, List<AbstractAllocateAllocationCommand>> entry : stalePrimaryAllocations.entrySet()) {
                             final String index = entry.getKey();
                             final ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>> indexStatus = status.get(index);
-                            assert indexStatus != null;
+                            if (indexStatus == null) {
+                                // The index in the stale primary allocation request was green and hence filtered out by the store status
+                                // request. We ignore it here since the relevant exception will be thrown by the reroute action later on.
+                                continue;
+                            }
                             for (AbstractAllocateAllocationCommand command : entry.getValue()) {
                                 final List<IndicesShardStoresResponse.StoreStatus> shardStatus =
                                     indexStatus.get(command.shardId());
                                 if (shardStatus == null || shardStatus.isEmpty()) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on any node"")
                                     );
                                 } else if (shardStatus.stream().noneMatch(storeStatus -> {
                                     final DiscoveryNode node = storeStatus.getNode();
                                     final String nodeInCommand = command.node();
                                     return nodeInCommand.equals(node.getName()) || nodeInCommand.equals(node.getId());
                                 })) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on node [""
                                             + command.node() + ']'));
                                 }
                             }
                         }
                         if (e == null) {
                             submitStateUpdate(request, listener);
                         } else {
                             listener.onFailure(e);
                         }
                     }, listener::onFailure
                 ), IndicesShardStoresResponse::new));
     }",Buggy,"Fix PrimaryAllocationIT Race Condition (#37355)

* Fix PrimaryAllocationIT Race Condition

* Forcing a stale primary allocation on a green index was tripping the assertion that was removed
   * Added a test that this case still errors out correctly
* Made the ability to wipe stopped datanode's data public on the internal test cluster and used it to ensure correct behaviour on the fixed test
   * Previously it simply passed because the test finished before the index went green and would NPE when the index was green at the time of the shard store status request, that would then come up empty
* Closes #37345
",Buggy,"The commit message indicates a fix for a race condition in `PrimaryAllocationIT`. The code diff modifies the `verifyThenSubmitUpdate` method. Specifically, it adds a null check for `indexStatus` after retrieving it from the `status` map. The comment explains that if `indexStatus` is null, it means the index was green and filtered out by the store status request. In this case, the code continues to the next entry, and the exception will be thrown later by the reroute action. This change addresses a scenario where the index becomes green before the shard store status request is processed, leading to a null `indexStatus` and potentially a `NullPointerException` or incorrect behavior. The addition of the null check prevents this issue. This aligns with the commit message's claim of fixing a race condition.

Therefore, the changes indicate a bug fix.
**Buggy**
"
elasticsearch,47709.json,049e122e8f653fb54f12e0f0aa1eb86c5a1a629d,"@@ -1,18 +1,18 @@
     public static Map<Integer, List<String>> getAutoExpandReplicaChanges(MetaData metaData, DiscoveryNodes discoveryNodes) {
         // used for translating ""all"" to a number
         final int dataNodeCount = discoveryNodes.getDataNodes().size();
 
         Map<Integer, List<String>> nrReplicasChanged = new HashMap<>();
 
         for (final IndexMetaData indexMetaData : metaData) {
-            if (indexMetaData.getState() != IndexMetaData.State.CLOSE) {
+            if (indexMetaData.getState() == IndexMetaData.State.OPEN || isIndexVerifiedBeforeClosed(indexMetaData)) {
                 AutoExpandReplicas autoExpandReplicas = SETTING.get(indexMetaData.getSettings());
                 autoExpandReplicas.getDesiredNumberOfReplicas(dataNodeCount).ifPresent(numberOfReplicas -> {
                     if (numberOfReplicas != indexMetaData.getNumberOfReplicas()) {
                         nrReplicasChanged.computeIfAbsent(numberOfReplicas, ArrayList::new).add(indexMetaData.getIndex().getName());
                     }
                 });
             }
         }
         return nrReplicasChanged;
     }",Buggy,"Auto-expand replicated closed indices (#48973)

Fixes a bug where replicated closed indices were not being auto-expanded.",Buggy,"The commit message indicates a bug fix related to auto-expanding replicated closed indices. The code diff modifies the condition under which the auto-expansion logic is applied.

Specifically, the original code only considered indices that were NOT in the `CLOSE` state:
```java
if (indexMetaData.getState() != IndexMetaData.State.CLOSE) {
```

The modified code now includes an additional condition:
```java
if (indexMetaData.getState() == IndexMetaData.State.OPEN || isIndexVerifiedBeforeClosed(indexMetaData)) {
```
This change suggests that the original logic was incorrectly excluding closed indices from the auto-expansion process, which aligns with the bug described in the commit message. The addition of `isIndexVerifiedBeforeClosed` further supports the idea that the original logic was flawed and needed correction to handle specific closed indices.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
elasticsearch,46897.json,cdb482eaae0517f202efdfaf445e8847917182f1,"@@ -1,9 +1,13 @@
     boolean cancelCommittedPublication() {
         synchronized (mutex) {
-            if (currentPublication.isPresent() && currentPublication.get().isCommitted()) {
-                currentPublication.get().cancel(""cancelCommittedPublication"");
-                return true;
+            if (currentPublication.isPresent()) {
+                final CoordinatorPublication publication = currentPublication.get();
+                if (publication.isCommitted()) {
+                    publication.cancel(""cancelCommittedPublication"");
+                    logger.debug(""Cancelled publication of [{}]."", publication);
+                    return true;
+                }
             }
             return false;
         }
     }",Buggy,"Fix RareClusterStateIT (#42430)

* It looks like we might be cancelling a previous publication instead of
the one triggered by the given request with a very low likelihood.
   * Fixed by adding a wait for no in-progress publications
   * Also added debug logging that would've identified this problem
* Closes #36813",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Fix RareClusterStateIT (#42430)"" explicitly states that the commit is intended to fix a bug. It mentions a potential issue where a previous publication might be incorrectly cancelled instead of the intended one. The fix involves waiting for in-progress publications and adding debug logging. The commit message also closes issue #36813, further suggesting a bug fix.

2.  **Git Diff:**
    *   The code modifies the `cancelCommittedPublication()` method.
    *   The original code only cancelled the `currentPublication` if it was present and committed.
    *   The modified code adds a debug log statement when a publication is cancelled: `logger.debug(""Cancelled publication of [{}]."", publication);`.
    *   The modified code now retrieves the `currentPublication` and assigns it to the `publication` variable before checking if it is committed and cancelling it. This suggests that the code is now referring to the correct publication when cancelling.

3.  **Reasoning:**
    *   The commit message indicates a bug fix related to incorrect publication cancellation.
    *   The code changes introduce debug logging, which is a common practice when investigating and fixing bugs.
    *   The modification of the `cancelCommittedPublication` method, specifically the addition of the `publication` variable, suggests a correction in how the publication to be cancelled is identified. This aligns with the commit message's description of potentially cancelling the wrong publication.
    *   The addition of debug logging strongly suggests that the original code lacked sufficient information to diagnose the problem, which is typical in bug-fixing scenarios.

**Conclusion:**

The commit message and the code changes strongly suggest that this commit is a bug fix. The code changes address a potential issue with incorrect publication cancellation, and the addition of debug logging further supports this conclusion.

**Buggy**
"
elasticsearch,19038.json,7597b7ce2bd280401fcbfbeb281dfbb205830d75,"@@ -1,34 +1,38 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        for (ApplicationPrivilegeDescriptor privilege : privileges) {
-            try {
-                ApplicationPrivilege.validateApplicationName(privilege.getApplication());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            try {
-                ApplicationPrivilege.validatePrivilegeName(privilege.getName());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            if (privilege.getActions().isEmpty()) {
-                validationException = addValidationError(""Application privileges must have at least one action"", validationException);
-            }
-            for (String action : privilege.getActions()) {
-                if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
-                    validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
-                        validationException);
-                }
+        if (privileges.isEmpty()) {
+            validationException = addValidationError(""At least one application privilege must be provided"", validationException);
+        } else {
+            for (ApplicationPrivilegeDescriptor privilege : privileges) {
                 try {
-                    ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    ApplicationPrivilege.validateApplicationName(privilege.getApplication());
                 } catch (IllegalArgumentException e) {
                     validationException = addValidationError(e.getMessage(), validationException);
                 }
-            }
-            if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
-                validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
-                    + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                try {
+                    ApplicationPrivilege.validatePrivilegeName(privilege.getName());
+                } catch (IllegalArgumentException e) {
+                    validationException = addValidationError(e.getMessage(), validationException);
+                }
+                if (privilege.getActions().isEmpty()) {
+                    validationException = addValidationError(""Application privileges must have at least one action"", validationException);
+                }
+                for (String action : privilege.getActions()) {
+                    if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
+                        validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
+                            validationException);
+                    }
+                    try {
+                        ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    } catch (IllegalArgumentException e) {
+                        validationException = addValidationError(e.getMessage(), validationException);
+                    }
+                }
+                if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
+                    validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
+                        + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                }
             }
         }
         return validationException;
     }",Buggy,"Add validation for empty PutPrivilegeRequest (#37569)

Return an error to the user if the put privilege api is called with
an empty body (no privileges)

Resolves: #37561",Buggy,"The commit message indicates that a validation check has been added to ensure that the `PutPrivilegeRequest` contains at least one privilege. The code diff confirms this by adding a check at the beginning of the `validate()` method to verify if the `privileges` list is empty. If it is, a validation error is added. The original validation logic is then wrapped in an `else` block, ensuring that it only executes when the `privileges` list is not empty. This change directly addresses the issue of handling empty requests, which could be considered a bug or at least an undesirable behavior.

**Buggy**"
elasticsearch,49943.json,adc195e30c4e237372f168086f7a57b0c3b5e7f1,"@@ -1,10 +1,10 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
         if (name == null) {
             validationException = addValidationError(""name is missing"", validationException);
         }
         if (indexPatterns == null || indexPatterns.size() == 0) {
-            validationException = addValidationError(""pattern is missing"", validationException);
+            validationException = addValidationError(""index patterns are missing"", validationException);
         }
         return validationException;
     }",Buggy,"Fix error message for a put index template request without index_patterns (#27102)

Just correct the error message from ""Validation Failed: 1: pattern is
missing;"" to ""Validation Failed: 1: index_patterns is missing;"".

Closes #27100",Buggy,"The commit message indicates a fix to an error message. The diff shows a change in the validation error message from ""pattern is missing"" to ""index patterns are missing"". This aligns with the commit message's intention to correct the error message. The change directly addresses a user-facing message, indicating a bug fix related to clarity and correctness of the error reporting.

**Buggy**"
elasticsearch,53233.json,a8bfa466b2f2a68c2384bc730a0b8c7c9ce7ea87,"@@ -1,12 +1,12 @@
     public void markAsCompleted(BulkItemResponse translatedResponse) {
         assertInvariants(ItemProcessingState.EXECUTED);
-        assert executionResult == null || translatedResponse.getItemId() == executionResult.getItemId();
+        assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();
         assert translatedResponse.getItemId() == getCurrentItem().id();
 
-        if (translatedResponse.isFailed() == false && requestToExecute != getCurrent())  {
+        if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent())  {
             request.items()[currentIndex] = new BulkItemRequest(request.items()[currentIndex].id(), requestToExecute);
         }
         getCurrentItem().setPrimaryResponse(translatedResponse);
         currentItemState = ItemProcessingState.COMPLETED;
         advance();
     }",Buggy,"Fix NOOP bulk updates (#32819)

#31821 introduced an unreleased bug where NOOP updates were incorrectly mutating the bulk
shard request, inserting null item to be replicated, which would result in NullPointerExceptions when
serializing the request to be shipped to the replicas.

Closes #32808",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix NOOP bulk updates (#32819)"" clearly states that the commit is intended to fix a bug. It references issue #32819 and closes issue #32808, further suggesting a bug fix. The message describes a specific scenario where NOOP updates were incorrectly mutating the bulk shard request, leading to NullPointerExceptions during serialization. This strongly indicates a bug fix.

**Git Diff Analysis:**

The diff focuses on the `markAsCompleted` method. Let's break down the changes:

1.  **Assertion Change:**
    -   Original: `assert executionResult == null || translatedResponse.getItemId() == executionResult.getItemId();`
    -   New: `assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();`
    -   This change modifies the assertion to explicitly check that `executionResult` is not null. This suggests that the original assertion was too lenient and allowed a null `executionResult` in some cases, which could have led to unexpected behavior or errors.

2.  **Conditional Change:**
    -   Original: `if (translatedResponse.isFailed() == false && requestToExecute != getCurrent())  {`
    -   New: `if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent())  {`
    -   This change adds a null check for `requestToExecute` before comparing it to `getCurrent()`. This is a crucial addition because, without this check, `requestToExecute` could be null, leading to a `NullPointerException` when dereferencing it in the comparison.

**Reasoning:**

The commit message describes a bug related to NOOP updates causing NullPointerExceptions. The diff directly addresses this by adding a null check for `requestToExecute` and modifying an assertion to prevent null `executionResult`. The added null check prevents a potential `NullPointerException` when `requestToExecute` is null, which aligns perfectly with the bug described in the commit message. The change in assertion also indicates a stricter validation of the execution result, which could prevent further unexpected behavior.

**Conclusion:**

The commit message and the code changes are highly relevant. The changes directly address the bug described in the commit message by adding a null check and modifying an assertion. This prevents a potential `NullPointerException` and improves the overall robustness of the code. Therefore, the changes indicate a bug fix.

**Buggy**
"
elasticsearch,265.json,c9dc55c3322b631f708867fdfd650e5ca27a30d0,"@@ -1,18 +1,20 @@
     static Tuple<Set<String>, Set<String>> findTasksWithoutConfig(ClusterState state, String transformId) {
         PersistentTasksCustomMetaData tasks = state.metaData().custom(PersistentTasksCustomMetaData.TYPE);
 
         Set<String> taskIds = new HashSet<>();
         Set<String> executorNodes = new HashSet<>();
 
-        Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
-            TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
-            return Regex.simpleMatch(transformId, transformParams.getId());
-        };
+        if (tasks != null) {
+            Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
+                TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
+                return Regex.simpleMatch(transformId, transformParams.getId());
+            };
 
-        for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
-            executorNodes.add(pTask.getExecutorNode());
-            taskIds.add(pTask.getId());
+            for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
+                executorNodes.add(pTask.getExecutorNode());
+                taskIds.add(pTask.getId());
+            }
         }
 
         return new Tuple<>(taskIds, executorNodes);
     }",Buggy,"check custom meta data to avoid NPE (#51163)

check custom meta data to avoid NPE, fixes a problem introduced in #51072

fixes #51153",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) related to custom metadata. The code diff adds a null check for `tasks` before proceeding with further operations. This prevents a potential NPE if `state.metaData().custom(PersistentTasksCustomMetaData.TYPE)` returns null. The commit message also mentions that the problem was introduced in a previous commit (#51072) and fixes issue #51153. This strongly suggests that the change is a bug fix.

**Buggy**"
elasticsearch,7551.json,341006e9913e831408f5bbc7f8ad8c453a7f630e,"@@ -1,91 +1,91 @@
     private static List<EsIndex> buildIndices(String[] indexNames, String javaRegex, Map<String, Map<String, FieldCapabilities>> fieldCaps,
             Function<String, String> indexNameProcessor,
             BiFunction<String, Map<String, FieldCapabilities>, InvalidMappedField> validityVerifier) {
 
         if (indexNames == null || indexNames.length == 0) {
             return emptyList();
         }
 
         final List<String> resolvedIndices = asList(indexNames);
         Map<String, Fields> indices = new LinkedHashMap<>(resolvedIndices.size());
         Pattern pattern = javaRegex != null ? Pattern.compile(javaRegex) : null;
 
         // sort fields in reverse order to build the field hierarchy
         Set<Entry<String, Map<String, FieldCapabilities>>> sortedFields = new TreeSet<>(
                 Collections.reverseOrder(Comparator.comparing(Entry::getKey)));
 
         sortedFields.addAll(fieldCaps.entrySet());
 
         for (Entry<String, Map<String, FieldCapabilities>> entry : sortedFields) {
             String fieldName = entry.getKey();
             Map<String, FieldCapabilities> types = entry.getValue();
 
             // ignore size added by the mapper plugin
             if (FIELD_NAMES_BLACKLIST.contains(fieldName)) {
                 continue;
             }
 
             // apply verification
             final InvalidMappedField invalidField = validityVerifier.apply(fieldName, types);
 
             // filter meta fields and unmapped
             FieldCapabilities unmapped = types.get(UNMAPPED);
             Set<String> unmappedIndices = unmapped != null ? new HashSet<>(asList(unmapped.indices())) : emptySet();
 
             // check each type
             for (Entry<String, FieldCapabilities> typeEntry : types.entrySet()) {
                 FieldCapabilities typeCap = typeEntry.getValue();
                 String[] capIndices = typeCap.indices();
 
                 // Skip internal fields (name starting with underscore and its type reported by field_caps starts
                 // with underscore as well). A meta field named ""_version"", for example, has the type named ""_version"".
                 if (typeEntry.getKey().startsWith(""_"") && typeCap.getType().startsWith(""_"")) {
                     continue;
                 }
 
                 // compute the actual indices - if any are specified, take into account the unmapped indices
                 List<String> concreteIndices = null;
                 if (capIndices != null) {
                     if (unmappedIndices.isEmpty() == true) {
                         concreteIndices = asList(capIndices);
                     } else {
-                        concreteIndices = new ArrayList<>(capIndices.length - unmappedIndices.size() + 1);
+                        concreteIndices = new ArrayList<>(capIndices.length);
                         for (String capIndex : capIndices) {
                             // add only indices that have a mapping
                             if (unmappedIndices.contains(capIndex) == false) {
                                 concreteIndices.add(capIndex);
                             }
                         }
                     }
                 } else {
                     concreteIndices = resolvedIndices;
                 }
 
                 // put the field in their respective mappings
                 for (String index : concreteIndices) {
                     if (pattern == null || pattern.matcher(index).matches()) {
                         String indexName = indexNameProcessor.apply(index);
                         Fields indexFields = indices.get(indexName);
                         if (indexFields == null) {
                             indexFields = new Fields();
                             indices.put(indexName, indexFields);
                         }
                         EsField field = indexFields.flattedMapping.get(fieldName);
                         if (field == null || (invalidField != null && (field instanceof InvalidMappedField) == false)) {
                             createField(fieldName, fieldCaps, indexFields.hierarchicalMapping, indexFields.flattedMapping,
                                     s -> invalidField != null ? invalidField : createField(s, typeCap.getType(), emptyMap(),
                                             typeCap.isAggregatable()));
                         }
                     }
                 }
             }
         }
 
         // return indices in ascending order
         List<EsIndex> foundIndices = new ArrayList<>(indices.size());
         for (Entry<String, Fields> entry : indices.entrySet()) {
             foundIndices.add(new EsIndex(entry.getKey(), entry.getValue().hierarchicalMapping));
         }
         foundIndices.sort(Comparator.comparing(EsIndex::name));
         return foundIndices;
     }",Buggy,"SQL: concrete indices array size bug fix (#43878)

* The created array didn't have the correct initial size while attempting to resolve multiple indices
",Buggy,"The commit message indicates a bug fix related to the size of an array used to resolve multiple indices in SQL queries.

The code diff focuses on the `buildIndices` method, specifically the section where `concreteIndices` is initialized. The original code `concreteIndices = new ArrayList<>(capIndices.length - unmappedIndices.size() + 1);` calculates the initial capacity of the `ArrayList` by subtracting the number of unmapped indices from the total number of indices and adding 1. This calculation could lead to an incorrect initial capacity, potentially causing performance issues or even errors if the estimated size is too small. The corrected code `concreteIndices = new ArrayList<>(capIndices.length);` initializes the `ArrayList` with the total number of indices, which is a more appropriate and safer approach.

The change directly addresses the issue described in the commit message, indicating a bug fix.

**Buggy**"
elasticsearch,41046.json,0354825914ba79d7e3ec500aa9eab0532e43850f,"@@ -1,24 +1,22 @@
     private FieldLookup loadFieldData(String name) {
         FieldLookup data = cachedFieldData.get(name);
         if (data == null) {
             FieldMapper mapper = mapperService.smartNameFieldMapper(name, types);
             if (mapper == null) {
                 throw new ElasticSearchIllegalArgumentException(""No field found for ["" + name + ""] in mapping with types "" + Arrays.toString(types) + """");
             }
             data = new FieldLookup(mapper);
             cachedFieldData.put(name, data);
         }
         if (data.doc() == null) {
             fieldVisitor.name(data.mapper().names().indexName());
             try {
                 reader.document(docId, fieldVisitor);
                 // LUCENE 4 UPGRADE: Only one field we don't need document
                 data.doc(fieldVisitor.createDocument());
             } catch (IOException e) {
                 throw new ElasticSearchParseException(""failed to load field ["" + name + ""]"", e);
-            } finally {
-                fieldVisitor.reset();
             }
         }
         return data;
     }",Buggy,"lucene 4: Fixed compile error
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""lucene 4: Fixed compile error"" clearly states that the purpose of the commit is to resolve a compilation error. This suggests that the previous code had a syntax or other compile-time issue that prevented the code from being built successfully.

**Git Diff Analysis:**

The diff shows a modification within the `loadFieldData` method. Specifically, the `finally` block containing `fieldVisitor.reset();` has been removed from the `try-catch` block.

**Reasoning:**

1.  **Compile Error Context:** Compile errors often arise from syntax mistakes, missing imports, incorrect type usage, or unhandled exceptions. The removal of the `finally` block doesn't immediately suggest a syntax error fix. However, it could be related to resource management or exception handling that was causing a compile-time issue.

2.  **Impact of Removal:** The `finally` block ensures that `fieldVisitor.reset()` is always called, regardless of whether an `IOException` is thrown within the `try` block. Removing it means that `fieldVisitor.reset()` will not be called if an `IOException` occurs.

3.  **Potential Bug Fix:** The removal of the `finally` block could be a bug fix if the `fieldVisitor.reset()` call was causing a problem during compilation or under certain error conditions. For instance, if `fieldVisitor.reset()` was throwing an exception itself, and that exception was not being handled, it could lead to a compile-time error or unexpected behavior. Alternatively, the `reset` call might have been interfering with the exception handling, leading to incorrect error reporting or resource leaks.

4.  **Lucene 4 Upgrade:** The comment ""// LUCENE 4 UPGRADE: Only one field we don't need document"" suggests that this change is related to an upgrade to Lucene 4. It's possible that the `fieldVisitor`'s behavior changed in Lucene 4, making the `reset` call unnecessary or even harmful.

**Conclusion:**

Given the commit message explicitly stating a ""compile error"" fix and the code modification involving the removal of a `finally` block related to resource management and exception handling, it's highly likely that this commit addresses a bug. The removal of the `finally` block suggests that the previous code was either causing a compile-time error or exhibiting incorrect behavior under certain error conditions.

**Buggy**
"
elasticsearch,11747.json,22415fa2de1d7d07cea7dd5e7263eb1ed4270503,"@@ -1,82 +1,88 @@
     CharsetMatch findCharset(List<String> explanation, InputStream inputStream) throws Exception {
 
         // We need an input stream that supports mark and reset, so wrap the argument
         // in a BufferedInputStream if it doesn't already support this feature
         if (inputStream.markSupported() == false) {
             inputStream = new BufferedInputStream(inputStream, BUFFER_SIZE);
         }
 
         // This is from ICU4J
         CharsetDetector charsetDetector = new CharsetDetector().setText(inputStream);
         CharsetMatch[] charsetMatches = charsetDetector.detectAll();
 
         // Determine some extra characteristics of the input to compensate for some deficiencies of ICU4J
         boolean pureAscii = true;
         boolean containsZeroBytes = false;
         inputStream.mark(BUFFER_SIZE);
         byte[] workspace = new byte[BUFFER_SIZE];
         int remainingLength = BUFFER_SIZE;
         do {
             int bytesRead = inputStream.read(workspace, 0, remainingLength);
             if (bytesRead <= 0) {
                 break;
             }
             for (int i = 0; i < bytesRead && containsZeroBytes == false; ++i) {
                 if (workspace[i] == 0) {
                     containsZeroBytes = true;
                     pureAscii = false;
                 } else {
                     pureAscii = pureAscii && workspace[i] > 0 && workspace[i] < 128;
                 }
             }
             remainingLength -= bytesRead;
         } while (containsZeroBytes == false && remainingLength > 0);
         inputStream.reset();
 
         if (pureAscii) {
             // If the input is pure ASCII then many single byte character sets will match.  We want to favour
             // UTF-8 in this case, as it avoids putting a bold declaration of a dubious character set choice
             // in the config files.
             Optional<CharsetMatch> utf8CharsetMatch = Arrays.stream(charsetMatches)
                 .filter(charsetMatch -> StandardCharsets.UTF_8.name().equals(charsetMatch.getName())).findFirst();
             if (utf8CharsetMatch.isPresent()) {
                 explanation.add(""Using character encoding ["" + StandardCharsets.UTF_8.name() +
                     ""], which matched the input with ["" + utf8CharsetMatch.get().getConfidence() + ""%] confidence - first ["" +
                     (BUFFER_SIZE / 1024) + ""kB] of input was pure ASCII"");
                 return utf8CharsetMatch.get();
             }
         }
 
         // Input wasn't pure ASCII, so use the best matching character set that's supported by both Java and Go.
         // Additionally, if the input contains zero bytes then avoid single byte character sets, as ICU4J will
         // suggest these for binary files but then
         for (CharsetMatch charsetMatch : charsetMatches) {
             String name = charsetMatch.getName();
             if (Charset.isSupported(name) && FILEBEAT_SUPPORTED_ENCODINGS.contains(name.toLowerCase(Locale.ROOT))) {
 
                 // This extra test is to avoid trying to read binary files as text.  Running the log config
                 // deduction algorithms on binary files is very slow as the binary files generally appear to
                 // have very long lines.
                 boolean spaceEncodingContainsZeroByte = false;
-                byte[] spaceBytes = "" "".getBytes(name);
-                for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
-                    spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                Charset charset = Charset.forName(name);
+                // Some character sets cannot be encoded.  These are extremely rare so it's likely that
+                // they've been chosen based on incorrectly provided binary data.  Therefore, err on
+                // the side of rejecting binary data.
+                if (charset.canEncode()) {
+                    byte[] spaceBytes = "" "".getBytes(charset);
+                    for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
+                        spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                    }
                 }
                 if (containsZeroBytes && spaceEncodingContainsZeroByte == false) {
                     explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                         ""%] confidence but was rejected as the input contains zero bytes and the ["" + name + ""] encoding does not"");
                 } else {
                     explanation.add(""Using character encoding ["" + name + ""], which matched the input with ["" +
                         charsetMatch.getConfidence() + ""%] confidence"");
                     return charsetMatch;
                 }
             } else {
                 explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                     ""%] confidence but was rejected as it is not supported by ["" +
                     (Charset.isSupported(name) ? ""Filebeat"" : ""the JVM"") + ""]"");
             }
         }
 
         throw new IllegalArgumentException(""Could not determine a usable character encoding for the input"" +
             (containsZeroBytes ? "" - could it be binary data?"" : """"));
     }",Buggy,"[ML] Fix character set finder bug with unencodable charsets (#33234)

Some character sets cannot be encoded and this was tripping
up the binary data check in the ML log structure character
set finder.

The fix is to assume that if ICU4J identifies that some bytes
correspond to a character set that cannot be encoded and those
bytes contain zeroes then the data is binary rather than text.

Fixes #33227",Buggy,"The commit message indicates a fix for a bug related to character set detection, specifically when dealing with character sets that cannot be encoded. The code changes include:

1.  Adding a check `charset.canEncode()` to determine if a character set can be encoded.
2.  Modifying the logic to handle cases where a character set cannot be encoded. If a character set cannot be encoded and the input contains zero bytes, the code now assumes the data is binary and rejects the character set.

These changes directly address the bug described in the commit message by preventing the application from attempting to use unencodable character sets, especially when binary data is involved. The changes improve error handling and prevent incorrect character set detection.

**Buggy**"
elasticsearch,54093.json,b3341da0779673c106db526ec58c4449bda59dc2,"@@ -1,9 +1,13 @@
     private static boolean usingBundledJdk() {
         /*
          * We are using the bundled JDK if java.home is the jdk sub-directory of our working directory. This is because we always set
          * the working directory of Elasticsearch to home, and the bundled JDK is in the jdk sub-directory there.
          */
         final String javaHome = System.getProperty(""java.home"");
         final String userDir = System.getProperty(""user.dir"");
-        return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        if (Constants.MAC_OS_X) {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk/Contents/Home"").toAbsolutePath());
+        } else {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        }
     }",Buggy,"Fix bug in detecting use of bundled JDK on macOS

This commit fixes a bug in detecting the use of the bundled JDK on
macOS. This bug arose because the path of Java home is different on
macOS.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states ""Fix bug in detecting use of bundled JDK on macOS"". This strongly suggests a bug fix.
    *   It explains the bug's origin: an incorrect path for Java home on macOS.

2.  **Git Diff Analysis:**
    *   The code change modifies the `usingBundledJdk()` method. This method is responsible for determining if the application is using the bundled JDK.
    *   The core of the change introduces a conditional check based on `Constants.MAC_OS_X`. This indicates that the logic for determining the bundled JDK path is different on macOS compared to other operating systems.
    *   Specifically, on macOS, the code now resolves the path to `jdk/Contents/Home`, while on other systems, it resolves to just `jdk`. This suggests that the original code was not correctly identifying the Java home path on macOS, which aligns with the commit message.

3.  **Synthesis:**
    *   The commit message and the code changes are highly aligned. The commit message describes a bug related to JDK detection on macOS, and the code changes introduce a macOS-specific path resolution to address this issue. The code now correctly identifies the Java home path on macOS.

**Conclusion:**

**Buggy**
"
elasticsearch,12667.json,7ae57d6e226bfc314ce31acc1a622fb0d111fa46,"@@ -1,36 +1,47 @@
     void refresh(PersistentTasksCustomMetaData persistentTasks, ActionListener<Void> onCompletion) {
 
         synchronized (fullRefreshCompletionListeners) {
             fullRefreshCompletionListeners.add(onCompletion);
             if (fullRefreshCompletionListeners.size() > 1) {
                 // A refresh is already in progress, so don't do another
                 return;
             }
         }
 
         ActionListener<Void> refreshComplete = ActionListener.wrap(aVoid -> {
             lastUpdateTime = Instant.now();
             synchronized (fullRefreshCompletionListeners) {
                 assert fullRefreshCompletionListeners.isEmpty() == false;
                 for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
                     listener.onResponse(null);
                 }
                 fullRefreshCompletionListeners.clear();
             }
-        }, onCompletion::onFailure);
+        },
+        e -> {
+            synchronized (fullRefreshCompletionListeners) {
+                assert fullRefreshCompletionListeners.isEmpty() == false;
+                for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
+                    listener.onFailure(e);
+                }
+                // It's critical that we empty out the current listener list on
+                // error otherwise subsequent retries to refresh will be ignored
+                fullRefreshCompletionListeners.clear();
+            }
+        });
 
         // persistentTasks will be null if there's never been a persistent task created in this cluster
         if (persistentTasks == null) {
             refreshComplete.onResponse(null);
         } else {
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlDataFrameAnalyticsJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             ActionListener<Void> refreshDataFrameAnalyticsJobs =
                 ActionListener.wrap(aVoid -> refreshAllDataFrameAnalyticsJobTasks(mlDataFrameAnalyticsJobTasks, refreshComplete),
                     refreshComplete::onFailure);
 
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlAnomalyDetectorJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.JOB_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             iterateAnomalyDetectorJobTasks(mlAnomalyDetectorJobTasks.iterator(), refreshDataFrameAnalyticsJobs);
         }
     }",Buggy,"[ML] Fix ML memory tracker lockup when inner step fails (#44158)

When the ML memory tracker is refreshed and a refresh is
already in progress the idea is that the second and
subsequent refresh requests receive the same response as
the currently in progress refresh.

There was a bug that if a refresh failed then the ML
memory tracker's view of whether a refresh was in progress
was not reset, leading to every subsequent request being
registered to receive a response that would never come.

This change makes the ML memory tracker pass on failures
as well as successes to all interested parties and reset
the list of interested parties so that further refresh
attempts are possible after either a success or failure.

This fixes problem 1 of #44156",Buggy,"The commit message describes a bug where a failed refresh operation in the ML memory tracker would not reset the ""refresh in progress"" state, leading to subsequent refresh requests being ignored. The code diff modifies the `refresh` method to handle failures correctly. Specifically, the `onFailure` callback now iterates through the `fullRefreshCompletionListeners`, calls `onFailure` on each listener, and then clears the listener list. This ensures that subsequent refresh attempts are not blocked after a failure. The addition of error handling and the clearing of the listener list upon failure directly address the bug described in the commit message.

**Buggy**"
elasticsearch,39911.json,4735e0a9d3c7c89f34100e4fd47f3de25e56e1ed,"@@ -1,11 +1,14 @@
     public HistogramAggregationBuilder extendedBounds(double minBound, double maxBound) {
-        if (minBound == Double.NEGATIVE_INFINITY) {
-            throw new IllegalArgumentException(""minBound must not be -Infinity, got: "" + minBound);
+        if (Double.isFinite(minBound) == false) {
+            throw new IllegalArgumentException(""minBound must be finite, got: "" + minBound);
         }
-        if (maxBound == Double.POSITIVE_INFINITY) {
-            throw new IllegalArgumentException(""maxBound must not be +Infinity, got: "" + maxBound);
+        if (Double.isFinite(maxBound) == false) {
+            throw new IllegalArgumentException(""maxBound must be finite, got: "" + maxBound);
+        }
+        if (maxBound < minBound) {
+            throw new IllegalArgumentException(""maxBound ["" + maxBound + ""] must be greater than minBound ["" + minBound + ""]"");
         }
         this.minBound = minBound;
         this.maxBound = maxBound;
         return this;
     }",Buggy,"Throw exception when maxBounds greater than minBounds

The recent changes to the Histogram Aggregator introduced a bug where
an exception would not be thrown if the maxBound of the extended bounds
is less that the minBound. This change fixes that bug.

Closes #19833
",Buggy,"The commit message states that a bug was introduced where an exception was not thrown when `maxBound` was less than `minBound`. The diff shows that a check `if (maxBound < minBound)` was added, and if this condition is true, an `IllegalArgumentException` is thrown. The diff also changes the checks for `minBound` and `maxBound` from checking against `-Infinity` and `+Infinity` to checking if the values are finite. This change aligns with the commit message, indicating a bug fix related to incorrect bounds.

**Buggy**"
elasticsearch,45276.json,c5b6f52eccb2a663770e0eb9fee496432e1a8d5c,"@@ -1,21 +1,22 @@
-        private ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
             for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> entry : shards) {
                 if (entry.value.state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.key.getIndex());
+                    final String indexName = entry.key.getIndexName();
+                    List<ShardId> waitingShards = waitingIndicesMap.get(indexName);
                     if (waitingShards == null) {
                         waitingShards = new ArrayList<>();
-                        waitingIndicesMap.put(entry.key.getIndexName(), waitingShards);
+                        waitingIndicesMap.put(indexName, waitingShards);
                     }
                     waitingShards.add(entry.key);
                 }
             }
             if (waitingIndicesMap.isEmpty()) {
                 return ImmutableOpenMap.of();
             }
             ImmutableOpenMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableOpenMap.builder();
             for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
                 waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
             }
             return waitingIndicesBuilder.build();
         }",Buggy,"Fixes maintaining the shards a snapshot is waiting on (#24289)

There was a bug in the calculation of the shards that a snapshot must
wait on, due to their relocating or initializing, before the snapshot
can proceed safely to snapshot the shard data.  In this bug, an
incorrect key was used to look up the index of the waiting shards,
resulting in the fact that each index would have at most one shard in
the waiting state causing the snapshot to pause.  This could be
problematic if there are more than one shard in the relocating or
initializing state, which would result in a snapshot prematurely
starting because it thinks its only waiting on one relocating or
initializing shard (when in fact there could be more than one).  While
not a common case and likely rare in practice, it is still problematic.

This commit fixes the issue by ensuring the correct key is used to look
up the waiting indices map as it is being built up, so the list of
waiting shards for each index (those shards that are relocating or
initializing) are aggregated for a given index instead of overwritten.
",Buggy,"The commit message describes a bug where the wrong key was used to look up the index of waiting shards, leading to premature snapshot starts. The code diff shows a change in the key used to retrieve and store the list of waiting shards in the `waitingIndicesMap`. Specifically, `entry.key.getIndex()` was replaced with `entry.key.getIndexName()` when retrieving the list and `entry.key.getIndexName()` was used when inserting into the map. This aligns with the commit message's explanation of using the incorrect key. The change ensures that shards waiting due to relocation or initialization are correctly aggregated by index, preventing the snapshot from starting prematurely.

**Buggy**
"
elasticsearch,60460.json,953a99c75cde29a18db58abde3fdee720fcddc4f,"@@ -1,6 +1,6 @@
     public boolean isSame(StoreFileMetaData other) {
-        if (checksum != null && other.checksum != null) {
-            return checksum.equals(other.checksum);
+        if (checksum == null || other.checksum == null) {
+            return false;
         }
-        return length == other.length;
+        return length == other.length && checksum.equals(other.checksum);
     }",Buggy,"fix a bug in new checksum mechanism that caused for replicas not to retain the _checksums file. Also, now that checksums are widely used, consider files without checksums as ones that need to be recovered
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message explicitly states:

*   ""fix a bug in new checksum mechanism that caused for replicas not to retain the \_checksums file."" This clearly indicates a bug fix related to the checksum mechanism.
*   ""now that checksums are widely used, consider files without checksums as ones that need to be recovered"" This suggests a change in behavior to handle cases where checksums are missing, implying a potential issue or edge case that needed addressing.

**Diff Analysis:**

The diff modifies the `isSame` method of the `StoreFileMetaData` class. The original code was:

```java
if (checksum != null && other.checksum != null) {
    return checksum.equals(other.checksum);
}
return length == other.length;
```

The modified code is:

```java
if (checksum == null || other.checksum == null) {
    return false;
}
return length == other.length && checksum.equals(other.checksum);
```

Here's a breakdown of the changes and their implications:

1.  **Checksum Null Check:** The original code only compared checksums if both checksums were not null. If either was null, it would proceed to compare lengths. The modified code now returns `false` immediately if either checksum is null. This aligns with the commit message's intention to treat files without checksums as needing recovery. The original logic could have incorrectly identified files as the same if they had the same length but one was missing a checksum.

2.  **Combined Length and Checksum Comparison:** The modified code now requires both the length and checksum to be equal for the files to be considered the same. This is a more robust comparison, as it ensures that files are truly identical before being considered the same.

**Reasoning:**

The changes in the diff directly address the issues described in the commit message. The original code had a flaw in its checksum comparison logic, potentially leading to replicas not retaining the `_checksums` file. The modified code corrects this by:

*   Explicitly handling cases where checksums are null, ensuring that files without checksums are not incorrectly considered the same as files with checksums.
*   Enforcing a stricter comparison that requires both length and checksum to match for files to be considered the same.

These changes clearly indicate a bug fix related to the checksum mechanism.

**Conclusion:**

**Buggy**
"
elasticsearch,52327.json,fe5af6f34d1204b6238c46c3aff7ea2a056eeb7a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""]"";
+        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""], cause ["" + (cause == null ? ""_na"" : ExceptionsHelper.stackTrace(cause)) + ""]"";
     }",Buggy,"Tests: fix test bug (foo -> bar)
",Buggy,"The commit message ""Tests: fix test bug (foo -> bar)"" indicates a bug fix related to tests, specifically mentioning a change from ""foo"" to ""bar"".

The provided code diff modifies the `toString()` method of a class. It adds the `cause` to the string representation, including the stack trace if the cause is not null. This change doesn't seem directly related to a test bug fix involving a simple replacement of ""foo"" with ""bar"". The diff introduces a new element (`cause`) into the string representation, which is more of an enhancement or a change in how the object is represented. It's possible that the original `toString()` method was inadequate for debugging purposes in tests, and this change improves testability, but without more context, it's hard to confirm. The commit message is also vague.

Given the limited information and the mismatch between the commit message's description of a simple replacement and the code diff's enhancement of the `toString()` method, it's difficult to definitively say this is a bug fix. However, the change improves the information provided by the `toString` method, which is useful for debugging, and the commit message mentions ""fix test bug"". Therefore, it is possible that the improved `toString` method helps to fix a bug in the tests.

**Buggy**"
elasticsearch,15357.json,368e5a1194c2d2f762c117dd84397bb65f835dec,"@@ -1,5 +1,6 @@
     public static boolean validate(ClusterState state) {
-        return state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) &&
+        return (state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) ||
+            state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME_NO_ILM)) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.TRIGGERED_TEMPLATE_NAME) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.WATCHES_TEMPLATE_NAME);
     }",Buggy,"fix unlikely bug that can prevent Watcher from restarting (#42030)

The bug fixed here is unlikely to happen. It requires ES to be started with
ILM disabled, Watcher enabled, and Watcher explicitly stopped and restarted.
Due to template validation Watcher does not fully start and can result in a
partially started state. This is an unlikely scenerio outside of the testing
framework.

Note - this bug was introduced while the test that would have caught it was
muted. The test remains muted since the underlying cuase of the random failures
has not been identified. When this test is un-muted it will now work.",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

*   **Explicit Bug Fix:** The commit message clearly states ""fix unlikely bug"".
*   **Specific Scenario:** It describes a specific, albeit unlikely, scenario where the bug occurs: ILM disabled, Watcher enabled, Watcher stopped and restarted.
*   **Root Cause:** The bug is related to template validation during Watcher startup, leading to a partially started state.
*   **Testing Context:** The commit message mentions a muted test that would have caught the bug, indicating that the bug was present before but not detected due to the test being disabled.

**Git Diff Analysis:**

*   **Template Validation Logic:** The diff modifies the `validate` method, which appears to be responsible for validating the cluster state based on the presence of certain index templates.
*   **Conditional Check:** The original code checked for the existence of `WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME`. The modified code adds an alternative check for `WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME_NO_ILM` using an `||` (OR) operator.
*   **ILM Consideration:** The addition of `HISTORY_TEMPLATE_NAME_NO_ILM` suggests that the original validation logic was failing when ILM was disabled, as the history template name might be different in that case.

**Reasoning:**

1.  **Alignment:** The commit message describes a bug related to template validation when ILM is disabled. The code change directly addresses this by adding a check for a different history template name (`HISTORY_TEMPLATE_NAME_NO_ILM`) when ILM is not enabled. This strongly suggests that the code change is intended to fix the described bug.
2.  **Error Handling/Logic Correction:** The original code was likely too strict in its template validation, assuming that `HISTORY_TEMPLATE_NAME` would always be present. The change relaxes this constraint by allowing `HISTORY_TEMPLATE_NAME_NO_ILM` as an alternative, which is a logical correction to handle the case where ILM is disabled.
3.  **Bug Fix Indication:** The commit message explicitly states that it's a bug fix. The code changes align with the description of the bug and the scenario in which it occurs.

**Conclusion:**

**Buggy**
"
elasticsearch,53154.json,b7314c87211d99c45569ae0cdd8797636220b980,"@@ -1,11 +1,11 @@
     public String toString() {
         StringBuilder builder = new StringBuilder();
         builder.append(""IndexResponse["");
         builder.append(""index="").append(getIndex());
         builder.append("",type="").append(getType());
         builder.append("",id="").append(getId());
         builder.append("",version="").append(getVersion());
         builder.append("",result="").append(getResult().getLowercase());
-        builder.append("",shards="").append(getShardInfo());
+        builder.append("",shards="").append(Strings.toString(getShardInfo(), true));
         return builder.append(""]"").toString();
     }",Buggy,"fix IndexResponse#toString to print out shards info (#20562)

IndexResponse#toString method outputs an error caused by the shards object needing to be wrapped into another object. It is fixed by calling a different variant of Strings.toString(XContent) which accepts a second boolean argument that makes sure that a new object is created before outputting ShardInfo. I didn't change ShardInfo#toString directly as whether it needs a new object or not very much depends on where it is printed out. IndexResponse seemed a specific case as the rest of the info were not json, hence the shards object was the first one, but it is usually not the case.",Buggy,"The commit message indicates that the `IndexResponse#toString` method was producing an error when printing shard information. The provided diff shows a change in how the `ShardInfo` object is converted to a string within the `IndexResponse#toString` method. Specifically, it uses `Strings.toString(getShardInfo(), true)` instead of `getShardInfo()`.

The commit message explains that the `shards` object needed to be wrapped in another object before outputting. The change in the code uses a variant of `Strings.toString` that accepts a boolean argument, which likely controls whether a new object is created during the string conversion. This suggests that the original implementation was not correctly handling the serialization of the `ShardInfo` object in the context of the `IndexResponse#toString` method. The explanation in the commit message and the code change clearly indicate a fix for an error in the original implementation.

Therefore, the changes indicate a bug fix.
**Buggy**
"
elasticsearch,45221.json,5c59b1206762df8461b7f827ce3922231250c49e,"@@ -1,33 +1,35 @@
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVInt(deletes.size());
             for (K delete : deletes) {
                 keySerializer.writeKey(delete, out);
             }
             Version version = out.getVersion();
             // filter out custom states not supported by the other node
             int diffCount = 0;
             for (Diff<T> diff : diffs.values()) {
                 if(valueSerializer.supportsVersion(diff, version)) {
                     diffCount++;
                 }
             }
             out.writeVInt(diffCount);
             for (Map.Entry<K, Diff<T>> entry : diffs.entrySet()) {
                 if(valueSerializer.supportsVersion(entry.getValue(), version)) {
                     keySerializer.writeKey(entry.getKey(), out);
                     valueSerializer.writeDiff(entry.getValue(), out);
                 }
             }
             // filter out custom states not supported by the other node
             int upsertsCount = 0;
             for (T upsert : upserts.values()) {
                 if(valueSerializer.supportsVersion(upsert, version)) {
                     upsertsCount++;
                 }
             }
             out.writeVInt(upsertsCount);
             for (Map.Entry<K, T> entry : upserts.entrySet()) {
-                keySerializer.writeKey(entry.getKey(), out);
-                valueSerializer.write(entry.getValue(), out);
+                if(valueSerializer.supportsVersion(entry.getValue(), version)) {
+                    keySerializer.writeKey(entry.getKey(), out);
+                    valueSerializer.write(entry.getValue(), out);
+                }
             }
         }",Buggy,"Fixing the custom object serialization bug in diffable utils. (#39544)

While serializing custom objects, the length of the list is computed after
filtering out the unsupported objects but while writing objects the filter
is not applied thus resulting in writing unsupported objects which will fail
to deserialize by the receiever. Adding the condition to filter out unsupported
custom objects.",Buggy,"The commit message indicates a bug fix related to custom object serialization in diffable utils. Specifically, it mentions that unsupported objects were being written during serialization, leading to deserialization failures on the receiving end. The commit message states that the length of the list is computed after filtering out the unsupported objects but while writing objects the filter is not applied.

The code diff shows that a condition `valueSerializer.supportsVersion(entry.getValue(), version)` has been added within the loop that iterates through `upserts.entrySet()`. This condition ensures that only supported custom objects are written to the output stream. This aligns with the commit message's description of filtering out unsupported custom objects during serialization.

The addition of the `if` condition directly addresses the bug described in the commit message. It prevents unsupported objects from being written, thus resolving the deserialization issue.

Therefore, the changes indicate a bug fix.
**Buggy**"
elasticsearch,56477.json,1792bd6b160993aae2d925ad4b7c0663d14a9e82,"@@ -1,14 +1,6 @@
     public static IndexShardState fromId(byte id) throws ElasticSearchIllegalArgumentException {
-        if (id == 0) {
-            return CREATED;
-        } else if (id == 1) {
-            return RECOVERING;
-        } else if (id == 2) {
-            return STARTED;
-        } else if (id == 3) {
-            return RELOCATED;
-        } else if (id == 4) {
-            return CLOSED;
+        if (id < ORDS[0].id && id > ORDS[ORDS.length - 1].id) {
+            throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
         }
-        throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
+        return ORDS[id];
     }",Buggy,"Fixed serialization error. POST_RECOVERY is now also serialized
",Buggy,"The commit message indicates a fix related to serialization, specifically mentioning `POST_RECOVERY`. However, the provided code diff doesn't seem to directly involve serialization or the `POST_RECOVERY` state. The code modifies the `fromId` method, which retrieves an `IndexShardState` based on an ID. The original code used a series of `if-else` statements to map IDs to states. The modified code replaces this with a check against an `ORDS` array and throws an exception if the ID is out of bounds. If the ID is valid, it retrieves the state from the `ORDS` array.

The change improves the code's structure and potentially its performance by replacing the `if-else` chain with an array lookup. The exception handling is also improved by throwing an exception when the ID is out of range. However, without more context about the `ORDS` array and the serialization process, it's difficult to definitively say if this change is directly related to fixing a serialization error for `POST_RECOVERY`.

The commit message mentions a ""serialization error"" and the code change improves the way `IndexShardState` is retrieved from an ID. It's possible that the previous implementation had issues during serialization/deserialization of `IndexShardState` objects, especially if the IDs were not handled correctly. The change ensures that only valid IDs are used, which could prevent errors during serialization.

Given the commit message and the code change, it's reasonable to assume that the change is related to fixing a bug, even if the connection is not immediately obvious from the diff alone.

**Buggy**"
checkstyle,1499.json,ca8fcd42047b2637e0027ff6aec79edd3656ba56,"@@ -1,3 +1,4 @@
-    public void setIgnoreEqualsIgnoreCase(boolean newValue) {
-        mIgnoreEqualsIgnoreCase = newValue;
+    public void setIgnoreEqualsIgnoreCase(boolean aNewValue)
+    {
+        mIgnoreEqualsIgnoreCase = aNewValue;
     }",Buggy,"fix up checkstyle errors.
",NotBuggy,"The commit message ""fix up checkstyle errors"" indicates a change to address code style issues, which is often related to formatting, naming conventions, or other stylistic guidelines enforced by Checkstyle.

The provided diff shows a change from `newValue` to `aNewValue` in the `setIgnoreEqualsIgnoreCase` method. This change likely addresses a naming convention rule enforced by Checkstyle, such as a requirement for parameter names to start with ""a"" or ""an"" when they represent a new value.

This change doesn't seem to be related to any bug fix, error handling, or logical correction. It's purely a stylistic change to comply with Checkstyle rules.

**NotBuggy**"
checkstyle,1058.json,9d41bddb46e5fbe291be4cdb347dee18ea4424fe,"@@ -1,4 +1,8 @@
     public int[] getRequiredTokens()
     {
-        return getDefaultTokens();
+        return new int[] {
+            TokenTypes.CTOR_DEF,
+            TokenTypes.METHOD_DEF,
+            TokenTypes.EXPR,
+        };
     }",Buggy,"Fixed bug #1579227: Fixed docs, added unit tests and changed the check implementation to allow turning off the | and & operators
",Buggy,"The commit message indicates a bug fix related to allowing turning off the '|' and '&' operators, along with documentation updates and added unit tests. The provided code diff changes the required tokens for a certain functionality.

The original code `return getDefaultTokens();` likely returned a default set of tokens that were always required. The modified code `return new int[] {TokenTypes.CTOR_DEF, TokenTypes.METHOD_DEF, TokenTypes.EXPR,};` now returns a specific set of tokens: `CTOR_DEF`, `METHOD_DEF`, and `EXPR`.

It's difficult to determine if this change is directly related to the bug fix mentioned in the commit message (""allowing turning off the | and & operators"") without more context. However, the commit message mentions a bug fix, and the code is being modified. The change in required tokens could be part of the fix to allow turning off certain operators. It's possible that the previous implementation was too restrictive and required tokens related to the operators that are now optional.

Given the commit message explicitly stating a bug fix and the code modification, it's likely this change is related to addressing the bug.

**Buggy**"
lombok,2739.json,aa40323ae8dde889f4bcf21304e5cffdc0003b4c,"@@ -1,21 +1,26 @@
 	public static TypeParameter[] copyTypeParams(TypeParameter[] params) {
 		if ( params == null ) return null;
 		TypeParameter[] out = new TypeParameter[params.length];
 		int idx = 0;
 		for ( TypeParameter param : params ) {
 			TypeParameter o = new TypeParameter();
 			o.annotations = param.annotations;
 			o.bits = param.bits;
 			o.modifiers = param.modifiers;
 			o.name = param.name;
 			o.type = copyType(param.type);
+			o.sourceStart = param.sourceStart;
+			o.sourceEnd = param.sourceEnd;
+			o.declarationEnd = param.declarationEnd;
+			o.declarationSourceStart = param.declarationSourceStart;
+			o.declarationSourceEnd = param.declarationSourceEnd;
 			if ( param.bounds != null ) {
 				TypeReference[] b = new TypeReference[param.bounds.length];
 				int idx2 = 0;
 				for ( TypeReference ref : param.bounds ) b[idx2++] = copyType(ref);
 				o.bounds = b;
 			}
 			out[idx++] = o;
 		}
 		return out;
 	}",Buggy,"Fixed a problem where @Data with a static constructor and generics params on the class would generate errors regarding IllegalArgumentException in setSourcePosition in ASTNode.
",Buggy,"The commit message indicates a fix for an issue where using `@Data` with a static constructor and generic parameters on the class would lead to `IllegalArgumentException` related to `setSourcePosition` in `ASTNode`.

The provided diff focuses on the `copyTypeParams` method, which seems to be involved in copying type parameters. The changes introduce copying several source position-related fields from the original `TypeParameter` to the copied `TypeParameter`. These fields include `sourceStart`, `sourceEnd`, `declarationEnd`, `declarationSourceStart`, and `declarationSourceEnd`.

The connection to the commit message is that the `IllegalArgumentException` likely arose because the source position information was not being copied, leading to incorrect or missing source position data when the copied type parameters were used in AST nodes. By copying these fields, the code ensures that the copied type parameters have the correct source position information, resolving the `IllegalArgumentException`.

Therefore, the changes in the diff directly address the problem described in the commit message.

**Buggy**
"
lombok,3095.json,6fce13a8db45629a76e31c2a1c676ab9dac021dc,"@@ -1,7 +1,7 @@
-	private static int getReplacementOffset(IJavaCompletionProposal proposal) {
+	private static int getReplacementOffset(Object proposal) {
 		try {
 			return Reflection.replacementOffsetField.getInt(proposal);
 		} catch (Exception ignore) {
 			return 0;
 		}
 	}",Buggy,"Somehow in eclipse mars wanted us to pull in more ecj deps, fixed that problem with a trivial code tweak.
",Buggy,"The commit message indicates a fix for an issue where Eclipse Mars was unexpectedly pulling in more ECJ dependencies. The code change modifies the `getReplacementOffset` method signature from accepting `IJavaCompletionProposal` to accepting a generic `Object`. This change, along with the try-catch block, suggests a workaround to avoid a specific type dependency issue, which aligns with the commit message's description of a ""trivial code tweak"" to address the ECJ dependency problem. The use of reflection and the broad `Exception` catch suggest a defensive approach to handle potential type mismatches or missing fields, indicating a bug fix or workaround.

**Buggy**"
lombok,1236.json,e4b61e1263eb0eb832eb6cfbd97ad92e869ca27e,"@@ -1,11 +1,11 @@
 	public static boolean typeMatches(String type, JavacNode node, JCTree typeNode) {
 		String typeName = typeNode == null ? null : typeNode.toString();
 		if (typeName == null || typeName.length() == 0) return false;
 		int lastIndexA = typeName.lastIndexOf('.') + 1;
-		int lastIndexB = type.lastIndexOf('.') + 1;
+		int lastIndexB = Math.max(type.lastIndexOf('.'), type.lastIndexOf('$')) + 1;
 		int len = typeName.length() - lastIndexA;
 		if (len != type.length() - lastIndexB) return false;
 		for (int i = 0; i < len; i++) if (typeName.charAt(i + lastIndexA) != type.charAt(i + lastIndexB)) return false;
 		TypeResolver resolver = node.getImportListAsTypeResolver();
 		return resolver.typeMatches(node, type, typeName);
 	}",Buggy,[performance] fixing bug in earlier performance commit on optimizing lookups of typeMatches.,Buggy,"The commit message indicates a bug fix related to performance optimization in type matching. The code change modifies the `lastIndexB` calculation, which is used to determine the starting index for comparing type names. The original code only considered the last index of a dot (`.`), while the corrected code also considers the last index of a dollar sign (`$`). This suggests that the previous implementation was not correctly handling nested classes (inner classes), which are often represented with a dollar sign in their fully qualified names. The bug fix ensures that the type matching logic correctly handles nested classes, which aligns with the commit message's description of fixing a bug in type matching.

**Buggy**"
lombok,314.json,9ac86c8a234f2ecd280f27f0dbda15440b1bf155,"@@ -1,9 +1,12 @@
 	public static LombokOptions getDelombokOptions(Context context) {
+		Options rawOptions = Options.instance(context);
+		if (rawOptions instanceof LombokOptions) return (LombokOptions) rawOptions;
+		
 		LombokOptions options;
 		if (Javac.getJavaCompilerVersion() < 8) {
 			options = LombokOptionCompilerVersion.JDK7_AND_LOWER.createAndRegisterOptions(context);
 		} else {
 			options = LombokOptionCompilerVersion.JDK8.createAndRegisterOptions(context);
 		}
 		return options;
 	}",Buggy,"fixed a bug where called LombokOptionsFactory.getLombokOptions() would actually cause the old options to be wrapped continuously, thus wrapping LombokOptions into itself infinitely. Now it just wraps if needed, other wise returns what's already been done.
",Buggy,"The commit message indicates a bug fix related to `LombokOptionsFactory.getLombokOptions()`, where the old options were being wrapped continuously, leading to infinite self-wrapping. The provided code diff shows a modification in `getDelombokOptions()` method. It now checks if the existing `rawOptions` is already an instance of `LombokOptions`. If it is, it directly returns the existing instance, preventing the redundant wrapping. This aligns perfectly with the commit message's description of the bug and its fix.

Reasoning:

1.  **Bug Description Alignment:** The commit message describes an infinite wrapping issue. The code diff introduces a check to prevent redundant wrapping of `LombokOptions`, directly addressing the described bug.
2.  **Logical Correction:** The original code, as implied by the fix, was unconditionally creating new `LombokOptions`, leading to the wrapping problem. The modified code adds a condition to reuse existing `LombokOptions` instances, correcting the flawed logic.
3.  **Error Prevention:** The change prevents a potential stack overflow or performance issue caused by the infinite wrapping, indicating a bug fix.

**Buggy**
"
hibernate-search,6397.json,58ae33f9813b56aa60ffb2d301bc1aca8f023f6c,"@@ -1,27 +1,41 @@
 	private void indexAllQueue(Session session) {
 		final InstanceInitializer sessionInitializer = new HibernateSessionLoadingInitializer(
-				(SessionImplementor) session );
+				(SessionImplementor) session
+		);
 		try {
 			ConversionContext contextualBridge = new ContextualExceptionBridgeHelper();
 			while ( true ) {
 				List<?> takeList = source.take();
 				if ( takeList == null ) {
 					break;
 				}
 				else {
 					log.tracef( ""received a list of objects to index: %s"", takeList );
-					for ( Object take : takeList ) {
+					for ( Object object : takeList ) {
 						//trick to attach the objects to session:
-						session.buildLockRequest( LockOptions.NONE ).lock( take );
-						index( take, session, sessionInitializer, contextualBridge );
-						monitor.documentsBuilt( 1 );
+						session.buildLockRequest( LockOptions.NONE ).lock( object );
+						try {
+							index( object, session, sessionInitializer, contextualBridge );
+							monitor.documentsBuilt( 1 );
+						}
+						catch (InterruptedException ie) {
+							// rethrowing the interrupted exception
+							throw ie;
+						}
+						catch (RuntimeException e) {
+							String errorMsg = log.massIndexerUnableToIndexInstance(
+									object.getClass().getName(),
+									object.toString()
+							);
+							errorHandler.handleException( errorMsg, e );
+						}
 						session.clear();
 					}
 				}
 			}
 		}
 		catch (InterruptedException e) {
 			// just quit
 			Thread.currentThread().interrupt();
 		}
 	}",Buggy,"HSEARCH-1354 Fixing error handling during indexing for mass indexer and adding test case
",Buggy,"The commit message indicates a fix to error handling during indexing for the mass indexer and the addition of a test case.

The code diff shows that a try-catch block has been added within the loop that iterates through the objects to be indexed. This try-catch block specifically catches `InterruptedException` and `RuntimeException`.

- `InterruptedException` is re-thrown, which is a good practice to ensure that the thread's interrupted status is preserved.
- `RuntimeException` is caught, and an error message is logged using `log.massIndexerUnableToIndexInstance`. The `errorHandler.handleException` is then called to handle the exception. This suggests that previously, exceptions during indexing might have been unhandled, potentially causing the mass indexer to fail silently or stop prematurely.

The addition of this error handling mechanism aligns with the commit message's claim of fixing error handling during indexing. The logging and error handling indicate that the code is now more robust and can handle exceptions that occur during the indexing of individual objects. This is a strong indicator of a bug fix, as it addresses a potential issue where errors were not being properly handled.

Therefore, the changes indicate a bug fix.

**Buggy**
"
hibernate-search,1888.json,835d01200cf073c05220f435d7ec47dc2593dddb,"@@ -1,13 +1,13 @@
-	public Query filterOrPassthrough(Query filteredQuery) {
+	public Query filterOrPassthrough(Query queryToFilter) {
 		if ( isEmpty() ) {
-			return filteredQuery;
+			return queryToFilter;
 		}
 		else {
 			BooleanQuery.Builder boolQueryBuilder = new BooleanQuery.Builder();
-			boolQueryBuilder.add( filteredQuery, Occur.MUST );
+			boolQueryBuilder.add( queryToFilter, Occur.MUST );
 			for ( Query bc : filterQueries ) {
 				boolQueryBuilder.add( bc, BooleanClause.Occur.FILTER );
 			}
 			return boolQueryBuilder.build();
 		}
 	}",Buggy,"HSEARCH-2698 Fix incomplete javadoc for some methods

This does not address all the problems, I intentionally didn't address
the more controversial ones (like the useless @return in fluent APIs).
",Buggy,"The commit message indicates a fix for incomplete Javadoc in some methods, but the provided diff doesn't show any Javadoc changes. Instead, it shows a change in a parameter name (`filteredQuery` to `queryToFilter`) and a corresponding variable name update within the method. While this change might improve code readability, it doesn't directly relate to Javadoc. Therefore, the diff doesn't align with the commit message's primary intention of fixing Javadoc issues. However, the change of the parameter name could indicate a bug fix.

**Buggy**"
wildfly,13142.json,77807f20fe0a38e6b867210ebf1b305edc14cf6a,"@@ -1,55 +1,59 @@
     private Method getTimeoutMethod(TimeoutMethod timeoutMethodInfo) {
 
         String declaringClass = timeoutMethodInfo.getDeclaringClass();
         Class<?> timeoutMethodDeclaringClass = null;
         try {
             timeoutMethodDeclaringClass = Class.forName(declaringClass, false, timedObjectInvoker.getClassLoader());
         } catch (ClassNotFoundException cnfe) {
-            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"");
+            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"", cnfe);
         }
 
         String timeoutMethodName = timeoutMethodInfo.getMethodName();
         String[] timeoutMethodParams = timeoutMethodInfo.getMethodParams();
         // load the method param classes
         Class<?>[] timeoutMethodParamTypes = new Class<?>[]
                 {};
         if (timeoutMethodParams != null) {
             timeoutMethodParamTypes = new Class<?>[timeoutMethodParams.length];
             int i = 0;
             for (String paramClassName : timeoutMethodParams) {
                 Class<?> methodParamClass = null;
                 try {
                     methodParamClass = Class.forName(paramClassName, false, timedObjectInvoker.getClassLoader());
                 } catch (ClassNotFoundException cnfe) {
-                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"");
+                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"", cnfe);
                 }
                 timeoutMethodParamTypes[i++] = methodParamClass;
             }
         }
         // now start looking for the method
         Class<?> klass = timeoutMethodDeclaringClass;
         while (klass != null) {
             Method[] methods = klass.getDeclaredMethods();
             for (Method method : methods) {
                 if (method.getName().equals(timeoutMethodName)) {
                     Class<?>[] methodParamTypes = method.getParameterTypes();
                     // param length doesn't match
                     if (timeoutMethodParamTypes.length != methodParamTypes.length) {
                         continue;
                     }
+                    boolean match = true;
                     for (int i = 0; i < methodParamTypes.length; i++) {
                         // param type doesn't match
                         if (!timeoutMethodParamTypes[i].equals(methodParamTypes[i])) {
-                            continue;
+                            match = false;
+                            break;
                         }
                     }
-                    // match found
-                    return method;
+                    if (match) {
+                        // match found
+                        return method;
+                    }
                 }
             }
             klass = klass.getSuperclass();
 
         }
         // no match found
         return null;
     }",Buggy,"Fix minor timer service bug
",Buggy,"The commit message ""Fix minor timer service bug"" suggests a bug fix in the timer service.

The diff shows changes in the `getTimeoutMethod` method. Specifically, the `ClassNotFoundException` catch blocks now include the original exception (`cnfe`) as the cause when throwing a new `RuntimeException`. This provides more context for debugging. Additionally, a boolean `match` variable is introduced to track whether the parameter types match within the inner loop. If a mismatch is found, the `match` variable is set to `false`, and the inner loop breaks. The method returns only if `match` is true after the inner loop completes.

The changes in exception handling and the addition of the `match` variable suggest that the original code might have been swallowing exceptions or incorrectly matching methods. These are strong indicators of a bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**"
wildfly,15035.json,162a90c311abfe554f09dc4b9c080098bd27ad89,"@@ -1,18 +1,18 @@
         public final T visit(final ContextNode contextNode) throws NamingException {
             if (isEmpty(currentName)) {
                 return found(contextNode);
             }
             final String childName = currentName.get(0);
             traversedName.add(childName);
             currentName = currentName.getSuffix(1);
             final TreeNode node = contextNode.children.get(childName);
             if (node == null) {
                 if (createIfMissing) {
-                    final NamingContext subContext = new NamingContext(traversedName, InMemoryNamingStore.this, new Hashtable<String, Object>());
-                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, traversedName, subContext)).accept(this);
+                    final NamingContext subContext = new NamingContext((Name)traversedName.clone(), InMemoryNamingStore.this, new Hashtable<String, Object>());
+                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, (Name)traversedName.clone(), subContext)).accept(this);
                 } else {
                     throw nameNotFoundException(childName, contextNode.fullName);
                 }
             }
             return node.accept(this);
         }",Buggy,"AS7-1407 - Fix bug in nested context binding
",Buggy,"The commit message ""AS7-1407 - Fix bug in nested context binding"" suggests a bug fix related to nested context binding in the application server.

The code diff shows a change in the `visit` method of a class, likely related to traversing a naming context tree. Specifically, the change involves how the `traversedName` is used when creating a new subcontext. The original code was passing `traversedName` directly to the `NamingContext` constructor and `ContextNode` constructor. The modified code now clones `traversedName` before passing it to these constructors: `(Name)traversedName.clone()`.

This change is significant because without cloning, modifications to `traversedName` in subsequent iterations of the `visit` method would affect the `traversedName` stored in the previously created `NamingContext` and `ContextNode`. This would lead to incorrect name resolution in nested contexts, which aligns with the commit message's description of a bug in nested context binding. The cloning ensures that each `NamingContext` and `ContextNode` receives a distinct copy of the traversed name, preventing unintended side effects and resolving the bug.

Therefore, the code change directly addresses the bug described in the commit message.

**Buggy**"
wildfly,11562.json,1af2b154ec4daa8436ba6f946ef54fe201062c6a,"@@ -1,6 +1,8 @@
     private synchronized void destroySingletonInstance() {
         if (this.singletonComponentInstance != null) {
-            this.destroyInstance(this.singletonComponentInstance);
+            // TODO: Implement destroying an instance
+            logger.warn(""Destorying of singleton instance not yet implemented"");
+            //this.destroyInstance(this.singletonComponentInstance);
             this.singletonComponentInstance = null;
         }
     }",Buggy,"Fix compilation errors in EJB3 module to start refactoring the EJB3 component
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""Fix compilation errors in EJB3 module to start refactoring the EJB3 component"" suggests that the primary goal is to resolve compilation issues, paving the way for subsequent refactoring.

2.  **Diff Analysis:**
    *   The code change involves commenting out a line `this.destroyInstance(this.singletonComponentInstance);` within the `destroySingletonInstance()` method.
    *   A `logger.warn()` statement is added to indicate that the destroying of a singleton instance is not yet implemented.
    *   A `TODO` comment is added.

3.  **Alignment and Bug Fix Indication:**
    *   The original code likely caused a compilation error or runtime exception because `destroyInstance()` was either not implemented correctly or was causing issues.
    *   Commenting out the problematic line and adding a warning message effectively bypasses the error, allowing the compilation to succeed.
    *   The `TODO` comment indicates that this is a temporary fix and the functionality needs to be properly implemented later.
    *   The fact that the original line was causing problems and was commented out strongly suggests a bug fix.

**Conclusion:**

**Buggy**
"
wildfly,9528.json,3b47b9b23bf3f10d124f26f37dff3245fe09ec56,"@@ -1,52 +1,52 @@
         public void execute(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final ModelNode model = context.readResource(PathAddress.EMPTY_ADDRESS).getModel();
 
             final ModelNode coreThreads;
             final ModelNode maxThreads;
             final ModelNode queueLength;
 
             if (isRuntimeStage) {
                 coreThreads = CORE_THREADS_AD.resolveModelAttribute(context, model);
                 maxThreads = MAX_THREADS_AD.resolveModelAttribute(context, model);
                 queueLength = QUEUE_LENGTH_AD.resolveModelAttribute(context, model);
             } else {
                 coreThreads = model.get(CORE_THREADS);
                 maxThreads = model.get(MAX_THREADS);
                 queueLength = model.get(QUEUE_LENGTH);
             }
 
             if (coreThreads.getType() == ModelType.EXPRESSION || maxThreads.getType() == ModelType.EXPRESSION ||
                     queueLength.getType() == ModelType.EXPRESSION) {
                 context.addStep(new ExecutorQueueValidationStepHandler(true), OperationContext.Stage.RUNTIME, true);
                 return;
             }
 
             // Validate an unbounded queue
             if (!queueLength.isDefined() || queueLength.asInt() == Integer.MAX_VALUE) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
 
             }
 
             // Validate a hand-off queue
             if (queueLength.isDefined() && queueLength.asInt() == 0) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
             }
 
             // max-threads must be defined and greater than 0 if core-threads is 0
             if (coreThreads.isDefined() && coreThreads.asInt() == 0) {
                 if (!maxThreads.isDefined() || maxThreads.asInt() <= 0) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.isDefined() ? maxThreads.asInt() : 0, coreThreads.asInt());
                 }
             }
 
             // max-threads must be greater than or equal to core-threads
             if (coreThreads.isDefined() && maxThreads.isDefined()) {
                 if (maxThreads.asInt() < coreThreads.asInt()) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.asInt(), coreThreads.asInt());
                 }
             }
         }",Buggy,"[WFLY-8516] Update schema documentation and fix error messages for the managed-executor-service.
",Buggy,"The commit message indicates updates to schema documentation and fixes to error messages for the managed-executor-service.

The code diff focuses on the `execute` method within what appears to be a handler for an operation related to managed executor services. The changes involve validation logic for core threads, max threads, and queue length.

Specifically, the diff highlights two instances where the error message being thrown was incorrect. Instead of referencing the `coreThreads` value in the error message, it was incorrectly referencing the `queueLength` value. The corrected code now throws an error message that includes the actual value of `coreThreads`.

This correction directly addresses an issue where the error message was misleading, which constitutes a bug fix.

Therefore, the final answer is:
**Buggy**"
wildfly,10266.json,2bb205f62a00e2d69781f4b468c299316f855764,"@@ -1,5 +1,5 @@
     public void start(StartContext context) throws StartException {
         this.value.setDefaultBootstrapContext(defaultBootstrapContext.getValue());
-        ROOT_LOGGER.startingSubsystem(""JCS"", Version.FULL_VERSION);
+        ROOT_LOGGER.startingSubsystem(""JCA"", Version.FULL_VERSION);
         ROOT_LOGGER.tracef(""config=%s"", value);
     }",Buggy,"Fix spelling error in a JCA subsystem log message.
",Buggy,"The commit message indicates a fix for a spelling error in a log message within the JCA subsystem. The provided diff shows a change from ""JCS"" to ""JCA"" in the `ROOT_LOGGER.startingSubsystem` call. This aligns perfectly with the commit message, suggesting a correction of a typographical error in the logging output. This is a bug fix.

**Buggy**"
wildfly,18502.json,a10bd882e880725b709e6745e140931720629a95,"@@ -1,17 +1,20 @@
     public InputStream sanitize(InputStream in) throws Exception {
+        byte [] content = IOUtils.toByteArray(in);
         try {
-            Document doc = builder.parse(in);
+            // storing the entire file in memory in case we need to bail.
+            Document doc = builder.parse(new ByteArrayInputStream(content));
             Object result = expression.evaluate(doc, XPathConstants.NODESET);
             NodeList nodes = (NodeList) result;
             for (int i = 0; i < nodes.getLength(); i++) {
                 nodes.item(i).setTextContent("""");
             }
             DOMSource source = new DOMSource(doc);
             ByteArrayOutputStream output = new ByteArrayOutputStream();
             StreamResult outStream = new StreamResult(output);
             transformer.transform(source, outStream);
             return new ByteArrayInputStream(output.toByteArray());
         } catch (Exception e) {
-            return in;
+            ROOT_LOGGER.debug(""Error while sanitizing an xml document"", e);
+            return new ByteArrayInputStream(content);
         }
     }",Buggy,"Addressing AS7-6731 [bz920113]

Module dependency on xalan now uses services=import
Adding javadoc to various bits of the code
Fixes an issue where an error during sanitation causes
0 length files to be stored in the archive.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message mentions:

*   ""Addressing AS7-6731 [bz920113]"": This strongly suggests a bug fix, as it refers to a specific issue tracker entry (AS7-6731) and a bugzilla number (920113).
*   ""Fixes an issue where an error during sanitation causes 0 length files to be stored in the archive."" This explicitly states that a bug is being fixed, where sanitation errors lead to empty files.
*   ""Module dependency on xalan now uses services=import"": This is likely a dependency management change, not directly related to a bug fix.
*   ""Adding javadoc to various bits of the code"": This is documentation improvement, not a bug fix.

**Git Diff Analysis:**

The code diff focuses on the `sanitize` method. Let's break down the changes:

1.  `byte [] content = IOUtils.toByteArray(in);`: The input stream is read into a byte array. This is likely done to allow re-reading the stream in case of an error.
2.  `Document doc = builder.parse(new ByteArrayInputStream(content));`: The document is parsed from the byte array.
3.  `return in;` is replaced with `ROOT_LOGGER.debug(""Error while sanitizing an xml document"", e); return new ByteArrayInputStream(content);`: This is the crucial part. Previously, if an exception occurred during the sanitation process, the original input stream (`in`) was returned. Now, the code logs the error and returns a new input stream containing the original content.

**Reasoning:**

The commit message explicitly states a bug where sanitation errors resulted in zero-length files. The code diff shows that the `sanitize` method now handles exceptions differently. Previously, an exception would lead to the original (potentially corrupted) input stream being returned. Now, the code logs the error and returns the original content as a new input stream. This change addresses the bug described in the commit message because it prevents the propagation of a potentially corrupted or incomplete stream when an error occurs during sanitation. The original input stream is now returned, preventing the creation of zero-length files.

**Conclusion:**

The commit message and code diff are strongly aligned and indicate a bug fix. The changes prevent zero-length files from being created when an error occurs during XML sanitation.

**Buggy**
"
wildfly,6365.json,7f64913774a8043da0b0cc545f4e1b2a8af1a082,"@@ -1,7 +1,13 @@
     public boolean isTransactionActive() {
         try {
-            return (injectedTransaction.getValue().getStatus() & Status.STATUS_ACTIVE) != 0;
+            final int status = injectedTransaction.getValue().getStatus();
+            return status == Status.STATUS_ACTIVE ||
+                    status == Status.STATUS_COMMITTING ||
+                    status == Status.STATUS_MARKED_ROLLBACK ||
+                    status == Status.STATUS_PREPARED ||
+                    status == Status.STATUS_PREPARING ||
+                    status == Status.STATUS_ROLLING_BACK;
         } catch (SystemException e) {
             throw new RuntimeException(""SystemException while getting transaction status"", e);
         }
     }",Buggy,"Fix bug in WeldTransactionServices
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

The commit message ""Fix bug in WeldTransactionServices"" clearly states that the commit is intended to fix a bug within the `WeldTransactionServices` class. This sets the expectation that the code changes should address some incorrect behavior or defect.

**2. Code Diff Analysis:**

The code diff modifies the `isTransactionActive()` method.  The original code checked if the transaction status, when bitwise ANDed with `Status.STATUS_ACTIVE`, was non-zero. This implies that the original code only considered the transaction active if its status was exactly `Status.STATUS_ACTIVE`.

The modified code expands the check to include several other transaction statuses: `STATUS_COMMITTING`, `STATUS_MARKED_ROLLBACK`, `STATUS_PREPARED`, `STATUS_PREPARING`, and `STATUS_ROLLING_BACK`. This suggests that the original code incorrectly reported the transaction as inactive in certain states where it should have been considered active.

**3. Reasoning:**

The original code's narrow interpretation of ""active"" likely caused issues where the application incorrectly assumed a transaction was no longer active when it was in a transitional state (e.g., committing, preparing to commit, rolling back). By expanding the status checks, the corrected code provides a more accurate representation of whether a transaction is truly active. The change addresses a logical error in determining the transaction's state.

**4. Conclusion:**

The code change directly addresses a flaw in the logic of determining whether a transaction is active. The commit message explicitly states that it fixes a bug. Therefore, the changes indicate a bug fix.

**Buggy**
"
wildfly,14704.json,59232bbc1b3a2808f2cbc125e1aca2647fa5abd5,"@@ -1,16 +1,11 @@
     public static void initializeNamingManager() {
         // Setup naming environment
         System.setProperty(Context.URL_PKG_PREFIXES, PACKAGE_PREFIXES);
         try {
             //If we are reusing the JVM. e.g. in tests we should not set this again
             if (!NamingManager.hasInitialContextFactoryBuilder())
                 NamingManager.setInitialContextFactoryBuilder(new InitialContextFactoryBuilder());
         } catch (NamingException e) {
             log.warn(""Failed to set InitialContextFactoryBuilder"", e);
         }
-        try {
-            NamingManager.setObjectFactoryBuilder(ObjectFactoryBuilder.INSTANCE);
-        } catch(Throwable t) {
-            log.warn(""Failed to set ObjectFactoryBuilder"", t);
-        }
     }",Buggy,"Fix problem where object factory builder can only be set once
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fix problem where object factory builder can only be set once"" clearly indicates that the commit aims to resolve an issue related to setting the object factory builder. The message suggests that there was a limitation or error that prevented the object factory builder from being set multiple times, implying a bug.

**Git Diff Analysis:**

The diff shows the removal of the following code block:

```java
try {
    NamingManager.setObjectFactoryBuilder(ObjectFactoryBuilder.INSTANCE);
} catch(Throwable t) {
    log.warn(""Failed to set ObjectFactoryBuilder"", t);
}
```

This code block attempts to set the `ObjectFactoryBuilder`. The removal of this block, coupled with the commit message, strongly suggests that the original implementation had a flaw. The problem likely stemmed from attempting to set the `ObjectFactoryBuilder` multiple times, which could lead to exceptions or unexpected behavior, especially in scenarios where the JVM is reused (as mentioned in the comment regarding `InitialContextFactoryBuilder`). The fact that the code was wrapped in a `try-catch` block with a `warn` log statement further supports the idea that setting the `ObjectFactoryBuilder` was potentially problematic.

**Reasoning:**

The commit message explicitly states a problem related to setting the object factory builder. The diff removes the code responsible for setting the `ObjectFactoryBuilder`. This removal, combined with the exception handling in the removed code, indicates that the original code was likely causing an issue when attempting to set the `ObjectFactoryBuilder` more than once. The removal of this code addresses the problem described in the commit message.

**Conclusion:**

**Buggy**
"
wildfly,14735.json,603f475d6af9bc7abc7bcee74a897d4577977d27,"@@ -1,3 +1,3 @@
     public void close() throws NamingException {
-        namingStore.close();
+        // NO-OP
     }",Buggy,"Fix bug in naming context with Context.close removing entries from the naming store
",Buggy,"The commit message indicates a bug fix related to the `Context.close` method removing entries from the naming store. The diff shows that the implementation of the `close` method has been changed to a no-op. This change directly addresses the bug described in the commit message, as it prevents the `namingStore.close()` method from being called, thus preventing the unintended removal of entries.

**Buggy**
"
wildfly,5832.json,8c3680a316585b462638de3f031affc527f5c723,"@@ -1,7 +1,7 @@
     public static <T> Class<T> loadClass(String className, ClassLoader classLoader) {
         try {
             return cast(classLoader.loadClass(className));
-        } catch (Exception e) {
+        } catch (Throwable e) {
             return null;
         }
     }",Buggy,"Capture RuntimeExceptions in dinamic class loading

Since 9.x, if you deploy an war with two (or more wars) with different libraries, deployment crash since it's trying to load classes from one WAR with the classloader from the other WAR. This cross load is intended (see BeanDeploymentArchiveImpl, method isAccesible, line 246, WFLY-4250), but it's waiting for a null load, not a RuntimeException.

With this fix, a warn is registered in log but application can be loaded without major problems.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

*   **Problem Description:** The commit message clearly states a problem: deploying multiple WARs with different libraries can lead to deployment crashes due to cross-loading of classes using different WAR classloaders. This issue was introduced in version 9.x.
*   **Root Cause:** The `BeanDeploymentArchiveImpl.isAccesible` method (related to WFLY-4250) expects a `null` load when a class cannot be loaded by a specific classloader, but it encounters a `RuntimeException` instead.
*   **Solution:** The commit aims to catch `RuntimeException` (or a broader exception type) to prevent the crash and log a warning, allowing the application to load, albeit with a potential classloading issue.

**Git Diff Analysis:**

*   **Code Change:** The `loadClass` method's `catch` block is modified to catch `Throwable` instead of `Exception`. This is a significant change because `Throwable` is the parent class of all errors and exceptions in Java, including `RuntimeException`.
*   **Impact:** By catching `Throwable`, the code now handles not only checked exceptions but also unchecked exceptions (like `RuntimeException`) and errors. This aligns with the commit message's intention to handle `RuntimeException` during class loading.

**Reasoning:**

1.  **Bug Identification:** The commit message explicitly describes a bug scenario where the application crashes due to unexpected `RuntimeException` during class loading.
2.  **Error Handling:** The code change directly addresses this bug by expanding the `catch` block to handle `Throwable`, which includes `RuntimeException`. This prevents the application from crashing and allows it to load with a warning message.
3.  **Alignment:** The code change perfectly aligns with the problem described in the commit message. The commit message explains the need to catch `RuntimeException`, and the code change implements this by catching `Throwable`.
4.  **Severity:** The original code's failure to catch `RuntimeException` resulted in a deployment crash, indicating a significant bug.

**Conclusion:**

The commit message describes a bug, and the code change directly addresses it by improving error handling. Therefore, the changes indicate a bug fix.

**Buggy**"
wildfly,10236.json,efb61994477cc0e8f0f1eca6da690a78c9c33699,"@@ -1,21 +1,21 @@
         private void writeWorkManagers(XMLExtendedStreamWriter writer, ModelNode parentNode) throws XMLStreamException {
             if (parentNode.hasDefined(WORKMANAGER) && parentNode.get(WORKMANAGER).asList().size() != 0) {
                 for (Property property : parentNode.get(WORKMANAGER).asPropertyList()) {
                     if (""default"".equals(property.getValue().get(NAME).asString())) {
                         writer.writeStartElement(Element.DEFAULT_WORKMANAGER.getLocalName());
                     } else {
                         writer.writeStartElement(Element.WORKMANAGER.getLocalName());
                         WorkManagerAdd.WmParameters.NAME.getAttribute().marshallAsAttribute(property.getValue(), writer);
                     }
-                    for (Property prop : property.getValue().asPropertyList()) {
+                    for (Property prop : property.getValue().asPropertyList() ) {
                         if (WORKMANAGER_LONG_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.LONG_RUNNING_THREADS.getLocalName(), false);
                         }
-                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName())) {
+                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.SHORT_RUNNING_THREADS.getLocalName(), false);
                         }
                     }
                     writer.writeEndElement();
                 }
             }
         }",Buggy,"Fixing persistemce problem when all thread executor config has been removed
",Buggy,"The commit message indicates a fix related to persistence when thread executor configurations are removed. The code diff modifies the conditions under which the `WORKMANAGER_SHORT_RUNNING` thread pool is written. Specifically, it adds a check to ensure that `prop.getValue()` is defined and that the property list size is not zero before attempting to write the bounded queue thread pool. This suggests that previously, the code might have been attempting to write the thread pool even when the configuration was missing or empty, leading to a persistence problem. The added checks prevent this from happening, aligning with the commit message's intent to fix a persistence issue when thread executor configurations are removed.

**Buggy**
"
wildfly,10893.json,e8dd2b36ea6a8dd25b8cffc37791e862fb08aa45,"@@ -1,12 +1,12 @@
         public ResourceAdapter doDeploy(URL url, String deploymentName, File root, ClassLoader cl, Connector cmd,
                 IronJacamar ijmd) throws Throwable {
             // TODO
             this.setConfiguration(getConfig().getValue());
 
             this.start();
 
-            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, null, cl, cmd, ijmd, null);
+            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, cl, cmd, ijmd);
 
             return dep.getResourceAdapter();
 
         }",Buggy,"fixing problem with latests ironjacamar snapshots
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message:** The commit message ""fixing problem with latests ironjacamar snapshots"" suggests a bug fix related to recent changes in IronJacamar.

2.  **Code Diff:** The code diff shows a change in the `doDeploy` method. Specifically, the `createObjectsAndInjectValue` method call has been modified. The `null` argument previously passed as the 4th argument has been removed, and `cl` (ClassLoader) is now passed in its place.

3.  **Analysis:**
    *   The change in the method signature of `createObjectsAndInjectValue` suggests that the previous implementation might have been incorrect or incompatible with the latest IronJacamar snapshots. Passing `null` where a `ClassLoader` is now expected could definitely lead to issues, indicating a bug.
    *   The commit message explicitly mentions a problem with ""latests ironjacamar snapshots,"" which strongly correlates with the code modification.

**Conclusion:**

**Buggy**
"
javaparser,3383.json,85b5cf5a98bc7fc59b3481d802bdff3b736ae8a1,"@@ -1,62 +1,65 @@
     public void accept(VarType node, ProblemReporter reporter) {
         // All allowed locations are within a VariableDeclaration inside a VariableDeclarationExpr inside something else.
         Optional<VariableDeclarator> variableDeclarator = node.findAncestor(VariableDeclarator.class);
         if (!variableDeclarator.isPresent()) {
             // Java 11's var in lambda's
             if (varAllowedInLambdaParameters) {
                 boolean valid = node
                         .findAncestor(Parameter.class)
                         .flatMap(Node::getParentNode)
                         .map((Node p) -> p instanceof LambdaExpr).orElse(false);
                 if (valid) {
                     return;
                 }
             }
             reportIllegalPosition(node, reporter);
             return;
         }
         variableDeclarator.ifPresent(vd -> {
+            if(vd.getType().isArrayType()){
+                reporter.report(vd, ""\""var\"" cannot have extra array brackets."");
+            }
             Optional<Node> variableDeclarationExpr = vd.getParentNode();
             if (!variableDeclarationExpr.isPresent()) {
                 reportIllegalPosition(node, reporter);
                 return;
             }
             variableDeclarationExpr.ifPresent(vdeNode -> {
                 if (!(vdeNode instanceof VariableDeclarationExpr)) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 VariableDeclarationExpr vde = (VariableDeclarationExpr) vdeNode;
                 if (vde.getVariables().size() > 1) {
                     reporter.report(vde, ""\""var\"" only takes a single variable."");
                 }
                 Optional<Node> container = vdeNode.getParentNode();
                 if (!container.isPresent()) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 container.ifPresent(c -> {
                     boolean positionIsFine = c instanceof ForStmt || c instanceof ForeachStmt || c instanceof ExpressionStmt;
                     if (!positionIsFine) {
                         reportIllegalPosition(node, reporter);
                     }
                     // A local variable declaration ends up inside an ExpressionStmt.
                     if (c instanceof ExpressionStmt) {
                         if (!vd.getInitializer().isPresent()) {
                             reporter.report(node, ""\""var\"" needs an initializer."");
                         }
                         vd.getInitializer().ifPresent(initializer -> {
                             if (initializer instanceof NullLiteralExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer type from just null."");
                             }
-                            if (initializer instanceof ArrayCreationExpr) {
+                            if (initializer instanceof ArrayInitializerExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer array types."");
                             }
                         });
 
                     }
                 });
             });
         });
 
     }",Buggy,"Fix mistake with var and array initializers.
",Buggy,"The commit message ""Fix mistake with var and array initializers"" suggests a correction related to how the `var` keyword interacts with array initializers.

The diff adds a check to report an error if `var` is used with extra array brackets:
```java
if(vd.getType().isArrayType()){
    reporter.report(vd, ""\""var\"" cannot have extra array brackets."");
}
```
It also changes the check for array initializers from `ArrayCreationExpr` to `ArrayInitializerExpr`:
```java
- if (initializer instanceof ArrayCreationExpr) {
+ if (initializer instanceof ArrayInitializerExpr) {
    reporter.report(node, ""\""var\"" cannot infer array types."");
}
```
This change is important because it correctly identifies the cases where `var` is used with an array initializer, which is a common mistake. The original code was checking for `ArrayCreationExpr`, which is a different construct.

These changes directly address the issue described in the commit message, indicating that there was a bug in how the code handled `var` with array initializers.

**Buggy**"
javaparser,6722.json,c620101966295e302b9ff94a73a0e2ab29c7ec6d,"@@ -1,3 +1,3 @@
     public boolean isWildcard() {
-        return getType().isArray();
+        return getType().isWildcard();
     }",Buggy,"Fix bug in LazyType
",Buggy,"The commit message indicates a bug fix in `LazyType`. The code change modifies the `isWildcard()` method to check if the underlying type is a wildcard instead of an array. This suggests that the original implementation incorrectly identified array types as wildcards, which is a bug. The change directly addresses this incorrect behavior.

**Buggy**"
javaparser,6689.json,a795de5cf63b37147e9efcd9ed89e486a613fd3c,"@@ -1,3 +1,3 @@
     public String getName() {
-        return constructor.getName();
+        return constructor.getDeclaringClass().getSimpleName();
     }",Buggy,"Fixed a bug in ReflectionConstructorDeclaration that caused the constructor's signature to be calculated incorrectly.
",Buggy,"Okay, let's analyze this commit.

**Reasoning:**

1.  **Commit Message:** The commit message states a bug fix in `ReflectionConstructorDeclaration` related to incorrect constructor signature calculation. This strongly suggests a bug fix.
2.  **Code Diff:** The code diff shows a change in the `getName()` method. It changes from returning `constructor.getName()` to `constructor.getDeclaringClass().getSimpleName()`.
3.  **Relevance:** The original code `constructor.getName()` would likely return the fully qualified name of the constructor's class, which isn't the constructor's name itself. The corrected code `constructor.getDeclaringClass().getSimpleName()` returns the simple name of the declaring class, which is more aligned with what one might expect as the constructor's ""name"" in a specific context (likely within the class itself). This change directly addresses the issue of incorrectly calculating the constructor's signature or name, as stated in the commit message.

**Conclusion:**

The code change aligns with the commit message, indicating a correction to how the constructor's name/signature is retrieved. This is a clear bug fix.

**Buggy**"
javaparser,6349.json,dc99432270600b2bb2a445c86b329a77601005e6,"@@ -1,10 +1,10 @@
     public SymbolReference<ResolvedAnnotationDeclaration> solve(AnnotationExpr annotationExpr) {
         Context context = JavaParserFactory.getContext(annotationExpr, typeSolver);
         SymbolReference<ResolvedTypeDeclaration> typeDeclarationSymbolReference = context.solveType(annotationExpr.getNameAsString(), typeSolver);
-        ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
         if (typeDeclarationSymbolReference.isSolved()) {
+            ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
             return SymbolReference.solved(annotationDeclaration);
         } else {
             return SymbolReference.unsolved(ResolvedAnnotationDeclaration.class);
         }
     }",Buggy,"Fixed minor bug: a corresponding declaration of a SymbolReference is only available if the SymbolReference is solved, and one should not try to retrieve it otherwise, lest a NullPointerException be thrown.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message states: ""Fixed minor bug: a corresponding declaration of a SymbolReference is only available if the SymbolReference is solved, and one should not try to retrieve it otherwise, lest a NullPointerException be thrown.""

This clearly indicates a bug fix related to a potential `NullPointerException`. The issue arises when attempting to access the corresponding declaration of a `SymbolReference` before ensuring it's actually solved (i.e., has a valid declaration).

**Git Diff Analysis:**

The diff shows a change in the `solve` method related to annotation processing.  The original code directly retrieved the `ResolvedAnnotationDeclaration` from `typeDeclarationSymbolReference` without checking if `typeDeclarationSymbolReference` was solved.

The modified code introduces a conditional check: `if (typeDeclarationSymbolReference.isSolved())`.  The `ResolvedAnnotationDeclaration` is now only retrieved *if* the `typeDeclarationSymbolReference` is solved.  This prevents the code from attempting to access the declaration when it's not available, which would lead to a `NullPointerException` as described in the commit message.

**Reasoning:**

The commit message and the code changes are perfectly aligned. The code modification directly addresses the potential `NullPointerException` described in the commit message by adding a check to ensure the `SymbolReference` is solved before attempting to retrieve its corresponding declaration. This is a classic example of a bug fix where a missing null check or a premature access to an object's properties is corrected.

**Conclusion:**

**Buggy**
"
intellij-community,15168.json,8983e75f816afa5ca0d149e488a252e54d16f50f,"@@ -1,22 +1,22 @@
   private static void showWhatsNewNotification(@NotNull Project project) {
     PropertiesComponent properties = PropertiesComponent.getInstance();
     String updateHtmlMessage = properties.getValue(UPDATE_WHATS_NEW_MESSAGE);
     if (updateHtmlMessage == null) {
       LOG.warn(""Cannot show what's new notification: no content found."");
       return;
     }
 
     String title = IdeBundle.message(""update.whats.new.notification.title"", ApplicationNamesInfo.getInstance().getFullProductName());
     UpdateChecker.getNotificationGroup().createNotification(title, null, null, NotificationType.INFORMATION, null)
       .addAction(new NotificationAction(IdeBundle.message(""update.whats.new.notification.action"")) {
         @Override
         public void actionPerformed(@NotNull AnActionEvent e, @NotNull Notification notification) {
           LightVirtualFile file = new LightVirtualFile(IdeBundle.message(""update.whats.new.file.name"", ApplicationInfo.getInstance().getFullVersion()), updateHtmlMessage);
           file.putUserData(HTMLEditorProvider.Companion.getHTML_CONTENT_TYPE(), true);
           FileEditorManager.getInstance(project).openFile(file, true);
           IdeUpdateUsageTriggerCollector.trigger(""update.whats.new"");
           notification.expire();
         }
-      });
+      }).notify(project);
     properties.setValue(UPDATE_WHATS_NEW_MESSAGE, null);
   }",Buggy,"Fix stupid mistake: lost call notify()

GitOrigin-RevId: 954900084aa113c881590d721b66974c560d9d9f",Buggy,"The commit message ""Fix stupid mistake: lost call notify()"" suggests that a call to `notify()` was missing, and this commit adds it back.

The diff shows that the `notify(project)` call was added to the end of the `createNotification` chain. This confirms that the missing `notify()` call has been added. The original code was creating a notification but not displaying it, which is a bug.

Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,24434.json,358dddc2d98464d660a801e43e3671cbd5b340ed,"@@ -1,5 +1,5 @@
   public static Icon loadApplicationIconImage(@NotNull ScaleContext ctx, int size) {
     String url = ApplicationInfoImpl.getShadowInstance().getApplicationSvgIconUrl();
     Image image = loadApplicationIconImage(url, ctx, size, null);
-    return image != null ? new JBImageIcon(ImageUtil.ensureHiDPI(image, ctx)) : null;
+    return image != null ? new JBImageIcon(image) : null;
   }",Buggy,"IDEA-217767 License dialog polishing

* App icon problems fixed

GitOrigin-RevId: babcb9bce8c091934088cc24bc7ce335170096a6",Buggy,"The commit message ""IDEA-217767 License dialog polishing"" indicates a focus on improving the appearance and user experience of the license dialog. The specific mention of ""App icon problems fixed"" directly relates to the code change.

The diff shows a modification in the `loadApplicationIconImage` method. The original code used `ImageUtil.ensureHiDPI(image, ctx)` to ensure the image was properly scaled for high-DPI displays. The modified code removes this call, directly using the loaded image.

Reasoning:

1.  **Commit Message Relevance:** The commit message explicitly states that app icon problems were fixed.
2.  **Code Change Analysis:** The code change directly affects how the application icon is loaded and displayed. The removal of `ImageUtil.ensureHiDPI` suggests that the previous implementation might have been causing issues with the icon's appearance, especially on high-DPI displays. This could be due to incorrect scaling or other visual artifacts.
3.  **Bug Fix Indication:** The removal of a scaling function to fix ""app icon problems"" strongly suggests that the original code had a bug related to icon display. The change aims to correct this visual defect.

Conclusion:

**Buggy**
"
intellij-community,8042.json,08de853c05b9eb1944f79c1ea7df05ae1f103aa7,"@@ -1,3 +1,3 @@
   protected static String getUserHome() {
-    return SystemProperties.getUserHome();
+    return StringUtil.trimEnd(SystemProperties.getUserHome(), ""/"");
   }",Buggy,"save all paths in .iml relatively except for the ones under USER_HOME (IDEA-60906)
reason: fixes the issue and I don't see any problems it could cause
",Buggy,"The commit message indicates a fix for IDEA-60906, which involves saving paths in `.iml` files relatively, except for those under `USER_HOME`. The provided code diff modifies the `getUserHome()` method to trim trailing slashes from the user's home directory path. This change likely ensures consistency in how the user's home directory is represented, preventing issues when comparing or constructing file paths. The trimming of trailing slashes suggests an attempt to normalize the path, which is a common practice when dealing with file system operations and path comparisons. This normalization can address inconsistencies that might lead to incorrect relative path calculations or other path-related issues. Therefore, the change is related to fixing a bug.

**Buggy**
"
intellij-community,32648.json,0922a97944f2ca7d388ba2807e0e64afb02fe427,"@@ -1,3 +1,3 @@
   public void load(Element element) {
-    setVagrantFolder(element.getAttributeValue(VAGRANT_FOLDER));
+    setVagrantFolder(StringUtil.notNullize(element.getAttributeValue(VAGRANT_FOLDER)));
   }",Buggy,"Vagrant error messaging fixed.
",Buggy,"The commit message ""Vagrant error messaging fixed"" suggests that the commit aims to improve error messages related to Vagrant.

The code diff modifies the `load` method to use `StringUtil.notNullize()` when setting the Vagrant folder. This change ensures that the Vagrant folder is never null, even if the attribute value is missing or null. This can prevent NullPointerExceptions later on, which would improve the robustness of the code and potentially prevent error messages related to null values.

The change directly relates to handling a potentially null value, which could lead to errors if not handled correctly. The commit message indicates a fix related to error messaging, and the code change prevents a potential error. Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,23329.json,bcb3520ffa862b37e8147a7dac6cddb7d67c82b8,"@@ -1,29 +1,29 @@
     public void doExecute(Editor editor, @Nullable Caret caret, DataContext dataContext) {
       int endOffset = editor.getDocument().getTextLength();
       List<Caret> carets = editor.getCaretModel().getAllCarets();
       if (editor.isColumnMode() && editor.getCaretModel().supportsMultipleCarets()) {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0) == editor.getCaretModel().getPrimaryCaret() ? carets.get(carets.size() - 1) : carets.get(0);
         }
         LogicalPosition leadSelectionPosition = editor.visualToLogicalPosition(caret.getLeadSelectionPosition());
         LogicalPosition targetPosition = editor.offsetToLogicalPosition(endOffset).leanForward(true);
         editor.getSelectionModel().setBlockSelection(leadSelectionPosition, targetPosition);
       }
       else {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0);
         }
         int selectionStart = caret.getLeadSelectionOffset();
         if (editor instanceof EditorImpl && ((EditorImpl)editor).myUseNewRendering) {
-          editor.getCaretModel().moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
+          caret.moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
         }
         else {
           caret.moveToOffset(endOffset);
         }
         caret.setSelection(selectionStart, endOffset);
       }
       ScrollingModel scrollingModel = editor.getScrollingModel();
       scrollingModel.disableAnimation();
       scrollingModel.scrollToCaret(ScrollType.CENTER);
       scrollingModel.enableAnimation();
     }",Buggy,"editor.new.rendering: fix a mistake in TextEndWithSelectionAction
",Buggy,"The commit message indicates a fix in `TextEndWithSelectionAction`, specifically related to a mistake. The diff shows a change within the `doExecute` method, focusing on how the caret is moved when `editor instanceof EditorImpl && ((EditorImpl)editor).myUseNewRendering` is true.

The original code had `editor.getCaretModel().moveToLogicalPosition(...)`, while the corrected code has `caret.moveToLogicalPosition(...)`. This suggests that the original code was attempting to move *all* carets to the end of the document, instead of just the current caret. The corrected code moves only the current caret, which aligns with the intended behavior of extending the selection to the end of the text from the caret's current position. This is a logical correction, indicating a bug fix.

**Buggy**"
intellij-community,11452.json,8a97673aeb2e23f7a0bafdb1907b513cd3514810,"@@ -1,3 +1,3 @@
-  protected boolean atToken(final IElementType tokenType) {
+  protected boolean atToken(@Nullable final IElementType tokenType) {
     return myBuilder.getTokenType() == tokenType;
   }",Buggy,"better recovery for syntax errors in parameter list (PY-3635); fix old bug with tuple parameters parsing
",Buggy,"The commit message indicates a bug fix related to syntax errors in parameter lists and tuple parameter parsing. The code change involves adding a `@Nullable` annotation to the `tokenType` parameter of the `atToken` method. This change suggests that the method might have been previously called with a null `tokenType`, potentially leading to a `NullPointerException` or incorrect behavior. By allowing null values, the code becomes more robust and handles potential null inputs gracefully, which aligns with the bug fix mentioned in the commit message.

**Buggy**"
intellij-community,28989.json,ac607f3f6a68a2dbccca5581d55264e5ecad1181,"@@ -1,4 +1,4 @@
     public void addNotify() {
       super.addNotify();
-      addMouseListenerToHierarchy(this, myEntranceListener);
+      ListenerUtil.addMouseListener(this, myEntranceListener);
     }",Buggy,"links in IDE Fatal errors fix
",Buggy,"The commit message ""links in IDE Fatal errors fix"" suggests that the commit aims to fix fatal errors related to links within the IDE.

The code diff shows a change in how a mouse listener is added to a hierarchy. Specifically, `addMouseListenerToHierarchy(this, myEntranceListener)` is replaced with `ListenerUtil.addMouseListener(this, myEntranceListener)`.

The change suggests a bug fix because:

1.  **Error Handling/Prevention:** The original method `addMouseListenerToHierarchy` might have been causing fatal errors in certain scenarios, possibly due to incorrect handling of the hierarchy or listener addition. The replacement with `ListenerUtil.addMouseListener` implies a more robust or safer way to add the listener, potentially addressing the root cause of the fatal errors. `ListenerUtil` might include checks or error handling that the original method lacked.
2.  **Abstraction/Encapsulation:** Moving the listener addition logic to `ListenerUtil` suggests an attempt to encapsulate the process and potentially handle edge cases or errors in a centralized manner. This is a common practice when fixing bugs related to event handling or UI updates.

Therefore, the code change aligns with the commit message's intent to fix fatal errors, and the nature of the change suggests a bug fix related to how mouse listeners are added.

**Buggy**"
intellij-community,10790.json,e3fc0ee66ac0e1b3cff41e6cf7cf90b5c16318f5,"@@ -1,26 +1,29 @@
   private static int getExpressionPriority(PyElement expr) {
     int priority = 0;
-    if (expr instanceof PySubscriptionExpression || expr instanceof PySliceExpression || expr instanceof PyCallExpression) priority = 1;
+    if (expr instanceof PyReferenceExpression ||
+        expr instanceof PySubscriptionExpression ||
+        expr instanceof PySliceExpression ||
+        expr instanceof PyCallExpression) priority = 1;
     else if (expr instanceof PyPrefixExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == PLUS || opType == MINUS || opType == TILDE) priority = 2;
       if (opType == NOT_KEYWORD) priority = 11;
     }
     else if (expr instanceof PyBinaryExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == EXP) priority =  3;
       if (opType == MULT || opType == DIV || opType == PERC || opType == FLOORDIV) priority =  4;
       if (opType == PLUS || opType == MINUS) priority =  5;
       if (opType == LTLT || opType == GTGT) priority = 6;
       if (opType == AND) priority = 7;
       if (opType == XOR) priority = 8;
       if (opType == OR) priority = 9;
       if (COMPARISON_OPERATIONS.contains(opType)) priority = 10;
       if (opType == AND_KEYWORD) priority = 12;
       if (opType == OR_KEYWORD) priority = 13;
     }
     else if (expr instanceof PyConditionalExpression) priority = 14;
     else if (expr instanceof PyLambdaExpression) priority = 15;
 
     return -priority;
   }",Buggy,"PY-13114 Inlining of method call does not add extra parenthesis

Fix prioritization problem: reference expressions should have the same
priority as indexing, slicing and function calls.
",Buggy,"The commit message indicates a fix for a problem where inlining a method call doesn't add extra parentheses when needed. This suggests a bug related to operator precedence during inlining.

The code diff modifies the `getExpressionPriority` method, specifically adding `PyReferenceExpression` to the list of expressions that should have priority 1. This change directly addresses the issue described in the commit message. By giving `PyReferenceExpression` the same priority as indexing, slicing, and function calls, the code ensures that inlined method calls (which are often represented as reference expressions) are correctly parenthesized to maintain the intended order of operations.

Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,11366.json,aa7434f6b1f2853d50f940681697db30743e0f2e,"@@ -1,12 +1,13 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
-        removeSlash(pyImportStatement, next);
+        final ASTNode prev = next.getTreePrev();
         pyImportStatement.deleteChildInternal(next);
+        removeSlash(pyImportStatement, prev);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"The commit message indicates a fix for a syntax error caused by refactoring or moving multi-line imports (PY-5489). The code diff modifies the `deleteAdjacentComma` method, specifically how it handles the removal of commas and slashes during import statement manipulation.

Here's a breakdown of the reasoning:

1. **Context:** The code operates on AST (Abstract Syntax Tree) nodes, suggesting it's part of a code parsing or manipulation process, likely within an IDE or code analysis tool. The mention of ""PyImportStatement"" strongly suggests it's related to Python import statements.

2. **Original Code:** The original code removes a slash after deleting a comma (`removeSlash(pyImportStatement, next);`).  The `next` variable holds either the next or previous comma node relative to the current child node.

3. **Modified Code:** The modified code introduces a new variable `prev` which stores the node *before* the comma node (`next`). It then removes the slash from this `prev` node (`removeSlash(pyImportStatement, prev);`).

4. **Bug Fix Indication:** The original code likely had a bug where it was removing the slash from the wrong node after deleting a comma.  The change to remove the slash from the *previous* node suggests that the original logic was incorrect, and this correction addresses the syntax error mentioned in the commit message. The original code was probably deleting the slash from the comma itself, or from a node after the comma, which would be incorrect. The corrected code now deletes the slash from the node preceding the comma, which is the correct behavior in many cases of multi-line imports.

5. **Relevance to Commit Message:** The code change directly relates to manipulating import statements and removing characters that could cause syntax errors. The commit message explicitly mentions a syntax error caused by refactoring multi-line imports, which aligns perfectly with the code modification.

**Conclusion:** **Buggy**
"
intellij-community,11366.json,319662174c861f6c1472a6d644db4513907f4c74,"@@ -1,11 +1,12 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
+        removeSlash(pyImportStatement, next);
         pyImportStatement.deleteChildInternal(next);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports"" clearly states that this commit is intended to fix a bug (PY-5489). The bug is related to refactoring or moving code that involves multi-line imports, which results in a syntax error.

**Git Diff Analysis:**

The diff shows a modification within the `deleteAdjacentComma` method. This method seems to be responsible for deleting commas adjacent to certain elements within a Python import statement. The added line `removeSlash(pyImportStatement, next);` suggests that the code now also handles and removes slashes (likely backslashes used for line continuation in multi-line imports) when deleting an adjacent comma.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly mentions a bug fix.
2.  **Contextual Relevance:** The code modification within `deleteAdjacentComma` directly relates to handling multi-line imports (as indicated by the addition of `removeSlash`). The original bug was triggered by refactoring/moving code involving multi-line imports, which resulted in a syntax error. The added code aims to prevent this syntax error by correctly handling line continuation characters (slashes) during comma deletion.
3.  **Error Handling:** The addition of `removeSlash` suggests that the previous implementation was not correctly handling the case where a comma to be deleted was followed by a backslash used for line continuation in a multi-line import statement. This could lead to a syntax error if the backslash was left dangling after the comma was removed.

**Conclusion:**

The commit message and the code changes strongly suggest that this commit is a bug fix. The code modification addresses a specific scenario related to multi-line imports that was causing a syntax error during refactoring/moving operations.

**Buggy**"
intellij-community,36036.json,84dd250e37af6f1a98d160eaae6a59022d24c7dd,"@@ -1,8 +1,11 @@
   public void revalidate(@NotNull PositionTracker<Balloon> tracker) {
+    if (ApplicationManager.getApplication().isDisposeInProgress()) {
+      return;
+    }
     RelativePoint newPosition = tracker.recalculateLocation(this);
 
     if (newPosition != null) {
       myTargetPoint = myPosition.getShiftedPoint(newPosition.getPoint(myLayeredPane), myCalloutShift);
       myPosition.updateBounds(this);
     }
   }",Buggy,"Fix for ""Application being disposed"" error
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""Fix for 'Application being disposed' error"" clearly indicates that the commit aims to resolve a bug related to the application's disposal process. This suggests a scenario where the application attempts to perform an action during or after its disposal, leading to an error.

2.  **Code Diff Analysis:** The code diff introduces a check at the beginning of the `revalidate` method:

    ```java
    if (ApplicationManager.getApplication().isDisposeInProgress()) {
      return;
    }
    ```

    This code snippet checks if the application is in the process of being disposed. If it is, the method immediately returns, effectively preventing any further actions within the `revalidate` method.

3.  **Relevance and Bug Fix Indication:** The code modification directly addresses the issue described in the commit message. By adding the `isDisposeInProgress()` check, the code prevents the `revalidate` method from executing when the application is being disposed. This likely avoids an error that was occurring due to the method attempting to access resources or perform operations during the disposal process. The addition of this check serves as a guard to prevent the error, which strongly suggests a bug fix.

**Conclusion:**

The commit message and code diff are highly relevant. The code change directly addresses the problem described in the commit message by preventing actions during application disposal. This indicates a bug fix.

**Buggy**"
intellij-community,29938.json,d5022fa9bef92fcb2e21a87b4a03cb0938a1f961,"@@ -1,12 +1,20 @@
   private void doCloseCommunication() {
     if (myPydevConsoleCommunication != null) {
-      try {
-        myPydevConsoleCommunication.close();
-        // waiting for REPL communication before destroying process handler
-        Thread.sleep(300);
-      }
-      catch (Exception e1) {
-        // Ignore
-      }
+
+      UIUtil.invokeAndWaitIfNeeded(new Runnable() {
+        @Override
+        public void run() {
+          try {
+            myPydevConsoleCommunication.close();
+            Thread.sleep(300);
+          }
+          catch (Exception e1) {
+            // Ignore
+          }
+        }
+      });
+
+      // waiting for REPL communication before destroying process handler
+
     }
   }",Buggy,"Fixed EDT bug (PY-3434).
",Buggy,"The commit message ""Fixed EDT bug (PY-3434)"" suggests a bug fix related to the Event Dispatch Thread (EDT).

The code diff shows that the `myPydevConsoleCommunication.close()` call, along with the `Thread.sleep(300)` call, which were previously executed directly, are now wrapped within `UIUtil.invokeAndWaitIfNeeded()`. This method ensures that the enclosed code is executed on the EDT.

This change strongly indicates a fix for an EDT-related issue.  The original code might have been causing exceptions or UI freezes if `myPydevConsoleCommunication.close()` or `Thread.sleep(300)` were called from a thread other than the EDT. By using `UIUtil.invokeAndWaitIfNeeded()`, the code ensures that these operations are performed on the EDT, thus addressing the potential EDT bug.

Therefore, the changes align with the commit message and indicate a bug fix.

**Buggy**
"
intellij-community,12571.json,d9a69b15763b0da7a03c07d0f9c8e5a8f7e3d7f7,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
+                if (colon != null && colon.getStartOffset() + 1 < colon.getTextRange().getEndOffset()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when indentation is wrong. (This whole thing begs to be rewritten.)
",Buggy,"The commit message indicates a fix for an assertion error that occurs when the indentation is incorrect. The code diff modifies a condition within the `appendDescriptors` method, specifically related to folding regions in Python code.

The original condition `colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()` was replaced with `colon.getStartOffset() + 1 < colon.getTextRange().getEndOffset()`.

The original condition was checking if the position after the colon is before the end of the node's text. The corrected condition checks if the position after the colon is before the end of the colon's text range.

It's highly likely that the original condition was incorrect, leading to an assertion error when the indentation was not as expected. The corrected condition seems more accurate in determining the valid folding range. This change directly addresses the issue described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,12571.json,df6834a4b25a87ff95c28760a761cae01c53874f,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null) {
+                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when a function is being defined and a colon is not yet typed in.
",Buggy,"The commit message indicates a fix for an assertion error that occurs when a colon is missing in a function definition. The code diff modifies the condition under which a folding descriptor is added for function or class declarations.

Specifically, the original code only checked if a colon exists (`colon != null`). The modified code adds an additional check: `colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()`. This additional check ensures that the colon's position is within the expected range before proceeding. This suggests that the original code might have been throwing an assertion or exception when the colon was present but not in the correct location relative to the node. The added condition prevents the creation of a folding descriptor in such cases, thus addressing the described error.

Therefore, the code change directly relates to the commit message and appears to be a bug fix.

**Buggy**
"
intellij-community,679.json,5dc1df3c0d442dd294ec203a212d97f07a9fb175,"@@ -1,80 +1,80 @@
   private static void compileCythonExtension(@NotNull Project project) {
     try {
       final RunManager runManager = RunManager.getInstance(project);
       final RunnerAndConfigurationSettings selectedConfiguration = runManager.getSelectedConfiguration();
       if (selectedConfiguration == null) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       final RunConfiguration configuration = selectedConfiguration.getConfiguration();
       if (!(configuration instanceof AbstractPythonRunConfiguration)) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       AbstractPythonRunConfiguration runConfiguration = (AbstractPythonRunConfiguration)configuration;
-      final String sdkPath = runConfiguration.getSdkHome();
+      final String interpreterPath = runConfiguration.getInterpreterPath();
       final String helpersPath = PythonHelpersLocator.getHelpersRoot().getPath();
 
       final String cythonExtensionsDir = PyDebugRunner.CYTHON_EXTENSIONS_DIR;
       final String[] cythonArgs =
         {""build_ext"", ""--build-lib"", cythonExtensionsDir, ""--build-temp"", String.format(""%s%sbuild"", cythonExtensionsDir, File.separator)};
 
       final List<String> cmdline = new ArrayList<>();
-      cmdline.add(sdkPath);
+      cmdline.add(interpreterPath);
       cmdline.add(FileUtil.join(helpersPath, FileUtil.toSystemDependentName(SETUP_CYTHON_PATH)));
       cmdline.addAll(Arrays.asList(cythonArgs));
       LOG.info(""Compile Cython Extensions "" + StringUtil.join(cmdline, "" ""));
 
       final Map<String, String> environment = new HashMap<>(System.getenv());
       PythonEnvUtil.addToPythonPath(environment, cythonExtensionsDir);
       PythonEnvUtil.setPythonUnbuffered(environment);
       PythonEnvUtil.setPythonDontWriteBytecode(environment);
-      if (sdkPath != null) {
-        PythonEnvUtil.resetHomePathChanges(sdkPath, environment);
+      if (interpreterPath != null) {
+        PythonEnvUtil.resetHomePathChanges(interpreterPath, environment);
       }
       GeneralCommandLine commandLine = new GeneralCommandLine(cmdline).withEnvironment(environment);
 
       final boolean canCreate = FileUtil.ensureCanCreateFile(new File(helpersPath));
       final boolean useSudo = !canCreate && !SystemInfo.isWindows;
       Process process;
       if (useSudo) {
         process = ExecUtil.sudo(commandLine, ""Please enter your password to compile cython extensions: "");
       }
       else {
         process = commandLine.createProcess();
       }
 
       ProgressManager.getInstance().run(new Task.Backgroundable(project, ""Compile Cython Extensions"") {
         @Override
         public void run(@NotNull ProgressIndicator indicator) {
           final CapturingProcessHandler handler =
             new CapturingProcessHandler(process, commandLine.getCharset(), commandLine.getCommandLineString());
           handler.addProcessListener(new ProcessAdapter() {
             @Override
             public void onTextAvailable(@NotNull ProcessEvent event, @NotNull Key outputType) {
               if (outputType == ProcessOutputTypes.STDOUT || outputType == ProcessOutputTypes.STDERR) {
                 for (String line : StringUtil.splitByLines(event.getText())) {
                   if (isSignificantOutput(line)) {
                     indicator.setText2(line.trim());
                   }
                 }
               }
             }
 
             private boolean isSignificantOutput(String line) {
               return line.trim().length() > 3;
             }
           });
           final ProcessOutput result = handler.runProcessWithProgressIndicator(indicator);
           final int exitCode = result.getExitCode();
           if (exitCode != 0) {
             final String message = StringUtil.isEmptyOrSpaces(result.getStdout()) && StringUtil.isEmptyOrSpaces(result.getStderr())
                                    ? ""Permission denied""
                                    : ""Non-zero exit code ("" + exitCode + ""): \n"" + result.getStderr();
             UIUtil.invokeLaterIfNeeded(() -> showErrorDialog(project, message));
           }
         }
       });
     }
     catch (IOException | ExecutionException e) {
       showErrorDialog(project, e.getMessage());
     }
   }",Buggy,"Fixes after review (PY-26101)

Fix stepping after error jump, fix tests, move code modification to an appropriate function and make it more general
",Buggy,"The commit message indicates several fixes: stepping after error jump, tests, code modification moved to an appropriate function and made more general.

The code diff focuses on the `compileCythonExtension` method. The primary change is renaming `sdkPath` to `interpreterPath`. The code now uses `runConfiguration.getInterpreterPath()` instead of `runConfiguration.getSdkHome()`. Also, `PythonEnvUtil.resetHomePathChanges` now uses `interpreterPath` instead of `sdkPath`.

This change suggests a correction in how the Python interpreter path is obtained and used, which could be related to fixing stepping after error jump or other issues related to the correct interpreter being used. The change aims to use the correct interpreter path for compiling Cython extensions. This is likely a bug fix related to incorrect path resolution.

Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,6419.json,39a18a8cfd9b96c783c9a408ad36fee10452c418,"@@ -1,24 +1,30 @@
-  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, char hostStringQuote) {
+  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, @NotNull StringNodeInfo hostStringInfo) {
     List<String> escaped = new ArrayList<>();
     for (String part : field.getAttributesAndLookups()) {
       if (part.startsWith(""."")) {
         escaped.add(part);
       }
       else if (part.startsWith(""["")) {
         if (part.contains(""\\"")) {
           return null;
         }
         final String indexText = part.substring(1, part.length() - 1);
         if (indexText.matches(""\\d+"")) {
           escaped.add(part);
           continue;
         }
-        final char quote = flipQuote(hostStringQuote);
-        if (indexText.indexOf(hostStringQuote) >= 0 || indexText.indexOf(quote) >= 0) {
-          return null;
+        final char originalQuote = hostStringInfo.getSingleQuote();
+        char targetQuote = flipQuote(originalQuote);
+        // there are no escapes inside the fragment, so the lookup key cannot contain 
+        // the host string quote unless it's a multiline string literal
+        if (indexText.indexOf(targetQuote) >= 0) {
+          if (!hostStringInfo.isTripleQuoted() || indexText.indexOf(originalQuote) >= 0) {
+            return null;
+          }
+          targetQuote = originalQuote;
         }
-        escaped.add(""["" + quote + indexText + quote + ""]"");
+        escaped.add(""["" + targetQuote + indexText + targetQuote + ""]"");
       }
     }
     return StringUtil.join(escaped, """");
   }",Buggy,"PY-21245 Handle quotes in lookup items inside multiline string literals better

Namely, allow lookup items that contain quotes in some cases if the
containing string literal is multiline, hence there is still a suitable
type of quotes to wrap injected string literal into.

The orignal error described in the issue has been fixed earlier as
part of other changes for PY-21161.
",Buggy,"Let's analyze the provided code diff and commit message to determine if it indicates a bug fix.

The commit message ""PY-21245 Handle quotes in lookup items inside multiline string literals better"" suggests that the code changes are addressing an issue related to how quotes are handled within lookup items inside multiline string literals. The message states that the changes allow lookup items containing quotes in certain cases when the string literal is multiline.

The code diff modifies the `quoteItemsInFragments` method. The key changes are:

1.  The second parameter's type changed from `char hostStringQuote` to `@NotNull StringNodeInfo hostStringInfo`. This suggests that more information about the host string is now being passed to the method.
2.  The logic for handling quotes within the index text (the part inside `[]`) has been significantly altered. It now considers whether the host string is triple-quoted (multiline) and chooses the appropriate quote character to use based on the presence of other quotes within the index text. Specifically, if the index text contains the flipped quote and the host string is *not* triple-quoted, or if the index text contains the original quote, the method returns `null`. Otherwise, it uses the original quote if the string is triple-quoted and the index text does not contain the original quote.

The original code would return `null` if the index text contained either the host string quote or its flipped quote. The modified code now allows the flipped quote if the string is triple-quoted and the original quote is not present in the index text. This suggests that the original code was too restrictive and incorrectly rejected valid lookup items in multiline strings.

The commit message also mentions that the original error described in the issue has been fixed earlier as part of other changes for PY-21161. This implies that the current commit is addressing a related, but perhaps slightly different, aspect of the same underlying problem.

Based on the commit message and the code diff, it appears that the changes are indeed addressing a bug related to quote handling in lookup items within multiline string literals. The code has been modified to correctly handle cases that were previously incorrectly rejected.

**Buggy**
"
intellij-community,3353.json,234eea4ef406b48f114d35e7385b2ebfeece9e6d,"@@ -1,10 +1,3 @@
-  private static AndroidFacet getFacet(@NotNull Module module) {
-    AndroidFacet androidFacet = null;
-
-    for (Facet facet : module.getFacets().values()) {
-      if (facet instanceof AndroidFacet) {
-        androidFacet = (AndroidFacet)facet;
-      }
-    }
-    return androidFacet;
-  }

+    public AndroidFacet getFacet() {
+      return myFacet;
+    }",Buggy,"android-jps: fix some bugs, support aapt and dex compilation
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""android-jps: fix some bugs, support aapt and dex compilation"" explicitly states that the commit includes bug fixes. This is a strong indicator that the changes are related to resolving issues.

2.  **Code Diff:** The code diff shows the removal of a `getFacet` method and the addition of another `getFacet` method. The original `getFacet` method iterated through all facets of a module to find an `AndroidFacet`. The new `getFacet` method simply returns `myFacet`.

3.  **Bug Fix Inference:** The original code's approach to finding the `AndroidFacet` by iterating through all facets suggests a potential inefficiency or a scenario where the correct facet might not be reliably found (e.g., if multiple facets of the same type exist, or if the order of facets is unpredictable). The new code directly returns `myFacet`, implying that the `AndroidFacet` is now directly accessible as a field. This change likely addresses a bug related to incorrect or unreliable facet retrieval. The change also implies that the `myFacet` field is now properly initialized and managed.

**Conclusion:**

Based on the commit message and the code diff, the changes likely represent a bug fix related to how the `AndroidFacet` is accessed. The original code's iterative approach was replaced with a direct access method, suggesting a more reliable and efficient solution.

**Buggy**
"
intellij-community,23124.json,295d3d1dd1470e62a6748a0baeded3ad4ca1d508,"@@ -1,13 +1,14 @@
-  public void beginObject() {
+  public JsonReaderEx beginObject() {
     int p = peeked;
     if (p == PEEKED_NONE) {
       p = doPeek();
     }
     if (p == PEEKED_BEGIN_OBJECT) {
       push(JsonScope.EMPTY_OBJECT);
       peeked = PEEKED_NONE;
     }
     else {
       throw createParseError(""Expected BEGIN_OBJECT but was "" + peek());
     }
+    return this;
   }",Buggy,"cleanup, fix WEB-14597 Breakpoints being ignored if functions called from live console

AsynPromise — don't ignore error if no rejected handler
",Buggy,"The commit message indicates a cleanup and a fix for a specific issue (WEB-14597) related to breakpoints being ignored in the live console when functions are called. The message also mentions a change in `AsynPromise` to not ignore errors if there's no rejected handler.

The provided code diff shows a change in the `beginObject` method of a `JsonReaderEx` class. The return type of the method is changed from `void` to `JsonReaderEx`, and `return this;` is added at the end of the method.

Reasoning:

1.  **Relevance to Commit Message:** The code diff itself doesn't directly relate to the breakpoint issue mentioned in the commit message. It's a change in a JSON reader class, which seems unrelated to debugging or live console functionality. The commit message also mentions `AsynPromise`, but the provided diff is for `JsonReaderEx`. This suggests that the provided diff is only a part of the entire commit.
2.  **Bug Fix Indication:** The change in `JsonReaderEx` doesn't inherently indicate a bug fix. Changing a method's return type and adding `return this;` is often done for method chaining or API design purposes, not necessarily to correct a bug. However, without the context of how this change is used, it's hard to definitively say if it's part of a bug fix.
3.  **Missing Context:** The provided diff is incomplete. The commit message mentions changes to `AsynPromise` which are not present in the diff.

Conclusion:

Based on the limited information, it is difficult to determine if the provided diff is directly related to the bug fix mentioned in the commit message. However, given the commit message explicitly mentions a bug fix, it's likely that other parts of the commit (not shown in the diff) address the breakpoint issue. The change in `JsonReaderEx` might be related or might be part of the ""cleanup"" mentioned in the commit message. Because the commit message explicitly mentions a bug fix, I will assume that the changes indicate a bug fix.

**Buggy**
"
intellij-community,35717.json,ac323a030f4b82bc3a41f4bb970f770de5a10462,"@@ -1,10 +1,10 @@
     private void updateFile(final VirtualFile file) {
-      myQueue.queue(new Update(""ProblemUpdate"") {
+      myQueue.queue(new Update(file) {
         public void run() {
           if (isFileOpen(file)) {
             updateFileIcon(file);
             updateFileColor(file);
           }
         }
       });
     }",Buggy,"fixed bug with file staying red underwaved in editor tab
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fixed bug with file staying red underwaved in editor tab"" clearly states that the commit addresses a bug. The bug involves a file in the editor tab incorrectly displaying a red ""underwave"" (likely indicating an error or warning) even after the issue causing the error has been resolved.

**Git Diff Analysis:**

The diff shows a change in the `updateFile` method. The key change is in the `Update` constructor:

*   **Original:** `new Update(""ProblemUpdate"")`
*   **Modified:** `new Update(file)`

The original code used a hardcoded string ""ProblemUpdate"" as the ID for the update. This likely meant that all file updates were grouped under the same update ID. The modified code uses the `VirtualFile` object itself as the update ID.

**Reasoning:**

1.  **Bug Indication:** The commit message explicitly mentions a bug fix.
2.  **Context:** The ""red underwave"" likely represents an error or warning marker in the editor. The bug is that the marker persists even after the underlying problem is resolved.
3.  **Code Change Implication:** By using the `VirtualFile` as the update ID, the code ensures that updates are specific to each file.  The original code's use of a single ""ProblemUpdate"" ID likely caused issues where updates for different files could interfere with each other, potentially preventing the red underwave from being cleared correctly for a specific file. The change ensures that each file's update is treated independently. This is a common cause of UI update issues.

**Conclusion:**

The commit message and code changes strongly suggest a bug fix. The change in the `Update` constructor likely resolves the issue of the red underwave not being cleared correctly for files in the editor tab.

**Buggy**
"
intellij-community,26650.json,aca5c269d7a12badc8892d6a204c4496583cd882,"@@ -1,4 +1,4 @@
   protected void buildPayload(Payload payload) {
     super.buildPayload(payload);
-    payload.add(myThreadId).add(myFrameId).add(""FRAME"").add(myExpression);
+    payload.add(""FRAME"").add(myExpression);
   }",Buggy,"fixed bug after refactoring
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""fixed bug after refactoring"" explicitly states that a bug was fixed. This is a strong indicator that the changes are related to a bug fix. The ""after refactoring"" part suggests the bug might have been introduced or exposed during a recent refactoring effort.

2.  **Code Diff:** The code diff shows a change in the `buildPayload` method. Specifically, the lines `payload.add(myThreadId).add(myFrameId).add(""FRAME"").add(myExpression);` have been modified to `payload.add(""FRAME"").add(myExpression);`. This means that `myThreadId` and `myFrameId` are no longer being added to the payload.

3.  **Bug Fix Inference:** The removal of `myThreadId` and `myFrameId` from the payload suggests that these values were either incorrect, unnecessary, or causing problems. It's plausible that these IDs were causing errors in the system that consumes the payload, or that they were simply redundant after the refactoring. The removal of these values directly addresses a potential issue, aligning with the commit message indicating a bug fix.

**Conclusion:**

The commit message explicitly mentions a bug fix, and the code diff shows the removal of potentially problematic data (`myThreadId` and `myFrameId`) from the payload. This strongly suggests that the changes are related to resolving a bug.

**Buggy**
"
intellij-community,15806.json,1a1325592a39001a011e5c0ece3664a582fc1fff,"@@ -1,9 +1,12 @@
-  public EditorWithProviderComposite findFileComposite(final VirtualFile file) {
+  public EditorWithProviderComposite findFileComposite(VirtualFile file) {
+    if (file instanceof BackedVirtualFile)
+      file = ((BackedVirtualFile)file).getOriginFile();
+
     for (int i = 0; i != getTabCount(); ++i) {
       final EditorWithProviderComposite editor = getEditorAt(i);
       if (editor.getFile().equals(file)) {
         return editor;
       }
     }
     return null;
   }",Buggy,"Add check of BackedVirtualFile in findFileComposite in EditorWindow.java
to fix problem with openedEditorWindow resolving.

Now if you try go to Usage of variables in Jupyter, Zeppelin Notebooks (BackedVirtualFile)
new window will be opened. We need to get original file for correct search of File Composite

GitOrigin-RevId: 5a3577cb3964fc0a7fa0c7fa91a4fed137fe2ebf",Buggy,"The commit message indicates a fix for a problem where navigating to usages of variables in Jupyter/Zeppelin notebooks (which use `BackedVirtualFile`) would open a new window instead of finding the existing one. The code change adds a check to see if the input `VirtualFile` is a `BackedVirtualFile`. If it is, it retrieves the original file using `getOriginFile()` before searching for the `EditorWithProviderComposite`. This ensures that the search is performed on the original file, thus fixing the issue described in the commit message. This is clearly a bug fix.

**Buggy**"
intellij-community,6447.json,f96c4ad9fa786eb5990bebe12f45dbbe4f698642,"@@ -1,16 +1,17 @@
   public void beforeCharDeleted(char c, PsiFile file, Editor editor) {
     isTripleQuote = false;
     if (c == '""' || c == '\'' && CodeInsightSettings.getInstance().AUTOINSERT_PAIR_QUOTE) {
       final QuoteHandler quoteHandler = TypedHandler.getQuoteHandler(file, editor);
       if (quoteHandler == null || !(quoteHandler instanceof BaseQuoteHandler)) return;
 
       final int offset = editor.getCaretModel().getCurrentCaret().getOffset();
       String text = editor.getDocument().getText();
       boolean mayBeTripleQuote = offset >= 3 && offset + 2 < text.length();
       if (mayBeTripleQuote) {
         HighlighterIterator iterator = ((EditorEx)editor).getHighlighter().createIterator(offset);
-        boolean hasTripleQuoteAfter = text.charAt(offset - 1) == c && text.charAt(offset - 2) == c && text.charAt(offset - 3) == c;
+        boolean hasTripleQuoteAfter = offset + 2 < text.length() &&
+                                      text.charAt(offset) == c && text.charAt(offset + 1) == c && text.charAt(offset + 2) == c;
         isTripleQuote = quoteHandler.isOpeningQuote(iterator, offset - 1) && hasTripleQuoteAfter;
       }
     }
   }",Buggy,"PY-1779 Fix problem with removing pair of triple quotes

Check if there is triple quote to the right of the cursor
",Buggy,"The commit message indicates a fix for removing pairs of triple quotes. The code diff modifies the `beforeCharDeleted` method, specifically focusing on handling triple quotes.

The key change is in the `hasTripleQuoteAfter` condition. The original code checked for triple quotes *before* the current offset (`offset - 1`, `offset - 2`, `offset - 3`). The corrected code checks for triple quotes *after* the current offset (`offset`, `offset + 1`, `offset + 2`). This suggests that the original logic was flawed in determining the presence of a triple quote to the right of the cursor, which is precisely what the commit message describes. The addition of `offset + 2 < text.length()` also adds a boundary check which could have caused an out of bounds exception.

Therefore, the code change directly addresses the problem described in the commit message, indicating a bug fix.

**Buggy**
"
intellij-community,11190.json,bf7f424b6599b6b662a69c63d5028263dcbee88b,"@@ -1,10 +1,10 @@
   public static String constructPythonPathCommand(Collection<String> pythonPath) {
     final String path = Joiner.on("", "").join(Collections2.transform(pythonPath, new Function<String, String>() {
       @Override
       public String apply(String input) {
-        return ""'"" + input.replace(""\\"", ""\\\\"") + ""'"";
+        return ""'"" + input.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"") + ""'"";
       }
     }));
 
     return ""sys.path.extend(["" + path + ""])"";
   }",Buggy,"fix problem with single quote in project path
http://youtrack.jetbrains.com/issue/PY-10179
",Buggy,"The commit message indicates a fix for a problem with single quotes in the project path. The code diff modifies the `constructPythonPathCommand` method, specifically the part where the input path is transformed. It adds a replacement for single quotes (`'`) with escaped single quotes (`\'`). This change directly addresses the issue described in the commit message. The original code likely failed when a project path contained a single quote, leading to incorrect Python path construction. The added escaping fixes this bug.

**Buggy**"
intellij-community,7728.json,9421b80596d7526f1637a84fb4bf457977dd635e,"@@ -1,16 +1,16 @@
     public boolean accept(File file) {
       final JavaSourceRootDescriptor rd = myBuildRootIndex.findJavaRootDescriptor(myContext, file);
       if (rd == null) {
         return true;
       }
-      final JpsModule moduleOfFile = rd.target.getModule();
-      if (myChunkModules.contains(moduleOfFile)) {
+      final ModuleBuildTarget targetOfFile = rd.target;
+      if (myChunkTargets.contains(targetOfFile)) {
         return true;
       }
-      Set<JpsModule> moduleOfFileWithDependencies = myCache.get(moduleOfFile);
-      if (moduleOfFileWithDependencies == null) {
-        moduleOfFileWithDependencies = ProjectPaths.getModulesWithDependentsRecursively(moduleOfFile, true);
-        myCache.put(moduleOfFile, moduleOfFileWithDependencies);
+      Set<BuildTarget<?>> targetOfFileWithDependencies = myCache.get(targetOfFile);
+      if (targetOfFileWithDependencies == null) {
+        targetOfFileWithDependencies = myBuildTargetIndex.getDependenciesRecursively(targetOfFile, myContext);
+        myCache.put(targetOfFile, targetOfFileWithDependencies);
       }
-      return Utils.intersects(moduleOfFileWithDependencies, myChunkModules);
+      return ContainerUtil.intersects(targetOfFileWithDependencies, myChunkTargets);
     }",Buggy,"external build: fixed problem with files remaining dirty after 'Make Project'
",Buggy,"The commit message indicates a fix for an issue where files remained dirty after a ""Make Project"" operation. This suggests a problem with the build process not correctly recognizing or handling file changes.

The code diff modifies the `accept` method, which appears to be part of a file filtering mechanism during the build process. The changes involve:

1.  Replacing `JpsModule` with `ModuleBuildTarget` and `BuildTarget<?>`. This suggests a shift towards a more generic or refined build target representation.
2.  Replacing `ProjectPaths.getModulesWithDependentsRecursively` with `myBuildTargetIndex.getDependenciesRecursively`. This indicates a change in how module dependencies are resolved, potentially addressing an issue with incomplete or incorrect dependency tracking.
3.  Replacing `Utils.intersects` with `ContainerUtil.intersects`. This is likely a utility method change, but could be related to the fix if the old utility had issues.

The changes in dependency resolution are particularly relevant to the commit message. If the build process wasn't correctly identifying all dependent files, it could lead to files remaining ""dirty"" because their changes weren't being incorporated into the build output. The switch to `myBuildTargetIndex.getDependenciesRecursively` likely fixes this by providing a more accurate and complete dependency graph.

Therefore, the code changes align with the commit message and address a bug related to file dependency tracking during the build process.

**Buggy**
"
intellij-community,10628.json,cfedbcc1be3a6aec0e05d57995ce6945e112168c,"@@ -1,15 +1,21 @@
   protected JPanel createAdvancedSettings() {
     JComponent advancedSettings = null;
     if (myProjectGenerator instanceof PythonProjectGenerator)
       advancedSettings = ((PythonProjectGenerator)myProjectGenerator).getSettingsPanel(myProjectDirectory);
     else if (myProjectGenerator instanceof WebProjectTemplate) {
       advancedSettings = ((WebProjectTemplate)myProjectGenerator).getPeer().getComponent();
     }
     if (advancedSettings != null) {
       final JPanel jPanel = new JPanel(new VerticalFlowLayout());
       final HideableDecorator deco = new HideableDecorator(jPanel, ""Mor&e Settings"", false);
+      boolean isValid = checkValid();
+      deco.setOn(!isValid);
+      if (myProjectGenerator instanceof PythonProjectGenerator && !deco.isExpanded()) {
+        final ValidationResult result = ((PythonProjectGenerator)myProjectGenerator).warningValidation(getSdk());
+        deco.setOn(!result.isOk());
+      }
       deco.setContentComponent(advancedSettings);
       return jPanel;
     }
     return null;
   }",Buggy,"fixed PY-13503 Create New Project: Expand more setting by default if there is an error in configuration
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""fixed PY-13503 Create New Project: Expand more setting by default if there is an error in configuration"" clearly states that this commit is a bug fix (indicated by the ""fixed"" keyword). The bug is related to the ""Create New Project"" functionality in the context of Python projects (PY-13503 likely refers to a specific issue tracker ID). The core of the fix is to automatically expand the ""more settings"" section if there's an error in the project configuration.

**Git Diff Analysis:**

The diff focuses on the `createAdvancedSettings()` method. Here's a breakdown of the changes:

1.  **`boolean isValid = checkValid();`**: A new boolean variable `isValid` is introduced, assigned the result of the `checkValid()` method. This suggests a validation check is being performed.
2.  **`deco.setOn(!isValid);`**: The `HideableDecorator`'s ""on"" state (which likely controls whether the advanced settings are expanded) is set to the inverse of `isValid`. This directly implements the core logic of the bug fix: if the configuration is invalid (`isValid` is false), the settings are expanded.
3.  **`if (myProjectGenerator instanceof PythonProjectGenerator && !deco.isExpanded()) { ... }`**: This conditional block specifically targets Python project generators and only executes if the decorator is *not* already expanded.
4.  **`final ValidationResult result = ((PythonProjectGenerator)myProjectGenerator).warningValidation(getSdk());`**: Inside the conditional, a `warningValidation` method is called on the `PythonProjectGenerator` to get a `ValidationResult`. This suggests a specific validation check for Python projects, potentially related to the SDK.
5.  **`deco.setOn(!result.isOk());`**: The decorator's ""on"" state is set to the inverse of `result.isOk()`. This means if the `warningValidation` returns a non-OK result (indicating a warning or error), the settings are expanded.

**Reasoning:**

The code changes directly implement the behavior described in the commit message. The `checkValid()` method likely checks for basic configuration errors. The additional check for Python projects using `warningValidation` suggests a more specific validation related to the Python SDK. The logic ensures that if either of these checks fails (indicating an error or warning), the ""more settings"" section is automatically expanded, allowing the user to see and correct the configuration issues. The commit message and the code changes are highly aligned. The changes address a specific scenario where the user needs to be prompted to configure the project properly, which is a bug fix.

**Conclusion:**

**Buggy**
"
intellij-community,35455.json,04eb55fb65bb931432ac5b51786825882d53027a,"@@ -1,21 +1,26 @@
     private boolean runNextEvent() {
       long startedAt = System.currentTimeMillis();
       final RunnableInfo lastInfo = getNextEvent(true);
       myLastInfo = lastInfo;
 
       if (lastInfo != null) {
         try {
           doRun(lastInfo);
           lastInfo.markDone();
         }
-        catch (ProcessCanceledException ignored) { }
+        catch (ProcessCanceledException ignored) {
+
+        }
         catch (Throwable t) {
+          if (ApplicationManager.getApplication().isUnitTestMode()) {
+            ExceptionUtil.rethrow(t);
+          }
           LOG.error(t);
         }
         finally {
           if (!DEBUG) myLastInfo = null;
           TransactionGuardImpl.logTimeMillis(startedAt, lastInfo.runnable);
         }
       }
       return lastInfo != null;
     }",Buggy,"make exceptions thrown in invokeLater() and friends visible in tests to help fix errors in abandoned futures

GitOrigin-RevId: aec5c2221220e3567f67c9a35d050a61f06a3a2e",Buggy,"The commit message indicates that the change aims to make exceptions thrown in `invokeLater()` and related methods visible in tests. This is intended to help fix errors in abandoned futures.

The code diff shows that within the `runNextEvent()` method, a `try-catch` block handles exceptions thrown during the execution of a `Runnable`. Specifically, it catches `ProcessCanceledException` and any other `Throwable`.

The key change is within the `catch (Throwable t)` block. It now includes a conditional statement: `if (ApplicationManager.getApplication().isUnitTestMode()) { ExceptionUtil.rethrow(t); }`. This means that if the application is running in unit test mode, any caught `Throwable` will be re-thrown. This aligns perfectly with the commit message's intention of making exceptions visible during tests. Before this change, exceptions were simply logged, making debugging more difficult.

The change directly addresses a bug-related scenario by exposing exceptions that were previously hidden, thus aiding in the identification and resolution of errors, especially in abandoned futures.

**Buggy**"
intellij-community,18837.json,7e6fe7d6c4d89fc902a471879c0244f8b19597db,"@@ -1,6 +1,8 @@
   public boolean isDumb() {
-    if (!ApplicationManager.getApplication().isReadAccessAllowed() && REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
+    if (!ApplicationManager.getApplication().isReadAccessAllowed() &&
+        Registry.is(""ide.check.is.dumb.contract"") &&
+        REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
       LOG.error(""To avoid race conditions isDumb method should be used only under read action or in EDT thread."");
     }
     return myState.get() != State.SMART;
   }",Buggy,"IDEA-CR-47405: IDEA-214525 Disable isDumb check by default

Need to fix found frequent problems and switch on it back

GitOrigin-RevId: 2a916fbfe43bb077042adf0e0515253c3aa7ce55
",Buggy,"The commit message indicates a fix for frequent problems related to the `isDumb` check. The change introduces a registry key `ide.check.is.dumb.contract` to control the execution of the check. The original code always executed the check if not in read access, potentially causing issues. The modified code adds a condition based on the registry value, effectively disabling the check by default and allowing it to be re-enabled. This aligns with the commit message's intention to disable the check and switch it back on later. The change addresses a problem related to the original implementation of the `isDumb` check.

**Buggy**"
intellij-community,2087.json,90a381b2f133397b7cbbfac2a4966e528b3e91cf,"@@ -1,10 +1,10 @@
   public boolean isAsyncAllowed() {
     final LanguageLevel languageLevel = LanguageLevel.forElement(this);
     final String functionName = getName();
 
     return languageLevel.isAtLeast(LanguageLevel.PYTHON35) && (
       functionName == null ||
-      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT) ||
+      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT, PyNames.CALL) ||
       !PyNames.getBuiltinMethods(languageLevel).containsKey(functionName)
     );
   }",Buggy,"PY-20662 Fixed: Making any magic method async displays an error, even though it is sometimes legal

Mark __call__ builtin as allowed to be async
",Buggy,"The commit message indicates a fix for a false positive error being displayed when a magic method is made async, specifically mentioning that `__call__` is now allowed to be async.

The code diff modifies the `isAsyncAllowed` method to include `PyNames.CALL` (which represents `__call__`) in the list of allowed async magic methods. This directly addresses the issue described in the commit message. The change adds `PyNames.CALL` to the `ArrayUtil.contains` check, allowing the `__call__` magic method to be async without triggering the false positive error.

This change is a bug fix because it corrects an incorrect validation that was causing an error to be displayed when it shouldn't have been.

**Buggy**"
intellij-community,10885.json,2b57b19667f12b0967107bbb6dd4052825fed484,"@@ -1,4 +1,7 @@
   public String getValue() {
+    if (myListPopUp == null || !myListPopUp.isVisible()) {
+      return null; // Nothing is selected if list is invisible
+    }
     final Object value = myList.getSelectedValue();
     return ((value == null) ? """" : getElement(value).mySuggestion.getText());
   }",Buggy,"PY-11855 Run manage.py task improvements

Tests enabled, completion bug fixed
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""PY-11855 Run manage.py task improvements"" suggests general improvements to the ""Run manage.py task"" functionality. The phrase ""Tests enabled, completion bug fixed"" explicitly states that a bug related to completion was fixed. This is a strong indicator of a bug fix.

**Git Diff Analysis:**

The diff shows a change within the `getValue()` method.  Specifically, it adds a check:

```java
if (myListPopUp == null || !myListPopUp.isVisible()) {
  return null; // Nothing is selected if list is invisible
}
```

This code checks if the `myListPopUp` is either null or not visible. If either condition is true, the method returns `null`. The comment ""// Nothing is selected if list is invisible"" explains the purpose of this check.

**Reasoning:**

The added code addresses a scenario where the list popup is not visible. Without this check, the code might have proceeded to access the selected value from a non-existent or invisible list, potentially leading to a `NullPointerException` or incorrect behavior. The commit message mentions ""completion bug fixed"", and this change likely addresses a situation where completion suggestions were being accessed even when the completion popup was not visible, leading to errors or unexpected results. The added null check prevents this from happening.

**Conclusion:**

The commit message explicitly mentions a bug fix, and the code changes introduce a null check that prevents potential errors when the list popup is not visible. This strongly suggests that the change is related to fixing a bug.

**Buggy**
"
intellij-community,38709.json,1e53ffa3e0e7fe08387b61f9aba2b48275685b41,"@@ -1,6 +1,8 @@
   public void setCancelButtonText(String text){
-    if (myDialog != null)
+    if (myDialog != null) {
       myDialog.changeCancelButtonText(text);
-    else
+    }
+    else {
       myCancelText = text;
+    }
   }",Buggy,"Fixing problem of progress window appearing over modal dialogs shown during runProcessWithProgressSynchronously().
",Buggy,"The commit message indicates a fix related to the progress window's appearance relative to modal dialogs. The code change involves a null check on `myDialog` before attempting to modify the cancel button text. If `myDialog` is null, it stores the cancel text in `myCancelText`. This suggests that the original code might have been trying to access `myDialog` when it was null, potentially causing a NullPointerException or other unexpected behavior, especially when modal dialogs were involved. The added `else` block handles the case where the dialog is not yet initialized, preventing a potential error. Therefore, the change is likely a bug fix.

**Buggy**"
intellij-community,18404.json,87fa91dd700f6b1cfc0885f0857cf501df46649f,"@@ -1,6 +1,7 @@
     private JComponent createErrorsLink() {
       IdeMessagePanel panel = new IdeMessagePanel(null, MessagePool.getInstance());
       panel.setBorder(JBUI.Borders.emptyRight(13));
+      panel.setOpaque(false);
       Disposer.register(this, panel);
       return panel;
     }",Buggy,"Fix error icon background on welcome screen

GitOrigin-RevId: 6412c1adb4627ebdd5895f3319a3878f7a68a44d
",Buggy,"The commit message ""Fix error icon background on welcome screen"" suggests a visual correction on the welcome screen, specifically related to the background of the error icon.

The provided diff introduces a single change: `panel.setOpaque(false);` within the `createErrorsLink()` method. This line sets the opacity of the `IdeMessagePanel` to false, effectively making its background transparent.

Reasoning:

1. **Alignment:** The commit message indicates a fix related to the background of an error icon. The code change sets the opacity of the panel containing the icon to `false`, which would make the background transparent. This aligns with the intent of the commit message.
2. **Bug Fix Indication:** The original background likely obscured or clashed with the welcome screen's background, creating a visual bug. Making the panel transparent resolves this visual issue.

Conclusion: **Buggy**
"
intellij-community,17737.json,29ea6cca1f95c474adfc2715fe7d091e26d043d7,"@@ -1,104 +1,109 @@
     private void processModules(final DiffState state, File fileName) {
       final Difference.Specifier<ModuleRepr, ModuleRepr.Diff> modulesDiff = state.myModulesDiff;
       if (modulesDiff.unchanged()) {
         return;
       }
+
+      for (ModuleRepr moduleRepr : modulesDiff.added()) {
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
+      }
+      
       for (ModuleRepr removedModule : modulesDiff.removed()) {
-        myDelta.addDeletedClass(removedModule, fileName);
+        myDelta.addDeletedClass(removedModule, fileName); // need this for integrate
         myPresent.affectDependentModules(state, removedModule.name, null, true);
       }
 
       for (Pair<ModuleRepr, ModuleRepr.Diff> pair : modulesDiff.changed()) {
         final ModuleRepr moduleRepr = pair.first;
         final ModuleRepr.Diff d = pair.second;
         boolean affectSelf = false;
         boolean affectDeps = false;
         UsageConstraint constraint = null;
 
-        myDelta.addChangedClass(moduleRepr.name);
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
 
         if (d.versionChanged()) {
           final int version = moduleRepr.getVersion();
           myPresent.affectDependentModules(state, moduleRepr.name, new UsageConstraint() {
             public boolean checkResidence(int dep) {
               final ModuleRepr depModule = myPresent.moduleReprByName(dep);
               if (depModule != null) {
                 for (ModuleRequiresRepr requires : depModule.getRequires()) {
                   if (requires.name == moduleRepr.name && requires.getVersion() == version) {
                     return true;
                   }
                 }
               }
               return false;
             }
           }, false);
         }
 
         final Difference.Specifier<ModuleRequiresRepr, ModuleRequiresRepr.Diff> requiresDiff = d.requires();
         for (ModuleRequiresRepr removed : requiresDiff.removed()) {
           affectSelf = true;
           if (removed.isTransitive()) {
             affectDeps = true;
             constraint = UsageConstraint.ANY;
             break;
           }
         }
         for (Pair<ModuleRequiresRepr, ModuleRequiresRepr.Diff> changed : requiresDiff.changed()) {
           affectSelf |= changed.second.versionChanged();
           if (changed.second.becameNonTransitive()) {
             affectDeps = true;
             // we could have created more precise constraint here: analyze if required module (recursively)
             // has only qualified exports that include given module's name. But this seems to be excessive since
             // in most cases module's exports are unqualified, so that any other module can access the exported API.
             constraint = UsageConstraint.ANY;
           }
         }
 
         final Difference.Specifier<ModulePackageRepr, ModulePackageRepr.Diff> exportsDiff = d.exports();
 
         if (!affectDeps) {
           for (ModulePackageRepr removedPackage : exportsDiff.removed()) {
             affectDeps = true;
             if (!removedPackage.isQualified()) {
               constraint = UsageConstraint.ANY;
               break;
             }
             for (Integer name : removedPackage.getModuleNames()) {
               final UsageConstraint matchName = UsageConstraint.exactMatch(name);
               if (constraint == null) {
                 constraint = matchName;
               }
               else {
                 constraint = constraint.or(matchName);
               }
             }
           }
         }
 
         if (!affectDeps || constraint != UsageConstraint.ANY) {
           for (Pair<ModulePackageRepr, ModulePackageRepr.Diff> p : exportsDiff.changed()) {
             final Collection<Integer> removedModuleNames = p.second.targetModules().removed();
             affectDeps |= !removedModuleNames.isEmpty();
             if (!removedModuleNames.isEmpty()) {
               affectDeps = true;
               for (Integer name : removedModuleNames) {
                 final UsageConstraint matchName = UsageConstraint.exactMatch(name);
                 if (constraint == null) {
                   constraint = matchName;
                 }
                 else {
                   constraint = constraint.or(matchName);
                 }
               }
             }
           }
         }
 
         if (affectSelf) {
           myPresent.affectModule(moduleRepr, myAffectedFiles);
         }
         if (affectDeps) {
           myPresent.affectDependentModules(state, moduleRepr.name, constraint, true);
         }
       }
     }",Buggy,"fix module-info integrate after errors (IDEA-181208)
",Buggy,"The commit message indicates a fix related to module-info integration after errors, specifically referencing IDEA-181208. The diff shows modifications within the `processModules` method. The added comments ""// need this for integrate"" suggest that these changes are essential for the integration process. The added loop for `modulesDiff.added()` and the comments indicate that the changes are related to handling errors or missing cases during module integration.

**Buggy**"
intellij-community,29881.json,89451925c29655bb8771859c792c18d6b28c9e55,"@@ -1,19 +1,19 @@
     public void visitPyFunction(final PyFunction node) {
       PyDecoratorList decolist = node.getDecoratorList();
       if (decolist != null) {
         PyDecorator[] decos = decolist.getDecorators();
         if (decos.length > 1) {
           for (int i = decos.length - 1; i >= 1; i -= 1) {
             PyDecorator deco = decos[i];
             String deconame = deco.getName();
             if ((PyNames.CLASSMETHOD.equals(deconame) || PyNames.STATICMETHOD.equals(deconame)) && deco.isBuiltin()) {
               registerProblem(
                 decos[i-1],
                 PyBundle.message(""INSP.decorator.receives.unexpected.builtin""),
-                ProblemHighlightType.GENERIC_ERROR_OR_WARNING
+                ProblemHighlightType.GENERIC_ERROR_OR_WARNING, null, new RemoveDecoratorQuickFix()
               );
             }
           }
         }
       }
     }",Buggy,"added Remove decorator quick fix to the Problematic nesting of decorators inspection
",Buggy,"The commit message indicates the addition of a quick fix, ""Remove decorator quick fix,"" to the ""Problematic nesting of decorators"" inspection. The code diff shows a modification within the `visitPyFunction` method, specifically when handling decorators in Python code. The change involves registering a problem with the `registerProblem` method. Critically, a `RemoveDecoratorQuickFix` is now passed as an argument to `registerProblem`. This strongly suggests that the inspection previously identified problematic nesting but lacked an automated way to resolve it. The addition of the quick fix addresses this, allowing users to automatically remove the offending decorator. This is a clear indication of fixing a previous limitation or deficiency in the inspection, which can be considered a bug or missing feature.

**Buggy**
"
intellij-community,27903.json,0b9404bccbc274086537d9c49bf8a31a26ba4d7a,"@@ -1,9 +1,11 @@
   private static boolean distinctTokens(@Nullable IElementType token1, @Nullable IElementType token2) {
     if (token1 == token2) return false;
     if (token1 == null || token2 == null) return true;
     if (StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token1) ||
         StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token2)) return false;
-    if (!token1.getLanguage().is(token2.getLanguage())) return true;
-    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(token1.getLanguage());
+    if (token1 != TokenType.WHITE_SPACE && token2 != TokenType.WHITE_SPACE && !token1.getLanguage().is(token2.getLanguage())) return true;
+    Language language = token1.getLanguage();
+    if (language == Language.ANY) language = token2.getLanguage();
+    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(language);
     return separator.createBorderBetweenTokens(token1, token2);
   }",Buggy,"IDEA-169157 Right-to-left bug - fix for JSX files
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""IDEA-169157 Right-to-left bug - fix for JSX files"" explicitly states that the commit addresses a bug related to right-to-left (RTL) text handling in JSX files. This strongly suggests that the code changes are intended to correct incorrect behavior.

**Code Diff Analysis:**

The code diff modifies the `distinctTokens` method, which appears to be involved in determining token boundaries, potentially for text processing or rendering purposes. Let's break down the changes:

1.  `if (token1 != TokenType.WHITE_SPACE && token2 != TokenType.WHITE_SPACE && !token1.getLanguage().is(token2.getLanguage())) return true;`

    This line adds a condition to check if both tokens are not whitespace tokens *and* if they belong to different languages. The original code directly checked language difference using `!token1.getLanguage().is(token2.getLanguage())`. The addition of the whitespace check suggests that the original logic might have incorrectly treated whitespace tokens from different languages as distinct, potentially causing issues in RTL text layout or processing.

2.  `Language language = token1.getLanguage();`
    `if (language == Language.ANY) language = token2.getLanguage();`
    `BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(language);`

    These lines introduce a fallback mechanism for determining the language when `token1`'s language is `Language.ANY`. This is important because if `token1` has no specific language associated with it, the code now attempts to use the language of `token2` to determine the appropriate `BidiRegionsSeparator`. This suggests a scenario where the language of one of the tokens was not being correctly identified, leading to incorrect RTL processing.

**Reasoning:**

The commit message indicates a bug fix related to RTL text handling in JSX files. The code changes modify the logic for determining distinct tokens, specifically addressing whitespace tokens and cases where a token's language is not explicitly defined. These changes are consistent with fixing a bug that could cause incorrect RTL text layout or processing in JSX files. The added checks and fallback mechanisms suggest that the original code had flaws in handling certain token combinations, leading to the reported bug.

**Conclusion:**

**Buggy**
"
intellij-community,35807.json,6555125d8e28088da575e0b10ca50780c50642d0,"@@ -1,22 +1,23 @@
   public static ScriptOutput executeScriptInConsoleWithFullOutput(String exePathString,
                                                                   @Nullable VirtualFile scriptFile,
                                                                   @Nullable String workingDirectory,
                                                                   long timeout,
                                                                   ScriptOutputType scriptOutputType,
                                                                   @NonNls String... parameters)
     throws ExecutionException {
     final OSProcessHandler processHandler = execute(exePathString, workingDirectory, scriptFile, parameters);
 
     final StringBuilder standardOutput = scriptOutputType.readStandardOutput() ? new StringBuilder() : null;
     final StringBuilder errorOutput = scriptOutputType.readErrorOutput() ? new StringBuilder() : null;
     final StringBuilder mergedOutput =
       (scriptOutputType.readStandardOutput() && scriptOutputType.readErrorOutput()) ? new StringBuilder() : null;
     addReadingProcessListener(scriptOutputType, processHandler, standardOutput, errorOutput, mergedOutput);
+    processHandler.startNotify();
 
     if (!processHandler.waitFor(timeout)) {
       LOG.warn(""Process did not complete in "" + timeout / 1000 + ""s"");
       throw new ExecutionException(ExecutionBundle.message(""script.execution.timeout"", String.valueOf(timeout / 1000)));
     }
     LOG.debug(""script output: "" + standardOutput);
     return new ScriptOutput(scriptOutputType, standardOutput, errorOutput, mergedOutput);
   }",Buggy,"WI-6566 Empty output in command line toolwindow:
fixed bug with too early processHandler.startNotify() in ScriptRunnerUtil; removed command execution ""cmd \c"" wrapping.
",Buggy,"The commit message indicates a fix for an issue where the command line toolwindow displayed empty output. The message mentions that the `processHandler.startNotify()` call was made too early and that the ""cmd /c"" wrapping was removed.

The diff shows that `processHandler.startNotify()` was moved up before the `processHandler.waitFor(timeout)` call. This change directly addresses the ""too early processHandler.startNotify()"" part of the commit message. Moving `startNotify` before `waitFor` ensures that the process handler is properly initialized and ready to capture output before the process starts and potentially finishes. This would prevent the scenario where the process completes before the handler is ready, thus resulting in empty output.

The commit message also mentions removing ""cmd /c"" wrapping. However, this change is not reflected in the provided diff. It's possible that this change was made in a different part of the codebase or in a separate commit.

The change in the diff directly addresses the bug described in the commit message. By starting the process notification earlier, the code ensures that the output streams are properly captured, resolving the issue of empty output in the command line toolwindow.

Therefore, the changes indicate a bug fix.

**Buggy**"
intellij-community,18229.json,6ee5fbcfe50c4e017eff321c5ebe208dd89d6945,"@@ -1,9 +1,10 @@
   public ExternalSystemTaskExecutionSettings clone() {
     ExternalSystemTaskExecutionSettings result = new ExternalSystemTaskExecutionSettings();
     result.setExternalSystemIdString(getExternalSystemIdString());
     result.setExternalProjectPath(getExternalProjectPath());
     result.setVmOptions(getVmOptions());
+    result.setScriptParameters(getScriptParameters());
     result.setTaskNames(ContainerUtilRt.newArrayList(getTaskNames()));
     result.setTaskDescriptions(ContainerUtilRt.newArrayList(getTaskDescriptions()));
     return result;
   }",Buggy,"IDEA-117792 Gradle Run/Debug Configuration: support adding Gradle specific parameters. Fix after review.
+ fix bug - http://youtrack.jetbrains.com/issue/IDEA-118470
",Buggy,"The commit message indicates a fix for IDEA-117792 and IDEA-118470, suggesting bug fixes or improvements. The code change involves adding `result.setScriptParameters(getScriptParameters());` to the `clone()` method of `ExternalSystemTaskExecutionSettings`. This addition ensures that script parameters are properly cloned when an `ExternalSystemTaskExecutionSettings` object is duplicated. The inclusion of ""fix bug"" in the commit message and the presence of a related YouTrack issue (IDEA-118470) strongly suggest that this change addresses a bug related to the incorrect cloning of script parameters.

**Buggy**
"
hector,2566.json,4e038b0c1c911e81c2ce68123f3ecdb588c0e090,"@@ -1,3 +1,3 @@
   public K getKey() {
-    return keySerializer.fromByteBuffer(entry.getKey()); 
+    return keySerializer.fromByteBuffer(entry.getKey().duplicate());
   }",Buggy,"#345: fix bug with multiple bytebuffer readings
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states ""fix bug with multiple bytebuffer readings"". This strongly suggests that the commit addresses a bug related to how the code handles `ByteBuffer` objects when they are read multiple times.

2.  **Code Diff:** The code diff shows a change in the `getKey()` method. Specifically, it modifies how the `ByteBuffer` is passed to the `keySerializer.fromByteBuffer()` method. The original code directly passed `entry.getKey()`, while the modified code passes `entry.getKey().duplicate()`.

3.  **ByteBuffer `duplicate()` Explanation:** The `duplicate()` method of `ByteBuffer` creates a new `ByteBuffer` that shares the same underlying data as the original but has its own independent position, limit, and mark. This is crucial when you need to read from the same buffer multiple times without affecting the original buffer's state. Without `duplicate()`, multiple reads might advance the position of the original buffer, leading to incorrect data being read in subsequent operations.

4.  **Bug Fix Inference:** Based on the commit message and the code change, it's highly likely that the original code had a bug where multiple calls to `getKey()` (or other methods that read from `entry.getKey()`) would result in incorrect or incomplete data being read because the `ByteBuffer`'s position was being advanced unexpectedly. The `duplicate()` call resolves this by ensuring that each read operates on a separate, independent `ByteBuffer`.

**Conclusion:**

The commit message explicitly mentions a bug fix related to `ByteBuffer` readings, and the code change introduces `duplicate()` to prevent issues with multiple reads from the same buffer. This strongly indicates that the commit addresses a bug.

**Buggy**"
hector,1056.json,77b7e5e1215b64a40f87488849cead5aa67e6ac4,"@@ -1,27 +1,30 @@
   public static void main(String[] args) throws HectorException {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
+    Keyspace keyspace = null;
     try {
-      Keyspace keyspace = client.getKeyspace(""Keyspace1"");
+      keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+    } finally {
       // This line makes sure that even if the client had failures and recovered, a correct
       // releaseClient is called, on the up to date client.
-      client = keyspace.getClient();
-    } finally {
-      // return client to pool. do it in a finally block to make sure it's executed
-      pool.releaseClient(client);
+      if (keyspace != null) {
+        client = keyspace.getClient();
+        // return client to pool. do it in a finally block to make sure it's executed
+        pool.releaseClient(client);
+      }
     }
   }",Buggy,"Fix a bug when releasing a client in the ExampleClient
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message explicitly states ""Fix a bug when releasing a client in the ExampleClient."" This strongly suggests that the changes are intended to correct a faulty behavior related to client release.

2.  **Code Diff Analysis:**
    *   The original code had a potential issue in the `finally` block. It was attempting to get the client from the `keyspace` and then release it to the pool. However, if an exception occurred *before* `keyspace` was initialized (e.g., during `client.getKeyspace(""Keyspace1"")`), `keyspace` would be null, leading to a `NullPointerException` when trying to access `keyspace.getClient()`. Also, the original code was getting the client from keyspace before returning it to the pool.
    *   The modified code addresses this by:
        *   Initializing `keyspace` to `null` before the `try` block.
        *   Adding a null check `if (keyspace != null)` inside the `finally` block to ensure that `keyspace.getClient()` and `pool.releaseClient(client)` are only called if `keyspace` was successfully initialized.

3.  **Alignment and Bug Fix Indication:** The code changes directly address the potential `NullPointerException` scenario described above. By adding the null check, the code now gracefully handles cases where `getKeyspace()` might fail, preventing the erroneous client release and potential resource leaks or connection errors. This aligns perfectly with the commit message's claim of fixing a bug related to client release. The client is now retrieved from the keyspace before being released to the pool.

**Conclusion:**

The commit message and the code diff are consistent. The changes prevent a potential `NullPointerException` and ensure proper client release, which indicates a bug fix.

**Buggy**
"
hector,1056.json,7e7fd435503f29b0bfbc6ec8a997e28ecf323a22,"@@ -1,25 +1,28 @@
   public static void main(String[] args) throws IllegalStateException, PoolExhaustedException,
       Exception {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
     try {
       Keyspace keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+      // This line makes sure that even if the client had failures and recovered, a correct
+      // releaseClient is called, on the up to date client.
+      client = keyspace.getClient();
     } finally {
       // return client to pool. do it in a finally block to make sure it's executed
       pool.releaseClient(client);
     }
   }",Buggy,"Fix ExampleClient to release the correct client even when there's possible errors and failover
(cherry picked from commit 184b6460812c21e7fde77821245848f9ed2f3e6e)
",Buggy,"The commit message indicates a fix related to releasing the correct client in `ExampleClient`, even in the presence of errors and failover scenarios. The code diff adds a line `client = keyspace.getClient();` within the `try` block. This line updates the `client` reference to the most current client associated with the `keyspace`. This update ensures that the `releaseClient` method in the `finally` block releases the correct, potentially recovered, client back to the pool. This addresses a potential bug where an outdated or faulty client might be released, leading to issues with the client pool.

Therefore, the changes indicate a bug fix.
**Buggy**
"
hector,1991.json,4be28e7d287b02b6f1036b415c5e732ec804065f,"@@ -1,14 +1,17 @@
   private String getContextPath() {
-    URL url = getClass().getClassLoader().getResource(""/"");
+    ClassLoader loader = getClass().getClassLoader();
+    if(loader == null)
+     return null;
+    URL url = loader.getResource(""/"");
     if (url != null) {
       String[] elements = url.toString().split(""/"");
       for (int i = elements.length - 1; i > 0; --i) {
         // URLs look like this: file:/.../ImageServer/WEB-INF/classes/
         // And we want that part that's just before WEB-INF
         if (""WEB-INF"".equals(elements[i])) {
           return elements[i - 1];
         }
       }
     }
     return null;
   }",Buggy,"fixed a bug which causes NullPointerException in JmxMonitor on some platforms.
",Buggy,"The commit message indicates a fix for a `NullPointerException` in `JmxMonitor`. The code diff shows a change in the `getContextPath()` method. Specifically, it adds a check to ensure that the class loader is not null before attempting to get a resource from it. If the class loader is null, the method returns null. This change directly addresses a potential `NullPointerException` that could occur if `getClass().getClassLoader()` returns null, which can happen in some environments. Therefore, the code change aligns with the commit message and fixes a bug.

**Buggy**"
hector,115.json,872da019f266973c0bf570d927371e33d838b4e8,"@@ -1,28 +1,28 @@
   public boolean addColumnToCollection(CFMappingDef<?> cfMapDef, Object obj, String colName,
       byte[] colValue) {
     // if can parse, then at least adheres to formatting
     CollectionItemColName collColumnName;
     try {
       collColumnName = parseCollectionItemColName(colName);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     // get property from mapping def - if not there, then isn't a collection
     // (but probably a problem elsewhere)
-    PropertyMappingDefinition md = cfMapDef.getPropMapByPropName(collColumnName.getPropertyName());
+    PropertyMappingDefinition md = cfMapDef.getPropMapByColumnName(collColumnName.getPropertyName());
     if (null == md) {
       return false;
     }
 
     Collection<Object> coll;
     try {
       coll = (Collection<Object>) reflectionHelper.invokeGetter(obj, md);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     Object value = deserializeCollectionValue(colValue);
     coll.add(value);
     return true;
   }",Buggy,"fix bug: when mapping java collections, if property name different from column name, would not map correctly
",Buggy,"The commit message indicates a bug fix related to mapping Java collections where the property name differs from the column name. The code diff shows a change in the `addColumnToCollection` method. Specifically, the line `PropertyMappingDefinition md = cfMapDef.getPropMapByPropName(collColumnName.getPropertyName());` is changed to `PropertyMappingDefinition md = cfMapDef.getPropMapByColumnName(collColumnName.getPropertyName());`.

This change suggests that the original code was incorrectly using the property name to retrieve the mapping definition, whereas it should have been using the column name. This aligns perfectly with the commit message, indicating that the bug was indeed related to the incorrect mapping when property and column names differed. The fix involves using `getPropMapByColumnName` instead of `getPropMapByPropName`, which directly addresses the issue described in the commit message.

**Buggy**"
hector,2740.json,bf6efaeda600325fdeed53aad52e1700a0556cfd,"@@ -1,28 +1,28 @@
   public long createClock() {
     switch (this) {
 	    case MICROSECONDS:
 	    	return System.currentTimeMillis() * ONE_THOUSAND;
 	    case MICROSECONDS_SYNC:
 	      // The following simulates a microseconds resolution by advancing a static counter
 	      // every time a client calls the createClock method, simulating a tick.
 	      long us = System.currentTimeMillis() * ONE_THOUSAND;
 	      // Synchronized to guarantee unique time within and across threads.
 	      synchronized (ClockResolution.class) {
 	         if (us > lastTime) {
 	        	 lastTime = us;
 	         } else {
 	        	 // the time i got from the system is equals or less
 	        	 // (hope not - clock going backwards)
 	        	 // One more ""microsecond""
-	        	 us = lastTime++;
+	        	 us = ++lastTime;
 	         }
 	      }
 	      return us;
 	    case MILLISECONDS:
 	      return System.currentTimeMillis();
 	    case SECONDS:
 	      return System.currentTimeMillis() / 1000;
 	}
 
     return System.currentTimeMillis();
   }",Buggy,"Fix bug. Increment lastTime first and later assign

The lastTime was getting incremented right after assigning it to us.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fix bug. Increment lastTime first and later assign"" clearly suggests that a bug related to the `lastTime` variable was present and has been addressed. The message indicates that the order of incrementing and assigning `lastTime` was incorrect.

2.  **Code Diff:** The code diff shows a change within the `MICROSECONDS_SYNC` case. Specifically, the line `us = lastTime++;` has been changed to `us = ++lastTime;`.

    *   `lastTime++` (post-increment) means the current value of `lastTime` is assigned to `us`, and then `lastTime` is incremented.
    *   `++lastTime` (pre-increment) means `lastTime` is incremented first, and then the new value of `lastTime` is assigned to `us`.

3.  **Bug Identification:** The original code `us = lastTime++;` was likely causing an issue where `us` was assigned the *previous* value of `lastTime`, and `lastTime` was only incremented *after* the assignment. This could lead to non-unique or incorrect time values being returned, especially if the clock was perceived as moving backwards. The corrected code `us = ++lastTime;` ensures that `lastTime` is incremented *before* its value is assigned to `us`, which should resolve the issue of potentially returning the same time twice or a time in the past.

4.  **Relevance:** The code change directly reflects the commit message. The commit message states that the increment should happen first, and the code change modifies the increment operation to be a pre-increment, thus incrementing before assignment.

**Conclusion:**

The commit message and the code diff are consistent and indicate a bug fix related to the order of incrementing and assigning the `lastTime` variable. Therefore, the change is a bug fix.

**Buggy**"
hector,52.json,d0401ba77a823d118c98a74db02678c948cf33bc,"@@ -1,16 +1,26 @@
   private byte[] generateColumnFamilyKeyFromPkObj(CFMappingDef<?> cfMapDef, Object pkObj) {
     List<byte[]> segmentList = new ArrayList<byte[]>(cfMapDef.getKeyDef().getIdPropertyMap().size());
-
+    
+    List<String> rm1 = new ArrayList<String>();
+    List<String> rm2 = new ArrayList<String>();
+    
     if (cfMapDef.getKeyDef().isComplexKey()) {
-      for (PropertyDescriptor pd : cfMapDef.getKeyDef().getPropertyDescriptorMap().values()) {
-        segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
-            new DefaultConverter()));
+    	
+      Map<String, PropertyDescriptor> propertyDescriptorMap = cfMapDef.getKeyDef().getPropertyDescriptorMap(); 	
+      Map<String, PropertyMappingDefinition> idPropertyMap =  cfMapDef.getKeyDef().getIdPropertyMap();
+    	
+      for (String key : cfMapDef.getKeyDef().getIdPropertyMap().keySet()) {
+    	  PropertyDescriptor pd = propertyDescriptorMap.get(key);
+    	  segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
+    	            new DefaultConverter()));
       }
+     
+    	 
     } else {
       PropertyMappingDefinition md = cfMapDef.getKeyDef().getIdPropertyMap().values().iterator()
                                              .next();
       segmentList.add(md.getConverter().convertObjTypeToCassType(pkObj));
     }
 
     return keyConcatStrategy.concat(segmentList);
   }",Buggy,"Fix for unordered Composite Key when finding values by key

A problem occurred when you attempted to find an object by composite
key.  The composite would be persisted in order by the order of the
composite fields in the class.  When attempting to find the object the
objectMapper would use the natural ordering of the field names instead.
",Buggy,"The commit message describes a bug where composite keys were not being used in the correct order when finding objects. The code changes modify the way the `generateColumnFamilyKeyFromPkObj` method constructs the key. Specifically, instead of iterating over the `PropertyDescriptor` map directly, the code now iterates over the keys of the `idPropertyMap` and uses those keys to retrieve the `PropertyDescriptor` from the `propertyDescriptorMap`. This ensures that the properties are processed in the order defined in the `idPropertyMap`, which should be the correct order for the composite key. This aligns with the commit message, which states that the object mapper was using the natural ordering of the field names instead of the order in which they were persisted.

**Buggy**"
hector,2393.json,befd460c79a1d16c64a1a9857678621106e4a111,"@@ -1,13 +1,16 @@
-        public boolean hasNext()
-        {
-            boolean retval = false;
-            if (isStart)
-            {
-                retval = res.hasResults();
-            }
-            else
-            {
-                retval = res.hasNext();
-            }
-            return retval;
-        }

+		public boolean hasNext() 
+		{
+			boolean retval = false;
+			if (isStart) 
+			{
+				if(res.hasResults() || res.hasNext()) 
+				{
+					retval = true; 
+				}
+			} 
+			else 
+			{
+				retval = res.hasNext();
+			}
+			return retval;
+		}",Buggy,"Fix bug in hasNext when no result for a key is returned
",Buggy,"The commit message indicates a fix for a bug in the `hasNext()` method when no result is returned for a key.

The original code's `hasNext()` method checks `res.hasResults()` only when `isStart` is true, and `res.hasNext()` otherwise. The modified code introduces a change within the `if (isStart)` block. It now checks if either `res.hasResults()` or `res.hasNext()` is true. This suggests that the original code might have incorrectly returned `false` even when there were more results available (accessible via `res.hasNext()`) after the initial results (`res.hasResults()`) were processed when `isStart` is true. The change ensures that `hasNext()` returns true if either `res.hasResults()` or `res.hasNext()` is true when `isStart` is true. This aligns with the commit message's intention to fix a bug related to `hasNext()` when no result is returned for a key.

**Buggy**"
cucumber-jvm,1443.json,5f8f31b3d2b34050a93e27990ed25ce9274f49ac,"@@ -1,22 +1,23 @@
     public void newWorld() {
+        stepDefinitions.clear();
         MutablePicoContainer pico = new DefaultPicoContainer();
         for(Class stepsClass : stepsClasses) {
             pico.addComponent(stepsClass);
         }
 
         for(Object stepObject : pico.getComponents()) {
             for (Method method : stepObject.getClass().getMethods()) {
                 String regexpString = null;
                 if (method.isAnnotationPresent(Given.class)) {
                     regexpString = method.getAnnotation(Given.class).value();
                 } else if (method.isAnnotationPresent(When.class)) {
                     regexpString = method.getAnnotation(When.class).value();
                 } else if (method.isAnnotationPresent(Then.class)) {
                     regexpString = method.getAnnotation(Then.class).value();
                 }
                 if(regexpString != null) {
                     stepDefinitions.add(new StepDefinition(regexpString, stepObject, method));
                 }
             }
         }
     }",Buggy,"Fix Ambiguous problem for Cucumber Java
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix Ambiguous problem for Cucumber Java"" strongly suggests that the commit addresses a bug related to ambiguity in Cucumber Java step definitions. This typically arises when multiple step definitions match the same step text, leading to Cucumber not knowing which step definition to execute.

**Code Diff Analysis:**

The code diff primarily focuses on the `newWorld()` method. The key change is the addition of `stepDefinitions.clear();` at the beginning of the method.

**Reasoning:**

1.  **Ambiguity and Step Definitions:** The ""Ambiguous problem"" in Cucumber usually occurs because the `stepDefinitions` list contains multiple matching step definitions for a given step.

2.  **`stepDefinitions.clear();`**: By clearing the `stepDefinitions` list at the start of the `newWorld()` method, the code ensures that the list is fresh and only contains the step definitions from the *current* set of step classes. Without this, previous step definitions might linger, causing ambiguity if they happen to match steps in the current scenario.

3.  **Bug Fix Indication:** The addition of `stepDefinitions.clear();` directly addresses the potential for duplicate or outdated step definitions causing ambiguity. This strongly suggests a bug fix. The previous implementation was likely accumulating step definitions across multiple calls to `newWorld()`, leading to the ""Ambiguous problem"".

**Conclusion:**

The code change directly relates to the commit message and resolves a likely cause of ambiguity in Cucumber step definitions. Therefore, the commit represents a bug fix.

**Buggy**
"
cucumber-jvm,1018.json,d8406dd1cbd14b82a79ccf5205f4367c9883a768,"@@ -1,10 +1,10 @@
         private String calculateElementName(cucumber.api.TestCase testCase) {
             String testCaseName = testCase.getName();
             if (testCaseName.equals(previousTestCaseName)) {
-                return testCaseName + (includesBlank(testCaseName) ? "" "" : ""_"") + ++exampleNumber;
+                return Utils.getUniqueTestNameForScenarioExample(testCaseName, ++exampleNumber);
             } else {
                 previousTestCaseName = testCase.getName();
                 exampleNumber = 1;
                 return testCaseName;
             }
         }",Buggy,"Android: Fix Cucumber execution on Gradle (#1094)

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

This is a re-integration of PR-1094. The original pull request code was performed on 1.2.6-SNAPSHOT base. This code bases on version 2.3.2-SNAPSHOT.

* Update Cukeulator example project to work with newest Android build tools

- Update to Android Studio 3.0.1, SDK 26+ and Gradle 4.1
- Replace Instrumentation class by CucumberRunner
- CucumberRunner uses AndroidJUnitRunner (JUnit 4+)
- CalculationSteps class uses ActivityTestRule instead of deprecated ActivityInstrumentationTestCase2
- Fix permissions to write reports on internal storage

* Improve Cukeulator example project

* Update README.md
* Enable local Maven dependencies for better development experience
* Describe, how to use Cukeulator example project with locally built Cucumber-JVM

* Rename duplicated test case names in AndroidInstrumentationReporter like JUnitFormatter in cucumber-core

* Fix typo

* Improve readability of AndroidInstrumentationReporterTest

* Share common logic for test case name between JUnitFormatter and AndroidInstrumentationReporter

* Improve code quality

- Create merged method calculateUniqueTestName from getUniqueTestName and ensureUniqueTestName and make it private
- Use better test method name: test_case_names_are_unique_on_equal_scenario_names (instead of scenario_outline_all_test_names_unique)
- Refactor test code: now it should be readable

* Change misleading variable names

* [Android] Split up test of making test names unique.

To provide better documentation of the functionality, that is:
* test names within feature are made unique by appending blank and
  number
* test names within are made unique by appending underscore and number
  when no blank in name
* test names in different features can be the same
* test names are made unique also when not consecutive
",Buggy,"The commit message indicates a bug fix related to non-unique test names in Android/Gradle builds when using Cucumber. The diff shows a change in the `calculateElementName` method where, if the test case name is the same as the previous one, a unique identifier is appended. The original code appended either a space or an underscore followed by a number. The modified code now uses `Utils.getUniqueTestNameForScenarioExample` to generate the unique test name. This change directly addresses the issue of non-unique test names, as highlighted in the commit message. Therefore, the changes indicate a bug fix.

**Buggy**"
hazelcast,11513.json,7a17eba9e37e1ce2953f5320c56c13c92620953d,"@@ -1,12 +1,11 @@
     public boolean shouldWait() {
         WriteBehindQueue<DelayedEntry> writeBehindQueue = store.getWriteBehindQueue();
-        int size = writeBehindQueue.size();
-        if (size == 0) {
+        DelayedEntry entry = writeBehindQueue.peek();
+        if (entry == null) {
             return false;
         }
 
-        DelayedEntry entry = writeBehindQueue.peek();
         long currentSequence = entry.getSequence();
-        return entry != null && currentSequence <= this.sequence
-                && size + currentSequence - 1 >= this.sequence;
+        return currentSequence <= this.sequence
+                && writeBehindQueue.size() + currentSequence - 1 >= this.sequence;
     }",Buggy,"Fixes findbugs problem
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixes findbugs problem"" is quite vague but explicitly states that the commit addresses a problem identified by FindBugs (or a similar static analysis tool). This suggests a potential bug fix, as FindBugs typically flags code that is likely to cause errors or is written in a suboptimal way.

**Code Diff Analysis:**

The code diff modifies the `shouldWait()` method, which appears to be related to a write-behind queue. Let's break down the changes:

1. **Original Code:**
   ```java
   int size = writeBehindQueue.size();
   if (size == 0) {
       return false;
   }

   DelayedEntry entry = writeBehindQueue.peek();
   long currentSequence = entry.getSequence();
   return entry != null && currentSequence <= this.sequence
           && size + currentSequence - 1 >= this.sequence;
   ```

2. **Modified Code:**
   ```java
   DelayedEntry entry = writeBehindQueue.peek();
   if (entry == null) {
       return false;
   }

   long currentSequence = entry.getSequence();
   return currentSequence <= this.sequence
           && writeBehindQueue.size() + currentSequence - 1 >= this.sequence;
   ```

**Reasoning:**

*   **Potential NullPointerException:** In the original code, `entry` was retrieved *after* checking if the queue `size` was zero. However, if the queue was empty, `writeBehindQueue.peek()` would return `null`, and the subsequent call to `entry.getSequence()` would throw a `NullPointerException`. The modified code addresses this by checking if `entry` is `null` *before* accessing its `sequence`.
*   **Redundant Size Check:** The original code checks `size == 0` and returns `false`. The modified code checks `entry == null` and returns `false`. Since `peek()` returns `null` if the queue is empty, the `size` check was redundant.
*   **FindBugs and NullPointerException:** FindBugs is known to flag potential `NullPointerException` issues. The original code had a clear risk of throwing a `NullPointerException` if the queue was empty.

**Conclusion:**

The code diff directly addresses a potential `NullPointerException`, which aligns with the commit message indicating a fix for a FindBugs problem. The changes improve the code's robustness and prevent a potential runtime error.

**Buggy**
"
hazelcast,27666.json,515671c5314975716054e45c0866ffd8be64f4c2,"@@ -1,17 +1,17 @@
     private void updateConfig(AliasedDiscoveryConfig config, Node node) {
         NamedNodeMap attributes = node.getAttributes();
         for (int a = 0; a < attributes.getLength(); a++) {
             Node att = attributes.item(a);
             String value = getTextContent(att).trim();
             if (""enabled"".equals(lowerCaseInternal(att.getNodeName()))) {
                 config.setEnabled(getBooleanValue(value));
             } else if (att.getNodeName().equals(""connection-timeout-seconds"")) {
                 config.setProperty(""connection-timeout-seconds"", value);
             }
         }
         for (Node n : childElements(node)) {            
-            String key = cleanNodeName(n, ""eureka"".equals(n.getParentNode().getLocalName()));
+            String key = cleanNodeName(n, !""eureka"".equals(n.getParentNode().getLocalName()));
             String value = getTextContent(n).trim();
             config.setProperty(key, value);
         }
     }",Buggy,eureka lowercase bug fix,Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""eureka lowercase bug fix"" suggests that the commit addresses an issue related to handling lowercase characters, specifically within the context of ""eureka."" This implies that there was likely a case-sensitivity problem that needed correction.

**Git Diff Analysis:**

The diff shows changes within the `updateConfig` method. Let's break down the relevant parts:

*   **Line 6:** `if (""enabled"".equals(lowerCaseInternal(att.getNodeName())))`

    This line suggests that the code is now explicitly converting the node name to lowercase using `lowerCaseInternal()` before comparing it to ""enabled"". This confirms the commit message's implication of addressing a case-sensitivity issue.
*   **Line 12:** `String key = cleanNodeName(n, !""eureka"".equals(n.getParentNode().getLocalName()));`

    The change here involves the condition passed to the `cleanNodeName` method. The original code had ` ""eureka"".equals(n.getParentNode().getLocalName())`, and it was changed to `!""eureka"".equals(n.getParentNode().getLocalName())`. This is a logical negation. The original code was likely intended to perform some cleaning when the parent node's local name *was* ""eureka"", but the condition was inverted. This correction suggests a bug where the cleaning was not happening when it should have, or was happening when it shouldn't have.

**Reasoning:**

1.  The commit message explicitly mentions a ""lowercase bug fix,"" and the diff confirms that case-insensitive comparison is being enforced.
2.  The logical negation in the second change strongly suggests a bug fix. The original condition was likely incorrect, leading to unintended behavior. The corrected condition ensures that the `cleanNodeName` method is called under the correct circumstances.

**Conclusion:**

Based on the commit message and the code changes, it's highly likely that this commit addresses a bug related to case sensitivity and an incorrect logical condition. Therefore, the answer is:

**Buggy**
"
hazelcast,16541.json,6ce43d4090fe4924dfc7dc45583a328e25bb1e9f,"@@ -1,14 +1,15 @@
     public void run() throws Exception {
         CollectionWrapper wrapper = getCollectionWrapper();
-        if (wrapper == null || wrapper.getVersion() == version){
+        if (wrapper == null || wrapper.getVersion() != version){
             notify = false;
             return;
         }
+        wrapper.incrementAndGetVersion();
         for (Operation op: opList){
             op.setNodeEngine(getNodeEngine()).setServiceName(getServiceName()).setPartitionId(getPartitionId());
             op.beforeRun();
             op.run();
             op.afterRun();
         }
         getOrCreateContainer().unlock(dataKey, getCallerUuid(), threadId);
     }",Buggy,"Multimap commit txn version bug fixed
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Analyze the Commit Message:**

The commit message ""Multimap commit txn version bug fixed"" clearly states that the commit is intended to fix a bug related to transaction versions in the context of a Multimap.

**2. Analyze the Code Diff:**

*   **Line 3: `if (wrapper == null || wrapper.getVersion() != version)`**
    *   The original code likely had `wrapper.getVersion() == version`. The change modifies this condition to `wrapper.getVersion() != version`. This suggests that the original code was incorrectly preventing the execution of the subsequent operations when the versions were equal, which is likely the bug.
*   **Line 7: `wrapper.incrementAndGetVersion();`**
    *   This line increments the version of the wrapper. This is a crucial part of the fix, as it ensures that the version is updated after the operations are performed, preventing the same operations from being re-applied if the transaction is retried.

**3. Reasoning:**

The combination of the commit message and the code changes strongly suggests a bug fix. The original code had a faulty version check (`==` instead of `!=`), which likely prevented the operations from being executed correctly in certain scenarios. The fix corrects the version check and increments the version after the operations, ensuring that the transaction is handled correctly. The increment of the version is a critical part of the fix, as it prevents the same operations from being re-applied if the transaction is retried.

**Conclusion:**

**Buggy**
"
hazelcast,28391.json,8a5e458780154dfd44ef41dd02770456c19c7dbf,"@@ -1,4 +1,13 @@
     public final void sendResponse(Object value) {
         OperationResponseHandler responseHandler = getOperationResponseHandler();
-        responseHandler.sendResponse(this, value);
+        if (responseHandler == null) {
+            if (value instanceof Throwable) {
+                // in case of a throwable, we want the stacktrace.
+                getLogger().warning(""Missing responseHandler for "" + toString(), (Throwable) value);
+            } else {
+                getLogger().warning(""Missing responseHandler for "" + toString() + "" value["" + value + ""]"");
+            }
+        } else {
+            responseHandler.sendResponse(this, value);
+        }
     }",Buggy,"NPE on operation sendResponse if no response handler is set

The problem is fixed by adding a check if the response handler is set.

If it isn't set a warning is printed so we don't loose the exception. It is better
that a response handler is always set to prevent loosing any track of problematic flows.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs when an `OperationResponseHandler` is not set before calling `sendResponse`. The code diff adds a null check for `responseHandler` before attempting to use it. If `responseHandler` is null, a warning message is logged, including the stack trace if the `value` is a `Throwable`. This prevents the NPE and provides a warning message to indicate the missing handler. The change directly addresses the problem described in the commit message.

Reasoning:

1.  **Problem Identification:** The commit message clearly states the problem: an NPE when `responseHandler` is null.
2.  **Code Solution:** The code adds a null check for `responseHandler`. This directly addresses the problem.
3.  **Error Handling:** Instead of throwing an NPE, the code now logs a warning message, which is a better way to handle the error, as suggested in the commit message.
4.  **Relevance:** The code change is directly relevant to the commit message and fixes the described bug.

Conclusion: **Buggy**
"
hazelcast,21845.json,7ab1db9edc8409f599bf01054636a1643e3cf2d3,"@@ -1,3 +1,11 @@
     public double getRatio() {
-        return ((double) hits / misses) * 100.0;
+        if (misses == 0) {
+            if (hits == 0) {
+                return Double.NaN;
+            } else {
+                return Double.POSITIVE_INFINITY;
+            }
+        } else {
+            return ((double) hits / misses) * PERCENTAGE;
+        }
     }",Buggy,"Fixed checkstyle issue and divide by zero problem for ratio in ""NearCacheStatsImpl""",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed checkstyle issue and divide by zero problem for ratio in ""NearCacheStatsImpl"""" explicitly states that the commit addresses a ""divide by zero problem"" in the `NearCacheStatsImpl` class, specifically related to the `ratio`. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `getRatio()` method. The original code `return ((double) hits / misses) * 100.0;` directly divides `hits` by `misses` without any checks. The modified code introduces a conditional check for `misses == 0`.

-   If `misses` is 0, it further checks if `hits` is also 0. If both are 0, it returns `Double.NaN` (Not a Number). If `misses` is 0 and `hits` is not 0, it returns `Double.POSITIVE_INFINITY`.
-   If `misses` is not 0, it proceeds with the original calculation `((double) hits / misses) * PERCENTAGE;` (where PERCENTAGE is likely 100.0).

This change directly addresses the potential `ArithmeticException` that would occur when dividing by zero. The added conditional logic handles the edge cases where `misses` is zero, preventing the application from crashing or producing incorrect results.

**Relevance and Conclusion:**

The commit message and the code changes are highly relevant. The code modification directly implements a fix for the ""divide by zero problem"" mentioned in the commit message. The added conditional logic is a standard way to handle such situations, ensuring the stability and correctness of the application. This is a clear indication of a bug fix.

**Buggy**
"
hazelcast,22872.json,0fd3a71c944b64d8b3a3ad75361015fb289be227,"@@ -1,4 +1,4 @@
     protected DiagnosticsLogWriter write(String s) {
-        printWriter.write(s);
+        printWriter.write(s == null ? ""null"" : s);
         return this;
     }",Buggy,"Fixed NPE in Diagnostics

Fix #9085

The problem was that the printWriter was called with a print(null). So the fix is to do a simple null check and write
""null"" in case of null.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the `DiagnosticsLogWriter` class. The issue arose when the `printWriter` was called with a `null` value, leading to the exception. The proposed solution involves adding a null check before writing to the `printWriter`. If the input string `s` is `null`, the code now writes the string ""null"" instead.

The provided Git diff confirms this fix. The line `printWriter.write(s);` has been replaced with `printWriter.write(s == null ? ""null"" : s);`. This change directly addresses the reported NPE by handling null input gracefully. The code now explicitly checks for null and substitutes it with the string ""null"", preventing the `printWriter` from receiving a null argument.

The commit message and the code changes are consistent, and the changes clearly indicate a bug fix.

**Buggy**"
hazelcast,10446.json,cd1d3e928d20e4ec4cc22d0cc91d93c2b6600c9e,"@@ -1,3 +1,3 @@
     public Object getKey() {
-        return key;
+        return new MapKey(name, key);
     }",Buggy,"map txn bug fix: same key overrides txn op with same key but different map name
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""map txn bug fix: same key overrides txn op with same key but different map name"" suggests a bug related to how map transactions handle keys. Specifically, it indicates that operations with the same key but for different map names were being incorrectly overridden, implying a collision or incorrect key comparison logic.

**Git Diff Analysis:**

The diff shows a change in the `getKey()` method.  Instead of returning the raw `key`, it now returns a `new MapKey(name, key)`. This strongly suggests that the original implementation of `getKey()` was only returning the key's value, without considering the map's name. This would indeed cause the described issue where operations on different maps with the same key would collide. The introduction of `MapKey` likely encapsulates both the map name and the key value, providing a composite key for accurate identification.

**Reasoning:**

1.  **Alignment:** The code change directly addresses the problem described in the commit message. The original `getKey()` implementation would lead to key collisions across different maps, which is exactly what the commit message describes.
2.  **Bug Fix Indication:** The change is designed to correct incorrect behavior. The original code was flawed in its key generation, leading to unintended overwrites. The new code fixes this by creating a composite key that includes the map name.
3.  **Error Handling/Logical Correction:** This is a logical correction. The original logic for generating keys was insufficient, leading to incorrect behavior. The new logic corrects this by including the map name in the key.

**Conclusion:**

The commit message and the code change are highly relevant and indicate a bug fix. The original code had a flaw in its key generation logic, leading to incorrect behavior. The new code corrects this flaw.

**Buggy**
"
hazelcast,6968.json,0ae63554413e19520e136b5a35fec3c05d801866,"@@ -1,4 +1,6 @@
     private void createCachesOnCluster() {
         ClientCacheProxyFactory proxyFactory = (ClientCacheProxyFactory) getClientProxyFactory(ICacheService.SERVICE_NAME);
-        proxyFactory.recreateCachesOnCluster();
+        if (proxyFactory != null) {
+            proxyFactory.recreateCachesOnCluster();
+        }
     }",Buggy,"Fix NPE when jcache not in classpath

fixing a bug recently introduced in #13810

fixes #13851
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states ""Fix NPE when jcache not in classpath"". NPE stands for NullPointerException, which is a common type of bug.
    *   It also mentions ""fixing a bug recently introduced"" and ""fixes #13851"", further suggesting a bug fix.

2.  **Diff Analysis:**
    *   The diff shows a change in the `createCachesOnCluster` method.
    *   A null check `if (proxyFactory != null)` has been added before calling `proxyFactory.recreateCachesOnCluster()`.
    *   This addition directly addresses the potential for a NullPointerException if `proxyFactory` is null. The commit message indicates that this scenario occurs when jcache is not in the classpath, implying that `getClientProxyFactory` returns null in this case.

3.  **Alignment and Conclusion:**

The commit message and the code changes are perfectly aligned. The code modification introduces a null check to prevent a NullPointerException, which is exactly what the commit message describes. This strongly suggests that the change is a bug fix.

**Conclusion:**

**Buggy**
"
hazelcast,29434.json,f701a369208b2acf3066ba1b306d68688a765a56,"@@ -1,7 +1,9 @@
     public void populate(LiveOperations liveOperations) {
         for (Queue<ParkedOperation> parkQueue : parkQueueMap.values()) {
-            for (ParkedOperation op : parkQueue) {
-                liveOperations.add(op.getCallerAddress(), op.getCallId());
+            for (ParkedOperation parkedOperation : parkQueue) {
+                // we need to read out the data from the BlockedOperation; not from the ParkerOperation-container.
+                Operation operation = parkedOperation.getOperation();
+                liveOperations.add(operation.getCallerAddress(), operation.getCallId());
             }
         }
     }",Buggy,"Fix heartbeat problem for BlockingOperations

The problem is that the ParkedOperation; the container around the BlockingOperation; is
asked for callid/calleraddress. But this container object doesn't have any sensible
information, so the heartbeat is not constructed correctly for a blocked Operation.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states a ""heartbeat problem"" related to `BlockingOperations`. It identifies that the `ParkedOperation` container was being incorrectly used to retrieve `callid` and `calleraddress` for heartbeat construction. The message suggests that `ParkedOperation` doesn't hold the correct information, leading to an incorrect heartbeat.

2.  **Diff Analysis:**
    *   The code iterates through `ParkedOperation` objects within queues.
    *   The original code directly used `op.getCallerAddress()` and `op.getCallId()` from the `ParkedOperation` instance.
    *   The modified code introduces a crucial change: It now retrieves the actual `Operation` from the `ParkedOperation` using `parkedOperation.getOperation()`. Then, it extracts `callerAddress` and `callId` from this `Operation` object.
    *   The comment `// we need to read out the data from the BlockedOperation; not from the ParkerOperation-container.` clearly explains the intent of the change: to get the correct data from the underlying `Operation` instead of the container.

3.  **Alignment and Bug Fix Indication:** The code change directly addresses the problem described in the commit message. The original code was fetching heartbeat-related data from the wrong object (`ParkedOperation`), leading to incorrect heartbeat construction. The corrected code fetches the data from the correct object (`Operation`), which should resolve the heartbeat issue. This strongly suggests a bug fix.

**Conclusion:**

**Buggy**
"
hazelcast,17124.json,fc8ec0a85ea7e388794baf5c7710ec824b6cb10c,"@@ -1,20 +1,27 @@
     public void close() throws IOException {
         if (!CLOSED.compareAndSet(this, FALSE, TRUE)) {
             return;
         }
 
+        // we execute this in its own try/catch block because we don't want to skip closing the socketChannel in case
+        // of problems.
+        try {
+            onClose();
+        } catch (Exception e) {
+            getLogger().severe(format(""Failed to call 'onClose' on channel [%s]"", this), e);
+        }
+
         try {
             socketChannel.close();
         } finally {
             for (ChannelCloseListener closeListener : closeListeners) {
                 // it is important we catch exceptions so that other listeners aren't obstructed when
                 // one of the listeners is throwing an exception.
                 try {
                     closeListener.onClose(this);
                 } catch (Exception e) {
-                    ILogger logger = Logger.getLogger(getClass());
-                    logger.severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
+                    getLogger().severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
                 }
             }
         }
     }",Buggy,"Fix AbstractChannel.close forwarding to reader/writer

A bug was introduced by me in the 3.9 io changes where the forwarding of
the close to the underlying channel reader/writer is skipped.

This pr restores these closes by adding a template method to the AbstractChannel
which is implemented by the NioChannel.
",Buggy,"The commit message indicates a bug fix related to the `close()` method in `AbstractChannel`. The bug caused the forwarding of the close operation to the underlying channel reader/writer to be skipped.

The code diff introduces an `onClose()` method call within a try-catch block before closing the `socketChannel`. This `onClose()` method is likely responsible for closing the reader/writer, and the try-catch block ensures that any exceptions during this process don't prevent the `socketChannel` from being closed. The diff also includes logging for exceptions thrown by `onClose()` and `closeListeners`.

The changes align with the commit message, restoring the intended behavior of closing the underlying reader/writer. The addition of the `onClose()` method and the surrounding try-catch block directly addresses the bug described in the commit message.

**Buggy**"
hazelcast,12126.json,8bbe3fbfaa5e85b1e504100aa7d4c425970dee3d,"@@ -1,3 +1,5 @@
     public boolean eval(Object arg) {
-        return (key == null || key.equals(arg)) && predicate.apply((Map.Entry)arg);
+        final QueryEntry entry = (QueryEntry) arg;
+        final Data keyData = entry.getKeyData();
+        return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry)arg);
     }",Buggy,"MapAddListenerRequest serialization bug fix and related to that  QueryEventFilter bug fix
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""MapAddListenerRequest serialization bug fix and related to that QueryEventFilter bug fix"" explicitly states that the commit includes bug fixes, specifically mentioning a serialization bug in `MapAddListenerRequest` and a related bug in `QueryEventFilter`.

**Git Diff Analysis:**

The diff shows a change within the `eval` method of what appears to be a `QueryEventFilter` class. Let's break down the changes:

*   **Original Code:**
    ```java
    return (key == null || key.equals(arg)) && predicate.apply((Map.Entry)arg);
    ```

*   **Modified Code:**
    ```java
    final QueryEntry entry = (QueryEntry) arg;
    final Data keyData = entry.getKeyData();
    return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry)arg);
    ```

The original code directly compares `key` with `arg`. The modified code introduces an intermediate step:

1.  Casts `arg` to `QueryEntry`.
2.  Retrieves `keyData` from the `QueryEntry`.
3.  Compares `key` with `keyData`.

**Reasoning:**

The change suggests that the original code was incorrectly comparing `key` with the entire `QueryEntry` object instead of the actual key data within the entry. This would lead to incorrect filtering results, which constitutes a bug. The introduction of `keyData` and its retrieval from `QueryEntry` indicates a correction to this incorrect comparison. The commit message mentions a `QueryEventFilter` bug fix, which aligns perfectly with the code changes observed in the `eval` method. The change ensures that the comparison is done with the actual key data, which is essential for the correct functioning of the filter.

**Conclusion:**

The commit message explicitly mentions bug fixes, and the code changes in the `QueryEventFilter` class confirm that a bug related to key comparison has been addressed. Therefore, the changes indicate a bug fix.

**Buggy**"
hazelcast,12000.json,777bd5c28ac36f1b948f94a68b21328e3743e877,"@@ -1,11 +1,10 @@
     public static int getMaxSizePerNode(MaxSizeConfig maxSizeConfig) {
-
-        int maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
+        double maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
 
         if (maxSizePerNode == MaxSizeConfig.DEFAULT_MAX_SIZE) {
             // unlimited
             return -1;
         }
 
-        return maxSizePerNode;
+        return (int) maxSizePerNode;
     }",Buggy,"Fixed getApproximateMaxSize calculation bug
",Buggy,"The commit message indicates a bug fix related to the `getApproximateMaxSize` calculation. The diff shows a change in the return type of `getApproximateMaxSize` from `int` to `double` and a casting to `int` before returning. This suggests that the original calculation might have resulted in a loss of precision or an incorrect value due to integer truncation, which is now addressed by using a double for intermediate calculations and then casting to int. This aligns with the commit message indicating a bug fix in the calculation.

**Buggy**"
jetty,11444.json,93b6877d724cbec52cc427e168144d539ea8e38b,"@@ -1,9 +1,9 @@
     protected void doStart() throws Exception
     {
         Objects.requireNonNull(httpClient, ""Provided HttpClient is null"");
 
+        super.doStart();
+
         if (!httpClient.isRunning())
             throw new IllegalStateException(""HttpClient is not running (did you forget to start it?): "" + httpClient);
-
-        super.doStart();
     }",Buggy,"Issue #2210 - fixing flaw in safety checks with JSR356 ClientContainer
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Issue #2210 - fixing flaw in safety checks with JSR356 ClientContainer"" explicitly states that the commit addresses a flaw in safety checks within the JSR356 ClientContainer. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `doStart()` method. The `super.doStart()` call has been moved from the end of the method to after the `Objects.requireNonNull` check and before the `httpClient.isRunning()` check.

**Reasoning:**

The original code had the following structure:

1.  Check if `httpClient` is null.
2.  Check if `httpClient` is running.
3.  Call `super.doStart()`.

The modified code has this structure:

1.  Check if `httpClient` is null.
2.  Call `super.doStart()`.
3.  Check if `httpClient` is running.

The change suggests that `super.doStart()` might have side effects that could affect the state of `httpClient`. Moving `super.doStart()` before the `isRunning()` check implies that the superclass's start logic might be necessary for the `httpClient` to be in a valid state for the `isRunning()` check. If `super.doStart()` initializes or modifies the `httpClient` in some way, then the original order could lead to a false negative in the `isRunning()` check, or even a NullPointerException if `httpClient` was not properly initialized by the superclass before the check.

The commit message mentions ""flaw in safety checks,"" and the code change directly affects the order of operations related to these checks. This strongly indicates that the original order was incorrect and could lead to unexpected behavior or errors, which constitutes a bug.

**Conclusion:**

Based on the commit message and the code changes, the commit appears to be fixing a bug related to the order of safety checks in the `doStart()` method.

**Buggy**
"
jetty,3568.json,6c81941142b9efe2b5b80198268ae75687dc6374,"@@ -1,61 +1,66 @@
     public void validate(Certificate[] certChain) throws CertificateException
     {
         try
         {
             ArrayList<X509Certificate> certList = new ArrayList<X509Certificate>();
             for (Certificate item : certChain)
             {
                 if (item == null)
                     continue;
                 
                 if (!(item instanceof X509Certificate))
                 {
                     throw new IllegalStateException(""Invalid certificate type in chain"");
                 }
                 
                 certList.add((X509Certificate)item);
             }
     
             if (certList.isEmpty())
             {
                 throw new IllegalStateException(""Invalid certificate chain"");
                 
             }
     
             X509CertSelector certSelect = new X509CertSelector();
             certSelect.setCertificate(certList.get(0));
             
             // Configure certification path builder parameters
             PKIXBuilderParameters pbParams = new PKIXBuilderParameters(_trustStore, certSelect);
             pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(certList)));
     
             // Set maximum certification path length
             pbParams.setMaxPathLength(_maxCertPathLength);
     
             // Enable revocation checking
             pbParams.setRevocationEnabled(true);
     
             // Set static Certificate Revocation List
             if (_crls != null && !_crls.isEmpty())
             {
                 pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(_crls)));
             }
     
             // Enable On-Line Certificate Status Protocol (OCSP) support
-            Security.setProperty(""ocsp.enable"",""true"");
-    
+            if (_enableOCSP)
+            {
+                Security.setProperty(""ocsp.enable"",""true"");
+            }
             // Enable Certificate Revocation List Distribution Points (CRLDP) support
-            System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            if (_enableCRLDP)
+            {
+                System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            }
     
             // Build certification path
             CertPathBuilderResult buildResult = CertPathBuilder.getInstance(""PKIX"").build(pbParams);               
             
             // Validate certification path
             CertPathValidator.getInstance(""PKIX"").validate(buildResult.getCertPath(),pbParams);
         }
         catch (GeneralSecurityException gse)
         {
             LOG.debug(gse);
             throw new CertificateException(""Unable to validate certificate: "" + gse.getMessage(), gse);
         }
     }",Buggy,"[Bug 373567] cert validation issue with ocsp and crldp always being enabled when validating turned on fixed
",Buggy,"The commit message indicates a fix for a bug where OCSP and CRLDP were always enabled during certificate validation, even when they should have been disabled.

The code diff shows that the lines `Security.setProperty(""ocsp.enable"",""true"");` and `System.setProperty(""com.sun.security.enableCRLDP"",""true"");` have been modified. They are now conditionally executed based on the values of `_enableOCSP` and `_enableCRLDP` respectively. This change ensures that OCSP and CRLDP are only enabled when the corresponding flags are set, which aligns with the bug fix described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
jetty,12794.json,c6d86122dbd737f2247b01653da0af938075370d,"@@ -1,45 +1,46 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
+        String authScheme = getAuthSchemeFromHeader(header);
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
         // The client has responded to the challenge we sent previously
-        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
+        if (header != null && isAuthSchemeNegotiate(authScheme))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
         // A challenge should be sent if any of the following cases are true:
         //   1. There was no Authorization header provided
         //   2. There was an Authorization header for a type other than Negotiate
         try
         {
              if (DeferredAuthentication.isDeferred(res))
              {
                  return Authentication.UNAUTHENTICATED;
              }
 
             LOG.debug(""SpengoAuthenticator: sending challenge"");
             res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
             res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
             return Authentication.SEND_CONTINUE;
         }
         catch (IOException ioe)
         {
             throw new ServerAuthException(ioe);
         }
     }",Buggy,"Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAut… (#1710)

* Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAuthenticator

Closes #1709

Signed-off-by: Josh Elser <elserj@apache.org>

* Clean up isAuthSchemeNegotiate(String) since we don't need to use startsWith()

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The commit message indicates a bug fix related to handling the ""Negotiate"" authentication scheme in a case-insensitive manner within the `SpnegoAuthenticator`. The code diff shows that the original code used `header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase())` to check for the ""Negotiate"" scheme. This is problematic because it only considers the lowercase version of ""Negotiate"".

The corrected code introduces a new method `isAuthSchemeNegotiate(String)` and uses it to check the authentication scheme. This suggests that the `isAuthSchemeNegotiate` method likely handles the case-insensitive comparison correctly. The removal of `startsWith()` also hints at a more robust check, possibly avoiding issues if ""Negotiate"" appears elsewhere in the header.

Therefore, the changes indicate a bug fix related to the case-insensitive handling of the ""Negotiate"" authentication scheme.

**Buggy**"
jetty,12794.json,1fd3e4ad1b8903bc9033dd24c86a0207996097b1,"@@ -1,46 +1,45 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
-        // check to see if we have authorization headers required to continue
-        if ( header == null )
-        {
-            try
-            {
-                 if (DeferredAuthentication.isDeferred(res))
-                 {
-                     return Authentication.UNAUTHENTICATED;
-                 }
-
-                LOG.debug(""SpengoAuthenticator: sending challenge"");
-                res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
-                res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
-                return Authentication.SEND_CONTINUE;
-            }
-            catch (IOException ioe)
-            {
-                throw new ServerAuthException(ioe);
-            }
-        }
-        else if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString()))
+        // The client has responded to the challenge we sent previously
+        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
-        return Authentication.UNAUTHENTICATED;
+        // A challenge should be sent if any of the following cases are true:
+        //   1. There was no Authorization header provided
+        //   2. There was an Authorization header for a type other than Negotiate
+        try
+        {
+             if (DeferredAuthentication.isDeferred(res))
+             {
+                 return Authentication.UNAUTHENTICATED;
+             }
+
+            LOG.debug(""SpengoAuthenticator: sending challenge"");
+            res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
+            res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
+            return Authentication.SEND_CONTINUE;
+        }
+        catch (IOException ioe)
+        {
+            throw new ServerAuthException(ioe);
+        }
     }",Buggy,"Sends the WWW-Authenticate header if a non-Negotiate authorization he… (#1700)

* Sends the WWW-Authenticate header if a non-Negotiate authorization header was given

Fixes #1698

Signed-off-by: Josh Elser <elserj@apache.org>

* Dumb compilation error

Signed-off-by: Josh Elser <elserj@apache.org>

* Adds a test to show the challenge is sent.

Signed-off-by: Josh Elser <elserj@apache.org>

* Refactor the conditionals per Greg's suggestion

Signed-off-by: Josh Elser <elserj@apache.org>

* Add the expected license header

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The commit message indicates that the changes are related to sending the `WWW-Authenticate` header when a non-Negotiate authorization header is given. The code diff shows that the logic for sending the `WWW-Authenticate` header has been modified.

The original code only sent the header if the `Authorization` header was null. The modified code sends the header if either the `Authorization` header is null or if it doesn't start with ""Negotiate"". This aligns with the commit message. The commit message also mentions that it fixes issue #1698, which implies that the previous behavior was incorrect, thus indicating a bug fix.

Therefore, the changes indicate a bug fix.

**Buggy**"
jetty,6485.json,c170801600f37d831007e6f496e7fc7d86e71dd1,"@@ -1,27 +1,27 @@
     protected void doStop() throws Exception
     {
         super.doStop();
 
         try
         {
             for (int i=_configurations.length;i-->0;)
                 _configurations[i].deconfigure(this);
             
             _configurations=null;
             
             // restore security handler
-            if (_securityHandler.getHandler()==null)
+            if (_securityHandler != null && _securityHandler.getHandler()==null)
             {
                 _sessionHandler.setHandler(_securityHandler);
                 _securityHandler.setHandler(_servletHandler);
             }
         }
         finally
         {
             if (_ownClassLoader)
                 setClassLoader(null);
 
             setAvailable(true);
             _unavailableException=null;
         }
     }",Buggy,"Bug 277027 Fix NPE if no security handler installed

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@239 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Bug 277027 Fix NPE if no security handler installed"" clearly states that the change addresses a `NullPointerException` (NPE) that occurs when no security handler is installed. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff focuses on the `doStop()` method. The key change is within the `if` condition:

- **Original Code:** `if (_securityHandler.getHandler()==null)`
- **Modified Code:** `if (_securityHandler != null && _securityHandler.getHandler()==null)`

The original code directly accessed `_securityHandler.getHandler()` without checking if `_securityHandler` itself was null. This could lead to an NPE if `_securityHandler` was not initialized or was set to null. The modified code adds a null check (`_securityHandler != null`) before accessing `_securityHandler.getHandler()`, preventing the potential NPE.

**Reasoning:**

1.  **Commit Message and Code Alignment:** The commit message explicitly mentions fixing an NPE related to the security handler. The code change directly addresses this by adding a null check for `_securityHandler` before accessing its methods.
2.  **Error Handling:** The added null check is a classic error-handling technique to prevent dereferencing a null object, which is a common cause of NPEs.
3.  **Bug Fix Indication:** The change prevents a potential runtime exception (NPE) under specific conditions (when no security handler is installed), which aligns with the definition of a bug fix.

**Conclusion:**

The commit message and code diff are consistent with a bug fix. The code change directly addresses the potential NPE described in the commit message.

**Buggy**
"
jetty,1562.json,4236f14955e828946c33f447fed3e65dfa8bfc1e,"@@ -1,15 +1,15 @@
         public void run()
         {
             try
             {
                 if (handle)
                     handleWithContext();
-                else
+                else if (getHttpConfiguration().isNotifyRemoteAsyncErrors())
                     getState().asyncError(failure);
                 callback.succeeded();
             }
             catch (Throwable x)
             {
                 callback.failed(x);
             }
         }",Buggy,"Fixes #1891 - Make HTTP/2 async error notifications configurable.

Introduced HttpConfiguration.notifyRemoteAsyncErrors, true by default.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Fixes #1891 - Make HTTP/2 async error notifications configurable"" clearly states that this commit is intended to fix a bug (as indicated by ""Fixes #1891""). The message also explains that the fix involves making HTTP/2 asynchronous error notifications configurable. A new configuration option, `HttpConfiguration.notifyRemoteAsyncErrors`, is introduced, and it defaults to `true`.

**Git Diff Analysis:**

The diff shows a modification within a `run()` method, likely related to handling HTTP/2 requests. The key change is the addition of a condition:

```java
-                else
+                else if (getHttpConfiguration().isNotifyRemoteAsyncErrors())
                     getState().asyncError(failure);
```

Previously, if `handle` was false, `getState().asyncError(failure)` was always called. Now, it's only called if `handle` is false *and* `getHttpConfiguration().isNotifyRemoteAsyncErrors()` returns `true`. This means that the asynchronous error notification can be disabled via the new configuration option.

**Reasoning:**

1.  **Bug Fix Indication:** The commit message explicitly states that it fixes a bug.
2.  **Configurability:** The change introduces a configuration option to control asynchronous error notifications. This suggests that the previous behavior (always notifying) was potentially problematic in certain scenarios, which could be considered a bug or undesirable behavior. Making it configurable allows users to opt-out of the notification if it causes issues.
3.  **Error Handling:** The code change directly affects how asynchronous errors are handled. The previous behavior was to always notify about async errors. The new behavior allows users to suppress these notifications. This is a change in error handling logic.

**Conclusion:**

Based on the commit message and the code changes, the commit introduces a configuration option to control asynchronous error notifications, which suggests that the previous behavior was not ideal and could be considered a bug.

**Buggy**
"
jetty,6487.json,0b13e6cddd8b8056311176389024fdb187305749,"@@ -1,12 +1,12 @@
     private void dumpUrl()
     {
         Connector[] connectors = getServer().getConnectors();
         for (int i=0;i<connectors.length;i++)
         {
             String displayName = getDisplayName();
             if (displayName == null)
-                displayName = ""WebApp@""+connectors.hashCode();
+                displayName = ""WebApp@""+Arrays.hashCode(connectors);
 
             LOG.info(displayName + "" at http://"" + connectors[i].toString() + getContextPath());
         }
     }",Buggy,"fix error prone error

Signed-off-by: olivier lamy <olamy@webtide.com>
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""fix error prone error"" is vague but suggests that the commit aims to correct an error identified by a static analysis tool like Error Prone or a similar linter.

**Diff Analysis:**

The diff modifies the `dumpUrl()` method.  Specifically, it changes how the `displayName` is generated when `getDisplayName()` returns `null`.  The original code used `connectors.hashCode()`, which returns the identity hash code of the `connectors` array object itself. This value is not useful for identifying the web application. The corrected code uses `Arrays.hashCode(connectors)`, which calculates a hash code based on the *contents* of the `connectors` array. This is a more meaningful representation of the connectors and likely what was intended.

**Reasoning:**

1.  **Bug Fix Indication:** The change from `connectors.hashCode()` to `Arrays.hashCode(connectors)` strongly suggests a bug fix. The original code produced a meaningless hash code, while the corrected code produces a hash code based on the array's contents. This is a logical correction.
2.  **Error Prone Relevance:** The commit message mentions ""error prone error,"" which aligns with the type of issue being fixed. Error Prone and similar tools often flag incorrect uses of `hashCode()` and suggest using `Arrays.hashCode()` for arrays.
3.  **Context:** The `displayName` is used in a log message. The original code would have resulted in confusing or misleading log messages, as the hash code of the array object is not related to the actual connectors.

**Conclusion:**

The code change corrects a logical error in how the `displayName` is generated, and the commit message suggests that this error was identified by a static analysis tool. Therefore, this commit represents a bug fix.

**Buggy**"
jetty,5128.json,8ec4e56681231a2370715cde72d46262aba65294,"@@ -1,17 +1,17 @@
     private static MultiMap<String> parseQueryString(String url)
     {
         MultiMap<String> res = new MultiMap<String>();
         int questionMarkIndex = url.indexOf('?');
         if (questionMarkIndex == -1)
         {
             return res;
         }
         int poundIndex = url.indexOf('#');
         if (poundIndex == -1)
         {
             poundIndex = url.length();
         }
-        UrlEncoded.decodeUtf8To(url.getBytes(), questionMarkIndex+1,
+        UrlEncoded.decodeUtf8To(url, questionMarkIndex+1,
                     poundIndex - questionMarkIndex - 1, res);
         return res;
     }",Buggy,"Fixing compile error
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixing compile error"" suggests that the change addresses a compilation issue. This strongly implies a bug fix, as code that doesn't compile is inherently broken.

2.  **Git Diff Analysis:**
    *   The diff shows a change in the `UrlEncoded.decodeUtf8To` method call. Specifically, the first argument is changed from `url.getBytes()` to `url`.
    *   The original code `url.getBytes()` attempts to convert the URL string into a byte array, which is then passed to the `decodeUtf8To` method. This could be problematic if the `decodeUtf8To` method expects a String object directly.
    *   The corrected code passes the `url` String directly to the `decodeUtf8To` method.

3.  **Alignment and Bug Fix Indication:**
    *   The commit message and the code change are highly aligned. The change directly addresses a potential type mismatch or incorrect argument passing that would cause a compilation error or runtime exception.
    *   The fact that the code was not compiling indicates a bug. The change fixes this bug by providing the correct argument type to the `decodeUtf8To` method.

**Conclusion:**

The commit message and the code changes are aligned and indicate a bug fix. The change resolves a compilation error by correcting the argument passed to the `decodeUtf8To` method.

**Buggy**"
jclouds,26844.json,c0e6a2c51cd131bcef2ee051cfe200b41afda528,"@@ -1,57 +1,57 @@
       public T read(JsonReader in) throws IOException {
          if (in.peek() == JsonToken.NULL) {
             in.nextNull();
             return null;
          }
 
          List<Parameter> params = parameterizedCtor.getParameters();
          Object[] values = new Object[params.size()];
          boolean empty = true;
 
          // Set all primitive constructor params to defaults
          for (Parameter param : params) {
             if (param.getType().getRawType() == boolean.class) {
                values[param.hashCode()] = Boolean.FALSE;
             } else if (param.getType().getRawType().isPrimitive()) {
                values[param.hashCode()] = 0;
             }
          }
 
          try {
             in.beginObject();
             while (in.hasNext()) {
                empty = false;
                String name = in.nextName();
                ParameterReader<?> parameter = parameterReaders.get(name);
-               if (parameter == null) {
+               if (parameter == null || in.peek() == JsonToken.NULL) {
                   in.skipValue();
                } else {
                   Object value = parameter.read(in);
                   if (value != null)
                      values[parameter.position] = value;
                }
             }
          } catch (IllegalStateException e) {
             throw new JsonSyntaxException(e);
          }
 
          for (Parameter param : params) {
             if (param.getType().getRawType().isPrimitive()) {
                checkArgument(values[param.hashCode()] != null,
                   ""Primitive param[%s] in constructor %s cannot be absent!"", param.hashCode(), parameterizedCtor);
             } else if (param.getType().getRawType() == Optional.class && values[param.hashCode()] == null) {
                values[param.hashCode()] = Optional.absent();
             }
          }
          in.endObject();
 
          try {
             return newInstance(values);
          } catch (NullPointerException ex) {
             // If {} was found and constructor threw NPE, we treat the field as null
             if (empty && values.length > 0) {
                return null;
             }
             throw ex;
          }
       }",Buggy,"Fixed bug found in docker provider where deserialization constructors don't handle json null.
",Buggy,"The commit message indicates a fix for a bug where deserialization constructors in the Docker provider don't handle JSON null values correctly.

The code diff shows a modification within the `read` method. Specifically, the condition `if (parameter == null)` has been changed to `if (parameter == null || in.peek() == JsonToken.NULL)`. This change ensures that if a JSON null value is encountered while reading parameters, the value is skipped, preventing potential errors during deserialization.

The added `in.peek() == JsonToken.NULL` check directly addresses the issue described in the commit message. This handles cases where a constructor parameter might be represented as a JSON null, preventing errors that could occur if the code attempted to process a null value as a non-null type.

The try-catch block remains unchanged, indicating no new exception handling was added. However, the added null check should prevent exceptions from occurring in the first place.

The change aligns with the commit message, indicating a bug fix related to handling null values during deserialization.

**Buggy**"
jclouds,27517.json,b59457a405f1741cebfa14775b397ae225dd714b,"@@ -1,4 +1,4 @@
    public <C extends Context> C unwrap(TypeToken<C> type) {
-      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable from %s"", backendType, type);
+      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable to %s"", backendType, type);
       return (C) backend;
    }",Buggy,"Fixing an error message about an unassignable backend
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixing an error message about an unassignable backend"" clearly states that the purpose of the commit is to correct an error message. This suggests that the original error message was inaccurate or misleading, which can be considered a bug.

**Git Diff Analysis:**

The diff shows a change in the `unwrap` method. Specifically, the error message within the `checkArgument` call has been modified. The original message used ""from"" in the context of assignability:

`""backend type: %s not assignable from %s""`

The corrected message uses ""to"":

`""backend type: %s not assignable to %s""`

This change directly addresses the error message described in the commit message. The original wording ""assignable from"" was likely confusing or incorrect in the context of how the types were being checked, and the change to ""assignable to"" makes the message more accurate and understandable.

**Reasoning:**

The commit message and the code change are perfectly aligned. The commit message states that an error message is being fixed, and the code change modifies the text of an error message. The change in wording from ""assignable from"" to ""assignable to"" suggests that the original message was logically flawed, leading to potential confusion for developers debugging type assignment issues.  An incorrect or confusing error message is a bug.

**Conclusion:**

**Buggy**
"
jclouds,25412.json,d41101df5932043c3a8614552f25ab91194595ba,"@@ -1,19 +1,19 @@
    private ImmutableMap<String, String> getContentMetadataForManifest(ContentMetadata contentMetadata) {
       Builder<String, String> mapBuilder = ImmutableMap.builder();
       if (contentMetadata.getContentType() != null) {
          mapBuilder.put(""content-type"", contentMetadata.getContentType());
       }
       /**
-       * Do not set content-length. Set automatically to manifest json string length by BindManifestToJsonPayload
+       * Do not set content-length. Set automatically to manifest json string length by BindToJsonPayload
        */
       if (contentMetadata.getContentDisposition() != null) {
          mapBuilder.put(""content-disposition"", contentMetadata.getContentDisposition());
       }
       if (contentMetadata.getContentEncoding() != null) {
          mapBuilder.put(""content-encoding"", contentMetadata.getContentEncoding());
       }
       if (contentMetadata.getContentLanguage() != null) {
          mapBuilder.put(""content-language"", contentMetadata.getContentLanguage());
       }
       return mapBuilder.build();
    }",Buggy,"JCLOUDS-1264: Swift Unicode multipart manifests

This fixes a bug where previously BindManifestToJsonPayload used the
character length as the ContentLength, instead of the byte length,
which caused issues if the JSON contained multi-byte Unicode
characters.
",Buggy,"The commit message ""JCLOUDS-1264: Swift Unicode multipart manifests"" indicates a fix related to handling Unicode characters in Swift multipart manifests. The message specifically mentions that `BindManifestToJsonPayload` was incorrectly using character length instead of byte length for `ContentLength`, leading to issues with multi-byte Unicode characters.

The code diff shows a change in the comment associated with the content-length metadata. The original comment stated that the content-length was set automatically to the manifest json string length by `BindManifestToJsonPayload`. The updated comment states that the content-length is set automatically to the manifest json string length by `BindToJsonPayload`.

Based on the commit message and the code diff, the change is related to correcting how the content length is calculated for JSON payloads, specifically addressing issues with Unicode characters. The original code likely had a bug where it was using the character length instead of the byte length, as stated in the commit message. The updated comment suggests that the class responsible for binding the manifest to JSON payload has been renamed or refactored to `BindToJsonPayload`. This change, along with the updated comment, indicates a bug fix related to the incorrect calculation of content length for Unicode characters.

**Buggy**"
jclouds,18000.json,9965fbcadb0913889f19d4f29e9f66f20e3f4f08,"@@ -1,24 +1,24 @@
    public Instance getWorkingInstance(String zone, String name, String flavorId, int size) {
       InstanceApi instanceApi = api.getInstanceApiForZone(zone);
       for (int retries = 0; retries < 10; retries++) {
          Instance instance = null;
          try {
             instance = instanceApi.create(flavorId, size, name);
          } catch (Exception e) {
 
             Uninterruptibles.sleepUninterruptibly(15, TimeUnit.SECONDS);
 
-            logger.error(e.getStackTrace().toString());
+            logger.error(Arrays.toString(e.getStackTrace()));
             continue;
          }
 
          Instance updatedInstance = awaitAvailable(instance, instanceApi);
          if (updatedInstance != null) {
             return updatedInstance;
          }
          instanceApi.delete(instance.getId());
          InstancePredicates.awaitDeleted(instanceApi).apply(instance);
          
       }
       return null;
    }",Buggy,"Address error-prone warning
",Buggy,"The commit message ""Address error-prone warning"" suggests that the commit aims to fix a warning reported by the Error Prone static analysis tool. The diff shows a change in the `getWorkingInstance` method where the logging of the stack trace is modified. Specifically, `e.getStackTrace().toString()` is replaced with `Arrays.toString(e.getStackTrace())`.

The original code `e.getStackTrace().toString()` is problematic because `StackTraceElement[]` does not have a meaningful `toString()` implementation. It simply returns the object's class name and hash code. The corrected code `Arrays.toString(e.getStackTrace())` provides a human-readable string representation of the stack trace elements.

This change directly addresses a bug where the stack trace was not being logged correctly, leading to unhelpful error messages. The fix ensures that the stack trace is properly formatted and logged, which is crucial for debugging and troubleshooting.

Therefore, the changes indicate a bug fix.
**Buggy**
"
jclouds,1113.json,347926543ee72a14f8587bc83f72e1064cef8784,"@@ -1,4 +1,8 @@
    @Override protected void configure() {
       super.configure();
       bindHttpApi(binder(), UseApiToResolveProjectName.GetProject.class);
+      bind(OAuthScopes.class).toInstance(ReadOrWriteScopes.create( //
+            ""https://www.googleapis.com/auth/compute.readonly"", //
+            ""https://www.googleapis.com/auth/compute"" //
+      ));
    }",Buggy," * Change OAuthScopes into an interface as opposed to boilerplating annotations.
 * Fixed errors because of boilerplating annotations.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states two main points:

1.  `Change OAuthScopes into an interface as opposed to boilerplating annotations.` - This suggests a refactoring or architectural change, moving away from using annotations to define OAuth scopes and instead using an interface.
2.  `Fixed errors because of boilerplating annotations.` - This explicitly mentions that the change addresses errors caused by the previous annotation-based approach. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows the following:

*   A binding for `OAuthScopes.class` is being created and bound to an instance of `ReadOrWriteScopes`.
*   The `ReadOrWriteScopes` instance is initialized with two Google Compute Engine OAuth scopes.

The diff itself doesn't directly reveal the nature of the errors caused by the annotations. However, the commit message claims that the change fixes errors related to the previous annotation-based approach. The diff shows the new way of doing things.

**Reasoning:**

The commit message explicitly states that the changes fix errors caused by the previous annotation-based approach. The diff shows the new implementation of OAuthScopes. The combination of these two points strongly indicates that the commit addresses a bug or issue related to the previous implementation.

**Conclusion:**

**Buggy**
"
jclouds,25772.json,f851271ae3d56d1549e62cd3723ad6207ced97f1,"@@ -1,3 +1,3 @@
-      public static NodeAttributes condition(String condition) {
-         return new NodeAttributes().condition(condition);
+      public static NodeAttributes condition(Condition condition) {
+         return new NodeAttributes().condition(condition.name());
       }",Buggy,"minor bug fixes plus wiring up
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""minor bug fixes plus wiring up"" suggests that the commit includes both bug fixes and some other changes related to wiring/configuration. The ""minor bug fixes"" part is a strong indicator that at least some part of the diff addresses a bug.

**Diff Analysis:**

The diff shows a change in the `condition` method's signature. Specifically, the type of the `condition` parameter has changed from `String` to `Condition`. The method now accepts a `Condition` enum/object and extracts its name using `.name()`.

**Reasoning:**

1.  **Type Change:** The change from `String` to `Condition` suggests a move towards type safety and potentially addressing an issue where arbitrary strings were being used as conditions. This could be a bug fix if the previous implementation allowed invalid or unexpected condition strings, leading to incorrect behavior.
2.  **`.name()` call:** The use of `.name()` implies that `Condition` is likely an enum. This further reinforces the idea of restricting the possible condition values to a predefined set, which can prevent errors caused by typos or invalid input.
3.  **Commit Message Alignment:** The commit message explicitly mentions ""minor bug fixes,"" which aligns with the observed code change. The change is likely related to ensuring that only valid conditions are used, thus fixing a potential bug where invalid condition strings could lead to unexpected behavior.

**Conclusion:**

Based on the commit message and the code diff, the changes strongly suggest a bug fix related to the handling of condition values. The type change and the use of `.name()` indicate an effort to enforce valid condition values and prevent errors caused by invalid input.

**Buggy**
"
jclouds,14356.json,6d657b1c90d590d38a6110f18f4683592932336d,"@@ -1,3 +1,3 @@
    public float getCpuUsed() {
-      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(9, cpuUsed.length() - 1)) : 0.0f;
+      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(0, cpuUsed.length() - 1)) : 0.0f;
    }",Buggy,"Fix bug in parsing VirtualMachine.cpuUsed
",Buggy,"The commit message indicates a bug fix related to parsing `VirtualMachine.cpuUsed`. The code diff shows a change in the substring method's starting index from 9 to 0. This suggests that the original index was incorrect, leading to a parsing error. The change corrects the starting index, which aligns with the commit message's intention to fix a parsing bug.

**Buggy**"
okhttp,1134.json,aed222454743ebe5724d6ad438fafed37956521e,"@@ -1,15 +1,25 @@
   @Override public void connectSocket(Socket socket, InetSocketAddress address,
       int connectTimeout) throws IOException {
     try {
       socket.connect(address, connectTimeout);
     } catch (AssertionError e) {
       if (Util.isAndroidGetsocknameError(e)) throw new IOException(e);
       throw e;
     } catch (SecurityException e) {
       // Before android 4.3, socket.connect could throw a SecurityException
       // if opening a socket resulted in an EACCES error.
       IOException ioException = new IOException(""Exception in connect"");
       ioException.initCause(e);
       throw ioException;
+    } catch (ClassCastException e) {
+      // On android 8.0, socket.connect throws a ClassCastException due to a bug
+      // see https://issuetracker.google.com/issues/63649622
+      if (Build.VERSION.SDK_INT == 26) {
+        IOException ioException = new IOException(""Exception in connect"");
+        ioException.initCause(e);
+        throw ioException;
+      } else {
+        throw e;
+      }
     }
   }",Buggy,"Add workaround for https://issuetracker.google.com/issues/63649622 (#3624)

* Add workaround for https://issuetracker.google.com/issues/63649622

This fixes #3438

* Rethrowing the Android O bug CCE as IOException
",Buggy,"The commit message indicates a workaround for a specific issue (https://issuetracker.google.com/issues/63649622) and fixes another issue (#3438). The code diff introduces a new `catch` block for `ClassCastException`. Inside this block, there's a check for `Build.VERSION.SDK_INT == 26` (Android 8.0). If the condition is met, the `ClassCastException` is wrapped in an `IOException` and rethrown. This aligns with the commit message's description of rethrowing the `ClassCastException` as an `IOException` on Android O. The added code specifically addresses a bug in Android 8.0 where `socket.connect` throws a `ClassCastException`. This is a clear indication of a bug fix.

**Buggy**
"
okhttp,55.json,8bce6897f9f7e0b16508c462665e4f9d26d92e39,"@@ -1,34 +1,34 @@
   private static Request transformRequest(HttpRequest request) {
     Request.Builder builder = new Request.Builder();
 
     RequestLine requestLine = request.getRequestLine();
     String method = requestLine.getMethod();
     builder.url(requestLine.getUri());
 
     String contentType = null;
     for (Header header : request.getAllHeaders()) {
       String name = header.getName();
-      if (""Content-Type"".equals(name)) {
+      if (""Content-Type"".equalsIgnoreCase(name)) {
         contentType = header.getValue();
       } else {
         builder.header(name, header.getValue());
       }
     }
 
     RequestBody body = null;
     if (request instanceof HttpEntityEnclosingRequest) {
       HttpEntity entity = ((HttpEntityEnclosingRequest) request).getEntity();
       if (entity != null) {
         // Wrap the entity in a custom Body which takes care of the content, length, and type.
         body = new HttpEntityBody(entity, contentType);
 
         Header encoding = entity.getContentEncoding();
         if (encoding != null) {
           builder.header(encoding.getName(), encoding.getValue());
         }
       }
     }
     builder.method(method, body);
 
     return builder.build();
   }",Buggy,"fixed a bug that content type is compared by case sensitive
",Buggy,"The commit message indicates a fix for a bug where content type comparison was case-sensitive. The code diff modifies the line `if (""Content-Type"".equals(name))` to `if (""Content-Type"".equalsIgnoreCase(name))`. This change replaces a case-sensitive comparison with a case-insensitive one, which directly addresses the bug described in the commit message.

**Buggy**"
okhttp,1015.json,8b17ab4e65b21c6480a38c383a16899f9ffabbcc,"@@ -1,39 +1,42 @@
   public void streamFailed(IOException e) {
     Socket socket;
     Connection releasedConnection;
     boolean noNewStreams = false;
 
     synchronized (connectionPool) {
       if (e instanceof StreamResetException) {
-        StreamResetException streamResetException = (StreamResetException) e;
-        if (streamResetException.errorCode == ErrorCode.REFUSED_STREAM) {
+        ErrorCode errorCode = ((StreamResetException) e).errorCode;
+        if (errorCode == ErrorCode.REFUSED_STREAM) {
+          // Retry REFUSED_STREAM errors once on the same connection.
           refusedStreamCount++;
-        }
-        // On HTTP/2 stream errors, retry REFUSED_STREAM errors once on the same connection. All
-        // other errors must be retried on a new connection.
-        if (streamResetException.errorCode != ErrorCode.REFUSED_STREAM || refusedStreamCount > 1) {
+          if (refusedStreamCount > 1) {
+            noNewStreams = true;
+            route = null;
+          }
+        } else if (errorCode != ErrorCode.CANCEL) {
+          // Keep the connection for CANCEL errors. Everything else wants a fresh connection.
           noNewStreams = true;
           route = null;
         }
       } else if (connection != null
           && (!connection.isMultiplexed() || e instanceof ConnectionShutdownException)) {
         noNewStreams = true;
 
         // If this route hasn't completed a call, avoid it for new connections.
         if (connection.successCount == 0) {
           if (route != null && e != null) {
             routeSelector.connectFailed(route, e);
           }
           route = null;
         }
       }
       releasedConnection = connection;
       socket = deallocate(noNewStreams, false, true);
       if (connection != null || !reportedAcquired) releasedConnection = null;
     }
 
     closeQuietly(socket);
     if (releasedConnection != null) {
       eventListener.connectionReleased(call, releasedConnection);
     }
   }",Buggy,"Fix stream cancel error stopping reused connection
",Buggy,"The commit message ""Fix stream cancel error stopping reused connection"" suggests that the code changes address an issue where a stream cancellation error was incorrectly preventing the reuse of a connection.

The code diff modifies the `streamFailed` method, specifically focusing on how `StreamResetException` exceptions are handled.

Here's a breakdown of the changes and their implications:

1.  **Refused Stream Handling:** The code now explicitly checks for `errorCode == ErrorCode.REFUSED_STREAM`. It increments `refusedStreamCount` and sets `noNewStreams = true` and `route = null` only if `refusedStreamCount > 1`. This aligns with the HTTP/2 retry mechanism where `REFUSED_STREAM` errors can be retried once on the same connection.

2.  **Cancel Error Handling:** The key change is the addition of `else if (errorCode != ErrorCode.CANCEL)`. This condition ensures that if the error code is `CANCEL`, the `noNewStreams` flag is *not* set. This means that the connection *will* be considered for reuse if the stream failed due to a cancellation.  This directly addresses the problem described in the commit message.

3.  **Other Error Handling:** For any other error besides `REFUSED_STREAM` and `CANCEL`, the `noNewStreams` flag is set, indicating that a new connection should be used.

The changes clearly indicate a bug fix related to how stream cancellation errors were preventing connection reuse. The added `else if` condition specifically targets the `CANCEL` error code, ensuring that connections are not unnecessarily marked as unusable in such cases.

**Buggy**
"
okhttp,2062.json,a87147e65424df05b7d7829555eef1220dbf4633,"@@ -1,18 +1,18 @@
     public Response response(Request request, DiskLruCache.Snapshot snapshot) {
       String contentType = responseHeaders.get(""Content-Type"");
       String contentLength = responseHeaders.get(""Content-Length"");
       Request cacheRequest = new Request.Builder()
           .url(url)
-          .method(message, null)
+          .method(requestMethod, null)
           .headers(varyHeaders)
           .build();
       return new Response.Builder()
           .request(cacheRequest)
           .protocol(protocol)
           .code(code)
           .message(message)
           .headers(responseHeaders)
           .body(new CacheResponseBody(snapshot, contentType, contentLength))
           .handshake(handshake)
           .build();
     }",Buggy,"Fix a bug where the cacheResponse's request method was wrong.

We were returning the message (like 'OK') rather than the method
(like 'GET'). Ugh.
",Buggy,"The commit message indicates a bug fix related to the `cacheResponse`'s request method. It states that the code was incorrectly returning the message (like 'OK') instead of the actual method (like 'GET').

The diff shows a change in the `response` method where `method(message, null)` is replaced with `method(requestMethod, null)`. This change directly addresses the bug described in the commit message by using the correct `requestMethod` instead of the `message` when building the `cacheRequest`.

The change aligns perfectly with the commit message, indicating a bug fix where the wrong value was being used for the request method.

**Buggy**"
openmrs-core,30.json,35fbc63bd8566c1b7800fcfdd1e34a6640ef0bd4,"@@ -1,30 +1,36 @@
 	protected File getFile(HttpServletRequest request) {
 		
 		String path = request.getPathInfo();
 		
 		Module module = ModuleUtil.getModuleForPath(path);
 		if (module == null) {
 			log.warn(""No module handles the path: "" + path);
 			return null;
 		}
 		
 		String relativePath = ModuleUtil.getPathForResource(module, path);
 		String realPath = getServletContext().getRealPath("""") + MODULE_PATH + module.getModuleIdAsPath() + ""/resources""
 		        + relativePath;
 		
 		//if in dev mode, load resources from the development directory
 		File devDir = ModuleUtil.getDevelopmentDirectory(module.getModuleId());
 		if (devDir != null) {
 			realPath = devDir.getAbsolutePath() + ""/omod/target/classes/web/module/resources"" + relativePath;
 		}
 		
 		realPath = realPath.replace(""/"", File.separator);
 		
 		File f = new File(realPath);
 		if (!f.exists()) {
+			if (isJstlFile(path)) {
+				f =  new File(realPath + "".withjstl"");
+				if (f.exists()) {
+					return f;
+				}
+			}
 			log.warn(""No file with path '"" + realPath + ""' exists for module '"" + module.getModuleId() + ""'"");
 			return null;
 		}
 		
 		return f;
 	}",Buggy,Fixing jslt file load error,Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Fixing jslt file load error"" strongly suggests that the commit addresses a bug related to loading JSLT files.

2.  **Code Diff Analysis:**
    *   The code adds a check within the `getFile` method to handle cases where a JSTL file (identified by `isJstlFile(path)`) might be missing its standard extension but exists with a "".withjstl"" extension.
    *   Specifically, if the initial file lookup fails (`!f.exists()`) and the path indicates a JSTL file, the code attempts to locate a file with the same path but with the "".withjstl"" extension appended.
    *   If this "".withjstl"" file exists, it is returned.
    *   The code includes logging to warn when a file is not found.

3.  **Alignment:** The code changes directly address the issue described in the commit message. The code attempts to load a file with a modified extension if the original file is not found. This suggests that the original code had a bug where it failed to load JSTL files under certain circumstances (likely when they had a "".withjstl"" extension).

4.  **Bug Fix Indicators:**
    *   The commit message explicitly states ""Fixing...error.""
    *   The code introduces a conditional check and an alternative file lookup mechanism, indicating a correction to the original file loading logic.
    *   The addition of the `.withjstl` extension handling suggests a specific scenario that was previously not handled correctly, leading to the ""jslt file load error.""

**Conclusion:**

The commit message and code changes are highly aligned and clearly indicate a bug fix related to loading JSTL files. The code addresses a specific scenario where JSTL files with a "".withjstl"" extension were not being loaded correctly.

**Buggy**"
openmrs-core,6620.json,6c5b1205526a53e2327f802adf31e1f4e340d93f,"@@ -1,20 +1,20 @@
 	public PatientIdentifier savePatientIdentifier(PatientIdentifier patientIdentifier) throws APIException {
 		//if the argument or the following required fields are not specified
 		PatientIdentifierType.LocationBehavior locationBehavior = null;
 		if (patientIdentifier != null) {
-			patientIdentifier.getIdentifierType().getLocationBehavior();
+			locationBehavior = patientIdentifier.getIdentifierType().getLocationBehavior();
 		}
 		
 		if (patientIdentifier == null
 		        || patientIdentifier.getPatient() == null
 		        || patientIdentifier.getIdentifierType() == null
 		        || StringUtils.isBlank(patientIdentifier.getIdentifier())
 		        || (locationBehavior == PatientIdentifierType.LocationBehavior.REQUIRED && patientIdentifier.getLocation() == null))
 			throw new APIException(""PatientIdentifier argument or one of its required fields is null or invalid"");
 		if (patientIdentifier.getPatientIdentifierId() == null) {
 			Context.requirePrivilege(PrivilegeConstants.ADD_PATIENT_IDENTIFIERS);
 		} else
 			Context.requirePrivilege(PrivilegeConstants.EDIT_PATIENT_IDENTIFIERS);
 		
 		return dao.savePatientIdentifier(patientIdentifier);
 	}",Buggy,"Fixing forgotten assignment bug for:
PatientService.savePatientIdentifier still requires location -
TRUNK-4056",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixing forgotten assignment bug for: PatientService.savePatientIdentifier still requires location - TRUNK-4056"" clearly states that the commit addresses a bug. The bug is described as a ""forgotten assignment bug"" related to the `PatientService.savePatientIdentifier` method and the requirement for a location. The TRUNK-4056 likely refers to a specific issue tracker entry.

**Git Diff Analysis:**

The diff shows a single line change within the `savePatientIdentifier` method.

-   **Before:** `patientIdentifier.getIdentifierType().getLocationBehavior();`
-   **After:** `locationBehavior = patientIdentifier.getIdentifierType().getLocationBehavior();`

The change involves assigning the result of `patientIdentifier.getIdentifierType().getLocationBehavior()` to the `locationBehavior` variable. Previously, the result of this method call was being discarded.

**Reasoning:**

The original code was calling `patientIdentifier.getIdentifierType().getLocationBehavior()` but not storing the result. The subsequent `if` condition checks `locationBehavior` to determine if a location is required. Without assigning the value to `locationBehavior`, the check would always use the default null value, potentially bypassing the location requirement validation.

The corrected code assigns the result of `patientIdentifier.getIdentifierType().getLocationBehavior()` to the `locationBehavior` variable, ensuring that the subsequent `if` condition correctly reflects the location requirement specified by the identifier type. This fixes a logical error that could lead to invalid patient identifiers being saved.

**Conclusion:**

The commit message explicitly states that it fixes a bug. The code change involves assigning a value to a variable that was previously unassigned, which directly impacts a conditional check related to data validation. This indicates a correction of a logical error, which is a clear sign of a bug fix.

**Buggy**"
openmrs-core,4909.json,837c70069a4bfca920ed92a7e73da0dbead6aff0,"@@ -1,6 +1,3 @@
 	public void purgeConceptClass(ConceptClass cc) throws DAOException  {
-		sessionFactory.getCurrentSession().createQuery(""delete from ConceptClass where concept_class_id = :c"")
-					.setInteger(""c"", cc.getConceptClassId())
-					.executeUpdate();			
 		sessionFactory.getCurrentSession().delete(cc);
 	}",Buggy,"Fixing conceptClass deletion error and fieldAnswer errors found by cmack

git-svn-id: http://svn.openmrs.org/openmrs/trunk@4553 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"Okay, let's analyze this Git commit.

**Step 1: Analyze the Commit Message**

The commit message ""Fixing conceptClass deletion error and fieldAnswer errors found by cmack"" indicates that the commit aims to resolve errors related to deleting `ConceptClass` objects and potentially other errors related to `fieldAnswer`. The mention of ""cmack"" suggests that these errors were reported or found by someone named cmack, implying a real-world problem.

**Step 2: Analyze the Code Diff**

The code diff shows a change in the `purgeConceptClass` method. The original code used a direct HQL (Hibernate Query Language) query to delete a `ConceptClass` based on its ID. The modified code uses `sessionFactory.getCurrentSession().delete(cc);`.

**Step 3: Reasoning**

The change from a direct HQL delete query to using `sessionFactory.getCurrentSession().delete(cc)` suggests a potential fix for a bug. Here's why:

*   **Potential Issues with HQL Delete:** Directly deleting using HQL might bypass Hibernate's object lifecycle management. This could lead to orphaned records in related tables if foreign key constraints are not properly handled or if Hibernate's cascade delete rules are not triggered.
*   **Hibernate's `delete()` Method:** Using `sessionFactory.getCurrentSession().delete(cc)` ensures that Hibernate manages the deletion process, respecting relationships and cascade rules defined in the entity mapping. This is generally the preferred way to delete entities in Hibernate to maintain data integrity.
*   **Error Context:** The commit message mentions a ""conceptClass deletion error."" The change aligns with addressing this error by using the standard Hibernate deletion mechanism.

**Step 4: Conclusion**

The commit message explicitly states that it fixes an error related to `conceptClass` deletion. The code change replaces a direct HQL delete with Hibernate's `delete()` method, which is a more robust and recommended approach for deleting entities and maintaining data integrity. This strongly suggests that the original code had a bug that was causing issues during deletion, and the change is a bug fix.

**Buggy**"
openmrs-core,7646.json,f6e02ac804e140765a89e0f983c794be1b3ffa14,"@@ -1,127 +1,127 @@
 	public void execute(Database database) throws CustomChangeException {
 		JdbcConnection connection = (JdbcConnection) database.getConnection();
 		Map<String, HashSet<Integer>> duplicates = new HashMap<String, HashSet<Integer>>();
 		Statement stmt = null;
 		PreparedStatement pStmt = null;
 		ResultSet rs = null;
 		boolean autoCommit = true;
 		try {
 			// set auto commit mode to false for UPDATE action
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			stmt = connection.createStatement();
-			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type""
+			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type ""
 			        + ""INNER JOIN (SELECT name FROM location_attribute_type GROUP BY name HAVING count(name) > 1) ""
 			        + ""dup ON location_attribute_type.name = dup.name"");
 			Integer id = null;
 			String name = null;
 			
 			while (rs.next()) {
 				id = rs.getInt(""location_attribute_type_id"");
 				name = rs.getString(""name"");
 				if (duplicates.get(name) == null) {
 					HashSet<Integer> results = new HashSet<Integer>();
 					results.add(id);
 					duplicates.put(name, results);
 				} else {
 					HashSet<Integer> results = duplicates.get(name);
 					results.add(id);
 				}
 			}
 			
 			Iterator it2 = duplicates.entrySet().iterator();
 			while (it2.hasNext()) {
 				Map.Entry pairs = (Map.Entry) it2.next();
 				HashSet values = (HashSet) pairs.getValue();
 				List<Integer> duplicateNames = new ArrayList<Integer>(values);
 				int duplicateNameId = 1;
 				for (int i = 1; i < duplicateNames.size(); i++) {
 					String newName = pairs.getKey() + ""_"" + duplicateNameId;
 					List<List<Object>> duplicateResult = null;
 					boolean duplicateName = false;
 					Connection con = DatabaseUpdater.getConnection();
 					do {
 						String sqlValidatorString = ""select * from location_attribute_type where name = '"" + newName + ""'"";
 						duplicateResult = DatabaseUtil.executeSQL(con, sqlValidatorString, true);
 						if (!duplicateResult.isEmpty()) {
 							duplicateNameId += 1;
 							newName = pairs.getKey() + ""_"" + duplicateNameId;
 							duplicateName = true;
 						} else {
 							duplicateName = false;
 						}
 					} while (duplicateName);
 					pStmt = connection
 					        .prepareStatement(""update location_attribute_type set name = ?, changed_by = ?, date_changed = ? where location_attribute_type_id = ?"");
 					if (!duplicateResult.isEmpty()) {
 						pStmt.setString(1, newName);
 					}
 					pStmt.setString(1, newName);
 					pStmt.setInt(2, DatabaseUpdater.getAuthenticatedUserId());
 					
 					Calendar cal = Calendar.getInstance();
 					Date date = new Date(cal.getTimeInMillis());
 					
 					pStmt.setDate(3, date);
 					pStmt.setInt(4, duplicateNames.get(i));
 					duplicateNameId += 1;
 					
 					pStmt.executeUpdate();
 				}
 			}
 		}
 		catch (BatchUpdateException e) {
 			log.warn(""Error generated while processsing batch insert"", e);
 			try {
 				log.debug(""Rolling back batch"", e);
 				connection.rollback();
 			}
 			catch (Exception rbe) {
 				log.warn(""Error generated while rolling back batch insert"", e);
 			}
 			// marks the changeset as a failed one
 			throw new CustomChangeException(""Failed to update one or more duplicate LocationAttributeType names"", e);
 		}
 		catch (Exception e) {
 			throw new CustomChangeException(""Error while updating duplicate LocationAttributeType object names"", e);
 		}
 		finally {
 			// reset to auto commit mode
 			try {
 				connection.commit();
 				connection.setAutoCommit(autoCommit);
 			}
 			catch (DatabaseException e) {
 				log.warn(""Failed to reset auto commit back to true"", e);
 			}
 			
 			if (rs != null) {
 				try {
 					rs.close();
 				}
 				catch (SQLException e) {
 					log.warn(""Failed to close the resultset object"");
 				}
 			}
 			
 			if (stmt != null) {
 				try {
 					stmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the select statement used to identify duplicate LocationAttributeType object names"");
 				}
 			}
 			
 			if (pStmt != null) {
 				try {
 					pStmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the prepared statement used to update duplicate LocationAttributeType object names"");
 				}
 			}
 		}
 	}",Buggy,Fixing liquibase upgrade error for: TRUNK-4334,Buggy,"The commit message indicates a fix for a liquibase upgrade error related to TRUNK-4334. The code changes involve updating duplicate names in the `location_attribute_type` table. The query to select duplicate names has been modified by adding a space after `location_attribute_type`. There's a potential bug where `pStmt.setString(1, newName)` is called twice, and the first call is within an `if` block that checks `!duplicateResult.isEmpty()`. This suggests that the code might have been setting the name incorrectly in some cases. The code also includes exception handling for `BatchUpdateException` and other `Exception` types, with rollback logic in case of errors. The presence of these error handling and rollback mechanisms, along with the modification of the SQL query and the potential double setting of the prepared statement parameter, strongly suggests a bug fix.

**Buggy**"
openmrs-core,6353.json,95b3ab3de2d8ec488d0ca0fcbcb1e8969d491313,"@@ -1,21 +1,28 @@
 	private Class datatypeClassHandled(Type t) {
 		if (t instanceof ParameterizedType) {
 			ParameterizedType pt = (ParameterizedType) t;
 			Type first = pt.getActualTypeArguments()[0];
 			if (first instanceof Class && CustomDatatype.class.isAssignableFrom((Class) first)) {
 				return (Class) first;
 			} else {
 				return datatypeClassHandled(pt.getRawType());
 			}
 			
 		} else if (t instanceof Class) {
+			Type genericSuperclass = ((Class) t).getGenericSuperclass();
+			if (genericSuperclass != null) {
+				Class ret = datatypeClassHandled(genericSuperclass);
+				if (ret != null) {
+					return ret;
+				}
+			}
 			for (Type candidate : ((Class) t).getGenericInterfaces()) {
 				Class ret = datatypeClassHandled(candidate);
 				if (ret != null) {
 					return ret;
 				}
 			}
 		}
 		
 		return null;
 	}",Buggy,"Fixed bug where a custom datatype handler that extends a generic abstract class can't be found  -TRUNK-3499
",Buggy,"The commit message indicates a fix for a bug where a custom datatype handler extending a generic abstract class could not be found. The code diff modifies the `datatypeClassHandled` method to handle cases where the type `t` is a Class. Specifically, it retrieves the generic superclass of `t` and recursively calls `datatypeClassHandled` on it. This suggests the original code was not correctly inspecting the generic superclass hierarchy, leading to the bug described in the commit message. The added code addresses this by explicitly checking the generic superclass.

**Buggy**"
openmrs-core,7456.json,4fff45254e5332a1b608a88d6daa83e7a330daf7,"@@ -1,9 +1,9 @@
 	public static byte[] getSavedSecretKey() {
-		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_VECTOR_RUNTIME_PROPERTY,
-		    OpenmrsConstants.ENCRYPTION_VECTOR_DEFAULT);
+		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_KEY_RUNTIME_PROPERTY,
+		    OpenmrsConstants.ENCRYPTION_KEY_DEFAULT);
 		
 		if (StringUtils.hasText(keyText))
 			return Base64.decode(keyText);
 		
 		throw new APIException(""no encryption secret key found"");
 	}",Buggy,"TRUNK-1919 - fixed glaring flaw where I pulled in the encryption.vector runtime property instead of encryption.key

git-svn-id: http://svn.openmrs.org/openmrs/trunk@18684 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix for a flaw where the wrong runtime property was being used. The code diff shows that `ENCRYPTION_VECTOR_RUNTIME_PROPERTY` was replaced with `ENCRYPTION_KEY_RUNTIME_PROPERTY` and `ENCRYPTION_VECTOR_DEFAULT` was replaced with `ENCRYPTION_KEY_DEFAULT`. This change aligns with the commit message, indicating a bug fix.

**Buggy**"
openmrs-core,6446.json,119e3fa6338f5401f0a75873e5a6526b8b23cd80,"@@ -1,10 +1,10 @@
 	private void checkPrivileges(Role role) {
-		Collection<Privilege> privileges = role.getPrivileges();
-		
 		Optional.ofNullable(role.getPrivileges())
 		.map(p -> p.stream().filter(pr -> !Context.hasPrivilege(pr.getPrivilege())).map(Privilege::getPrivilege)
 			.distinct().collect(Collectors.joining("", "")))
 		.ifPresent(missing -> {
-			throw new APIException(""Role.you.must.have.privileges: "", new Object[] { missing });
+			if (StringUtils.isNotBlank(missing)) {
+				throw new APIException(""Role.you.must.have.privileges"", new Object[] { missing });
+			}
 		});
     }",Buggy,"Fix for TRUNK-5726

* Only throws exception if actually missing permissions are found
* Correct error string
* Adds unit tests
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fix for TRUNK-5726"" immediately suggests a bug fix. The message further elaborates:

*   ""Only throws exception if actually missing permissions are found"": This indicates a conditional check was added to prevent unnecessary exceptions, implying a previous bug where exceptions were thrown incorrectly.
*   ""Correct error string"": This points to a bug where the error message was either incorrect or unclear.
*   ""Adds unit tests"": This is a good practice when fixing bugs to prevent regressions.

**Code Diff Analysis:**

The code diff focuses on the `checkPrivileges` method. Let's break down the changes:

1.  **`Optional.ofNullable(role.getPrivileges())`**: This handles the case where the role might not have any privileges assigned. This is defensive programming and could be related to preventing a `NullPointerException` or other unexpected behavior if `role.getPrivileges()` returned null.

2.  **.map(p -> p.stream().filter(pr -> !Context.hasPrivilege(pr.getPrivilege())).map(Privilege::getPrivilege)\n.distinct().collect(Collectors.joining("", "")))**: This part filters the privileges to find the missing ones and joins them into a comma-separated string.

3.  **.ifPresent(missing -> { ... })**: This ensures that the code inside the block is executed only if there are missing privileges.

4.  **`if (StringUtils.isNotBlank(missing)) { ... }`**: This is the crucial part related to the first point in the commit message. It checks if the `missing` string is not blank before throwing the exception. This prevents the exception from being thrown if there are no missing privileges.

5.  **`throw new APIException(""Role.you.must.have.privileges"", new Object[] { missing });`**: The change from `""Role.you.must.have.privileges: ""` to `""Role.you.must.have.privileges""` suggests a correction in the error message, aligning with the second point in the commit message. The removal of the colon likely fixes a formatting issue or a grammatical error in the original message.

**Reasoning:**

The commit message and code diff are highly correlated. The code changes directly address the issues described in the commit message:

*   The `StringUtils.isNotBlank(missing)` check ensures that the exception is only thrown when there are actual missing privileges, fixing a potential bug where the exception was thrown unnecessarily.
*   The change in the error string `""Role.you.must.have.privileges: ""` to `""Role.you.must.have.privileges""` corrects the error message.
*   The addition of unit tests (mentioned in the commit message but not visible in the diff) further supports the notion of a bug fix.

**Conclusion:**

The changes indicate a bug fix because they address incorrect exception throwing and an incorrect error message.

**Buggy**
"
openmrs-core,2762.json,29eae1d038bc6036af4903c9b43f42a157b10a1a,"@@ -1,49 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
 		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
 			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
-					specificErrorMsg = ""with provider identifier:"" + id;
+					specificErrorMsg = ""with provider identifier"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 				specificErrorMsg = ""associated to a person with person id"";
 			}
 			
 			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
 		} else {
 			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
 		if (provider == null) {
 			throw new HL7Exception(errorMessage);
 		}
 		
 		return provider;
 	}",Buggy,"Follow up to ,Fixed the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26248 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates that the error messages have been improved to be more helpful when the provider cannot be resolved.

The code changes modify the `specificErrorMsg` variable to provide more context about the type of identifier being used when resolving the provider. Specifically, the change removes the provider id from the error message when the provider is searched by identifier.

The original code included the provider identifier in the error message, which might not be helpful in all cases. The modified code provides a more generic message indicating that the provider could not be resolved using the specified identifier type. This change aligns with the commit message's goal of providing more helpful error messages.

The change also includes exception handling improvements. The code now catches `NumberFormatException` when converting the provider ID to an integer, which could occur if the ID is not a valid number. This prevents the application from crashing and allows it to handle the error more gracefully.

Therefore, the changes indicate a bug fix.

**Buggy**"
openmrs-core,2762.json,dc4c547246dcf46aa2ff0dcb5e43caee7cd5f1d7,"@@ -1,38 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
+		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
+			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
+					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
+					specificErrorMsg = ""with provider identifier:"" + id;
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
+					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
+				specificErrorMsg = ""associated to a person with person id"";
 			}
+			
+			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
+		} else {
+			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
-		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider"");
+		if (provider == null) {
+			throw new HL7Exception(errorMessage);
+		}
 		
 		return provider;
 	}",Buggy,"Improved the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26246 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates an improvement in error messages when a provider cannot be resolved. The diff shows that the code has been modified to provide more specific error messages based on the different scenarios where the provider resolution fails. The original code threw a generic ""Could not resolve provider"" exception. The modified code now constructs a more informative error message, including details about the identifier used (provider Id, identifier, uuid, or person id) and its value. This change directly addresses the commit message's goal of providing more helpful error messages. The original code's lack of specific error information could be considered a bug or at least a deficiency in error handling, which has now been corrected.

**Buggy**
"
openmrs-core,2762.json,7932358da8194ef18be1ce23e0b72b0c4f51a63a,"@@ -1,39 +1,39 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = ((HD) hl7Provider.getComponent(8)).getNamespaceID().getValue();
 		String nameTypeCode = ((ID) hl7Provider.getComponent(9)).getValue();
 		
 		if (StringUtils.hasText(id)) {
 			if (OpenmrsUtil.nullSafeEquals(""L"", nameTypeCode)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 			}
 		}
 		
 		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider with personId or identifier as '"" + id + ""'"");
+			throw new HL7Exception(""Could not resolve provider"");
 		
 		return provider;
 	}",Buggy,"Rephrased the error message when the provider can't be resolved when processing an HL7 message

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26240 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates a change in the error message when a provider cannot be resolved during HL7 message processing. The diff shows that the original error message, which included the provider ID, has been replaced with a generic ""Could not resolve provider"" message.

The change removes specific information (the provider ID) from the error message. This could be done for security reasons (avoiding exposing internal IDs) or to simplify the error message. While not directly fixing a bug, it addresses a potential issue with the original error message.

Therefore, the change is related to error handling and message content, but not a bug fix in the traditional sense.

**NotBuggy**"
openmrs-core,6970.json,6f7de13f5e4d0b9e4d8ea553bfd92842cd0c161a,"@@ -1,19 +1,24 @@
 	public FormResource saveFormResource(FormResource formResource) throws APIException {
-		if (formResource == null) {
+	    	if (formResource == null) {
 			return null;
 		}
-		
 		// If a form resource with same name exists, replace it with current value
 		FormResource toPersist = formResource;
 		FormResource original = Context.getFormService().getFormResource(formResource.getForm(), formResource.getName());
 		if (original != null) {
 			original.setName(formResource.getName());
 			original.setValue(formResource.getValue());
 			original.setDatatypeClassname(formResource.getDatatypeClassname());
 			original.setDatatypeConfig(formResource.getDatatypeConfig());
 			original.setPreferredHandlerClassname(formResource.getPreferredHandlerClassname());
 			toPersist = original;
 		}
-		CustomDatatypeUtil.saveIfDirty(toPersist);
+		try {
+		    CustomDatatypeUtil.saveIfDirty(toPersist);
+		}
+		catch (ConstraintViolationException ex) {
+		    throw new InvalidFileTypeException(ex.getMessage(), ex);
+		}
+		
 		return dao.saveFormResource(toPersist);
 	}",Buggy,"TRUNK-4473: Fixed error occuring when user tries to upload binary files with form resource
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""TRUNK-4473: Fixed error occurring when user tries to upload binary files with form resource"" clearly states that the commit addresses a bug. The bug involves an error that occurs when a user attempts to upload binary files as a form resource.

**Git Diff Analysis:**

The diff shows a modification within the `saveFormResource` method. Specifically, a `try-catch` block has been added around the `CustomDatatypeUtil.saveIfDirty(toPersist)` call. The `catch` block handles `ConstraintViolationException` and throws a new `InvalidFileTypeException` with the original exception's message and the exception itself.

**Reasoning:**

1.  **Error Handling:** The addition of the `try-catch` block indicates that the original code was not properly handling a potential exception (`ConstraintViolationException`) that could occur during the `saveIfDirty` operation. This exception likely arises when the uploaded binary file violates some constraint, such as file type or size limitations.

2.  **Exception Translation:** The code catches the `ConstraintViolationException` and re-throws it as an `InvalidFileTypeException`. This suggests that the original exception was not specific enough to the problem of uploading invalid file types. The new exception provides a more meaningful error message to the user or calling code.

3.  **Bug Fix Indication:** The commit message explicitly mentions fixing an error related to uploading binary files. The code changes directly address this issue by adding error handling and providing a more specific exception for invalid file types.

**Conclusion:**

The commit message and the code changes are strongly aligned. The changes introduce error handling for a specific scenario (uploading binary files) and provide a more informative exception. This clearly indicates that the commit is a bug fix.

**Buggy**
"
openmrs-core,105.json,f2d3fdd76e95f7783e530e4c3e7b4f0b7d48a852,"@@ -1,9 +1,10 @@
 	public static String escapeQuotesAndNewlines(String s) {
 		if (s == null)
 			return """";
 		
 		s = s.replace(""\"""", ""\\\"""");
+		s = s.replace(""\r\n"", ""\\r\\n"");
 		s = s.replace(""\n"", ""\\n"");
 		
 		return s;
 	}",Buggy,"Fixed javascript error on form schema designer when field descriptions had multiple lines - #594

git-svn-id: http://svn.openmrs.org/openmrs/trunk@11807 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Fixed javascript error on form schema designer when field descriptions had multiple lines - #594"" clearly states that the commit addresses a bug. The bug occurs in the form schema designer, specifically when field descriptions contain multiple lines, leading to a JavaScript error. The ""#594"" likely refers to an issue tracker number, further suggesting a bug fix.

**Code Diff Analysis:**

The code diff shows a modification to the `escapeQuotesAndNewlines` method. This method appears to be responsible for escaping special characters (quotes and newlines) within a string. The changes introduce the following:

1.  `s = s.replace(""\r\n"", ""\\r\\n"");`

This line replaces carriage return and newline characters (`\r\n`) with escaped versions (`\\r\\n`). This is crucial for handling Windows-style line endings.

2. `s = s.replace(""\n"", ""\\n"");`

This line replaces newline characters (`\n`) with escaped versions (`\\n`). This is crucial for handling Linux/Unix-style line endings.

**Reasoning:**

The commit message indicates a JavaScript error related to multi-line field descriptions. The code diff modifies a method that escapes special characters, including newlines. JavaScript strings often have issues when they contain unescaped newlines. By escaping the newlines (`\r\n` and `\n`), the code ensures that the string is properly formatted for JavaScript, thus resolving the reported error. The fix specifically targets the issue described in the commit message.

**Conclusion:**

The changes in the code diff directly address the problem described in the commit message, indicating a bug fix.

**Buggy**"
hadoop,10762.json,d3949058b84c393413ffea11de5c81ab8ad2ae3c,"@@ -1,40 +1,40 @@
   void doRollback(StorageDirectory bpSd, NamespaceInfo nsInfo)
       throws IOException {
     File prevDir = bpSd.getPreviousDir();
     // regular startup if previous dir does not exist
     if (!prevDir.exists())
       return;
     // read attributes out of the VERSION file of previous directory
-    DataStorage prevInfo = new DataStorage();
+    BlockPoolSliceStorage prevInfo = new BlockPoolSliceStorage();
     prevInfo.readPreviousVersionProperties(bpSd);
 
     // We allow rollback to a state, which is either consistent with
     // the namespace state or can be further upgraded to it.
     // In another word, we can only roll back when ( storedLV >= software LV)
     // && ( DN.previousCTime <= NN.ctime)
     if (!(prevInfo.getLayoutVersion() >= HdfsConstants.LAYOUT_VERSION && 
         prevInfo.getCTime() <= nsInfo.getCTime())) { // cannot rollback
       throw new InconsistentFSStateException(bpSd.getRoot(),
           ""Cannot rollback to a newer state.\nDatanode previous state: LV = ""
               + prevInfo.getLayoutVersion() + "" CTime = "" + prevInfo.getCTime()
               + "" is newer than the namespace state: LV = ""
               + nsInfo.getLayoutVersion() + "" CTime = "" + nsInfo.getCTime());
     }
     
     LOG.info(""Rolling back storage directory "" + bpSd.getRoot()
         + "".\n   target LV = "" + nsInfo.getLayoutVersion()
         + ""; target CTime = "" + nsInfo.getCTime());
     File tmpDir = bpSd.getRemovedTmp();
     assert !tmpDir.exists() : ""removed.tmp directory must not exist."";
     // 1. rename current to tmp
     File curDir = bpSd.getCurrentDir();
     assert curDir.exists() : ""Current directory must exist."";
     rename(curDir, tmpDir);
     
     // 2. rename previous to current
     rename(prevDir, curDir);
     
     // 3. delete removed.tmp dir
     deleteDir(tmpDir);
     LOG.info(""Rollback of "" + bpSd.getRoot() + "" is complete"");
   }",Buggy,"HDFS-3970. Fix bug causing rollback of HDFS upgrade to result in bad VERSION file. Contributed by Vinay and Andrew Wang.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430037 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to HDFS upgrade rollback, specifically concerning a bad VERSION file. The code diff modifies the `doRollback` method in `BlockPoolSliceStorage`.

Here's a step-by-step analysis:

1.  **Class Name Change:** The diff replaces `DataStorage prevInfo = new DataStorage();` with `BlockPoolSliceStorage prevInfo = new BlockPoolSliceStorage();`. This suggests a change in the object type used to read previous version properties. This could be related to how the VERSION file is being read and interpreted during rollback.

2.  **Rollback Condition:** The code checks if a rollback is possible based on layout version (LV) and creation time (CTime). The condition `prevInfo.getLayoutVersion() >= HdfsConstants.LAYOUT_VERSION && prevInfo.getCTime() <= nsInfo.getCTime()` ensures that the rollback is to a state consistent with the namespace or one that can be further upgraded. This condition is crucial for data integrity during rollback.

3.  **Rollback Process:** The code renames directories to revert to the previous state. It renames the current directory to a temporary directory, renames the previous directory to the current directory, and then deletes the temporary directory. This is the standard rollback procedure.

The change in class name from `DataStorage` to `BlockPoolSliceStorage` and the overall logic of the `doRollback` method strongly suggest that the commit addresses a bug in how the previous version information was being handled during rollback. The commit message explicitly states that the bug caused a bad VERSION file, which aligns with the code modifications.

**Buggy**"
hadoop,78812.json,64f537da0a216055d5b6eae49a2a9129189cacbf,"@@ -1,18 +1,26 @@
-    public FileStatus[] globStatus(Path pathPattern, PathFilter filter)
+    public FileStatus[] globStatus(final Path pathPattern, final PathFilter filter)
       throws IOException {
+      
       String filename = pathPattern.toUri().getPath();
+      
       List<String> filePatterns = GlobExpander.expand(filename);
       if (filePatterns.size() == 1) {
-        return globStatusInternal(pathPattern, filter);
+        Path p = fixRelativePart(pathPattern);
+        FileSystem fs = getFSofPath(p);
+        URI uri = fs.getUri();
+        return globStatusInternal(uri, p, filter);
       } else {
         List<FileStatus> results = new ArrayList<FileStatus>();
         for (String filePattern : filePatterns) {
-          FileStatus[] files = 
-                      globStatusInternal(new Path(filePattern), filter);
+          Path p = new Path(filePattern);
+          p = fixRelativePart(p);
+          FileSystem fs = getFSofPath(p);
+          URI uri = fs.getUri();
+          FileStatus[] files = globStatusInternal(uri, p, filter);
           for (FileStatus file : files) {
             results.add(file);
           }
         }
         return results.toArray(new FileStatus[results.size()]);
       }
     }",Buggy,"HADOOP-6286. Fix bugs in related to URI handling in glob methods in FileContext. Contributed by Boris Shkolnik.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@822805 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HADOOP-6286. Fix bugs in related to URI handling in glob methods in FileContext"" explicitly states that the commit addresses bugs related to URI handling within the `glob` methods of `FileContext`. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff modifies the `globStatus` method. Let's break down the changes:

1.  **`filename = pathPattern.toUri().getPath();`**: This line extracts the path component from the URI of the input `pathPattern`. This is a standard way to handle paths, but the subsequent changes indicate potential issues with how this path was being used.

2.  **`if (filePatterns.size() == 1)` block:**
    *   `Path p = fixRelativePart(pathPattern);` : This line suggests that the original path might have had relative components that needed fixing. This is a common source of bugs in path handling.
    *   `FileSystem fs = getFSofPath(p);`: This line retrieves the `FileSystem` associated with the (potentially fixed) path.
    *   `URI uri = fs.getUri();`: This line gets the URI of the filesystem.
    *   `return globStatusInternal(uri, p, filter);`: The code now calls `globStatusInternal` with the filesystem URI and the potentially fixed path. This indicates that the original path was not sufficient on its own and needed the context of the filesystem's URI.

3.  **`else` block (when `filePatterns.size() > 1`):**
    *   The code iterates through a list of file patterns.
    *   `Path p = new Path(filePattern);`: Creates a new Path object from the file pattern string.
    *   `p = fixRelativePart(p);`: Fixes relative parts of the path.
    *   `FileSystem fs = getFSofPath(p);`: Gets the FileSystem for the path.
    *   `URI uri = fs.getUri();`: Gets the URI of the filesystem.
    *   `FileStatus[] files = globStatusInternal(uri, p, filter);`: Calls `globStatusInternal` with the filesystem URI and the potentially fixed path.

**Reasoning:**

The changes in the code diff, combined with the commit message, strongly suggest a bug fix related to URI handling. The original code likely had issues with:

*   Relative paths: The `fixRelativePart` method indicates that the code now handles relative paths correctly.
*   Missing filesystem context: The code now explicitly passes the filesystem URI to `globStatusInternal`. This suggests that the original code was not providing enough context for the `globStatusInternal` method to correctly resolve paths, especially when dealing with different filesystems or when the path was not fully qualified.

The fact that the code now explicitly retrieves the filesystem URI and uses `fixRelativePart` indicates that the previous implementation was likely flawed in its URI handling, leading to incorrect path resolution and potential errors. The commit message explicitly mentions ""URI handling"" and ""bugs,"" which further supports the conclusion that this commit fixes a bug.

**Conclusion:**

**Buggy**
"
hadoop,37278.json,3a154f75ed85d864b3ffd35818992418f2b6aa59,"@@ -1,21 +1,25 @@
   public RLESparseResourceAllocation getRangeOverlapping(long start, long end) {
     readLock.lock();
     try {
       NavigableMap<Long, Resource> a = this.getCumulative();
 
       if (a != null && !a.isEmpty()) {
         // include the portion of previous entry that overlaps start
         if (start > a.firstKey()) {
           long previous = a.floorKey(start);
           a = a.tailMap(previous, true);
         }
-        a = a.headMap(end, true);
+
+        if (end < a.lastKey()) {
+          a = a.headMap(end, true);
+        }
+
       }
       RLESparseResourceAllocation ret =
           new RLESparseResourceAllocation(a, resourceCalculator);
       return ret;
     } finally {
       readLock.unlock();
     }
 
   }",Buggy,"YARN-4525. Fix bug in RLESparseResourceAllocation.getRangeOverlapping(). (Ishai Menache and Carlo Curino via asuresh)
",Buggy,"The commit message indicates a bug fix in `RLESparseResourceAllocation.getRangeOverlapping()`. The diff shows a modification to the `getRangeOverlapping` method, specifically adding a condition to check if `end` is less than the last key in the navigable map `a`. If it is, the map is truncated using `headMap`. This suggests that the original code might have been incorrectly handling the upper bound of the range, potentially including entries beyond the desired end point. The addition of the `if (end < a.lastKey())` condition aims to correct this behavior, indicating a bug fix.

**Buggy**"
hadoop,3337.json,46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3,"@@ -1,18 +1,18 @@
   private static LocatedBlock toLocatedBlock(final Map<?, ?> m) throws IOException {
     if (m == null) {
       return null;
     }
 
     final ExtendedBlock b = toExtendedBlock((Map<?, ?>)m.get(""block""));
     final DatanodeInfo[] locations = toDatanodeInfoArray(
         (Object[])m.get(""locations""));
     final long startOffset = (Long)m.get(""startOffset"");
     final boolean isCorrupt = (Boolean)m.get(""isCorrupt"");
     final DatanodeInfo[] cachedLocations = toDatanodeInfoArray(
         (Object[])m.get(""cachedLocations""));
 
     final LocatedBlock locatedblock = new LocatedBlock(b, locations,
-        startOffset, isCorrupt, cachedLocations);
+        null, null, startOffset, isCorrupt, cachedLocations);
     locatedblock.setBlockToken(toBlockToken((Map<?, ?>)m.get(""blockToken"")));
     return locatedblock;
   }",Buggy,"HDFS-5508. Fix compilation error after merge. (Contributed by szetszwo)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1541352 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""HDFS-5508. Fix compilation error after merge. (Contributed by szetszwo)"" indicates that the commit is intended to fix a compilation error that arose after a merge operation.

The provided diff shows a change in the `toLocatedBlock` method. Specifically, the constructor call for `LocatedBlock` is modified. The original constructor call was:

```java
new LocatedBlock(b, locations, startOffset, isCorrupt, cachedLocations);
```

The modified constructor call is:

```java
new LocatedBlock(b, locations, null, null, startOffset, isCorrupt, cachedLocations);
```

It appears that the constructor of `LocatedBlock` was changed in another commit, and this commit updates the call to the constructor to match the new signature. The addition of `null, null` suggests that two new parameters were added to the `LocatedBlock` constructor. This change is necessary to resolve a compilation error caused by the mismatch between the constructor call and the constructor definition.

Since the commit message explicitly states that the change fixes a compilation error and the diff confirms a modification to a constructor call to align with a likely updated constructor signature, this commit is addressing a bug introduced by a merge.

**Buggy**"
hadoop,33132.json,c6ea28c480940d1f95cabe3af452dce197c6669d,"@@ -1,23 +1,24 @@
   public List<PrivilegedOperation> bootstrap(Configuration configuration)
       throws ResourceHandlerException {
     Set<Device> availableDevices = null;
     try {
       availableDevices = devicePlugin.getDevices();
     } catch (Exception e) {
       throw new ResourceHandlerException(""Exception thrown from""
           + "" plugin's \""getDevices\"""" + e.getMessage());
     }
     /**
      * We won't fail the NM if plugin returns invalid value here.
      * */
     if (availableDevices == null) {
       LOG.error(""Bootstrap "" + resourceName
           + "" failed. Null value got from plugin's getDevices method"");
       return null;
     }
     // Add device set. Here we trust the plugin's return value
     deviceMappingManager.addDeviceSet(resourceName, availableDevices);
-    // TODO: Init cgroups
-
+    // Init cgroups
+    this.cGroupsHandler.initializeCGroupController(
+        CGroupsHandler.CGroupController.DEVICES);
     return null;
   }",Buggy,"YARN-9331. [YARN-8851] Fix a bug that lacking cgroup initialization when bootstrap DeviceResourceHandlerImpl. Contributed by Zhankun Tang.
",Buggy,"The commit message indicates a bug fix related to missing cgroup initialization in `DeviceResourceHandlerImpl` during bootstrap. The diff shows that the code now initializes cgroups using `this.cGroupsHandler.initializeCGroupController(CGroupsHandler.CGroupController.DEVICES);`. This aligns with the commit message, suggesting that the missing initialization was indeed a bug.

**Buggy**
"
hadoop,22809.json,b524501d4f4b48edeb02901114087f3b5f57691f,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return reduceContext.getCacheArchives();
+      return reduceContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5685. Fixed a bug with JobContext getCacheFiles API inside the WrappedReducer class. Contributed by Yi Song.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554320 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `JobContext.getCacheFiles()` API within the `WrappedReducer` class. The diff shows that `reduceContext.getCacheArchives()` was replaced with `reduceContext.getCacheFiles()`. This change directly addresses the bug mentioned in the commit message, as it corrects the API call to retrieve cache files.

**Buggy**"
hadoop,12133.json,68d5dfdc78d121e89eeae4e577d670028a14a955,"@@ -1,90 +1,90 @@
   void startDataNode(List<StorageLocation> dataDirectories,
                      SecureResources resources
                      ) throws IOException {
 
     // settings global for all BPs in the Data Node
     this.secureResources = resources;
     synchronized (this) {
       this.dataDirs = dataDirectories;
     }
     this.dnConf = new DNConf(this);
     checkSecureConfig(dnConf, getConf(), resources);
 
     if (dnConf.maxLockedMemory > 0) {
       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {
         throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) is greater than zero and native code is not available."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));
       }
       if (Path.WINDOWS) {
         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);
       } else {
         long ulimit = NativeIO.POSIX.getCacheManipulator().getMemlockLimit();
         if (dnConf.maxLockedMemory > ulimit) {
           throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) of %d bytes is more than the datanode's available"" +
             "" RLIMIT_MEMLOCK ulimit of %d bytes."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,
             dnConf.maxLockedMemory,
             ulimit));
         }
       }
     }
     LOG.info(""Starting DataNode with maxLockedMemory = {}"",
         dnConf.maxLockedMemory);
 
     int volFailuresTolerated = dnConf.getVolFailuresTolerated();
     int volsConfigured = dnConf.getVolsConfigured();
     if (volFailuresTolerated < MAX_VOLUME_FAILURE_TOLERATED_LIMIT
         || volFailuresTolerated >= volsConfigured) {
       throw new DiskErrorException(""Invalid value configured for ""
           + ""dfs.datanode.failed.volumes.tolerated - "" + volFailuresTolerated
-          + "". Value configured is either greater than -1 or >= ""
+          + "". Value configured is either less than -1 or >= ""
           + ""to the number of configured volumes ("" + volsConfigured + "")."");
     }
 
     storage = new DataStorage();
     
     // global DN settings
     registerMXBean();
     initDataXceiver();
     startInfoServer();
     pauseMonitor = new JvmPauseMonitor();
     pauseMonitor.init(getConf());
     pauseMonitor.start();
   
     // BlockPoolTokenSecretManager is required to create ipc server.
     this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();
 
     // Login is done by now. Set the DN user name.
     dnUserName = UserGroupInformation.getCurrentUser().getUserName();
     LOG.info(""dnUserName = {}"", dnUserName);
     LOG.info(""supergroup = {}"", supergroup);
     initIpcServer();
 
     metrics = DataNodeMetrics.create(getConf(), getDisplayName());
     peerMetrics = dnConf.peerStatsEnabled ?
         DataNodePeerMetrics.create(getDisplayName()) : null;
     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);
 
     ecWorker = new ErasureCodingWorker(getConf(), this);
     blockRecoveryWorker = new BlockRecoveryWorker(this);
 
     blockPoolManager = new BlockPoolManager(this);
     blockPoolManager.refreshNamenodes(getConf());
 
     // Create the ReadaheadPool from the DataNode context so we can
     // exit without having to explicitly shutdown its thread pool.
     readaheadPool = ReadaheadPool.getInstance();
     saslClient = new SaslDataTransferClient(dnConf.getConf(),
         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);
     saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);
     startMetricsLogger();
 
     if (dnConf.diskStatsEnabled) {
       diskMetrics = new DataNodeDiskMetrics(this,
           dnConf.outliersReportIntervalMs);
     }
   }",Buggy,"HDFS-14056. Fix error messages in HDFS-12716. Contributed by Ayush Saxena.
",Buggy,"The commit message indicates a fix to error messages introduced in HDFS-12716. The diff shows a change in the error message within the `startDataNode` method. Specifically, the condition `volFailuresTolerated < MAX_VOLUME_FAILURE_TOLERATED_LIMIT` is checked, and the error message has been corrected from ""greater than -1"" to ""less than -1"". This suggests that the original error message was misleading or incorrect, and this commit aims to rectify it.

**Buggy**
"
hadoop,33173.json,a457a8951a1b35f06811c40443ca44bb9c698c30,"@@ -1,46 +1,48 @@
   public boolean initPlugin(Configuration conf) {
     this.aliasMap = new HashMap<>();
     if (this.initialized) {
       return true;
     }
     // Find the proper toolchain, mainly aocl
     String pluginDefaultBinaryName = getDefaultBinaryName();
     String pathToExecutable = conf.get(YarnConfiguration.NM_FPGA_PATH_TO_EXEC,
         """");
     if (pathToExecutable.isEmpty()) {
       pathToExecutable = pluginDefaultBinaryName;
     }
     // Validate file existence
     File binaryPath = new File(pathToExecutable);
     if (!binaryPath.exists()) {
       // When binary not exist, fail
       LOG.warn(""Failed to find FPGA discoverer executable configured in "" +
           YarnConfiguration.NM_FPGA_PATH_TO_EXEC +
           "", please check! Try default path"");
       pathToExecutable = pluginDefaultBinaryName;
       // Try to find in plugin's preferred path
       String pluginDefaultPreferredPath = getDefaultPathToExecutable();
       if (null == pluginDefaultPreferredPath) {
         LOG.warn(""Failed to find FPGA discoverer executable from system environment "" +
             getDefaultPathEnvName()+
             "", please check your environment!"");
       } else {
         binaryPath = new File(pluginDefaultPreferredPath + ""/bin"", pluginDefaultBinaryName);
         if (binaryPath.exists()) {
-          pathToExecutable = pluginDefaultPreferredPath;
+          pathToExecutable = binaryPath.getAbsolutePath();
+          LOG.info(""Succeed in finding FPGA discoverer executable: "" +
+              pathToExecutable);
         } else {
           pathToExecutable = pluginDefaultBinaryName;
           LOG.warn(""Failed to find FPGA discoverer executable in "" +
               pluginDefaultPreferredPath + "", file doesn't exists! Use default binary"" + pathToExecutable);
         }
       }
     }
     setPathToExecutable(pathToExecutable);
     if (!diagnose(10*1000)) {
       LOG.warn(""Intel FPGA for OpenCL diagnose failed!"");
       this.initialized = false;
     } else {
       this.initialized = true;
     }
     return this.initialized;
   }",Buggy,"YARN-8456. Fix a configuration handling bug when user leave FPGA discover executable path configuration default but set OpenCL SDK path environment variable. (Zhankun Tang via wangda)

Change-Id: Iff150ea98ba0c60d448474fd940eb121afce6965
",Buggy,"The code modification addresses a scenario where the user leaves the FPGA discover executable path configuration at its default value but sets the OpenCL SDK path environment variable. The original code had a potential issue in resolving the correct path to the executable in this specific configuration.

The changes include:

1.  **Path Resolution Logic**: The code now correctly constructs the absolute path to the executable when it's found in the plugin's preferred path.
2.  **Logging**: Added logging to indicate when the FPGA discoverer executable is successfully found.

The commit message accurately reflects the changes made in the code. The code fixes a configuration handling bug, as stated in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
hadoop,16528.json,c3ca348b81bdf6aa0857a1d4d140c6c9d64be490,"@@ -1,29 +1,29 @@
   public void checkSuperuserPrivilege() throws  AccessControlException {
 
     // Try to get the ugi in the RPC call.
     UserGroupInformation ugi = null;
     try {
       ugi = NameNode.getRemoteUser();
     } catch (IOException e) {
       // Ignore as we catch it afterwards
     }
     if (ugi == null) {
       LOG.error(""Cannot get the remote user name"");
       throw new AccessControlException(""Cannot get the remote user name"");
     }
 
     // Is this by the Router user itself?
-    if (ugi.getUserName().equals(superUser)) {
+    if (ugi.getShortUserName().equals(superUser)) {
       return;
     }
 
     // Is the user a member of the super group?
     List<String> groups = Arrays.asList(ugi.getGroupNames());
     if (groups.contains(superGroup)) {
       return;
     }
 
     // Not a superuser
     throw new AccessControlException(
         ugi.getUserName() + "" is not a super user"");
   }",Buggy,"HDFS-14620. RBF: Fix 'not a super user' error when disabling a namespace in kerberos with superuser principal. Contributed by luhuachao.
",Buggy,"The commit message indicates a fix for a ""not a super user"" error in a Kerberos environment when disabling a namespace. The code diff modifies the `checkSuperuserPrivilege` method.

Here's a breakdown of the changes and reasoning:

1.  **`ugi.getUserName().equals(superUser)` changed to `ugi.getShortUserName().equals(superUser)`**: This is the core of the fix. The original code was comparing the full Kerberos principal name (e.g., `user/host@REALM`) with the `superUser` which likely only contains the short user name (e.g., `user`). This would cause the superuser check to fail in Kerberos environments even when the user *was* a superuser. The change to `getShortUserName()` extracts the short user name from the principal, allowing the comparison to succeed.

The change directly addresses the problem described in the commit message. It fixes a bug where a superuser was incorrectly denied access due to a mismatch in the username format.

**Conclusion: Buggy**"
hadoop,75002.json,c8abf5f20a7ca802e3e7c93c8c5d260902cb4052,"@@ -1,37 +1,44 @@
-  private int init(String[] args) throws IOException {
+  protected int init(String[] args) throws IOException {
+    // no args should print the help message
+    if (0 == args.length) {
+      printCredShellUsage();
+      ToolRunner.printGenericCommandUsage(System.err);
+      return 1;
+    }
+
     for (int i = 0; i < args.length; i++) { // parse command line
       if (args[i].equals(""create"")) {
         String alias = args[++i];
         command = new CreateCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""delete"")) {
         String alias = args[++i];
         command = new DeleteCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""list"")) {
         command = new ListCommand();
       } else if (args[i].equals(""-provider"")) {
         userSuppliedProvider = true;
         getConf().set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, 
             args[++i]);
       } else if (args[i].equals(""-i"") || (args[i].equals(""-interactive""))) {
         interactive = true;
       } else if (args[i].equals(""-v"") || (args[i].equals(""-value""))) {
         value = args[++i];
       } else if (args[i].equals(""-help"")) {
         printCredShellUsage();
-        return -1;
+        return 0;
       } else {
         printCredShellUsage();
         ToolRunner.printGenericCommandUsage(System.err);
-        return -1;
+        return 1;
       }
     }
     return 0;
   }",Buggy,"HADOOP-10927. Fix CredentialShell help behavior and error codes. Contributed by Josh Elser.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615827 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix to the `CredentialShell`'s help behavior and error codes. The diff shows changes in the `init` method, specifically how it handles the `-help` argument and invalid arguments.

Here's a breakdown of the changes:

1.  **Handling no arguments:** The code now explicitly checks for the case where no arguments are provided. In this scenario, it prints the `CredShellUsage` and generic command usage, then returns 1. This aligns with fixing the help behavior.
2.  **`-help` argument handling:** Previously, encountering `-help` would return -1. Now, it returns 0. This suggests a correction in how the help command's exit code is handled. Returning 0 typically indicates success, which is appropriate for displaying help.
3.  **Invalid argument handling:** The code now returns 1 when an invalid argument is encountered, after printing the usage messages. The previous code returned -1. This change suggests a correction of the error code.
4.  **Return code for `create` and `delete` commands:** The code now returns 0 instead of -1 when the alias is ""-help"" for `create` and `delete` commands.

These changes directly address the issues mentioned in the commit message: fixing help behavior and error codes. The modifications ensure that the `CredentialShell` provides appropriate help messages and returns correct error codes based on the input.

**Buggy**
"
hadoop,22351.json,1d915238a6a06d09e1789532994f00f496bd969c,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return mapContext.getCacheArchives();
+      return mapContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5385. Fixed a bug with JobContext getCacheFiles API. Contributed by Omkar Vinit Joshi.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508595 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `JobContext.getCacheFiles` API. The diff shows a change in the implementation of `getCacheFiles()` method, where `mapContext.getCacheArchives()` is replaced with `mapContext.getCacheFiles()`. This suggests that the original implementation was incorrect and returned the cache archives instead of the cache files, which aligns with the bug fix described in the commit message.

**Buggy**"
hadoop,8777.json,3b5ea8750202ad9ed0e297d92a90d6dc772ce12a,"@@ -1,45 +1,45 @@
   FSImageStorageInspector readAndInspectDirs()
       throws IOException {
     Integer layoutVersion = null;
     boolean multipleLV = false;
     StringBuilder layoutVersions = new StringBuilder();
 
     // First determine what range of layout versions we're going to inspect
     for (Iterator<StorageDirectory> it = dirIterator();
          it.hasNext();) {
       StorageDirectory sd = it.next();
       if (!sd.getVersionFile().exists()) {
         FSImage.LOG.warn(""Storage directory "" + sd + "" contains no VERSION file. Skipping..."");
         continue;
       }
       readProperties(sd); // sets layoutVersion
       int lv = getLayoutVersion();
       if (layoutVersion == null) {
         layoutVersion = Integer.valueOf(lv);
       } else if (!layoutVersion.equals(lv)) {
         multipleLV = true;
       }
       layoutVersions.append(""("").append(sd.getRoot()).append("", "").append(lv).append("") "");
     }
     
     if (layoutVersion == null) {
       throw new IOException(""No storage directories contained VERSION information"");
     }
     if (multipleLV) {            
       throw new IOException(
-          ""Storage directories containe multiple layout versions: ""
+          ""Storage directories contain multiple layout versions: ""
               + layoutVersions);
     }
     // If the storage directories are with the new layout version
     // (ie edits_<txnid>) then use the new inspector, which will ignore
     // the old format dirs.
     FSImageStorageInspector inspector;
     if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {
       inspector = new FSImageTransactionalStorageInspector();
     } else {
       inspector = new FSImagePreTransactionalStorageInspector();
     }
     
     inspectStorageDirs(inspector);
     return inspector;
   }",Buggy,"HDFS-3629. Fix the typo in the error message about inconsistent storage layout version. Contributed by Brandon Li. (harsh)


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1359905 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a typo in an error message related to inconsistent storage layout versions. The diff shows a change in the error message string from ""Storage directories containe multiple layout versions"" to ""Storage directories contain multiple layout versions"". This confirms that the commit fixes a typo in the error message.

**Buggy**
"
hadoop,35650.json,c4d7bbda5cb3ceaf54a022f3cf97109e8e190d71,"@@ -1,8 +1,8 @@
-  protected void serviceStart() throws Exception {
-    super.serviceStart();
+  protected void serviceInit(Configuration conf) throws Exception {
+    super.serviceInit(conf);
+    getDispatcher().register(SystemMetricsEventType.class,
+        new TimelineV2EventHandler());
     publishContainerMetrics = getConfig().getBoolean(
         YarnConfiguration.RM_PUBLISH_CONTAINER_METRICS_ENABLED,
         YarnConfiguration.DEFAULT_RM_PUBLISH_CONTAINER_METRICS_ENABLED);
-    getDispatcher().register(SystemMetricsEventType.class,
-        new TimelineV2EventHandler());
   }",Buggy,"YARN-4460. [Bug fix] RM fails to start when SMP is enabled. (Li Lu via Varun Saxena)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""YARN-4460. [Bug fix] RM fails to start when SMP is enabled"" clearly states that this commit is a bug fix. The bug is that the Resource Manager (RM) fails to start when Shared Memory Persistence (SMP) is enabled.

**Git Diff Analysis:**

The diff shows a change in the `serviceStart()` method to `serviceInit(Configuration conf)`. The registration of `TimelineV2EventHandler` with the dispatcher is moved from `serviceStart()` to `serviceInit(Configuration conf)`. The `serviceStart()` method now only calls the super class's `serviceStart()` and sets the `publishContainerMetrics` flag.

**Reasoning:**

The change from `serviceStart()` to `serviceInit(Configuration conf)` suggests that the initialization order or timing of the `TimelineV2EventHandler` registration was causing an issue when SMP was enabled. Moving the registration to `serviceInit(Configuration conf)` likely resolves a dependency or timing issue that prevented the RM from starting correctly in the presence of SMP. The change addresses the problem described in the commit message.

**Conclusion:**

Based on the commit message and the code changes, this commit appears to be a bug fix. The commit message explicitly states it's a bug fix, and the code changes address a startup issue related to SMP.

**Buggy**"
hadoop,34713.json,3d00c8f3942da931150de79f42cd4913bf751123,"@@ -1,3 +1,3 @@
   public Service[] getServices() {
-    return nodeManagerServices;
+    return NODE_MANAGER_SERVICES;
   }",Buggy,"Made fixes for whitespace errors and checstyle warnings before merge.
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Made fixes for whitespace errors and checstyle warnings before merge"" suggests purely cosmetic changes related to code style and formatting. It explicitly mentions whitespace and Checkstyle, which are tools for enforcing coding standards.

2.  **Git Diff:** The diff shows a change from `nodeManagerServices` to `NODE_MANAGER_SERVICES`. This change strongly suggests a modification to adhere to a coding standard where constant variables are named using uppercase with underscores. It's a change in naming convention, not in the underlying logic.

3.  **Alignment:** The code change aligns perfectly with the commit message. The change is about code style, not about fixing a functional defect.

**Conclusion:**

**NotBuggy**
"
hadoop,71639.json,9591765040b85927ac69179ab46383eef9560a28,"@@ -1,25 +1,44 @@
   private byte remoteLookup(Message response, Name name, int type,
       int iterations) {
+    // If retrieving the root zone, query for NS record type
+    if (name.toString().equals(""."")) {
+      type = Type.NS;
+    }
+
+    // Always add any CNAMEs to the response first
+    if (type != Type.CNAME) {
+      Record[] cnameAnswers = getRecords(name, Type.CNAME);
+      if (cnameAnswers != null) {
+        for (Record cnameR : cnameAnswers) {
+          if (!response.findRecord(cnameR)) {
+            response.addRecord(cnameR, Section.ANSWER);
+          }
+        }
+      }
+    }
+
     // Forward lookup to primary DNS servers
     Record[] answers = getRecords(name, type);
     try {
       for (Record r : answers) {
-        if (r.getType() == Type.SOA) {
-          response.addRecord(r, Section.AUTHORITY);
-        } else {
-          response.addRecord(r, Section.ANSWER);
+        if (!response.findRecord(r)) {
+          if (r.getType() == Type.SOA) {
+            response.addRecord(r, Section.AUTHORITY);
+          } else {
+            response.addRecord(r, Section.ANSWER);
+          }
         }
         if (r.getType() == Type.CNAME) {
           Name cname = ((CNAMERecord) r).getAlias();
           if (iterations < 6) {
-            remoteLookup(response, cname, Type.CNAME, iterations + 1);
+            remoteLookup(response, cname, type, iterations + 1);
           }
         }
       }
     } catch (NullPointerException e) {
       return Rcode.NXDOMAIN;
     } catch (Throwable e) {
       return Rcode.SERVFAIL;
     }
     return Rcode.NOERROR;
   }",Buggy,"YARN-8410.  Fixed a bug in A record lookup by CNAME record.
            Contributed by Shane Kumpf
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""YARN-8410. Fixed a bug in A record lookup by CNAME record"" clearly states that the commit addresses a bug related to A record lookups when CNAME records are involved. This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff modifies the `remoteLookup` method, which appears to be responsible for performing DNS lookups. Let's break down the changes:

1.  **Root Zone Handling:**
    ```java
    if (name.toString().equals(""."")) {
      type = Type.NS;
    }
    ```
    This change adds a condition to check if the lookup is for the root zone ("".""). If so, it changes the query type to `Type.NS` (Name Server). This might be related to correctly handling root zone lookups, which could have been a source of errors.

2.  **CNAME Handling:**
    ```java
    if (type != Type.CNAME) {
      Record[] cnameAnswers = getRecords(name, Type.CNAME);
      if (cnameAnswers != null) {
        for (Record cnameR : cnameAnswers) {
          if (!response.findRecord(cnameR)) {
            response.addRecord(cnameR, Section.ANSWER);
          }
        }
      }
    }
    ```
    This section retrieves CNAME records associated with the current name and adds them to the response *before* performing the lookup for the requested type. This seems to prioritize CNAME resolution, which is crucial for correct DNS resolution. The `if (type != Type.CNAME)` condition ensures that this logic is not applied when the original query was for a CNAME record. The `!response.findRecord(cnameR)` check prevents adding duplicate records.

3.  **Duplicate Record Prevention:**
    ```java
    if (!response.findRecord(r)) {
      if (r.getType() == Type.SOA) {
        response.addRecord(r, Section.AUTHORITY);
      } else {
        response.addRecord(r, Section.ANSWER);
      }
    }
    ```
    This change adds a check `!response.findRecord(r)` before adding records to the response. This prevents adding duplicate records, which could lead to incorrect DNS resolution or other issues.

4.  **Recursive CNAME Lookup:**
    ```java
    remoteLookup(response, cname, type, iterations + 1);
    ```
    The recursive call to `remoteLookup` now passes the original `type` instead of `Type.CNAME`. This is a crucial change. Previously, it was always looking up CNAME records recursively, which is incorrect. It should lookup the original record type (e.g., A record) for the CNAME alias. This is the core of the bug fix described in the commit message.

**Reasoning:**

The code changes directly address the issue of A record lookups via CNAME records. The original code was likely failing to resolve A records correctly when a CNAME record was encountered. The changes ensure that CNAME records are handled correctly and that the correct record type is used during recursive lookups. The addition of duplicate record prevention further improves the robustness of the DNS resolution process.

**Conclusion:**

The code changes align with the commit message, and the modifications clearly indicate a bug fix related to CNAME resolution and A record lookups.

**Buggy**
"
hadoop,4984.json,076ecf79ca38cbb908f54ea58d985d24486ceefc,"@@ -1,4 +1,4 @@
   public String toString() {
-    return getClass().getSimpleName() + "": "" + snapshotId + "" (post=""
-        + (posteriorDiff == null? null: posteriorDiff.snapshotId) + "")"";
+    return getClass().getSimpleName() + "": "" + this.getSnapshotId() + "" (post=""
+        + (posteriorDiff == null? null: posteriorDiff.getSnapshotId()) + "")"";
   }",Buggy,"HDFS-5726. Fix compilation error in AbstractINodeDiff for JDK7. Contributed by Jing Zhao.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556433 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error in `AbstractINodeDiff` specifically for JDK7. The provided diff shows a change in the `toString()` method of the `AbstractINodeDiff` class. Instead of directly accessing the `snapshotId` field, the code now uses `this.getSnapshotId()` and `posteriorDiff.getSnapshotId()`. This suggests that `snapshotId` might have been made private or protected, and the change was necessary to access it through a getter method, which is a common pattern for encapsulation. This change directly addresses a compilation error related to accessing a field that is no longer directly accessible, which is a bug fix.

**Buggy**"
hadoop,43166.json,23f394240e1568a38025e63e9dc0842e8c5235f7,"@@ -1,61 +1,61 @@
   public int initiateUpgrade(Service service) throws YarnException,
       IOException {
     boolean upgradeEnabled = getConfig().getBoolean(
         YARN_SERVICE_UPGRADE_ENABLED,
         YARN_SERVICE_UPGRADE_ENABLED_DEFAULT);
     if (!upgradeEnabled) {
       throw new YarnException(ErrorStrings.SERVICE_UPGRADE_DISABLED);
     }
     Service persistedService =
         ServiceApiUtil.loadService(fs, service.getName());
     if (!StringUtils.isEmpty(persistedService.getId())) {
       cachedAppInfo.put(persistedService.getName(), new AppInfo(
           ApplicationId.fromString(persistedService.getId()),
           persistedService.getKerberosPrincipal().getPrincipalName()));
     }
 
     if (persistedService.getVersion().equals(service.getVersion())) {
       String message =
           service.getName() + "" is already at version "" + service.getVersion()
               + "". There is nothing to upgrade."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Service liveService = getStatus(service.getName());
     if (!liveService.getState().equals(ServiceState.STABLE)) {
       String message = service.getName() + "" is at "" +
           liveService.getState()
-          + "" state, upgrade can not be invoked when service is STABLE."";
+          + "" state and upgrade can only be initiated when service is STABLE."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Path serviceUpgradeDir = checkAppNotExistOnHdfs(service, true);
     ServiceApiUtil.validateAndResolveService(service, fs, getConfig());
     ServiceApiUtil.createDirAndPersistApp(fs, serviceUpgradeDir, service);
 
     ApplicationReport appReport =
         yarnClient.getApplicationReport(getAppId(service.getName()));
     if (StringUtils.isEmpty(appReport.getHost())) {
       throw new YarnException(service.getName() + "" AM hostname is empty"");
     }
     ClientAMProtocol proxy = createAMProxy(service.getName(), appReport);
 
     UpgradeServiceRequestProto.Builder requestBuilder =
         UpgradeServiceRequestProto.newBuilder();
     requestBuilder.setVersion(service.getVersion());
     if (service.getState().equals(ServiceState.UPGRADING_AUTO_FINALIZE)) {
       requestBuilder.setAutoFinalize(true);
     }
     UpgradeServiceResponseProto responseProto = proxy.upgrade(
         requestBuilder.build());
     if (responseProto.hasError()) {
       LOG.error(""Service {} upgrade to version {} failed because {}"",
           service.getName(), service.getVersion(), responseProto.getError());
       throw new YarnException(""Failed to upgrade service "" + service.getName()
           + "" to version "" + service.getVersion() + "" because "" +
           responseProto.getError());
     }
     return EXIT_SUCCESS;
   }",Buggy,"YARN-8610.  Fixed initiate upgrade error message.
            Contributed by Chandni Singh
",Buggy,"The commit message ""YARN-8610. Fixed initiate upgrade error message"" indicates a bug fix related to an error message during the service upgrade initiation process.

The code diff modifies an error message within the `initiateUpgrade` method. Specifically, the message thrown when the service is not in a `STABLE` state has been reworded:

- **Original:** `"" state, upgrade can not be invoked when service is STABLE.""`
- **Modified:** `"" state and upgrade can only be initiated when service is STABLE.""`

This change suggests that the original error message was not clear or accurate, and the modification aims to provide a more informative and user-friendly message. This correction directly aligns with the commit message's intent to fix an error message.

Therefore, the changes indicate a bug fix.

**Buggy**"
hadoop,81000.json,1ddb48872f6a4985f4d0baadbb183899226cff68,"@@ -1,9 +1,9 @@
   public long getTimeDuration(String name, long defaultValue,
       TimeUnit defaultUnit, TimeUnit returnUnit) {
     String vStr = get(name);
     if (null == vStr) {
-      return defaultValue;
+      return returnUnit.convert(defaultValue, defaultUnit);
     } else {
       return getTimeDurationHelper(name, vStr, defaultUnit, returnUnit);
     }
   }",Buggy,"HADOOP-16265. Fix bug causing Configuration#getTimeDuration to use incorrect units when the default value is used. Contributed by starphin.
",Buggy,"Okay, let's analyze this commit message and the provided Java code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message clearly states that it's fixing a bug in `Configuration#getTimeDuration`.
    *   The bug is related to incorrect units being used when the default value is returned. This implies that the original code was not properly converting the default value to the desired `returnUnit`.

2.  **Code Diff Analysis:**
    *   The diff shows a change within the `getTimeDuration` method.
    *   Specifically, the line `return defaultValue;` has been replaced with `return returnUnit.convert(defaultValue, defaultUnit);`.
    *   This change directly addresses the issue described in the commit message. The original code was simply returning the `defaultValue` without any unit conversion. The corrected code now converts the `defaultValue` (which is assumed to be in `defaultUnit`) to the `returnUnit` before returning it.

3.  **Alignment and Bug Fix Indication:**
    *   The code change perfectly aligns with the commit message. The commit message describes a bug related to incorrect units when using the default value, and the code change fixes exactly that by adding the necessary unit conversion.
    *   The original code's behavior was clearly incorrect (returning a value in the wrong units), which constitutes a bug. The corrected code fixes this incorrect behavior.

**Conclusion:**

**Buggy**
"
hadoop,10220.json,3335e502446b1542fc99c0c831e0542e53eac6f1,"@@ -1,11 +1,11 @@
-  void shutDownAll() throws InterruptedException {
-    BPOfferService[] bposArray = this.getAllNamenodeThreads();
-    
-    for (BPOfferService bpos : bposArray) {
-      bpos.stop(); //interrupts the threads
-    }
-    //now join
-    for (BPOfferService bpos : bposArray) {
-      bpos.join();
+  void shutDownAll(BPOfferService[] bposArray) throws InterruptedException {
+    if (bposArray != null) {
+      for (BPOfferService bpos : bposArray) {
+        bpos.stop(); //interrupts the threads
+      }
+      //now join
+      for (BPOfferService bpos : bposArray) {
+        bpos.join();
+      }
     }
   }",Buggy,"HDFS-3616. Fix a ConcurrentModificationException bug that BP actor threads may not be shutdown properly in DataNode.  Contributed by Jing Zhao 


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402608 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a `ConcurrentModificationException` related to Block Pool (BP) actor threads in DataNode. The provided diff shows a modification to the `shutDownAll` method.

The original code iterates through `BPOfferService` threads obtained from `getAllNamenodeThreads()` and stops/joins them. The modified code adds a null check for the `bposArray` before iterating and performing stop/join operations. This suggests that `getAllNamenodeThreads()` might return null under certain conditions, which could lead to a `NullPointerException` if not handled. The addition of the null check prevents this exception, which could be related to the `ConcurrentModificationException` mentioned in the commit message if the list of namenode threads is being modified concurrently while the shutdown process is ongoing.

Therefore, the changes indicate a bug fix.

**Buggy**"
hadoop,26655.json,977d7cc5b947682478ad7b38bd442f0efa1cd204,"@@ -1,45 +1,38 @@
     protected void setup(JobImpl job) throws IOException {
 
       String oldJobIDString = job.oldJobId.toString();
       String user = 
         UserGroupInformation.getCurrentUser().getShortUserName();
       Path path = MRApps.getStagingAreaDir(job.conf, user);
       if(LOG.isDebugEnabled()) {
         LOG.debug(""startJobs: parent="" + path + "" child="" + oldJobIDString);
       }
 
       job.remoteJobSubmitDir =
           FileSystem.get(job.conf).makeQualified(
               new Path(path, oldJobIDString));
       job.remoteJobConfFile =
           new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);
 
       // Prepare the TaskAttemptListener server for authentication of Containers
       // TaskAttemptListener gets the information via jobTokenSecretManager.
       JobTokenIdentifier identifier =
           new JobTokenIdentifier(new Text(oldJobIDString));
       job.jobToken =
           new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);
       job.jobToken.setService(identifier.getJobId());
       // Add it to the jobTokenSecretManager so that TaskAttemptListener server
       // can authenticate containers(tasks)
       job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);
       LOG.info(""Adding job token for "" + oldJobIDString
           + "" to jobTokenSecretManager"");
 
       // Upload the jobTokens onto the remote FS so that ContainerManager can
       // localize it to be used by the Containers(tasks)
       Credentials tokenStorage = new Credentials();
       TokenCache.setJobToken(job.jobToken, tokenStorage);
 
       if (UserGroupInformation.isSecurityEnabled()) {
         tokenStorage.addAll(job.fsTokens);
       }
-
-      Path remoteJobTokenFile =
-          new Path(job.remoteJobSubmitDir,
-              MRJobConfig.APPLICATION_TOKENS_FILE);
-      tokenStorage.writeTokenStorageFile(remoteJobTokenFile, job.conf);
-      LOG.info(""Writing back the job-token file on the remote file system:""
-          + remoteJobTokenFile.toString());
     }",Buggy,"MAPREDUCE-3233. Fixed a bug in MR Job so as to be able to restart the application on AM crash. Contributed by Mahadev Konar.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187669 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to restarting a MapReduce job after an Application Master (AM) crash. The code diff removes the writing of the job-token file to the remote file system. This change suggests that the previous implementation might have had an issue with how job tokens were handled during AM restarts, potentially leading to failures or authentication problems. Removing the redundant writing of the token to the remote file system could be a way to fix the bug.

**Buggy**
"
hadoop,10300.json,eb7fe1d588de903be2ff6e20384c25c184881532,"@@ -1,24 +1,26 @@
   private void breakHardlinks(File file, Block b) throws IOException {
     final FileIoProvider fileIoProvider = getFileIoProvider();
     final File tmpFile = DatanodeUtil.createFileWithExistsCheck(
         getVolume(), b, DatanodeUtil.getUnlinkTmpFile(file), fileIoProvider);
-    try (FileInputStream in = fileIoProvider.getFileInputStream(
-        getVolume(), file)) {
-      try (FileOutputStream out = fileIoProvider.getFileOutputStream(
-          getVolume(), tmpFile)) {
-        IOUtils.copyBytes(in, out, 16 * 1024);
+    try {
+      try (FileInputStream in = fileIoProvider.getFileInputStream(
+          getVolume(), file)) {
+        try (FileOutputStream out = fileIoProvider.getFileOutputStream(
+            getVolume(), tmpFile)) {
+          IOUtils.copyBytes(in, out, 16 * 1024);
+        }
       }
       if (file.length() != tmpFile.length()) {
-        throw new IOException(""Copy of file "" + file + "" size "" + file.length()+
-                              "" into file "" + tmpFile +
-                              "" resulted in a size of "" + tmpFile.length());
+        throw new IOException(""Copy of file "" + file + "" size "" + file.length()
+            + "" into file "" + tmpFile + "" resulted in a size of ""
+            + tmpFile.length());
       }
       fileIoProvider.replaceFile(getVolume(), tmpFile, file);
     } catch (IOException e) {
       if (!fileIoProvider.delete(getVolume(), tmpFile)) {
         DataNode.LOG.info(""detachFile failed to delete temporary file "" +
                           tmpFile);
       }
       throw e;
     }
   }",Buggy,"HDFS-13509. Bug fix for breakHardlinks() of ReplicaInfo/LocalReplica, and fix TestFileAppend failures on Windows. Contributed by Xiao Liang.
",Buggy,"The commit message indicates a bug fix in the `breakHardlinks()` method and a fix for `TestFileAppend` failures on Windows. The code diff modifies the `breakHardlinks()` method.

The original code had nested try-with-resources blocks for `FileInputStream` and `FileOutputStream`. The change encloses the entire block within a single `try` block, with a `catch` block to handle `IOExceptions`. Inside the `catch` block, there's an attempt to delete a temporary file if the `replaceFile` operation fails. The `IOException` is then re-thrown.

The change ensures that even if an exception occurs during the copy process or file replacement, the temporary file is cleaned up, preventing potential resource leaks. The added check `file.length() != tmpFile.length()` throws an `IOException` if the sizes of the original and temporary files differ after the copy, indicating a potential data corruption issue. This suggests that the original implementation might have had issues with handling exceptions during the hardlink breaking process, potentially leading to incomplete or corrupted files. The changes address this by adding more robust error handling and checks.

Therefore, the changes indicate a bug fix.

**Buggy**
"
hadoop,62895.json,2e61ed306f1d525096a800f28546601ef585a832,"@@ -1,14 +1,14 @@
   public boolean equals(Object obj) {
     if (this == obj)
       return true;
-    if (!super.equals(obj))
+    if (obj == null)
       return false;
     if (getClass() != obj.getClass())
       return false;
     NodeId other = (NodeId) obj;
     if (!this.getHost().equals(other.getHost()))
       return false;
     if (this.getPort() != other.getPort())
       return false;
     return true;
   }",Buggy,"MAPREDUCE-3030. Fixed a bug in NodeId.equals() that was causing RM to reject all NMs. Contributed by Devaraj K.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172638 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""MAPREDUCE-3030. Fixed a bug in NodeId.equals() that was causing RM to reject all NMs. Contributed by Devaraj K."" clearly states that a bug in the `NodeId.equals()` method was fixed. This bug was causing the Resource Manager (RM) to reject all Node Managers (NMs). This strongly suggests a bug fix.

**Code Diff Analysis:**

The code diff shows a change in the `equals()` method of the `NodeId` class. The original code had the following line:

```java
if (!super.equals(obj))
```

This line was removed and replaced with:

```java
if (obj == null)
```

**Reasoning:**

The original code was attempting to call `super.equals(obj)`. However, `NodeId` does not inherit from a class that overrides the `equals()` method. Therefore, `super.equals(obj)` would simply call `Object.equals(obj)`, which checks for object identity (i.e., `this == obj`). This check was redundant because the first line of the method already performs this check.

The corrected code now checks if the input object `obj` is `null`. This is a standard and necessary check in an `equals()` method to prevent a `NullPointerException`. If `obj` is `null`, the method should return `false`.

The bug was likely that the original `super.equals(obj)` call was causing unexpected behavior or incorrect comparisons, leading to the RM rejecting NMs. The corrected code ensures that `null` objects are handled correctly, which is crucial for the proper functioning of the `equals()` method.

**Conclusion:**

The commit message explicitly states that a bug was fixed in the `NodeId.equals()` method. The code diff shows a change to the `equals()` method that addresses a potential `NullPointerException` and corrects the logic of the comparison. The changes align with the commit message and indicate a bug fix.

**Buggy**
"
facebook-android-sdk,2616.json,e3a111ea10f0646633777dd8cf5ed06838bf2a62,"@@ -1,13 +1,14 @@
     static float[] maxPool1D(float[] x, int rows, int cols, int pool_size) {
         int len = rows - pool_size + 1;
         float[] res = new float[len * cols];
 
         for (int c = 0; c < cols; c++) {
             for (int i = 0; i < len; i++) {
-                for (int r = i; r < i + pool_size; r++) {
+                res[i * cols + c] = x[i * cols + c];
+                for (int r = i + 1; r < i + pool_size; r++) {
                     res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);
                 }
             }
         }
         return res;
     }",Buggy,"Fix the bug of MaxPool1D

Summary:
Previously, for code:
```
for (int r = i; r < i + pool_size; r++) {
    res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);​
```
will get 0 if all the number in x is smaller than 0, which would make the prediction not accurate if there is no relu() after maxpooling.

Reviewed By: ninanina0815

Differential Revision: D20881716

fbshipit-source-id: a8a50f1fb584a35780683377a10548f19ab4fede
",Buggy,"The commit message indicates a bug fix in the `maxPool1D` function. The original code would return 0 if all input values were negative, leading to inaccurate predictions. The fix initializes `res[i * cols + c]` with `x[i * cols + c]` before the inner loop, ensuring that the maximum value is correctly computed even when all input values are negative. The loop condition is also changed from `r = i` to `r = i + 1` to avoid comparing the initial value with itself. This change directly addresses the bug described in the commit message.

**Buggy**"
facebook-android-sdk,2859.json,6b275f68c69db30100c542919db3325837938c25,"@@ -1,5 +1,6 @@
     public void onReceive(Context context, Intent intent) {
-        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())) {
+        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())
+                && FacebookSdk.isInitialized()) {
             AccessTokenManager.getInstance().currentAccessTokenChanged();
         }
     }",Buggy,"Fixes bug with SDK Initialize on broadcast

Summary: [android sdk] Fixes bug with SDK Initialize on broadcast

Reviewed By: ct2mak

Differential Revision: D15450566

fbshipit-source-id: 7ec5a7f8f55a658959a5ac7143bd8577c57a2a74
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message explicitly states ""Fixes bug with SDK Initialize on broadcast"". This strongly suggests a bug fix. The message also indicates the fix relates to the SDK initialization process when a broadcast is received.

2.  **Code Diff:** The code modifies the `onReceive` method of a class, presumably a `BroadcastReceiver`. The change adds a condition to the `if` statement: `FacebookSdk.isInitialized()`. This condition checks if the Facebook SDK is initialized before proceeding with `AccessTokenManager.getInstance().currentAccessTokenChanged()`.

3.  **Reasoning:**
    *   The original code potentially called `AccessTokenManager.getInstance().currentAccessTokenChanged()` even when the Facebook SDK was not properly initialized. This could lead to errors or unexpected behavior, especially if `currentAccessTokenChanged()` relies on the SDK being initialized.
    *   The added check `FacebookSdk.isInitialized()` prevents the execution of `AccessTokenManager.getInstance().currentAccessTokenChanged()` before the SDK is ready. This addresses the potential bug described in the commit message.
    *   The commit message and the code change are highly relevant to each other. The code change directly implements the fix described in the commit message.

**Conclusion:**

The commit message explicitly states a bug fix, and the code change adds a check to prevent a method call before the SDK is initialized. This strongly indicates a bug fix related to the SDK initialization process.

**Buggy**"
facebook-android-sdk,1309.json,39a0d134e7f137fc0b7d727eb0d0f229d43f1db0,"@@ -1,20 +1,20 @@
     private String getChromePackage() {
         if (currentPackage != null) {
             return currentPackage;
         }
         Context context = loginClient.getActivity();
-        Intent serviceIntent = new Intent(CUSTOM_TABS_SERVICE_ACTION);
+        Intent serviceIntent = new Intent(CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION);
         List<ResolveInfo> resolveInfos =
                 context.getPackageManager().queryIntentServices(serviceIntent, 0);
         if (resolveInfos != null) {
             Set<String> chromePackages = new HashSet<>(Arrays.asList(CHROME_PACKAGES));
             for (ResolveInfo resolveInfo : resolveInfos) {
                 ServiceInfo serviceInfo = resolveInfo.serviceInfo;
                 if (serviceInfo != null && chromePackages.contains(serviceInfo.packageName)) {
                     currentPackage = serviceInfo.packageName;
                     return currentPackage;
                 }
             }
         }
         return null;
     }",Buggy,"AndroidX Custom Tab Issue Fix (#670)

Summary:
Thanks for proposing a pull request!

To help us review the request, please complete the following:

- [ ] sign [contributor license agreement](https://developers.facebook.com/opensource/cla)
- [ ] I've ensured that all existing tests pass and added tests (when/where necessary)
- [ ] I've updated the documentation (when/where necessary) and [Changelog](CHANGELOG.md) (when/where necessary)
- [ ] I've added the proper label to this pull request (e.g. `bug` for bug fixes)

## Pull Request Details
Custom Tab has to be working for AndroidX enabled projects

## Expected Results
What do you expect to happen?
If the Facebook app is not installed, then on click of the Facebook login button in App, Custom Chrome Tab has to be open.

## Actual Results
What actually happened? Can you provide a stack trace?
If the Facebook app is not installed, then on click of Facebook login button in App, by default WebView is opening instead of Custom Chrome Tab even I have enabled the Custom Tab for my project.

## Steps to Reproduce
What are the steps necessary to reproduce this issue?
Integrate Facebook Login Button in App
Uninstall Facebook App from the Device
Make sure your project is migrated to AndroidX.
Click on the ""Login With Facebook"" button, WebView is opening instead of Custom Chrome Tab

Please merge the Pull Requests, so that we can use the Custom Chrome Tab in Facebook SDK for AndroidX Projects
Pull Request resolved: https://github.com/facebook/facebook-android-sdk/pull/670

Test Plan:
Built Hackbook with Gradle and buck and chrome custom tabs works for both
Buck: buck install hb4a. This uses Android X and is not working without this fix
Gradle: On Android studio

Went to Login > Login with custom tab to test the feature

Reviewed By: Mxiim

Differential Revision: D19603133

Pulled By: ct2mak

fbshipit-source-id: 44d2f463d2fbd3a50646dc8caf3e4f7cb02c026b
",Buggy,"The commit message clearly describes a bug where Custom Chrome Tabs were not opening in AndroidX-enabled projects when the Facebook app was not installed. Instead, a WebView was opening. The provided steps to reproduce confirm this behavior. The code diff shows a change in the Intent action used to find a Custom Tabs service. The original code used a constant `CUSTOM_TABS_SERVICE_ACTION`, while the updated code uses `CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION`. This change likely fixes the issue of the app not correctly identifying and using the Custom Tabs service in AndroidX environments. The commit message, steps to reproduce, and code diff all point to a bug fix.

**Buggy**
"
facebook-android-sdk,2911.json,4c16e0ff5234b07ad26111a74cade26f1fecfcb0,"@@ -1,78 +1,80 @@
     public static void setAppEventExtendedDeviceInfoParameters(
             JSONObject params,
             Context appContext
     ) throws JSONException {
         JSONArray extraInfoArray = new JSONArray();
         extraInfoArray.put(EXTRA_APP_EVENTS_INFO_FORMAT_VERSION);
 
         Utility.refreshPeriodicExtendedDeviceInfo(appContext);
 
         // Application Manifest info:
         String pkgName = appContext.getPackageName();
         int versionCode = -1;
         String versionName = """";
 
         try {
             PackageInfo pi = appContext.getPackageManager().getPackageInfo(pkgName, 0);
             versionCode = pi.versionCode;
             versionName = pi.versionName;
         } catch (PackageManager.NameNotFoundException e) {
             // Swallow
         }
 
         // Application Manifest info:
         extraInfoArray.put(pkgName);
         extraInfoArray.put(versionCode);
         extraInfoArray.put(versionName);
 
         // OS/Device info
         extraInfoArray.put(Build.VERSION.RELEASE);
         extraInfoArray.put(Build.MODEL);
 
         // Locale
         Locale locale;
         try {
             locale = appContext.getResources().getConfiguration().locale;
         } catch (Exception e) {
             locale = Locale.getDefault();
         }
         extraInfoArray.put(locale.getLanguage() + ""_"" + locale.getCountry());
 
         // Time zone
         extraInfoArray.put(deviceTimezoneAbbreviation);
 
         // Carrier
         extraInfoArray.put(carrierName);
 
         // Screen dimensions
         int width = 0;
         int height = 0;
         double density = 0;
         try {
             WindowManager wm = (WindowManager) appContext.getSystemService(Context.WINDOW_SERVICE);
             if (wm != null) {
                 Display display = wm.getDefaultDisplay();
                 DisplayMetrics displayMetrics = new DisplayMetrics();
                 display.getMetrics(displayMetrics);
                 width = displayMetrics.widthPixels;
                 height = displayMetrics.heightPixels;
                 density = displayMetrics.density;
             }
         } catch (Exception e) {
             // Swallow
         }
         extraInfoArray.put(width);
         extraInfoArray.put(height);
-        extraInfoArray.put(String.format(""%.2f"", density));
+
+        final DecimalFormat df = new DecimalFormat(""#.##"");
+        extraInfoArray.put(df.format(density));
 
         // CPU Cores
         extraInfoArray.put(refreshBestGuessNumberOfCPUCores());
 
         // External Storage
         extraInfoArray.put(totalExternalStorageGB);
         extraInfoArray.put(availableExternalStorageGB);
 
         extraInfoArray.put(deviceTimeZoneName);
 
         params.put(""extinfo"", extraInfoArray.toString());
     }",Buggy,"Resolve Issue with String Format

Summary:
Should fix a problem where `String.format` crashes with:

```
Fatal Exception: java.lang.NullPointerException
Attempt to get length of null array
```

For more context, view the [bug report](https://developers.internmc.facebook.com/bugs/406495719925086/)

Reviewed By: KylinChang

Differential Revision: D14219986

fbshipit-source-id: 7d2a4ec0720deb48661fabcbcb39d861ff5b70a7
",Buggy,"The commit message indicates a fix for a `NullPointerException` that occurs during `String.format`. The code diff replaces `String.format(""%.2f"", density)` with a `DecimalFormat` object to format the density. The original code might have crashed if `density` was null, which would cause `String.format` to throw a `NullPointerException` when trying to format the null value. The new code uses `DecimalFormat`, which handles null values more gracefully. This change aligns with the commit message and fixes the reported bug.

**Buggy**"
facebook-android-sdk,3863.json,1583b645976fdcdbe41d34020b13fb6939c5bf8a,"@@ -1,5 +1,5 @@
     public final String getAccessToken() {
         synchronized (this.lock) {
-            return this.tokenInfo.getToken();
+            return (this.tokenInfo == null) ? null : this.tokenInfo.getToken();
         }
     }",Buggy,"[android-sdk] Don't add access_token parameters from a closed Session.

Summary:
Request would try to append an access_token if it could get one from a supplied Session, even if the Session
was closed. This is incorrect; it should act as if no Session was provided and let the server return an error if
appropriate.

Test Plan:
- Modified HelloFacebook to always enable the ""Pick Some Friends"" button
- Logged in
- Picked friends
- Logged out
- Picked friends, observed an error message indicating no access token was provided, as expected
- Added unit tests (and fixed one broken one), verified they ran

Revert Plan:

Reviewers: mmarucheck, mingfli, karthiks, gregschechte

Reviewed By: mmarucheck

Differential Revision: https://phabricator.fb.com/D616959

Task ID: 1852996
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message clearly states the issue: the code was incorrectly trying to append an `access_token` to a request even when the `Session` was closed.
    *   It explains the expected behavior: when the `Session` is closed, the code should behave as if no `Session` was provided, allowing the server to handle the missing token appropriately.
    *   The test plan confirms the fix: logging out and then attempting to pick friends results in an expected error message indicating the absence of an access token.

2.  **Code Diff Analysis:**
    *   The code diff modifies the `getAccessToken()` method.
    *   The original code directly returned `this.tokenInfo.getToken()`. This would likely cause a `NullPointerException` if `this.tokenInfo` was null, which could happen if the session was closed or not properly initialized.
    *   The modified code adds a null check: `(this.tokenInfo == null) ? null : this.tokenInfo.getToken()`. This prevents the `NullPointerException` by returning `null` if `this.tokenInfo` is null.

3.  **Relationship between Commit Message and Code Diff:**
    *   The code change directly addresses the issue described in the commit message. The null check ensures that the code doesn't try to access the token from a non-existent `tokenInfo` when the session is closed, which aligns with the intended behavior.
    *   The original code's potential for a `NullPointerException` when the session is closed can be considered a bug. The fix prevents this exception and ensures the correct behavior.

**Conclusion:**

The commit message describes a bug where the code attempts to access an access token from a closed session. The code diff introduces a null check to prevent a `NullPointerException` in this scenario. This clearly indicates a bug fix.

**Buggy**
"
titan,920.json,74166f9174459bcbd8b7b8d7812d6b14c90faba5,"@@ -1,15 +1,34 @@
-    public void unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
+    public boolean unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
 
-        assert locks.containsKey(kc);
-
-        AuditRecord audit = new AuditRecord(requestor, 0);
-
-        assert locks.get(kc).equals(audit);
-
-        locks.remove(kc, audit);
-
-        if (log.isTraceEnabled()) {
-            log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
-                    new Object[]{kc, name, requestor});
+        if (!locks.containsKey(kc)) {
+            log.error(""Local unlock failed: no locks found for {}"", kc);
+            return false;
         }
+
+        AuditRecord unlocker = new AuditRecord(requestor, 0);
+
+        AuditRecord holder = locks.get(kc);
+
+        if (!holder.equals(unlocker)) {
+            log.error(""Local unlock of {} by {} failed: it is held by {}"",
+                    new Object[] { kc, unlocker, holder });
+            return false;
+        }
+
+        boolean removed = locks.remove(kc, unlocker);
+
+        if (removed) {
+            if (log.isTraceEnabled()) {
+                log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
+                        new Object[] { kc, name, requestor });
+            }
+        } else {
+            log.warn(""Local unlock warning: lock record for {} disappeared ""
+                    + ""during removal; this suggests the lock either expired ""
+                    + ""while we were removing it, or that it was erroneously ""
+                    + ""unlocked multiple times."", kc);
+        }
+
+        // Even if !removed, we're finished unlocking, so return true
+        return true;
     }",Buggy,"Locking logging tweaks and renewal fix

* Added new logging statements and reworded existing statements in
  ConsistentKeyLockTransaction and LocalLockMediator

* LocalLockMediator#unlock() now logs errors and returns false instead
  of generating assertion failures when called on an expired or
  nonexistent lock; call sites updated accordingly

* Replaced broken unit test in LockKeyColumnValueStoreTest named
  relockExtendsLocalExpiration with a new test method called
  repeatLockingDoesNotExtendExpiration (closes #264)
",Buggy,"The commit message indicates several changes: logging tweaks, a fix in `LocalLockMediator#unlock()`, and a unit test replacement.

The diff shows a modification in `LocalLockMediator#unlock()`. The original code used assertions to check if the lock existed and if the requestor was the lock holder. Now, it uses `if` statements to check these conditions and logs errors if they are not met, returning `false` instead of throwing an assertion error. It also adds a warning log if the lock record disappears during removal, suggesting a possible expiration or multiple unlocks. The return type of the method is also changed from `void` to `boolean`.

These changes suggest a bug fix because the original code used assertions, which are typically disabled in production environments. Replacing assertions with error logging and returning a boolean value makes the code more robust and provides better error handling. The added logging also helps in diagnosing potential issues with lock management.

Therefore, the changes indicate a bug fix.
**Buggy**
"
titan,316.json,9b77325b51d00dd5c81ddce7b8cf312b04d7057c,"@@ -1,4 +1,9 @@
     public <O> void set(String key, O value) {
-        if (value==null) config.clearProperty(key);
-        else config.setProperty(key,value);
+        if (value==null) {
+            config.clearProperty(key);
+        } else if (Duration.class.isAssignableFrom(value.getClass())) {
+            config.setProperty(key,((Duration)value).getLength(TimeUnit.MILLISECONDS));
+        } else {
+            config.setProperty(key,value);
+        }
     }",Buggy,"Make CommonsConfiguration write Durations as ms

CommonsConfiguration read Durations as unitless positive milliseconds
expressed as a bare integer, but would serialize through the toString
behavior used for the general case.  The toString was something like
""Duration [60 ms]"", so it was asymmetrical.  Special-casing Duration
on the write path to make it symmetrical.  This fixes an error when
copying the graph's local configuration into a Hadoop configuration
object prior to launching a MapReduce-based index management job.
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message clearly states a bug fix related to how `CommonsConfiguration` handles `Duration` objects. It highlights an asymmetry in how `Duration`s are read (as milliseconds) versus how they are written (using `toString`, resulting in a string like ""Duration [60 ms]""). This asymmetry caused problems when copying configurations, specifically in a MapReduce context. The commit aims to serialize `Duration` objects as milliseconds to maintain symmetry.

**Diff Analysis:**

The diff shows a modification to the `set` method.  The key part is the added `else if` condition:

```java
else if (Duration.class.isAssignableFrom(value.getClass())) {
    config.setProperty(key,((Duration)value).getLength(TimeUnit.MILLISECONDS));
}
```

This code explicitly checks if the value being set is a `Duration`. If it is, it extracts the duration in milliseconds using `getLength(TimeUnit.MILLISECONDS)` and then sets this millisecond value in the `config`. This directly addresses the issue described in the commit message, ensuring that `Duration` objects are written as milliseconds, consistent with how they are read.

**Relevance and Bug Fix Identification:**

The code change perfectly aligns with the commit message. The commit message describes an asymmetry in `Duration` handling, and the code change introduces a specific mechanism to serialize `Duration` objects as milliseconds, resolving the asymmetry. The commit message explicitly mentions that this fixes an error in a specific use case (copying configurations for MapReduce). This strongly suggests that the change is a bug fix.

**Conclusion:**

**Buggy**
"
titan,3867.json,a457c6bc3a440d72e045436f8be7a93948568f2e,"@@ -1,21 +1,21 @@
     public List<SliceQuery> getQueries() {
         if (isGlobalGraphIndex()) {
             //Everything
-            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(128), BufferUtil.oneBuffer(128)));
+            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(1), BufferUtil.oneBuffer(128)));
         } else {
             RelationTypeIndexWrapper wrapper = (RelationTypeIndexWrapper)index;
             InternalRelationType wrappedType = wrapper.getWrappedType();
             Direction direction=null;
             for (Direction dir : Direction.values()) if (wrappedType.isUnidirected(dir)) direction=dir;
             assert direction!=null;
 
             StandardTitanTx tx = (StandardTitanTx)graph.get().buildTransaction().readOnly().start();
             try {
                 QueryContainer qc = new QueryContainer(tx);
                 qc.addQuery().type(wrappedType).direction(direction).relations();
                 return qc.getSliceQueries();
             } finally {
                 tx.rollback();
             }
         }
     }",Buggy,"Fixed bug in IndexRemoveJob. The issue was that a slicequery starting with 128 0s was used as the start slice in an attempt to get all columns. However, if the contents of the buffer are identical it compares the lengths. Hence, a staticbuffer with just a single 0 in it will be considered smaller and hence such index records werent' returned.
",Buggy,"The commit message indicates a bug fix in `IndexRemoveJob` related to how slice queries are constructed, specifically when fetching all columns. The issue was that a slice query starting with 128 zeros was being used, which caused problems when comparing buffer lengths, leading to index records not being returned.

The code diff shows a change in the `getQueries()` method. In the case of a global graph index, the start slice of the `SliceQuery` is being modified.  It changes from `BufferUtil.zeroBuffer(128)` to `BufferUtil.zeroBuffer(1)`. This aligns perfectly with the commit message, which states that the original code used a buffer of 128 zeros, and the fix involves using a different (presumably smaller) buffer. This change directly addresses the bug described in the commit message.

**Buggy**"
spring-boot,2459.json,ccb1eaf8eda94ae41db2a432b1781efb421c86c8,"@@ -1,3 +1,3 @@
-		public RedisFlushMode getFlushMode() {
-			return this.flushMode;
-		}

+	public HazelcastFlushMode getFlushMode() {
+		return this.flushMode;
+	}",NotBuggy,"Ensure compatibility with Spring Session module split

This commit updates Spring Session auto-configuration to ensure
compatibility with extraction of `SessionRepository` implementations into
separate Spring Session modules.

See gh-9554
",NotBuggy,"The commit message indicates a compatibility update related to Spring Session module splitting. The code diff changes `RedisFlushMode` to `HazelcastFlushMode`. This suggests a change in the underlying session management implementation or configuration. The change itself doesn't scream bug fix, but rather an adaptation to a new module structure or a switch in session management strategy. The commit message mentions ""compatibility,"" which could imply fixing an incompatibility issue arising from the module split. However, without more context, it's difficult to definitively say if this is a bug fix. It seems more like a configuration or adaptation change.

**NotBuggy**"
spring-boot,686.json,d6977ea89f526f45319fb1f57f8eb679e0f5132a,"@@ -1,13 +1,13 @@
 	public void validate() {
-		if (this.getOpaqueToken().getIntrospectionUri() != null) {
+		if (this.getOpaquetoken().getIntrospectionUri() != null) {
 			if (this.getJwt().getJwkSetUri() != null) {
 				handleError(""jwt.jwk-set-uri"");
 			}
 			if (this.getJwt().getIssuerUri() != null) {
 				handleError(""jwt.issuer-uri"");
 			}
 			if (this.getJwt().getPublicKeyLocation() != null) {
 				handleError(""jwt.public-key-location"");
 			}
 		}
 	}",NotBuggy,"Remove hyphenation of opaquetoken in OAuth2 property names

See gh-17290
",NotBuggy,"The commit message indicates the removal of hyphenation in OAuth2 property names, specifically ""opaquetoken"". The diff shows a change from `this.getOpaqueToken()` to `this.getOpaquetoken()`. This change directly reflects the commit message's intention to remove the hyphen. There is no indication of bug fixing, error handling, or logical correction in the code change. The change seems to be purely related to renaming properties.

**NotBuggy**"
spring-boot,8615.json,a6a19292745d2e70aee12feed6660ef0a14840d2,"@@ -1,12 +1,12 @@
-	private void addAstTransformations(final CompilationUnit compilationUnit) {
+	private LinkedList[] getPhaseOperations(final CompilationUnit compilationUnit) {
 		try {
 			Field field = CompilationUnit.class.getDeclaredField(""phaseOperations"");
 			field.setAccessible(true);
 			LinkedList[] phaseOperations = (LinkedList[]) field.get(compilationUnit);
-			processConversionOperations(phaseOperations[Phases.CONVERSION]);
+			return phaseOperations;
 		}
-		catch (Exception npe) {
+		catch (Exception ex) {
 			throw new IllegalStateException(
 					""Phase operations not available from compilation unit"");
 		}
 	}",NotBuggy,"Extract inner-classes from GroovyCompiler

Extract and refactor inner-classes from the GroovyCompiler to improve
code readability.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Extract inner-classes from GroovyCompiler"" suggests a refactoring effort focused on improving code readability by extracting inner classes. The message doesn't explicitly mention fixing a bug.

**Git Diff Analysis:**

The diff shows a change in the `addAstTransformations` method, which has been renamed to `getPhaseOperations`. The code now returns the `phaseOperations` array instead of processing the conversion operations. The exception handling has also been updated to throw an `IllegalStateException` with a more informative message if the `phaseOperations` field cannot be accessed.

**Reasoning:**

The commit message indicates a refactoring for readability. The code changes involve renaming a method and changing its return type. The exception handling is slightly improved with a more specific error message. While the changes improve the code, they don't directly address a bug. The renaming and change in return type suggest a restructuring of the code, rather than a fix for incorrect behavior.

**Conclusion:**

NotBuggy
"
spring-boot,4024.json,2dfd916c96b8fa16c21a9dfcd1a4b9a06893fff4,"@@ -1,12 +1,10 @@
 	public CloudFoundryInfoEndpointWebExtension cloudFoundryInfoEndpointWebExtension(
 			GitProperties properties, ObjectProvider<InfoContributor> infoContributors) {
 		List<InfoContributor> contributors = infoContributors.orderedStream()
-				.map((infoContributor) -> {
-					if (infoContributor instanceof GitInfoContributor) {
-						return new GitInfoContributor(properties,
-								InfoPropertiesInfoContributor.Mode.FULL);
-					}
-					return infoContributor;
-				}).collect(Collectors.toList());
+				.map((infoContributor) -> (infoContributor instanceof GitInfoContributor)
+						? new GitInfoContributor(properties,
+								InfoPropertiesInfoContributor.Mode.FULL)
+						: infoContributor)
+				.collect(Collectors.toList());
 		return new CloudFoundryInfoEndpointWebExtension(new InfoEndpoint(contributors));
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests minor improvements or refinements rather than a bug fix.

The code diff modifies how `InfoContributor` instances are processed within a stream. Specifically, it replaces a multi-line lambda expression with a more concise ternary operator. The logic remains the same: if an `InfoContributor` is a `GitInfoContributor`, it's replaced with a new `GitInfoContributor` instance configured with `InfoPropertiesInfoContributor.Mode.FULL`; otherwise, the original `InfoContributor` is retained.

The change improves code readability and conciseness without altering the underlying functionality or addressing any specific bug. There are no error handling updates, logical corrections, or exception-handling improvements.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
spring-boot,13196.json,8a6e254465a7a11001ba0b3a6234b1a2cf9ee501,"@@ -1,10 +1,10 @@
 	private void writeDefaultValue(AsciidocBuilder builder) {
 		String defaultValue = (this.defaultValue != null) ? this.defaultValue : """";
-		defaultValue = defaultValue.replace(""\\"", ""\\\\"").replace(""|"", ""{vbar}"" + System.lineSeparator());
+		defaultValue = defaultValue.replace(""\\"", ""\\\\"");
 		if (defaultValue.isEmpty()) {
-			builder.appendln(""|"");
+			builder.appendln(""ￂﾦ"");
 		}
 		else {
-			builder.appendln(""|`+"", defaultValue, ""+`"");
+			builder.appendln(""ￂﾦ`+"", defaultValue, ""+`"");
 		}
 	}",NotBuggy,"Avoid need to escape pipe character in reference tables
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Avoid need to escape pipe character in reference tables"" suggests that the previous implementation required users to manually escape the pipe character (`|`) within reference tables, and this commit aims to remove that requirement. This implies that the previous behavior was undesirable and potentially a source of errors or inconvenience, which can be considered a bug or at least a usability issue.

**Git Diff Analysis:**

The diff focuses on the `writeDefaultValue` method within an `AsciidocBuilder` class. The key changes are:

1.  **Removal of Pipe Character Replacement:** The line `defaultValue = defaultValue.replace(""\\"", ""\\\\"").replace(""|"", ""{vbar}"" + System.lineSeparator());` has been modified to `defaultValue = defaultValue.replace(""\\"", ""\\\\"");`. This confirms that the code no longer attempts to replace the pipe character with ""{vbar}"" and a newline.
2.  **Replacement of `|` with `ￂﾦ`:** The lines `builder.appendln(""|"");` and `builder.appendln(""|`+"", defaultValue, ""+`"");` have been changed to `builder.appendln(""ￂﾦ"");` and `builder.appendln(""ￂﾦ`+"", defaultValue, ""+`"");` respectively. This indicates that instead of using the pipe character, a different character `ￂﾦ` is used.

**Reasoning:**

The commit message and the code changes are strongly related. The code modifications directly address the issue described in the commit message. The removal of the pipe character replacement and the substitution with a different character suggest that the previous method of handling pipe characters was problematic, likely requiring manual escaping by the user. The change aims to simplify the process by avoiding the need for escaping. The replacement of the pipe character with `ￂﾦ` is likely a workaround to avoid conflicts with the table syntax in AsciiDoc.

**Conclusion:**

The changes directly address a usability issue (the need to escape pipe characters), which can be considered a bug or at least an undesirable behavior. The code modifications align perfectly with the commit message.

**Buggy**
"
spring-boot,11072.json,9ebe15232e20e61edc91089dd0a88d3bcf5f7524,"@@ -1,4 +1,4 @@
-	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(InputStream in)
-			throws IOException {
-		return withJsonResource(in, defaultCharset);
+	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(
+			InputStream inputStream) throws IOException {
+		return withJsonResource(inputStream, this.defaultCharset);
 	}",NotBuggy,"Polish
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** ""Polish"" - This is a very generic commit message. It suggests general improvements or refinements but doesn't explicitly mention any bug being fixed.

2.  **Code Diff:**
    *   The method `withJsonResource` is being modified.
    *   The parameter name `in` is changed to `inputStream`.
    *   `defaultCharset` is changed to `this.defaultCharset`.

3.  **Reasoning:**
    *   The change from `in` to `inputStream` is purely a cosmetic change, improving readability.
    *   The change from `defaultCharset` to `this.defaultCharset` ensures that the instance variable is being used instead of a potentially shadowed local variable. This is a good practice and could potentially prevent subtle bugs related to incorrect charset usage if there was a local variable with the same name. However, without more context, it's hard to say if this was actually fixing a bug.

**Conclusion:**

While the change to `this.defaultCharset` could be argued as a preventative measure against a potential bug, the commit message ""Polish"" doesn't suggest a bug fix, and the changes are minor and could be considered general improvements.

**NotBuggy**"
spring-boot,2926.json,f7032bd8d3c8bb6a721bd5860bba9a43e611561e,"@@ -1,16 +1,16 @@
 		public void process(AnnotationMetadata annotationMetadata,
 				DeferredImportSelector deferredImportSelector) {
 			Assert.state(
 					deferredImportSelector instanceof AutoConfigurationImportSelector,
 					() -> String.format(
-							""AutoConfigurationImportSelector only supports %s implementations, got %s"",
+							""Only %s implementations are supported, got %s"",
 							AutoConfigurationImportSelector.class.getSimpleName(),
 							deferredImportSelector.getClass().getName()));
 			AutoConfigurationEntry autoConfigurationEntry = ((AutoConfigurationImportSelector) deferredImportSelector)
 					.getAutoConfigurationEntry(getAutoConfigurationMetadata(),
 							annotationMetadata);
 			this.autoConfigurationEntries.add(autoConfigurationEntry);
 			for (String importClassName : autoConfigurationEntry.getConfigurations()) {
 				this.entries.putIfAbsent(importClassName, annotationMetadata);
 			}
 		}",NotBuggy,"Polish contribution

Closes gh-13963
",NotBuggy,"The commit message ""Polish contribution"" suggests general improvements or refinements. The code diff focuses on improving the error message within the `process` method of a class related to auto-configuration. Specifically, the change refines the error message to be more concise and readable by removing redundant phrasing. This doesn't directly indicate a bug fix, but rather an improvement in clarity.

**NotBuggy**"
spring-boot,5928.json,9bdad06c10ad40c7ef02971cb923d52fd84c8f53,"@@ -1,3 +1,3 @@
 		public String chooseServerAlias(String keyType, Principal[] issuers, Socket socket) {
-			return this.keyManager.chooseServerAlias(keyType, issuers, socket);
+			return this.delegate.chooseServerAlias(keyType, issuers, socket);
 		}",NotBuggy,"Polish Netty SSL key alias support

See gh-19197
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Polish Netty SSL key alias support"" suggests an improvement or refinement of existing functionality related to SSL key alias handling in Netty. The phrase ""polish"" often implies making something smoother, more reliable, or more user-friendly, rather than directly fixing a broken piece of code. The reference to ""gh-19197"" might provide more context, but we'll focus on the diff and commit message as requested.

2.  **Code Diff:** The code diff shows a change within the `chooseServerAlias` method. The original code directly used `this.keyManager.chooseServerAlias`, while the modified code uses `this.delegate.chooseServerAlias`. This suggests that the `keyManager` field was replaced or refactored to a `delegate` field, or that the logic was moved to a delegate object.

3.  **Bug Fix Indicators:**
    *   **Error Handling:** The diff doesn't show any explicit error handling being added or modified (e.g., try-catch blocks, null checks related to error conditions).
    *   **Logical Corrections:** While the change *could* be correcting a logical error (e.g., the `keyManager` was incorrectly being used), the commit message doesn't strongly suggest this. ""Polish"" implies more of a refinement than a correction.
    *   **Exception Handling:** No changes to exception handling are apparent in the diff.
    *   **Other Bug-Related Changes:** The change itself is quite small and doesn't immediately scream ""bug fix."" It looks more like a refactoring or a change in how the key alias selection is delegated.

4.  **Alignment:** The commit message ""Polish Netty SSL key alias support"" aligns reasonably well with the code change. The change likely improves the way the server alias is chosen, possibly by delegating to a more appropriate component.

**Conclusion:**

Based on the analysis, the changes seem to be more about refactoring or improving the existing SSL key alias support rather than directly fixing a bug. Therefore, I will conclude with:

**NotBuggy**
"
spring-boot,4528.json,90949669be923b3b9b7692b663b49e9dee6702ff,"@@ -1,7 +1,4 @@
-	Map<String, Object> parseMap(String json,
+	protected final Map<String, Object> parseMap(String json,
 			Function<String, Map<String, Object>> parser) {
-		assert parser != null;
-
-		return trimIfStartsWith(json, START_MAP).map(parser::apply)
-				.orElseThrow(AbstractJsonParser::cannotParseJson);
+		return trimParse(json, ""{"", parser);
 	}",NotBuggy,"Polish ""refactor spring-boot JSON parser""

Polish JSON parser refactoring and remove some more duplication.

Closes gh-12428
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

*   The commit message ""Polish 'refactor spring-boot JSON parser'"" suggests improvements to a previous refactoring effort. The term ""polish"" implies cleaning up, refining, or optimizing existing code rather than fixing a specific bug.
*   The phrase ""remove some more duplication"" further supports the idea of refactoring and code cleanup.
*   The ""Closes gh-12428"" part indicates that this commit might be related to an issue. However, without knowing the details of gh-12428, it's difficult to say definitively if it's a bug fix or a feature/improvement request.

**2. Code Diff Analysis:**

*   The code diff shows a change in the `parseMap` method. The original version had an assertion (`assert parser != null`) and used `trimIfStartsWith` followed by `map` and `orElseThrow`.
*   The modified version replaces this with a call to `trimParse(json, ""{"", parser)`. This suggests a simplification and potential reuse of existing functionality.
*   The removal of the assertion could be considered a minor risk, but it's likely that the `trimParse` method handles null `parser` arguments appropriately or that the `parser` is guaranteed to be non-null in the calling context.
*   The change appears to be a refactoring to reduce duplication and improve code clarity.

**3. Relevance Assessment:**

*   The commit message and code diff align well. The diff shows a refactoring that removes duplication, as stated in the commit message.
*   The change doesn't obviously indicate a bug fix. There's no explicit error handling being added or any logical corrections being made. The change seems to be focused on code cleanup and simplification.

**4. Conclusion:**

Based on the analysis of the commit message and code diff, the changes appear to be related to refactoring and code cleanup rather than fixing a specific bug. Therefore, the conclusion is:

**NotBuggy**
"
spring-boot,8354.json,76ad1975844550609ff58caa0ce6255be15a9cb7,"@@ -1,11 +1,12 @@
 		private void initializeServlet(Context context, String name) {
 			Container child = context.findChild(name);
 			if (child instanceof StandardWrapper) {
 				try {
-					((StandardWrapper) child).allocate();
+					StandardWrapper wrapper = (StandardWrapper) child;
+					wrapper.deallocate(wrapper.allocate());
 				}
 				catch (ServletException ex) {
 					// Continue
 				}
 			}
 		}",NotBuggy,"Deallocate servlet after forcing initialization in mappings endpoint

Previously, when using Tomcat, a call to mappings endpoint would force
the initialization of any DispatcherServlets in the context. This was
done by calling allocate on Tomcat's StandardWrapper. This left the
wrapper in a state that would cause it to block for two seconds during
shutdown as the wrapper has an outstanding allocation.

This commit immediately deallocates the servlet after it has been
allocated. This ensures that the DispatcherServlet has been initialized
while also leaving the wrapper in a state that it can shut down
immediately when asked to do so.

Closes gh-14898
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message clearly states a problem: calling the `mappings` endpoint in Tomcat would force initialization of `DispatcherServlets`, but leave the `StandardWrapper` in a state that caused a 2-second delay during shutdown. The message explains that the `allocate()` method was the culprit. The solution is to immediately `deallocate` the servlet after allocating it. This ensures initialization without the shutdown delay. The commit message also references a specific issue (`gh-14898`), suggesting this is a bug fix.

**Diff Analysis:**

The diff shows a change within the `initializeServlet` method. Specifically, the original code:

```java
((StandardWrapper) child).allocate();
```

is replaced with:

```java
StandardWrapper wrapper = (StandardWrapper) child;
wrapper.deallocate(wrapper.allocate());
```

This change aligns perfectly with the commit message's description. The code now allocates the servlet using `wrapper.allocate()` and immediately deallocates it using `wrapper.deallocate()`. This confirms the fix described in the commit message.

**Reasoning:**

The commit message describes a specific bug related to servlet initialization and shutdown delays in Tomcat. The code diff directly addresses this issue by adding a deallocation step immediately after allocation. The commit message and the code changes are highly relevant to each other and clearly indicate a bug fix.

**Conclusion:**

**Buggy**
"
spring-boot,1398.json,685babc8295d48710b0c93861ca6b1c3e1a90d8d,"@@ -1,19 +1,20 @@
 	public ConditionEvaluationReport getDelta(ConditionEvaluationReport previousReport) {
 		ConditionEvaluationReport delta = new ConditionEvaluationReport();
-		this.outcomes.forEach((key, value) -> {
-			ConditionAndOutcomes previous = previousReport.outcomes.get(key);
+		this.outcomes.forEach((source, sourceOutcomes) -> {
+			ConditionAndOutcomes previous = previousReport.outcomes.get(source);
 			if (previous == null
-					|| previous.isFullMatch() != value.isFullMatch()) {
-				value.forEach((conditionAndOutcome) -> delta.recordConditionEvaluation(
-								key, conditionAndOutcome.getCondition(),
+					|| previous.isFullMatch() != sourceOutcomes.isFullMatch()) {
+				sourceOutcomes.forEach(
+						(conditionAndOutcome) -> delta.recordConditionEvaluation(source,
+								conditionAndOutcome.getCondition(),
 								conditionAndOutcome.getOutcome()));
 			}
 		});
 		List<String> newExclusions = new ArrayList<>(this.exclusions);
 		newExclusions.removeAll(previousReport.getExclusions());
 		delta.recordExclusions(newExclusions);
 		List<String> newUnconditionalClasses = new ArrayList<>(this.unconditionalClasses);
 		newUnconditionalClasses.removeAll(previousReport.unconditionalClasses);
 		delta.unconditionalClasses.addAll(newUnconditionalClasses);
 		return delta;
 	}",NotBuggy,"Polish ""Use lambdas for map entry iteration where possible""

Closes gh-12626
",NotBuggy,"The commit message ""Polish ""Use lambdas for map entry iteration where possible"""" suggests a code cleanup and refactoring to use lambdas for iterating over map entries. The diff shows that the code has been modified to use lambdas in the `forEach` method calls on the `outcomes` map. The key variable name `key` has been changed to `source` and `value` to `sourceOutcomes` for better readability. The logic of the code remains the same. There are no error handling updates, logical corrections, or exception-handling improvements. Therefore, the changes do not indicate a bug fix.
**NotBuggy**"
spring-boot,1012.json,99e72664d9eda43c170d339ca7b5c5986f6b8a33,"@@ -1,11 +1,5 @@
 	protected PlatformTransactionManager createTransactionManager() {
-		PlatformTransactionManager txManager;
-		if (this.entityManagerFactory != null) {
-			txManager = new JpaTransactionManager(this.entityManagerFactory);
-		}
-		else {
-			txManager = new DataSourceTransactionManager(this.dataSource);
-		}
-		this.transactionProperties.applyTo(txManager);
-		return txManager;
+		AbstractPlatformTransactionManager transactionManager = createAppropriateTransactionManager();
+		this.properties.getTransaction().applyTo(transactionManager);
+		return transactionManager;
 	}",NotBuggy,"Polish spring transaction manager properties

Polish and update contribution so that TransactionManager properties can
be defined per technology, rather than globally.

Closes gh-7561
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Polish spring transaction manager properties"" suggests a refinement or enhancement of how transaction manager properties are handled. The phrase ""TransactionManager properties can be defined per technology, rather than globally"" indicates a change in the scope or granularity of property configuration. The ""Closes gh-7561"" implies that this commit addresses a specific issue reported in GitHub issue #7561. Without access to the issue, we can only infer that the issue likely relates to the inflexible or global nature of transaction manager property configuration.

**2. Code Diff Analysis:**

The code diff shows a change in the `createTransactionManager()` method.

*   **Original Code:** The original code creates a `JpaTransactionManager` if an `entityManagerFactory` is present, otherwise it creates a `DataSourceTransactionManager`. Then, it applies transaction properties using `this.transactionProperties.applyTo(txManager)`. This implies that `transactionProperties` is a single, global set of properties applied to either type of transaction manager.

*   **Modified Code:** The modified code replaces the conditional transaction manager creation with a call to `createAppropriateTransactionManager()`. It then applies transaction properties using `this.properties.getTransaction().applyTo(transactionManager)`. This suggests that the `properties` object now has a nested `getTransaction()` method, which likely returns a technology-specific set of transaction properties. The `createApproriateTransactionManager()` method likely encapsulates the logic for determining the correct transaction manager type based on the available resources (e.g., `entityManagerFactory`, `dataSource`).

**3. Reasoning:**

The changes suggest a shift from a global, one-size-fits-all approach to a more granular, technology-specific approach for configuring transaction manager properties. This refactoring likely addresses a limitation or inflexibility in the original design where properties were applied globally, potentially leading to incorrect or suboptimal configurations for certain technologies. The fact that this commit closes a GitHub issue further strengthens the argument that the changes address a specific problem or limitation. While it's not a direct bug fix in the sense of correcting a calculation error, it's a fix to an architectural issue that could lead to misconfiguration and unexpected behavior.

**Conclusion:**

**Buggy**
"
spring-boot,4009.json,3f00ba3cad8c708eaa06ef599be104e077736132,"@@ -1,9 +1,10 @@
-	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel, Map<String, Link> links) {
+	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel,
+			Map<String, Link> links) {
 		if (accessLevel == null) {
 			return new LinkedHashMap<>();
 		}
 		return links.entrySet().stream()
-				.filter((e) -> e.getKey().equals(""self"")
-						|| accessLevel.isAccessAllowed(e.getKey()))
+				.filter((entry) -> entry.getKey().equals(""self"")
+						|| accessLevel.isAccessAllowed(entry.getKey()))
 				.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests improvements to the code without necessarily fixing a bug. The code changes involve reformatting the method signature for better readability and renaming the `e` variable in the stream to `entry` for clarity. These changes improve code style and readability but do not indicate a bug fix.

**NotBuggy**"
spring-boot,3679.json,598e9bb842d02aba092e290045bd53d280bddbe0,"@@ -1,4 +1,4 @@
-	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig config,
+	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig wavefrontConfig,
 			Clock clock) {
-		return new WavefrontMeterRegistry(config, clock);
+		return new WavefrontMeterRegistry(wavefrontConfig, clock);
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" suggests minor improvements or refinements rather than a bug fix.

The code diff involves a simple renaming of a parameter from `config` to `wavefrontConfig` in the `wavefrontMeterRegistry` method. This change doesn't introduce any error handling, logical corrections, or exception-handling improvements. It's purely a cosmetic change to improve code readability or consistency.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**
"
guava,14599.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,11 +1,11 @@
     public ImmutableTable<R, C, V> build() {
       int size = cells.size();
       switch (size) {
         case 0:
           return of();
         case 1:
-          return new SingletonImmutableTable<R, C, V>(Iterables.getOnlyElement(cells));
+          return new SingletonImmutableTable<>(Iterables.getOnlyElement(cells));
         default:
           return RegularImmutableTable.forCells(cells, rowComparator, columnComparator);
       }
     }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The commit message ""Use diamond operator in collect"" suggests a change to use the diamond operator (`<>`) in the code, which is a shorthand for type inference in Java. The diff shows a change from `new SingletonImmutableTable<R, C, V>(...)` to `new SingletonImmutableTable<>(...)`. This change aligns with the commit message and uses the diamond operator. There is no indication of a bug fix in this change.

**NotBuggy**"
guava,22386.json,8894dba44923a6e06842b6696c6eb2bc08c3c7e0,"@@ -1,13 +1,13 @@
     ImmutableList<E> createAsList() {
       return new ImmutableAsList<E>() {
         @Override
         public E get(int index) {
-          return entries[index].getElement();
+          return Indexed.this.get(index);
         }
 
         @Override
-        ImmutableCollection<E> delegateCollection() {
-          return ElementSet.this;
+        Indexed<E> delegateCollection() {
+          return Indexed.this;
         }
       };
     }",NotBuggy,"Pull out a subclass of ImmutableSet for indexed implementations, which are common enough that it saves code overall.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=90549526
",NotBuggy,"The commit message indicates a refactoring to extract a subclass `Indexed` from `ImmutableSet` for indexed implementations, aiming to reduce code duplication. The diff shows changes within the `createAsList` method of an anonymous inner class. Specifically, `entries[index].getElement()` is replaced with `Indexed.this.get(index)`, and `ElementSet.this` is replaced with `Indexed.this`.

The change `entries[index].getElement()` to `Indexed.this.get(index)` suggests that the anonymous class now relies on the `Indexed` superclass to retrieve elements by index. Similarly, `ElementSet.this` to `Indexed.this` indicates that the anonymous class now delegates to the `Indexed` superclass for collection-related operations.

These changes align with the commit message's intention to extract a common indexed implementation. There's no indication of a bug fix in this refactoring. The changes are focused on code organization and reuse.

**NotBuggy**"
guava,16888.json,afe6ff88023693d9c825b8d619dab56276fa8885,"@@ -1,3 +1,3 @@
-    Entry<E> get(int index) {
-      return getEntry(index);
+    E get(int index) {
+      return entries.get(index).getElement();
     }",NotBuggy,"Add hash flooding protection to ImmutableMultiset, falling back to a JDK HashMap backed implementation when probable flooding is detected.

While we're at it, rearchitect ImmutableMultiset on GWT to use that implementation rather than the handrolled hash table in RegularImmutableMultiset.

RELNOTES=Add hash flooding protection to ImmutableMultiset, and significantly improve its GWT impl while we're at it.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=185173745
",Buggy,"The commit message indicates the addition of hash flooding protection to `ImmutableMultiset` and a re-architecture for GWT. The provided diff snippet shows a change in the `get` method, where it now returns the element directly instead of an `Entry<E>`. This change seems related to the re-architecture mentioned in the commit message, potentially to align with the new JDK HashMap backed implementation or the improved GWT implementation. The commit message doesn't explicitly mention a bug fix, but the addition of hash flooding protection suggests a potential vulnerability or performance issue related to hash collisions. The change in the `get` method signature could be a consequence of adapting to the new underlying implementation.

**Buggy**
"
guava,9796.json,4a6ce819542d5fe440bd5d13956bb4e46084325c,"@@ -1,3 +1,3 @@
     public Set<N> predecessors(N node) {
-      return network.successors(node); // transpose
+      return delegate().successors(node); // transpose
     }",NotBuggy,"common.graph: create (package-protected) ForwardingValueGraph and ForwardingNetwork classes, and use those (and ForwardingGraph) to simplify the implementation of the Transposed* inner classes in Graphs.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=161856111
",NotBuggy,"The commit message indicates a refactoring effort to simplify the implementation of `Transposed*` inner classes in `Graphs` by introducing `ForwardingValueGraph`, `ForwardingNetwork`, and leveraging `ForwardingGraph`. The provided diff shows a change in the `predecessors` method where `network.successors(node)` is replaced with `delegate().successors(node)`. This change aligns with the commit message's intention to use forwarding classes. The change itself doesn't scream bug fix, but rather a change to use the delegate instead of directly accessing the network. It's part of the refactoring process described in the commit message.

**NotBuggy**"
guava,12300.json,78b54f8250067837828e9b7235d7db6f8d728a24,"@@ -1,6 +1,11 @@
   public void forEach(BiConsumer<? super K, ? super V> action) {
     checkNotNull(action);
-    for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
-      action.accept((K) keys[i], (V) values[i]);
+    @Nullable Map<K, V> delegate = delegateOrNull();
+    if (delegate != null) {
+      delegate.forEach(action);
+    } else {
+      for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
+        action.accept((K) keys[i], (V) values[i]);
+      }
     }
   }",NotBuggy,"Make Compact(Linked)HashMap fall back to a java.util.LinkedHashMap if hash flooding is detected, just as the immutable collections do.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=286604248
",Buggy,"The commit message indicates a change to handle hash flooding in `Compact(Linked)HashMap` by falling back to `java.util.LinkedHashMap`, similar to how immutable collections are handled. The provided code diff modifies the `forEach` method. It introduces a delegation mechanism where, if a delegate map exists (`delegateOrNull() != null`), the `forEach` operation is performed on the delegate. Otherwise, the original iteration logic is used. This delegation suggests a fallback mechanism, potentially triggered by hash flooding. The change aligns with the commit message's intention to handle hash flooding by falling back to a standard `LinkedHashMap`. This is a bug fix because hash flooding is an abnormal condition that can cause performance degradation and needs to be handled gracefully.

**Buggy**"
guava,17771.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,3 +1,3 @@
   ImmutableCollection<V> createValues() {
-    return new Values<K, V>(this);
+    return new Values<>(this);
   }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The commit message ""Use diamond operator in collect"" suggests a change to use the diamond operator (`<>`) in the `collect` method. The provided diff shows a change from `new Values<K, V>(this)` to `new Values<>(this)`. This change replaces the explicit type arguments `<K, V>` with the diamond operator `<>`, which allows the compiler to infer the type arguments. This is a stylistic change to improve code readability and reduce redundancy. It doesn't directly indicate a bug fix.

**NotBuggy**"
guava,3767.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-  protected boolean standardEquals(@Nullable Object object) {
+  protected boolean standardEquals(@NullableDecl Object object) {
     return Multisets.equalsImpl(this, object);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from JSR-305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The diff shows a single change where the `@Nullable` annotation is replaced with `@NullableDecl` in the `standardEquals` method. This change directly reflects the commit message's intent. There's no indication of a bug fix; it's simply a change in the annotation used for nullability.

**NotBuggy**"
guava,15700.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,4 +1,4 @@
-  public V remove(@Nullable Object rowKey, @Nullable Object columnKey) {
+  public V remove(@NullableDecl Object rowKey, @NullableDecl Object columnKey) {
     Map<C, V> row = Maps.safeGet(rowMap(), rowKey);
     return (row == null) ? null : Maps.safeRemove(row, columnKey);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from JSR-305's `@Nullable` annotation to Checker Framework's `@NullableDecl` annotation. The provided diff shows a direct replacement of `@Nullable` with `@NullableDecl` in the `remove` method's parameter list. This change doesn't inherently indicate a bug fix. It's a change in annotation usage, likely for better nullness analysis or compatibility with a specific framework. There are no error handling updates, logical corrections, or exception handling improvements.

**NotBuggy**"
guava,22187.json,109421efed0515d0921da5895cd67e8c102bed0e,"@@ -1,4 +1,5 @@
   public static PairwiseEquivalence<?> instantiate(SerializationStreamReader reader)
       throws SerializationException {
+    checkGwtRpcEnabled();
     return create((Equivalence<?>) reader.readObject());
   }",NotBuggy,"Prepare for removing GWT-RPC support for Guava types by adding a system property that disables it.

When the time comes, we'll flip the default value, and users who didn't see our earlier warnings can use a system property to restore support until they can stop serializing our types.

[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265490471
",Buggy,"The commit message indicates a preparation for removing GWT-RPC support for Guava types. This is achieved by adding a system property to disable it, allowing users to temporarily restore support if needed. The code diff shows the addition of `checkGwtRpcEnabled()` at the beginning of the `instantiate` method. This function likely checks the system property and throws an exception or disables GWT-RPC support if the property is set. This change aligns with the commit message's intention to prepare for the removal of GWT-RPC support and provide a way to disable it. The addition of `checkGwtRpcEnabled()` suggests a proactive measure to control the usage of GWT-RPC, which could be related to preventing potential issues or bugs arising from its eventual removal.

**Buggy**
"
guava,271.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,7 +1,7 @@
-    public boolean equals(@Nullable Object obj) {
+    public boolean equals(@NullableDecl Object obj) {
       if (obj instanceof SupplierFunction) {
         SupplierFunction<?> that = (SupplierFunction<?>) obj;
         return this.supplier.equals(that.supplier);
       }
       return false;
     }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from `jsr305 @Nullable` to `Checker Framework @NullableDecl`. The diff shows a change from `@Nullable Object obj` to `@NullableDecl Object obj` in the `equals` method. This change aligns with the commit message and doesn't indicate a bug fix. It's a simple replacement of one annotation with another.

**NotBuggy**"
guava,20365.json,de8eeba94bd6a12a83a05786cf2ddd00d8547b48,"@@ -1,9 +1,9 @@
     HashCode makeHash(Hasher[] hashers) {
       // TODO(user): Get rid of the ByteBuffer here?
       byte[] bytes = new byte[bits / 8];
       ByteBuffer buffer = ByteBuffer.wrap(bytes);
       for (Hasher hasher : hashers) {
         buffer.put(hasher.hash().asBytes());
       }
-      return HashCodes.fromBytes(bytes);
+      return HashCodes.fromBytesNoCopy(bytes);
     }",NotBuggy,"Expose c.g.common.hash.HashCodes and its static factory methods.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=27671928
",Buggy,"The commit message indicates an intention to expose `c.g.common.hash.HashCodes` and its static factory methods. The provided diff modifies the `makeHash` method to use `HashCodes.fromBytesNoCopy` instead of `HashCodes.fromBytes`. This change suggests an optimization or a fix related to how byte arrays are handled when creating HashCodes. The use of `fromBytesNoCopy` implies that the byte array is no longer copied, which could be related to performance or memory usage. Although the commit message doesn't explicitly mention a bug fix, the change from `fromBytes` to `fromBytesNoCopy` suggests an underlying issue or inefficiency that was addressed.

**Buggy**"
guava,17383.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-      public V2 apply(@Nullable V1 v1) {
+      public V2 apply(@NullableDecl V1 v1) {
         return transformer.transformEntry(key, v1);
       }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from `jsr305 @Nullable` to `Checker Framework @NullableDecl`. The code diff shows a replacement of `@Nullable` with `@NullableDecl` in the `apply` method's parameter annotation. This change directly reflects the commit message's intention. There's no indication of a bug fix, error handling, or logical correction. The change is purely a migration of annotations.

**NotBuggy**"
guava,20839.json,8cdd8c718cceb67bd94c83fe60544ff46f9f4460,"@@ -1,3 +1,3 @@
   public static NetworkBuilder<Object, Object> undirected() {
-    return new NetworkBuilder<Object, Object>(false);
+    return new NetworkBuilder<>(false);
   }",NotBuggy,"Use diamond operator for rest.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165581940
",NotBuggy,"The commit message ""Use diamond operator for rest"" suggests a change to use the diamond operator `<>` in Java code to simplify generic type instantiation. The provided diff shows exactly that: the explicit type arguments `<Object, Object>` in `new NetworkBuilder<Object, Object>(false)` are replaced with the diamond operator `<>` in `new NetworkBuilder<>(false)`. This change aligns perfectly with the commit message and is a standard practice for improving code readability. There's no indication of a bug fix in this change.

**NotBuggy**"
guava,1897.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,24 +1,24 @@
-  public List<V> replaceValues(@Nullable K key, Iterable<? extends V> values) {
+  public List<V> replaceValues(@NullableDecl K key, Iterable<? extends V> values) {
     List<V> oldValues = getCopy(key);
     ListIterator<V> keyValues = new ValueForKeyIterator(key);
     Iterator<? extends V> newValues = values.iterator();
 
     // Replace existing values, if any.
     while (keyValues.hasNext() && newValues.hasNext()) {
       keyValues.next();
       keyValues.set(newValues.next());
     }
 
     // Remove remaining old values, if any.
     while (keyValues.hasNext()) {
       keyValues.next();
       keyValues.remove();
     }
 
     // Add remaining new values, if any.
     while (newValues.hasNext()) {
       keyValues.add(newValues.next());
     }
 
     return oldValues;
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from the jsr305 `@Nullable` annotation to the Checker Framework's `@NullableDecl` annotation. The provided diff shows a single change where `@Nullable` is replaced with `@NullableDecl` in the `replaceValues` method signature. This change directly reflects the commit message's intent. There is no indication of a bug fix in this change, it's purely an annotation update.

**NotBuggy**"
pmd,1864.json,e1b12da1747de470397d4e02e01caaf4e2c81eed,"@@ -1,4 +1,12 @@
-    protected static String[] packageNamesIn(Map<String, String> params) {
-        // TODO
-        return null;
+    protected static String[] packageNamesIn(Map<PropertyDescriptorField, String> params) {
+        String[] packageNames = StringUtil.substringsOf(params.get(LEGAL_PACKAGES),
+                                                        PACKAGE_NAME_DELIMITER);
+
+        for (String name : packageNames) {
+            if (!packageNamePattern.matcher(name).matches()) {
+                throw new IllegalArgumentException(""One name is not a package: '"" + name + ""'"");
+            }
+        }
+
+        return packageNames;
     }",NotBuggy,"Some documentation
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message:** ""Some documentation""

This commit message is extremely vague and unhelpful. It provides no context about the purpose or nature of the changes.  It doesn't suggest a bug fix or any specific improvement.

**Git Diff Analysis:**

The code diff shows a significant change to the `packageNamesIn` method. Here's a breakdown:

1.  **Parameter Type Change:** The method signature changed from `Map<String, String>` to `Map<PropertyDescriptorField, String>`. This suggests a change in how parameters are passed to the method, potentially related to a refactoring or a change in the data model.

2.  **Implementation Added:** The original implementation was a `// TODO` comment and `return null;`. This indicates that the method was previously unimplemented or a placeholder. The diff introduces a concrete implementation.

3.  **Package Name Extraction:** The new implementation extracts package names from the `params` map using `StringUtil.substringsOf` and a `PACKAGE_NAME_DELIMITER`.

4.  **Package Name Validation:** The code iterates through the extracted package names and validates each name against a `packageNamePattern` using `matcher().matches()`. If a name doesn't match the pattern, an `IllegalArgumentException` is thrown.

**Reasoning:**

The diff clearly shows the addition of a new implementation for a previously unimplemented method. The implementation includes validation logic that throws an exception if a package name is invalid. This strongly suggests that the original code was incomplete and potentially buggy (returning `null` when it should have returned a valid array of package names). The addition of validation and exception handling further supports the idea that the change is related to fixing a potential issue with invalid package names.

While the commit message is poor, the code changes themselves indicate a bug fix or at least a significant improvement in functionality and error handling. The original code was essentially a stub, and the new code provides a functional and validated implementation.

**Conclusion:**

**Buggy**
"
pmd,4829.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,13 +1,13 @@
     private boolean insideLoop(AbstractNode node) {
-        Node n = node.jjtGetParent();
+        Node n = node.getParent();
 
         while (n != null) {
             if (n instanceof ASTDoLoopStatement || n instanceof ASTWhileLoopStatement
                     || n instanceof ASTForLoopStatement || n instanceof ASTForEachStatement) {
                 return true;
             }
-            n = n.jjtGetParent();
+            n = n.getParent();
         }
 
         return false;
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message states ""Replace usages of node methods everywhere"". The diff shows that `node.jjtGetParent()` is replaced with `node.getParent()`. This change aligns perfectly with the commit message, indicating a systematic replacement of a specific method call across the codebase. The change itself doesn't inherently suggest a bug fix, but rather a refactoring or standardization of method calls.

**NotBuggy**"
pmd,3801.json,320339240fc1d34844b1f6eb123265898a0fed4f,"@@ -1,3 +1,3 @@
     public Object visit(ASTColonSubsript node, Object data) {
-        return visit((AbstractModelicaNode) node, data);
+        return visit((ModelicaNode) node, data);
     }",NotBuggy,"[modelica] Small cleanups

* Move AbstractModelicaRule into the `rule` package
* Make the visitor interface mention only published types (ie not
AbstractModelicaNode)
* Other cleanups(eg make interfaces extend ModelicaNode)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message describes several cleanup operations:

1.  **Package Relocation:** Moving `AbstractModelicaRule` to the `rule` package. This is a refactoring task, improving code organization.
2.  **Visitor Interface Update:** Modifying the visitor interface to only mention published types (excluding `AbstractModelicaNode`). This suggests a change in how the visitor pattern is implemented, potentially improving type safety or reducing unnecessary dependencies.
3.  **Other Cleanups:** General cleanups, such as making interfaces extend `ModelicaNode`. This indicates further refactoring and code improvements.

**Git Diff Analysis:**

The provided Git diff shows a change within the `visit(ASTColonSubsript node, Object data)` method. Specifically, it changes the cast from `(AbstractModelicaNode) node` to `(ModelicaNode) node`.

**Reasoning:**

The commit message mentions that the visitor interface should only mention published types, and not `AbstractModelicaNode`. The diff directly reflects this change by altering the type cast within the `visit` method. This suggests that `AbstractModelicaNode` was previously being used in a way that was deemed inappropriate or unnecessary, and the change aims to use the more general `ModelicaNode` instead.

While this change aligns with the commit message's description of ""cleanups,"" it's possible that the original code using `AbstractModelicaNode` was causing issues or was considered an anti-pattern. The change to `ModelicaNode` might be addressing a potential type safety problem or simplifying the code structure. It's difficult to say definitively whether this is a bug fix without more context, but the change does suggest a refinement of the code's design and potentially the removal of a source of potential errors.

Given the context of ""cleanups"" and the specific change in type casting, it's plausible that the original code was not strictly correct or optimal, and the modification is intended to address this.

**Conclusion:**

**Buggy**
"
pmd,8296.json,4175460d3559266c2046a632e40e6c878ade355f,"@@ -1,38 +1,35 @@
     private boolean isMemberVisibleFromClass(Class<?> classWithMember, int modifiers, Class<?> accessingClass) {
         if (accessingClass == null) {
             return false;
         }
 
         // public members
         if (Modifier.isPublic(modifiers)) {
             return true;
         }
 
-        Package accessingPackage = accessingClass.getPackage();
         boolean areInTheSamePackage;
-        if (accessingPackage != null) {
-            areInTheSamePackage = accessingPackage.getName().startsWith(
+        if (accessingClass.getPackage() != null) {
+            areInTheSamePackage = accessingClass.getPackage().getName().startsWith(
                     classWithMember.getPackage().getName());
         } else {
-            return false;
+            return false; // if the package information is null, we can't do nothin'
         }
 
         // protected members
-        if (Modifier.isProtected(modifiers)
-                && (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass))) {
-            return true;
-        }
-
-        // package private
-        if (!(Modifier.isPrivate(modifiers) || Modifier.isPublic(modifiers) || Modifier.isProtected(modifiers))
-                && areInTheSamePackage) {
-            return true;
-        }
-
-        // private members
-        if (Modifier.isPrivate(modifiers) && classWithMember.equals(accessingClass)) {
+        if (Modifier.isProtected(modifiers)) {
+            if (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass)) {
+                return true;
+            }
+            // private members
+        } else if (Modifier.isPrivate(modifiers)) {
+            if (classWithMember.equals(accessingClass)) {
+                return true;
+            }
+            // package private members
+        } else if (areInTheSamePackage) {
             return true;
         }
 
         return false;
     }",NotBuggy,"Java, typeres: clean up code, add comments
",Buggy,"The commit message indicates code cleanup and the addition of comments. The diff shows changes in the logic of the `isMemberVisibleFromClass` method, specifically in how it determines visibility based on modifiers (public, protected, private) and package relationships. The original code had a slightly different structure for checking protected and package-private access. The modified code introduces a more explicit `else if` structure to handle the different visibility modifiers. Additionally, a null check is added with a return statement. These changes suggest a refinement of the logic, potentially addressing a bug or edge case in the original implementation.

**Buggy**"
pmd,2554.json,9ade75a621f58c22e6aa2eeb71419c7901d1076d,"@@ -1,13 +1,11 @@
      private Language getLanguageByExtension(String extension) {
          Language result = null;
-         File dir = new File(""."");
-         String filename = ""file."" + extension;
 
          for (Language language : languages.values()) {
-             if (language.getFileFilter().accept(dir, filename)) {
+             if (language.getExtensions().contains(extension)) {
                  result = language;
                  break;
              }
          }
          return result;
      }",NotBuggy,"Refactor CPDGui to detect available languages dynamically
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Refactor CPDGui to detect available languages dynamically"" suggests a refactoring effort to improve how the CPDGui (likely a component of a Copy/Paste Detector tool) identifies available programming languages. The term ""dynamically"" implies a change from a static or hardcoded approach to a more flexible and adaptable method. This doesn't inherently indicate a bug fix, but rather an improvement in the system's design or functionality.

**Git Diff Analysis:**

The code diff shows a change in the `getLanguageByExtension` method.

*   **Original Code:** The original code creates a `File` object and a filename using the provided extension. It then iterates through available languages and checks if the language's `FileFilter` accepts the constructed file.
*   **New Code:** The modified code directly checks if the language's extensions list contains the given extension.

**Reasoning:**

The change in the `getLanguageByExtension` method suggests a shift from using `FileFilter` to directly checking the language's extensions. This could be for several reasons:

1.  **Performance:** Directly checking the extensions list is likely faster than creating `File` objects and using `FileFilter`.
2.  **Accuracy:** The `FileFilter` approach might have been unreliable or inaccurate in certain cases, potentially leading to incorrect language detection. For example, the current directory `"".""` might not be relevant to the language detection logic.
3.  **Simplicity:** The new approach is simpler and easier to understand.

While the commit message indicates a refactoring, the code change itself suggests a potential fix for inaccurate or inefficient language detection. The original method relied on creating a dummy file and using a `FileFilter`, which is an indirect and potentially problematic way to determine the language. The new method directly checks if the language supports the given extension, which is more direct and reliable.

**Conclusion:**

The change from using `FileFilter` to directly checking the language's extensions list suggests a fix for a potential bug related to inaccurate language detection. Therefore, the commit likely includes a bug fix.

**Buggy**"
pmd,6893.json,40ce9313aef9adcbefb4769d9639751286c14a92,"@@ -1,8 +1,3 @@
     public String getOperator() {
-        if (getImage() != null) {
-            return getImage(); // <<
-        }
-
-        ASTRSIGNEDSHIFT rsshift = (ASTRSIGNEDSHIFT) jjtGetChild(1);
-        return rsshift != null ? "">>"" : "">>>"";
+        return getImage();
     }",NotBuggy,"Deprecate R[UN]SIGNEDSHIFT

No usages to report
",Buggy,"The commit message ""Deprecate R[UN]SIGNEDSHIFT"" suggests the removal or deprecation of right shift operators. The provided diff shows a change in the `getOperator()` method.

Here's a step-by-step analysis:

1. **Commit Message Relevance:** The commit message indicates a change related to right shift operators (R[UN]SIGNEDSHIFT).
2. **Code Changes:** The original code had a conditional statement that checked if `getImage()` was null. If not null, it returned the image. Otherwise, it casted a child node to `ASTRSIGNEDSHIFT` and returned "">>"" or "">>>"" based on whether the child node was null. The modified code simply returns `getImage()`. This change removes the logic related to handling right shift operators (""<<"" or "">>>"").
3. **Bug Fix Indication:** The original code seems to be handling different ways of representing the operator. The removal of the conditional logic and the direct return of `getImage()` suggests that the previous logic might have been incorrect or unnecessary. The commit message also mentions ""No usages to report"", which could mean that the `ASTRSIGNEDSHIFT` path was not being used or was causing issues.

Based on the analysis, the changes align with the commit message and indicate a bug fix. The original logic related to handling right shift operators was removed, suggesting that it was either incorrect or no longer needed.

**Buggy**"
pmd,1967.json,6b50104cb57d40e5057c169fd817b7468f187c48,"@@ -1,3 +1,3 @@
     public static <E> EnumPBuilder<E> builder(String name, Class<E> type) {
-        return new EnumPBuilder<>(type).name(name);
+        return new EnumPBuilder<>(name, type);
     }",NotBuggy,"Corrections for PR #669
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Corrections for PR #669"" suggests that the changes are addressing issues or improvements identified during a review of pull request #669. This could potentially include bug fixes, but it's not explicitly stated.

2.  **Code Diff:** The code diff shows a change in the `builder` method of an `EnumPBuilder` class. Specifically, the order of arguments passed to the `EnumPBuilder` constructor has been changed.

    *   **Original Code:** `return new EnumPBuilder<>(type).name(name);`
    *   **Modified Code:** `return new EnumPBuilder<>(name, type);`

    The original code was passing the `type` to the constructor and then calling the `name` method. The modified code passes both `name` and `type` to the constructor. This suggests that the original constructor might have had the parameters in the wrong order, or that the `name` method was being called unnecessarily.

3.  **Bug Fix Indication:** The change in constructor argument order strongly suggests a bug fix. It's likely that the original code was either:

    *   Passing the wrong values to the constructor, leading to incorrect object initialization.
    *   Relying on a separate `name` method call to set the name, which might have been error-prone or less efficient.

**Conclusion:**

The code change, combined with the commit message indicating corrections, strongly suggests that the original code had a bug related to the order of arguments in the `EnumPBuilder` constructor. Therefore, the changes indicate a bug fix.

**Buggy**
"
pmd,8796.json,88ff3f0628f4ff6c1228771c4d9297422d7bb28a,"@@ -1,4 +1,4 @@
     public Object visit(ASTClassOrInterfaceDeclaration decl, Object data) {
-        checkCommentMeetsRequirement(data, decl, HEADER_CMT_REQUIREMENT_DESCRIPTOR);
+        checkCommentMeetsRequirement(data, decl, CLASS_CMT_REQUIREMENT_DESCRIPTOR);
         return super.visit(decl, data);
     }",NotBuggy,"[java] CommentRequired: Separate header comment from class comment and header top-level comment

Refs #1683
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message states: ""[java] CommentRequired: Separate header comment from class comment and header top-level comment"". It suggests that the change aims to differentiate between header comments, class-level comments, and top-level header comments within the `CommentRequired` check. The ""Refs #1683"" indicates that this commit is related to issue #1683, which might provide more context.

**Git Diff Analysis:**

The diff shows a change within the `visit(ASTClassOrInterfaceDeclaration decl, Object data)` method.
- The original code calls `checkCommentMeetsRequirement` with `HEADER_CMT_REQUIREMENT_DESCRIPTOR`.
- The modified code calls `checkCommentMeetsRequirement` with `CLASS_CMT_REQUIREMENT_DESCRIPTOR`.

This change directly aligns with the commit message's intention to separate header and class comment checks. It appears the original code was incorrectly using the `HEADER_CMT_REQUIREMENT_DESCRIPTOR` for class declarations, which is now corrected to use `CLASS_CMT_REQUIREMENT_DESCRIPTOR`.

**Reasoning:**

The code change directly reflects the intent described in the commit message. The original code likely had a bug where it was checking the class-level comments against the header comment requirements. The modification fixes this by using the correct descriptor for class comments. This indicates a correction of a logical error in how comments were being validated.

**Conclusion:**

**Buggy**
"
pmd,1304.json,32b7cba79a9a8350fb3b13cbedce804ec3d24157,"@@ -1,4 +1,4 @@
         public void syntaxError(Recognizer<?, ?> recognizer, Object offendingSymbol, int line, int charPositionInLine,
-                String msg, RecognitionException ex) {
+                                String msg, RecognitionException ex) {
             throw new ANTLRSyntaxError(msg, line, charPositionInLine, ex);
         }",NotBuggy,"abstraction for cpd with antlr
",NotBuggy,"The commit message ""abstraction for cpd with antlr"" suggests a general change related to the integration of CPD (Copy/Paste Detector) with ANTLR (ANother Tool for Language Recognition). The code diff shows a minor formatting change in the `syntaxError` method, specifically adjusting the indentation of the `String msg` parameter. This change does not appear to be related to fixing a bug, but rather to improving code readability or consistency. There's no error handling, logical correction, or exception handling improvement evident in the diff. Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
pmd,1359.json,af5130ab0037fc9b0c6a13305d0037bf7afd88d3,"@@ -1,3 +1,3 @@
     public boolean isDeprecated() {
-        return deprecated != null && deprecated.booleanValue();
+        return deprecated != null && deprecated;
     }",NotBuggy,"Improvements and planned updates around PropertySource
",NotBuggy,"The commit message indicates general improvements and planned updates around `PropertySource`. The code change simplifies the return statement in the `isDeprecated()` method by directly returning the boolean value of the `deprecated` field, assuming it's a `Boolean` object. This change avoids unnecessary unboxing and comparison. There's no explicit indication of a bug fix in either the commit message or the code change itself. The change appears to be a minor code improvement for readability and efficiency.

**NotBuggy**"
pmd,2224.json,785d663553072e5aeca8d8448347087e9d6c8aa3,"@@ -1,3 +1,3 @@
-    protected Object createFrom(String value) {
-        return Integer.valueOf(value);
+    protected Integer createFrom(String toParse) {
+        return Integer.valueOf(toParse);
     }",NotBuggy,"Finished refactoring properties
",NotBuggy,"The commit message ""Finished refactoring properties"" suggests a general code improvement or restructuring rather than a specific bug fix.

The code diff shows a change in the method signature of `createFrom` from returning `Object` to returning `Integer`. Also, the parameter name is changed from `value` to `toParse`. This change aligns with the commit message, indicating a refactoring effort to improve type safety and code clarity. There's no explicit error handling, logical correction, or exception handling improvement visible in the diff. The change seems to be about making the code more specific in its return type.

**NotBuggy**
"
pmd,7422.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,3 +1,3 @@
     public ASTStatement getBody() {
-        return (ASTStatement) jjtGetChild(jjtGetNumChildren() - 1);
+        return (ASTStatement) getChild(getNumChildren() - 1);
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message ""Replace usages of node methods everywhere"" suggests a systematic replacement of specific node methods across the codebase. The provided diff shows a change from `jjtGetChild` to `getChild`. This aligns with the commit message's intent to replace node methods. There is no indication of a bug fix in the traditional sense (error handling, logical correction, exception handling, etc.). The change appears to be a refactoring or standardization effort.

**NotBuggy**"
pmd,4286.json,89fcd45f75f79f5f223e4f572ef332f75869dcef,"@@ -1,8 +1,8 @@
-    public boolean visit(AstNode node) {
-	if (parents.peek() == node) {
-	    return true;
-	} else {
-	    buildInternal(node);
-	    return false;
-	}
-    }

+	public boolean visit(AstNode node) {
+		if (parents.peek() == node) {
+			return true;
+		} else {
+			build(node);
+			return false;
+		}
+	}",NotBuggy,Next step towards compile ;-),NotBuggy,"The commit message ""Next step towards compile ;-)"" suggests ongoing development or refactoring related to the compilation process. The code diff shows a change from `buildInternal(node)` to `build(node)`.

Reasoning:

1.  **Commit Message Relevance:** The commit message indicates a step in the compilation process, suggesting the changes are related to how the code is compiled or processed.
2.  **Code Change Analysis:** The change from `buildInternal(node)` to `build(node)` implies a refactoring or renaming of a method. It's unlikely to be a bug fix in the traditional sense, but rather a change in the internal structure or naming conventions.
3.  **Bug Fix Indicators:** There are no explicit signs of error handling updates, logical corrections, or exception handling improvements. The change appears to be a simple method name update.

Conclusion: **NotBuggy**"
pmd,1620.json,0eaac2fbdb4fe4207b3f329fec2fe0b0e19bbc32,"@@ -1,8 +1,8 @@
         public Status step(int type) {
             for (int i = 0; i < this.nextSteps.size(); i++) {
-                if (type == ((Status) nextSteps.get(i)).type) {
-                    return (Status) nextSteps.get(i);
+                if (type == nextSteps.get(i).type) {
+                    return nextSteps.get(i);
                 }
             }
             return null;
         }",NotBuggy,"Checking in some Java 5 changes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4993 51baf565-9d33-0410-a72c-fc3788e3496d
",NotBuggy,"The commit message ""Checking in some Java 5 changes"" suggests a general update to leverage Java 5 features. The diff shows a change in how elements are retrieved from the `nextSteps` list. Specifically, it removes explicit casting `(Status)` when retrieving elements from the `nextSteps` list. This indicates a transition to using generics, a feature introduced in Java 5, which allows specifying the type of elements within a collection, thus eliminating the need for manual casting. This change aligns with the commit message and doesn't inherently indicate a bug fix.

**NotBuggy**"
pmd,3945.json,eb7738c3416f467477c81b805c5f28313527b610,"@@ -1,3 +1,3 @@
     public Object visit(ASTExpression node, Object data) {
-        return visit((AbstractApexNodeBase) node, data);
+        return visit((ApexNode<?>) node, data);
     }",NotBuggy,"Make FormalComment extend ApexNode
",NotBuggy,"The commit message ""Make FormalComment extend ApexNode"" indicates a change in the inheritance structure of the `FormalComment` class, making it a subclass of `ApexNode`.

The provided diff shows a change in the `visit` method where `AbstractApexNodeBase` is replaced with `ApexNode<?>`. This change suggests a generalization of the type being visited, potentially to accommodate the new inheritance structure where `FormalComment` (which is now an `ApexNode`) can be passed to the `visit` method.

The change doesn't directly indicate a bug fix. It seems more like a refactoring or an adaptation to a new class hierarchy. There's no explicit error handling, logical correction, or exception handling improvement visible in this diff. The change is related to type compatibility after the inheritance change.

**NotBuggy**
"
hbase,11147.json,d822ee3a7ccc4959ed5a4b85bb54ff6142aa7d6e,"@@ -1,5 +1,5 @@
   protected void resetActive() {
     // Reset heap to not include any keys
-    this.active = SegmentFactory.instance().createMutableSegment(conf, comparator);
-    this.timeOfOldestEdit = Long.MAX_VALUE;
+    active = SegmentFactory.instance().createMutableSegment(conf, comparator);
+    timeOfOldestEdit = Long.MAX_VALUE;
   }",NotBuggy,"HBASE-20542: Better heap utilization for IMC with MSLABs
",NotBuggy,"The commit message ""HBASE-20542: Better heap utilization for IMC with MSLABs"" suggests changes aimed at improving memory usage, specifically within the context of the in-memory component (IMC) and Memory Store Local Allocation Buffers (MSLABs) in HBase.

The provided code diff is very small. It shows a change in the `resetActive` method where `this.active` and `this.timeOfOldestEdit` are directly assigned instead of using `this.` prefix.

Reasoning:

1.  **Commit Message Relevance:** The commit message focuses on heap utilization. The code change itself doesn't directly reveal how heap utilization is improved. The change is about removing `this.` prefix.
2.  **Bug Fix Indicators:** The code change doesn't seem to be related to error handling, logical corrections, or exception handling. It's a minor code style change. It's unlikely to be a bug fix. The commit message mentions better heap utilization, but the code diff doesn't show any direct changes related to memory management.

Conclusion: **NotBuggy**"
hbase,10378.json,d1147eeb7e1d5f41161c7cf5bc5ddb4744ca5b57,"@@ -1,8 +1,7 @@
   public double getFlushPressure() {
     if (getRegionServerAccounting() == null || cacheFlusher == null) {
       // return 0 during RS initialization
       return 0.0;
     }
-    return getRegionServerAccounting().getGlobalMemstoreSize() * 1.0
-        / cacheFlusher.globalMemStoreLimitLowMark;
+    return getRegionServerAccounting().getFlushPressure();
   }",NotBuggy,"HBASE-15787 Change the flush related heuristics to work with offheap size
configured (Ram)
",Buggy,"The commit message indicates a change in flush-related heuristics to work with offheap size. The code diff shows that the `getFlushPressure` method is modified to use `getRegionServerAccounting().getFlushPressure()` instead of calculating the flush pressure directly using `getRegionServerAccounting().getGlobalMemstoreSize()` and `cacheFlusher.globalMemStoreLimitLowMark`. This change suggests that the previous calculation might not have been correct when offheap size is configured, and the new method `getRegionServerAccounting().getFlushPressure()` likely takes offheap size into account. This indicates a bug fix.

**Buggy**"
hbase,5011.json,4c6a98bd767f3bec4d2e6107016267a09d58dd12,"@@ -1,6 +1,6 @@
-  public void toStringClassDetails(StringBuilder sb) {
+  public void toStringClassDetails(final StringBuilder sb) {
     sb.append(getClass().getSimpleName());
     sb.append("" (table="");
     sb.append(getTableName());
     sb.append("")"");
   }",NotBuggy,"HBASE-16618 Procedure v2 - Add base class for table and ns procedures
",NotBuggy,"The commit message ""HBASE-16618 Procedure v2 - Add base class for table and ns procedures"" indicates the addition of a base class for table and namespace procedures. The provided code diff shows a minor change in the `toStringClassDetails` method, specifically adding the `final` keyword to the `StringBuilder sb` parameter. This change doesn't seem directly related to a bug fix but rather a minor code style improvement or a defensive programming practice to prevent accidental modification of the `StringBuilder` object within the method. The change doesn't introduce any error handling, logical corrections, or exception handling improvements. Therefore, it's unlikely to be a bug fix.

**NotBuggy**
"
hbase,12378.json,30424ec73f7f100b233e27196c0a6a90abd62ad2,"@@ -1,3 +1,3 @@
-  public long getModificationTime() {
-    return this.fileStatus.getModificationTime();
+  public long getModificationTime() throws IOException {
+    return getFileStatus().getModificationTime();
   }",NotBuggy,"HBASE-12749 Tighten HFileLink api to enable non-snapshot uses
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-12749 Tighten HFileLink api to enable non-snapshot uses"" suggests that the changes are aimed at improving the HFileLink API, possibly to make it more robust or flexible for use cases beyond snapshots. The term ""tighten"" could imply making the API more restrictive or precise in some way. It doesn't explicitly mention a bug fix, but it does imply a potential improvement or refinement of existing functionality.

**Git Diff Analysis:**

The diff shows a change in the `getModificationTime()` method of the `HFileLink` class. Specifically, it adds `throws IOException` to the method signature and changes `this.fileStatus.getModificationTime()` to `getFileStatus().getModificationTime()`.

*   **Adding `throws IOException`:** This is a significant change. It indicates that the method can now potentially throw an `IOException`, which suggests that the underlying operation of getting the modification time from the file status might fail due to I/O issues. This addition of exception handling is a strong indicator of a potential bug fix or at least a defensive programming measure to handle potential errors.
*   **Changing `this.fileStatus` to `getFileStatus()`:** This change suggests that `fileStatus` is no longer directly accessed as a field but is now accessed through a getter method. This could be part of the ""tightening"" of the API mentioned in the commit message, potentially encapsulating the `fileStatus` field and controlling access to it. It could also be related to lazy initialization or ensuring that the `fileStatus` is properly initialized before being accessed, which could address a potential bug.

**Reasoning:**

The addition of `throws IOException` strongly suggests that the original code might have been missing error handling for potential I/O exceptions when retrieving the file modification time. This omission could have led to unexpected behavior or crashes if an I/O error occurred. The change to use `getFileStatus()` could be related to ensuring proper initialization or handling of the `fileStatus` object, which could also address a potential bug.

**Conclusion:**

Based on the addition of exception handling and the change in how `fileStatus` is accessed, the changes indicate a bug fix or at least a significant improvement in error handling. Therefore, the conclusion is:

**Buggy**
"
hbase,35546.json,f3f17fa111f37233ddc42ddb9c38594e35d8d501,"@@ -1,8 +1,8 @@
-  private void releaseLock(final Procedure proc, final boolean force) {
-    final TEnvironment env = getEnvironment();
+  private void releaseLock(Procedure<TEnvironment> proc, boolean force) {
+    TEnvironment env = getEnvironment();
     // For how the framework works, we know that we will always have the lock
     // when we call releaseLock(), so we can avoid calling proc.hasLock()
-    if (force || !proc.holdLock(env)) {
-      proc.doReleaseLock(env);
+    if (force || !proc.holdLock(env) || proc.isFinished()) {
+      proc.doReleaseLock(env, store);
     }
   }",NotBuggy,"HBASE-20846 Restore procedure locks when master restarts
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-20846 Restore procedure locks when master restarts"" suggests a fix related to procedure locks in HBase. Specifically, it implies that upon master restart, the procedure locks were not being correctly restored, leading to potential issues.

**Git Diff Analysis:**

The diff shows a modification to the `releaseLock` method.  The key change is the addition of `|| proc.isFinished()` to the `if` condition:

```diff
-    if (force || !proc.holdLock(env)) {
-      proc.doReleaseLock(env);
+    if (force || !proc.holdLock(env) || proc.isFinished()) {
+      proc.doReleaseLock(env, store);
    }
```

**Reasoning:**

1.  **Bug Indication:** The original code only released the lock if `force` was true or if the procedure didn't hold the lock (`!proc.holdLock(env)`). The added condition `proc.isFinished()` suggests that the lock was *not* being released even when the procedure had already finished. This is a bug because a finished procedure should not hold a lock.

2.  **Relevance to Commit Message:** The commit message mentions restoring procedure locks on master restart. If a procedure finishes *before* a master restart, but its lock is not released, then after the restart, the lock might be incorrectly considered as still held, preventing other procedures from acquiring it. The added condition `proc.isFinished()` directly addresses this scenario by ensuring that finished procedures release their locks.

3.  **Error Handling/Logic Correction:** The change represents a logical correction. The original logic was incomplete, as it didn't account for the scenario where a procedure finishes but the lock isn't released. The fix ensures that locks are released when they should be, preventing potential deadlocks or other concurrency issues.

4. **Additional Argument:** The change `proc.doReleaseLock(env)` to `proc.doReleaseLock(env, store)` suggests that the `store` is now needed to release the lock. This could be related to persisting the lock state or updating some metadata, which is relevant to restoring locks after a restart.

**Conclusion:**

The code change directly addresses a scenario where procedure locks are not released even after the procedure has finished. This aligns with the commit message about restoring procedure locks on master restart, as unreleased locks from finished procedures could cause problems after a restart. The change represents a logical correction and likely fixes a bug related to procedure lock management.

**Buggy**
"
hbase,6414.json,6c22333599b9910314f57d0b6a580fb69eb7aa2b,"@@ -1,14 +1,14 @@
   public void whoAmI(RpcController controller, AuthenticationProtos.WhoAmIRequest request,
                      RpcCallback<AuthenticationProtos.WhoAmIResponse> done) {
-    User requestUser = RequestContext.getRequestUser();
+    User requestUser = RpcServer.getRequestUser();
     AuthenticationProtos.WhoAmIResponse.Builder response =
         AuthenticationProtos.WhoAmIResponse.newBuilder();
     if (requestUser != null) {
       response.setUsername(requestUser.getShortName());
       AuthenticationMethod method = requestUser.getUGI().getAuthenticationMethod();
       if (method != null) {
         response.setAuthMethod(method.name());
       }
     }
     done.run(response.build());
   }",NotBuggy,"HBASE-13397 Purge duplicate rpc request thread local
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-13397 Purge duplicate rpc request thread local"" suggests that the change aims to remove a duplicate thread-local variable related to RPC requests. This implies a potential bug where the same information was being stored redundantly in thread-local storage, possibly leading to memory leaks or incorrect data access.

**Code Diff Analysis:**

The code diff shows a single line change within the `whoAmI` method:

- `User requestUser = RequestContext.getRequestUser();`
+ `User requestUser = RpcServer.getRequestUser();`

This change replaces `RequestContext.getRequestUser()` with `RpcServer.getRequestUser()`.

**Reasoning:**

1.  **Context:** The `whoAmI` method is part of an RPC service, likely used for authentication or authorization. It retrieves the user associated with the current request.

2.  **ThreadLocal Implication:** The commit message mentions ""rpc request thread local"". Both `RequestContext` and `RpcServer` might be using thread-local storage to manage request-specific data, including the authenticated user.

3.  **Potential Bug:** The original code used `RequestContext.getRequestUser()`. The change replaces it with `RpcServer.getRequestUser()`. This suggests that `RequestContext` might have been incorrectly or redundantly storing the user information. The commit message indicates that this was a duplicate, which implies a bug. The fix is to use the correct source of truth for the request user, which is now `RpcServer`.

4.  **Bug Fix Indication:** The change aims to remove a duplicate thread-local variable related to RPC requests. This directly aligns with the commit message and indicates a bug fix.

**Conclusion:**

The code change, in conjunction with the commit message, strongly suggests that the change is a bug fix. The original code had a duplicate thread-local variable, and the change corrects this by using the correct source of truth for the request user.

**Buggy**
"
hbase,35116.json,17703f03614e0803f46eadb70a2242060d04125c,"@@ -1,23 +1,23 @@
   public String toString() {
     StringBuilder sb = new StringBuilder();
     toStringClassDetails(sb);
 
     if (procId != null) {
       sb.append("" id="");
       sb.append(getProcId());
     }
 
     if (hasParent()) {
       sb.append("" parent="");
       sb.append(getParentProcId());
     }
 
     if (hasOwner()) {
       sb.append("" owner="");
       sb.append(getOwner());
     }
 
     sb.append("" state="");
-    sb.append(getState());
+    toStringState(sb);
     return sb.toString();
   }",NotBuggy,"HBASE-14015 Allow setting a richer state value when toString a pv2
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-14015 Allow setting a richer state value when toString a pv2"" suggests an enhancement to the `toString()` method of a class related to ""pv2"" (likely a Process or Procedure v2). The message indicates that the state information displayed by `toString()` is being improved to be more detailed or informative. This doesn't inherently imply a bug fix, but rather an improvement in debuggability or logging.

**Git Diff Analysis:**

The diff shows a modification to the `toString()` method. Specifically, the line:

```java
sb.append("" state="");
sb.append(getState());
```

is replaced with:

```java
sb.append("" state="");
sb.append(toStringState(sb));
```

This change suggests that the original `getState()` method was returning a simple representation of the state (e.g., an enum value or a simple string). The new `toStringState(sb)` method likely formats the state information in a more detailed or human-readable way, potentially including additional context or attributes related to the state.

**Reasoning:**

The commit message and the code change align. The commit message states that the state value in the `toString` method is being enriched, and the code shows that the original `getState()` call is being replaced with a call to `toStringState(sb)`.

While this change improves the information provided by the `toString()` method, it doesn't necessarily indicate a bug fix. It's more likely an enhancement to make debugging or monitoring easier. The original code wasn't necessarily *wrong*, just less informative. There's no indication of error handling, logical corrections, or exception handling improvements.

**Conclusion:**

NotBuggy
"
hbase,7109.json,a9b671b31f07ade8968b42956aa60c722032dcc8,"@@ -1,48 +1,48 @@
   private byte[] createModifiedVisExpression(final List<Tag> tags)
       throws IOException {
     StringBuilder visibilityString = new StringBuilder();
     for (Tag tag : tags) {
       if (tag.getType() == TagType.VISIBILITY_TAG_TYPE) {
         if (visibilityString.length() != 0) {
           visibilityString.append(VisibilityConstants.CLOSED_PARAN).append(
               VisibilityConstants.OR_OPERATOR);
         }
-        int offset = tag.getTagOffset();
-        int endOffset = offset + tag.getTagLength();
+        int offset = tag.getValueOffset();
+        int endOffset = offset + tag.getValueLength();
         boolean expressionStart = true;
         while (offset < endOffset) {
-          Pair<Integer, Integer> result = StreamUtils.readRawVarint32(tag.getBuffer(), offset);
+          Pair<Integer, Integer> result = TagUtil.readVIntValuePart(tag, offset);
           int currLabelOrdinal = result.getFirst();
           if (currLabelOrdinal < 0) {
             int temp = -currLabelOrdinal;
             String label = this.labelsCache.getLabel(temp);
             if (expressionStart) {
               // Quote every label in case of unicode characters if present
               visibilityString.append(VisibilityConstants.OPEN_PARAN)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             }
           } else {
             String label = this.labelsCache.getLabel(currLabelOrdinal);
             if (expressionStart) {
               visibilityString.append(VisibilityConstants.OPEN_PARAN).append(
                   CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR).append(
                   CellVisibility.quote(label));
             }
           }
           expressionStart = false;
           offset += result.getSecond();
         }
       }
     }
     if (visibilityString.length() != 0) {
       visibilityString.append(VisibilityConstants.CLOSED_PARAN);
       // Return the string formed as byte[]
       return Bytes.toBytes(visibilityString.toString());
     }
     return null;
   }",NotBuggy,"HBASE-12593 Tags to work with ByteBuffer.
",Buggy,"The commit message indicates that the changes are related to making tags work with `ByteBuffer`. The code diff shows changes in how the visibility expression is created from tags. Specifically, `tag.getTagOffset()` and `tag.getTagLength()` are replaced with `tag.getValueOffset()` and `tag.getValueLength()`, respectively. Also, `StreamUtils.readRawVarint32(tag.getBuffer(), offset)` is replaced with `TagUtil.readVIntValuePart(tag, offset)`. These changes suggest that the code was modified to correctly handle tags stored in `ByteBuffer`s, likely because the original code was not correctly accessing the tag's value offset and length when the tag was backed by a `ByteBuffer`. This indicates a bug fix.

**Buggy**"
hbase,20259.json,d82c1a6c2b534c63bb347cf1d21c36af400b0f19,"@@ -1,9 +1,9 @@
       public void read(org.apache.thrift.protocol.TProtocol prot, addColumnFamily_result struct) throws org.apache.thrift.TException {
-        TTupleProtocol iprot = (TTupleProtocol) prot;
-        BitSet incoming = iprot.readBitSet(1);
+        org.apache.thrift.protocol.TTupleProtocol iprot = (org.apache.thrift.protocol.TTupleProtocol) prot;
+        java.util.BitSet incoming = iprot.readBitSet(1);
         if (incoming.get(0)) {
           struct.io = new TIOError();
           struct.io.read(iprot);
           struct.setIoIsSet(true);
         }
       }",NotBuggy,"HBASE-21791 Upgrade thrift dependency to 0.12.0
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-21791 Upgrade thrift dependency to 0.12.0"" suggests a dependency upgrade of the Thrift library to version 0.12.0.  This type of change is usually done for performance improvements, bug fixes in the Thrift library itself, or to address security vulnerabilities.  It doesn't directly imply a bug fix in the HBase code itself, but rather a fix or improvement coming from an external library.

**Git Diff Analysis:**

The diff shows a change in the `read` method of the `addColumnFamily_result` class. Specifically, the type of `TTupleProtocol` and `BitSet` are explicitly qualified with their package names:

*   `TTupleProtocol iprot = (TTupleProtocol) prot;` is changed to `org.apache.thrift.protocol.TTupleProtocol iprot = (org.apache.thrift.protocol.TTupleProtocol) prot;`
*   `BitSet incoming = iprot.readBitSet(1);` is changed to `java.util.BitSet incoming = iprot.readBitSet(1);`

This change suggests that the unqualified class names `TTupleProtocol` and `BitSet` might have been ambiguous or conflicting with other classes in the project's classpath *before* the Thrift upgrade. The upgrade to Thrift 0.12.0 may have introduced a class name collision or changed the default imports, necessitating the explicit qualification. This is a common issue when upgrading dependencies.

**Reasoning:**

The explicit qualification of class names is often a workaround for classloading issues or conflicts that arise during dependency upgrades. While the change itself doesn't directly fix a bug in the application logic, it resolves a problem caused by the upgraded dependency. The need to explicitly qualify the class names indicates that the previous implicit resolution was no longer valid after the upgrade. This can be considered a bug fix in the sense that it resolves a problem introduced by the dependency upgrade.

**Conclusion:**

**Buggy**
"
hbase,5138.json,826b9436fbba39ce78650680c71425bc8547cd39,"@@ -1,9 +1,9 @@
   private TableQueue getTableQueue(TableName tableName) {
     TableQueue node = AvlTree.get(tableMap, tableName, TABLE_QUEUE_KEY_COMPARATOR);
     if (node != null) return node;
 
-    NamespaceQueue nsQueue = getNamespaceQueue(tableName.getNamespaceAsString());
-    node = new TableQueue(tableName, nsQueue, getTablePriority(tableName));
+    node = new TableQueue(tableName, tablePriorities.getPriority(tableName),
+        locking.getTableLock(tableName), locking.getNamespaceLock(tableName.getNamespaceAsString()));
     tableMap = AvlTree.insert(tableMap, node);
     return node;
   }",NotBuggy,"HBASE-17605 Changes
- Moved locks out of MasterProcedureScheduler#Queue. One Queue object is used for each namespace/table, which aren't more than 100. So we don't need complexity arising from all functionalities being in one place. SchemaLocking now owns locks and locking implementaion has been moved to procedure2 package.
- Removed NamespaceQueue because it wasn't being used as Queue (add,peek,poll,etc functions threw UnsupportedOperationException). It's was only used for locks on namespaces. Now that locks have been moved out of Queue class, it's not needed anymore.
- Remoed RegionEvent which was there only for locking on regions. Tables/namespaces used locking from Queue class and regions couldn't (there are no separate proc queue at region level), hence the redundance. Now that locking is separate, we can use the same for regions too.
- Removed QueueInterface class. No declarations, except one implementaion, which makes the point of having an interface moot.
- Removed QueueImpl, which was the only concrete implementation of abstract Queue class. Moved functions to Queue class itself to avoid unnecessary level in inheritance hierarchy.
- Removed ProcedureEventQueue class which was just a wrapper around ArrayDeque class. But we now have ProcedureWaitQueue as 'Type class'.
- Encapsulated table priority related stuff in a single class.
- Removed some unused functions.
Change-Id: I6a60424cb41e280bc111703053aa179d9071ba17
",Buggy,"The commit message describes a significant refactoring effort focused on simplifying the `MasterProcedureScheduler` and related classes. The changes involve:

1.  **Moving locks out of `MasterProcedureScheduler.Queue`:** This suggests a design improvement to reduce complexity and centralize locking logic.
2.  **Removing `NamespaceQueue`:** This indicates that the class was redundant and its functionality has been replaced.
3.  **Removing `RegionEvent`:** Similar to `NamespaceQueue`, this class was deemed unnecessary after the locking mechanism was refactored.
4.  **Removing `QueueInterface` and `QueueImpl`:** This simplifies the class hierarchy by removing an unnecessary interface and its only implementation.
5.  **Removing `ProcedureEventQueue`:** This suggests a simplification of the event queue management.
6.  **Encapsulating table priority:** This improves code organization and maintainability.
7.  **Removing unused functions:** This reduces code clutter and potential for errors.

The provided code diff shows the modification of `getTableQueue` method.

*   The original code retrieves a `NamespaceQueue` and then creates a `TableQueue` with a reference to the `NamespaceQueue`.
*   The modified code directly retrieves the table priority and locks (table lock and namespace lock) from dedicated components (`tablePriorities` and `locking`).

This change aligns with the commit message's description of moving locks out of the `Queue` class and removing the `NamespaceQueue`. The code diff reflects the refactoring efforts described in the commit message. There's no immediate indication of a bug fix in the traditional sense (e.g., fixing an incorrect calculation or handling a specific error case). However, the refactoring itself could address potential concurrency issues or deadlocks related to the previous locking implementation, which would be considered a bug fix in a broader sense. The removal of redundant classes and simplification of the locking mechanism can be seen as preventative measures against future bugs.

**Buggy**
"
hbase,28109.json,dec0ec229070465ce5a1f7381366a82278149f06,"@@ -1,7 +1,4 @@
   public HBaseProtos.TableState convert() {
     return HBaseProtos.TableState.newBuilder()
-        .setState(this.state.convert())
-        .setTable(ProtobufUtil.toProtoTableName(this.tableName)) // set for backward compatibility
-        .setTimestamp(this.timestamp)
-            .build();
+        .setState(this.state.convert()).build();
   }",NotBuggy,"HBASE-13016 Clean up remnants of table states stored in table descriptors (Andrey Stepachev)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HBASE-13016 Clean up remnants of table states stored in table descriptors"" suggests the removal of obsolete or unnecessary data related to table states from table descriptors. The phrase ""clean up remnants"" strongly hints at the removal of something that should no longer be there, which is often a consequence of a previous bug or design flaw.

**Git Diff Analysis:**

The diff shows a change in the `convert()` method of a class related to `TableState`. Specifically, the lines:

```java
.setTable(ProtobufUtil.toProtoTableName(this.tableName)) // set for backward compatibility
.setTimestamp(this.timestamp)
```

are being removed.

The comment ""// set for backward compatibility"" suggests that the `setTable` call was added to maintain compatibility with older versions. The removal of this line, along with the `setTimestamp` call, indicates that these fields are no longer needed in the `TableState` representation within the table descriptor. This could be because the information is now stored elsewhere or is no longer relevant.

**Reasoning:**

The commit message and the diff align well. The commit message talks about cleaning up remnants, and the diff shows the removal of fields from the `TableState` object that were likely used for backward compatibility but are no longer needed. The removal of backward compatibility code often happens when a bug or design flaw related to data storage or representation is fixed, and the old way of doing things is no longer necessary. The removal of the timestamp also suggests that it was related to the removed table name.

**Conclusion:**

**Buggy**
"
hbase,3595.json,eddc746457d0d25db1b51bef953db684131d4dd0,"@@ -1,3 +1,3 @@
-      protected void chore() {
-        master.balance();
-      }

+  protected void chore() {
+    master.balance();
+  }",NotBuggy,"HBASE-6730 Enable rolling averages in StochasticLoadBalancer

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1387865 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""HBASE-6730 Enable rolling averages in StochasticLoadBalancer"" suggests changes related to enabling rolling averages within the StochasticLoadBalancer in HBase.

The provided diff is minimal and only shows a change in whitespace. It doesn't seem to be related to enabling rolling averages or any kind of bug fix. The change is simply reformatting the `chore()` method by removing leading whitespace.

Therefore, the changes do not align with the commit message and do not indicate a bug fix.

**NotBuggy**"
hbase,14572.json,33bedf8d4d7ec320c5cc01c1c031035a1523f973,"@@ -1,14 +1,14 @@
   public void put(E e) throws InterruptedException {
-    if (e == null) throw new NullPointerException();
+    Objects.requireNonNull(e);
 
     lock.lock();
     try {
       while (queue.remainingCapacity() == 0) {
         notFull.await();
       }
       this.queue.add(e);
       notEmpty.signal();
     } finally {
       lock.unlock();
     }
   }",NotBuggy,"HBASE-23308: Review of NullPointerExceptions (#836)

Signed-off-by: stack <stack@apache.org>",Buggy,"The commit message ""HBASE-23308: Review of NullPointerExceptions"" indicates a review and potential fix for NullPointerExceptions. The code diff shows a change from `if (e == null) throw new NullPointerException();` to `Objects.requireNonNull(e);`. This change is related to handling null values, which directly addresses the issue of NullPointerExceptions. The use of `Objects.requireNonNull(e)` provides a more concise and potentially optimized way to check for null and throw a NullPointerException if necessary. This change indicates a bug fix related to null pointer handling.

**Buggy**"
hbase,883.json,4b91a6425eadc3578ec291496bbf7fde9742f96a,"@@ -1,31 +1,32 @@
-  public static void copySnapshotForScanner(Configuration conf, FileSystem fs, Path rootDir,
-      Path restoreDir, String snapshotName) throws IOException {
+  public static RestoreMetaChanges copySnapshotForScanner(Configuration conf, FileSystem fs,
+      Path rootDir, Path restoreDir, String snapshotName) throws IOException {
     // ensure that restore dir is not under root dir
     if (!restoreDir.getFileSystem(conf).getUri().equals(rootDir.getFileSystem(conf).getUri())) {
       throw new IllegalArgumentException(""Filesystems for restore directory and HBase root "" +
           ""directory should be the same"");
     }
     if (restoreDir.toUri().getPath().startsWith(rootDir.toUri().getPath())) {
       throw new IllegalArgumentException(""Restore directory cannot be a sub directory of HBase "" +
           ""root directory. RootDir: "" + rootDir + "", restoreDir: "" + restoreDir);
     }
 
     Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
     SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
     SnapshotManifest manifest = SnapshotManifest.open(conf, fs, snapshotDir, snapshotDesc);
 
     MonitoredTask status = TaskMonitor.get().createStatus(
         ""Restoring  snapshot '"" + snapshotName + ""' to directory "" + restoreDir);
     ForeignExceptionDispatcher monitor = new ForeignExceptionDispatcher();
 
     // we send createBackRefs=false so that restored hfiles do not create back reference links
     // in the base hbase root dir.
     RestoreSnapshotHelper helper = new RestoreSnapshotHelper(conf, fs,
       manifest, manifest.getTableDescriptor(), restoreDir, monitor, status, false);
-    helper.restoreHdfsRegions(); // TODO: parallelize.
+    RestoreMetaChanges metaChanges = helper.restoreHdfsRegions(); // TODO: parallelize.
 
     if (LOG.isDebugEnabled()) {
       LOG.debug(""Restored table dir:"" + restoreDir);
       FSUtils.logFileSystemState(fs, restoreDir, LOG);
     }
+    return metaChanges;
   }",NotBuggy,"HBASE-14714 some cleanup to snapshot code
",NotBuggy,"The commit message indicates a cleanup of the snapshot code. The code diff shows a change in the return type of the `copySnapshotForScanner` method from `void` to `RestoreMetaChanges`. This suggests a modification to how the snapshot restoration process is handled, potentially involving the tracking or management of metadata changes during the restore operation. The comment `// TODO: parallelize.` indicates that the code is not yet fully optimized and may be subject to further improvements. There is no explicit evidence of a bug fix in the traditional sense (e.g., fixing a crash, correcting a calculation error). However, the change in return type and the introduction of `RestoreMetaChanges` suggest an enhancement or refactoring of the snapshot restoration process. Therefore, the changes do not indicate a bug fix.
**NotBuggy**"
hbase,40253.json,6af4292630daca370de7fce3a2b2b3401cd1abfa,"@@ -1,4 +1,5 @@
-  public String toString() {
-    return ""id: "" + transactionId + "", particpants: ""
-        + participatingRegions.size();
+  public static String toString(final byte [] b1,
+                                String sep,
+                                final byte [] b2) {
+    return toString(b1, 0, b1.length) + sep + toString(b2, 0, b2.length);
   }",NotBuggy,"HBASE-1304 - New client server implementation of how gets and puts are handled. -- Thanks to jgray,holstad,stack,rawson



git-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@782178 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""HBASE-1304 - New client server implementation of how gets and puts are handled"" suggests a major change related to client-server communication for get and put operations in HBase. The provided code diff, however, shows a seemingly unrelated change: the addition of a `toString` method that concatenates two byte arrays with a separator.

The code diff does not directly relate to client-server communication or get/put operations. It seems to be a utility method for string representation of byte arrays. There's no indication of bug fixing in the provided diff. The change is simply adding a utility function.

**NotBuggy**
"
ant,1414.json,020f9a52c4a54560f617aea642a180e96294e9cc,"@@ -1,3 +1,3 @@
-        public String getFile() {
-            return m_filename;
-        }

+    public String getFile() {
+        return m_filename;
+    }",NotBuggy,"Make tagEntry a top level class.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272453 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Make tagEntry a top level class"" suggests a structural change in the code. The provided diff shows a minor modification related to the `getFile()` method, specifically the removal of indentation. This change doesn't seem directly related to making `tagEntry` a top-level class or fixing a bug. The diff is too small to determine if it's part of a larger change related to the commit message. However, based on the provided information, it doesn't indicate a bug fix.

**NotBuggy**"
ant,9732.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,7 +1,7 @@
-    public Object setProperty(String key, String value)
+	public Object setProperty(final String key, final String value)
         throws NullPointerException {
-        Object obj = super.setProperty(key, value);
+        final Object obj = super.setProperty(key, value);
         // the above call will have failed if key or value are null
         innerSetProperty(key, value);
         return obj;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" is vague and doesn't provide much context about the changes.

The code diff involves a minor refactoring in the `setProperty` method. Specifically, it adds `final` keywords to the `key`, `value`, and `obj` variables. This change doesn't inherently indicate a bug fix. It's more likely a style improvement or a way to enforce immutability. The core logic of the method remains the same. There are no error handling updates, logical corrections, or exception-handling improvements.

Therefore, based on the provided information, the changes do not indicate a bug fix.

**NotBuggy**
"
ant,632.json,761f82ae3158f6a4b131f0f4f3efff152c72bd95,"@@ -1,3 +1,3 @@
     public void setExecutor(Executor e) {
-        addReference(""ant.executor"", e);
+        addReference(MagicNames.ANT_EXECUTOR_REFERENCE, e);
     }",NotBuggy,"moving some magic names into place, using them where needed.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278417 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a refactoring effort to replace hardcoded strings (magic names) with named constants. The diff shows that the string ""ant.executor"" is being replaced with `MagicNames.ANT_EXECUTOR_REFERENCE`. This change improves code readability and maintainability by centralizing the definition of these magic names. There is no indication of a bug fix in the commit message or the code change itself.

**NotBuggy**"
ant,8540.json,a738000acab66d8d7ce5cf563f6dd96d9c7024fb,"@@ -1,3 +1,7 @@
     public Reference getLoader() {
+        if (isReference()) {
+            return ((AbstractClasspathResource) getCheckedRef()).getLoader();
+        }
+        dieOnCircularReference();
         return loader;
     }",NotBuggy,"refid-audit of indirect subclasses of DataType

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@722437 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""refid-audit of indirect subclasses of DataType"" suggests an audit or review process related to how reference IDs (refids) are handled in indirect subclasses of the `DataType` class. This could imply that there were potential issues or inconsistencies in how these subclasses were managing references, possibly leading to bugs.

**Git Diff Analysis:**

The diff shows a modification within the `getLoader()` method. Specifically, it adds a conditional check:

```java
if (isReference()) {
    return ((AbstractClasspathResource) getCheckedRef()).getLoader();
}
```

This check appears to be handling the case where the current object is a reference to another object. If it *is* a reference, the code retrieves the actual object being referenced (`getCheckedRef()`), casts it to `AbstractClasspathResource`, and then calls `getLoader()` on that referenced object.

**Reasoning:**

1.  **Reference Handling:** The added `if (isReference())` block strongly suggests that the original code was not correctly handling cases where the `DataType` instance was actually a reference. Without this check, the code would directly proceed to `dieOnCircularReference()` and `return loader;` even when a reference existed, which is likely incorrect.

2.  **Potential for NullPointerException or Incorrect Loader:** Without the `isReference()` check, `getLoader()` might have returned an incorrect loader or even thrown a `NullPointerException` if `loader` was not properly initialized in the case of a reference.

3.  **Audit Implication:** The commit message mentions ""refid-audit,"" implying a systematic review of how reference IDs are used. This further supports the idea that the original code had a flaw in how it handled references, which was discovered during the audit.

4.  **Bug Fix Indication:** The added conditional check fixes a potential bug where the loader was not correctly retrieved when the DataType instance was a reference. The original code likely assumed that `loader` was always directly available, which was not the case when dealing with references.

**Conclusion:**

Based on the commit message and the code changes, the modification addresses a bug related to the handling of references in `DataType` subclasses. The added `if (isReference())` check corrects the logic for retrieving the loader when the instance is a reference.

**Buggy**
"
ant,7011.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    protected void addExtdirsToClasspath(Path classpath) {
+	protected void addExtdirsToClasspath(final Path classpath) {
         classpath.addExtdirs(extdirs);
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" is too vague to provide any meaningful context. The code change itself is a minor modification, changing the method parameter declaration from `Path classpath` to `final Path classpath`. This change makes the `classpath` parameter immutable within the method, which is a good practice to prevent accidental modifications. However, without more context, it's impossible to determine if this change is related to a bug fix. It could simply be a code style improvement or part of a larger refactoring.

**NotBuggy**
"
ant,9308.json,048caece4061746150290b0264128118a4be3870,"@@ -1,30 +1,4 @@
     protected String getElementName(Project project, Object element)
     {
-        Hashtable elements = project.getTaskDefinitions();
-        String typeName = ""task"";
-        if (!elements.contains( element.getClass() ))
-        {
-            elements = project.getDataTypeDefinitions();
-            typeName = ""data type"";
-            if (!elements.contains( element.getClass() ))
-            {
-                elements = null;
-            }
-        }
-
-        if (elements != null)
-        {
-            Enumeration e = elements.keys();
-            while (e.hasMoreElements())
-            {
-                String elementName = (String) e.nextElement();
-                Class elementClass = (Class) elements.get( elementName );
-                if ( element.getClass().equals( elementClass ) )
-                {
-                    return ""The <"" + elementName + ""> "" + typeName;
-                }
-            }
-        }
-
-        return ""Class "" + element.getClass().getName();
+        return project.getElementName(element);
     }",NotBuggy,"move method refactoring - applied in a way that keeps
IntrospectionHelper's API backwards compatible.

Inspired by:	Jon Skeet <jon.skeet@peramon.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271451 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a refactoring of a method to maintain backward compatibility, inspired by Jon Skeet. The diff shows that the original `getElementName` method's implementation has been replaced with a call to `project.getElementName(element)`. This suggests that the original logic has been moved to the `Project` class, and the `IntrospectionHelper` class now simply delegates the call.

The commit message doesn't explicitly mention a bug fix. The refactoring aims to improve the code's structure or maintainability while preserving the existing API. There are no error handling updates, logical corrections, or exception-handling improvements visible in the diff.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**
"
ant,3462.json,789422e131b6c2c003d94f394169a64297e986c6,"@@ -1,3 +1,3 @@
-    public void setTimeout(long timeout) {
+    public void setTimeout(final long timeout) {
         maxwait = timeout;
     }",NotBuggy,oops - make it compile under Java5 again,Buggy,"The commit message ""oops - make it compile under Java5 again"" indicates a fix related to Java 5 compatibility. The code change involves adding the `final` keyword to the `timeout` parameter of the `setTimeout` method. This change likely addresses a compilation error or warning that arises in Java 5 due to how the `timeout` variable is used within the method's scope (e.g., accessed by an inner class). This is a bug fix.

**Buggy**"
ant,7372.json,ff9b546d81da21c253e71319dc8a756380bac15c,"@@ -1,3 +1,3 @@
     public void setString(String str) {
-        str_text = str;
+        strText = str;
     }",NotBuggy,"checkstyle - mostly removal of _


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278353 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""checkstyle - mostly removal of _"" indicates a style change focused on removing underscores. The diff shows a change from `str_text` to `strText`, which aligns perfectly with the commit message's intention to remove underscores. This change is purely stylistic and doesn't suggest any bug fix.
**NotBuggy**"
ant,8584.json,f67efecf7b71abb13cde1c89fcb56e4e8b7c5c9e,"@@ -1,4 +1,4 @@
-    public void setBase(File b) {
+    public void setBaseDir(File b) {
         checkAttributesAllowed();
-        base = b;
+        baseDir = b;
     }",NotBuggy,"Renamed <file> resource's ""base"" attribute to ""basedir"",
for consistency / familiarity.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278388 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that the attribute ""base"" of a resource was renamed to ""basedir"" for consistency and familiarity. The diff shows that the method `setBase` was renamed to `setBaseDir`, and the field `base` was renamed to `baseDir`. This aligns perfectly with the commit message. There's no indication of a bug fix in this change; it's purely a renaming for better consistency.

**NotBuggy**"
ant,7270.json,9caeed5081071d6a2de28844d0a724918f8c6d86,"@@ -1,18 +1,18 @@
-    protected synchronized void dieOnCircularReference(Stack stk, Project p)
+    protected synchronized void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
             if (mapper != null) {
                 pushAndInvokeCircularReferenceCheck(mapper, stk, p);
             }
-            for (Iterator i = setRefs.iterator(); i.hasNext(); ) {
-                pushAndInvokeCircularReferenceCheck((PropertySet) i.next(), stk,
+            for (PropertySet propertySet : setRefs) {
+                pushAndInvokeCircularReferenceCheck(propertySet, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",NotBuggy,"Java 1.5, continued

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1376015 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Java 1.5, continued"" suggests an update to align with Java 1.5 standards.

The code diff shows the following changes:

1.  `Stack stk` changed to `Stack<Object> stk`: This change updates the `Stack` declaration to use generics, specifically `Stack<Object>`, which is a Java 1.5 feature.
2.  `for (Iterator i = setRefs.iterator(); i.hasNext(); )` changed to `for (PropertySet propertySet : setRefs)`: This change replaces the `Iterator` with an enhanced for loop (for-each loop), which is also a Java 1.5 feature.

These changes are related to adopting Java 1.5 features, such as generics and enhanced for loops. There is no indication of a bug fix in this commit. The changes are purely related to code modernization.

**NotBuggy**"
ant,9623.json,ffea0a9f7459b7c64148ae244b22e847bad9128b,"@@ -1,3 +1,3 @@
     public String getPattern() {
-        return pattern.pattern();
+        return pattern;
     }",NotBuggy,"Rewritten part of the regexp stuff, adding substitution and options.
New optional replaceregexp tasks that takes advantage of it.

Submitted by:	Matthew Inger <matti@sedonacorp.com>

This doesn't really work (but doesn't break anything either), but I
wanted to get the stuff committed now, I will put work into it
tomorrow.  See mail to the list for details.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269820 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a rewrite of regular expression functionality, including the addition of substitution and options. It also introduces a new optional task, `replaceregexp`. The message explicitly states that the changes ""doesn't really work"" but also ""doesn't break anything either"". The provided diff shows a change in the `getPattern()` method, where `pattern.pattern()` is replaced with `pattern`.

Reasoning:

1.  **Commit Message Analysis:** The commit message suggests new functionality and modifications to existing regular expression handling. The explicit mention of the code not fully working implies potential issues or bugs in the introduced changes.
2.  **Diff Analysis:** The diff shows a change in how the pattern is retrieved. The original code used `pattern.pattern()`, suggesting that `pattern` was a `java.util.regex.Pattern` object, and this method returns the string representation of the pattern. The modified code simply returns `pattern`. This change could be related to the new functionality or a correction in how the pattern is handled.
3.  **Bug Fix Indication:** Given the commit message indicating incomplete functionality and the code modification in pattern handling, it's plausible that the change in `getPattern()` is related to addressing a bug or an issue that arose during the rewrite. The developer acknowledges that the changes are not fully functional, suggesting that the modifications could be part of a larger bug fix or feature implementation that is still in progress.

Conclusion:

**Buggy**
"
ant,7050.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    public void setAddproperty (String addproperty) {
+    public void setAddproperty (final String addproperty) {
         this.addproperty = addproperty;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" suggests that the changes are related to code style improvements or adherence to checkstyle rules.

The diff shows a minor change: adding the `final` keyword to the `addproperty` parameter in the `setAddproperty` method. This change doesn't seem to be related to fixing a bug but rather to enforcing a coding style convention (making the parameter immutable within the method). There are no error handling updates, logical corrections, or exception-handling improvements.

**NotBuggy**
"
ant,1185.json,dccbf1fcec0fc4e0812e8906494b15a1301ac32a,"@@ -1,7 +1,3 @@
     public boolean hasMoreTokens() {
-        if (lookahead != null) {
-            return true;
-        }
-
-        return tokenizer.hasMoreTokens();
+        return lookahead != null || tokenizer.hasMoreTokens();
     }",NotBuggy,"Checkstyle and simplify, next iteration",NotBuggy,"The commit message ""Checkstyle and simplify, next iteration"" suggests improvements to code style and simplification.

The code diff simplifies the `hasMoreTokens` method by combining the `if` statement and its return statements into a single return statement using the `||` (OR) operator. This change reduces the number of lines of code and makes the logic more concise.

The change doesn't appear to be directly related to fixing a bug. It's more about improving code readability and efficiency.

**NotBuggy**"
ant,1488.json,70f35e75416468e31f537f33b248caaae29ee587,"@@ -1,19 +1,19 @@
-    private static String removePackageName(String line, List packageNames) {
+    private static String removePackageName(String line,
+                                            String[] packagePrefixes,
+                                            int[] prefixLengths) {
+        if (line.length() < FILE_STRING_LENGTH) {
+            return null;
+        }
         boolean matched = false;
-        for (Iterator iter = packageNames.iterator(); iter.hasNext(); ) {
-            String toBeRemoved = FILE_STRING + iter.next() + ""/"";
-            int len = toBeRemoved.length();
-            if (line.length() > len) {
-                if (line.startsWith(toBeRemoved)) {
-                    matched = true;
-                    line = line.substring(len);
-                    break;
-                }
+        for (int i = 0; i < packagePrefixes.length; i++) {
+            if (line.startsWith(packagePrefixes[i])) {
+                matched = true;
+                line = line.substring(prefixLengths[i]);
+                break;
             }
         }
-        if (!matched && line.length() > FILE_STRING.length()) {
-            line = line.substring(FILE_STRING.length());
-            matched = true;
+        if (!matched) {
+            line = line.substring(FILE_STRING_LENGTH);
         }
-        return !matched ? null : line;
+        return line;
     }",NotBuggy,"optimize handling of package names

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@705452 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates an optimization in handling package names. The diff shows changes to the `removePackageName` method. Let's analyze the code changes:

1.  **Parameter Change:** The method signature changes from `removePackageName(String line, List packageNames)` to `removePackageName(String line, String[] packagePrefixes, int[] prefixLengths)`. This suggests an optimization by pre-processing the package names into arrays of prefixes and their lengths, likely to avoid repeated `length()` calls within the loop.
2.  **Early Exit:** A check `if (line.length() < FILE_STRING_LENGTH)` is added at the beginning. If the line is shorter than `FILE_STRING_LENGTH`, the method returns `null`. This is likely an optimization to avoid unnecessary processing of short lines.
3.  **Loop Optimization:** The code iterates through `packagePrefixes` and `prefixLengths` instead of a `List` of package names. It directly uses `line.startsWith(packagePrefixes[i])` and `line.substring(prefixLengths[i])`, which are more efficient than creating `toBeRemoved` in each iteration.
4.  **Simplification:** The logic for removing `FILE_STRING` is simplified. It is now done unconditionally if no package prefix matches.
5.  **Return Value:** The return value changes. It now always returns the modified line, instead of returning `null` if no match was found.

These changes indicate an optimization of the `removePackageName` method. The use of arrays for package prefixes and lengths, the early exit condition, and the simplified loop all contribute to improved performance. The change in return value suggests a possible bug fix or at least a change in behavior to avoid returning null. The original code returned null if no package name was matched, which might have led to unexpected behavior in the calling code. The new code always returns the modified line, which is more consistent.

**Buggy**
"
ant,393.json,8741512d8c2e4cad30f0c6e6134d9a793f00d4b7,"@@ -1,3 +1,3 @@
     public void setName(String name) {
-        this.name = new StringBuffer(normalizeFileName(name));
+        this.name = new StringBuffer(normalizeFileName(name, false));
     }",NotBuggy,"Make preserveLeadingSlashes in tar work again, was broken by revision 755231

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@807513 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `preserveLeadingSlashes` functionality in the tar task, which was broken by a previous revision. The code change modifies the `setName` method to call `normalizeFileName` with an additional `false` argument. This suggests that the `normalizeFileName` method's behavior was changed in revision 755231, and this commit reverts or adjusts the call to restore the original behavior when `preserveLeadingSlashes` is enabled. The addition of the `false` argument likely disables a normalization step that was incorrectly applied, thus fixing the bug.

**Buggy**"
lucene-solr,41126.json,e167e9124757b3f3597db8149c49b7f388c48627,"@@ -1,23 +1,22 @@
   static MatchesIterator fromTermsEnum(LeafReaderContext context, int doc, String field, BytesRefIterator terms) throws IOException {
     Objects.requireNonNull(field);
     List<MatchesIterator> mis = new ArrayList<>();
     Terms t = context.reader().terms(field);
     if (t == null)
       return null;
     TermsEnum te = t.iterator();
     PostingsEnum reuse = null;
     for (BytesRef term = terms.next(); term != null; term = terms.next()) {
       if (te.seekExact(term)) {
         PostingsEnum pe = te.postings(reuse, PostingsEnum.OFFSETS);
         if (pe.advance(doc) == doc) {
-          // TODO do we want to use the copied term here, or instead create a label that associates all of the TMIs with a single term?
-          mis.add(new TermMatchesIterator(BytesRef.deepCopyOf(term), pe));
+          mis.add(new TermMatchesIterator(pe));
           reuse = null;
         }
         else {
           reuse = pe;
         }
       }
     }
     return fromSubIterators(mis);
   }",NotBuggy,"LUCENE-8270: Remove MatchesIterator.term()
",Buggy,"The commit message ""LUCENE-8270: Remove MatchesIterator.term()"" suggests the removal of a `term()` method from the `MatchesIterator` class or related classes.

The provided diff shows changes within the `fromTermsEnum` method.  Specifically, the `TermMatchesIterator` constructor is being called differently.  Instead of passing a copy of the term (`BytesRef.deepCopyOf(term)`) and a `PostingsEnum`, it now only receives the `PostingsEnum`. This implies that the `TermMatchesIterator` class no longer needs the term itself, meaning the `term()` method likely existed in that class and has been removed or is in the process of being removed.

The change directly reflects the commit message's intention to remove the `term()` method, which likely involved removing the term from the `TermMatchesIterator`'s constructor and internal state. This indicates a refactoring or simplification of the code, potentially to address a bug or improve performance.

**Buggy**
"
lucene-solr,27518.json,72f99b47b4673d858e3ffd42afdca76e0a5bb860,"@@ -1,3 +1,7 @@
     public int startOffset() {
-      return startOffsets[nextPos-1];
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[nextPos-1];
+      }
     }",NotBuggy,"LUCENE-4230: allow consumer to specify that payloads are not require when pulling D&PEnum, so codec can optimize if possible

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1367186 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-4230: allow consumer to specify that payloads are not require when pulling D&PEnum, so codec can optimize if possible"" indicates a performance optimization by allowing consumers to specify whether payloads are required. The code diff modifies the `startOffset()` method to handle the case where `startOffsets` is null. If `startOffsets` is null, it returns -1; otherwise, it returns the start offset as before. This change suggests that the previous implementation might have thrown an exception (e.g., `NullPointerException`) when `startOffsets` was null, indicating a bug fix or at least a defensive programming improvement. The change addresses a potential error condition where `startOffsets` might be null, which could lead to unexpected behavior or crashes.

Reasoning:
1. The commit message describes an optimization related to payloads.
2. The code diff introduces a null check for `startOffsets` in the `startOffset()` method.
3. If `startOffsets` is null, the method now returns -1 instead of potentially throwing a `NullPointerException`.
4. This null check suggests that `startOffsets` could be null in certain scenarios, and the previous implementation did not handle this case, which could be considered a bug.

Conclusion: **Buggy**
"
lucene-solr,22902.json,aa1d5feba01a50d4bc002e346e54ff26f6abc664,"@@ -1,14 +1,14 @@
-    private void rehash() {
-      long[] oldVals = vals;
-      int newCapacity = vals.length << 1;
-      vals = new long[newCapacity];
-      mask = newCapacity - 1;
-      threshold = (int) (newCapacity * LOAD_FACTOR);
-      cardinality = 0;
+  private void rehash() {
+    long[] oldVals = vals;
+    int newCapacity = vals.length << 1;
+    vals = new long[newCapacity];
+    mask = newCapacity - 1;
+    threshold = (int) (newCapacity * LOAD_FACTOR);
+    cardinality = 0;
 
-      for (long val : oldVals) {
-        if (val != 0) {
-          add(val);
-        }
+    for (long val : oldVals) {
+      if (val != 0) {
+        add(val);
       }
-    }

+    }
+  }",NotBuggy,"SOLR-11093: add Points to GraphQuery
",NotBuggy,"The commit message ""SOLR-11093: add Points to GraphQuery"" indicates that the commit is related to adding functionality for handling points within the GraphQuery feature.

The provided code diff shows a change in the `rehash()` method. This method seems to be part of a data structure that dynamically resizes its internal storage. The changes themselves are minor, only involving whitespace and formatting adjustments. There are no logical changes, error handling updates, or exception handling improvements. The rehash method is a common operation in hash tables or similar data structures to maintain performance as the number of elements increases.

The code diff doesn't seem directly related to ""adding Points to GraphQuery"". The rehash operation is a low-level detail that might be used internally by the GraphQuery implementation, but the diff itself doesn't reveal any specific logic for handling points or any bug fixes. It's possible that this change is part of a larger effort to improve the performance or scalability of GraphQuery, but based on the provided information, it's difficult to establish a direct connection to the commit message or identify a bug fix.

Therefore, based on the available information, it's unlikely that this commit is a bug fix.

**NotBuggy**"
lucene-solr,49335.json,831c0648f992faea7024e36167749cd210cddc14,"@@ -1,6 +1,6 @@
   public long get(int index) {
     final int o = blockOffset(index);
     final int b = offsetInBlock(index);
 
-    return (blocks[o] >> shifts[b]) & readMask;
+    return (blocks[o] >>> shifts[b]) & readMask;
   }",NotBuggy,"LUCENE-4062: PackedInts improvements.

 * Direct64 now uses System.arraycopy for its bulk operations,
 * Packed64SingleBlock unnecessary optimizations have been removed.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1351682 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-4062: PackedInts improvements. Direct64 now uses System.arraycopy for its bulk operations, Packed64SingleBlock unnecessary optimizations have been removed."" indicates improvements to the PackedInts class, specifically focusing on `Direct64` and `Packed64SingleBlock`.

The provided code diff shows a change in the `get` method of an unspecified class. The change involves replacing the right shift operator `>>` with the unsigned right shift operator `>>>`. This change is likely related to handling negative values correctly when reading from the `blocks` array. The use of `>>>` ensures that the sign bit is not extended during the shift operation, which is crucial for unsigned integer operations.

The commit message mentions ""improvements"" and ""unnecessary optimizations removed"". The change from `>>` to `>>>` can be considered a bug fix if the original code was incorrectly handling negative values, leading to incorrect results. This is a logical correction to ensure the correct behavior of the `get` method when dealing with potentially negative values in the `blocks` array.

Therefore, the code change indicates a bug fix related to handling signedness in bitwise operations.

**Buggy**"
lucene-solr,2051.json,f7aa200d406dbd05a35d6116198302d90b92cb29,"@@ -1,3 +1,3 @@
-      public boolean exists(int doc) {
+      public boolean exists(int doc) throws IOException {
         return vals.exists(doc);
       }",NotBuggy,"LUCENE-7407: switch doc values usage to an iterator API, based on DocIdSetIterator, instead of random acces, freeing codecs for future improvements
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""LUCENE-7407: switch doc values usage to an iterator API, based on DocIdSetIterator, instead of random acces, freeing codecs for future improvements"" describes a refactoring effort. The core idea is to move from random access to an iterator-based approach for accessing doc values. This is primarily done for performance and to allow for future codec improvements. It doesn't explicitly mention fixing a bug.

**Git Diff Analysis:**

The diff shows a change in the `exists` method signature:

```diff
-      public boolean exists(int doc) {
+      public boolean exists(int doc) throws IOException {
        return vals.exists(doc);
      }
```

The change adds `throws IOException` to the method signature. This indicates that the underlying `vals.exists(doc)` method might now throw an `IOException`, which wasn't handled before. This is a common pattern when refactoring code to use iterators, as I/O operations are often involved in reading data from disk.

**Reasoning:**

The addition of `throws IOException` suggests that the original code might have been implicitly swallowing or ignoring potential I/O errors. While the commit message focuses on refactoring, the introduction of exception handling points to a potential improvement in robustness. The original code might have failed silently in cases where an `IOException` occurred during doc value access. The change ensures that these exceptions are now propagated, allowing for proper error handling higher up the call stack. This can be considered a bug fix because it addresses a potential silent failure.

**Conclusion:**

**Buggy**
"
lucene-solr,7428.json,ce172acb8fec6c3bbb18837a4d640da6c5aad649,"@@ -1,7 +1,6 @@
   public static void close(HttpClient httpClient) { 
-    if (httpClient instanceof CloseableHttpClient) {
-      org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
-    } else {
-      httpClient.getConnectionManager().shutdown();
-    }
+
+    org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
+
+    assert ObjectReleaseTracker.release(httpClient);
   }",NotBuggy,"SOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client."" suggests several things:

1.  **Deprecated API Usage:** The code was using deprecated `HttpClient` implementation classes.
2.  **Stale Connection Checks:** The old implementation was performing stale connection checks on every request, which is inefficient.
3.  **Connection Lifecycle Management:** The goal is to shift connection lifecycle management to the client.

**Code Diff Analysis:**

The code diff shows a change in the `close` method that takes an `HttpClient` as input. The original code had a conditional check:

*   If the `httpClient` was an instance of `CloseableHttpClient`, it would be closed using `IOUtils.closeQuietly`.
*   Otherwise, the connection manager would be shut down using `httpClient.getConnectionManager().shutdown()`.

The modified code removes this conditional check and simply attempts to close the `httpClient` as a `CloseableHttpClient` using `IOUtils.closeQuietly`. It also adds an assertion `ObjectReleaseTracker.release(httpClient)`.

**Reasoning:**

1.  **Removal of Conditional Logic:** The removal of the `instanceof` check and the direct casting to `CloseableHttpClient` suggests that the code is now *only* intended to work with `CloseableHttpClient` instances. This aligns with the commit message's intention to move to non-deprecated `HttpClient` implementations, as `CloseableHttpClient` is the preferred way to manage HTTP connections in newer versions of HttpClient.
2.  **Stale Connection Check Implication:** The commit message mentions removing stale connection checks. While the diff itself doesn't directly show the removal of these checks, the change in how connections are managed (specifically, the shift to `CloseableHttpClient`) likely implies that the stale connection checks are now handled more efficiently or are no longer necessary due to the improved connection management within `CloseableHttpClient`.
3.  **Connection Lifecycle Management:** The change in the `close` method indicates a shift in how connections are closed. By always attempting to close the `HttpClient` as a `CloseableHttpClient`, the code is relying on the `CloseableHttpClient`'s internal mechanisms for connection management, which aligns with the commit message's goal of moving connection lifecycle management towards the client.
4.  **Assertion:** The addition of `ObjectReleaseTracker.release(httpClient)` suggests a mechanism for tracking and ensuring that resources are properly released. This is often related to fixing resource leaks or ensuring proper cleanup, which are common bug-related concerns.

**Conclusion:**

The changes in the code diff, combined with the commit message, strongly suggest that this commit is addressing a bug or inefficiency related to the use of deprecated `HttpClient` implementations, inefficient connection management, and potentially resource leaks. The shift to `CloseableHttpClient` and the addition of the `ObjectReleaseTracker` assertion point towards a bug fix.

**Buggy**
"
lucene-solr,2821.json,f01b3e97d19c4b96ca9eb7d4e2a8647f68fbb969,"@@ -1,13 +1,16 @@
   public void finishStage(ResponseBuilder rb) {
     if (rb.isAnalytics() && rb.stage == ResponseBuilder.STAGE_GET_FIELDS) {
       AnalyticsRequestManager reqManager = getAnalyticsRequestManager(rb);
       // Generate responses from the merged shard data
       if (rb.isOlapAnalytics()) {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_OLD_HEADER, reqManager.createOldResponse());
       } else {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_HEADER, reqManager.createResponse());
       }
+      if (reqManager.isPartialResults()) {
+        rb.rsp.getResponseHeader().asShallowMap().put(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY,true);
+      }
     }
 
     super.finishStage(rb);
   }",NotBuggy,"SOLR-13904: Make Analytics component sensitive to timeAllowed.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-13904: Make Analytics component sensitive to timeAllowed"" suggests that the Analytics component in Solr was previously *not* properly considering the `timeAllowed` parameter (likely a request timeout). The implication is that queries could run indefinitely or longer than expected, which is a bug. The commit aims to fix this by making the component respect the specified time limit.

**Git Diff Analysis:**

The diff shows a modification within the `finishStage` method, specifically when `rb.isAnalytics()` is true and the stage is `ResponseBuilder.STAGE_GET_FIELDS`. The key addition is:

```java
if (reqManager.isPartialResults()) {
  rb.rsp.getResponseHeader().asShallowMap().put(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY,true);
}
```

This code snippet checks if the `AnalyticsRequestManager` has partial results. If it does, it adds a flag (`RESPONSE_HEADER_PARTIAL_RESULTS_KEY`) to the response header indicating that the results are partial.

**Reasoning:**

The connection to the commit message is as follows:

1.  **`timeAllowed` and Partial Results:** If the Analytics component is now sensitive to `timeAllowed`, it means that queries can be terminated prematurely if they exceed the allowed time.
2.  **Indicating Premature Termination:** When a query is terminated due to `timeAllowed`, it's likely that the results are incomplete or ""partial"".
3.  **The Diff's Purpose:** The added code explicitly sets a flag in the response header to indicate that the results are partial. This is a direct consequence of the Analytics component now respecting `timeAllowed`.  Without this change, a user might assume the results are complete when they are not, leading to incorrect interpretations.

The addition of the `RESPONSE_HEADER_PARTIAL_RESULTS_KEY` flag is a way to signal to the client that the query was cut short due to `timeAllowed`. This is important for correctness and prevents misinterpretation of results. The fact that this flag is now being set indicates that the analytics component was not handling timeouts correctly before.

**Conclusion:**

The commit message and the code changes are strongly related. The code change directly supports the claim that the Analytics component is now sensitive to `timeAllowed` by adding a mechanism to signal partial results when a query is terminated prematurely. This indicates that the previous behavior (ignoring `timeAllowed` or not signaling partial results) was a bug.

**Buggy**
"
lucene-solr,42269.json,765403b2af47c2aef71dd824f57c1d90d3a21cef,"@@ -1,17 +1,17 @@
-  public Iterable<? extends Accountable> getChildResources() {
+  public Iterable<Accountable> getChildResources() {
     List<Accountable> resources = new ArrayList<>();
     
     long docBaseDeltaBytes = RamUsageEstimator.shallowSizeOf(docBasesDeltas);
     for (PackedInts.Reader r : docBasesDeltas) {
       docBaseDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""doc base deltas"", docBaseDeltaBytes));
     
     long startPointerDeltaBytes = RamUsageEstimator.shallowSizeOf(startPointersDeltas);
     for (PackedInts.Reader r : startPointersDeltas) {
       startPointerDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""start pointer deltas"", startPointerDeltaBytes));
     
-    return resources;
+    return Collections.unmodifiableList(resources);
   }",NotBuggy,"LUCENE-6104: simplify internals of Lucene50NormsProducer

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1645711 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""LUCENE-6104: simplify internals of Lucene50NormsProducer"" suggests an internal simplification of the `Lucene50NormsProducer` class.

The diff shows two changes:

1.  The return type of `getChildResources()` is changed from `Iterable<? extends Accountable>` to `Iterable<Accountable>`. This tightens the return type, which can be considered a simplification.
2.  The returned list is now wrapped with `Collections.unmodifiableList()`. This prevents modification of the list after it's returned, which improves encapsulation and can be considered a simplification.

Both changes align with the commit message's intent to simplify the internals. There's no indication of a bug fix in the provided diff. The changes seem to be related to code cleanup and improved encapsulation.

**NotBuggy**
"
lucene-solr,48313.json,93fa72f77bd024aa09eef043c65c64a6524613dc,"@@ -1,3 +1,13 @@
   final int getCharClass(int c) {
-    return Operations.findIndex(c, points);
+
+    // binary search
+    int a = 0;
+    int b = points.length;
+    while (b - a > 1) {
+      int d = (a + b) >>> 1;
+      if (points[d] > c) b = d;
+      else if (points[d] < c) a = d;
+      else return d;
+    }
+    return a;
   }",NotBuggy,"LUCENE-7465: add SimplePatternTokenizer and SimpleSplitPatternTokenizer, for tokenization using Lucene's regexp/automaton implementation
",Buggy,"The commit message indicates the addition of two new tokenizers, `SimplePatternTokenizer` and `SimpleSplitPatternTokenizer`, utilizing Lucene's regular expression and automaton implementation. The provided code diff focuses on the `getCharClass` method, which appears to be part of the underlying regular expression or automaton processing logic. The change replaces a call to `Operations.findIndex` with a binary search implementation. This suggests an optimization or a fix to how character classes are determined within the regular expression matching process. The original method `Operations.findIndex` might have been inefficient or incorrect in certain scenarios, leading to the adoption of a binary search for improved performance and correctness. This change is likely addressing a bug or performance issue related to character class lookup within the regular expression engine.

Reasoning:

1.  **Commit Message Focus:** The commit message highlights the introduction of new tokenizers based on regular expressions.
2.  **Code Change Context:** The code diff modifies the `getCharClass` method, which is crucial for character classification in regular expression matching.
3.  **Algorithm Change:** The change from `Operations.findIndex` to a binary search indicates a potential performance improvement or bug fix in character class lookup.
4.  **Bug Fix Indication:** The replacement of a method with a more efficient and correct algorithm suggests that the original method might have had issues, pointing towards a bug fix.

Conclusion: **Buggy**
"
lucene-solr,32009.json,af680af77f3f80c779e038a0ad8a136c9dcb9f5d,"@@ -1,3 +1,3 @@
-  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, SimScorer scorer, int flags) throws IOException {
+  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, int flags) throws IOException {
     throw new UnsupportedOperationException(""Should never be called, IDVersionSegmentTermsEnum implements impacts directly"");
   }",NotBuggy,"LUCENE-8142: Make postings APIs expose raw impacts rather than scores.
",NotBuggy,"The commit message states that the postings APIs should expose raw impacts rather than scores. The provided diff shows a change in the `impacts` method signature, removing the `SimScorer scorer` parameter. This aligns with the commit message's intention to expose raw impacts, as the `SimScorer` is related to scoring, which is being removed from this API. The removal of the scorer suggests a change in how impacts are being handled, potentially to provide a more direct or raw representation. This change doesn't clearly indicate a bug fix, but rather a modification of the API to expose different data.

**NotBuggy**"
lucene-solr,22923.json,8007ac0cb0c88838ba6e58e56e2bc23374c15dc4,"@@ -1,19 +1,19 @@
   private void markAndSweepByRamSize() {
     assert markAndSweepLock.isHeldByCurrentThread() : ""markAndSweepLock held by another thread"";
     List<CacheEntry<K, V>> entriesInAccessOrder = new ArrayList<>(map.size());
     map.forEach((o, kvCacheEntry) -> {
       kvCacheEntry.lastAccessedCopy = kvCacheEntry.lastAccessed; // important because we want to avoid volatile read during comparisons
       entriesInAccessOrder.add(kvCacheEntry);
     });
 
     Collections.sort(entriesInAccessOrder); // newer access is smaller, older access is bigger
 
     // iterate in oldest to newest order
     for (int i = entriesInAccessOrder.size() - 1; i >= 0; i--) {
       CacheEntry<K, V> kvCacheEntry = entriesInAccessOrder.get(i);
       evictEntry(kvCacheEntry.key);
-      if (ramBytes.get() <= ramLowerWatermark)  {
+      if (ramBytes.sum() <= ramLowerWatermark)  {
         break; // we are done!
       }
     }
   }",NotBuggy,"SOLR-8241: Add CaffeineCache, an efficient implementation of SolrCache.
",Buggy,"The commit message ""SOLR-8241: Add CaffeineCache, an efficient implementation of SolrCache"" indicates the addition of a new caching mechanism. However, the provided diff focuses on modifications within an existing cache implementation, specifically the `markAndSweepByRamSize` method. The change involves altering a condition from `ramBytes.get() <= ramLowerWatermark` to `ramBytes.sum() <= ramLowerWatermark`. This suggests a correction in how the RAM usage is checked during the cache eviction process. The original code likely had a bug where it wasn't correctly calculating or accessing the total RAM usage, leading to premature or delayed cache eviction. The use of `sum()` implies that `ramBytes` is not a simple atomic value but something that needs to be aggregated, which was missed in the original implementation. Therefore, the code change fixes a bug related to RAM usage calculation in the cache eviction logic.

**Buggy**"
lucene-solr,16275.json,f177a660f5745350207dc61b46396b49404fd383,"@@ -1,29 +1,30 @@
   public ZkWriteCommand updateShardState(ClusterState clusterState, ZkNodeProps message) {
     String collectionName = message.getStr(ZkStateReader.COLLECTION_PROP);
     if (!checkCollectionKeyExistence(message)) return ZkStateWriter.NO_OP;
     log.info(""Update shard state invoked for collection: "" + collectionName + "" with message: "" + message);
 
     DocCollection collection = clusterState.getCollection(collectionName);
     Map<String, Slice> slicesCopy = new LinkedHashMap<>(collection.getSlicesMap());
     for (String key : message.keySet()) {
       if (ZkStateReader.COLLECTION_PROP.equals(key)) continue;
       if (Overseer.QUEUE_OPERATION.equals(key)) continue;
 
       Slice slice = collection.getSlice(key);
       if (slice == null) {
         throw new RuntimeException(""Overseer.updateShardState unknown collection: "" + collectionName + "" slice: "" + key);
       }
       log.info(""Update shard state "" + key + "" to "" + message.getStr(key));
       Map<String, Object> props = slice.shallowCopy();
       
-      if (Slice.State.getState((String) props.get(ZkStateReader.STATE_PROP)) == Slice.State.RECOVERY
-          && Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
+      if (Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
         props.remove(Slice.PARENT);
+        props.remove(""shard_parent_node"");
+        props.remove(""shard_parent_zk_session"");
       }
       props.put(ZkStateReader.STATE_PROP, message.getStr(key));
       Slice newSlice = new Slice(slice.getName(), slice.getReplicasCopy(), props);
       slicesCopy.put(slice.getName(), newSlice);
     }
 
     return new ZkWriteCommand(collectionName, collection.copyWithSlices(slicesCopy));
   }",NotBuggy,"SOLR-9438: Shard split can be marked successful and sub-shard states switched to 'active' even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover
",Buggy,"The commit message describes a scenario where a shard split can be incorrectly marked as successful even if some sub-shard replicas fail to recover due to leader crashes or restarts. The code diff modifies the `updateShardState` method in `ZkWriteCommand`.

Specifically, the code now removes the `Slice.PARENT`, `""shard_parent_node""`, and `""shard_parent_zk_session""` properties when a slice's state transitions to `ACTIVE`. The removal of `Slice.PARENT` property when shard state becomes active makes sense in the context of shard splitting. After a split, the sub-shards become independent and should no longer be considered children of the original shard. The removal of `""shard_parent_node""` and `""shard_parent_zk_session""` further solidifies this independence.

The original code only removed the `Slice.PARENT` property if the slice was in `RECOVERY` state and then transitioned to `ACTIVE`. The modified code removes these properties directly when the state becomes `ACTIVE`. This change addresses the issue described in the commit message by ensuring that these properties are removed regardless of the previous state, which is crucial for preventing the parent shard from being marked as active prematurely when sub-shards haven't fully recovered.

Therefore, the changes in the code directly address the bug described in the commit message.

**Buggy**"
lucene-solr,27550.json,54f256503867c4265a994b94af2917e1375c2181,"@@ -1,14 +1,14 @@
   protected int split(byte[] minPackedValue, byte[] maxPackedValue) {
     // Find which dim has the largest span so we can split on it:
     int splitDim = -1;
     for(int dim=0;dim<numDims;dim++) {
       NumericUtils.subtract(bytesPerDim, dim, maxPackedValue, minPackedValue, scratchDiff);
-      if (splitDim == -1 || StringHelper.compare(bytesPerDim, scratchDiff, 0, scratch1, 0) > 0) {
+      if (splitDim == -1 || FutureArrays.compareUnsigned(scratchDiff, 0, bytesPerDim, scratch1, 0, bytesPerDim) > 0) {
         System.arraycopy(scratchDiff, 0, scratch1, 0, bytesPerDim);
         splitDim = dim;
       }
     }
 
     //System.out.println(""SPLIT: "" + splitDim);
     return splitDim;
   }",NotBuggy,"LUCENE-8469: Inline calls to the deprecated StringHelper.compare, removed StringHelper.compare from master.
",NotBuggy,"The commit message indicates the removal of `StringHelper.compare` and inlining its calls. The provided diff shows that `StringHelper.compare` is replaced with `FutureArrays.compareUnsigned`. This aligns with the commit message's intention to remove the deprecated method. The change doesn't inherently suggest a bug fix, but rather a refactoring to remove deprecated code.

**NotBuggy**"
lucene-solr,47988.json,f1fda6880b784c8554439efcce4e6d464d97763b,"@@ -1,56 +1,54 @@
-  public static void decompress(DataInput compressed, int decompressedLen, BytesRef destBytes) throws IOException {
-    final byte[] dest = destBytes.bytes;
+  public static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {
     final int destEnd = dest.length;
-    int dOff = 0;
 
-    while (dOff < decompressedLen) {
+    do {
       // literals
       final int token = compressed.readByte() & 0xFF;
       int literalLen = token >>> 4;
 
       if (literalLen != 0) {
         if (literalLen == 0x0F) {
           byte len;
           while ((len = compressed.readByte()) == (byte) 0xFF) {
             literalLen += 0xFF;
           }
           literalLen += len & 0xFF;
         }
         compressed.readBytes(dest, dOff, literalLen);
         dOff += literalLen;
       }
 
       if (dOff >= decompressedLen) {
         break;
       }
 
       // matchs
       final int matchDec = (compressed.readByte() & 0xFF) | ((compressed.readByte() & 0xFF) << 8);
       assert matchDec > 0;
 
       int matchLen = token & 0x0F;
       if (matchLen == 0x0F) {
         int len;
         while ((len = compressed.readByte()) == (byte) 0xFF) {
           matchLen += 0xFF;
         }
         matchLen += len & 0xFF;
       }
       matchLen += MIN_MATCH;
 
       // copying a multiple of 8 bytes can make decompression from 5% to 10% faster
       final int fastLen = ((matchLen - 1) & 0xFFFFFFF8) + 8;
       if (matchDec < matchLen || dOff + fastLen > destEnd) {
         // overlap -> naive incremental copy
         for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {
           dest[dOff] = dest[ref];
         }
       } else {
         // no overlap -> arraycopy
         System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);
         dOff += matchLen;
       }
-    }
-    destBytes.offset = 0;
-    destBytes.length = dOff;
+    } while (dOff < decompressedLen);
+
+    return dOff;
   }",NotBuggy,"LUCENE-4554: CompressingStoredFieldsFormat: don't write the original length at the beginning of the chunk with LZ4 compressors


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1408476 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""LUCENE-4554: CompressingStoredFieldsFormat: don't write the original length at the beginning of the chunk with LZ4 compressors"" indicates a change related to the LZ4 compression algorithm used for storing fields in Lucene. The message suggests that the original length of the uncompressed data was previously written at the beginning of each compressed chunk, and this practice has been removed.

The code diff shows a modification to the `decompress` method. The method signature has changed, and the logic within the method has been adjusted. The original method took a `BytesRef` as input and modified it. The new method takes a byte array and an offset, and returns the number of bytes written. The core decompression logic remains similar, but the way the decompressed data is handled has changed.

The removal of writing the original length at the beginning of the chunk is likely an optimization or a correction to the compression format. It could be related to a bug where the length was incorrectly written or not needed, or it could be an optimization to reduce the size of the compressed data. Given the context of compression and decompression, and the change in method signature, it's likely that this commit addresses a bug or inefficiency in the compression/decompression process.

**Buggy**
"
lucene-solr,4912.json,23c73d8ff85f9b6eea30e2c03930d9e8ba116da0,"@@ -1,13 +1,10 @@
-  private void loadParams(String params, Properties props) throws SQLException {
-    try {
-      String[] pairs = params.split(""&"");
-      for (String pair : pairs) {
-        String[] keyValue = pair.split(""="");
-        String key = URLDecoder.decode(keyValue[0], ""UTF-8"");
-        String value = URLDecoder.decode(keyValue[1], ""UTF-8"");
-        props.put(key, value);
+  private void loadParams(URI uri, Properties props) throws SQLException {
+    List<NameValuePair> parsedParams = URLEncodedUtils.parse(uri, ""UTF-8"");
+    for (NameValuePair pair : parsedParams) {
+      if (pair.getValue() != null) {
+        props.put(pair.getName(), pair.getValue());
+      } else {
+        props.put(pair.getName(), """");
       }
-    } catch(Exception e) {
-      throw new SQLException(e);
     }
   }",NotBuggy,"SOLR-8179: SQL JDBC - DriverImpl loadParams doesn't support keys with no values in the connection string

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1716198 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SOLR-8179: SQL JDBC - DriverImpl loadParams doesn't support keys with no values in the connection string"" clearly states that the `loadParams` method in `DriverImpl` had a bug: it couldn't handle connection string parameters where a key existed but had no associated value (e.g., `key1&key2=value2`).

**Diff Analysis:**

The diff shows a significant change in how the `loadParams` method processes connection parameters.

1.  **Input Change:** The method signature changed from accepting a `String` to a `URI`. This suggests a more robust way of parsing the connection string.
2.  **Parsing Logic:** The original code used `String.split()` to separate key-value pairs and then again to separate the key from the value. This approach is prone to errors if the connection string isn't perfectly formatted. The new code uses `URLEncodedUtils.parse()`, which is a standard library function designed for parsing URI-encoded parameters.
3.  **Handling Missing Values:** The original code assumed that every key had a value and would throw an exception if `keyValue[1]` was missing (due to the `split(""="")`. The new code explicitly checks if `pair.getValue()` is null. If it is, it puts an empty string as the value. This directly addresses the bug described in the commit message.
4.  **Error Handling:** The original code had a `try-catch` block that wrapped the entire parsing logic and re-threw any exception as an `SQLException`. The new code removes this `try-catch` block. `URLEncodedUtils.parse` handles the exceptions internally.

**Reasoning:**

The code changes directly address the issue described in the commit message. The original code was failing to parse connection strings with keys that didn't have values. The new code correctly handles this case by explicitly checking for null values and assigning an empty string when a value is missing. The change from `String.split()` to `URLEncodedUtils.parse()` also makes the parsing logic more robust and less prone to errors. The removal of the `try-catch` block indicates that the `URLEncodedUtils.parse` handles the exceptions internally, which is an improvement.

**Conclusion:**

The changes in the code directly fix the bug described in the commit message.

**Buggy**
"
jna,3166.json,81908ed5b40ce3cdab32aabd29db56e2d0611e1f,"@@ -1,18 +1,18 @@
         public void fromXModifierKeymap(X11.XModifierKeymapRef xModifierKeymapRef) {
             int count = xModifierKeymapRef.max_keypermod;
             byte[] keys = xModifierKeymapRef.modifiermap.getByteArray(0, 8*count);
 
             ArrayList<Byte>[] allModifiers = getAllModifiers();
 
             for (int modNr = 0; modNr < 8; modNr++) {
                 ArrayList<Byte> modifier = allModifiers[modNr];
                 modifier.clear();
 
                 for (int keyNr = 0; keyNr < count; keyNr++) {
                     byte key = keys[modNr*count + keyNr];
                     if (key != 0) {
-                        modifier.add(new Byte(key));
+                        modifier.add(Byte.valueOf(key));
                     }
                 }
             }
         }",NotBuggy,"Apply generic definitions wherever applicable
",NotBuggy,"The commit message ""Apply generic definitions wherever applicable"" suggests a refactoring to use generics. The code diff shows a change from `new Byte(key)` to `Byte.valueOf(key)`. This change replaces the explicit object creation with a static factory method. While this can be considered a best practice, it doesn't directly indicate a bug fix. The change improves code clarity and potentially memory usage by leveraging the `Byte` cache, but it doesn't address any specific error or incorrect behavior. The code was likely working before the change.

**NotBuggy**"
jna,2910.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,9 +1,7 @@
     public static void lSetXAttr(String path, String name, byte[] value) throws IOException {
-        Memory valueMem = bytesToMemory(value);
-        final int retval = XAttr.INSTANCE.lsetxattr(path, name, valueMem,
-            new size_t(valueMem.size()), 0);
+        final int retval = XAttr.INSTANCE.lsetxattr(path, name, value, new size_t(value.length), 0);
         if (retval != 0) {
             final int eno = Native.getLastError();
             throw new IOException(""errno: "" + eno);
         }
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",NotBuggy,"The commit message states ""Add byte[] based bindings to c.s.j.p.linux.XAttr"". The code diff shows that the `lSetXAttr` method now directly accepts a `byte[]` as the value, instead of converting it to a `Memory` object first. This change aligns with the commit message, as it introduces a new binding that accepts a `byte[]`. The previous implementation used `bytesToMemory` to convert the byte array to a `Memory` object. The new implementation directly passes the byte array to the native function. This change also removes the need to manage the `Memory` object, which could potentially lead to memory leaks if not handled correctly. The change also uses `value.length` instead of `valueMem.size()`, which is more straightforward. This change seems to be an improvement and doesn't directly indicate a bug fix, but rather an enhancement by providing a more convenient way to set extended attributes using byte arrays.

**NotBuggy**"
jna,1326.json,3bba92c542dbb2ccd4daf6120ae914a43c297294,"@@ -1,3 +1,4 @@
     public static boolean IsWindowsXPSP3OrGreater() {
-        return IsWindowsVersionOrGreater(HIBYTE(WinNT.WIN32_WINNT_WINXP), LOBYTE(WinNT.WIN32_WINNT_WINXP), 3);
+        return IsWindowsVersionOrGreater((byte) (Kernel32.WIN32_WINNT_WINXP >>> 8), (byte) Kernel32.WIN32_WINNT_WINXP,
+                3);
     }",NotBuggy,"Javadocs, better typecasting",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** ""Javadocs, better typecasting""
    *   The commit message suggests improvements to Javadocs (documentation) and typecasting.

2.  **Git Diff:**
    *   The code change involves the `IsWindowsXPSP3OrGreater()` method.
    *   The original code used `HIBYTE` and `LOBYTE` with `WinNT.WIN32_WINNT_WINXP`.
    *   The modified code uses `(byte) (Kernel32.WIN32_WINNT_WINXP >>> 8)` and `(byte) Kernel32.WIN32_WINNT_WINXP`.

3.  **Reasoning:**
    *   The change from `WinNT` to `Kernel32` suggests a potential correction of the source of the `WIN32_WINNT_WINXP` constant. This could be due to an incorrect import or a more appropriate location for the constant.
    *   The explicit typecasting to `byte` using `(byte)` indicates an attempt to ensure the correct data type is used in the `IsWindowsVersionOrGreater` method. This is a form of defensive programming and might be addressing a potential issue where the values were not being correctly interpreted.
    *   The use of `>>> 8` (unsigned right shift) to extract the high byte is a standard technique, but its introduction here alongside the typecasting suggests a potential bug fix related to how the version information was being processed. The original `HIBYTE` macro might have been problematic in some contexts.
    *   The commit message mentions ""better typecasting,"" which directly aligns with the code changes. The change likely addresses a potential issue where the version numbers were not being correctly interpreted due to incorrect type handling.

**Conclusion:**

The code changes, combined with the commit message, strongly suggest that the ""better typecasting"" was introduced to fix a potential bug related to how Windows version numbers were being processed. The use of explicit type casting and the change in how the high and low bytes are extracted indicate a correction to ensure the `IsWindowsVersionOrGreater` method receives the correct input.

**Buggy**"
jna,3850.json,d395318a8ba8744fb61805cde5644863daaf5f94,"@@ -1,3 +1,3 @@
     public String[] getWideStringArray(long offset, int length) {
-        return getStringArray(offset, -1, NativeString.WIDE_STRING);
+        return getStringArray(offset, length, NativeString.WIDE_STRING);
     }",NotBuggy,"Pointer.getWideStringArray respects the length parameter
",Buggy,"The commit message states that the `Pointer.getWideStringArray` method now respects the `length` parameter.

The diff shows that the `getWideStringArray` method's implementation was changed to pass the `length` parameter to the internal `getStringArray` method. Previously, it was passing `-1` as the length, which likely meant that the length was not being respected, and the method was probably reading until a null terminator.

This change aligns with the commit message and indicates a bug fix where the `length` parameter was previously ignored.

**Buggy**"
jna,324.json,e4a6950d98b96f258fa31576ae9fc606091e66b6,"@@ -1,38 +1,41 @@
-	public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
-		IntByReference pcbNeeded = new IntByReference();
-		IntByReference pcReturned = new IntByReference();
-		HANDLEByReference pHandle = new HANDLEByReference();
+    public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
+        IntByReference pcbNeeded = new IntByReference();
+        IntByReference pcReturned = new IntByReference();
+        HANDLEByReference pHandle = new HANDLEByReference();
 
-		if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null))
-			throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null)) {
+            throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        }
 
-		Win32Exception we = null;
-		PRINTER_INFO_2 pinfo2 = null;
+        Win32Exception we = null;
+        PRINTER_INFO_2 pinfo2 = null;
 
-		try {
-			Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
-			if (pcbNeeded.getValue() <= 0)
-				return new PRINTER_INFO_2();
+        try {
+            Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
+            if (pcbNeeded.getValue() <= 0) {
+                return new PRINTER_INFO_2();
+            }
 
-			pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
-			if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned))
-				throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
+            if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned)) {
+                throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            }
 
-			pinfo2.read();
-		} catch (Win32Exception e) {
-			we = e;
-		} finally {
-			if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
-				Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
-				if (we != null) {
-					ex.addSuppressedReflected(we);
-				}
-			}
-		}
+            pinfo2.read();
+        } catch (Win32Exception e) {
+            we = e;
+        } finally {
+            if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
+                Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
+                if (we != null) {
+                    ex.addSuppressedReflected(we);
+                }
+            }
+        }
 
-		if (we != null) {
-			throw we;
-		}
+        if (we != null) {
+            throw we;
+        }
 
-		return pinfo2;
-	}

+        return pinfo2;
+    }",NotBuggy,"Checkstyle FileTabCharacter, Trailing Whitespace, line endings to \n, indention
",NotBuggy,"The commit message indicates changes related to code style and formatting, such as Checkstyle, file tab characters, trailing whitespace, line endings, and indentation. The diff shows changes in indentation and the addition of braces for single-line `if` statements. These changes align with the commit message's description of code style and formatting improvements. There is no indication of bug fixes in the code.
**NotBuggy**"
jna,3493.json,6705513b86030fd7d609d8e03539527d2a553651,"@@ -1,6 +1,9 @@
         protected void finalize() {
-            for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
-                Map.Entry e = (Map.Entry)i.next();
-                unregister((Class)e.getKey(), (long[])e.getValue());
+            synchronized(registeredClasses) {
+                for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
+                    Map.Entry e = (Map.Entry)i.next();
+                    unregister((Class)e.getKey(), (long[])e.getValue());
+                    i.remove();
+                }
             }
         }",NotBuggy,"add structure, string, NIO Buffer and primitive array support to direct type mapping

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@841 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates the addition of support for various data types to direct type mapping. The code diff shows a change in the `finalize` method where a `synchronized` block is added around the loop that iterates through `registeredClasses`. Additionally, `i.remove()` is called within the loop. This suggests a potential issue where the `registeredClasses` map was being modified concurrently, leading to errors during finalization. The addition of synchronization and the explicit removal of elements from the iterator likely address this concurrency problem. Therefore, the changes indicate a bug fix related to concurrent modification during finalization.

**Buggy**"
jna,3649.json,8e6cbda11092f95b4e00a0f650dfd3de52ac5bb8,"@@ -1,3 +1,3 @@
-        static boolean isBuffer(Class cls) {
+        static boolean isBuffer(Class<?> cls) {
             return Buffer.class.isAssignableFrom(cls);
         }",NotBuggy,"Use generic definition of Native#loadLibrary
",NotBuggy,"The commit message ""Use generic definition of Native#loadLibrary"" suggests a change related to using a generic type definition in the `Native#loadLibrary` method.

The provided code diff changes the parameter type of the `isBuffer` method from `Class` to `Class<?>`. This change involves using a generic type (`<?>`) in the method signature. This aligns with the commit message's intention to use a generic definition. The change itself doesn't directly indicate a bug fix, but rather a refinement of the type definition for better type safety or compatibility. There is no error handling, logical correction, or exception handling improvement.

**NotBuggy**"
jna,2373.json,596022a4f955a5decfd41e6c709a90a4680184f9,"@@ -1,43 +1,57 @@
 	public Object invoke(final Object proxy, final java.lang.reflect.Method method, final Object[] args)
 			throws Throwable {
                 boolean declaredAsInterface = 
                         (method.getAnnotation(ComMethod.class) != null)
                         ||(method.getAnnotation(ComProperty.class) != null);
             
 		if ((! declaredAsInterface) && (method.getDeclaringClass().equals(Object.class)
                         || method.getDeclaringClass().equals(IRawDispatchHandle.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IUnknown.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IDispatch.class)
                         || method.getDeclaringClass().equals(IConnectionPoint.class)
                         )) {
                         try {
                             return method.invoke(this, args);
                         } catch (InvocationTargetException ex) {
                             throw ex.getCause();
                         }
 		}
 
 		Class<?> returnType = method.getReturnType();
 		boolean isVoid = Void.TYPE.equals(returnType);
 
 		ComProperty prop = method.getAnnotation(ComProperty.class);
 		if (null != prop) {
+                        int dispId = prop.dispId();
 			if (isVoid) {
-				String propName = this.getMutatorName(method, prop);
-				this.setProperty(propName, args[0]);
-				return null;
+                                if(dispId != -1) {
+                                    this.setProperty(new DISPID(dispId), args[0]);
+                                    return null;
+                                } else {
+                                    String propName = this.getMutatorName(method, prop);
+                                    this.setProperty(propName, args[0]);
+                                    return null;
+                                }
 			} else {
-				String propName = this.getAccessorName(method, prop);
-				return this.getProperty(returnType, propName, args);
+                                if(dispId != -1) {
+                                    return this.getProperty(returnType, new DISPID(dispId), args);
+                                } else {
+                                    String propName = this.getAccessorName(method, prop);
+                                    return this.getProperty(returnType, propName, args);
+                                }
 			}
 		}
 
 		ComMethod meth = method.getAnnotation(ComMethod.class);
 		if (null != meth) {
-			String methName = this.getMethodName(method, meth);
-			Object res = this.invokeMethod(returnType, methName, args);
-			return res;
+                        int dispId = meth.dispId();
+                        if(dispId != -1) {
+                            return this.invokeMethod(returnType, new DISPID(dispId), args);
+                        } else {
+                            String methName = this.getMethodName(method, meth);
+                            return this.invokeMethod(returnType, methName, args);
+                        }
 		}
 
 		return null;
 	}",NotBuggy,"Add dispId to ComProperty and use value to simplify COM calls (skip GetIDsOfNames)

dispIDs can be queried via GetIDsOfNames at runtime or retrieved from the
typelibrary when bindings are generated.

Using the dispID eliminates one additional call into native environment.

It was also observed, that runtime reflection sometimes retrieves wrong
values (toString on IHTMLDocument2), so using a previously retrieved value
seems to be saver.
",Buggy,"The commit message indicates an optimization by using `dispId` to avoid `GetIDsOfNames` calls and a potential bug fix related to runtime reflection retrieving wrong values.

The code changes introduce the usage of `dispId` from the `ComProperty` and `ComMethod` annotations. The code now checks if `dispId` is present (not equal to -1). If it is, the code uses the `dispId` to call `setProperty`, `getProperty`, or `invokeMethod` with a `DISPID` object. Otherwise, it falls back to the original behavior of using the property or method name.

The commit message mentions that runtime reflection sometimes retrieves wrong values. This suggests that the previous implementation had a bug where the retrieved property or method name was incorrect, leading to unexpected behavior. The use of `dispId` is intended to address this issue by using a pre-defined identifier instead of relying on runtime reflection.

Based on the commit message and the code changes, the commit addresses a bug related to incorrect values retrieved during runtime reflection.

**Buggy**"
jna,1645.json,429bed5aba63585863f9d734b7a1b0e8364610ff,"@@ -1,3 +1,3 @@
-		public SCODE getValue() {
-			return new SCODE(getPointer().getLong(0));
+		public CHAR getValue() {
+			return new CHAR(getPointer().getChar(0));
 		}",NotBuggy,"datatypes overworked 2
",Buggy,"The commit message ""datatypes overworked 2"" is vague but suggests modifications related to data types.

The code diff shows a change in the return type of the `getValue()` method from `SCODE` to `CHAR`. It also changes the method call from `getLong(0)` to `getChar(0)`. This indicates a change in how the value is being retrieved and interpreted, likely to align with the correct data type. This suggests a correction related to data types, which could be due to a bug where the wrong data type was being used.

**Buggy**
"
jna,602.json,2cea7182931b6f5c314d9d26afa8d1f73a0fd400,"@@ -1,3 +1,3 @@
         public String getPrintName() {
-            return String.copyValueOf(PathBuffer, this.PrintNameOffset.intValue() / 2, this.PrintNameLength.intValue() / 2);
+            return String.copyValueOf(PathBuffer, PrintNameOffset / 2, PrintNameLength / 2);
         }",NotBuggy,"Changes based on feedback
",NotBuggy,"The commit message ""Changes based on feedback"" is vague and doesn't provide specific details about the changes made.

The code diff shows a change in the `getPrintName()` method where `this.PrintNameOffset.intValue()` and `this.PrintNameLength.intValue()` are replaced with `PrintNameOffset` and `PrintNameLength` respectively. This suggests that the code was refactored to directly use the `PrintNameOffset` and `PrintNameLength` variables instead of calling `intValue()` on them. This could be due to a change in the type of `PrintNameOffset` and `PrintNameLength` or a simplification of the code.

Without more context, it's difficult to determine if this change is a bug fix. It could be a performance optimization or a code cleanup. The commit message doesn't indicate a bug fix.

**NotBuggy**"
jna,3448.json,fcf1f5c283ee83da50cfbf2b6d2900cc944eee18,"@@ -1,8 +1,3 @@
 	public ActiveWorkbook getActiveWorkbook() {
-		VARIANT.ByReference result = new VARIANT.ByReference();
-		HRESULT hr = oleMethod(OleAuto.DISPATCH_PROPERTYGET, result,
-				this.iDispatch, ""ActiveWorkbook"");
-
-		COMUtils.SUCCEEDED(hr);
-		return new ActiveWorkbook((IDispatch) result.getValue());
+		return new ActiveWorkbook(this.getAutomationProperty(""ActiveWorkbook""));
 	}",NotBuggy,"restored because of lost source code
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""restored because of lost source code"" strongly suggests that the code was reverted to a previous state due to an issue like accidental deletion or corruption. This doesn't inherently indicate a bug fix in the traditional sense. It implies a recovery operation rather than a correction of faulty logic.

**Code Diff Analysis:**

The code diff shows a change in the `getActiveWorkbook()` method. The original code used `VARIANT.ByReference` and `oleMethod` to retrieve the active workbook, along with a check using `COMUtils.SUCCEEDED(hr)`. The restored code uses `this.getAutomationProperty(""ActiveWorkbook"")` directly.

The original code appears more complex, potentially handling COM object interactions and error checking (`COMUtils.SUCCEEDED(hr)`). The restored code seems simpler, possibly relying on a higher-level abstraction provided by `getAutomationProperty()`.

**Reasoning:**

Given the commit message and the code diff, the most likely scenario is that the original code was lost or corrupted, and the developer reverted to an older version. The older version may or may not have had bugs, but the commit itself doesn't introduce a bug fix. It's a restoration. The removal of the explicit error checking (`COMUtils.SUCCEEDED(hr)`) could even be seen as a potential step backward, although it's impossible to say for sure without more context.

**Conclusion:**

**NotBuggy**
"
jna,657.json,e69d7fa485d90d0ae2541f2adbb0f194035188f7,"@@ -1,3 +1,3 @@
 		public void setValue(String value) {
-			getPointer().setString(0, value, true);
+			getPointer().setString(0, value);
 		}",NotBuggy,"some changes
",Buggy,"The commit message ""some changes"" is not informative.

The code change modifies the `setString` method call. The original code `getPointer().setString(0, value, true)` sets a string value with a boolean parameter `true`. The modified code `getPointer().setString(0, value)` removes the boolean parameter.

Without knowing the context and the `setString` method's behavior, it's hard to determine if this change is a bug fix. The boolean parameter might have controlled null termination or some other encoding behavior. Removing it might fix a bug related to incorrect string encoding or handling. It's possible that the `true` value was causing issues, and removing it resolves them.

Given the lack of information in the commit message and the potential for the boolean parameter to be related to a bug, I will assume this is a bug fix.

**Buggy**"
jna,3553.json,ca95bfb424336336be4b3e2ef8d670281a554690,"@@ -1,18 +1,19 @@
     static void removeTemporaryFiles() throws IOException {
         File dir = getTempDir();
         FilenameFilter filter = new FilenameFilter() {
+            @Override
             public boolean accept(File dir, String name) {
                 return name.endsWith("".x"") && name.startsWith(JNA_TMPLIB_PREFIX);
             }
         };
         File[] files = dir.listFiles(filter);
         for (int i=0;files != null && i < files.length;i++) {
             File marker = files[i];
             String name = marker.getName();
             name = name.substring(0, name.length()-2);
             File target = new File(marker.getParentFile(), name);
             if (!target.exists() || target.delete()) {
                 marker.delete();
             }
         }
     }",NotBuggy,"Added volume management functions
",NotBuggy,"The commit message ""Added volume management functions"" indicates the introduction of new functionalities related to volume management. However, the provided code diff focuses on temporary file removal, specifically targeting files ending with "".x"" and starting with ""JNA_TMPLIB_PREFIX"". This operation is related to cleaning up temporary files, not volume management. The code attempts to delete both a target file and a marker file, with a check to ensure the target file either doesn't exist or is successfully deleted before deleting the marker file. This looks like a cleanup routine. The addition of `@Override` annotation to the `accept` method is a good practice, but it doesn't inherently indicate a bug fix. The logic itself seems reasonable for its intended purpose. Therefore, the changes do not align with the commit message, and there's no indication of a bug fix in the provided code.

**NotBuggy**"
jna,3910.json,5dc6bfb195a7e309feb17ec9a69b4540b03fc7da,"@@ -1,3 +1,3 @@
-        public void setChar(int bOff, char value) {
+        public String getString(long bOff, boolean wide) {
             throw new UnsupportedOperationException(MSG); 
         }",NotBuggy,"use long offsets and sizes for pointer arithmetic

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@381 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message ""use long offsets and sizes for pointer arithmetic"" suggests a change in how offsets and sizes are handled, likely to accommodate larger values.

The diff shows a method `setChar` being replaced by `getString`. The signature of `getString` takes a `long bOff` as a parameter, which aligns with the commit message's intention to use long offsets. The original method `setChar` took an `int` as offset. The change from `int` to `long` for offset `bOff` strongly suggests an effort to fix a potential bug related to handling large offsets, where the `int` type might have been insufficient, leading to incorrect memory access or calculations.

Therefore, the changes indicate a bug fix.

**Buggy**
"
jna,2915.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,6 +1,4 @@
     public static String getXAttr(String path, String name, String encoding) throws IOException {
-        Memory valueMem = getXAttrAsMemory(path, name);
-        return Charset.forName(encoding)
-            .decode(valueMem.getByteBuffer(0, valueMem.size()))
-            .toString();
+        byte[] valueMem = getXAttrBytes(path, name);
+        return new String(valueMem, Charset.forName(encoding));
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Add byte[] based bindings to c.s.j.p.linux.XAttr"" suggests that the change introduces a new way to access XAttr values as byte arrays. This could be for performance reasons or to provide more flexibility in handling different character encodings.

2.  **Code Diff:** The code diff shows that the `getXAttr` method is being modified.
    *   **Original Code:** The original code retrieves the XAttr value as a `Memory` object, then converts it to a `ByteBuffer`, and finally decodes it into a String using the specified encoding.
    *   **New Code:** The modified code now calls `getXAttrBytes` (presumably a newly added or modified method to directly retrieve the XAttr value as a byte array). It then directly creates a String from the byte array using the specified encoding.

3.  **Bug Fix Assessment:**
    *   The change replaces a more complex process (Memory -> ByteBuffer -> decode) with a simpler one (byte[] -> String). This could be done for performance reasons, or to avoid potential issues with the `Memory` object or `ByteBuffer` conversion.
    *   The commit message indicates the addition of byte[] based bindings, which suggests a new feature or enhancement rather than a bug fix.
    *   There's no explicit error handling being added or corrected in the diff.
    *   The change simplifies the code, which could be a refactoring effort.

**Conclusion:**

Based on the commit message and the code diff, the changes appear to be an enhancement or refactoring to provide a more direct way to access XAttr values as byte arrays. There's no clear indication of a bug being fixed.

**NotBuggy**"
spring-framework,19645.json,fe7e2a7f54c49efd99de2236dfdddf2639c2b354,"@@ -1,3 +1,3 @@
-	public int compareTo(MediaTypeExpression other) {
+	public int compareTo(AbstractMediaTypeExpression other) {
 		return MediaType.SPECIFICITY_COMPARATOR.compare(this.getMediaType(), other.getMediaType());
 	}",NotBuggy,"Improvements to the way the content of RequestConditions is exposed.

RequestCondition types keep individual expression types (e.g. the
discrete header or param expressions) package private. Although the 
implementation of these types should remain private, there is no 
reason not to provide access to the underlying expression data --
e.g. for creating a REST endpoint documentation tool, or if you 
want to know which of the ""consumes""/""produces"" media types
are negated.

This change ensures that all RequestCondition types have a public
getter that makes available the basic expression data. 



",NotBuggy,"The commit message describes improvements to the way the content of `RequestConditions` is exposed, specifically focusing on providing access to the underlying expression data for various purposes like documentation or understanding negated media types. The change involves adding public getters to `RequestCondition` types to expose this data.

The provided code diff shows a change in the `compareTo` method signature of `MediaTypeExpression`. It changes the parameter type from `MediaTypeExpression` to `AbstractMediaTypeExpression`. This change seems to be related to exposing the underlying expression data, as it allows the comparison to be done with a more general type, potentially exposing the data through the abstract class.

The change aligns with the commit message, as it provides a way to access the underlying expression data by using a more general type in the `compareTo` method. This could be part of a larger effort to add public getters as described in the commit message.

The change doesn't directly indicate a bug fix. It's more about improving the API and making the underlying data accessible.

**NotBuggy**"
spring-framework,1371.json,00ed8da5c5b5d82cf60c90de1fe41c687c822b2f,"@@ -1,3 +1,3 @@
-	public Session getSession(Class<? extends Session> sessionType) {
+	public <S extends Session> S getSession(Class<S> sessionType) {
 		return getSession(sessionType, null);
 	}",NotBuggy,"JmsTransactionManager supports lazy resource retrieval

Closes gh-22468
",NotBuggy,"The commit message ""JmsTransactionManager supports lazy resource retrieval"" and the provided code diff suggest a change related to how JMS sessions are retrieved, potentially optimizing resource usage.

The code diff modifies the `getSession` method signature in `JmsTransactionManager`. Specifically, it changes the return type from `Session` to a generic type `S` that extends `Session`. This allows the method to return a more specific type of session if needed, which aligns with the idea of lazy resource retrieval. The change from `Class<? extends Session>` to `Class<S>` in the method parameter also supports this.

The commit message mentions ""lazy resource retrieval,"" which implies that the change aims to delay the acquisition of JMS resources (like sessions) until they are actually needed. The generic return type allows for potentially returning specialized session types that might be created on demand or retrieved from a pool only when necessary.

The change doesn't explicitly indicate a bug fix in the traditional sense of correcting erroneous behavior. Instead, it seems to be an enhancement or optimization to improve resource management.

**NotBuggy**
"
spring-framework,5771.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,4 +1,4 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		return (this == other || (other instanceof ApplicationListenerDetector &&
 				this.applicationContext == ((ApplicationListenerDetector) other).applicationContext));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message indicates that the change involves adding a `@Nullable` annotation to the `equals` method's parameter. This suggests that the parameter `other` could potentially be null, and the code should handle this case gracefully.

The diff shows the addition of the `@Nullable` annotation to the `other` parameter of the `equals` method. This change doesn't modify the logic of the `equals` method itself. It simply provides a hint to static analysis tools that the `other` parameter can be null.

Since the change is simply adding an annotation, it doesn't directly fix a bug. It's more of an enhancement or clarification.

**NotBuggy**"
spring-framework,29158.json,347f34c68a7a5bab46cf869e762fb3cee9a1fedc,"@@ -1,3 +1,3 @@
 	public String getParameter(String paramName) {
-		return (String) getExternalContext().getRequestParameterMap().get(paramName);
+		return getExternalContext().getRequestParameterMap().get(paramName);
 	}",NotBuggy,"EL container integration; support for contextual objects; removal of deprecated Spring 2.0 functionality; Java 5 code style
",Buggy,"The commit message mentions ""EL container integration; support for contextual objects; removal of deprecated Spring 2.0 functionality; Java 5 code style"". The code change involves a simplification of retrieving a request parameter. Specifically, it removes an explicit cast `(String)` from the result of `getExternalContext().getRequestParameterMap().get(paramName)`.

Reasoning:

1.  **Alignment with Commit Message:** The change aligns with the ""Java 5 code style"" part of the commit message. Before Java 5, generic types were not available, and casts were often necessary. After Java 5, with generics, the need for explicit casting is reduced. The removal of the cast suggests an update to leverage Java 5 features.
2.  **Bug Fix Indication:** The removal of the cast suggests that the code was potentially buggy. If the `getRequestParameterMap()` could return a non-String value for a given parameter, the cast would throw a `ClassCastException`. Removing the cast implies either:
    *   The code is now relying on the caller to handle the type of the returned object.
    *   The `getRequestParameterMap()` is now guaranteed to return a String or null.

Since the commit message doesn't explicitly state a bug fix, but the code change suggests a potential bug related to type casting, it's reasonable to assume this change addresses a potential type-related issue.

Conclusion:

**Buggy**
"
spring-framework,12706.json,a7c736915a4c23b3e3a5eb17d0c6aea904b53bd1,"@@ -1,16 +1,15 @@
 	public DataBuffer encodeValue(ByteBuf byteBuf, DataBufferFactory bufferFactory,
 			ResolvableType valueType, @Nullable MimeType mimeType, @Nullable Map<String, Object> hints) {
 
-		DataBuffer dataBuffer;
-		if (bufferFactory instanceof NettyDataBufferFactory) {
-			dataBuffer = ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
-		} else {
-			dataBuffer = bufferFactory.wrap(byteBuf.nioBuffer());
-		}
-
 		if (logger.isDebugEnabled() && !Hints.isLoggingSuppressed(hints)) {
 			String logPrefix = Hints.getLogPrefix(hints);
-			logger.debug(logPrefix + ""Writing "" + dataBuffer.readableByteCount() + "" bytes"");
+			logger.debug(logPrefix + ""Writing "" + byteBuf.readableBytes() + "" bytes"");
 		}
-		return dataBuffer;
+		if (bufferFactory instanceof NettyDataBufferFactory) {
+			return ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
+		}
+		byte[] bytes = new byte[byteBuf.readableBytes()];
+		byteBuf.readBytes(bytes);
+		byteBuf.release();
+		return bufferFactory.wrap(bytes);
 	}",NotBuggy,"Polishing and completing contribution

See gh-24866
",Buggy,"The commit message indicates polishing and completing a contribution, referencing issue gh-24866. The code diff modifies the `encodeValue` method in a class related to data buffer handling.

Here's a breakdown of the changes:

1. **DataBuffer Creation:** The original code created a `DataBuffer` instance early on, based on whether the `bufferFactory` was a `NettyDataBufferFactory` or not. The modified code delays the creation of the `DataBuffer` until after the debug logging.

2. **Logging:** The logging statement was modified to use `byteBuf.readableBytes()` instead of `dataBuffer.readableByteCount()`. This makes sense because the `DataBuffer` isn't created until later.

3. **NettyDataBufferFactory Handling:** The handling of `NettyDataBufferFactory` remains the same, wrapping the `byteBuf`.

4. **Generic DataBufferFactory Handling:** The original code used `bufferFactory.wrap(byteBuf.nioBuffer())` for non-Netty factories. The modified code now reads all bytes from `byteBuf` into a `byte[]`, releases the `byteBuf`, and then wraps the byte array using `bufferFactory.wrap(bytes)`. This is a significant change. The original code directly wrapped the `ByteBuffer`, while the new code copies the data into a byte array. The `byteBuf.release()` call suggests that the `ByteBuf` is reference-counted and needs to be released to avoid memory leaks.

The changes suggest a bug fix related to how `ByteBuf` is handled when a `NettyDataBufferFactory` is not used. The original code might have had issues with the lifecycle of the `ByteBuf` or the way the `ByteBuffer` was being wrapped. The explicit copying of the data into a byte array and releasing the `ByteBuf` indicates an attempt to manage resources correctly and prevent potential memory leaks or data corruption.

**Buggy**
"
spring-framework,23230.json,9d6c38bd54fe8931ee73fbb7a076625deb934e28,"@@ -1,3 +1,3 @@
 	public boolean isCompilable() {
-		return this.exitTypeDescriptor != null;
+		return (this.exitTypeDescriptor != null);
 	}",NotBuggy,"Consistent bracket alignment
",NotBuggy,"The commit message ""Consistent bracket alignment"" suggests a stylistic change related to code formatting, specifically the alignment of brackets. The diff shows the addition of parentheses around the expression `this.exitTypeDescriptor != null`. This change doesn't introduce new logic, fix errors, or improve exception handling. It's purely a formatting adjustment to improve code readability and consistency.

**NotBuggy**"
spring-framework,25172.json,e881d4b1441b4ade35806b6f06d5b7ebe1828e92,"@@ -1,4 +1,3 @@
-	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include serializationInclusion) {
-		this.serializationInclusion = serializationInclusion;
-		return this;
+	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include inclusion) {
+		return serializationInclusion(JsonInclude.Value.construct(inclusion, inclusion));
 	}",NotBuggy,"Allow use of JsonInclude.Value

Closes gh-24824
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** ""Allow use of JsonInclude.Value"" suggests the change is about enabling a new feature or expanding the functionality related to `JsonInclude.Value`. The ""Closes gh-24824"" part indicates that this commit addresses a specific issue reported on GitHub (issue #24824).

2.  **Code Diff:**
    *   The original code had a method `serializationInclusion(JsonInclude.Include serializationInclusion)`. This method directly accepted a `JsonInclude.Include` enum value.
    *   The modified code replaces the direct acceptance of `JsonInclude.Include` with a call to `serializationInclusion(JsonInclude.Value.construct(inclusion, inclusion))`. This suggests that the method now accepts a `JsonInclude.Value` object instead. The `JsonInclude.Value.construct(inclusion, inclusion)` part indicates that a `JsonInclude.Value` is being constructed from the provided `JsonInclude.Include` value.

3.  **Analysis:**
    *   The commit message and the code diff align. The change is about allowing the use of `JsonInclude.Value`. The code modification demonstrates how a `JsonInclude.Value` can be constructed from a `JsonInclude.Include` value.
    *   The change doesn't seem to directly address a bug. It's more about providing a new way to configure serialization inclusion, likely to offer more fine-grained control or flexibility.
    *   There is no error handling, logical correction, or exception handling improvement in the diff.

**Conclusion:**

NotBuggy
"
spring-framework,470.json,b5e5e3307838359b42c7f4b1ed592b03b249eb4f,"@@ -1,3 +1,3 @@
 	public TransactionContext getParent() {
-		return parent;
+		return this.parent;
 	}",NotBuggy,"Remove legacy config options from AbstractReactiveTransactionManager

Includes general revision of reactive transaction sources.

See gh-22646
",NotBuggy,"The commit message ""Remove legacy config options from AbstractReactiveTransactionManager"" indicates a cleanup and refactoring effort, potentially removing outdated configuration settings. The provided diff shows a minor change: replacing `parent` with `this.parent` within the `getParent()` method. This change doesn't seem directly related to removing legacy config options or fixing a bug. It's more likely a stylistic or code clarity improvement. The change doesn't involve error handling, logical corrections, or exception handling.

**NotBuggy**
"
spring-framework,18397.json,b0790bf5e7b6091552ccf17a2e707ab438d129d7,"@@ -1,3 +1,5 @@
-	protected final void copyModelParameters(Map model, Transformer transformer) {
-		copyMapEntriesToTransformerParameters(model, transformer);
+	protected final void copyModelParameters(Map<String, Object> model, Transformer transformer) {
+		for (Map.Entry<String, Object> entry : model.entrySet()) {
+			transformer.setParameter(entry.getKey(), entry.getValue());
+		}
 	}",NotBuggy,"Java 5 code style
",NotBuggy,"The commit message ""Java 5 code style"" suggests a change in coding style to align with Java 5 standards. The provided code diff shows a change in the `copyModelParameters` method. Specifically, it changes the `Map` type from raw `Map` to a parameterized `Map<String, Object>`. It also replaces the call to `copyMapEntriesToTransformerParameters` with an explicit loop that iterates through the map entries and sets the parameters on the transformer.

The change from raw `Map` to `Map<String, Object>` and the explicit loop are consistent with adopting Java 5 coding style. The code is more type-safe and readable. There is no indication of a bug fix in this change. The code is simply updated to use Java 5 features.

**NotBuggy**"
spring-framework,27849.json,d0c8545d9bf4b97c47d257b445cfe0a2ed09db88,"@@ -1,3 +1,3 @@
 	public String getCookieName() {
-		return cookieName;
+		return this.cookieName;
 	}",NotBuggy,"updated setCookieMaxAge to match getCookieMaxAge return type (SPR-6070)
",NotBuggy,"The commit message indicates a change related to matching the return type of `setCookieMaxAge` with `getCookieMaxAge`. The provided diff, however, only shows a change in `getCookieName` to explicitly use `this.cookieName`. This change doesn't seem to be related to the commit message's intent of aligning `setCookieMaxAge` and `getCookieMaxAge`. It's a simple change to explicitly reference the member variable. It doesn't indicate any bug fix.

**NotBuggy**"
spring-framework,19387.json,f2b926467491098b09b38296b95d9543c97e22ad,"@@ -1,3 +1,3 @@
-		public int read(@NotNull byte[] b, int off, int len) throws IOException {
+		public int read(byte[] b, int off, int len) throws IOException {
 			return this.delegate.read(b, off, len);
 		}",NotBuggy,"Remove Jetbrains annotations
",NotBuggy,"The commit message ""Remove Jetbrains annotations"" indicates a cleanup operation to remove Jetbrains-specific annotations from the codebase. The provided diff shows the removal of the `@NotNull` annotation from the `read` method's parameter `b`. This change directly aligns with the commit message. There's no indication of a bug fix, error handling, or logical correction. The change is purely about removing an annotation.

**NotBuggy**"
spring-framework,402.json,8dabb3e6269ab18bb2256266a70d4c285bbb4ff5,"@@ -1,8 +1,8 @@
-	private DefaultReactiveTransactionStatus newTransactionStatus(
-			ReactiveTransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
+	private GenericReactiveTransaction newReactiveTransaction(
+			TransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
 			@Nullable Object transaction, boolean newTransaction, boolean debug, @Nullable Object suspendedResources) {
 
-		return new DefaultReactiveTransactionStatus(transaction, newTransaction,
+		return new GenericReactiveTransaction(transaction, newTransaction,
 				!synchronizationManager.isSynchronizationActive(),
 				definition.isReadOnly(), debug, suspendedResources);
 	}",NotBuggy,"Shorter class names for common reactive transaction API types

Introduces TransactionExecution base interface for TransactionStatus as well as ReactiveTransaction. Renames getTransaction method to getReactiveTransaction, allowing for combined implementations of PlatformTransactionManager and ReactiveTransactionManager.

See gh-22646
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message focuses on refactoring and renaming related to reactive transaction management. The key points are:

*   **Shorter class names:** This suggests a change in naming conventions for better readability or conciseness.
*   **`TransactionExecution` interface:** Introduction of a base interface for `TransactionStatus` and `ReactiveTransaction` indicates a structural change, likely to promote code reuse or abstraction.
*   **Renaming `getTransaction` to `getReactiveTransaction`:** This is a clear API change, likely to avoid naming conflicts or to clarify the method's purpose within a reactive context.
*   **Combined implementations:** The message mentions allowing combined implementations of `PlatformTransactionManager` and `ReactiveTransactionManager`, suggesting a unification or simplification of transaction management strategies.
*   **gh-22646:** References an issue, which could potentially contain more context about the changes.

**Git Diff Analysis:**

The diff shows a change in the method `newTransactionStatus` to `newReactiveTransaction`, and a change from `DefaultReactiveTransactionStatus` to `GenericReactiveTransaction`.

*   **`newTransactionStatus` to `newReactiveTransaction`:** This renaming aligns with the commit message's intention to clarify the reactive nature of the transaction.
*   **`DefaultReactiveTransactionStatus` to `GenericReactiveTransaction`:** This change suggests a move from a specific implementation to a more general one, possibly related to the introduction of the `TransactionExecution` interface.

**Reasoning:**

The changes described in the commit message and reflected in the diff are primarily focused on refactoring, renaming, and introducing a new interface to improve the structure and clarity of the reactive transaction management code. There's no explicit mention of fixing a bug, nor do the code changes directly suggest a bug fix. The changes are more about improving the design and API of the transaction management system.

**Conclusion:**

NotBuggy
"
spring-framework,6430.json,d93303c0089d311f2b014f45f1b345ca7ab9cb1f,"@@ -1,20 +1,21 @@
 		public void processGroupImports() {
 			for (DeferredImportSelectorGrouping grouping : this.groupings.values()) {
+				Predicate<String> candidateFilter = grouping.getCandidateFilter();
 				grouping.getImports().forEach(entry -> {
-					ConfigurationClass configurationClass = this.configurationClasses.get(
-							entry.getMetadata());
+					ConfigurationClass configurationClass = this.configurationClasses.get(entry.getMetadata());
 					try {
-						processImports(configurationClass, asSourceClass(configurationClass),
-								asSourceClasses(entry.getImportClassName()), false);
+						processImports(configurationClass, asSourceClass(configurationClass, candidateFilter),
+								Collections.singleton(asSourceClass(entry.getImportClassName(), candidateFilter)),
+								candidateFilter, false);
 					}
 					catch (BeanDefinitionStoreException ex) {
 						throw ex;
 					}
 					catch (Throwable ex) {
 						throw new BeanDefinitionStoreException(
 								""Failed to process import candidates for configuration class ["" +
 										configurationClass.getMetadata().getClassName() + ""]"", ex);
 					}
 				});
 			}
 		}",NotBuggy,"ImportSelector.getCandidateFilter() for transitive filtering of classes

Closes gh-24175
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""ImportSelector.getCandidateFilter() for transitive filtering of classes"" and ""Closes gh-24175"" suggests that the change is related to filtering classes during the import selection process, specifically in a transitive manner. The reference to ""gh-24175"" indicates that this commit likely addresses a reported issue (bug or feature request) on GitHub.

**Git Diff Analysis:**

The diff shows modifications within the `processGroupImports()` method. Here's a breakdown:

1.  **`Predicate<String> candidateFilter = grouping.getCandidateFilter();`**: A `candidateFilter` is obtained from the `grouping` object. This filter is likely used to determine which classes are considered as candidates for import.
2.  **`processImports(configurationClass, asSourceClass(configurationClass, candidateFilter), Collections.singleton(asSourceClass(entry.getImportClassName(), candidateFilter)), candidateFilter, false);`**: The `processImports` method call is modified to include the `candidateFilter`. The `asSourceClass` method, which likely converts class names to `SourceClass` objects, now also utilizes the `candidateFilter`. The `candidateFilter` is also passed directly to `processImports`.

**Reasoning:**

The introduction of `candidateFilter` and its usage in `asSourceClass` and `processImports` strongly suggests that the change is related to filtering classes during the import process. The commit message mentions ""transitive filtering,"" which implies that the filtering applies not only to the initially imported classes but also to classes imported by those classes.

The original code likely lacked a mechanism to filter classes during transitive imports, potentially leading to unwanted or incorrect classes being imported. The addition of the `candidateFilter` addresses this by providing a way to selectively include classes based on certain criteria.

The fact that a `candidateFilter` is now being used, and that it's related to a GitHub issue, indicates that the previous behavior was likely not the intended behavior, and thus can be considered a bug. The change is correcting the import process to properly filter classes.

**Conclusion:**

**Buggy**
"
spring-framework,42.json,a25e24f37ee46595ce30045c6fb9efdb692b7386,"@@ -1,3 +1,6 @@
-		public Object getResult() {
+		public T getResult() {
+			if (this.exception != null) {
+				ReflectionUtils.rethrowRuntimeException(this.exception);
+			}
 			return this.result;
 		}",NotBuggy,"generified TransactionCallback; WebSphereUowTransactionManager preserves original exception in case of rollback (SPR-5270)
",Buggy,"The commit message indicates two changes: generification of `TransactionCallback` and preservation of the original exception in `WebSphereUowTransactionManager` during rollback. The provided code snippet focuses on the first part of the commit message, specifically the generification of `TransactionCallback`.

The code diff shows a change in the return type of the `getResult()` method from `Object` to `T`. This change directly relates to the generification of the `TransactionCallback` interface. The added code checks for an exception and rethrows it if it exists. This exception handling is related to preserving the original exception, as mentioned in the commit message. This suggests a bug fix related to exception handling during transaction rollback.

Therefore, the changes indicate a bug fix.

**Buggy**
"
spring-framework,2798.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,10 +1,10 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		if (this == other) {
 			return true;
 		}
 		if (!(other instanceof DefaultIntroductionAdvisor)) {
 			return false;
 		}
 		DefaultIntroductionAdvisor otherAdvisor = (DefaultIntroductionAdvisor) other;
 		return (this.advice.equals(otherAdvisor.advice) && this.interfaces.equals(otherAdvisor.interfaces));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message ""Annotate Object#equals parameter with @Nullable"" indicates that the change involves adding the `@Nullable` annotation to the `other` parameter of the `equals` method.

The diff confirms this: the `Object other` parameter in the `equals` method is annotated with `@Nullable`. This change suggests that the parameter `other` can be null, and the code should handle this case gracefully. While this annotation itself doesn't directly fix a bug, it clarifies the expected behavior and can prevent potential NullPointerExceptions if the `equals` method is called with a null argument without proper null checks. It's a defensive programming practice.

Given that the change is primarily an annotation and doesn't involve any explicit bug fix, but rather improves code clarity and prevents potential issues, I would classify this as not directly fixing a bug.

**NotBuggy**"
sonarqube,9746.json,1df148803610cd54f182b8636f01c0e6ece92b19,"@@ -1,3 +1,3 @@
-  public Date getUpdatedAt() {
+  public Long getUpdatedAt() {
     return updatedAt;
   }",NotBuggy,"Extract module sonar-db
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff.

**Commit Message:** ""Extract module sonar-db""

This commit message suggests a refactoring or modularization effort, specifically extracting a module named ""sonar-db"". It doesn't inherently imply a bug fix.

**Git Diff:**

```diff
-  public Date getUpdatedAt() {
+  public Long getUpdatedAt() {
     return updatedAt;
   }
```

The diff shows a change in the return type of the `getUpdatedAt()` method from `Date` to `Long`. This could be due to several reasons:

1.  **Data Type Change:** The underlying `updatedAt` field might have been changed from `Date` to `Long` (representing milliseconds since the epoch, for example). This is a common practice for storing timestamps.
2.  **API Simplification:** The module might be intended to expose timestamps as `Long` values for easier consumption by other modules.
3.  **Bug Fix:** It's *possible* that the original `Date` representation was causing issues (e.g., timezone problems, incorrect comparisons), and changing it to `Long` is a fix. However, without more context, this is less likely.

**Reasoning:**

The commit message focuses on modularization. While the data type change *could* be related to a bug fix, it's more likely a consequence of the module extraction and a decision about how to represent timestamps within the new module's API. The change itself doesn't scream ""bug fix"" in the same way that, say, adding a null check or fixing a calculation error would. It's a change in representation, which is common during refactoring.

**Conclusion:**

NotBuggy
"
sonarqube,10338.json,85b940dd935f57860f26dff7f1bfaa9de1ba8af2,"@@ -1,4 +1,4 @@
   public List<String> selectQProfileUuidsByOrganizationAndGroups(DbSession dbSession, OrganizationDto organization, Collection<GroupDto> groups) {
-    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getId).collect(toList()),
+    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getUuid).collect(toList()),
       g -> mapper(dbSession).selectQProfileUuidsByOrganizationAndGroups(organization.getUuid(), g));
   }",NotBuggy,"SONAR-13221 change PK of GROUPS and update FKs
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-13221 change PK of GROUPS and update FKs"" suggests a schema change related to the `GROUPS` table. Specifically, it mentions changing the primary key (PK) and updating foreign keys (FKs) that reference it. This usually happens when the original primary key was not suitable (e.g., an auto-incrementing integer that could lead to issues in distributed systems) or when a more appropriate unique identifier becomes available.

**Git Diff Analysis:**

The diff shows a change in the `selectQProfileUuidsByOrganizationAndGroups` method. The code is modified from `groups.stream().map(GroupDto::getId).collect(toList())` to `groups.stream().map(GroupDto::getUuid).collect(toList())`. This indicates that the code was previously using the `id` field of the `GroupDto` object, and now it's using the `uuid` field.

**Reasoning:**

Given the commit message and the code change, it's highly likely that the `id` field was the old primary key of the `GROUPS` table, and the `uuid` field is the new primary key. The code change reflects the update of foreign keys to now reference the `uuid` column instead of the `id` column.

The fact that the code was changed to use `uuid` instead of `id` suggests that the previous code was incorrect or would become incorrect after the database schema change. This implies a bug fix. The original code was relying on the old primary key, which is no longer valid or preferred.

**Conclusion:**

**Buggy**
"
sonarqube,18294.json,74c4ac9a4a5c9df13116862dd5dca982cc3f84bd,"@@ -1,13 +1,13 @@
   public boolean equals(Object obj) {
     if (!(obj instanceof Rule)) {
       return false;
     }
     if (this == obj) {
       return true;
     }
     Rule other = (Rule) obj;
     return new EqualsBuilder()
-        .append(pluginName, other.getPluginName())
+        .append(pluginName, other.getRepositoryKey())
         .append(key, other.getKey())
         .isEquals();
   }",NotBuggy,"Deprecate Rule.setPluginName() , Rule.getPluginName() and related stuff, because was replaced by repositoryKey
",NotBuggy,"Okay, let's analyze this Git diff and commit message.

**Commit Message Analysis:**

The commit message states that the methods `Rule.setPluginName()`, `Rule.getPluginName()`, and related functionality are being deprecated because they have been replaced by `repositoryKey`. This suggests a refactoring or renaming of a concept within the codebase.

**Git Diff Analysis:**

The diff shows a change in the `equals()` method of the `Rule` class. Specifically, it replaces `other.getPluginName()` with `other.getRepositoryKey()` in the `EqualsBuilder`. This aligns perfectly with the commit message's intention to replace `pluginName` with `repositoryKey`.

**Reasoning:**

The commit message and the code changes are highly relevant. The code modification directly reflects the deprecation and replacement mentioned in the commit message. The change in the `equals()` method ensures that the comparison of `Rule` objects now uses the new `repositoryKey` instead of the deprecated `pluginName`. This indicates a refactoring rather than a bug fix.

**Conclusion:**

NotBuggy
"
sonarqube,14645.json,f6b2a269c44043f1c230b646dbb8662aed3fa5a4,"@@ -1,15 +1,15 @@
     private int computeValueForMetric(NewCoverageCounter counter, Metric metric) {
-      if (metric.getKey().equals(outputMetricKeys.getNewLinesToCover())) {
+      if (metric.getKey().equals(NEW_LINES_TO_COVER_KEY)) {
         return counter.getNewLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredLines())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_LINES_KEY)) {
         return counter.getNewLines() - counter.getNewCoveredLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewConditionsToCover())) {
+      if (metric.getKey().equals(NEW_CONDITIONS_TO_COVER_KEY)) {
         return counter.getNewConditions();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredConditions())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_CONDITIONS_KEY)) {
         return counter.getNewConditions() - counter.getNewCoveredConditions();
       }
       throw new IllegalArgumentException(""Unsupported metric "" + metric.getKey());
     }",NotBuggy,"SONAR-11795 Drop persistit and move coverage measures computation to CE side
",NotBuggy,"The commit message indicates a refactoring effort, specifically dropping `persistit` and moving coverage measures computation to the CE (Compute Engine) side. The provided code diff shows changes in how metric values are computed based on the `NewCoverageCounter`. The changes involve replacing direct field access like `outputMetricKeys.getNewLinesToCover()` with constant keys like `NEW_LINES_TO_COVER_KEY`. This suggests a shift towards using predefined constants instead of dynamic lookups, which aligns with refactoring and potentially improving performance or maintainability. There's no explicit error handling, logical correction, or exception handling improvement in the diff. The changes seem to be part of a larger refactoring effort as described in the commit message.

**NotBuggy**"
sonarqube,2497.json,184dbe792ce14f192f75b13507c30a4b697d5852,"@@ -1,3 +1,3 @@
   private static Predicate<QProfileDto> byName(SearchWsRequest request) {
-    return p -> request.getProfileName() == null || Objects.equals(p.getName(), request.getProfileName());
+    return p -> request.getQualityProfile() == null || Objects.equals(p.getName(), request.getQualityProfile());
   }",NotBuggy,"SONAR-9865 Sanitize WS api/qualityprofiles/*

- functional key of a quality profile is: name, language and organization
- quality profile name is named 'name'
- quality profile key is named 'key'
- quality profile key parameter is deprecated when appropriate
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-9865 Sanitize WS api/qualityprofiles/*"" suggests a cleanup or sanitization of the `api/qualityprofiles/*` web service endpoints. The message highlights the following:

*   **Functional Key Definition:** Clarifies that the functional key of a quality profile consists of its name, language, and organization.
*   **Naming Conventions:** States that the quality profile's name is referred to as 'name' and its key as 'key'.
*   **Key Parameter Deprecation:** Indicates that the 'key' parameter is being deprecated where appropriate.

**Git Diff Analysis:**

The provided Git diff shows a change within the `byName` method:

```diff
--- a/file.java
+++ b/file.java
@@ -1,3 +1,3 @@
   private static Predicate<QProfileDto> byName(SearchWsRequest request) {
-    return p -> request.getProfileName() == null || Objects.equals(p.getName(), request.getProfileName());
+    return p -> request.getQualityProfile() == null || Objects.equals(p.getName(), request.getQualityProfile());
   }
```

The change replaces `request.getProfileName()` with `request.getQualityProfile()` in the predicate. This suggests a renaming or refactoring of the parameter used to search for quality profiles by name.

**Reasoning:**

The commit message indicates a general cleanup and standardization of the quality profile API. The diff shows a change in the parameter name used to filter quality profiles. This could be part of the sanitization process, ensuring consistent naming conventions and potentially deprecating older parameters. The change from `getProfileName` to `getQualityProfile` seems like a refactoring to align with the intended naming convention as described in the commit message. It's plausible that `getProfileName` was a source of confusion or inconsistency, and the change aims to address that. While it's possible that `getProfileName` was not working as expected, the change itself doesn't scream ""bug fix"" in the traditional sense of correcting a functional error. It's more about improving the API's clarity and consistency.

**Conclusion:**

NotBuggy
"
sonarqube,4754.json,0f0c3b73cc423fe90d6f4b97d6e73fe7bcdc485d,"@@ -1,5 +1,5 @@
   private static <PARAM> PARAM getNonNullValue(LiveMeasureDto measure, Function<LiveMeasureDto, PARAM> function) {
     PARAM value = function.apply(measure);
-    checkState(value != null, ""Measure not found"");
+    checkState(value != null, ""Measure has not been found"");
     return value;
   }",NotBuggy,"SONAR-10713 Badges can only be used on projects, long living branches and applications
",NotBuggy,"The commit message ""SONAR-10713 Badges can only be used on projects, long living branches and applications"" indicates a feature enhancement or restriction related to where badges can be used within the SonarQube platform. The provided code diff changes the error message from ""Measure not found"" to ""Measure has not been found"". This change is purely textual and doesn't seem to relate to the commit message's intent of restricting badge usage. It's likely a minor correction or clarification of an existing message. There's no indication of a bug fix in the traditional sense (e.g., correcting a calculation, handling an exception, or fixing a logical error).

**NotBuggy**"
sonarqube,21891.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,4 +1,4 @@
-  public SelectWsRequest setProjectKey(String projectKey) {
+  public RemoveGroupRequest setProjectKey(String projectKey) {
     this.projectKey = projectKey;
     return this;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",Buggy,"The commit message ""Auto-generate sonar-ws for integration tests"" indicates a change related to automatically generating `sonar-ws` (likely web services related to SonarQube) for integration tests.

The provided diff shows a change in the `setProjectKey` method. Specifically, the return type of the method has changed from `SelectWsRequest` to `RemoveGroupRequest`. This change doesn't seem directly related to auto-generating anything. It looks more like a refactoring or a correction of a type mismatch. It's possible that the original return type was incorrect, which would constitute a bug.

Reasoning:

1. **Commit Message Context:** The commit message suggests changes related to automation for integration tests.
2. **Code Change Analysis:** The code modifies the return type of a method.
3. **Relevance Assessment:** The change in return type doesn't directly implement auto-generation. However, if the original return type was incorrect, it represents a bug fix.

Conclusion: **Buggy**
"
sonarqube,17860.json,a4f8bbcf6f4d44cd0b637eff93ee8a3e49906edb,"@@ -1,4 +1,4 @@
-    public NewRule setType(Type t) {
+    public NewRule setType(RuleType t) {
       this.type = t;
       return this;
     }",NotBuggy,"SONAR-7353 Move RuleType to API
",NotBuggy,"The commit message ""SONAR-7353 Move RuleType to API"" indicates a refactoring or API change related to the `RuleType`. The code diff shows that the `setType` method's parameter type has been changed from `Type` to `RuleType`. This change aligns with the commit message, suggesting that the `RuleType` is being moved or exposed as part of the API. There is no indication of bug fixing, error handling, or logical corrections in the provided diff.

**NotBuggy**"
sonarqube,16129.json,3821d2d07b3d299f44a643d77aec3d48fadf6c74,"@@ -1,10 +1,10 @@
   public static SecurityStandards fromSecurityStandards(Set<String> securityStandards) {
     Set<String> standards = securityStandards.stream()
       .filter(Objects::nonNull)
       .collect(MoreCollectors.toSet());
     Set<String> owaspTop10 = toOwaspTop10(standards);
     Set<String> cwe = toCwe(standards);
     Set<String> sansTop25 = toSansTop25(cwe);
-    Set<String> sq = toSonarSourceSecurityCategories(cwe);
+    Set<SQCategory> sq = toSQCategories(cwe);
     return new SecurityStandards(standards, cwe, owaspTop10, sansTop25, sq);
   }",NotBuggy,"SONAR-12717 use enum for SQ categories in SecurityStandards
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""SONAR-12717 use enum for SQ categories in SecurityStandards"" suggests a refactoring or enhancement rather than a bug fix. It indicates that the code is being modified to use an enum for representing ""SQ categories"" within the `SecurityStandards` class. This is likely done for better type safety, readability, and maintainability.

**Git Diff Analysis:**

The diff shows a change in the `fromSecurityStandards` method within what appears to be a `SecurityStandards` class.  The key change is:

- `Set<String> sq = toSonarSourceSecurityCategories(cwe);`  changed to `Set<SQCategory> sq = toSQCategories(cwe);`

This confirms the commit message's intent.  The type of the `sq` variable has changed from `Set<String>` to `Set<SQCategory>`.  This means that the `toSonarSourceSecurityCategories` method (likely renamed to `toSQCategories`) now returns a `Set` of an enum type `SQCategory` instead of a `Set` of strings.

**Reasoning:**

1.  **Refactoring, not bug fix:** The commit message explicitly states the intention to use an enum. This is a refactoring activity, aimed at improving the code's structure and type safety.
2.  **Type change confirms refactoring:** The diff confirms the change from `String` to `SQCategory`, solidifying the refactoring nature of the commit.
3.  **No error handling or logic fixes:** The diff doesn't show any changes related to error handling, boundary conditions, or logical corrections. It's purely a type change.

**Conclusion:**

The commit message and diff clearly indicate a refactoring to use an enum for representing SQ categories. There's no evidence of a bug fix.

**NotBuggy**
"
sonarqube,22005.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,3 +1,3 @@
-  public String getComponentId() {
-    return componentId;
+  public String getComponent() {
+    return component;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",NotBuggy,"The commit message ""Auto-generate sonar-ws for integration tests"" suggests changes related to automatically generating something for integration tests, likely related to SonarQube web services (sonar-ws).

The provided code diff is a simple renaming of a getter method from `getComponentId()` to `getComponent()`. This change doesn't seem directly related to auto-generating anything or integration tests. It's a simple refactoring or renaming. There's no error handling, logical correction, or exception handling involved. It's unlikely this change is a bug fix.

**NotBuggy**
"
sonarqube,16517.json,34cdbe891e9b45f42d35ee6deb2776cdfe50b431,"@@ -1,3 +1,3 @@
-  public Optional<Long> getDurationInMs() {
+  public Optional<Integer> getDurationInMs() {
     return Optional.ofNullable(durationInMs);
   }",NotBuggy,"SONAR-8353 persist deliveries in Compute Engine

Deliveries older than 1 month are purged.
",Buggy,"The commit message ""SONAR-8353 persist deliveries in Compute Engine"" indicates a feature related to persisting deliveries, likely within a Compute Engine environment. The description mentions purging deliveries older than 1 month.

The code diff changes the return type of `getDurationInMs()` from `Optional<Long>` to `Optional<Integer>`. This suggests a potential bug fix or at least a type correction. It's possible that the original code used `Long` unnecessarily, or that there was an overflow issue with `Long` that is now addressed by using `Integer`. It's also possible that the database column was changed from long to integer.

Given the change in return type, it's likely that this commit addresses a bug or an incorrect implementation.

**Buggy**
"
sonarqube,19146.json,6adb553ddc2711cb88632be9bd96b4e91c51082e,"@@ -1,3 +1,3 @@
   protected Logger newInstance(String name) {
-    return new ConsoleLogger(name);
+    return new ConsoleLogger();
   }",NotBuggy,"SONAR-5700 ability to change level of loggers of 3rd-party libraries
",NotBuggy,"The commit message indicates a change in logging configuration, specifically the ability to change the level of loggers for third-party libraries. The code diff shows a change in the `newInstance` method of a class, where a `ConsoleLogger` is instantiated. The original code `return new ConsoleLogger(name);` creates a new `ConsoleLogger` with a given name, while the modified code `return new ConsoleLogger();` creates a `ConsoleLogger` without a name.

Reasoning:
1. **Commit Message Context:** The commit message suggests changes related to logging configuration.
2. **Code Diff Analysis:** The code diff modifies the instantiation of `ConsoleLogger`. The original version takes a name as an argument, while the new version does not.
3. **Relevance Assessment:** The change in the `ConsoleLogger` instantiation could be related to the ability to change the level of loggers. Removing the name during instantiation might allow for a more generic configuration that applies to all loggers of a certain type, or it could be part of a larger change that allows for external configuration of the logger's level.
4. **Bug Fix Indication:** The change does not directly indicate a bug fix. It seems to be related to a new feature or enhancement that allows for more flexible logging configuration.

Conclusion: **NotBuggy**"
sonarqube,12862.json,1d8455866b14fc692bfebe2de64e4437438a5ce8,"@@ -1,4 +1,4 @@
-  public MetricDto setDomain(String domain) {
+  public MetricDto setDomain(@Nullable String domain) {
     this.domain = domain;
     return this;
   }",NotBuggy,"SONAR-8116 Return rating metrics in api/qualitygates/app
",NotBuggy,"The commit message ""SONAR-8116 Return rating metrics in api/qualitygates/app"" indicates a change related to returning rating metrics in the `api/qualitygates/app` endpoint. The code diff modifies the `setDomain` method in `MetricDto` to accept a nullable `domain` parameter. This change itself doesn't scream bug fix. It's more likely an enhancement or a change to accommodate cases where the domain might be null. There's no explicit error handling, logical correction, or exception handling improvement visible in the diff. The change seems to be related to allowing null values for the domain, which might be part of the feature described in the commit message, but it doesn't inherently fix a bug.

**NotBuggy**
"
sonarqube,3675.json,8de13aed2d978699d7f8d379f6af8610c173c29e,"@@ -1,4 +1,7 @@
   private Map<String, String> getKeysToDisplayMap(Set<String> keys) {
     return keys.stream()
-      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity()));
+      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity(),
+        (u, v) -> {
+          throw new IllegalArgumentException(format(""'%s' and '%s' cannot be used at the same time as they refer to the same setting"", u, v));
+        }));
   }",NotBuggy,"SONAR-7969 Fail when using key and deprecated in values WS

Display specific message when key and deprecated key are used at the same time
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""SONAR-7969 Fail when using key and deprecated in values WS"" and ""Display specific message when key and deprecated key are used at the same time"" clearly indicates a bug fix. The message suggests that the code previously allowed the use of both a key and its deprecated counterpart simultaneously, which is incorrect. The fix aims to prevent this and provide a specific error message.

2.  **Code Diff Analysis:** The code diff modifies the `getKeysToDisplayMap` method. Specifically, it changes how the `toMap` collector handles duplicate keys.

    *   **Original Code:** The original code used `Collectors.toMap(propertyDefinitions::validKey, Function.identity())`. This would likely throw an `IllegalStateException` if `propertyDefinitions::validKey` returned the same key for two different input keys in the `keys` set. This is because the default `toMap` collector doesn't handle duplicate keys.

    *   **Modified Code:** The modified code adds a merge function to the `toMap` collector: `(u, v) -> { throw new IllegalArgumentException(format(""'%s' and '%s' cannot be used at the same time as they refer to the same setting"", u, v)); }`. This merge function is invoked when the `toMap` collector encounters duplicate keys. Instead of silently overwriting or throwing a generic exception, it now throws an `IllegalArgumentException` with a specific error message indicating that the key and its deprecated version cannot be used together.

3.  **Relevance and Bug Fix Indication:** The code change directly addresses the issue described in the commit message. The original code likely failed in an unhelpful way (or potentially even silently overwrote data) when both a key and its deprecated version were used. The modified code now explicitly detects this condition and throws an exception with a clear error message. This prevents the incorrect usage and provides better feedback to the user. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
sonarqube,12954.json,e0ad2379d3e1f0e4c286570b5b86dd0a351589da,"@@ -1,4 +1,3 @@
   public void deleteGlobalProperty(String key, DbSession session) {
-    PropertiesMapper mapper = session.getMapper(PropertiesMapper.class);
-    mapper.deleteGlobalProperty(key);
+    getMapper(session).deleteGlobalProperty(key);
   }",NotBuggy,"SONAR-7676 some modernization of PropertiesDao and PropertiesDaoTest
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** ""SONAR-7676 some modernization of PropertiesDao and PropertiesDaoTest""

    *   The commit message suggests a general modernization effort, which could include code cleanup, refactoring, or performance improvements. It doesn't explicitly mention bug fixes.

2.  **Git Diff:**

    ```diff
    --- a/src/main/java/org/sonar/server/properties/PropertiesDao.java
    +++ b/src/main/java/org/sonar/server/properties/PropertiesDao.java
    @@ -1,4 +1,3 @@
      public void deleteGlobalProperty(String key, DbSession session) {
    -    PropertiesMapper mapper = session.getMapper(PropertiesMapper.class);
    -    mapper.deleteGlobalProperty(key);
    +    getMapper(session).deleteGlobalProperty(key);
      }
    ```

    *   The code change replaces the explicit creation of a `PropertiesMapper` instance with a call to `getMapper(session)`. This suggests a refactoring where the `getMapper` method likely handles the retrieval or creation of the mapper instance.
    *   The change simplifies the code by removing a local variable. This is a common modernization technique.
    *   There's no indication of error handling being added or modified, nor any logical corrections or exception handling improvements.

3.  **Relevance:**

    *   The code change aligns with the commit message's intention of modernization. It simplifies the code and potentially centralizes the mapper retrieval logic.
    *   There's no strong evidence to suggest that this change is related to fixing a bug. It appears to be a refactoring for better code structure or maintainability.

**Conclusion:**

The commit message and the code changes indicate a modernization effort rather than a bug fix. The code is simplified and potentially made more maintainable, but there's no indication of any bug being addressed.

**NotBuggy**"
voldemort,6120.json,45f14abf745371e4920d9c4d7ea0164ddaa2b9e5,"@@ -1,4 +1,9 @@
     public static voldemort.client.protocol.pb.VProto.VoldemortRequest parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeDelimitedFrom(input).buildParsed();
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
     }",NotBuggy,"Updated Protobuff to 2.3
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Updated Protobuff to 2.3"" suggests a library upgrade. This doesn't inherently imply a bug fix, but it's possible the upgrade was done to address a bug in the older version of Protobuf.

**Code Diff Analysis:**

The code diff modifies the `parseDelimitedFrom` method.  The original code directly returned the result of `newBuilder().mergeDelimitedFrom(input).buildParsed()`. The updated code introduces a conditional check:

1.  A `Builder` is created.
2.  `builder.mergeDelimitedFrom(input)` is called, and its boolean return value is checked.
3.  If `mergeDelimitedFrom` returns `true` (indicating success), the builder's result is returned.
4.  If `mergeDelimitedFrom` returns `false` (indicating failure), `null` is returned.

**Reasoning:**

The key change is the introduction of the conditional check based on the return value of `mergeDelimitedFrom`.  The original code *assumed* that `mergeDelimitedFrom` would always succeed. The updated code now handles the case where `mergeDelimitedFrom` might fail (presumably due to a malformed input stream or other I/O error). Returning `null` in case of failure is a defensive programming technique to prevent potential `NullPointerException`s or other unexpected behavior downstream.

This change strongly suggests a bug fix. The original code likely crashed or produced incorrect results when `mergeDelimitedFrom` failed. The updated code provides a more robust and predictable behavior in such cases. The fact that the return value of `mergeDelimitedFrom` is now being checked indicates that the previous code was not handling a possible failure scenario.

**Conclusion:**

**Buggy**
"
voldemort,9648.json,b80351a00a6a6f3a0938aac86e57245badfe0daa,"@@ -1,8 +1,8 @@
     public void clearCount(int nodeId) {
         AtomicLong counter = values.get(nodeId);
         if(counter == null) {
             counter = new AtomicLong(0L);
-            values.putIfAbsent(nodeId, counter);
-        }
-        counter.set(0L);
+            values.put(nodeId, counter);
+        } else
+            counter.set(0L);
     }",NotBuggy,"Minor code cleanup
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Minor code cleanup"" suggests that the changes are primarily focused on improving code readability, maintainability, or style, rather than addressing a specific bug.

**Code Diff Analysis:**

The code diff modifies the `clearCount` method. Let's break down the changes:

-   **Original Code:**

    ```java
    if(counter == null) {
        counter = new AtomicLong(0L);
        values.putIfAbsent(nodeId, counter);
    }
    counter.set(0L);
    ```

-   **Modified Code:**

    ```java
    if(counter == null) {
        counter = new AtomicLong(0L);
        values.put(nodeId, counter);
    } else
        counter.set(0L);
    ```

The key change is replacing `values.putIfAbsent(nodeId, counter)` with `values.put(nodeId, counter)` and adding an `else` block.

**Reasoning:**

1.  **`putIfAbsent` vs. `put`:** The `putIfAbsent` method only puts the value into the map if the key is not already present. `put` will always put the value in the map, potentially overwriting an existing value.
2.  **Potential Bug:** The original code had a potential race condition. If multiple threads called `clearCount` concurrently with the same `nodeId` and `counter` was null, multiple `AtomicLong` instances could be created, but only one would be added to the `values` map due to `putIfAbsent`. The other `AtomicLong` instances would be lost. The `counter.set(0L)` call would then operate on the `AtomicLong` that was *not* in the map, leading to incorrect behavior.
3.  **Bug Fix Indication:** The modified code ensures that the `AtomicLong` instance is always placed in the `values` map, resolving the potential race condition. The `else` block ensures that the `counter.set(0L)` is only called if the counter is not null. This change addresses a potential concurrency issue, which qualifies as a bug fix.

**Conclusion:**

The code change addresses a potential race condition and ensures correct behavior in concurrent scenarios. This indicates a bug fix, even if the commit message only mentions ""Minor code cleanup.""

**Buggy**
"
voldemort,938.json,2150275cb31c85a0a8d56744abd030c96f8ba804,"@@ -1,29 +1,28 @@
         public void pruneJob(int nodeId, String store) {
             logger.info(""Kicking off prune job on Node "" + nodeId + "" for store "" + store);
             VAdminProto.PruneJobRequest.Builder jobRequest = VAdminProto.PruneJobRequest.newBuilder()
                                                                                         .setStoreName(store);
 
             VAdminProto.VoldemortAdminRequest adminRequest = VAdminProto.VoldemortAdminRequest.newBuilder()
                                                                                               .setPruneJob(jobRequest)
                                                                                               .setType(VAdminProto.AdminRequestType.PRUNE_JOB)
                                                                                               .build();
-            // FIXME VC probably need a helper to do all this, at some point..
-            // All
+            // TODO probably need a helper to do all this, at some point.. all
             // of this file has repeated code
             Node node = AdminClient.this.getAdminClientCluster().getNodeById(nodeId);
             SocketDestination destination = new SocketDestination(node.getHost(),
                                                                   node.getAdminPort(),
                                                                   RequestFormatType.ADMIN_PROTOCOL_BUFFERS);
             SocketAndStreams sands = socketPool.checkout(destination);
 
             try {
                 DataOutputStream outputStream = sands.getOutputStream();
                 ProtoUtils.writeMessage(outputStream, adminRequest);
                 outputStream.flush();
             } catch(IOException e) {
                 helperOps.close(sands.getSocket());
                 throw new VoldemortException(e);
             } finally {
                 socketPool.checkin(destination, sands);
             }
         }",NotBuggy,"Adding test for Prune job
",NotBuggy,"The commit message ""Adding test for Prune job"" indicates that the commit is adding a test case related to the Prune job functionality.

The provided code diff doesn't seem to be adding a test case. Instead, it shows modifications within the `pruneJob` method. The changes include:

1.  A comment change from `// FIXME` to `// TODO`. This suggests a change in the type of comment, from indicating a bug that needs fixing to a task that needs to be done.

The code diff doesn't add any test-related code or files. The change in the comment from `FIXME` to `TODO` might indicate a previous bug or issue that was acknowledged but not directly fixed in this commit. The commit message doesn't align with the code changes.

**NotBuggy**
"
voldemort,4195.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public int getRoStoreVersionsCount() {
-        return result.getRoStoreVersionsCount();
+      public voldemort.client.protocol.pb.VAdminProto.ROMetadataMap getRoStoreMetadata(int index) {
+        return result.getRoStoreMetadata(index);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message ""Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options"" indicates ongoing work related to rebalancing functionality, with rollback options yet to be implemented.

The code diff shows a change in the `getRoStoreVersionsCount()` method to `getRoStoreMetadata(int index)`. The original method returns the count of read-only store versions, while the modified method returns the read-only store metadata at a given index. This change doesn't explicitly indicate a bug fix. It appears to be a modification of the API to retrieve metadata instead of just the count. The commit message doesn't mention any bug being fixed by this change.

**NotBuggy**
"
voldemort,7783.json,94be1a5451f7c763e84401ce4eb6a9e58b27b91b,"@@ -1,11 +1,10 @@
-    private void writeResults(DataOutputStream outputStream, List<Versioned<byte[]>> values)
-            throws IOException {
-        outputStream.writeInt(values.size());
-        for(Versioned<byte[]> v: values) {
-            byte[] clock = ((VectorClock) v.getVersion()).toBytes();
-            byte[] value = v.getValue();
-            outputStream.writeInt(clock.length + value.length);
+    public void writeResponse(DataOutputStream outputStream) throws IOException {
+        outputStream.writeShort(0);
+        outputStream.writeInt(results.size());
+        for(Version v: results) {
+            byte[] clock = ((VectorClock) v).toBytes();
+
+            outputStream.writeInt(clock.length);
             outputStream.write(clock);
-            outputStream.write(value);
         }
     }",NotBuggy,"ShareBuffer Refactoring

Refactored the Shared Buffer code to eliminate the separate read and
write buffers. Now a common buffer is used and the code is refactored
into its own classes. running the unit test.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""ShareBuffer Refactoring"" suggests a code reorganization and simplification. The key points are:

*   Elimination of separate read and write buffers.
*   Use of a common buffer.
*   Refactoring into new classes.
*   Unit tests are running (implying functionality is maintained).

**Git Diff Analysis:**

The diff shows a change in the `writeResults` method, renamed to `writeResponse`. Let's break down the changes:

1.  `outputStream.writeInt(values.size());` changed to `outputStream.writeInt(results.size());`: This seems like a variable rename, likely due to the refactoring.
2.  The loop iterates through `Versioned<byte[]>` values and extracts the clock and value. The original code wrote the combined length of the clock and value, then wrote the clock and value. The new code writes a short (0), the size of the results, then iterates through `Version` objects, writing the length of the clock and the clock itself. The value is no longer written.

**Reasoning:**

The commit message indicates a refactoring, which is supported by the code changes. The change in the loop, where the value is no longer written, is suspicious. This could be a bug if the value is supposed to be written. The commit message doesn't explicitly mention fixing a bug, but the removal of writing the value suggests a potential data loss issue, which could be considered a bug.

**Conclusion:**

**Buggy**
"
voldemort,3071.json,a4f42bf9d1d1b83d91d8f562019eb1f9e225460d,"@@ -1,4 +1,5 @@
       public Builder clearStoreName() {
-        result.storeName_ = java.util.Collections.emptyList();
+        result.hasStoreName = false;
+        result.storeName_ = getDefaultInstance().getStoreName();
         return this;
       }",NotBuggy,"Removed the new cleaner way to get read-only metadata. Fall back to the old dirty multiple message format to maintain backwards compatibility...Sigh!
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message explicitly states a removal of a ""new cleaner way"" and a fallback to an ""old dirty"" method for backwards compatibility. The ""Sigh!"" suggests this is a reluctant change, likely forced by an issue with the new approach.

2.  **Diff Analysis:** The code diff shows a change in the `clearStoreName()` method. Instead of using `java.util.Collections.emptyList()`, it now sets `hasStoreName` to `false` and assigns the default instance's store name. This change suggests a reversion to a previous state.

3.  **Relevance:** The commit message and diff are strongly related. The message describes reverting to an older method, and the diff shows a specific code change that accomplishes this reversion in the `clearStoreName()` method. The new method likely introduced a bug or incompatibility, forcing the fallback.

4.  **Bug Fix Indication:** The commit message strongly implies that the ""new cleaner way"" had a problem. Reverting to the old method is a common strategy to fix a bug introduced by a newer feature or optimization. The ""backwards compatibility"" mention further supports the idea that the new method broke existing functionality.

**Conclusion:**

**Buggy**
"
voldemort,978.json,0cc38b8ce750c9940eefe9b3274c4fb2f9e5437e,"@@ -1,28 +1,28 @@
     public static void validateReadOnlyStores(Cluster cluster,
                                               List<StoreDefinition> storeDefs,
                                               AdminClient adminClient) {
         List<StoreDefinition> readOnlyStores = filterStores(storeDefs, true);
 
         if(readOnlyStores.size() == 0) {
             // No read-only stores
             return;
         }
 
-        List<String> storeNames = RebalanceUtils.getStoreNames(readOnlyStores);
+        List<String> storeNames = getStoreNames(readOnlyStores);
         for(Node node: cluster.getNodes()) {
             if(node.getNumberOfPartitions() != 0) {
                 for(Entry<String, String> storeToStorageFormat: adminClient.getROStorageFormat(node.getId(),
                                                                                                storeNames)
                                                                            .entrySet()) {
                     if(storeToStorageFormat.getValue()
                                            .compareTo(ReadOnlyStorageFormat.READONLY_V2.getCode()) != 0) {
                         throw new VoldemortRebalancingException(""Cannot rebalance since node ""
                                                                 + node.getId() + "" has store ""
                                                                 + storeToStorageFormat.getKey()
                                                                 + "" not using format ""
                                                                 + ReadOnlyStorageFormat.READONLY_V2);
                     }
                 }
             }
         }
     }",NotBuggy,"a) Add metadata file for fetch files in case of new node
b) Moved down ""show-plan"" option to get better idea of what is happening
",Buggy,"The commit message indicates two changes:
1.  Adding a metadata file for fetching files in the case of a new node.
2.  Moving down the ""show-plan"" option for better understanding.

The provided code diff only shows a change in the `validateReadOnlyStores` method. Specifically, `RebalanceUtils.getStoreNames(readOnlyStores)` is changed to `getStoreNames(readOnlyStores)`. This suggests that the `getStoreNames` method was moved to the current class or is now directly accessible.

Reasoning:

*   The code change doesn't directly relate to adding a metadata file or moving the ""show-plan"" option. The change is a simple method call update, likely a refactoring or code cleanup.
*   The code change involves validating read-only stores and their storage format. If the storage format is not `READONLY_V2`, a `VoldemortRebalancingException` is thrown. This exception handling suggests a potential bug fix or preventative measure against incorrect storage formats during rebalancing. The original code might have been using an incorrect method to get the store names, which could lead to incorrect validation.

Conclusion: **Buggy**"
voldemort,8068.json,97d3f70279da869411b1d6c6344d5fe8b164c891,"@@ -1,9 +1,9 @@
     public int getAvailableNodeCount() {
         int available = 0;
 
         for(NodeStatus nodeStatus: nodeStatusMap.values())
-            if(nodeStatus.isAvailable())
+            if(isAvailable(nodeStatus))
                 available++;
 
         return available;
     }",NotBuggy,"Work on implementation-specific unit tests for FailureDetectors. Also reintroduced usage of Time to allow for easier unit testing.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states two main goals:

1.  ""Work on implementation-specific unit tests for FailureDetectors."" This suggests the commit primarily focuses on improving the test suite, which doesn't directly imply a bug fix in the production code.
2.  ""Also reintroduced usage of Time to allow for easier unit testing."" This indicates a change to facilitate testing, likely by making the code more deterministic or controllable during tests. This, again, doesn't directly point to a bug fix in the main logic.

**Git Diff Analysis:**

The diff shows a change in the `getAvailableNodeCount()` method:

*   **Original Code:**

    ```java
    if(nodeStatus.isAvailable())
    ```

*   **Modified Code:**

    ```java
    if(isAvailable(nodeStatus))
    ```

The change replaces a direct call to `nodeStatus.isAvailable()` with a call to `isAvailable(nodeStatus)`. This suggests that `isAvailable` is a method within the same class as `getAvailableNodeCount`.

**Reasoning:**

The change from `nodeStatus.isAvailable()` to `isAvailable(nodeStatus)` indicates a refactoring or a change in how availability is determined. It's possible that the original direct call was problematic in some way, or that the new `isAvailable(NodeStatus)` method encapsulates additional logic or checks related to availability.

Given the commit message mentioning unit testing and the change in the `getAvailableNodeCount` method, it's plausible that the original `nodeStatus.isAvailable()` method was difficult to test or didn't accurately reflect the intended availability logic under certain conditions. The introduction of the `isAvailable(NodeStatus)` method could be an attempt to address these testing challenges or to correct a subtle flaw in the availability determination.

However, without more context (e.g., the implementation of the `isAvailable(NodeStatus)` method), it's difficult to definitively say whether this change constitutes a bug fix. It could also be a refactoring for better testability or maintainability.

**Conclusion:**

Based on the information available, the change could be related to a bug fix, but it's not definitively clear. The commit message mentions unit testing, and the code change involves a potentially more robust or testable way to determine node availability.

**Buggy**
"
voldemort,577.json,4c276b83ff229e20956e3f25803a31d633727318,"@@ -1,7 +1,7 @@
     public int taskCount() {
         int numTasks = 0;
         for(RebalanceBatchPlan batchPlan: batchPlans) {
-            numTasks += batchPlan.getBatchPlan().size();
+            numTasks += batchPlan.getTaskCount();
         }
         return numTasks;
     }",NotBuggy,"Added ""progress bar"" for rebalance batch plan

Added RebalanceBatchPlanProgressBar
- progress tracking object for each rebalancing batch
- integrated with RebalanceController, AsyncRebalanceTask, and
  (Donor|Stealer)BasedRebalanceTask

Did other general clean up of logging during rebalance to make it
appropriately verbose (more verbose in some places, less verbose in
others...)
",NotBuggy,"The commit message indicates the addition of a progress bar for rebalancing batch plans and general cleanup of logging. The provided diff shows a change in the `taskCount()` method, where `batchPlan.getBatchPlan().size()` is replaced with `batchPlan.getTaskCount()`.

Reasoning:
1.  **Relevance to Commit Message:** The change in `taskCount()` is likely related to the introduction of the `RebalanceBatchPlanProgressBar`. The `getTaskCount()` method might be a new method introduced in the `RebalanceBatchPlan` class as part of the progress bar implementation, or it could be an existing method that provides a more accurate or efficient way to retrieve the task count.
2.  **Bug Fix Indication:** The change itself doesn't directly indicate a bug fix. It seems to be part of the new feature implementation. The original code might have been using the wrong method to get the task count, but without more context, it's difficult to say for sure.
3.  **Error Handling/Logic Correction:** There is no explicit error handling or logic correction in the provided diff.
4.  **Exception Handling:** No changes related to exception handling are present.
5.  **Other Signs:** The change is consistent with the feature addition described in the commit message.

Conclusion:
**NotBuggy**
"
voldemort,42.json,e011531aa5e8504081b92044bd4b1ecf7baf791d,"@@ -1,8 +1,9 @@
             public boolean accept(Path input) {
-                if(input.getName().startsWith(Integer.toString(partitionId) + ""_""
-                                              + Integer.toString(replicaType))) {
+                if(input.getName().matches(""^"" + Integer.toString(partitionId) + ""_""
+                                           + Integer.toString(replicaType) + ""_""
+                                           + Integer.toString(chunkId) + ""\\.data"")) {
                     return true;
                 } else {
                     return false;
                 }
             }",NotBuggy,"a) Changed back RO to use one reducer per chunk - Why? Finding diff at one chunk level is simpler than at chunk set level
b) Changed collision iterator to return key as well
c) Loads of unit tests to test other wrapper data file chunk set code
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Breakdown:**

*   **a) Changed back RO to use one reducer per chunk - Why? Finding diff at one chunk level is simpler than at chunk set level:** This suggests a change in the processing strategy for ""RO"" (likely some data processing component). The rationale indicates a simplification for finding differences, implying a potential issue or inefficiency in the previous approach.
*   **b) Changed collision iterator to return key as well:** This points to an enhancement or fix in how collision iterators function. Returning the key suggests that the previous implementation might have been missing crucial information during collision handling.
*   **c) Loads of unit tests to test other wrapper data file chunk set code:** This indicates increased test coverage, potentially to address or prevent issues in the ""wrapper data file chunk set code.""

**Git Diff Analysis:**

The diff focuses on a file acceptance filter. The original code checks if the input file name *starts with* a specific pattern: `partitionId_replicaType`. The modified code changes this to a more precise check using `matches` (regular expression matching) and a more specific pattern: `^partitionId_replicaType_chunkId\\.data`.

**Reasoning:**

1.  **Specificity of File Matching:** The original `startsWith` check is quite broad. If a file named ""1_2_somethingelse"" existed, it would be accepted even if it wasn't a valid data file. The new `matches` check, using a regular expression, enforces a stricter naming convention, ensuring that only files with the exact pattern `partitionId_replicaType_chunkId.data` are accepted. This strongly suggests a bug fix. The original code could have been incorrectly processing files that didn't conform to the expected naming scheme.
2.  **Alignment with Commit Message:** While the diff itself doesn't directly relate to the ""reducer per chunk"" or ""collision iterator"" parts of the commit message, it aligns with the overall theme of improving the robustness and correctness of data processing, especially related to chunk handling. The increased specificity in file filtering is a common type of bug fix.
3.  **Potential Error Handling:** The change from `startsWith` to `matches` combined with the specific regex pattern suggests that the previous implementation was prone to accepting incorrect files, potentially leading to errors or unexpected behavior during data processing. The new implementation adds a layer of validation, which is a common characteristic of bug fixes.

**Conclusion:**

The change in file acceptance logic, from a broad `startsWith` check to a more precise regular expression match, strongly indicates a bug fix. The original code was likely accepting files that didn't conform to the expected naming convention, potentially leading to errors. The commit message, while covering multiple areas, supports the idea of improving the correctness and robustness of data processing, which aligns with the observed code change.

**Buggy**
"
voldemort,3034.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public java.util.List<voldemort.client.protocol.pb.VAdminProto.ROStoreVersionDirMap> getRoStoreVersionsList() {
-        return java.util.Collections.unmodifiableList(result.roStoreVersions_);
+      public java.util.List<voldemort.client.protocol.pb.VAdminProto.RebalancePartitionInfoMap> getRebalancePartitionInfoListList() {
+        return java.util.Collections.unmodifiableList(result.rebalancePartitionInfoList_);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message indicates ongoing work related to rebalancing, specifically changing the rebalancing state. The diff shows a change in the return type of a method from `ROStoreVersionDirMap` to `RebalancePartitionInfoMap` and a corresponding change in the field name from `roStoreVersions_` to `rebalancePartitionInfoList_`. This suggests that the code is being modified to handle rebalancing partition information instead of read-only store version directories. The change aligns with the commit message's focus on rebalancing. However, there's no direct indication of a bug fix in this specific diff. It appears to be a feature enhancement or refactoring related to rebalancing functionality.

**NotBuggy**"
voldemort,10042.json,6f1cf577b021124cd3f377420c69299819ee3c40,"@@ -1,3 +1,5 @@
-        public List<Version> execute(Store<ByteArray, byte[]> store, ByteArray key) {
+        public List<Version> execute(Store<ByteArray, byte[], byte[]> store,
+                                     ByteArray key,
+                                     byte[] transforms) {
             return store.getVersions(key);
         }",NotBuggy,"Intermediate check-in for server-side transforms
Conflicts:

	contrib/mongodb/example/MongoDBClient.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageConfiguration.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageEngine.java
	contrib/mongodb/test/voldemort/store/mongodb/MongoDBStorageEngineTest.java
	src/java/voldemort/client/AbstractStoreClientFactory.java
	src/java/voldemort/client/SocketStoreClientFactory.java
	src/java/voldemort/server/StoreRepository.java
	src/java/voldemort/server/storage/StorageService.java
	src/java/voldemort/store/invalidmetadata/InvalidMetadataCheckingStore.java
	src/java/voldemort/store/readonly/ReadOnlyStorageEngine.java
	src/java/voldemort/store/rebalancing/RebootstrappingStore.java
	src/java/voldemort/store/rebalancing/RedirectingStore.java
	src/java/voldemort/store/routed/RoutedStore.java
	src/java/voldemort/store/socket/SocketStore.java
	test/integration/voldemort/performance/AbstractLoadTestHarness.java
	test/integration/voldemort/performance/LocalDirectLoadTest.java
	test/integration/voldemort/performance/LocalRoutedStoreLoadTest.java
	test/integration/voldemort/performance/ReadOnlyStorePerformanceTest.java
	test/integration/voldemort/performance/RemoteHttpStoreLoadTest.java
	test/integration/voldemort/performance/RemoteStoreComparisonTest.java
	test/integration/voldemort/performance/RemoteTest.java
	test/integration/voldemort/performance/SemiLocalHttpStoreLoadTest.java
	test/unit/voldemort/server/EndToEndTest.java
	test/unit/voldemort/store/routed/ReadRepairerTest.java
	test/unit/voldemort/store/routed/RoutedStoreTest.java
	test/unit/voldemort/store/socket/AbstractSocketStoreTest.java
",NotBuggy,"The commit message ""Intermediate check-in for server-side transforms"" suggests the introduction of functionality related to server-side data transformations. The provided code diff shows a change in the `execute` method signature. Specifically, the `execute` method now accepts an additional `byte[] transforms` parameter. This parameter likely contains the transformation logic to be applied on the server side. The change from `Store<ByteArray, byte[]>` to `Store<ByteArray, byte[], byte[]>` also indicates that the store now handles transforms. This diff aligns with the commit message, indicating the addition of server-side transform capabilities. There's no explicit error handling, logical correction, or exception handling improvement in this specific diff. It seems to be a feature addition rather than a bug fix.

**NotBuggy**"
voldemort,7698.json,7871933f0f0f056e2eeac03a01db1e9cf81f8bda,"@@ -1,3 +1,29 @@
     public boolean isCompleteRequest(ByteBuffer buffer) {
-        throw new VoldemortException(""Non-blocking server not supported for AdminServiceRequestHandler"");
+        DataInputStream inputStream = new DataInputStream(new ByteBufferBackedInputStream(buffer));
+
+        try {
+            int dataSize = inputStream.readInt();
+
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, dataSize: "" + dataSize + "", buffer position: ""
+                             + buffer.position());
+
+            if(dataSize == -1)
+                return true;
+
+            // Here we skip over the data (without reading it in) and
+            // move our position to just past it.
+            buffer.position(buffer.position() + dataSize);
+
+            return true;
+        } catch(Exception e) {
+            // This could also occur if the various methods we call into
+            // re-throw a corrupted value error as some other type of exception.
+            // For example, updating the position on a buffer past its limit
+            // throws an InvalidArgumentException.
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, probable partial read occurred: "" + e);
+
+            return false;
+        }
     }",NotBuggy,"Changes to support streaming in NIO-based socket server. No longer using interrupts to stop the NioSocketServer as this can interfere with server-side routing. Also deprecated voldemort.server.socket.AdminService class.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states the following:

*   **Streaming Support in NIO:** The primary goal is to enable streaming in the NIO-based socket server.
*   **Interrupt Removal:** Interrupts are no longer used to stop the `NioSocketServer` to prevent interference with server-side routing.
*   **AdminService Deprecation:** The `voldemort.server.socket.AdminService` class is deprecated.

**Git Diff Analysis:**

The diff focuses on the `isCompleteRequest` method. Here's a breakdown:

1.  **Original Implementation:** The original implementation simply threw a `VoldemortException` indicating that the non-blocking server was not supported for `AdminServiceRequestHandler`. This suggests a placeholder or incomplete implementation.

2.  **New Implementation:** The new implementation attempts to read the size of the data from the `ByteBuffer` using a `DataInputStream`. It then checks if `dataSize` is -1, returning true if it is. Otherwise, it skips over the data in the buffer and returns true. A try-catch block handles potential exceptions during the read or skip operations.

3.  **Exception Handling:** The `catch` block specifically handles exceptions that might occur due to partial reads or corrupted data. It logs a trace message and returns `false`, indicating that the request is not yet complete.

**Reasoning:**

*   The original implementation of `isCompleteRequest` was clearly incomplete and would always throw an exception. This is indicative of a bug or missing functionality.
*   The new implementation provides a functional way to determine if a request is complete by reading the data size and skipping over the data.
*   The addition of exception handling, specifically to catch partial reads and corrupted data, further suggests that the original implementation was not robust and could lead to errors. The `catch` block is designed to handle potential issues that arise during the streaming process, which is a common characteristic of bug fixes.
*   The commit message mentions streaming support, and the code changes directly implement the logic to read data size and skip data, which is essential for streaming.

**Conclusion:**

The code changes replace a placeholder implementation with a functional one that supports streaming and includes error handling for partial reads and corrupted data. This strongly suggests that the original code had a bug or missing functionality that has now been addressed.

**Buggy**
"
cassandra,18667.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,5 +1,5 @@
-    public void close() throws IOException
+    public void close()
     {
-        if (subIterator != null)
-            subIterator.close();
+        if (iterator != null)
+            iterator.close();
     }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

*   ""Storage engine refactor, a.k.a CASSANDRA-8099"" - This suggests a larger restructuring of the storage engine, potentially encompassing multiple changes.
*   ""Initial patch, see ticket for details"" - This indicates that the provided diff is part of a larger effort, and more context can be found in the referenced ticket (CASSANDRA-8099).

**Git Diff Analysis:**

The diff shows a change in the `close()` method. Specifically, it replaces `subIterator` with `iterator`.

*   `subIterator != null` is changed to `iterator != null`.
*   `subIterator.close()` is changed to `iterator.close()`.

**Reasoning:**

The change from `subIterator` to `iterator` suggests a renaming or a change in the internal structure of the class. Without more context (specifically, the ticket CASSANDRA-8099), it's difficult to definitively say if this is a bug fix. It could be part of the refactoring process to use a more general `iterator` instead of a specific `subIterator`.

However, the fact that the code is checking for null before closing the iterator is a good practice to prevent `NullPointerException`. This change might be related to a bug where the iterator was not properly initialized, leading to a null pointer exception when closing.

Given the limited information, it's hard to be certain. However, the change in iterator name and the null check suggest a potential issue with iterator management, which could be related to a bug.

**Conclusion:**

**Buggy**
"
cassandra,17698.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,4 +1,4 @@
-    public int hashCode()
+    public final int hashCode()
     {
-        return Objects.hash(timestamp(), ttl(), localDeletionTime());
+        return Objects.hashCode(partitionDeletion, ranges);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Simplify some 8099's implementations"" suggests a refactoring or simplification of existing code related to ""8099's implementations."" It doesn't explicitly mention a bug fix, but simplification can sometimes be a consequence of fixing an underlying issue or making the code more robust.

**Diff Analysis:**

The diff shows a change in the `hashCode()` method. Specifically:

*   The original `hashCode()` implementation used `Objects.hash(timestamp(), ttl(), localDeletionTime())`.
*   The modified `hashCode()` implementation uses `Objects.hashCode(partitionDeletion, ranges)`.
*   The `hashCode()` method is now `final`.

The change in `hashCode()` implementation suggests a change in the data used to calculate the hash code. This could be due to a change in the underlying data structure or a correction in how the hash code is calculated. The use of `final` suggests that the implementation is now considered stable and should not be overridden.

**Reasoning:**

The commit message indicates simplification. The diff shows a change in the `hashCode()` method, which could be related to a bug fix. The original implementation might have been incorrect or inefficient, and the new implementation is simpler and more correct. The change to `final` suggests that the new implementation is considered stable.

**Conclusion:**

**Buggy**
"
cassandra,2369.json,831bebdba86ac1956852bd216a4cc62d898c87d7,"@@ -1,4 +1,4 @@
     public boolean isEmpty()
     {
-        return getDelegate().isEmpty();
+        return this.restrictions.isEmpty();
     }",NotBuggy,"Refactor Restriction hierarchy

patch by Benjamin Lerer; reviewed by Tyler Hobbs for CASSANDRA-11354
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Refactor Restriction hierarchy"" suggests a structural reorganization or improvement of the `Restriction` hierarchy in the codebase. It doesn't explicitly mention fixing a bug, but refactoring can sometimes uncover or resolve latent issues.

**Code Diff Analysis:**

The code diff shows a change in the `isEmpty()` method.  The original implementation delegated the `isEmpty()` call to `getDelegate()`, while the modified version directly checks `this.restrictions.isEmpty()`.

**Reasoning:**

1.  **Potential Bug Fix:** The change in `isEmpty()` suggests a potential bug fix. The original implementation using `getDelegate()` might have been incorrect or inefficient. The refactored version directly accessing `this.restrictions` implies a more direct and potentially correct way to determine if the restriction is empty. It's possible that `getDelegate()` was returning an object whose `isEmpty()` method didn't accurately reflect the state of the restriction.

2.  **Refactoring Context:** While the commit message focuses on refactoring, the specific change in `isEmpty()` points towards correcting a potentially flawed logic. Refactoring often involves identifying and fixing subtle errors.

3.  **Error Handling/Logic Correction:** The change directly impacts the logic of determining if a restriction is empty. If `getDelegate()` was not correctly representing the state of the restrictions, then this change is correcting that logic.

**Conclusion:**

Based on the code diff and the context of refactoring, the change in the `isEmpty()` method suggests a correction of potentially flawed logic. Therefore, it's likely that this commit includes a bug fix.

**Buggy**
"
cassandra,5527.json,2d991a7e4cc73b522f2c8adf14b5ff37a7947427,"@@ -1,4 +1,4 @@
     public ByteBuffer serialize(UUID value)
     {
-        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
+        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : UUIDGen.toByteBuffer(value);
     }",NotBuggy,"Minor optimizations

patch by slebresne; reviewed by blambov for CASSANDRA-10410

The patch includes the 3 minor optimizations desribed below:
1) Faster copy in CompositeType.build(). We call CompositeType.build()
   relatively often when a table has a composite partition key. When
   copying source buffers into the composite result in that method, we
   can use our existing ByteBuffer.arrayCopy() method which is supposed
   to be faster since it uses FastByteOperations and saves the
   duplication of the buffer to copy
2) Faster UUID->ByteBuffer. We convert UUID to their ByteBuffer
    representation in a few places and do that by converting the UUID to
    a byte[] and then wrapping it. But our convertion of UUID to byte[]
    writes the 2 long the UUID is composed of byte by byte, while for a
    ByteBuffer we can easily just ""put"" both long. So the patch
    introduces a specific method to do that. That's arguably cleaner
    anyway.
3) Memoize hash value for DataResource. Every request does a hash map
   lookup on a DataResource (for validating access rights) which imply
   calling DataResource.hashCode(). Currently that uses
   Objects.hashCode(), which is a varargs methods and allocate an array
   (which, according to MissionControl seems to not be stack allocated).
   The patch does 2 things: it computes the hash only once in the ctor
   and it stores the DataResource for each table in the CFMetaData and
   uses that when checking access permissions for SelectStatement and
   ModificationStatement (the 2 statements where performance matters).
   The 2nd point also saves some unecessary test that the table exists
   when we actually know it does.
",NotBuggy,"The commit message describes minor optimizations, including a faster UUID to ByteBuffer conversion. The provided diff shows a change in the `serialize` method where `ByteBuffer.wrap(UUIDGen.decompose(value))` is replaced with `UUIDGen.toByteBuffer(value)`. This aligns with the commit message's description of a faster UUID to ByteBuffer conversion by using a more efficient method. This change doesn't seem to be directly related to fixing a bug but rather improving performance.

**NotBuggy**"
cassandra,12544.json,490c1c27c9b700f14212d9591a516ddb8d0865c7,"@@ -1,18 +1,18 @@
         public void merge(CombinedValue<Long> other)
         {
             if (!(other instanceof Token))
                 return;
 
             Token o = (Token) other;
             assert o.get().equals(token);
 
             if (o instanceof DKToken)
             {
                 keys.addAll(((DKToken) o).keys);
             }
             else
             {
-                for (RowKey key : o)
+                for (DecoratedKey key : o)
                     keys.add(key);
             }
         }",NotBuggy,"Revert ""Add row offset support to SASI""

This reverts commit 7d857b46fb070548bf5e5f6ff81db588f08ec22a.
",Buggy,"The commit message indicates a revert of a previous commit that added row offset support to SASI. The provided diff shows a change in the `merge` method of a `Token` class, specifically changing `RowKey` to `DecoratedKey` in a loop.

The commit message states that the change is a revert. This means the code is going back to a previous state. The diff shows a change from `RowKey` to `DecoratedKey`. If the original commit changed `DecoratedKey` to `RowKey`, then this revert is undoing that change.

Given that the commit message indicates a revert, and the code change reflects a reversal of a previous modification, it's highly probable that the original commit introduced an issue or was deemed unnecessary. Reverting a commit often implies that the original commit had a bug or unintended consequence.

Therefore, the change is likely related to fixing a bug or removing an unwanted feature.

**Buggy**
"
cassandra,11667.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,10 +1,12 @@
-    protected CBuilder buildIndexClusteringPrefix(ByteBuffer rowKey, ClusteringPrefix prefix, CellPath path)
+    public CBuilder buildIndexClusteringPrefix(ByteBuffer partitionKey,
+                                               ClusteringPrefix prefix,
+                                               CellPath path)
     {
         CBuilder builder = CBuilder.create(getIndexComparator());
-        builder.add(rowKey);
-        for (int i = 0; i < Math.min(columnDef.position(), prefix.size()); i++)
+        builder.add(partitionKey);
+        for (int i = 0; i < Math.min(indexedColumn.position(), prefix.size()); i++)
             builder.add(prefix.get(i));
-        for (int i = columnDef.position() + 1; i < prefix.size(); i++)
+        for (int i = indexedColumn.position() + 1; i < prefix.size(); i++)
             builder.add(prefix.get(i));
         return builder;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",NotBuggy,"The commit message indicates a new API and implementations for built-in indexes, related to CASSANDRA-9459. The diff shows a change in the `buildIndexClusteringPrefix` method. The parameter `rowKey` is renamed to `partitionKey`, and `columnDef` is renamed to `indexedColumn`. The logic within the method remains the same, but the renaming suggests a shift in terminology or a clarification of the method's purpose within the context of the new indexing API. The renaming of variables suggests a refactoring or enhancement rather than a bug fix.
**NotBuggy**"
cassandra,17797.json,c055ab997ea66faccdb10eddd5241f909ff73408,"@@ -1,5 +1,5 @@
-    public static void writeDelTime(DeletionTime dt, SerializationHeader header, DataOutputPlus out) throws IOException
+    public void writeDeletionTime(DeletionTime dt, DataOutputPlus out) throws IOException
     {
-        out.writeVInt(header.encodeTimestamp(dt.markedForDeleteAt()));
-        out.writeVInt(header.encodeDeletionTime(dt.localDeletionTime()));
+        writeTimestamp(dt.markedForDeleteAt(), out);
+        writeLocalDeletionTime(dt.localDeletionTime(), out);
     }",NotBuggy,"Minor improvements to RowStats

patch by slebresne; reviewed by JoshuaMcKenzie for CASSANDRA-9828
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Minor improvements to RowStats"" suggests general improvements rather than a specific bug fix. The reference to ""CASSANDRA-9828"" might provide more context, but without access to the issue tracker, we'll focus on the diff itself. The phrase ""minor improvements"" usually doesn't imply a bug fix.

**Diff Analysis:**

The diff shows a change in the `writeDelTime` method (renamed to `writeDeletionTime` and moved to the RowStats class). The original method took a `DeletionTime`, `SerializationHeader`, and `DataOutputPlus` as input, while the modified method takes only `DeletionTime` and `DataOutputPlus`. The original method used `header.encodeTimestamp` and `header.encodeDeletionTime` to encode the timestamp and local deletion time, respectively. The modified method uses `writeTimestamp` and `writeLocalDeletionTime` methods.

The change suggests a refactoring where the encoding logic is moved from the `SerializationHeader` to the `RowStats` class (or a helper class it uses). This could be for better encapsulation or performance reasons. The removal of `SerializationHeader` from the method signature indicates that the encoding is now handled internally.

**Reasoning:**

The changes appear to be a refactoring or code improvement rather than a bug fix. The commit message supports this interpretation. There's no indication of error handling, logical corrections, or exception handling improvements in the diff. The code is simply reorganized to handle timestamp and local deletion time encoding differently.

**Conclusion:**

NotBuggy
"
cassandra,4841.json,17624248efc316de125d1bc3c6be4f0cb4e174a2,"@@ -1,14 +1,10 @@
                 protected Row computeNext()
                 {
-                    try {
-                        while (currentPage == null || !currentPage.hasNext())
-                        {
-                            if (pager.isExhausted())
-                                return endOfData();
-                            currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
-                        }
-                        return new Row(metadata, currentPage.next());
-                    } catch (RequestValidationException | RequestExecutionException e) {
-                        throw new RuntimeException(e);
+                    while (currentPage == null || !currentPage.hasNext())
+                    {
+                        if (pager.isExhausted())
+                            return endOfData();
+                        currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
                     }
+                    return new Row(metadata, currentPage.next());
                 }",NotBuggy,"Make CassandraException unchecked, extend RuntimeException

Patch by Robert Stupp, reviewed by Sylvain Lebresne for CASSANDRA-8560
",Buggy,"The commit message indicates a change in exception handling, specifically making `CassandraException` unchecked by extending `RuntimeException`. This suggests a potential issue where the checked exception was causing problems, and the change aims to simplify exception handling.

The provided code diff removes a `try-catch` block that was catching `RequestValidationException` and `RequestExecutionException` and re-throwing them as a `RuntimeException`. This change aligns with the commit message's intention of simplifying exception handling. The original code was likely handling these exceptions in a way that was deemed problematic or unnecessary, leading to the decision to allow them to propagate as unchecked exceptions.

The removal of the `try-catch` block suggests that the previous handling was either masking a bug or causing issues with the flow of execution. By removing the block, the exceptions are now allowed to propagate up the call stack, potentially revealing the root cause of the issue or allowing a higher-level handler to deal with it more appropriately.

Therefore, the changes indicate a bug fix or at least an improvement in exception handling related to a potential bug.

**Buggy**"
cassandra,992.json,7dffa83a0619613a17deb3f0d08858006f320e03,"@@ -1,22 +1,8 @@
-    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
+    static void serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
     {
-        boolean bVal = true;
-        int size = gDigestList.size();                        
-        dos.writeInt(size);
-        
-        int estimate = 0;            
+        dos.writeInt(gDigestList.size());
         for ( GossipDigest gDigest : gDigestList )
         {
-            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-            {
-                logger_.info(""@@@@ Breaking out to respect the MTU size in GD @@@@"");
-                bVal = false;
-                break;
-            }
-            int pre = dos.size();               
             GossipDigest.serializer().serialize( gDigest, dos );
-            int post = dos.size();
-            estimate = post - pre;
         }
-        return bVal;
     }",NotBuggy,"remove obsolete gossip size limit.  patch by Anthony Molinaro and jbellis for CASSANDRA-1138

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@957593 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the removal of an obsolete gossip size limit. The diff shows the removal of code that checks and limits the size of the gossip digest list being serialized. The code removed includes a check against `Gossiper.MAX_GOSSIP_PACKET_SIZE` and a mechanism to break out of the serialization loop if the size limit is exceeded. This change directly aligns with the commit message. The removal of this size limit suggests that it was either unnecessary or causing problems, indicating a bug fix.

**Buggy**"
cassandra,10965.json,59b5b6bef0fa76bf5740b688fcd4d9cf525760d0,"@@ -1,4 +1,4 @@
     public String getDatacenter(String host) throws UnknownHostException
     {
-        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddress.getByName(host));
+        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddressAndPort.getByName(host));
     }",NotBuggy,"Allow storage port to be configurable per node

Patch by Ariel Weisberg; Reviewed by Jason Brown for CASSANDRA-7544
",NotBuggy,"The commit message indicates a configuration change, specifically making the storage port configurable per node. The code diff replaces `InetAddress.getByName(host)` with `InetAddressAndPort.getByName(host)`. `InetAddressAndPort` likely encapsulates both the IP address and the port number, while `InetAddress` only encapsulates the IP address. This change allows the system to use a specific port associated with the host, aligning with the commit message's intention of making the storage port configurable per node. There is no explicit bug fix indicated in the commit message or the code diff.

**NotBuggy**"
cassandra,11631.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,4 +1,7 @@
-    protected ByteBuffer getIndexedValue(ByteBuffer rowKey, Clustering clustering, ByteBuffer cellValue, CellPath path)
+    public ByteBuffer getIndexedValue(ByteBuffer partitionKey,
+                                      Clustering clustering,
+                                      CellPath path,
+                                      ByteBuffer cellValue)
     {
         return cellValue;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",Buggy,"The commit message indicates a new API for 2i (secondary indexes) and implementations for built-in indexes. The diff shows a change in the `getIndexedValue` method signature in `AbstractSimplePerColumnSecondaryIndex.java`. The method now takes `partitionKey`, `clustering`, `CellPath`, and `cellValue` as arguments instead of `rowKey`, `clustering`, `cellValue`, and `path`. This change suggests a modification to how indexed values are retrieved, potentially to improve efficiency or correctness. The change in parameters suggests a shift in how the indexed value is derived, possibly to accommodate new indexing strategies or to fix issues with the previous approach. This could be related to a bug fix or an enhancement.

**Buggy**
"
cassandra,20574.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,5 +1,4 @@
     public Mutation makeMutation()
     {
-        assert update != null;
         return new Mutation(update);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",NotBuggy,"The commit message ""Simplify some 8099's implementations"" suggests a general simplification of code related to ""8099"". The provided diff removes an assertion `assert update != null;`. Removing an assertion can be considered a simplification, as it reduces the amount of code executed and removes a check. However, it could also be a bug fix if the assertion was causing problems or if the condition it was checking was no longer valid. Without more context about the surrounding code and the purpose of the assertion, it's difficult to definitively say whether this is a bug fix. It's possible the assertion was redundant or causing false positives. Given the commit message indicates simplification, and the diff shows removal of a check, it's more likely a simplification than a bug fix, unless the assertion was masking an underlying issue.

**NotBuggy**"
cassandra,4540.json,35668435090eb47cf8c5e704243510b6cee35a7b,"@@ -1,14 +1,14 @@
     public boolean equals(Object o)
     {
         if (!(o instanceof UDAggregate))
             return false;
 
         UDAggregate that = (UDAggregate) o;
         return Objects.equal(name, that.name)
-            && Functions.typeEquals(argTypes, that.argTypes)
-            && Functions.typeEquals(returnType, that.returnType)
+            && Functions.typesMatch(argTypes, that.argTypes)
+            && Functions.typesMatch(returnType, that.returnType)
             && Objects.equal(stateFunction, that.stateFunction)
             && Objects.equal(finalFunction, that.finalFunction)
             && Objects.equal(stateType, that.stateType)
             && Objects.equal(initcond, that.initcond);
     }",NotBuggy,"Improve handling of UDA and UDF metadata

patch by Aleksey Yeschenko; reviewed by Robert Stupp for CASSANDRA-9665
",Buggy,"The commit message indicates an improvement in handling UDA (User-Defined Aggregate) and UDF (User-Defined Function) metadata. The provided code diff focuses on the `UDAggregate` class, specifically the `equals` method. The change replaces `Functions.typeEquals` with `Functions.typesMatch`.

Here's a step-by-step analysis:

1.  **Commit Message Relevance:** The commit message mentions improvements in handling UDA metadata. The code diff directly modifies the `UDAggregate` class, which is related to UDA metadata. Thus, the diff is relevant to the commit message.
2.  **Code Change Analysis:** The `equals` method is used to compare `UDAggregate` objects. The original code used `Functions.typeEquals` to compare argument types (`argTypes`) and return types (`returnType`). The updated code uses `Functions.typesMatch` instead.
3.  **Potential Bug Fix:** The change from `typeEquals` to `typesMatch` suggests a potential bug fix. The original method `typeEquals` might have been too strict or incorrect in its type comparison logic, leading to incorrect equality checks for `UDAggregate` objects. The new method `typesMatch` likely provides a more accurate or lenient comparison, resolving the issue. It's reasonable to assume that `typesMatch` was introduced to address a specific problem with `typeEquals`.

Based on the analysis, the change likely addresses a bug related to the comparison of UDA metadata, specifically the types of arguments and return values.

**Conclusion:** Buggy
"
cassandra,20466.json,af3fe39dcabd9ef77a00309ce6741268423206df,"@@ -1,5 +1,9 @@
         public Builder add(KeyspaceMetadata keyspace)
         {
             keyspaces.put(keyspace.name, keyspace);
+
+            keyspace.tables.forEach(t -> tables.put(t.id, t));
+            keyspace.views.forEach(v -> tables.put(v.metadata.id, v.metadata));
+
             return this;
         }",NotBuggy,"Make TableMetadata immutable, optimize Schema

patch by Aleksey Yeschenko; reviewed by Sylvain Lebresne for
CASSANDRA-9425
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**1. Understanding the Commit Message:**

The commit message states:

*   ""Make TableMetadata immutable"" - This suggests a change in how `TableMetadata` objects are handled, likely to prevent unintended modifications.
*   ""optimize Schema"" - This indicates performance improvements related to the schema management.
*   ""CASSANDRA-9425"" - This links the commit to a specific issue in the Cassandra project.

**2. Analyzing the Code Diff:**

The code diff shows a modification within the `Builder.add(KeyspaceMetadata keyspace)` method. The added lines are:

```java
keyspace.tables.forEach(t -> tables.put(t.id, t));
keyspace.views.forEach(v -> tables.put(v.metadata.id, v.metadata));
```

These lines iterate through the `tables` and `views` collections within a `KeyspaceMetadata` object and add them to a `tables` map using their IDs as keys.

**3. Reasoning and Bug Fix Assessment:**

*   **Immutability and Optimization:** The commit message mentions making `TableMetadata` immutable and optimizing the schema. While the diff doesn't directly show immutability changes, the addition of tables and views to a central `tables` map suggests a potential optimization for schema lookups. This could be related to the immutability aspect, as a centralized, immutable map might be more efficient.
*   **Potential Bug Fix:** The original code might have been missing the addition of tables and views to the `tables` map within the `Builder`. This would lead to inconsistencies or incorrect behavior when querying or accessing table/view metadata. The added lines ensure that all tables and views within a keyspace are properly registered in the `tables` map. This strongly suggests a bug fix.
*   **CASSANDRA-9425:** The presence of a JIRA issue number further supports the likelihood of a bug fix.

**4. Conclusion:**

Based on the commit message and the code diff, the changes appear to address a bug where tables and views were not being correctly added to the `tables` map during schema building. This would lead to inconsistencies and incorrect metadata lookups. The addition of the missing logic constitutes a bug fix.

**Buggy**
"
cassandra,16107.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,4 +1,4 @@
-    public int remainingCount()
-    {
-        return ctype.size() - size;
-    }

+        public int remainingCount()
+        {
+            return values.length - size;
+        }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",NotBuggy,"The commit message ""Storage engine refactor, a.k.a CASSANDRA-8099"" indicates a refactoring of the storage engine. The provided code diff shows a change in the `remainingCount()` method. Specifically, `ctype.size()` is replaced with `values.length`. While this change might be part of the refactoring, it doesn't inherently indicate a bug fix. It's a modification of how the remaining count is calculated, likely due to underlying data structure changes as part of the refactoring. There's no explicit error handling, logical correction, or exception handling improvement in this isolated diff.

**NotBuggy**
"
jgit,6893.json,6e05d98cce318056f95700e562cec6b68fcf7475,"@@ -1,4 +1,5 @@
 	public CloneCommand setDirectory(File directory) {
+		validateDirs(directory, gitDir, bare);
 		this.directory = directory;
 		return this;
 	}",NotBuggy,"Allow explicit configuration of git directory in CloneCommand

This feature is needed to support the new submodule layout where the
.git folder of the submodules is under .git/modules/<submodule>.

Change-Id: If5f13426cfd09b7677e23478e9700c8c25a6dae5
",NotBuggy,"The commit message indicates a feature addition to allow explicit configuration of the Git directory, particularly to support a new submodule layout. The provided code diff shows the addition of `validateDirs(directory, gitDir, bare)` in the `setDirectory` method. This change likely validates the provided directory against the Git directory and the bare repository setting.

The commit message describes a feature enhancement rather than a bug fix. The code change appears to be related to validating the directory, which is part of the new feature implementation. There's no indication of error handling, logical corrections, or exception handling improvements that would suggest a bug fix.

**NotBuggy**
"
jgit,4585.json,683bd09092e90aef5b7cf963355995d76aefa439,"@@ -1,3 +1,3 @@
 		public CompressedBitmap andNot(Bitmap other) {
-			return new CompressedBitmap(bitmap.andNot(bitmapOf(other)));
+			return new CompressedBitmap(bitmap.andNot(ewahBitmap(other)), bitmapIndex);
 		}",NotBuggy,"Make BitmapIndexImpl.CompressedBitmap, CompressedBitmapBuilder static

A CompressedBitmap represents a pair (EWAH bit vector, PackIndex
assigning bit positions to git objects).  The bit vector is a member
field and the PackIndex is implicit via the 'this' reference to the
outer class.

Make this clearer by making CompressedBitmap a static class and
replacing the 'this' reference by an explicit field.

Likewise for CompressedBitmapBuilder.

Change-Id: Id85659fc4fc3ad82034db3370cce4cdbe0c5492c
Suggested-by: Terry Parker <tparker@google.com>
",NotBuggy,"The commit message indicates a refactoring to improve clarity by making `CompressedBitmap` and `CompressedBitmapBuilder` static classes. This involves replacing implicit `this` references with explicit field references.

The provided code diff shows a change in the `andNot` method within `BitmapIndexImpl.CompressedBitmap`. Specifically, the `CompressedBitmap` constructor is now being called with an additional argument, `bitmapIndex`. This aligns with the commit message's intention to replace the implicit `this` reference (which would have provided access to the outer class's `bitmapIndex` field) with an explicit field. This change does not seem to be directly fixing a bug, but rather improving the code structure and making the dependency on the outer class explicit.

**NotBuggy**"
jgit,319.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	protected boolean containsHelp(final String... args) {
+	protected boolean containsHelp(String... args) {
 		return TextBuiltin.containsHelp(args);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message states that the change removes the `final` keyword from parameter lists. The diff shows exactly that: the `final` keyword was removed from the `args` parameter in the `containsHelp` method. This change doesn't seem to be related to fixing a bug; it's more likely a stylistic or code convention update. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**"
jgit,1651.json,1f2022e3a7e9482caa4823e031504b6bbe8246af,"@@ -1,6 +1,6 @@
 		public void onSendPack(UploadPack up,
 				Collection<? extends ObjectId> wants,
 				Collection<? extends ObjectId> haves)
-				throws UploadPackMayNotContinueException {
+				throws ServiceMayNotContinueException {
 			// Do nothing.
 		}",NotBuggy,"Modify refs in UploadPack/ReceivePack using a hook interface

This is intended to replace the RefFilter interface (but does not yet,
for backwards compatibility). That interface required lots of extra
scanning and copying in filter cases such as only advertising a subtree
of the refs directory. Instead, provide a hook that can be executed
right before ref advertisement, using the public methods on
UploadPack/ReceivePack to explicitly set the map of advertised refs.

Change-Id: I0067019a191c8148af2cfb71a675f2258c5af0ca
",Buggy,"The commit message describes the introduction of a hook interface to modify refs in `UploadPack` and `ReceivePack`. The goal is to replace the `RefFilter` interface with a more efficient mechanism that avoids extra scanning and copying. The provided diff shows a change in the exception type thrown by the `onSendPack` method of the hook interface, from `UploadPackMayNotContinueException` to `ServiceMayNotContinueException`.

Reasoning:

1.  **Commit Message Focus:** The commit message primarily focuses on introducing a new hook interface for ref advertisement and modification. It mentions replacing the `RefFilter` interface for efficiency reasons.
2.  **Diff Analysis:** The diff shows a change in the exception type thrown by the `onSendPack` method. This suggests a modification to the error handling or control flow within the `UploadPack` process.
3.  **Bug Fix Indication:** While the commit message doesn't explicitly state a bug fix, the change in exception type could be related to improving error handling or addressing a specific issue where the wrong exception was being thrown. It's possible that the original exception type was too specific and didn't cover all cases where the service might need to discontinue.
4.  **Contextual Understanding:** Without more context, it's difficult to definitively say if this is a bug fix. However, the change in exception type suggests a refinement in how errors are handled, which could be a response to a previously identified issue.

Conclusion:

**Buggy**
"
jgit,3458.json,89b91ad40677c7048d31925986e48150e975bcce,"@@ -1,3 +1,3 @@
-			public InMemoryRepository build() throws IOException {
-				throw new UnsupportedOperationException();
-			}

+		public InMemoryRepository build() throws IOException {
+			return new InMemoryRepository(this);
+		}",NotBuggy,"InMemoryRepository: Use a real Builder class

Change-Id: I161b98a58503415955a21f2720395611f439ce98
",NotBuggy,"The commit message ""InMemoryRepository: Use a real Builder class"" indicates that the change is about implementing a proper builder pattern for the `InMemoryRepository` class.

The diff shows that the `build()` method, which previously threw an `UnsupportedOperationException`, is now implemented to return a new `InMemoryRepository` instance. This aligns perfectly with the commit message, suggesting the completion of a builder pattern implementation. There is no indication of bug fixing, error handling, or logical corrections in this change.

**NotBuggy**"
jgit,1828.json,eadfcd3ec166c55c1ff3f3fe0b5e97dd94ff8d83,"@@ -1,8 +1,8 @@
-	private static boolean abort(Iterable<Command> cmdList) {
-		for (Command cmd : cmdList) {
-			if (cmd.getResult() == NOT_ATTEMPTED) {
-				reject(cmd, JGitText.get().transactionAborted);
+	public static void abort(Iterable<ReceiveCommand> commands) {
+		for (ReceiveCommand c : commands) {
+			if (c.getResult() == NOT_ATTEMPTED) {
+				c.setResult(REJECTED_OTHER_REASON,
+						JGitText.get().transactionAborted);
 			}
 		}
-		return false;
 	}",NotBuggy,"ReceiveCommand.abort(): Utility to mark batch of commands as failed

If one or more commands is failing the entire group usually has to
also fail with ""transaction aborted"". Pull this loop into a helper
so the idiom can be easily reused in several places throughout JGit.

Change-Id: I3b9399b7e26ce2b0dc5f7baa85d585a433b4eaed
",Buggy,"The commit message indicates a refactoring to introduce a utility method `abort` to mark a batch of commands as failed, specifically when a transaction is aborted. The diff shows the creation of a public static method `abort` that iterates through a collection of `ReceiveCommand` objects. Inside the loop, it checks if the command's result is `NOT_ATTEMPTED`. If so, it sets the result to `REJECTED_OTHER_REASON` with the message ""transactionAborted"". This change directly addresses the scenario described in the commit message, indicating a bug fix or improvement related to handling transaction aborts. The original method was private and returned a boolean, while the new method is public, static, and void. It also changes the type of the command from `Command` to `ReceiveCommand`. The core logic of setting the result to `REJECTED_OTHER_REASON` when the command has not been attempted is consistent with the commit message's intention of marking commands as failed due to a transaction abort.

**Buggy**"
jgit,5753.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,7 +1,7 @@
-	static RawCharSequence textFor(final RevCommit cmit) {
+	static RawCharSequence textFor(RevCommit cmit) {
 		final byte[] raw = cmit.getRawBuffer();
 		final int b = RawParseUtils.commitMessage(raw, 0);
 		if (b < 0)
 			return RawCharSequence.EMPTY;
 		return new RawCharSequence(raw, b, raw.length);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates a systematic removal of the `final` keyword from specific locations in the Java code, such as package-private functions, try blocks, and for loops. A Python script was used to automate this process. The provided diff shows the removal of the `final` keyword from the parameter of the `textFor` method. This change aligns perfectly with the commit message's intention to remove unnecessary `final` keywords. There is no indication of a bug fix in this change; it seems to be a code cleanup or style change.

**NotBuggy**"
jgit,5925.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,20 +1,20 @@
-	private void insertChild(final int stIdx, final DirCacheTree st) {
+	private void insertChild(int stIdx, DirCacheTree st) {
 		final DirCacheTree[] c = children;
 		if (childCnt + 1 <= c.length) {
 			if (stIdx < childCnt)
 				System.arraycopy(c, stIdx, c, stIdx + 1, childCnt - stIdx);
 			c[stIdx] = st;
 			childCnt++;
 			return;
 		}
 
 		final int n = c.length;
 		final DirCacheTree[] a = new DirCacheTree[n + 1];
 		if (stIdx > 0)
 			System.arraycopy(c, 0, a, 0, stIdx);
 		a[stIdx] = st;
 		if (stIdx < n)
 			System.arraycopy(c, stIdx, a, stIdx + 1, n - stIdx);
 		children = a;
 		childCnt++;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message ""Remove 'final' in parameter lists"" indicates a stylistic change rather than a bug fix. The diff shows the removal of the `final` keyword from the parameter list of the `insertChild` method. This change doesn't inherently fix a bug or introduce new functionality; it's primarily about code style or potential future modification flexibility. There are no error handling updates, logical corrections, or exception-handling improvements.

**NotBuggy**"
jgit,1557.json,d4e7b7060610678e863fbf091746ae1fcc240b0d,"@@ -1,18 +1,18 @@
 		byte[] toArray() throws IOException {
 			try {
 				if (length >= 0) {
 					final byte[] r = new byte[(int) length];
-					NB.readFully(in, r, 0, r.length);
+					IO.readFully(in, r, 0, r.length);
 					return r;
 				}
 
 				final ByteArrayOutputStream r = new ByteArrayOutputStream();
 				final byte[] buf = new byte[2048];
 				int n;
 				while ((n = in.read(buf)) >= 0)
 					r.write(buf, 0, n);
 				return r.toByteArray();
 			} finally {
 				in.close();
 			}
 		}",NotBuggy,"Move pure IO utility functions to a utility class of its own.

According the javadoc, and implied by the name of the class, NB
is about network byte order. The purpose of moving the IO only,
and non-byte order related functions to another class is to
make it easier for new contributors to understand that they
can use these functions in general and it's also makes it easier
to understand where to put new IO related utility functions

Change-Id: I4a9f6b39d5564bc8a694b366e7ff3cc758c5181b
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
Signed-off-by: Shawn O. Pearce <spearce@spearce.org>
",NotBuggy,"The commit message indicates a refactoring effort to improve code clarity and organization by moving pure IO utility functions to a dedicated utility class. The diff shows a change where `NB.readFully` is replaced with `IO.readFully`. This aligns with the commit message's intention of separating IO-related functions from the `NB` class, which is focused on network byte order. There is no indication of a bug fix in this change.
**NotBuggy**"
jgit,8136.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,17 +1,17 @@
-	public static Boolean toBooleanOrNull(final String stringValue) {
+	public static Boolean toBooleanOrNull(String stringValue) {
 		if (stringValue == null)
 			return null;
 
 		if (equalsIgnoreCase(""yes"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""true"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""1"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""on"", stringValue)) //$NON-NLS-1$
 			return Boolean.TRUE;
 		else if (equalsIgnoreCase(""no"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""false"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""0"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""off"", stringValue)) //$NON-NLS-1$
 			return Boolean.FALSE;
 		else
 			return null;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message states that it removes the `final` keyword from parameter lists. The diff shows exactly that: the `final` keyword is removed from the `stringValue` parameter in the `toBooleanOrNull` method. This change does not indicate a bug fix. It's likely a style change or a change to allow modification of the parameter within the method (though the code doesn't actually do that).

**NotBuggy**"
jgit,1282.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,4 +1,4 @@
-	public void setPushRefSpecs(final List<RefSpec> specs) {
+	public void setPushRefSpecs(List<RefSpec> specs) {
 		push.clear();
 		push.addAll(specs);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message states that the change removes the `final` keyword from parameter lists.
The diff shows that the `final` keyword was removed from the `specs` parameter in the `setPushRefSpecs` method.
The change aligns with the commit message.
The removal of `final` does not indicate a bug fix. It's more likely a style change or a change to allow modification of the parameter within the method (though unlikely in this case as the parameter is copied).

**NotBuggy**"
jgit,348.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,3 +1,3 @@
-	void enableBoundary(final boolean on) {
+	void enableBoundary(boolean on) {
 		enableRevSort(RevSort.BOUNDARY, on);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates a systematic removal of the `final` keyword from specific contexts (package-private functions, try blocks, and for loops) using a Python script. The provided diff shows the removal of the `final` keyword from a method parameter declaration. This aligns perfectly with the commit message's intention to remove unnecessary `final` keywords. There's no indication of a bug fix in this change; it's purely a stylistic cleanup.

**NotBuggy**"
jgit,1850.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	public void setResult(final Result s) {
+	public void setResult(Result s) {
 		setResult(s, null);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message states the removal of the `final` keyword from parameter lists. The provided diff shows exactly that: the `final` keyword was removed from the `Result s` parameter in the `setResult` method. This change doesn't inherently indicate a bug fix. It's more likely a style change or a modification to allow the parameter to be modified within the method (though that's unlikely given the method's name). There's no error handling, logical correction, or exception handling involved.

**NotBuggy**"
jgit,7804.json,effc2f34b8fe282ac26f7eb3be028bdfab651ee3,"@@ -1,8 +1,8 @@
-	private boolean isBinary(Entry entry) throws IOException {
+	private static boolean isBinary(Entry entry) throws IOException {
 		InputStream in = entry.openInputStream();
 		try {
 			return RawText.isBinary(in);
 		} finally {
 			safeClose(in);
 		}
 	}",NotBuggy,"[findBugs] Declare some private methods of WorkingTreeIterator static

Change-Id: I09cd39c367f408b5a963ff004f235f558990f338
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",NotBuggy,"The commit message indicates a refactoring to declare some private methods as static. The diff shows that the method `isBinary` is changed from a non-static method to a static method. This change does not seem to be related to any bug fix.
**NotBuggy**"
jgit,1077.json,17fb542e9eaca9dffc4c84a6d23608dadb4b1ea1,"@@ -1,4 +1,4 @@
 		public String getMessage() {
 			return MessageFormat.format(JGitText.get().largeObjectExceedsLimit,
-					getObjectName(), limit, size);
+					getObjectName(), Long.valueOf(limit), Long.valueOf(size));
 		}",NotBuggy,"Remove 86 boxing warnings

Use Integer, Character, and Long valueOf methods when
passing parameters to MessageFormat and other places
that expect objects instead of primitives

Change-Id: I5942fbdbca6a378136c00d951ce61167f2366ca4
",NotBuggy,"The commit message indicates a change to remove boxing warnings by using `Integer`, `Character`, and `Long` `valueOf` methods when passing parameters to `MessageFormat`.

The provided code diff shows a change in the `getMessage` method where `limit` and `size` are now wrapped using `Long.valueOf()` before being passed to `MessageFormat.format()`. This aligns perfectly with the commit message's intention to avoid boxing warnings by explicitly converting primitive `long` values to `Long` objects. This change is not directly related to fixing a bug, but rather improving code quality by addressing warnings.

**NotBuggy**"
weka,8307.json,9aaad03db32231667841ba32d5c424452de6029f,"@@ -1,18 +1,20 @@
   public void updateClassifier(Instance instance) throws Exception {
-    if (m_Filter.numPendingOutput() > 0)
+    if (m_Filter.numPendingOutput() > 0) {
       throw new Exception(""Filter output queue not empty!"");
+    }
     if (!m_Filter.input(instance)) {
-      if (m_Filter.numPendingOutput() > 0)
+      if (m_Filter.numPendingOutput() > 0) {
         throw new Exception(""Filter output queue not empty!"");
+      }
 
       // nothing to train on if the filter does not make an instance available
       return;
       // throw new
       // Exception(""Filter didn't make the train instance immediately available!"");
     }
 
     m_Filter.batchFinished();
     Instance newInstance = m_Filter.output();
 
     ((UpdateableClassifier) m_Classifier).updateClassifier(newInstance);
   }",NotBuggy,"Now implements UpdateableBatchProcessor

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11099 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Now implements UpdateableBatchProcessor"" indicates a feature addition or enhancement related to the `UpdateableBatchProcessor` interface. The diff shows modifications within the `updateClassifier` method. Specifically, the code adds curly braces around the `if` statement bodies that check `m_Filter.numPendingOutput() > 0`. This change doesn't seem to be related to fixing a bug, but rather to improve code clarity or structure. The core logic remains the same. The commit message doesn't align with a bug fix, and the code changes don't suggest one either.

**NotBuggy**"
weka,31489.json,6567cae2752e3d3cbfb7eefd7f1f5230c3f800fa,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.7 $"");
+    return RevisionUtils.extract(""$Revision: 1.8 $"");
   }",NotBuggy,"Now uses the current class loader rather than the system class loader to load icons


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4400 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a change in how icons are loaded, specifically switching from the system class loader to the current class loader. The provided diff only shows a change in the revision number. This suggests that the actual code modification related to the class loader change is not present in this diff. However, the commit message itself doesn't inherently indicate a bug fix. It's more of a change in implementation strategy.

Therefore, based on the available information, it's not possible to determine if this commit fixes a bug.

**NotBuggy**
"
weka,25251.json,19e3b5a26bbba1eeced04be767b1839216872d5f,"@@ -1,7 +1,8 @@
   public int graphType() {
-    
-    if (m_Classifier instanceof Drawable)
-      return ((Drawable)m_Classifier).graphType();
-    else 
+
+    if (m_Classifier instanceof Drawable) {
+      return ((Drawable) m_Classifier).graphType();
+    } else {
       return Drawable.NOT_DRAWABLE;
+    }
   }",NotBuggy,"Further cleanup of option handling and general code cleanup, removing warnings.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10153 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states ""Further cleanup of option handling and general code cleanup, removing warnings."" This suggests improvements to the code's structure, readability, and the elimination of compiler warnings. It doesn't explicitly mention fixing a bug, but rather focuses on code quality and maintainability.

**Git Diff Analysis:**

The diff shows a minor change in the `graphType()` method. Specifically, braces `{}` have been added to the `if` and `else` blocks.

**Reasoning:**

The addition of braces around the `if` and `else` blocks improves code clarity and reduces the risk of introducing bugs in the future when modifying the code. While this change enhances code quality, it doesn't directly address a bug. The original code without the braces was syntactically correct, but the added braces improve readability and maintainability. The change aligns with the ""general code cleanup"" part of the commit message. There's no error handling, logical correction, or exception handling improvement in this diff.

**Conclusion:**

The changes in the diff align with the commit message's description of code cleanup and improved readability. There's no indication of a bug fix.

**NotBuggy**
"
weka,4982.json,1d7e46e21d7fb5c04f903c0b15f5348eaf41faaa,"@@ -1,19 +1,13 @@
   protected Vector computeMean(Instances data, double[] totalWeight, int aI) {
 
     Vector meanVector = new DenseVector(data.numAttributes() - 1);
     totalWeight[aI] = 0;
     for (Instance inst : data) {
       if (!inst.classIsMissing()) {
-        int index = 0;
-        for (int j = 0; j < inst.numAttributes(); j++) {
-          if (j != inst.classIndex()) {
-            meanVector.add(index, inst.weight() * inst.value(index));
-            index++;
-          }
-        }
+        meanVector.add(inst.weight(), instanceToVector(inst));
         totalWeight[aI] += inst.weight();
       }
     }
     meanVector.scale(1.0 / totalWeight[aI]);
     return meanVector;
   }",NotBuggy,"Further progress but probably not correct yet.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13005 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message ""Further progress but probably not correct yet"" suggests ongoing work and uncertainty about the correctness of the changes. The code diff modifies the `computeMean` method. The original code iterates through the attributes of each instance, skipping the class index, and adds the weighted value to the `meanVector`. The modified code replaces this loop with a call to `instanceToVector(inst)` and adds the weight to the vector. This change seems to be refactoring the code to use a helper method `instanceToVector` to convert an instance to a vector, which might be part of a larger effort to improve or fix the `computeMean` method. Given the commit message and the code changes, it's likely that the changes are related to fixing a bug or improving the algorithm, even though the commit message expresses uncertainty.

**Buggy**"
weka,4307.json,99f772d675df4a96af5dd8b8e60e50c3fc90ce78,"@@ -1,26 +1,26 @@
-  public void setMatrix ( double[] v, boolean columnFirst ) {
+  public void setMatrix(double[] v, boolean columnFirst) {
     try {
-      if( v.length != m * n ) 
-	throw new IllegalArgumentException(""sizes not match."");
-      int i, j, count = 0;
-      if( columnFirst ) {
-	for( i = 0; i < m; i++ ) {
-	  for( j = 0; j < n; j++ ) {
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      if (v.length != m * n) {
+        throw new IllegalArgumentException(""sizes not match."");
       }
-      else {
-	for( j = 0; j < n; j++ ) {
-	  for( i = 0; i < m; i++ ){
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      int i, j, count = 0;
+      if (columnFirst) {
+        for (i = 0; i < m; i++) {
+          for (j = 0; j < n; j++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
+      } else {
+        for (j = 0; j < n; j++) {
+          for (i = 0; i < m; i++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
       }
 
-    } catch( ArrayIndexOutOfBoundsException e ) {
-      throw new ArrayIndexOutOfBoundsException( ""Submatrix indices"" );
+    } catch (ArrayIndexOutOfBoundsException e) {
+      throw new ArrayIndexOutOfBoundsException(""Submatrix indices"");
     }
   }",NotBuggy,"Code clean-up: generic type arguments, elimination of FastVector, clean-up of option handling.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10374 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates code cleanup activities such as generic type arguments, elimination of FastVector, and option handling. The diff shows changes in the `setMatrix` method, specifically related to formatting and spacing. It also includes a `try-catch` block to handle `ArrayIndexOutOfBoundsException`. The exception handling was already in place, and the changes mainly involve reformatting the code. The changes do not seem to be related to fixing a bug.

**NotBuggy**"
weka,27566.json,7d9903c92f07f756915f15332672dfdcaf118c5d,"@@ -1,32 +1,39 @@
   public void buildClassifier(Instances data) throws Exception {
   
-    if (!data.classAttribute().isNumeric()) {
-      throw new Exception(""Class attribute has to be numeric for regression!"");
-    }
-    if (data.numInstances() == 0) {
-      throw new Exception(""No instances in training file!"");
-    }
-    if (data.checkForStringAttributes()) {
-      throw new Exception(""Can't handle string attributes!"");
+    if (!m_checksTurnedOff) {
+      if (!data.classAttribute().isNumeric()) {
+	throw new Exception(""Class attribute has to be numeric for regression!"");
+      }
+      if (data.numInstances() == 0) {
+	throw new Exception(""No instances in training file!"");
+      }
+      if (data.checkForStringAttributes()) {
+	throw new Exception(""Can't handle string attributes!"");
+      }
     }
 
     // Preprocess instances
     m_TransformedData = data;
-    m_TransformFilter = new NominalToBinaryFilter();
-    m_TransformFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
-    m_MissingFilter = new ReplaceMissingValuesFilter();
-    m_MissingFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
-    m_TransformedData.deleteWithMissingClass();
+    if (!m_checksTurnedOff) {
+      m_TransformFilter = new NominalToBinaryFilter();
+      m_TransformFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
+      m_MissingFilter = new ReplaceMissingValuesFilter();
+      m_MissingFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
+      m_TransformedData.deleteWithMissingClass();
+    } else {
+      m_TransformFilter = null;
+      m_MissingFilter = null;
+    }
     m_ClassIndex = m_TransformedData.classIndex();
 
     // Calculate attribute standard deviations
     calculateAttributeDeviations();
 
     // Perform the regression
     findBestModel();
 
     // Save memory
     m_TransformedData = new Instances(m_TransformedData, 0);
   }",NotBuggy,"Added more options


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1166 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Added more options"" suggests the introduction of new configurable parameters or functionalities. The diff introduces a boolean flag `m_checksTurnedOff`. When this flag is true, the code skips checks for numeric class attributes, empty training files, and string attributes. Additionally, data preprocessing steps involving `NominalToBinaryFilter` and `ReplaceMissingValuesFilter` are bypassed. This indicates the addition of an option to disable certain checks and preprocessing steps. This change does not appear to be fixing a bug.

**NotBuggy**"
weka,32015.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,44 +1,44 @@
    public Object readCollection(Element node) throws Exception {
-      Collection           coll;
-      Vector               v;
-      Vector               children;
+      Collection<Object>           coll;
+      Vector<Object>               v;
+      Vector<Element>               children;
       Element              child;
       int                  i;
       int                  index;
       int                  currIndex;
 
       // for debugging only
       if (DEBUG)
          trace(new Throwable(), node.getAttribute(ATT_NAME));
 
       m_CurrentNode = node;
       
       children = XMLDocument.getChildTags(node); 
-      v        = new Vector();
+      v        = new Vector<Object>();
 
       // determine highest index for size
       index    = children.size() - 1;
       for (i = 0; i < children.size(); i++) {
         child     = (Element) children.get(i);
         currIndex = Integer.parseInt(child.getAttribute(ATT_NAME));
         if (currIndex > index)
           index = currIndex;
       }
       v.setSize(index + 1);
 
 
       // put the children in the vector to sort them according their index
       for (i = 0; i < children.size(); i++) {
          child = (Element) children.get(i);
          v.set(
                Integer.parseInt(child.getAttribute(ATT_NAME)), 
                invokeReadFromXML(child));
       }
       
       // populate collection
-      coll = (Collection) Class.forName(
-                  node.getAttribute(ATT_CLASS)).newInstance();
+      coll = Utils.cast(Class.forName(node.getAttribute(ATT_CLASS)).
+                        newInstance());
       coll.addAll(v);
       
       return coll;
    }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a major refactoring of the core package to remove the dependency on `FastVector` and eliminate unchecked compile-time warnings. The diff shows changes in the `readCollection` method, specifically:

1.  **Type Parameterization:** The raw `Vector` types are now parameterized with `<Object>` and `<Element>`, like `Vector<Object>` and `Vector<Element>`. This addresses unchecked type warnings.
2.  **Casting:** The line `coll = (Collection) Class.forName(...)` is replaced with `coll = Utils.cast(Class.forName(...))`. This suggests a safer casting mechanism is being used, likely to avoid potential `ClassCastException` issues.

The changes directly relate to the commit message's goals of removing unchecked warnings and refactoring the code. The use of `Utils.cast` implies a potential bug fix or at least a more robust approach to type handling, which can prevent runtime exceptions.

**Buggy**
"
weka,36275.json,ae9b86a4d68c6d5a823bd20243149bb187a082e9,"@@ -1,39 +1,40 @@
   protected static weka.core.converters.Loader readStepPropertyLoader(
     JSONNode loaderNode) throws WekaException {
 
     String clazz = loaderNode.getChild(CLASS).getValue().toString();
     try {
       weka.core.converters.Loader loader =
-        (weka.core.converters.Loader) Beans.instantiate(
-          JSONFlowUtils.class.getClassLoader(), clazz);
+        (weka.core.converters.Loader) WekaPackageClassLoaderManager.objectForName(clazz);
+          /* Beans.instantiate(
+          JSONFlowUtils.class.getClassLoader(), clazz); */
 
       if (loader instanceof OptionHandler) {
         String optionString =
           loaderNode.getChild(OPTIONS).getValue().toString();
         if (optionString != null && optionString.length() > 0) {
           ((OptionHandler) loader).setOptions(Utils.splitOptions(optionString));
         }
       }
 
       if (loader instanceof weka.core.converters.AbstractFileLoader) {
         String filePath = loaderNode.getChild(""filePath"").getValue().toString();
         if (filePath.length() > 0) {
 
           ((weka.core.converters.AbstractFileLoader) loader)
             .setSource(new File(filePath));
         }
       }
 
       if (loader instanceof weka.core.converters.FileSourcedConverter) {
         Boolean relativePath =
           (Boolean) loaderNode.getChild(""useRelativePath"").getValue();
         ((weka.core.converters.FileSourcedConverter) loader)
           .setUseRelativePath(relativePath);
 
       }
 
       return loader;
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",NotBuggy,"Changes necessary to support using child classloaders for packages

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13476 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a change to support using child classloaders for packages. The code diff replaces `Beans.instantiate(JSONFlowUtils.class.getClassLoader(), clazz)` with `WekaPackageClassLoaderManager.objectForName(clazz)`. This change directly relates to the commit message's intention of using child classloaders. The original code used the classloader of `JSONFlowUtils`, while the new code uses `WekaPackageClassLoaderManager` which presumably handles child classloaders. The change also includes commenting out the original line, further suggesting a deliberate replacement rather than an addition. There is no explicit error handling or exception handling improvement in the diff. The change seems to be an architectural one to support a new feature (child classloaders for packages).

**NotBuggy**"
weka,23944.json,08514d45c889fe77d6b7780452f396dd83daf165,"@@ -1,4 +1,5 @@
   public void setRowHidden(int index, boolean hidden) {
-    if ( (index >= 0) && (index < getRowCount()) )
+    if ((index >= 0) && (index < getRowCount())) {
       m_RowHidden[index] = hidden;
+    }
   }",NotBuggy,"DefaultListModels is not generic in Java 1.6. Cleaned up a further few classes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10204 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""DefaultListModels is not generic in Java 1.6. Cleaned up a further few classes."" suggests two things:

1.  A change related to generics due to Java 1.6 compatibility. This implies a potential workaround or adaptation for older Java versions.
2.  General cleanup of some classes.

**Git Diff Analysis:**

The diff shows a modification within the `setRowHidden` method. Specifically, braces have been added to the `if` statement:

```diff
-    if ( (index >= 0) && (index < getRowCount()) )
+    if ((index >= 0) && (index < getRowCount())) {
       m_RowHidden[index] = hidden;
+    }
```

**Reasoning:**

The addition of braces to the `if` statement, while seemingly minor, doesn't directly relate to generics or a specific bug fix. It's more likely a code style improvement or a defensive measure to ensure the intended scope of the `if` statement. It makes the code more explicit and less prone to errors if someone later adds another statement to the `if` block without adding braces.

The commit message mentions ""cleanup,"" and this change could fall under that category. However, it doesn't strongly suggest a bug was present and being fixed. There's no error handling, logical correction, or exception handling involved.

**Conclusion:**

The changes appear to be related to code style and maintainability rather than a specific bug fix. Therefore, I conclude:

**NotBuggy**
"
weka,19765.json,1e7162781f076ec9a5024dfa5a04bec8d4e4e0f5,"@@ -1,10 +1,12 @@
   public void processHeadlessEvents(List<EventObject> headless) {
     // only process if we're not headless
     if (!java.awt.GraphicsEnvironment.isHeadless()) {
+      m_processingHeadlessEvents = true;
       for (EventObject e : headless) {
         if (e instanceof DataSetEvent) {
           acceptDataSet((DataSetEvent)e);
         }
       }
     }
+    m_processingHeadlessEvents = false;
   }",NotBuggy,"No longer generates image events when processing previously collected events received during headless execution (prevents charts from being generated twice - once when executed headless on the server and then a second time when results are retrieved).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7680 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the change prevents duplicate chart generation when processing events received during headless execution. The code diff introduces a boolean flag `m_processingHeadlessEvents` to track whether headless events are being processed. This flag is set to `true` before processing the events and set back to `false` after processing. This suggests that the flag is used elsewhere to prevent duplicate chart generation. The change directly addresses the issue described in the commit message.

**Buggy**
"
weka,35996.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,41 +1,41 @@
   protected static Instances getMiningSchemaAsInstances(Element model,
                                                         Instances dataDictionary) 
     throws Exception {
-    FastVector attInfo = new FastVector();
+    ArrayList<Attribute> attInfo = new ArrayList<Attribute>();
     NodeList fieldList = model.getElementsByTagName(""MiningField"");
     int classIndex = -1;
     int addedCount = 0;
     for (int i = 0; i < fieldList.getLength(); i++) {
       Node miningField = fieldList.item(i);
       if (miningField.getNodeType() == Node.ELEMENT_NODE) {
         Element miningFieldEl = (Element)miningField;
         String name = miningFieldEl.getAttribute(""name"");
         String usage = miningFieldEl.getAttribute(""usageType"");
         // TO-DO: also missing value replacement etc.
 
         // find this attribute in the dataDictionary
         Attribute miningAtt = dataDictionary.attribute(name);
         if (miningAtt != null) {
           if (usage.length() == 0 || usage.equals(""active"") || usage.equals(""predicted"")) {
-            attInfo.addElement(miningAtt);
+            attInfo.add(miningAtt);
             addedCount++;
           }
           if (usage.equals(""predicted"")) {
             classIndex = addedCount - 1;
           }
         } else {
           throw new Exception(""Can't find mining field: "" + name 
                               + "" in the data dictionary."");
         }
       }
     }
     
     Instances insts = new Instances(""miningSchema"", attInfo, 0);
     //    System.out.println(insts);
     if (classIndex != -1) {
       insts.setClassIndex(classIndex);
     }
 
 
     return insts;
   }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a transition away from using `FastVector` in the core package, while maintaining it for backward compatibility. The provided diff shows a change from `FastVector` to `ArrayList<Attribute>` in the `getMiningSchemaAsInstances` method. This aligns with the commit message's intention to remove `FastVector` from the core package. There is no indication of a bug fix.

**NotBuggy**"
weka,20858.json,60be28974823b8c8f15a54035f2cdee1449d7617,"@@ -1,6 +1,7 @@
             public void actionPerformed(ActionEvent e) {
-              bc.remove();
+              bc.remove(m_mainKFPerspective.getCurrentTabIndex());
               m_beanLayout.revalidate();
               m_beanLayout.repaint();
+              m_mainKFPerspective.setEditedStatus(true);
               notifyIsDirty();
             }",NotBuggy,"Fisrt stage of modernizing the KnowledgeFlow user interface and functionality.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7124 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""First stage of modernizing the KnowledgeFlow user interface and functionality"" suggests a broad set of changes related to improving the user interface and underlying functionality of the KnowledgeFlow component. It doesn't explicitly mention bug fixes, but modernization efforts often uncover and address existing bugs or inconsistencies.

**Git Diff Analysis:**

The diff shows a modification within an `actionPerformed` method, likely associated with a button click or similar UI event.

-   `bc.remove();` is changed to `bc.remove(m_mainKFPerspective.getCurrentTabIndex());` This indicates that instead of removing a default or potentially incorrect element, the code now removes an element based on the currently selected tab index. This is a significant change in logic.
-   `m_mainKFPerspective.setEditedStatus(true);` This line sets the edited status to true, likely indicating that the KnowledgeFlow perspective has been modified. This is consistent with the modernization effort.

**Reasoning:**

The change from `bc.remove()` to `bc.remove(m_mainKFPerspective.getCurrentTabIndex())` strongly suggests a bug fix. The original code likely had a flaw where it was removing the wrong element, or an element without considering the current context (tab index). The modified code now correctly removes the element associated with the current tab, which is a logical correction. The addition of `m_mainKFPerspective.setEditedStatus(true);` is related to the UI update and is consistent with the modernization effort.

**Conclusion:**

**Buggy**
"
weka,15488.json,55b9fb2a4bd0d399e6823c494c1c78ae0af0979c,"@@ -1,45 +1,27 @@
   public void setOptions(String[] options) throws Exception {
     
-    String attributeIndex = Utils.getOption('C', options);
-    if (attributeIndex.length() != 0) {
-      if (attributeIndex.toLowerCase().equals(""last"")) {
-	setAttributeIndex(-1);
-      } else if (attributeIndex.toLowerCase().equals(""first"")) {
-	setAttributeIndex(0);
-      } else {
-	setAttributeIndex(Integer.parseInt(attributeIndex) - 1);
-      }
+    String attIndex = Utils.getOption('C', options);
+    if (attIndex.length() != 0) {
+      setAttributeIndex(attIndex);
     } else {
-      setAttributeIndex(-1);
+      setAttributeIndex(""last"");
     }
-    
-    String firstIndex = Utils.getOption('F', options);
-    if (firstIndex.length() != 0) { 
-      if (firstIndex.toLowerCase().equals(""last"")) {
-	setFirstValueIndex(-1);
-      } else if (firstIndex.toLowerCase().equals(""first"")) {
-	setFirstValueIndex(0);
-      } else {
-	setFirstValueIndex(Integer.parseInt(firstIndex) - 1);
-      }
+
+    String firstValIndex = Utils.getOption('F', options);
+    if (firstValIndex.length() != 0) {
+      setFirstValueIndex(firstValIndex);
     } else {
-      setFirstValueIndex(-1);
+      setFirstValueIndex(""first"");
     }
-     
-    String secondIndex = Utils.getOption('S', options);
-    if (secondIndex.length() != 0) {
-      if (secondIndex.toLowerCase().equals(""last"")) {
-	setSecondValueIndex(-1);
-      } else if (secondIndex.toLowerCase().equals(""first"")) {
-	setSecondValueIndex(0);
-      } else {
-	setSecondValueIndex(Integer.parseInt(secondIndex) - 1); 
-      }
+
+    String secondValIndex = Utils.getOption('S', options);
+    if (secondValIndex.length() != 0) {
+      setSecondValueIndex(secondValIndex);
     } else {
-      setSecondValueIndex(-1);
+      setSecondValueIndex(""last"");
     }
    
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",NotBuggy,"Indices now start from 1 for (hopefully) all filters. Changed default behaviour of Remove...java filters in unsupervised.instances. Changed some meta classifiers accordingly.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1795 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a change in how indices are handled, now starting from 1, and a change in the default behavior of `Remove...java` filters. The diff shows modifications to the `setOptions` method, specifically how the attribute, first value, and second value indices are parsed from the options.

The original code parsed the index from the options and then subtracted 1 to adjust for a 0-based index. It also handled ""last"" and ""first"" keywords. The modified code directly sets the index using `setAttributeIndex`, `setFirstValueIndex`, and `setSecondValueIndex` with the provided string. The default values have also changed to ""last"", ""first"", and ""last"" respectively.

The change to start indices from 1 and the modification of default behavior suggest a correction or adjustment to how the filter was interpreting user-provided indices. This is indicative of a bug fix or an adjustment to align with user expectations.

**Buggy**
"
weka,25044.json,22fe9ffdd42d2d198ece80fef7d71fe91de6c1d6,"@@ -1,4 +1,4 @@
   public Clusterer getClusterer() {
 
-    return m_Clusterer;
+    return m_wrappedClusterer;
   }",NotBuggy,"Has been rewritten.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1183 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Has been rewritten"" is vague and doesn't provide specific details about the changes.

The code diff shows a change from `m_Clusterer` to `m_wrappedClusterer` in the `getClusterer()` method. This suggests that the class might have been refactored or rewritten, and the clusterer instance variable was renamed or replaced. Without more context, it's difficult to determine if this change is related to a bug fix. It could be simply a renaming during a refactoring process.

Therefore, based on the limited information, it's not possible to definitively say that this commit fixes a bug.

**NotBuggy**
"
weka,9558.json,9632387e9fe84f73e41daa585774e86c8cc3c617,"@@ -1,92 +1,124 @@
   public void doGet(HttpServletRequest request, HttpServletResponse response)
     throws ServletException, IOException {
 
     if (!request.getRequestURI().startsWith(CONTEXT_PATH)) {
       return;
     }
 
     PrintWriter out = null;
     InputStream in = request.getInputStream();
     ObjectOutputStream oos = null;
 
-    String clientParam = request.getParameter(""client"");
-    boolean client = (clientParam != null && clientParam.equalsIgnoreCase(""y""));
+    String clientParamLegacy = request.getParameter(Legacy.LEGACY_CLIENT_KEY);
+    String clientParamNew = request.getParameter(JSONProtocol.JSON_CLIENT_KEY);
+    boolean clientLegacy =
+      clientParamLegacy != null && clientParamLegacy.equalsIgnoreCase(""y"");
+    boolean clientNew =
+      clientParamNew != null && clientParamNew.equalsIgnoreCase(""y"");
+
     String taskName = request.getParameter(""name"");
 
     NamedTask task = m_taskMap.getTask(taskName);
 
-    if (client) {
+    if (clientLegacy) {
       // response.setCharacterEncoding(""UTF-8"");
       // response.setContentType(""text/plain"");
       response.setContentType(""application/octet-stream"");
       OutputStream outS = response.getOutputStream();
       oos = new ObjectOutputStream(new BufferedOutputStream(outS));
+    } else if (clientNew) {
+      out = response.getWriter();
+      response.setCharacterEncoding(""UTF-8"");
+      response.setContentType(""application/json"");
     } else {
       out = response.getWriter();
       response.setCharacterEncoding(""UTF-8"");
       response.setContentType(""text/html;charset=UTF-8"");
       out.println(""<HTML>"");
       out.println(""<HEAD><TITLE>Schedule</TITLE></HEAD>"");
       out.println(""<BODY>"");
     }
 
     response.setStatus(HttpServletResponse.SC_OK);
 
     try {
       if (task == null) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR
-            + "": Can't find task "" + taskName;
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + "": Can't find task "" + taskName;
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""Can't find task "" + taskName);
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out
             .println(WekaServlet.RESPONSE_ERROR + "": Unknown task "" + taskName);
         }
       } else if (!(task instanceof Scheduled)) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR + ""'"" + taskName
-            + ""' "" + ""is not a scheduled task."";
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
+              + ""is not a scheduled task."";
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""'"" + taskName
+              + ""' is not a scheduled task"");
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out.println(WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
             + ""is not a scheduled task."");
         }
       } else {
         Schedule sched = ((Scheduled) task).getSchedule();
-        if (client) {
+        if (clientLegacy) {
           oos.writeObject(sched);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> scheduleJ = JSONProtocol.scheduleToJsonMap(sched);
+          Map<String, Object> jResponse =
+            JSONProtocol.createOKResponseMap(""OK. Schedule"");
+          JSONProtocol.addPayloadMap(jResponse, scheduleJ,
+            JSONProtocol.SCHEDULE_PAYLOAD_ID);
+
+          String encodedResonse = JSONProtocol.encodeToJSONString(jResponse);
+          out.println(encodedResonse);
+          out.flush();
         } else {
-          String optionsString = weka.core.Utils
-            .joinOptions(sched.getOptions());
+          String optionsString =
+            weka.core.Utils.joinOptions(sched.getOptions());
           out.println(optionsString + ""<p>"");
         }
       }
     } catch (Exception ex) {
-      if (client && oos != null) {
+      if (clientLegacy && oos != null) {
         oos.writeObject(WekaServlet.RESPONSE_ERROR + "" "" + ex.getMessage());
         oos.flush();
-      } else if (out != null) {
+      } else if (out != null && !clientNew) {
         out.println(""<p><pre>"");
         ex.printStackTrace(out);
         out.println(""</pre>\n"");
       }
       ex.printStackTrace();
     } finally {
-      if (!client && out != null) {
+      if (!clientLegacy && !clientNew && out != null) {
         out.println(""</BODY>\n</HTML>"");
       }
 
       if (out != null) {
         out.close();
         out = null;
       }
       if (oos != null) {
         oos.close();
         oos = null;
       }
     }
-
   }",NotBuggy,"Implementation for the new Knowledge Flow

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Implementation for the new Knowledge Flow"" suggests the introduction of new functionality related to Knowledge Flow.

The diff introduces a new client parameter `clientNew` that uses `JSONProtocol` for communication, alongside the existing `clientLegacy` parameter. The code now handles three cases: `clientLegacy`, `clientNew`, and the default case. It uses `JSONProtocol` to create and encode responses when `clientNew` is true. Error handling is updated to provide JSON responses for the new client type. Exception handling includes a check for `clientNew` before printing the stack trace to the output.

The changes introduce new functionality and update error handling to support the new `clientNew` parameter and `JSONProtocol`. There are no explicit bug fixes, but the changes improve the existing code to support new functionality.

**NotBuggy**"
xerces2-j,109.json,1281996de10d3af1b51f0b4d78cfcd75d8ade6fe,"@@ -1,4 +1,4 @@
-    public void endElement(QName element)
+    public void endElement(QName element, Augmentations augs)
         throws XNIException {
-        super.endElement(toUpperCase(element));
+        super.endElement(toUpperCase(element), augs);
     } // endElement(QName)",NotBuggy,"Modify XNI to include Augmentations parameter on each XMLDocumentHandler call.
Modify samples.
Modify docs http://nagoya.apache.org/bugzilla/show_bug.cgi?id=5058


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317940 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a modification to the XNI interface to include an `Augmentations` parameter in the `XMLDocumentHandler` calls. It also mentions modifications to samples and documentation, potentially related to a bug fix reported in Bugzilla (bug ID 5058).

The provided code diff shows a change in the `endElement` method signature to include an `Augmentations` parameter. This change aligns with the commit message's description of modifying the `XMLDocumentHandler` calls. The inclusion of `Augmentations` suggests that additional information or context is being passed along with the element, which could be related to fixing a bug where this information was previously missing or inaccessible.

Given the context of the commit message and the code modification, it's likely that this change is related to a bug fix. The addition of the `Augmentations` parameter suggests a correction or enhancement to the way XML documents are handled, possibly addressing a previously identified issue.

**Buggy**"
xerces2-j,6765.json,62b65e27622f6a1657e2e6a6c0776c95151c9dd0,"@@ -1,24 +1,27 @@
     public Object getParameter(String name) throws DOMException {
-		if (name.equals(Constants.DOM_ERROR_HANDLER)){
-			if (fErrorHandler != null){
-				return fErrorHandler.getErrorHandler();
-			}
-		}
+        
+        if (name.equals(Constants.DOM_ERROR_HANDLER)){
+            return (fErrorHandler != null) ? fErrorHandler.getErrorHandler() : null;
+        }
+        else if (name.equals(Constants.DOM_RESOURCE_RESOLVER)) {
+            return (fResourceResolver != null) ? fResourceResolver.getEntityResolver() : null;
+        }
+        
         try {
             boolean feature = getFeature(name);
             return (feature) ? Boolean.TRUE : Boolean.FALSE;
         } catch (Exception e) {
             Object property;
             try {
                 property = getProperty(name);
                 return property;
             } catch (Exception ex) {
                 String msg =
                     DOMMessageFormatter.formatMessage(
                         DOMMessageFormatter.DOM_DOMAIN,
                         ""FEATURE_NOT_SUPPORTED"",
                         new Object[] { name });
                 throw new DOMException(DOMException.NOT_SUPPORTED_ERR, msg);
             }
         }
     }",NotBuggy,"The XML Schema API [1] states that the DOMConfiguration of an XSLoader may
allow setting of a resource-resolver (LSResourceResolver). Supporting this
parameter so that applications may use their own resolver to locate
schema imports and includes.

[1] http://www.w3.org/Submission/2004/SUBM-xmlschema-api-20040309/xml-schema-api.html#Interface-Loader


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319940 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of support for a resource resolver in the XML Schema API, allowing applications to use custom resolvers for locating schema imports and includes. The code diff shows the addition of a new condition in the `getParameter` method to handle the `Constants.DOM_RESOURCE_RESOLVER` parameter. This aligns with the commit message's intention to support resource resolvers. The code returns the resource resolver if it's set, otherwise, it returns null. This addition doesn't seem to be related to fixing a bug but rather adding a new feature.

**NotBuggy**"
xerces2-j,5928.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,7 +1,6 @@
-    public void startDocument() {
-
-        if (!canonical) {
-            out.println(""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>"");
+    public void startDocument(XMLLocator locator, String encoding) 
+        throws XNIException {
+        if (fDocumentHandler != null && !fOnlyPassPrefixMappingEvents) {
+            fDocumentHandler.startDocument(locator, encoding);
         }
-
-    } // startDocument()

+    } // startDocument(XMLLocator,String)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Merge/move of Xerces 2 to the trunc!"" suggests a large-scale operation, likely involving moving or merging code from Xerces 2 into the trunk (main development branch). This type of commit is generally not associated with specific bug fixes but rather with structural changes or integration of a new version.

2.  **Code Diff:** The code diff shows a change in the `startDocument` method.
    *   The original `startDocument` method had no parameters and simply printed the XML declaration.
    *   The modified `startDocument` method now takes `XMLLocator` and `String encoding` as parameters and throws `XNIException`. It also now calls `fDocumentHandler.startDocument(locator, encoding)` if `fDocumentHandler` is not null and `fOnlyPassPrefixMappingEvents` is false.

3.  **Analysis:**
    *   The change in the `startDocument` method's signature indicates a change in the API. The new method signature suggests that the method now receives information about the document's location and encoding, which is useful for processing the XML document correctly.
    *   The addition of the `XNIException` to the `throws` clause suggests that the method can now potentially throw an exception, which could be due to an error during the document processing.
    *   The call to `fDocumentHandler.startDocument` indicates that the event is now being passed to a document handler, which is a common pattern in XML processing.
    *   The conditional check `fDocumentHandler != null && !fOnlyPassPrefixMappingEvents` suggests that the call to `fDocumentHandler.startDocument` is only made under certain conditions, which could be related to how the XML document is being processed.

4.  **Bug Fix Assessment:** While the changes are significant, they do not explicitly point to a bug fix. The changes appear to be related to API changes and the addition of new functionality. The addition of the `XNIException` to the `throws` clause could be related to error handling, but it is not clear if it is a fix for a specific bug. The commit message also does not suggest a bug fix.

**Conclusion:**

NotBuggy
"
xerces2-j,6658.json,42ca322ea8ac4be5de3f277f111700384c06f8d7,"@@ -1,4 +1,4 @@
     public XSSimpleType createTypeRestriction(String name, String targetNamespace,
-                                              short finalSet, XSSimpleType base) {
-        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false);
+                                              short finalSet, XSSimpleType base, XSObjectList annotations) {
+        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false, annotations);
     }",NotBuggy,"as part of annotation support, enabling schema datatype factories to create XSSimpleType implementations containing lists of annotations


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319276 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""as part of annotation support, enabling schema datatype factories to create XSSimpleType implementations containing lists of annotations"" suggests the introduction of a new feature or enhancement related to annotation handling within the schema datatype factories. It indicates that the code is being modified to allow `XSSimpleType` implementations to hold lists of annotations.

**Git Diff Analysis:**

The diff shows a change in the `createTypeRestriction` method. Specifically, a new parameter `XSObjectList annotations` has been added to the method signature. This parameter is then passed to the `XSSimpleTypeDecl` constructor. This confirms that the change is related to adding support for annotations to `XSSimpleType` objects.

**Reasoning:**

The commit message and the diff align perfectly. The diff introduces a new parameter to allow passing annotations during the creation of `XSSimpleType` objects, which directly corresponds to the commit message's description of enabling schema datatype factories to create `XSSimpleType` implementations containing lists of annotations. There's no indication of error handling, logical corrections, or exception handling improvements that would suggest a bug fix. The change appears to be a straightforward feature addition.

**Conclusion:**

NotBuggy
"
xerces2-j,3176.json,1130527a86137ec0fa5201483136b96ac2b25135,"@@ -1,8 +1,9 @@
     public Node getFirstChild() {
 
         if (needsSyncChildren()) {
             synchronizeChildren();
         }
-    	return firstChild;
+        makeChildNode();
+    	return (Node) value;
 
     }   // getFirstChild():Node",NotBuggy,"This commit brinds yet another optimization to this DOM implementation.
The whole idea is to try and avoid to always creating a Text node to hold
the value of an attribute. The DOM spec requires it, so we still have
to do it in case getFirstChild() is called for instance. The reason
attribute values are stored as a list of nodes is so that they can carry
more than a simple string. They can also contain EntityReference nodes.
However, most of the times people only have a single string that they
only set and get through Element.set/getAttribute or Attr.set/getValue.
In this new version, the Attr node has a value pointer which can either
be the String directly or a pointer to the first ChildNode. A flag tells
which one it currently is.
Note that while we try to stick with the direct String as much as possible
once we've switched to a node there is no going back. This is because we
have no way to know whether the application keeps referring to the node
we once returned.

The gain in memory varies on the density of attributes in the document.
But in the tests I've run I've seen up to 12% of memory gain. And the good
thing is that it also leads to a slight gain in speed because we allocate
fewer objects! I mean, that's until we have to actually create the node...

To avoid too much duplicated code, I got rid of ParentNode and renamed
ChildAndParentNode, which I never really liked, to ParentNode for
simplicity, this doesn't make much of a difference in memory usage because
there are only very objects that are only a Parent. This is only true now
because AttrImpl now inherits directly from NodeImpl and has its own
implementation of the ParentNode's node behavior. So there is still some
duplicated code there.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316776 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes a performance optimization in the DOM implementation, specifically aimed at reducing memory usage and improving speed by avoiding the creation of Text nodes for attribute values when possible. The diff shows a change in the `getFirstChild()` method of a node, where instead of returning `firstChild` directly, it now calls `makeChildNode()` and returns `value` cast to a `Node`. This suggests that the `value` field is now being used to store either a String or a ChildNode, as described in the commit message. The `makeChildNode()` method likely handles the creation of a Text node if it doesn't already exist.

The change in `getFirstChild()` is directly related to the optimization described in the commit message. It's not immediately apparent if this change is fixing a bug, but rather implementing a new strategy for storing attribute values. The commit message focuses on performance improvements and memory savings, not bug fixes.

**NotBuggy**
"
xerces2-j,6593.json,1b20fdfd0e92e36d5d66b17da8c7cbeb6e59c899,"@@ -1,4 +1,3 @@
-    public XSObjectList getAnnotations() {
-        // REVISIT: SCAPI: to implement
-        return fAnnotations;
-    }

+        public XSObjectList getAnnotations() {
+            return annotations;
+        }",NotBuggy,"Store and expose annotation for simpleTypes and facets. Add a new interface to
represent enumeration and pattern facets.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319311 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates changes related to annotations for simpleTypes and facets, along with a new interface for enumeration and pattern facets. The provided code diff shows a modification in the `getAnnotations()` method. The original implementation had a comment `// REVISIT: SCAPI: to implement` and returned `fAnnotations`. The modified version directly returns `annotations`.

The change suggests that the `getAnnotations()` method was previously a placeholder or incomplete implementation. The current change provides a concrete implementation by returning the `annotations` field. This aligns with the commit message's intention to store and expose annotations. Since the original code was incomplete and the new code provides a functional implementation, this can be considered a bug fix.

**Buggy**
"
xerces2-j,6062.json,5846060722ccc5d1a1b47182834caa718a3fa65b,"@@ -1,25 +1,25 @@
         protected boolean scanRootElementHook()
             throws IOException, XNIException {
-            if (fDTDValidator == null) {
-                fBindNamespaces = true;
-            } else if (!fDTDValidator.hasGrammar()) {
-                fBindNamespaces = true;
-                fPerformValidation = fDTDValidator.validate();
-                // re-configure pipeline
-                XMLDocumentSource source = fDTDValidator.getDocumentSource();
-                XMLDocumentHandler handler = fDTDValidator.getDocumentHandler();
-                source.setDocumentHandler(handler);
-                if (handler != null)
-                    handler.setDocumentSource(source);
-                fDTDValidator.setDocumentSource(null);
-                fDTDValidator.setDocumentHandler(null);
+            
+            if (fExternalSubsetResolver != null && !fSeenDoctypeDecl 
+                && (fValidation || fLoadExternalDTD)) {
+                scanStartElementName();
+                resolveExternalSubsetAndRead();
+                reconfigurePipeline();
+                if (scanStartElementAfterName()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
-
-            if (scanStartElement()) {
-                setScannerState(SCANNER_STATE_TRAILING_MISC);
-                setDispatcher(fTrailingMiscDispatcher);
-                return true;
+            else {
+                reconfigurePipeline();
+                if (scanStartElement()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
             return false;
 
         } // scanRootElementHook():boolean",NotBuggy,"Infrastructure work in the scanners to support the SAX2 Extensions 1.1
interface EntityResolver2, specifically getExternalSubset which
allows an application to provide an external subset for a document
which doesn't otherwise have one.

The scanners are now able to handle all three cases where a resolver
may be queried for external subset:

1) Neither an external or internal subset exist.
2) Only an internal subset exists.
3) No DOCTYPE declaration in the document.

In the third case, scanning of the root element is broken up into
three segments:

1) Scan the root element QName.
2) Query the resolver with the root name and base URI, if it 
locates an external subset then read it.
3) Scan the remainder of the start tag.

If an external subset is located the public id and system id
from the XMLInputSource returned from the resolver are
reported to doctypeDecl. This is what SAX expects.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319878 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes infrastructure work to support SAX2 Extensions 1.1, specifically the `getExternalSubset` method of `EntityResolver2`. The changes allow the scanner to handle cases where a resolver may be queried for an external subset, including when no DOCTYPE declaration is present.

The code diff shows a modification to the `scanRootElementHook` method. The original code block seems to be related to DTD validation and grammar handling. The new code block introduces a check for `fExternalSubsetResolver`, `fSeenDoctypeDecl`, `fValidation`, and `fLoadExternalDTD`. If these conditions are met, it calls `scanStartElementName()`, `resolveExternalSubsetAndRead()`, and `reconfigurePipeline()`. If not it calls `reconfigurePipeline()` and `scanStartElement()`.

The commit message and the code diff are highly related. The code changes implement the logic described in the commit message for handling external subsets, especially when no DOCTYPE declaration is present. The original code was replaced by a new implementation that handles the different cases described in the commit message. This looks like a new feature implementation rather than a bug fix.

**NotBuggy**
"
xerces2-j,8338.json,34aab31cc59d829a17b9f74ce192423af5788d9f,"@@ -1,3 +1,4 @@
-    public void startExternalSubset(Augmentations augs) throws XNIException {
+    public void startExternalSubset(XMLResourceIdentifier identifier,
+                                    Augmentations augs) throws XNIException {
         fReadingExternalDTD = true;
     } // startExternalSubset(Augmentations)",NotBuggy," Add implementation for baseURI and documentURI (DOM Level 3).
 Add XMLResourceIdentifier to startExternalSubset() method defined in  XNI XMLDTDHandler to
allow supporting baseURI for an external subset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318382 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states:

*   ""Add implementation for baseURI and documentURI (DOM Level 3)."" This suggests the commit is adding new functionality related to URI handling, specifically `baseURI` and `documentURI` as defined in DOM Level 3.
*   ""Add XMLResourceIdentifier to startExternalSubset() method defined in XNI XMLDTDHandler to allow supporting baseURI for an external subset."" This indicates a modification to the `startExternalSubset()` method in the `XNI XMLDTDHandler` interface. The addition of `XMLResourceIdentifier` suggests that the method will now receive information about the resource being processed, likely including its base URI.

**Git Diff Analysis:**

The diff shows a change to the `startExternalSubset()` method signature:

```diff
-    public void startExternalSubset(Augmentations augs) throws XNIException {
+    public void startExternalSubset(XMLResourceIdentifier identifier,
+                                    Augmentations augs) throws XNIException {
         fReadingExternalDTD = true;
     } // startExternalSubset(Augmentations)
```

The method now takes an `XMLResourceIdentifier` as its first argument. This aligns perfectly with the commit message's description of adding support for `baseURI` for external subsets.

**Reasoning:**

The commit message clearly indicates the addition of new functionality related to URI handling, specifically `baseURI` and `documentURI`. The diff confirms this by showing the modification of the `startExternalSubset()` method to accept an `XMLResourceIdentifier`. This identifier likely contains information about the resource's URI, allowing the system to resolve relative URIs correctly.

While this change introduces new functionality, it doesn't inherently suggest a bug fix. It's more of an enhancement or feature addition. There's no mention of correcting erroneous behavior or resolving a specific problem. The change seems to be about implementing a DOM Level 3 feature.

**Conclusion:**

NotBuggy
"
xerces2-j,1322.json,5453aaa1b3b312be38b47f18a2a5c6d9f874d3f0,"@@ -1,3 +1,3 @@
-    void setNextIndent( int indent )
+    public void setNextIndent( int indent )
     {
     }",NotBuggy,"Changed methods to public modifier


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315918 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Changed methods to public modifier"" indicates a change in the visibility of a method. The diff shows that the `setNextIndent` method's visibility was changed from package-private (default) to public. This change doesn't inherently suggest a bug fix. It's more likely a change in API design or accessibility requirements. There's no indication of error handling, logical corrections, or exception handling improvements.

**NotBuggy**"
xerces2-j,29.json,e94cfece717a676ae2799d5c030d2ed5982d1e27,"@@ -1,19 +1,20 @@
     public void internalEntityDecl(String name, XMLString text,
-                                   XMLString nonNormalizedText)
+                                   XMLString nonNormalizedText,
+                                   Augmentations augs)
         throws XNIException {
 
         printIndent();
         fOut.print(""internalEntityDecl("");
         fOut.print(""name="");
         printQuotedString(name);
         fOut.print(',');
         fOut.print(""text="");
         printQuotedString(text.ch, text.offset, text.length);
         fOut.print(',');
         fOut.print(""nonNormalizedText="");
         printQuotedString(nonNormalizedText.ch, nonNormalizedText.offset,
                           nonNormalizedText.length);
         fOut.println(')');
         fOut.flush();
 
     } // internalEntityDecl(String,XMLString)",NotBuggy,"committing some (slightly modified to take care of regressions) massive XNI changes on behalf of Andy Clark.  These make use of XMLResourceIdentifier in start.*Entity calls, and add Augmentations to various DTD Handler callbacks.  Things yet to do:
1.  Make Augmentation callbacks consistent.  They are null sometimes, not others; need to make them null by default.
2.  Propagate XMLResourceIdentifier to remaining methods it could and is not being used in.
3.  Tie XMLResourceIdentifier and XMLLocator together.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318116 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a large set of changes related to XNI (XML Native Interface), focusing on using `XMLResourceIdentifier` in `start.*Entity` calls and adding `Augmentations` to DTD handler callbacks. The provided diff shows a modification to the `internalEntityDecl` method, adding an `Augmentations` parameter.

The commit message mentions adding `Augmentations` to DTD Handler callbacks, and the diff directly reflects this by adding an `Augmentations` parameter to the `internalEntityDecl` method, which is part of the DTD handler.  The commit message also mentions that the Augmentations are not yet consistently handled, which suggests that the change is part of a larger effort to improve the handling of Augmentations, possibly to address inconsistencies or missing information. While the commit message doesn't explicitly state that this change fixes a bug, the mention of ""regressions"" and the need to make Augmentation callbacks consistent suggests that previous implementations might have had issues or were incomplete. The addition of Augmentations could be addressing a missing feature or incorrect behavior.

**Buggy**
"
xerces2-j,6592.json,962564ff9af52f2561b55bcf3fc5f3f21e1f43e9,"@@ -1,3 +1,3 @@
-		public XSAnnotation getAnnotation() {
-			return annotation;
-		}

+        public XSAnnotation getAnnotation() {
+            return (XSAnnotation) annotations.item(0);
+        }",NotBuggy,"Committing code and unit tests for the following line items from the Xerces 2.8.0 release plan:
- Expose annotations in XSAttributeUse and XSParticle
- Support lists of annotations for other XSObjects 

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@379204 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the implementation of new features related to annotations in the Xerces library, specifically focusing on exposing annotations in `XSAttributeUse` and `XSParticle`, and supporting lists of annotations for other `XSObjects`.

The code diff shows a change in the `getAnnotation()` method. The original code simply returned the `annotation` field. The modified code now returns the first item from an `annotations` list.

Reasoning:

1.  **Feature Implementation:** The commit message clearly states the intention to ""expose annotations"" and ""support lists of annotations."" The code change aligns with this intention by modifying the `getAnnotation()` method to retrieve the first annotation from a list of annotations instead of directly returning a single annotation field. This suggests the introduction of a new feature rather than fixing a bug.
2.  **No Error Handling:** The code change does not include any explicit error handling, such as checking if the `annotations` list is empty before attempting to access the first element. This could potentially lead to a `NullPointerException` or `IndexOutOfBoundsException` if the list is empty. However, the absence of error handling doesn't necessarily indicate a bug fix. It could simply mean that the error handling is done elsewhere or is not considered necessary in this specific context.
3.  **Logical Correction:** The change does not appear to be correcting any logical errors in the existing code. Instead, it's implementing a new way of accessing annotations, which is consistent with the feature being introduced.
4.  **Exception Handling:** There are no changes related to exception handling in the code diff.
5.  **Bug Fix Indicators:** There are no explicit indicators in the commit message or the code diff that suggest a bug fix. The changes are more aligned with the implementation of new features.

Based on the analysis, the code change is related to implementing new features as described in the commit message, rather than fixing a bug.

**Buggy** if the change was to fix a bug in the old code.
**NotBuggy** if the change was not to fix a bug in the old code.

**Conclusion:** NotBuggy
"
xerces2-j,3145.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public void setNotationName(String name) {
         
-        if (syncData()) {
+        if (needsSyncData()) {
             synchronizeData();
         }
     	notationName = name;
 
     } // setNotationName(String)",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a global renaming of internal methods. The provided diff shows a change from `syncData()` to `needsSyncData()`. This aligns with the commit message's intention of renaming internal methods. The change itself doesn't inherently suggest a bug fix, but rather a refactoring or renaming for clarity or consistency.

**NotBuggy**"
xerces2-j,3103.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public Node getFirstChild() {
 
-        if (syncChildren()) {
+        if (needsSyncChildren()) {
             synchronizeChildren();
         }
     	return firstChild;
 
     }   // getFirstChild():Node",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a global renaming of internal methods. The diff shows a change from `syncChildren()` to `needsSyncChildren()`. This aligns with the commit message's description of renaming. There's no indication of bug fixing, error handling, or logical corrections in the provided diff. The change appears to be purely a refactoring effort to rename a method.

**NotBuggy**"
xerces2-j,7895.json,97033b9bcdb0ebcdfc9a24df498cab2d12790b44,"@@ -1,13 +1,19 @@
     synchronized SAXParser getSAXParser() {
-        if (fSAXParser != null) return fSAXParser;
+        if (fSAXParser != null) {
+            SAXParser parser = (SAXParser) fSAXParser.get();
+            if (parser != null) {
+                return parser;
+            }
+        }
         // REVISIT:  when schema handles XML 1.1, will need to 
         // revisit this (and the practice of not prepending an XML decl to the annotation string
-        IntegratedParserConfiguration config = new IntegratedParserConfiguration(fSymbolTable);
+        XML11Configuration config = new XML11Configuration(fSymbolTable);
         // note that this should never produce errors or require
         // entity resolution, so just a barebones configuration with
         // a couple of feature  set will do fine
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.NAMESPACES_FEATURE, true);
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.VALIDATION_FEATURE, false);
-        fSAXParser = new SAXParser(config);
-        return fSAXParser;
+        SAXParser parser = new SAXParser(config);
+        fSAXParser = new SoftReference(parser);
+        return parser;
     }",NotBuggy,"Improvement for writing annotations:
- IntegratedParserConfiguration is ancient and slow. Use the better performing XML11Configuration.
- Parser instances use quite a bit of memory. Store them in SoftReferences so they can be
  reclaimed if there is high demand for memory.
- Use adopt node when possible to avoid creating a copy of the DOM before attaching it to the target.
- Turn off the deferred DOM feature. There's no benefit to having it on since the target 
  is another document. We would end up traversing the entire DOM even when calling adoptNode.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@348887 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes several improvements to annotation writing, focusing on performance and memory usage. Let's analyze the code diff:

1.  **Parser Configuration:** The code changes `IntegratedParserConfiguration` to `XML11Configuration`. The commit message explicitly states that `IntegratedParserConfiguration` is slow and `XML11Configuration` is better performing. This aligns with the commit message.
2.  **Parser Instance Storage:** The code now stores the `SAXParser` in a `SoftReference`. The commit message mentions that parser instances use a lot of memory and should be stored in `SoftReferences` to allow garbage collection. This aligns with the commit message. The code also checks if the `SoftReference` has been collected and re-creates the parser if necessary.
3.  **Return Value:** The code now returns the parser instance directly instead of storing it in a field.

The changes directly implement the improvements described in the commit message. The use of `SoftReference` suggests a potential memory leak or high memory consumption issue that was addressed. Therefore, the commit indicates a bug fix or at least a workaround for a performance bottleneck caused by high memory usage.

**Buggy**"
xerces2-j,8568.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,3 +1,3 @@
-    public void writeChildrenXml (XmlWriteContext context) throws IOException
-    {
-    }

+    public void setProperty(String propertyId, Object value)
+        throws XMLConfigurationException {
+    } // setProperty(String,Object)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Merge/move of Xerces 2 to the trunc!"" indicates a large-scale operation, likely involving moving or merging code. The diff shows a small change: the removal of `writeChildrenXml` and the addition of `setProperty`. While `setProperty` could potentially be related to fixing a configuration issue (which could be considered a bug fix), the commit message suggests a broader restructuring rather than a specific bug fix. The code change itself doesn't provide enough context to determine if it's directly related to resolving a bug. Therefore, based on the available information, it's difficult to definitively say that this commit is a bug fix.

**NotBuggy**"
hibernate-orm,7648.json,ff4774ad7ef488ce7505784f9591684d096bdb99,"@@ -1,3 +1,3 @@
-	public Schema getDefaultSchema() {
-		return implicitSchema;
+	public Namespace getDefaultNamespace() {
+		return implicitNamespace;
 	}",NotBuggy,"HHH-9974 - Rename org.hibernate.boot.model.relational.Schema to Namespace
",NotBuggy,"The commit message ""HHH-9974 - Rename org.hibernate.boot.model.relational.Schema to Namespace"" indicates a renaming of a class or concept from `Schema` to `Namespace`.

The provided diff shows a change from `getDefaultSchema()` to `getDefaultNamespace()` and `implicitSchema` to `implicitNamespace`. This aligns perfectly with the commit message, indicating a renaming operation. There's no indication of a bug fix here; it's purely a refactoring/renaming change.

**NotBuggy**
"
hibernate-orm,19080.json,1e44e7420b7af10adc9547a461f02a6d979624f1,"@@ -1,17 +1,13 @@
 	public void afterReassociate(Object entity, SessionImplementor session) {
 		if ( getEntityMetamodel().getBytecodeEnhancementMetadata().isEnhancedForLazyLoading() ) {
 			LazyAttributeLoadingInterceptor interceptor = getEntityMetamodel().getBytecodeEnhancementMetadata().extractInterceptor( entity );
 			if ( interceptor == null ) {
-				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor(
-						entity,
-						null,
-						session
-				);
+				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor( entity, session );
 			}
 			else {
 				interceptor.setSession( session );
 			}
 		}
 
 		handleNaturalIdReattachment( entity, session );
 	}",NotBuggy,"HHH-10267 - Support defining lazy attribute fetch groups
",Buggy,"The commit message ""HHH-10267 - Support defining lazy attribute fetch groups"" suggests the commit introduces or modifies functionality related to lazy attribute loading.

The code diff modifies the `afterReassociate` method. Specifically, it changes how the `injectInterceptor` method is called. The original code calls `injectInterceptor` with `null` as the second argument and `session` as the third argument. The modified code calls `injectInterceptor` with only `entity` and `session` as arguments. This suggests a change in the method signature of `injectInterceptor` or a change in how the interceptor is initialized. The removal of the `null` argument might indicate a correction or simplification of the interceptor injection process, potentially addressing a bug or inefficiency.

Given the context of lazy attribute loading and the modification to the interceptor injection process, it's plausible that this commit addresses a bug related to how lazy attributes are fetched or initialized.

**Buggy**
"
hibernate-orm,1521.json,0e17f9adf85e14cb34c385c7886ed0cddfabe0f6,"@@ -1,3 +1,3 @@
 	public <X> ValueExtractor<X> getExtractor(final JavaTypeDescriptor<X> javaTypeDescriptor) {
-		return (ValueExtractor<X>) new PGGeometryValueExtractor();
+		return (ValueExtractor<X>) new PGGeometryValueExtractor(javaTypeDescriptor);
 	}",NotBuggy,"HHH-6509 - Prepare introduction of Geolatte-geom library as model

Updates testing approach
Refactors ValueBinders and ValueExtractors
",Buggy,"The commit message ""HHH-6509 - Prepare introduction of Geolatte-geom library as model"" indicates a preparation for integrating the Geolatte-geom library. The message also mentions updating the testing approach and refactoring ValueBinders and ValueExtractors.

The provided code diff shows a change in the `getExtractor` method. Specifically, the `PGGeometryValueExtractor` is now being instantiated with a `javaTypeDescriptor` argument. This suggests a modification to how the `PGGeometryValueExtractor` handles different Java types, which could be related to ensuring correct data extraction when using the Geolatte-geom library.

The change itself doesn't scream ""bug fix"", but the refactoring of ValueExtractors, combined with the introduction of a new library, suggests that the previous implementation might have had limitations or potential issues that are being addressed with this change. Passing the `javaTypeDescriptor` to the `PGGeometryValueExtractor` suggests a more robust and type-aware extraction process, which could be fixing a bug or preventing future bugs related to type handling.

**Buggy**
"
hibernate-orm,19918.json,dc7cdf9d8803ff58191a35907414b7dd81210422,"@@ -1,3 +1,8 @@
 								public EntityDefinition toEntityDefinition() {
+									if ( getAssociationNature() != AssociationNature.ENTITY ) {
+										throw new WalkingException(
+												""Cannot build EntityDefinition from non-entity-typed attribute""
+										);
+									}
 									return (EntityPersister) aType.getAssociatedJoinable( ownerEntityPersister.getFactory() );
 								}",NotBuggy,"HHH-8276 - Integrate LoadPlans into UniqueEntityLoader (PoC)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HHH-8276 - Integrate LoadPlans into UniqueEntityLoader (PoC)"" suggests that the primary goal is to integrate LoadPlans into the UniqueEntityLoader. The ""(PoC)"" part indicates that this is a proof of concept, meaning it's likely experimental and may not be a complete or fully tested solution. The commit message doesn't explicitly mention bug fixes.

**Git Diff Analysis:**

The Git diff shows the addition of a check for `getAssociationNature() != AssociationNature.ENTITY`. If the association nature is not an entity, a `WalkingException` is thrown. This suggests that the original code might have been attempting to build an `EntityDefinition` from a non-entity-typed attribute, which is an invalid operation. The added check prevents this invalid operation and throws an exception, indicating a potential bug fix or at least a defensive programming measure to prevent unexpected behavior.

**Reasoning:**

The added check and exception handling suggest that the original code might have been susceptible to errors when dealing with non-entity-typed attributes. By adding this check, the code becomes more robust and prevents potential issues. This is a strong indicator of a bug fix or at least a preventative measure against a potential bug. The fact that the code now explicitly throws an exception when encountering an invalid state suggests that the previous behavior was either undefined or incorrect.

**Conclusion:**

**Buggy**
"
hibernate-orm,14224.json,208e789bb107503a5df7a62531570ececaa4c0ed,"@@ -1,10 +1,10 @@
 	public Object getProxy() {
 		try {
 			final ProxyConfiguration proxy = (ProxyConfiguration) proxyClass.newInstance();
-			proxy.$$_hibernate_set_interceptor( new PassThroughInterceptor( proxy, proxyClass.getName() ) );
+			proxy.$$_hibernate_set_interceptor( this.interceptor );
 			return proxy;
 		}
 		catch (Throwable t) {
 			throw new HibernateException( ""Unable to instantiate proxy instance"", t );
 		}
 	}",NotBuggy,"HHH-12786 Improve the basic proxy interceptor

Apart from cosmetic changes, we were testing in the equals() method that the
instance == the proxied object which will always be true.

We should use the argument of the equals() method instead to do the
comparison.

And we can do the comparison on the instance, instead of requiring
passing the proxiedObject into the interceptor.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""HHH-12786 Improve the basic proxy interceptor"" suggests a general improvement. However, the details reveal a specific issue in the `equals()` method of the proxy. It mentions that the `equals()` method was incorrectly comparing the instance with the proxied object, which would always be true. The commit message states that the argument of the `equals()` method should be used for comparison instead.

2.  **Git Diff:** The diff shows a change in how the interceptor is set on the proxy instance.
    *   The original code `proxy.$$_hibernate_set_interceptor( new PassThroughInterceptor( proxy, proxyClass.getName() ) );` creates a new `PassThroughInterceptor` instance and sets it on the proxy. The `PassThroughInterceptor` seems to be taking the `proxy` and `proxyClass.getName()` as arguments.
    *   The modified code `proxy.$$_hibernate_set_interceptor( this.interceptor );` sets an existing `interceptor` instance on the proxy.

3.  **Relevance:** The commit message describes a bug in the `equals()` method related to proxy comparison. The diff itself doesn't directly show the fix in the `equals()` method. However, the change in how the interceptor is set *could* be related to providing the correct context or data for the `equals()` method to function correctly. The commit message also mentions ""we can do the comparison on the instance, instead of requiring passing the proxiedObject into the interceptor."" This suggests that the change in interceptor setup is part of the fix. The original `PassThroughInterceptor` was likely used to pass the proxied object for comparison, but now the interceptor is directly available as `this.interceptor`.

4.  **Bug Fix Indicators:** The commit message explicitly states a problem with the `equals()` method's logic, which is a clear indicator of a bug. The code change, while not directly modifying the `equals()` method, appears to be a necessary step in resolving the bug by providing the correct interceptor with the necessary context.

**Conclusion:**

Based on the commit message describing a logical error in the `equals()` method and the code change that seems to be related to providing the correct context for the fix, I conclude that this commit indicates a bug fix.

**Buggy**
"
hibernate-orm,19882.json,4620ff4b4fe670bbf121b709e9da6b418c24ee43,"@@ -1,3 +1,3 @@
 			public EntityDefinition getEntityDefinition() {
-				return AbstractEntityPersister.this;
+				return entityPersister;
 			}",NotBuggy,"HHH-7841 - Redesign Loader
",NotBuggy,"The commit message ""HHH-7841 - Redesign Loader"" suggests a broader change than a simple bug fix. The code change replaces `AbstractEntityPersister.this` with `entityPersister`. Without knowing the context of `AbstractEntityPersister` and `entityPersister`, it's difficult to definitively say if this is a bug fix. However, the change itself seems like a refactoring or redesign, aligning with the commit message. There's no explicit error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**
"
hibernate-orm,12109.json,36dbd3a06e5867cc830b8537f6e24618a8a47137,"@@ -1,10 +1,12 @@
-	public InputStream getAsciiStream() throws SQLException {
+	private InputStream getStream() throws SQLException {
 		try {
-			if (needsReset) reader.reset();
+			if (needsReset) {
+				stream.reset();
+			}
 		}
-		catch (IOException ioe) {
+		catch ( IOException ioe) {
 			throw new SQLException(""could not reset reader"");
 		}
 		needsReset = true;
-		return new ReaderInputStream(reader);
+		return stream;
 	}",NotBuggy,"HHH-2412 - Support for JDBC4


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17767 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message ""HHH-2412 - Support for JDBC4"" indicates a feature enhancement related to JDBC4 support. The code diff shows a change in how a stream is handled, specifically adding a `stream.reset()` call within a `try-catch` block. The `catch` block handles an `IOException` and throws an `SQLException`. The original code used `reader.reset()` and `ReaderInputStream`, while the modified code uses `stream.reset()` and returns `stream`.

The change appears to be related to ensuring the stream can be reset properly, which could be necessary for JDBC4 compatibility. The handling of `IOException` and throwing `SQLException` suggests a potential issue with stream resetting that needed to be addressed. This looks like a bug fix related to stream handling.

**Buggy**
"
hibernate-orm,12790.json,617975fefb42fe5bd6d3ae7893e96077cb44c84a,"@@ -1,17 +1,17 @@
 	public Blob createBlob(InputStream inputStream, long length) {
 		try {
 			Blob blob = createBlob();
 			OutputStream byteStream = blob.setBinaryStream( 1 );
 			StreamUtils.copy( inputStream, byteStream );
 			byteStream.flush();
 			byteStream.close();
 			// todo : validate length written versus length given?
 			return blob;
 		}
 		catch ( SQLException e ) {
-			throw new IllegalStateException( ""Unable to prepare BLOB binary stream for writing"", e );
+			throw new JDBCException( ""Unable to prepare BLOB binary stream for writing"",e );
 		}
 		catch ( IOException e ) {
-			throw new IllegalStateException( ""Unable to write stream contents to BLOB"", e );
+			throw new HibernateException( ""Unable to write stream contents to BLOB"", e );
 		}
 	}",NotBuggy,"HHH-4560 - JDBC4 support inadvertently missed 1.4 compatibility


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17951 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message states that JDBC4 support inadvertently missed 1.4 compatibility. The diff shows changes in exception handling within the `createBlob` method. Specifically, `IllegalStateException` is replaced with `JDBCException` for `SQLExceptions` and `HibernateException` for `IOExceptions`. These changes suggest an attempt to align exception types with the specific context (JDBC operations and Hibernate operations, respectively) and potentially address compatibility issues. The change from `IllegalStateException` to more specific exception types indicates a refinement of error handling, which is often associated with bug fixes or improvements.

**Buggy**
"
hibernate-orm,3126.json,9e063ffa2577f06d98a9e912bb16d20424df8d6d,"@@ -1,3 +1,3 @@
 	public Getter getGetter(Class clazz) throws PropertyNotFoundException, MappingException {
-		return getPropertyAccessor(clazz).getGetter( clazz, name );
+		return getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter();
 	}",NotBuggy,"HHH-9837 - Remove reliance during annotation binding on org.hibernate.internal.util.ClassLoaderHelper
HHH-9841 - Redesign org.hibernate.property.PropertyAccessorFactory
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HHH-9837 - Remove reliance during annotation binding on org.hibernate.internal.util.ClassLoaderHelper"" and ""HHH-9841 - Redesign org.hibernate.property.PropertyAccessorFactory"" suggests a refactoring and redesign effort. It indicates a move away from using `org.hibernate.internal.util.ClassLoaderHelper` during annotation binding and a redesign of `org.hibernate.property.PropertyAccessorFactory`. These changes are more architectural and structural rather than direct bug fixes.

**Git Diff Analysis:**

The provided Git diff shows a change in how a `Getter` is obtained.

-   **Old Code:** `getPropertyAccessor(clazz).getGetter( clazz, name )`
-   **New Code:** `getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter()`

The change involves replacing `getPropertyAccessor(clazz).getGetter( clazz, name )` with `getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter()`. This suggests a change in the way property accessors are handled, potentially as part of the redesign mentioned in the commit message. The new code appears to be using a `PropertyAccessStrategy` to build a `PropertyAccess` object, from which the `Getter` is then obtained. This looks like a refactoring to decouple the getter retrieval process.

**Reasoning:**

The commit message describes refactoring and redesign. The code diff shows a change in how a `Getter` is retrieved, which aligns with the redesign of `org.hibernate.property.PropertyAccessorFactory`. There's no explicit indication of a bug being fixed. The changes seem to be related to improving the architecture and decoupling components. The change in getter retrieval could be part of a larger effort to improve the flexibility and maintainability of the property access mechanism.

**Conclusion:**

NotBuggy
"
hibernate-orm,30500.json,2f92109c48fb487870af6d93ba8dc884838179d0,"@@ -1,13 +1,11 @@
-	public static Map<String, ParsedPersistenceXmlDescriptor> parse(
+	public static Map<String,ParsedPersistenceXmlDescriptor> parse(
 			URL persistenceXmlUrl,
 			PersistenceUnitTransactionType transactionType,
-			Map integration
-	) {
+			Map integration) {
 		PersistenceXmlParser parser = new PersistenceXmlParser(
 				ClassLoaderServiceImpl.fromConfigSettings( integration ),
 				transactionType
 		);
 
-		parser.doResolve( integration );
-		return parser.persistenceUnits;
+		return parser.doResolve( integration );
 	}",NotBuggy,"Revert ""HHH-11845 - Warn user when multiple persistence-units use the same name""

This reverts commit 1242fd9580fdfb2a8b14ebb419ccc5d40ae01a01.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Revert ""HHH-11845 - Warn user when multiple persistence-units use the same name"""" clearly indicates that this commit is undoing a previous commit (1242fd9580fdfb2a8b14ebb419ccc5d40ae01a01) that implemented a warning mechanism for duplicate persistence unit names. Reverting a feature often happens when the feature introduced a bug or had unintended consequences.

2.  **Git Diff:** The diff shows a change in the `parse` method. Specifically, the line `parser.doResolve( integration );` is now the return value of the method. In the original code, the return value was `parser.persistenceUnits`. This suggests that the `doResolve` method now returns the `persistenceUnits` map directly, instead of the parser holding it as a field. The removal of `parser.persistenceUnits` as the return value is a direct consequence of reverting the previous commit.

**Reasoning:**

Since the commit message explicitly states that it's a revert, and the code changes reflect the removal of the functionality introduced by the reverted commit, it's highly likely that the original commit (1242fd9580fdfb2a8b14ebb419ccc5d40ae01a01) introduced a bug or had undesirable side effects. Reverting it is a way to remove the problematic code.

**Conclusion:**

**Buggy**
"
hibernate-orm,24481.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
-		public boolean isPassNullsEnabled() {
-			return nativeParamRegistration.isPassNullsEnabled();
-		}

+	public boolean isPassNullsEnabled() {
+		return nativeParamRegistration.isPassNullsEnabled();
+	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,"The commit message ""HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first successful full compile of consolidated hibernate-core)"" indicates a merge operation in preparation for the 6.0 feature branch, specifically merging `hibernate-entitymanager` into `hibernate-core`. The goal is to achieve a successful compilation of the consolidated `hibernate-core`.

The provided diff shows a minor change in the `isPassNullsEnabled()` method. The only difference is the removal of a leading tab character on the first line of the method definition. This change is likely a consequence of code formatting adjustments during the merge process to ensure consistent code style within the consolidated codebase.

The change itself doesn't suggest a bug fix. It's more related to code style or formatting adjustments that are often necessary during large-scale merge operations. The commit message also doesn't mention any bug fixes. It focuses on the merge and compilation aspects.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**
"
hibernate-orm,13970.json,ba3359fe62be258638554fe23a2a0a6a50f7e732,"@@ -1,3 +1,3 @@
-		public boolean isEntityClass(CtClass classDescriptor) {
-			return wrappedContext.isEntityClass( classDescriptor );
-		}

+	public boolean isEntityClass(CtClass classDescriptor) {
+		return enhancementContext.isEntityClass( new UnloadedCtClass( classDescriptor ) );
+	}",NotBuggy,"HHH-11152: Added BytecodeProvider based on Byte Buddy
",NotBuggy,"The commit message ""HHH-11152: Added BytecodeProvider based on Byte Buddy"" indicates the addition of a new feature or component related to bytecode generation using Byte Buddy.

The code diff shows a change in the `isEntityClass` method. Specifically, the argument passed to `wrappedContext.isEntityClass` is changed from `classDescriptor` to `new UnloadedCtClass( classDescriptor )`. This suggests a modification to how entity classes are identified, potentially to accommodate the new Byte Buddy-based bytecode provider. The change involves wrapping the `classDescriptor` in an `UnloadedCtClass` before passing it to the `enhancementContext`. This could be related to how Byte Buddy handles class descriptors or how the `enhancementContext` expects them.

The change doesn't explicitly indicate a bug fix. It seems more like an adaptation or adjustment to the existing code to integrate with the new Byte Buddy-based bytecode provider. It's possible that the original code didn't work correctly with the new provider, and this change is necessary to ensure proper entity class identification. However, without more context, it's difficult to definitively say whether this is a bug fix or simply a necessary adaptation.

Given the information available, it's more likely that this change is part of the feature implementation rather than a bug fix.

**NotBuggy**"
hibernate-orm,9325.json,08747fc2f4b33414ef05ac039b67e1d33c14db5f,"@@ -1,6 +1,6 @@
 	private Object resolveAny(String entityName, Serializable id, SharedSessionContractImplementor session)
 			throws HibernateException {
 		return entityName==null || id==null
 				? null
-				: session.internalLoad( entityName, id, false, false );
+				: session.internalLoad( entityName, id, eager, false );
 	}",NotBuggy,"HHH-13094 - Respect @Any.fetch setting to FetchType.EAGER
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""HHH-13094 - Respect @Any.fetch setting to FetchType.EAGER"" indicates that the change aims to make the `@Any` annotation's `fetch` attribute (specifically, when set to `FetchType.EAGER`) actually be respected during entity loading.  This suggests a previous behavior where the `FetchType.EAGER` setting was being ignored or not correctly applied.

**Git Diff Analysis:**

The diff shows a change within the `resolveAny` method.  The original code:

```java
session.internalLoad( entityName, id, false, false );
```

is modified to:

```java
session.internalLoad( entityName, id, eager, false );
```

The key change is the replacement of `false` with `eager` as the third argument to `session.internalLoad`.  Based on the commit message, it's highly likely that `eager` is a boolean variable that reflects the `FetchType` setting from the `@Any` annotation.  The original code was forcing a `false` value (i.e., lazy loading) regardless of the `@Any.fetch` setting. The modified code now passes the correct `eager` flag to `internalLoad`, thus respecting the specified fetch type.

**Reasoning:**

The commit message and the code change are strongly aligned. The original code was ignoring the `@Any.fetch` setting, effectively treating all `@Any` associations as lazy-loaded. The change fixes this by passing the `eager` flag to the `internalLoad` method, ensuring that the specified fetch type is honored. This correction directly addresses the problem described in the commit message. The fact that the original code was ignoring a user-specified setting suggests a bug.

**Conclusion:**

**Buggy**
"
hibernate-orm,15177.json,ae43670290d05db28c322acb67431fe88db537d2,"@@ -1,3 +1,3 @@
-	public String getCreateTemporaryTablePostfix() {
+	public String getCreateIdTableStatementOptions() {
 		return "" on commit preserve rows"";
 	}",NotBuggy,"HHH-9724 - More complete ""temp table"" coverage and allow Dialects to influence which strategy is used
",Buggy,"The commit message ""HHH-9724 - More complete ""temp table"" coverage and allow Dialects to influence which strategy is used"" suggests enhancements related to temporary table handling in Hibernate, potentially involving dialect-specific configurations.

The code diff replaces `getCreateTemporaryTablePostfix()` with `getCreateIdTableStatementOptions()`. The original method returned a fixed string "" on commit preserve rows"". The new method name suggests a more generalized purpose related to ID table creation options, rather than being specific to temporary tables.

The change from a method returning a fixed string related to temporary tables to a more generic method for ID table options, along with the commit message mentioning ""more complete 'temp table' coverage and allow Dialects to influence which strategy is used"", implies that the original implementation might have been incomplete or inflexible, and this change aims to address those limitations. This could be considered a bug fix or enhancement to address previous shortcomings in temporary table handling.

**Buggy**
"
hibernate-orm,11271.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
 	public int executeNativeUpdate(NativeSQLQuerySpecification specification, QueryParameters queryParameters) throws HibernateException {
-		return sessionImplementor.executeNativeUpdate( specification, queryParameters );
+		return delegate.executeNativeUpdate( specification, queryParameters );
 	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first successful full compile of consolidated hibernate-core)"" suggests a large-scale refactoring or merging of codebases (hibernate-entitymanager into hibernate-core). The primary goal seems to be compilation success after the merge, rather than fixing a specific bug.

2.  **Git Diff:** The diff shows a single line change within the `executeNativeUpdate` method. The change replaces `sessionImplementor.executeNativeUpdate` with `delegate.executeNativeUpdate`. This indicates a change in how the native update execution is delegated. It's likely part of the broader refactoring implied by the commit message.

3.  **Relevance and Bug Fix Indicators:**
    *   The change from `sessionImplementor` to `delegate` suggests a shift in responsibility or a change in the internal architecture after the merge.
    *   There's no explicit error handling, logical correction, or exception handling improvement visible in this specific diff.
    *   The commit message doesn't mention any bug being fixed. It focuses on the merge and compilation aspect.
    *   The change is likely related to adapting the code to the new merged structure, ensuring that the `executeNativeUpdate` method still functions correctly after the integration.

**Conclusion:**

Based on the commit message and the code diff, the change appears to be part of a larger refactoring effort to merge `hibernate-entitymanager` into `hibernate-core`. The change in delegation (`sessionImplementor` to `delegate`) is likely a consequence of this merge. There's no clear indication of a specific bug being fixed in this particular diff.

**NotBuggy**
"
presto,14100.json,8cb9c88804a6c7f7cc995fc729b862933e8d6b4d,"@@ -1,4 +1,4 @@
-    public static long cardinality(@SqlType(HyperLogLogType.NAME) Slice serializedHll)
+    public static long cardinality(@SqlType(StandardTypes.HYPER_LOG_LOG) Slice serializedHll)
     {
         return HyperLogLog.newInstance(serializedHll).cardinality();
     }",NotBuggy,"Move type names to their own class
",NotBuggy,"The commit message ""Move type names to their own class"" suggests a refactoring or code organization change. The diff shows a change from `HyperLogLogType.NAME` to `StandardTypes.HYPER_LOG_LOG`. This change aligns with the commit message, indicating that type names are being moved to a dedicated class (`StandardTypes`). There's no indication of a bug fix in this change; it's purely a code reorganization.

**NotBuggy**"
presto,34438.json,279eb7e2af95bddf5d357327eaf249021ead9f8d,"@@ -1,10 +1,5 @@
     public <T> Optional<LocalProperty<T>> translate(Function<E, Optional<T>> translator)
     {
-        Optional<T> translated = translator.apply(column);
-
-        if (translated.isPresent()) {
-            return Optional.of(new SortingProperty<>(translated.get(), order));
-        }
-
-        return Optional.empty();
+        return translator.apply(column)
+                .map(translated -> new SortingProperty<>(translated, order));
     }",NotBuggy,"Simplify the use of Optionals
",NotBuggy,"Okay, let's analyze this Git commit.

**Reasoning:**

1.  **Commit Message:** The commit message ""Simplify the use of Optionals"" suggests a refactoring to make the code more concise by leveraging the features of `Optional`. It doesn't explicitly mention a bug fix.

2.  **Code Diff:** The code diff shows a change in how the `Optional` returned by `translator.apply(column)` is handled.
    *   **Original Code:** The original code explicitly checks if the `Optional` `translated` is present using `isPresent()`. If it is, it creates a `SortingProperty` and wraps it in another `Optional`. Otherwise, it returns an empty `Optional`.
    *   **New Code:** The new code uses `Optional.map()` to achieve the same result in a more concise way. The `map()` function automatically handles the case where the `Optional` is empty, preventing the need for an explicit `isPresent()` check.

3.  **Bug Fix Assessment:** The change is purely a simplification of the code. It replaces a verbose `if` statement with a more streamlined `Optional.map()` operation. There is no indication of error handling, logical correction, or exception handling improvements. The code's functionality remains the same; it's just written more elegantly.

**Conclusion:**

NotBuggy
"
presto,26381.json,472538a02722c5409649533f9a602b5b76d9dd2d,"@@ -1,23 +1,23 @@
     public Configuration getConfiguration(FileSystemContext context, URI uri)
     {
         @SuppressWarnings(""resource"")
         Configuration config = new CachingJobConf((factoryConfig, factoryUri) -> {
             try {
                 FileSystem fileSystem = (new Path(factoryUri)).getFileSystem(hiveHdfsConfiguration.getConfiguration(context, factoryUri));
                 checkState(fileSystem instanceof ExtendedFileSystem);
-                return new CachingFileSystem(
+                return new FileMergeCachingFileSystem(
                         factoryUri,
                         factoryConfig,
                         cacheManager,
                         (ExtendedFileSystem) fileSystem,
                         cacheValidationEnabled);
             }
             catch (IOException e) {
                 throw new PrestoException(GENERIC_INTERNAL_ERROR, ""cannot create caching FS"", e);
             }
         });
         Configuration defaultConfig = hiveHdfsConfiguration.getConfiguration(context, uri);
 
         copy(defaultConfig, config);
         return config;
     }",NotBuggy,"Add Alluxio based data caching

This change enables Alluxio client-side local data cache on the worker.
This change also does a bit of refactoring of caching related pieces.
To enable Alluxio local cache, use following configuration
cache.enabled=true
cache.type=ALLUXIO

Co-authored-by: Bin Fan <fanbin103@gmail.com>
Co-authored-by: Bin Feng <binfeng@alluxio.com>
",NotBuggy,"The commit message indicates the addition of Alluxio-based data caching and some refactoring. The diff shows a change from `CachingFileSystem` to `FileMergeCachingFileSystem`. This change, while seemingly related to caching, doesn't explicitly indicate a bug fix. The commit message focuses on adding a new feature (Alluxio caching) and refactoring, not on resolving a defect. The change in class name suggests a modification or enhancement of the caching mechanism rather than a correction of faulty behavior.

**NotBuggy**"
presto,37097.json,9ffb60c93e86e94c0c5a5a4bc21d61337344615d,"@@ -1,4 +1,4 @@
-            public MetadataReader createMetadataReader()
-            {
-                return new DwrfMetadataReader();
-            }

+        public MetadataWriter createMetadataWriter()
+        {
+            return new DwrfMetadataWriter();
+        }",NotBuggy,"Add OrcEncoding enum to differentiate between ORC and DWRF
",NotBuggy,"The commit message ""Add OrcEncoding enum to differentiate between ORC and DWRF"" suggests the introduction of a new enumeration to distinguish between ORC and DWRF file formats. However, the provided diff only shows a change in a method name from `createMetadataReader` to `createMetadataWriter` and a corresponding change from `DwrfMetadataReader` to `DwrfMetadataWriter`.

This diff doesn't seem to be directly related to adding an enum or differentiating between ORC and DWRF encodings. The change appears to be more about creating a writer instead of a reader, which might be part of a larger feature or refactoring, but it doesn't inherently indicate a bug fix. The commit message and the diff do not align.

**NotBuggy**"
presto,26237.json,c8447e695d5434c811fc53d4920eaa8df9ec8bd0,"@@ -1,12 +1,4 @@
-    private void startBackupCleanup()
+    public void startBackupCleanup()
     {
-        scheduler.scheduleWithFixedDelay(() -> {
-            try {
-                cleanBackupShards();
-            }
-            catch (Throwable t) {
-                log.error(t, ""Error cleaning backup shards"");
-                backupJobErrors.update(1);
-            }
-        }, 0, backupCleanerInterval.toMillis(), MILLISECONDS);
+        scheduler.submit(this::runBackupCleanup);
     }",NotBuggy,"Expose recovery and cleaning methods through JMX
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Expose recovery and cleaning methods through JMX"" suggests that the primary goal is to make existing methods accessible via JMX (Java Management Extensions) for monitoring and management purposes. This doesn't inherently imply a bug fix. It's more likely a feature enhancement or operational improvement.

**Diff Analysis:**

The diff shows a change in the `startBackupCleanup()` method.

1.  **Visibility Change:** The method's visibility is changed from `private` to `public`. This aligns with the commit message's intention to expose the method, presumably for JMX access.
2.  **Scheduling Mechanism:** The original code used `scheduler.scheduleWithFixedDelay` to repeatedly execute the `cleanBackupShards()` method. The new code uses `scheduler.submit(this::runBackupCleanup)`. This change suggests a shift from a fixed-delay scheduled task to a single, submitted task. The original code also had error handling within the scheduled task, logging errors and updating a metric.

**Reasoning:**

The change from `scheduleWithFixedDelay` to `submit` is significant. The original code was designed to run the cleanup task repeatedly at a fixed interval. The new code submits the task once. This could be a bug fix if the original intent was for the cleanup to run only once, or if the fixed-delay scheduling was causing issues (e.g., overlapping executions, resource contention). However, without more context, it's difficult to say definitively. The removal of the error handling within the scheduled task is also concerning. If `runBackupCleanup` throws an exception, it will no longer be caught and logged, potentially masking errors.

Given the change in scheduling and the removal of error handling within the scheduled task, it's possible that the original implementation had a bug related to how the cleanup task was scheduled or handled errors.

**Conclusion:**

**Buggy**
"
presto,29934.json,db03518bf342c8fb74ace9ba25f2b0c78991953a,"@@ -1,24 +1,24 @@
-    protected void renameTable(String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
+    protected void renameTable(JdbcIdentity identity, String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
     {
-        try (Connection connection = connectionFactory.openConnection()) {
+        try (Connection connection = connectionFactory.openConnection(identity)) {
             DatabaseMetaData metadata = connection.getMetaData();
             String schemaName = oldTable.getSchemaName();
             String tableName = oldTable.getTableName();
             String newSchemaName = newTable.getSchemaName();
             String newTableName = newTable.getTableName();
             if (metadata.storesUpperCaseIdentifiers()) {
                 schemaName = schemaName.toUpperCase(ENGLISH);
                 tableName = tableName.toUpperCase(ENGLISH);
                 newSchemaName = newSchemaName.toUpperCase(ENGLISH);
                 newTableName = newTableName.toUpperCase(ENGLISH);
             }
             String sql = format(
                     ""ALTER TABLE %s RENAME TO %s"",
                     quoted(catalogName, schemaName, tableName),
                     quoted(catalogName, newSchemaName, newTableName));
             execute(connection, sql);
         }
         catch (SQLException e) {
             throw new PrestoException(JDBC_ERROR, e);
         }
     }",NotBuggy,"Pass JdbcIdentity To DriverConnectionFactory#openConnection

Co-Authored-By: Ke Wang <ke1024@fb.com>
",NotBuggy,"The commit message ""Pass JdbcIdentity To DriverConnectionFactory#openConnection"" suggests that the `JdbcIdentity` object is now being passed to the `openConnection` method of the `DriverConnectionFactory`.

The diff shows that the `renameTable` method's signature has changed to accept a `JdbcIdentity` parameter. Consequently, the call to `connectionFactory.openConnection()` now includes the `identity` object.

The change doesn't explicitly indicate a bug fix. It seems like a feature enhancement or a requirement to pass identity information when opening a connection. There is no error handling update, logical correction, or exception-handling improvement that would suggest a bug fix.

**NotBuggy**
"
presto,17845.json,1e1df56d263e3338de3adc636df13ec5800e1413,"@@ -1,4 +1,4 @@
-    public ConnectorSplitManager getSplitManager()
+    public TransactionalConnectorSplitManager getSplitManager()
     {
         return splitManager;
     }",NotBuggy,"Make SystemTables transactional
",Buggy,"The commit message ""Make SystemTables transactional"" suggests a change related to making system tables transactional. The provided code diff replaces `ConnectorSplitManager` with `TransactionalConnectorSplitManager`. This change aligns with the commit message, indicating that the split manager is being updated to support transactional operations on system tables. This could be part of a bug fix or a new feature implementation. Since the change directly relates to the transactional nature of system tables as described in the commit message, and it's plausible that the previous implementation lacked transactional support, this change is likely addressing a bug or deficiency.

**Buggy**
"
presto,14579.json,7cf87bd218ba996089fd42fc2796e53c69d40d7b,"@@ -1,4 +1,4 @@
-    public static double log10(@SqlType(DoubleType.class) double num)
+    public static double log10(@SqlType(DoubleType.NAME) double num)
     {
         return Math.log10(num);
     }",NotBuggy,"Change @SqlType to take a String
",NotBuggy,"The commit message ""Change @SqlType to take a String"" indicates a modification in how the `@SqlType` annotation is used, specifically changing the expected input from a class to a String.

The provided code diff shows that the `@SqlType` annotation's argument has been changed from `DoubleType.class` to `DoubleType.NAME`. This aligns perfectly with the commit message, suggesting a change in the expected type for the annotation. There is no indication of bug fixing in the traditional sense (error handling, logical correction, exception handling). It's a change in the API or usage of the `@SqlType` annotation.

**NotBuggy**
"
presto,23057.json,41c77cf7606f1e111c66c9a5ee720577f6268bac,"@@ -1,8 +1,8 @@
     public String toString()
     {
-        return MoreObjects.toStringHelper(this)
+        return toStringHelper(this)
                 .add(""major"", major)
                 .add(""minor"", minor)
                 .add(""update"", update)
                 .toString();
     }",NotBuggy,"Static import toStringHelper
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Static import toStringHelper"" suggests that the change involves replacing a qualified call to `MoreObjects.toStringHelper` with a static import of the `toStringHelper` method. This usually aims to improve code readability by removing unnecessary verbosity.

**Git Diff Analysis:**

The diff shows a single change within the `toString()` method:

`-       return MoreObjects.toStringHelper(this)`
`+       return toStringHelper(this)`

This confirms that the code has been modified to use `toStringHelper` directly, implying a static import was added elsewhere in the file (though the diff doesn't explicitly show the import statement).

**Reasoning:**

1.  **Relevance:** The commit message accurately describes the code change. The code was refactored to use a static import.
2.  **Bug Fix Indicators:** This change doesn't inherently indicate a bug fix. It's a refactoring change to improve code style and readability. There's no error handling, logical correction, or exception handling involved.

**Conclusion:**

**NotBuggy**
"
presto,18660.json,54478b6f30fab15d960676ad2dbc1c0005c1fb47,"@@ -1,8 +1,7 @@
-    public Optional<PlanNode> apply(PlanNode node, Context context)
+    public Optional<PlanNode> apply(ApplyNode applyNode, Captures captures, Context context)
     {
-        ApplyNode applyNode = (ApplyNode) node;
         if (applyNode.getSubqueryAssignments().isEmpty()) {
             return Optional.of(applyNode.getInput());
         }
         return Optional.empty();
     }",NotBuggy,"Parameterize Rule with pattern output type, remove root node casts
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Parameterize Rule with pattern output type, remove root node casts"" suggests two main changes:

1.  **Parameterize Rule with pattern output type:** This implies that the `apply` method of a rule is being updated to accept a more specific type related to pattern matching or rule application.
2.  **Remove root node casts:** This indicates that explicit type casting of a root node (likely a `PlanNode` in this context) is being removed. This often happens when the type system is improved or when the code is made more generic.

**Git Diff Analysis:**

The Git diff shows the following changes to the `apply` method:

*   **Parameter Change:** The `apply` method's signature has changed from `apply(PlanNode node, Context context)` to `apply(ApplyNode applyNode, Captures captures, Context context)`.  This confirms the ""Parameterize Rule with pattern output type"" part of the commit message. The method now takes `ApplyNode` directly, which is more specific than `PlanNode`, and also takes a `Captures` object, likely related to pattern matching results.
*   **Removed Cast:** The line `ApplyNode applyNode = (ApplyNode) node;` has been removed. This confirms the ""remove root node casts"" part of the commit message.

**Reasoning:**

The changes strongly suggest a bug fix or at least a code improvement related to type safety and correctness:

*   **Type Safety:** Removing the cast `(ApplyNode) node` indicates that the code was previously relying on an assumption that `node` would always be an `ApplyNode`. If this assumption was not always true, it could lead to runtime `ClassCastException` errors. By changing the method signature to accept `ApplyNode` directly, the compiler can now enforce this type constraint, preventing potential runtime errors.
*   **Pattern Matching and Rule Application:** The introduction of the `Captures` parameter suggests that the rule is now being applied in the context of pattern matching. This could be part of a larger effort to improve the rule engine or query optimizer.

The removal of the cast is a strong indicator of a bug fix because casts are often used to circumvent the type system, and removing them usually makes the code more robust. The more specific type in the method signature also contributes to this robustness.

**Conclusion:**

**Buggy**
"
presto,10382.json,7c58b7cfb786bc2dc1309c42e18479577f0d2ffc,"@@ -1,4 +1,4 @@
-        public int size()
-        {
-            return delegate.size();
-        }

+    public int size()
+    {
+        return delegate.size();
+    }",NotBuggy,"Rename LinkedHashQueue to FifoQueue

Also make it a top-level class
",NotBuggy,"The commit message indicates a renaming and refactoring operation, specifically renaming `LinkedHashQueue` to `FifoQueue` and making it a top-level class. The provided diff snippet only shows a minor formatting change (indentation). This change doesn't seem related to bug fixing but rather code style improvement. The diff doesn't provide enough context to determine if the renaming and making it a top-level class are bug fixes. Therefore, based on the given information, it's unlikely a bug fix.

**NotBuggy**"
presto,11220.json,efd979afa5efcf616ac1ae0b806fcad3a0cf1ab0,"@@ -1,4 +1,4 @@
-    public void checkCanShowRoleGrants(TransactionId transactionId, Identity identity, String catalogName)
+    public void checkCanShowCurrentRoles(TransactionId transactionId, Identity identity, AccessControlContext context, String catalogName)
     {
-        denyShowRoleGrants(catalogName);
+        denyShowCurrentRoles(catalogName);
     }",NotBuggy,"Add AccessControlContext to store client information for security purposes
",NotBuggy,"The commit message indicates the addition of `AccessControlContext` to store client information for security purposes.
The code diff shows a change in the method signature from `checkCanShowRoleGrants(TransactionId transactionId, Identity identity, String catalogName)` to `checkCanShowCurrentRoles(TransactionId transactionId, Identity identity, AccessControlContext context, String catalogName)`. The `AccessControlContext` parameter has been added, which aligns with the commit message. Also, the method name changed from `checkCanShowRoleGrants` to `checkCanShowCurrentRoles` and `denyShowRoleGrants` to `denyShowCurrentRoles`.

The addition of `AccessControlContext` suggests an enhancement for security purposes, and the renaming of methods indicates a refactoring or change in functionality. There is no direct indication of a bug fix in the code diff.

**NotBuggy**"
presto,32284.json,2d550a5123000f7965f16c0c5c594fc030e349a0,"@@ -1,4 +1,4 @@
     public List<String> listSchemaNames(ConnectorSession session)
     {
-        return ImmutableList.of(SCHEMA_NAME);
+        return ImmutableList.copyOf(schemas);
     }",NotBuggy,"Support CREATE SCHEMA in blackhole connector
",NotBuggy,"The commit message ""Support CREATE SCHEMA in blackhole connector"" suggests the addition of functionality to create schemas in the blackhole connector. The diff shows that the `listSchemaNames` method now returns a copy of the `schemas` field, instead of a hardcoded ""default"" schema name. This change aligns with the commit message, as it allows the connector to manage multiple schemas, which is necessary for supporting `CREATE SCHEMA`. The change itself doesn't inherently indicate a bug fix, but rather a feature addition.

**NotBuggy**
"
presto,28423.json,4edc6ea6dc2ee43794889122d51652a8c88be5a8,"@@ -1,4 +1,4 @@
     public String toString()
     {
-        return Objects.toStringHelper(this).toString();
+        return toStringHelper(this).toString();
     }",NotBuggy,"Update to Airbase 28 with Guava 18.0
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Update to Airbase 28 with Guava 18.0"" suggests a dependency update. It doesn't explicitly mention any bug fixes or specific issues being resolved. It primarily focuses on upgrading the Airbase library and Guava library to specific versions.

**Git Diff Analysis:**

The diff shows a change in the `toString()` method. Specifically, `Objects.toStringHelper(this)` is replaced with `toStringHelper(this)`. This suggests that the code was likely using `com.google.common.base.Objects.toStringHelper` and has been changed to use a locally defined `toStringHelper` method (likely within the same class or a parent class) or a static import.

**Reasoning:**

1.  **Dependency Update Implication:** Dependency updates can sometimes indirectly fix bugs if the newer version of the library contains bug fixes. However, the commit message doesn't highlight any specific bug being addressed.
2.  **`toStringHelper` Change:** The change from `Objects.toStringHelper` to `toStringHelper` suggests a change in how the `toString` method is implemented. This could be due to a change in the library's API, or a decision to use a custom implementation. Without more context (the surrounding code), it's difficult to determine if this change is related to a bug fix. It's possible the old method was deprecated or had performance issues, prompting the change. It's also possible that the import statement for `com.google.common.base.Objects` was removed, necessitating the change.
3.  **Lack of Error Handling or Logic Fixes:** The diff doesn't show any changes related to error handling, exception handling, or logical corrections that would strongly indicate a bug fix.

**Conclusion:**

Based on the available information, the commit message primarily indicates a dependency update. The code change, while potentially significant, doesn't directly suggest a bug fix. Without more context, it's impossible to say for sure, but the evidence leans towards a refactoring or adaptation to the new library version rather than a bug fix.

**NotBuggy**
"
presto,11869.json,4ae5d1ecaf56ad7ef3d07bd86090eec806a9cf23,"@@ -1,6 +1,7 @@
     private void addRowsToAggregates(Page page, int startPosition, int endPosition)
     {
+        Page region = page.getRegion(startPosition, endPosition - startPosition + 1);
         for (Aggregator aggregator : aggregates) {
-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));
+            aggregator.processPage(region);
         }
     }",NotBuggy,"Remove unnecessary copies in StreamingAggregationOperator

The previous implementation copied the aggregation group out of the
page for each aggregation operator which is wasteful and extremely
slow when the number of aggregates performed is large.

Performance improvement depends on the number of aggregations and
the number of groups contained in the page, but some workloads can
improve throughput by more than 100% and generate significantly
less garbage
",Buggy,"The commit message indicates a performance optimization by removing unnecessary copies of the aggregation group in the `StreamingAggregationOperator`. The diff shows that the `page.getRegion` call, which extracts a portion of the page, is now done once and the result is stored in the `region` variable. This `region` is then passed to all aggregators. This change avoids repeatedly extracting the same region for each aggregator, aligning with the commit message's goal of reducing wasteful copying and improving performance. The commit message highlights performance improvements and reduced garbage generation, suggesting the previous implementation had a performance bug.

**Buggy**
"
netty,14898.json,e6c9ac968d3923080822dc36fe14aa10e38af15b,"@@ -1,59 +1,60 @@
     public void channelRead(final ChannelHandlerContext ctx, final Object msg) throws Exception {
         long size = calculateSize(msg);
         long now = TrafficCounter.milliSecondFromNano();
         if (size > 0) {
             // compute the number of ms to wait before reopening the channel
             long waitGlobal = trafficCounter.readTimeToWait(size, getReadLimit(), maxTime, now);
             Integer key = ctx.channel().hashCode();
             PerChannel perChannel = channelQueues.get(key);
             long wait = 0;
             if (perChannel != null) {
                 wait = perChannel.channelTrafficCounter.readTimeToWait(size, readChannelLimit, maxTime, now);
                 if (readDeviationActive) {
                     // now try to balance between the channels
                     long maxLocalRead;
                     maxLocalRead = perChannel.channelTrafficCounter.cumulativeReadBytes();
                     long maxGlobalRead = cumulativeReadBytes.get();
                     if (maxLocalRead <= 0) {
                         maxLocalRead = 0;
                     }
                     if (maxGlobalRead < maxLocalRead) {
                         maxGlobalRead = maxLocalRead;
                     }
                     wait = computeBalancedWait(maxLocalRead, maxGlobalRead, wait);
                 }
             }
             if (wait < waitGlobal) {
                 wait = waitGlobal;
             }
             wait = checkWaitReadTime(ctx, wait, now);
             if (wait >= MINIMAL_WAIT) { // At least 10ms seems a minimal
                 // time in order to try to limit the traffic
                 // Only AutoRead AND HandlerActive True means Context Active
-                ChannelConfig config = ctx.channel().config();
+                Channel channel = ctx.channel();
+                ChannelConfig config = channel.config();
                 if (logger.isDebugEnabled()) {
                     logger.debug(""Read Suspend: "" + wait + ':' + config.isAutoRead() + ':'
                             + isHandlerActive(ctx));
                 }
                 if (config.isAutoRead() && isHandlerActive(ctx)) {
                     config.setAutoRead(false);
-                    ctx.attr(READ_SUSPENDED).set(true);
+                    channel.attr(READ_SUSPENDED).set(true);
                     // Create a Runnable to reactive the read if needed. If one was create before it will just be
                     // reused to limit object creation
-                    Attribute<Runnable> attr = ctx.attr(REOPEN_TASK);
+                    Attribute<Runnable> attr = channel.attr(REOPEN_TASK);
                     Runnable reopenTask = attr.get();
                     if (reopenTask == null) {
                         reopenTask = new ReopenReadTimerTask(ctx);
                         attr.set(reopenTask);
                     }
                     ctx.executor().schedule(reopenTask, wait, TimeUnit.MILLISECONDS);
                     if (logger.isDebugEnabled()) {
                         logger.debug(""Suspend final status => "" + config.isAutoRead() + ':'
                                 + isHandlerActive(ctx) + "" will reopened at: "" + wait);
                     }
                 }
             }
         }
         informReadOperation(ctx, now);
         ctx.fireChannelRead(msg);
     }",NotBuggy,"Cleanup: replaced deprecated ctx.attr() and ctx.hasAttr() methods usage with ch.attr() and ch.hasAttr().

Motivation:

Will allow easy removal of deprecated methods in future.

Modification:

Replaced ctx.attr(), ctx.hasAttr() with ctx.channel().attr(), ctx.channel().hasAttr().

Result:

No deprecated ctx.attr(), ctx.hasAttr() methods usage.
",NotBuggy,"The commit message indicates a cleanup task focused on replacing deprecated methods `ctx.attr()` and `ctx.hasAttr()` with their updated counterparts `ch.attr()` and `ch.hasAttr()`. The provided diff shows that `ctx.channel().attr()` and `ctx.channel().hasAttr()` are used instead of `ctx.attr()` and `ctx.hasAttr()`. This change aligns perfectly with the commit message's intention to replace deprecated methods, which is a refactoring task and not directly related to fixing a bug.

**NotBuggy**"
netty,7557.json,ecfa241768499dae2bffa174a874e8f9d81d6d1f,"@@ -1,3 +1,3 @@
-        public void setTrailingHeaders(HttpHeaders trailingHeaders) {
+        void setTrailingHeaders(HttpHeaders trailingHeaders) {
             this.trailingHeaders = trailingHeaders;
         }",NotBuggy,"Make sure AggregatedFullHttpMessage.trailingHeaders() return non-null

When handling an oversized message, HttpObjectAggregator does not wait
until the last chunk is received to produce the failed message, making
AggregatedFullHttpMessage.trailingHeaders() return null.
",Buggy,"The commit message indicates a bug where `AggregatedFullHttpMessage.trailingHeaders()` could return `null` in certain scenarios (oversized messages). The provided code diff changes the visibility of the `setTrailingHeaders` method from `public` to package-private (default).

Reasoning:

1.  **Bug Indication:** The commit message explicitly mentions a bug related to `trailingHeaders()` returning `null`. This strongly suggests a bug fix.
2.  **Code Change Relevance:** While the code change itself (changing visibility) doesn't directly fix the `null` return, it's likely part of a larger fix. Making the setter package-private suggests that the `trailingHeaders` are now being managed internally within the same package, potentially to ensure they are always initialized correctly, even in error scenarios like oversized messages. The visibility change itself doesn't solve the problem, but it's a step towards controlling how `trailingHeaders` is set, which is relevant to the bug described in the commit message.
3.  **Error Handling:** The commit message mentions ""handling an oversized message,"" which implies error handling. The change is likely related to improving how the aggregator handles errors and ensures that `trailingHeaders` is properly set or at least initialized to a non-null value (e.g., an empty `HttpHeaders` object) even when an error occurs.

Conclusion:

**Buggy**
"
netty,525.json,00d2cea8bad744d15a5daed1e36590308f715ee3,"@@ -1,3 +1,3 @@
-    public QoS qualityOfService() {
+    public MqttQoS qualityOfService() {
         return qualityOfService;
     }",NotBuggy,"Overall clean-up on codec-mqtt

- Use simple string concatenation instead of String.format()
- Rewrite exception messages so that it follows our style
- Merge MqttCommonUtil and MqttValidationUtil into MqttCodecUtil
- Hide MqttCodecUtil from users
- Rename MqttConnectReturnCode.value to byteValue
- Rename MqttMessageFactory.create*() to new*()
- Rename QoS to MqttQoS
- Make MqttSubAckPayload.grantedQoSLevels immutable and add more useful
  constructor
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message Breakdown:**
    *   ""Overall clean-up on codec-mqtt"": This suggests a general refactoring and improvement effort rather than a specific bug fix.
    *   ""Use simple string concatenation instead of String.format()"": This is a performance optimization and code style improvement.
    *   ""Rewrite exception messages so that it follows our style"": This is a code style and consistency improvement.
    *   ""Merge MqttCommonUtil and MqttValidationUtil into MqttCodecUtil"": This is a refactoring to consolidate utility classes.
    *   ""Hide MqttCodecUtil from users"": This is an encapsulation improvement.
    *   ""Rename MqttConnectReturnCode.value to byteValue"": This is a renaming for clarity.
    *   ""Rename MqttMessageFactory.create*() to new*()"": This is a renaming for consistency with Java conventions.
    *   ""Rename QoS to MqttQoS"": This is to avoid naming conflicts and improve clarity.
    *   ""Make MqttSubAckPayload.grantedQoSLevels immutable and add more useful constructor"": This improves the design and safety of the `MqttSubAckPayload` class.

2.  **Git Diff Analysis:**
    *   `public QoS qualityOfService() {`  `public MqttQoS qualityOfService() {`
        This change reflects the renaming of `QoS` to `MqttQoS` as mentioned in the commit message.

3.  **Relevance Assessment:**
    The changes described in the commit message are primarily focused on code style improvements, refactoring, renaming, and design enhancements. The Git diff confirms the renaming of `QoS` to `MqttQoS`. None of the changes directly indicate the correction of a specific bug or faulty behavior. While improved code quality can indirectly prevent bugs, the changes themselves don't seem to be addressing a known issue.

**Conclusion:**

NotBuggy
"
netty,3934.json,3d81afb8a5ec31420fb503bc9476ecf23bb6bbfa,"@@ -1,9 +1,11 @@
         public void addTimeout(HashedWheelTimeout timeout) {
+            assert timeout.bucket == null;
+            timeout.bucket = this;
             if (head == null) {
                 head = tail = timeout;
             } else {
                 tail.next = timeout;
                 timeout.prev = tail;
                 tail = timeout;
             }
         }",NotBuggy,"Make sure cancelled Timeouts are able to be GC'ed fast.

Motivation:
At the moment the HashedWheelTimer will only remove the cancelled Timeouts once the HashedWheelBucket is processed again. Until this the instance will not be able to be GC'ed as there are still strong referenced to it even if the user not reference it by himself/herself. This can cause to waste a lot of memory even if the Timeout was cancelled before.

Modification:
Add a new queue which holds CancelTasks that will be processed on each tick to remove cancelled Timeouts. Because all of this is done only by the WorkerThread there is no need for synchronization and only one extra object creation is needed when cancel() is executed. For addTimeout(...) no new overhead is introduced.

Result:
Less memory usage for cancelled Timeouts.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message clearly states a problem: cancelled `Timeout` instances in `HashedWheelTimer` are not garbage collected promptly because they remain strongly referenced within the `HashedWheelBucket` until the bucket is processed again. This can lead to memory wastage. The proposed solution involves introducing a `CancelTask` queue to expedite the removal of cancelled `Timeout` instances. The message highlights that this optimization reduces memory usage for cancelled timeouts.

**Git Diff Analysis:**

The provided Git diff snippet only shows the `addTimeout` method. It adds `timeout.bucket = this;` and `assert timeout.bucket == null;`. This change sets the bucket reference of the timeout to the current bucket when the timeout is added. The assertion checks that the timeout doesn't already have a bucket assigned.

**Reasoning:**

The commit message describes a memory leak scenario related to cancelled `Timeout` objects not being garbage collected promptly. The provided diff snippet, however, only shows modifications to the `addTimeout` method. While the change itself seems reasonable (setting the bucket reference), it doesn't directly address the core issue of delayed garbage collection of *cancelled* timeouts as described in the commit message. The diff doesn't show the introduction of the `CancelTask` queue or any logic related to processing cancelled timeouts.

Therefore, based on the provided information, it's difficult to definitively say if this commit fixes a bug. The commit message describes a bug fix, but the provided diff is incomplete and doesn't show the changes that directly address the described problem. However, the commit message describes a situation where memory is not released as expected, which can be considered a bug.

**Conclusion:**

**Buggy**
"
netty,17881.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public ByteBuf resumeIntermediaryDeallocations() {
-            return this;
-        }

+    public ByteBuf markWriterIndex() {
+        return this;
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message indicates a significant refactoring effort focused on implementing reference counting using `ReferenceCounted` and related classes. The provided code diff snippet, however, shows a seemingly unrelated change: replacing `resumeIntermediaryDeallocations()` with `markWriterIndex()`.

This discrepancy raises concerns. The commit message doesn't mention any changes to methods like `resumeIntermediaryDeallocations()`. The diff itself doesn't inherently indicate a bug fix, but the lack of alignment with the commit message suggests a potential issue. It's possible this change was included accidentally or represents an incomplete part of a larger refactoring not fully described in the commit message.

Given the mismatch, it's difficult to definitively say if this specific diff fixes a bug. However, the refactoring itself, as described in the commit message, could be in response to memory leaks or other resource management issues, which are often considered bugs. Without more context, it's hard to be certain. The change in the diff doesn't appear to be directly related to fixing a bug, but the overall commit might be. Since the diff itself doesn't clearly indicate a bug fix, and the commit message describes a refactoring, I will lean towards NotBuggy.

**NotBuggy**
"
netty,12649.json,17280116c48b91c2dd8988d625f749318150de3b,"@@ -1,11 +1,9 @@
-    private void fail(ChannelHandlerContext ctx, long frameLength) {
+    private void fail(long frameLength) {
         if (frameLength > 0) {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded"");
         } else {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "" - discarding""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "" - discarding"");
         }
     }",NotBuggy,"[#2643] Throw TooLongFrameException instead of using fireExceptionCaught

Motivation:

It's not always the case that there is another handler in the pipeline that will intercept the exceptionCaught event because sometimes users just sub-class. In this case the exception will just hit the end of the pipeline.

Modification:
Throw the TooLongFrameException so that sub-classes can handle it in the exceptionCaught(...) method directly.

Result:
Sub-classes can correctly handle the exception,
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message explicitly states the motivation for the change: to allow subclasses to handle `TooLongFrameException` directly in their `exceptionCaught(...)` method. The current implementation relies on `fireExceptionCaught`, which might not be intercepted by a handler in the pipeline, especially if users are subclassing.
    *   The commit message suggests that the previous behavior was problematic because exceptions could reach the end of the pipeline unhandled. This implies a potential bug or at least an undesirable behavior.

2.  **Git Diff Analysis:**
    *   The diff shows that `ctx.fireExceptionCaught(...)` is replaced with `throw new TooLongFrameException(...)`. This confirms the commit message's description of the change.
    *   The `fail` method now directly throws the exception instead of relying on the channel pipeline to handle it. This gives the caller (presumably a subclass) more direct control over exception handling.
    *   The removal of `ctx` from the `fail` method signature further supports the idea that the method is no longer interacting with the ChannelHandlerContext to propagate the exception.

3.  **Bug Fix Indication:**
    *   The change addresses a scenario where exceptions might be unhandled, which can be considered a bug or at least a potential source of errors.
    *   The modification provides a more robust and predictable way for subclasses to handle `TooLongFrameException`, indicating an improvement over the previous implementation.
    *   The commit message explicitly mentions a problem with the previous approach and how the new approach resolves it.

**Conclusion:**

The changes clearly indicate a bug fix by providing a more reliable way to handle exceptions in subclasses. Therefore, the answer is:

**Buggy**"
netty,14065.json,59973e93dd7da715eee709788573e3515cc50238,"@@ -1,3 +1,3 @@
-    public String getPeerHost() {
+    public final String getPeerHost() {
         return wrapped.getPeerHost();
     }",NotBuggy,"Ensure X509KeyManager methods are called on the correct time when using server-side and support more methods of ExtendedSSLSession. (#8283)

Motivation:

Before when on server-side we just called the X509KeyManager methods when handshake() was called the first time which is not quite correct as we may not have received the full SSL hello / handshake and so could not extra for example the SNI hostname that was requested.
OpenSSL exposes the SSL_CTX_set_cert_cb function which allows to set a callback which is executed at the correct moment, so we should use it. This also allows us to support more methods of ExtendedSSLSession easily.

Modifications:

- Make use of new methods exposed by netty-tcnative since https://github.com/netty/netty-tcnative/pull/388 to ensure we select the key material at the correct time.
- Implement more methods of ExtendedOpenSslSession
- Add unit tests to ensure we are able to retrieve various things on server-side in the X509KeyManager and so verify it is called at the correct time.
- Simplify code by using new netty-tcnative methods.

Result:

More correct implementation for server-side usage and more complete implemented of ExtendedSSLSession.",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message clearly states that the previous implementation of `X509KeyManager` method calls on the server-side was incorrect. It mentions that these methods were being called too early in the handshake process, before all necessary information (like SNI hostname) was available. The commit aims to rectify this by using the `SSL_CTX_set_cert_cb` callback in OpenSSL, which is triggered at the correct moment during the handshake. The commit also mentions supporting more methods of `ExtendedSSLSession`.

2.  **Git Diff:** The provided Git diff is very small, only showing a change from `public String getPeerHost()` to `public final String getPeerHost()`. This change in itself doesn't indicate a bug fix. Making a method `final` is usually an optimization or a design decision to prevent overriding, not a correction of faulty behavior.

3.  **Overall Assessment:** While the provided diff snippet doesn't directly show a bug fix, the commit message strongly suggests that the overall commit addresses a bug related to the timing of `X509KeyManager` calls. The commit message indicates that the previous implementation was not correct and that the changes are intended to fix this incorrect behavior. The other changes mentioned in the commit message (usage of `SSL_CTX_set_cert_cb`, support for more `ExtendedSSLSession` methods, and added unit tests) further support the idea of a bug fix. The provided diff is likely just a small part of the overall commit.

**Conclusion:**

**Buggy**
"
netty,15217.json,7d4c077492ba5b40595b0742e7b22182c544f7b7,"@@ -1,5 +1,5 @@
-    public ByteBuf setInt(int index, int value) {
-        wrapped.checkIndex(index, 4);
-        _setInt(index, value);
+    public ByteBuf setByte(int index, int value) {
+        checkIndex(index);
+        _setByte(index, value);
         return this;
     }",NotBuggy,"Add *UnsafeHeapByteBuf for improve performance on systems with sun.misc.Unsafe

Motivation:

sun.misc.Unsafe allows us to handle heap ByteBuf in a more efficient matter. We should use special ByteBuf implementation when sun.misc.Unsafe can be used to increase performance.

Modifications:

- Add PooledUnsafeHeapByteBuf and UnpooledUnsafeHeapByteBuf that are used when sun.misc.Unsafe is ready to use.
- Add UnsafeHeapSwappedByteBuf

Result:

Better performance when using heap buffers and sun.misc.Unsafe is ready to use.
",Buggy,"The commit message indicates a performance improvement by utilizing `sun.misc.Unsafe` for heap `ByteBuf` operations. The diff shows a change in the `setInt` method to `setByte` and a simplified index check. This suggests a potential correction in how bytes are set within the buffer, possibly related to unsafe memory access. The original code `wrapped.checkIndex(index, 4)` checks if there are 4 bytes available from the index, while the new code `checkIndex(index)` only checks for a single byte. This could be an optimization, but it could also be a fix for incorrect bounds checking in the unsafe implementation. Given the context of unsafe operations and the change in index checking, it's plausible that the original code had a bug related to index handling.

**Buggy**
"
netty,9425.json,c8ca3299324a1e2b9ce501c95715cf10ea70c238,"@@ -1,4 +1,4 @@
     public SpdyHeadersFrame setInvalid() {
-        super.setInvalid();
+        invalid = true;
         return this;
     }",NotBuggy,"SPDY: update object hierarchy
",NotBuggy,"The commit message ""SPDY: update object hierarchy"" suggests a structural change in the SPDY object model.

The code diff shows a change in the `setInvalid()` method. Instead of calling `super.setInvalid()`, it directly sets the `invalid` field to `true`. This change is likely related to how the invalid state is managed within the SPDY object hierarchy. The original implementation might have relied on a parent class to handle the invalid state, but the update directly modifies the field in the current class. This could be due to a change in the inheritance structure or a decision to manage the invalid state locally.

The change doesn't explicitly indicate a bug fix. It seems more like a refactoring or an adjustment to the object hierarchy. There's no error handling, logical correction, or exception handling improvement apparent in the diff.

**NotBuggy**
"
netty,1157.json,dfa3bbbf0035e6eb39403194c9aabae0f9c2c1a3,"@@ -1,3 +1,3 @@
-    public List<DnsCacheEntry> get(String hostname) {
+    public List<DnsCacheEntry> get(String hostname, DnsRecord[] additionals) {
         return Collections.emptyList();
     }",NotBuggy,"Add support for Client Subnet in DNS Queries (RFC7871)

Motivation:

RFC7871 defines an extension which allows to request responses for a given subset.

Modifications:

- Add DnsOptPseudoRrRecord which can act as base class for extensions based on EDNS(0) as defined in RFC6891
- Add DnsOptEcsRecord to support the Client Subnet in DNS Queries extension
- Add tests

Result:

Client Subnet in DNS Queries extension is now supported.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Add support for Client Subnet in DNS Queries (RFC7871)"" clearly indicates a new feature implementation rather than a bug fix. It describes the addition of support for the Client Subnet extension in DNS queries, as defined by RFC7871. The motivation section further clarifies that the changes are to enable requesting responses for a specific subset, which aligns with the RFC. The modifications listed (adding `DnsOptPseudoRrRecord`, `DnsOptEcsRecord`, and tests) support the feature implementation claim.

2.  **Git Diff:** The provided Git diff shows a change in the `get` method signature of a class (presumably related to DNS caching). The method now accepts a `DnsRecord[] additionals` parameter. This change aligns with the commit message, as the `additionals` parameter could be used to pass the EDNS(0) options, including the Client Subnet option.

3.  **Relevance:** The commit message and the code modifications are highly relevant. The commit message describes the addition of a new feature (Client Subnet support), and the code modification introduces a parameter that can be used to pass the necessary information for this feature. There is no indication of error handling updates, logical corrections, or exception handling improvements that would suggest a bug fix. The changes are focused on adding new functionality.

**Conclusion:**

Based on the commit message and the code modifications, the changes are related to adding a new feature (Client Subnet support) rather than fixing a bug.

**NotBuggy**"
netty,12457.json,3a9f47216143082bdfba62e8940160856767d672,"@@ -1,4 +1,3 @@
     public ByteBuf setShort(int index, int value) {
-        reject();
-        return this;
+        throw reject();
     }",NotBuggy,"Make retained derived buffers recyclable

Related: #4333 #4421 #5128

Motivation:

slice(), duplicate() and readSlice() currently create a non-recyclable
derived buffer instance. Under heavy load, an application that creates a
lot of derived buffers can put the garbage collector under pressure.

Modifications:

- Add the following methods which creates a non-recyclable derived buffer
  - retainedSlice()
  - retainedDuplicate()
  - readRetainedSlice()
- Add the new recyclable derived buffer implementations, which has its
  own reference count value
- Add ByteBufHolder.retainedDuplicate()
- Add ByteBufHolder.replace(ByteBuf) so that..
  - a user can replace the content of the holder in a consistent way
  - copy/duplicate/retainedDuplicate() can delegate the holder
    construction to replace(ByteBuf)
- Use retainedDuplicate() and retainedSlice() wherever possible
- Miscellaneous:
  - Rename DuplicateByteBufTest to DuplicatedByteBufTest (missing 'D')
  - Make ReplayingDecoderByteBuf.reject() return an exception instead of
    throwing it so that its callers don't need to add dummy return
    statement

Result:

Derived buffers are now recycled when created via retainedSlice() and
retainedDuplicate() and derived from a pooled buffer
",Buggy,"The commit message indicates a performance improvement by making derived buffers recyclable, which reduces garbage collector pressure. The diff shows a change in `setShort` method where `reject()` now throws an exception instead of simply calling it and returning. The commit message mentions that `ReplayingDecoderByteBuf.reject()` now returns an exception instead of throwing it. This change aligns with the commit message's description of miscellaneous changes. This change is likely related to handling an invalid state or operation, which can be considered a bug fix or preventative measure against potential issues.

**Buggy**
"
netty,14470.json,5ac84760c4f5d4f09a61fb5cd2cac7d95a782f9f,"@@ -1,11 +1,15 @@
     public SslContextBuilder keyManager(File keyCertChainFile, File keyFile, String keyPassword) {
-        if (forServer) {
-            checkNotNull(keyCertChainFile, ""keyCertChainFile required for servers"");
-            checkNotNull(keyFile, ""keyFile required for servers"");
+        X509Certificate[] keyCertChain;
+        PrivateKey key;
+        try {
+            keyCertChain = SslContext.toX509Certificates(keyCertChainFile);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid certificates: "" + keyCertChainFile, e);
         }
-        this.keyCertChainFile = keyCertChainFile;
-        this.keyFile = keyFile;
-        this.keyPassword = keyPassword;
-        this.keyManagerFactory = null;
-        return this;
+        try {
+            key = SslContext.toPrivateKey(keyFile, keyPassword);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid private key: "" + keyFile, e);
+        }
+        return keyManager(key, keyPassword, keyCertChain);
     }",NotBuggy,"Allow to create SslContext from existing PrivateKey / X509Certificate

Motivation:

Sometimes the user already has a PrivateKey / X509Certificate which should be used to create a new SslContext. At the moment we only allow to construct it via Files.

Modifications:

- Add new methods to the SslContextBuilder to allow creating a SslContext from PrivateKey / X509Certificate
- Mark all public constructors of *SslContext as @Deprecated, the user should use SslContextBuilder
- Update tests to us SslContextBuilder.

Result:

Creating of SslContext is possible with PrivateKay/X509Certificate
",NotBuggy,"The commit message describes a feature addition: allowing the creation of `SslContext` from existing `PrivateKey` and `X509Certificate` objects, instead of just from files. It also mentions deprecating the public constructors of `SslContext` in favor of using `SslContextBuilder`. The provided diff snippet shows changes to the `keyManager` method in `SslContextBuilder`. It appears to be refactoring the existing file-based key loading to use the `SslContext.toX509Certificates` and `SslContext.toPrivateKey` methods. The code includes exception handling to provide more informative error messages if the files do not contain valid certificates or private keys. This commit introduces new functionality and improves the existing code by adding error handling and promoting the use of `SslContextBuilder`. There's no indication of a bug fix in the traditional sense (correcting incorrect behavior). The changes align with the commit message.

**NotBuggy**
"
netty,11382.json,f9001b9fc07a71a9d6eaf0462470416780302107,"@@ -1,21 +1,21 @@
-    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, HpackUtil.IndexType indexType,
+    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, IndexType indexType,
                                int nameIndex) {
         boolean nameIndexValid = nameIndex != -1;
         switch (indexType) {
             case INCREMENTAL:
                 encodeInteger(out, 0x40, 6, nameIndexValid ? nameIndex : 0);
                 break;
             case NONE:
                 encodeInteger(out, 0x00, 4, nameIndexValid ? nameIndex : 0);
                 break;
             case NEVER:
                 encodeInteger(out, 0x10, 4, nameIndexValid ? nameIndex : 0);
                 break;
             default:
                 throw new Error(""should not reach here"");
         }
         if (!nameIndexValid) {
             encodeStringLiteral(out, name);
         }
         encodeStringLiteral(out, value);
     }",NotBuggy,"HTTP/2 move internal HPACK classes to the http2 package

Motivation:
The internal.hpack classes are no longer exposed in our public APIs and can be made package private in the http2 package.

Modifications:
- Make the hpack classes package private in the http2 package

Result:
Less APIs exposed as public.
",NotBuggy,"The commit message indicates a refactoring effort to move internal HPACK classes to the `http2` package and make them package-private. This is done to reduce the number of exposed public APIs. The provided diff snippet shows a change in the `encodeLiteral` method where `HpackUtil.IndexType` is changed to `IndexType`. This change aligns with the commit message, suggesting that the `IndexType` class, which was previously accessed via `HpackUtil`, is now directly accessible within the `http2` package, likely due to the classes being moved. There's no indication of a bug fix in this change.

**NotBuggy**"
netty,16567.json,57063b6db0193f3d53847f3754005a1cd63d463a,"@@ -1,3 +1,3 @@
     protected int _getIntLE(int index) {
-        return buffer.getIntLE(index);
+        return unwrap().getIntLE(index);
     }",NotBuggy,"Implement the ByteBuf methods that should have been overridden

Motivation:

Some ByteBuf implementations do not override all necessary methods,
which can lead to potentially sub-optimal behavior.

Also, SlicedByteBuf does not perform the range check correctly due to
missing overrides.

Modifications:

- Add missing overrides
- Use unwrap() instead of direct member access in derived buffers for
  consistency
- Merge unwrap0() into unwrap() using covariant return type
- Deprecate AbstractDerivedByteBuf and its subtypes, because they were
  not meant to be public

Result:

Correctness
",Buggy,"The commit message indicates that the changes aim to correct suboptimal behavior and fix range check issues in `ByteBuf` implementations. The diff shows a modification in the `_getIntLE` method where `buffer.getIntLE(index)` is replaced with `unwrap().getIntLE(index)`. This change suggests that the original code might have been directly accessing the underlying buffer in a way that bypassed necessary checks or transformations, and `unwrap()` is intended to provide the correct underlying buffer. This aligns with the commit message's claim of fixing potentially sub-optimal behavior and correcting range checks.

**Buggy**
"
netty,17888.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public byte getByte(int index) {
-            throw new IndexOutOfBoundsException();
-        }

+    public byte getByte(int index) {
+        throw new IndexOutOfBoundsException();
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message indicates a significant refactoring related to reference counting, involving the introduction of new classes and the replacement of `Freeable` with `ReferenceCounted`. The provided code diff shows a minor change in `getByte(int index)` method, specifically adjusting the indentation. This change doesn't seem directly related to the core refactoring described in the commit message, nor does it obviously fix a bug. It's more likely a formatting adjustment.

**NotBuggy**"
argouml,2154.json,992cfe1a2840c838a401450e5232b03826cb956c,"@@ -1,15 +1,22 @@
-    public void actionPerformed(ActionEvent e) {
-        super.actionPerformed(e);
-        Object source = e.getSource();
-        if (source instanceof UMLComboBox2) {
-            Object selected = ((UMLComboBox2) source).getSelectedItem();
-            Object target = ((UMLComboBox2) source).getTarget();
-            if (Model.getFacade().isACallAction(target)
-                    && Model.getFacade().isAOperation(selected)) {
-                if (Model.getFacade().getOperation(target) != selected) {
-                    Model.getCommonBehaviorHelper()
-                    .setOperation(target, selected);
-                }
-            }
-        }
-    }

+       public void actionPerformed(ActionEvent e) {
+           super.actionPerformed(e);
+           Object source = e.getSource();
+           Object oldAssoc = null;
+           Object newAssoc = null;
+           Object link = null;
+           if (source instanceof UMLComboBox2) {
+               UMLComboBox2 box = (UMLComboBox2) source;
+               Object o = box.getTarget();
+               if (Model.getFacade().isALink(o)) {
+                   link = o;
+                   oldAssoc = Model.getFacade().getAssociation(o);
+               }
+               Object n = box.getSelectedItem();
+               if (Model.getFacade().isAAssociation(n)) {
+                   newAssoc = n;
+               }
+           }
+           if (newAssoc != oldAssoc && link != null && newAssoc != null) {
+               Model.getCoreHelper().setAssociation(link, newAssoc);
+           }
+       }",NotBuggy,"Change the model when the combo selection is changed

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17668 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a change in the model when a combo box selection changes. The diff shows modifications to the `actionPerformed` method, specifically when the source is a `UMLComboBox2`. The code now handles the case where the target of the combo box is a `Link` and the selected item is an `Association`. It retrieves the old association and the new association from the combo box. If the new association is different from the old one, it updates the association of the link using `Model.getCoreHelper().setAssociation(link, newAssoc)`. This suggests that the previous implementation might not have correctly updated the model when the association of a link was changed through the combo box, which would be a bug.

**Buggy**"
argouml,2119.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,7 +1,7 @@
-    private static Object getType(Object target) {
-        Object type = Model.getFacade().getType(target);
-        if (Model.getFacade().isAClassifierInState(type)) {
-            type = Model.getFacade().getType(type);
-        }
-        return type;
-    }

+        private static Object getType(Object target) {
+            Object type = Model.getFacade().getType(target);
+            if (Model.getFacade().isAClassifierInState(type)) {
+                type = Model.getFacade().getType(type);
+            }
+            return type;
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Make Actions inner classes of whatever uses them"" suggests a refactoring of the code structure, specifically moving Action classes to be inner classes of the classes that use them.

The provided diff shows a change in indentation of the `getType` method. The code itself remains the same. This indicates that the method has been moved from one class to another, likely to be nested within the class that uses it. This aligns with the commit message.

There is no indication of bug fixes, error handling, or logical corrections in the provided diff. The change is purely structural.

**NotBuggy**"
argouml,5773.json,026fc657d606390b8457286c18c7dc5b9fe57803,"@@ -1,3 +1,4 @@
             public void undo() {
-                showStereotypes = !showem;
+                showVisibilities = !showem;
+                showVisibilitiesSet = oldValid;
             }",NotBuggy,"showStereotype setting is for the explorer (ie project-wide).  move it back to the ProjectSettings and undeprecate the related methods.


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@16435 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a change in the scope of the `showStereotype` setting and its relocation to `ProjectSettings`. The provided diff, however, shows changes related to `showVisibilities` and `showVisibilitiesSet` within an `undo()` method.

Reasoning:

1. **Mismatch between commit message and diff:** The commit message talks about `showStereotype`, but the diff modifies `showVisibilities`. This is a clear discrepancy.
2. **Potential Bug Fix:** The `undo()` method suggests that the change is related to reverting a previous action. The addition of `showVisibilitiesSet = oldValid;` implies that the previous state of `showVisibilitiesSet` was not being correctly restored during an undo operation, which can be considered a bug fix.

Conclusion:

**Buggy**
"
argouml,2643.json,2a18b45c346f54b0340fd7f7cf7fe8644919d2f0,"@@ -1,47 +1,36 @@
-    public void setTarget(Object theNewTarget) {
+    protected void setTarget(Object theNewTarget) {
+        assert (getTarget() == null);
+        assert (!(getTarget() instanceof Fig));
+        
         theNewTarget = theNewTarget instanceof Fig
             ? ((Fig) theNewTarget).getOwner() : theNewTarget;
         if (Model.getFacade().isAModelElement(theNewTarget)
                 || theNewTarget instanceof Diagram) {
-            if (getTarget() != null) {
-                Enumeration enumeration = elements();
-                while (enumeration.hasMoreElements()) {
-                    Object base = enumeration.nextElement();
-                    Model.getPump().removeModelEventListener(
-                        this,
-                        base,
-                        ""ownedElement"");
-                }
-                Model.getPump().removeModelEventListener(
-                    this,
-                    getTarget(),
-                    ""base"");
-            }
             setListTarget(theNewTarget);
             if (getTarget() != null) {
                 Collection bases = Model.getFacade().getBases(getTarget());
                 Iterator it = bases.iterator();
                 while (it.hasNext()) {
                     Object base =  it.next();
                     Model.getPump().addModelEventListener(
                         this,
                         base,
                         ""ownedElement"");
                 }
                 // make sure we know it when a classifier is added as a base
                 Model.getPump().addModelEventListener(
                     this,
                     getTarget(),
                     ""base"");
             }
             if (getTarget() != null) {
                 removeAllElements();
                 setBuildingModel(true);
                 buildModelList();
                 setBuildingModel(false);
                 if (getSize() > 0) {
                     fireIntervalAdded(this, 0, getSize() - 1);
                 }
             }
         }
     }",NotBuggy,"Get rid of any target listeners

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17506 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message ""Get rid of any target listeners"" suggests the removal of event listeners associated with a target object.

The code diff shows the removal of code that removes model event listeners when the target is changed. Specifically, the code within the `if (getTarget() != null)` block that iterates through elements and removes listeners for ""ownedElement"" and ""base"" events has been removed.

The added assertions `assert (getTarget() == null);` and `assert (!(getTarget() instanceof Fig));` suggest that the target should be null and not an instance of `Fig` before setting a new target. This implies a change in how the target is managed, potentially to avoid redundant or incorrect listener registrations.

The removal of the listener removal code, combined with the added assertions, indicates a change in how the target is handled and its associated listeners are managed. This could be a bug fix if the previous listener management was causing issues like memory leaks or incorrect event handling.

Therefore, the changes indicate a bug fix.

**Buggy**"
argouml,13058.json,c594f19e891bb817975f314e7f9a190bf073e003,"@@ -1,12 +1,10 @@
     private boolean isDeleteEventRequired(
             final Object element) {
-        if (element instanceof Property) {
-            synchronized (deleteEventIgnoreList) {
-                if (deleteEventIgnoreList.contains(element)) {
-                    deleteEventIgnoreList.remove(element);
-                    return false;
-                }
+        synchronized (deleteEventIgnoreList) {
+            if (deleteEventIgnoreList.contains(element)) {
+                deleteEventIgnoreList.remove(element);
+                return false;
             }
         }
         return true;
     }",NotBuggy,"Make sure that changing namespace or region does not fire a delete event

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19406 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a fix to prevent unwanted delete events when changing namespace or region. The code diff modifies the `isDeleteEventRequired` method. The original code only checked the `deleteEventIgnoreList` if the element was a `Property`. The modified code checks the `deleteEventIgnoreList` regardless of the element type. This change ensures that delete events are suppressed for all element types present in the `deleteEventIgnoreList`, which aligns with the commit message's intention to prevent unwanted delete events during namespace or region changes. This suggests a bug where delete events were incorrectly triggered for non-`Property` elements when they should have been ignored.

**Buggy**
"
argouml,2284.json,5df7baaa147ae1e07eda64a63bf1a7bd187a5176,"@@ -1,14 +1,20 @@
     protected void addAll(Collection col) {
         Object selected = getSelectedItem();
         fireListEvents = false;
         int oldSize = objects.size();
         for (Object o : col) {
-            addElement(o);
+            if (filter != null && filter.length() > 0) {
+        	if (Model.getFacade().getName(o).toLowerCase().startsWith(filter.toLowerCase())) {
+                    addElement(o);
+        	}
+            } else {
+                addElement(o);
+            }
         }
         setSelectedItem(external2internal(selected));
         fireListEvents = true;
         if (objects.size() != oldSize) {
             fireIntervalAdded(this, oldSize == 0 ? 0 : oldSize - 1, 
                     objects.size() - 1);
         }
     }",NotBuggy,"Allow filtering of namespaces in combo

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18737 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Allow filtering of namespaces in combo"" suggests that the changes introduce a filtering mechanism for namespaces displayed in a combo box.

The code diff introduces a filter based on the `filter` variable. It checks if the name of the object (presumably a namespace) starts with the `filter` string (case-insensitive). If the filter is not null or empty, only elements whose names start with the filter are added to the combo box. Otherwise, all elements are added.

The code change directly implements the feature described in the commit message. It adds a filtering mechanism to the `addAll` method, which is likely used to populate the combo box with namespace elements. The filtering is based on the name of the namespace and a user-provided filter string. This aligns perfectly with the commit message. There is no indication of a bug fix in the commit message or the code changes.

**NotBuggy**"
argouml,1917.json,02860b3ad2e8258250f31a3ea52907e4e4df61b0,"@@ -1,3 +1,3 @@
-    public Fig getNameDisplay() {
-        return nameFig;
+    public DiagramElement getNameDisplay() {
+        return nameDisplay;
     }",NotBuggy,"Remove all knowledge of FigNodeModelElement

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19249 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states the removal of all knowledge of `FigNodeModelElement`. The code diff shows a change from `Fig getNameDisplay()` to `DiagramElement getNameDisplay()` and `nameFig` to `nameDisplay`. This indicates a type change that aligns with the commit message's intention to remove a specific type. The change suggests a refactoring or abstraction to remove the dependency on `FigNodeModelElement`. There is no direct indication of a bug fix.

**NotBuggy**"
argouml,14398.json,c1eb0dea7c36300fe037cd721449f3f0e45449ce,"@@ -1,10 +1,16 @@
     public void selectionChanged(GraphSelectionEvent selectionEvent) {
         removeAll();
         if (selectionEvent.getSelections().size() == 1) {
             Fig selectedFig = (Fig)selectionEvent.getSelections().get(0);
             DefaultMutableTreeNode tn = new DefaultMutableTreeNode(getDescr(selectedFig));
             buildTree(selectedFig, tn);
-            JTree tree = new JTree(tn);
-            add(tree);
+            if (selectedFig instanceof FigClassifierRole) {
+                MessageNodeBuilder.addNodeTree(tn, (FigClassifierRole)selectedFig);
+            }
+            FigTree tree = new FigTree(tn);
+            tree.expandAll();
+            
+            JScrollPane scroller = new JScrollPane(tree);
+            add(scroller);
         }
     }",NotBuggy,"Specialize FigInspector for FigClassifierRole so I can see the contained MessageNodes


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@10031 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"Okay, let's analyze the provided Git commit message and diff to determine if it indicates a bug fix.

**1. Commit Message Analysis:**

The commit message ""Specialize FigInspector for FigClassifierRole so I can see the contained MessageNodes"" suggests an enhancement rather than a bug fix. It indicates that the `FigInspector` is being modified to provide more detailed information (specifically, `MessageNodes`) when a `FigClassifierRole` is selected. This implies adding functionality to inspect the contents of a specific type of figure.

**2. Code Diff Analysis:**

The code diff shows the following changes within the `selectionChanged` method:

*   **Conditional Logic:** An `if` statement is added to check if the `selectedFig` is an instance of `FigClassifierRole`.
*   **Specialized Processing:** If the condition is true, `MessageNodeBuilder.addNodeTree(tn, (FigClassifierRole)selectedFig)` is called. This suggests that a specific tree structure is being built for `FigClassifierRole` instances, likely to display the `MessageNodes`.
*   **Tree Expansion:** The `FigTree` is expanded using `tree.expandAll()`.
*   **Scroll Pane:** The tree is added to a `JScrollPane`.

**3. Reasoning:**

The changes clearly indicate an enhancement to the `FigInspector` to provide more detailed information about `FigClassifierRole` objects. The addition of the `if` statement and the call to `MessageNodeBuilder.addNodeTree` confirm that the goal is to display the `MessageNodes` contained within a `FigClassifierRole`. The use of `tree.expandAll()` and `JScrollPane` suggests an improvement in the user interface to make the information more accessible. There's no evidence of error handling, logical corrections, or exception handling improvements that would typically indicate a bug fix. The commit message and the code changes are consistent with each other.

**Conclusion:**

NotBuggy
"
argouml,3102.json,e5ccb0e741ae79fe74f24969ededccffcb04652e,"@@ -1,3 +1,3 @@
     public String getLabel() {
-        return name;
+        return propertyName;
     }",NotBuggy,"Refactor getName() -> getPropertyName()

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18704 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Refactor getName() -> getPropertyName()"" suggests a renaming refactoring. The diff shows that the method `getLabel()` now returns `propertyName` instead of `name`. This aligns perfectly with the commit message. There's no indication of a bug fix, error handling, or logical correction. It's a simple variable renaming.

**NotBuggy**"
argouml,1992.json,a09bb80652d16906a2c08e561094768d32d17431,"@@ -1,3 +1,10 @@
     public boolean disable() {
+
+        PropPanelFactoryManager.removePropPanelFactory(propPanelFactory);
+
+        // TODO: Remove the casting to DiagramFactoryInterface2
+        // as soon as DiagramFactoryInterface is removed.
+        DiagramFactory.getInstance().registerDiagramFactory(
+                DiagramType.Activity, (DiagramFactoryInterface2) null);
         return true;
     }",NotBuggy,"Move all files down (I seem to created a rogue directory in previous commit)

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18519 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Move all files down (I seem to created a rogue directory in previous commit)"" indicates a file reorganization or directory structure correction. The provided code diff removes a `PropPanelFactory` and unregisters a `DiagramFactory`. These changes don't seem directly related to moving files or correcting directory structures. The code modifications suggest a change in how diagram types and property panels are managed, possibly related to a feature removal or refactoring. The comment ""// TODO: Remove the casting..."" hints at ongoing refactoring. The code changes themselves don't strongly suggest a bug fix, but rather a cleanup or modification of existing functionality. Therefore, the changes don't align with the commit message.

**NotBuggy**"
argouml,2047.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,3 +1,3 @@
-    protected String getDialogTitle() {
-        return Translator.localize(""dialog.title.add-contents"");
-    }

+        protected String getDialogTitle() {
+            return Translator.localize(""dialog.title.add-baseclasses"");
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message ""Make Actions inner classes of whatever uses them"" suggests a refactoring or restructuring of the code where Action classes are moved to be inner classes of the classes that use them.

The provided diff snippet shows a change in the `getDialogTitle()` method. Specifically, the return value is changed from `""dialog.title.add-contents""` to `""dialog.title.add-baseclasses""`. This change doesn't seem directly related to moving Action classes. It looks more like a correction of the dialog title, which could be considered a bug fix if the original title was incorrect. The change in the dialog title suggests a correction or improvement, which aligns with fixing a potential mislabeling issue.

Therefore, the changes indicate a bug fix.

**Buggy**"
argouml,11027.json,c82d790257c6dbfd74c4ead021354b2d7a75c6be,"@@ -1,3 +1,2 @@
     public void popupMenuCanceled(PopupMenuEvent e) {
-        LOG.info(""popupMenuCanceled"");
     }",NotBuggy,"Remove logging commited accidentally

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15539 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message ""Remove logging commited accidentally"" indicates that the change is to remove logging statements that were unintentionally added. The diff shows the removal of `LOG.info(""popupMenuCanceled"");`. This aligns perfectly with the commit message. The logging statement was likely added during debugging and should not have been committed. This removal constitutes a bug fix, as it corrects an unintended addition to the codebase.

**Buggy**
"
argouml,14041.json,280cf1fa18a226a8dbf707f91b08ae68097be66c,"@@ -1,4 +1,3 @@
     public Object getCollaborationInstanceSet() {
-        // TODO: Need UML 2.x equivalent
-        return null /*CollaborationInstanceSet.class*/;
+        throw new NotYetImplementedException();
     }",NotBuggy,"Define more model element types that are used by activity diagrams

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18525 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates the addition of model element types for activity diagrams. The code change replaces a `TODO` comment and a `return null` statement with a `throw new NotYetImplementedException()` statement. This change doesn't seem to fix a bug but rather indicates that a feature is not yet implemented. The commit message aligns with the code change, as it describes the addition of model element types, and the exception suggests that the implementation is incomplete.

**NotBuggy**"
argouml,2590.json,e7865aa82550c72061b5ffc4c566f5b716ae8299,"@@ -1,25 +1,40 @@
     public void buildPanel() {
 	// ///////////////////////////////////////
 	// Build the field
 	// ///////////////////////////////////////
 
-	checkbox = new JCheckBox();
-	// TODO ? find a Tool tips, add a label
-	// checkbox.setToolTipText(Translator.localize(""label.body.tooltip""));
-	checkbox.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	trueButton = new JRadioButton(Translator.localize(""misc.boolean.true""));
+	falseButton = new JRadioButton(Translator
+		.localize(""misc.boolean.false""));
+
+	trueFalseGroup = new ButtonGroup();
+	trueFalseGroup.add(trueButton);
+	trueFalseGroup.add(falseButton);
+
+	trueButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	falseButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
 
 	/**
 	 * on change : Change the value in the model
 	 */
-	checkbox.addActionListener(new ActionListener() {
+	trueButton.addActionListener(new ActionListener() {
+	    public void actionPerformed(ActionEvent e) {
+		updateModel();
+	    }
+	});
+
+	falseButton.addActionListener(new ActionListener() {
 	    public void actionPerformed(ActionEvent e) {
 		updateModel();
 	    }
 	});
 
 	// ///////////////////////////////////////
 	// Add field(s) to panel
 	// ///////////////////////////////////////
-	this.allField = new JScrollPane(checkbox);
+	JPanel panel = new JPanel();
+	panel.add(trueButton);
+	panel.add(falseButton);
+	this.allField = new JScrollPane(panel);
 
     }",NotBuggy,"Allow different types of initial value to be specified

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19429 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Allow different types of initial value to be specified"" suggests that the code changes should enable the specification of different initial values.

The diff shows that a `JCheckBox` has been replaced with two `JRadioButton`s (trueButton and falseButton) and a `ButtonGroup`. This change allows the user to select either ""true"" or ""false"" as an initial value, instead of just a boolean value represented by a checkbox. The action listeners on the radio buttons call `updateModel()`, which presumably updates the model with the selected boolean value.

The changes align with the commit message because they allow the user to specify different types of initial values (specifically, choosing between ""true"" and ""false""). There's no explicit indication of a bug fix in the traditional sense (no error handling, logical corrections, or exception handling improvements). The change seems to be an enhancement or a new feature.

**NotBuggy**"
argouml,10209.json,604299fb46334e559a0cbc4aaf66e7471c0e693f,"@@ -1,12 +1,10 @@
     private void initFigs() {
         setEditable(false);
-        setBigPort(new FigRect(X, Y, STATE_WIDTH, HEIGHT, DEBUG_COLOR,
-                DEBUG_COLOR));
         head = new FigRect(X, Y, STATE_WIDTH, HEIGHT, LINE_COLOR,
                 SOLID_FILL_COLOR);
         // add Figs to the FigNode in back-to-front order
         addFig(getBigPort());
         addFig(head);
 
         setBlinkPorts(false); //make port invisible unless mouse enters
     }",NotBuggy,"Make code consistent. 
Remove the use of deprecated method setBigPort().

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18728 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates two changes: making the code consistent and removing the use of the deprecated method `setBigPort()`.

The diff shows that the `setBigPort()` method is indeed removed. The code also removes the instantiation of `FigRect` that was passed as an argument to `setBigPort()`. The subsequent code adds `getBigPort()` to the figures.

The removal of a deprecated method can be considered a bug fix, as deprecated methods are often associated with potential issues or future removal. However, the commit message focuses on consistency and deprecation removal rather than explicitly stating a bug fix. The code change itself doesn't clearly indicate a bug being fixed, but rather a cleanup and adherence to newer practices.

**Buggy**
"
docx4j,1091.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,26 +1,16 @@
     public static void main(String args[])
         throws IOException
     {
         if (args.length != 2)
         {
             System.err.println(
                 ""two arguments required: input filename and output filename"");
             System.exit(1);
         }
         FileInputStream  istream = new FileInputStream(args[ 0 ]);
-        try {
-            FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
-            try {
-                NPOIFSFileSystem fs = new NPOIFSFileSystem(istream);
-                try {
-                    fs.writeFilesystem(ostream);
-                } finally {
-                    fs.close();
-                }
-            } finally {
-                ostream.close();
-            }
-        } finally {
-            istream.close();
-        }
+        FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
+
+        new OPOIFSFileSystem(istream).writeFilesystem(ostream);
+        istream.close();
+        ostream.close();
     }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""repackaged poifs; password encryption (load/save) now works"" suggests two primary changes:

1.  **Repackaged poifs:** This indicates a reorganization of the `poifs` package structure, which is unlikely to be a bug fix in itself, but could be related to fixing dependency issues.
2.  **Password encryption (load/save) now works:** This strongly suggests that password encryption for loading and saving POIFS files was previously broken and has now been fixed. This is a clear indication of a bug fix.

**Git Diff Analysis:**

The diff shows a significant simplification of the `main` method within the provided Java code. The original code used nested `try-finally` blocks to ensure that both the input and output streams were closed, even if exceptions occurred during file processing. The modified code removes these nested blocks and directly closes the streams after the `writeFilesystem` operation. Also, the code now uses `OPOIFSFileSystem` instead of `NPOIFSFileSystem`.

**Reasoning:**

The commit message explicitly states that password encryption now works. This implies that there was a bug related to password encryption in the previous version. The code changes, while seemingly simplifying the stream handling, are likely related to the underlying fix for the password encryption issue. The change from `NPOIFSFileSystem` to `OPOIFSFileSystem` might be related to the password encryption implementation. The simplification of the `try-finally` blocks could be a consequence of changes in how the filesystem handles resource management after the encryption fix. The original code's complex `try-finally` structure suggests an attempt to handle potential errors during file operations, which might have been related to the password encryption bug.

**Conclusion:**

Based on the commit message and the code changes, it is highly probable that the commit includes a bug fix related to password encryption in POIFS.

**Buggy**
"
docx4j,6115.json,6fc95a3b720163091946b0c51ad6200c6d780fb4,"@@ -1,7 +1,7 @@
 	public boolean shouldTraverse(Object o) {
 		
-		System.out.println(o.getClass().getName() );
+//		System.out.println(o.getClass().getName() );
 		
 		return !(o instanceof P
 				|| o instanceof org.docx4j.wml.Tbl); 
 	}",NotBuggy,"trivial
",Buggy,"The commit message ""trivial"" is vague and doesn't provide much information about the purpose of the change.

The code diff shows a commented-out line: `// System.out.println(o.getClass().getName() );`. This line was likely used for debugging purposes. Removing or commenting out debugging statements is a common practice after a bug has been identified and fixed. Although the commit message is not descriptive, the code change suggests the removal of a debugging statement, which is often associated with bug fixing.

Reasoning:
1. **Commit Message:** The commit message ""trivial"" is uninformative.
2. **Code Change:** The code change involves commenting out a `System.out.println` statement, which is typically used for debugging.
3. **Inference:** The removal of debugging code suggests that the developer was investigating an issue and has now resolved it.

Conclusion: **Buggy**"
docx4j,25366.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<XMLGregorianCalendar> createDate(XMLGregorianCalendar value) {
+        return new JAXBElement<XMLGregorianCalendar>(_Date_QNAME, XMLGregorianCalendar.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Move to maven multi-module project (Work in progress)"" suggests a structural change to the project, specifically refactoring it into a multi-module Maven project. This typically involves reorganizing the codebase, adjusting dependencies, and modifying build configurations. It doesn't inherently imply a bug fix. The ""(Work in progress)"" part further reinforces that this is an ongoing refactoring effort.

**2. Git Diff Analysis:**

The diff shows a change in a method within what appears to be a JAXB-related class. The original method `createPDir` was creating a `JAXBElement<P.Dir>`. The modified method `createDate` now creates a `JAXBElement<XMLGregorianCalendar>`. The `_PDir_QNAME` is replaced by `_Date_QNAME` and the third parameter `P.class` is replaced by `null`.

**3. Relevance Assessment:**

The code change doesn't seem directly related to the commit message. The commit message talks about project structure, while the code change modifies a JAXB element creation method. It's possible that this code change is part of the refactoring process, but it's not immediately clear why this specific change is necessary for the multi-module conversion.  It's also possible that the original code had an incorrect type and this is fixing it. Without more context, it's hard to say definitively. However, the change from `P.Dir` to `XMLGregorianCalendar` suggests a potential data type correction, which could be related to fixing a bug.

**4. Conclusion:**

While the commit message focuses on refactoring, the code change itself hints at a potential data type correction, which could indicate a bug fix. Therefore, I will classify this as potentially related to a bug fix.

**Buggy**"
docx4j,20658.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public CTWrapSquare createCTWrapSquare() {
-        return new CTWrapSquare();
+    public CTPageSetup createCTPageSetup() {
+        return new CTPageSetup();
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",Buggy,"The commit message ""More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd"" indicates an update to the DrawingML (DML) schema based on a specific version (TC45 1.0 final) and an XSD file.

The provided code diff shows a change in a method that creates a specific type of object. The original code had a method `createCTWrapSquare` which created a `CTWrapSquare` object. The modified code has a method `createCTPageSetup` which creates a `CTPageSetup` object.

The change in method name and the type of object created suggests a schema update or a correction in the object model. The commit message supports this interpretation. The change from `CTWrapSquare` to `CTPageSetup` could be due to a correction in how the DML objects are created or organized based on the updated schema. This looks like a correction based on the new schema.

**Buggy**
"
docx4j,14816.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,28 +1,26 @@
   public static String toShortString(DiffXEvent e) {
     // an element to open
-    if (e instanceof OpenElementEvent) {
+    if (e instanceof OpenElementEvent)
       return '<'+((OpenElementEvent)e).getName()+'>';
     // an element to close
-    } else if (e instanceof CloseElementEvent) {
+    else if (e instanceof CloseElementEvent)
       return ""</""+((CloseElementEvent)e).getName()+'>';
     // an attribute
-    } else if (e instanceof AttributeEvent) {
+    else if (e instanceof AttributeEvent)
       return ""@""+((AttributeEvent)e).getName();
     // a word
-    } else if (e instanceof WordEvent) {
+    else if (e instanceof WordEvent)
       return '""'+((CharactersEventBase)e).getCharacters()+'""';
     // a white space event
-    } else if (e instanceof SpaceEvent) {
+    else if (e instanceof SpaceEvent)
       return ""_s_"";
     // a single character
-    } else if (e instanceof CharEvent) {
+    else if (e instanceof CharEvent)
       return '\''+((CharactersEventBase)e).getCharacters()+'\'';
     // an ignorable space event
-    } else if (e instanceof IgnorableSpaceEvent) {
+    else if (e instanceof IgnorableSpaceEvent)
       return ""_i_"";
     // a single line
-    } else if (e instanceof LineEvent) {
-      return ""L#""+((LineEvent)e).getLineNumber();
-    }
+    else if (e instanceof LineEvent) return ""L#""+((LineEvent)e).getLineNumber();
     return ""???"";
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,"The commit message ""update to diffx-0.7.4"" suggests an upgrade to a new version of the diffx library. The code changes involve simplifying `if-else` statements by removing redundant `else` keywords. These changes do not inherently indicate a bug fix. They are more related to code style or potentially taking advantage of changes in the upgraded library. There's no error handling, logical correction, or exception handling improvement.

**NotBuggy**"
docx4j,18855.json,beb51743e46de6c74bf4a56faa3fa1a4322fea8a,"@@ -1,3 +1,3 @@
-    public Theme.ExtLst createThemeExtLst() {
-        return new Theme.ExtLst();
+    public CTTextBody createCTTextBody() {
+        return new CTTextBody();
     }",NotBuggy,"Map the non-fonts bit to xsd:any, doesn't marshall namespaces properly with JAXB in Java 6, so model the non-font bits properly.
",Buggy,"The commit message indicates a fix related to namespace marshalling issues with JAXB in Java 6, specifically concerning non-font elements. The code diff shows a change from creating a `Theme.ExtLst` object to creating a `CTTextBody` object. While the diff itself doesn't scream ""bug fix"", the commit message strongly suggests that the original code was not handling non-font elements correctly, leading to marshalling problems. The change to `CTTextBody` likely represents a correction in how these elements are modeled to resolve the JAXB issue.

**Buggy**"
docx4j,25367.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<Drawing> createRDrawing(Drawing value) {
-        return new JAXBElement<Drawing>(_RDrawing_QNAME, Drawing.class, R.class, value);
+    public JAXBElement<Variant> createVariant(Variant value) {
+        return new JAXBElement<Variant>(_Variant_QNAME, Variant.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message ""Move to maven multi-module project (Work in progress)"" indicates a structural change to the project, likely involving the reorganization of code into multiple modules within a Maven project.

The provided diff shows a change in a method that creates a JAXBElement. Specifically, the method name changed from `createRDrawing` to `createVariant`, the type of the JAXBElement changed from `Drawing` to `Variant`, the QNAME changed from `_RDrawing_QNAME` to `_Variant_QNAME`, and the scope of the class `R` was removed.

These code changes don't appear to be directly related to fixing a bug. They seem to be related to refactoring or restructuring the code, which aligns with the commit message about moving to a multi-module Maven project. The changes suggest a renaming or restructuring of data models, which is a common task during project reorganization.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
docx4j,6963.json,c9021fdc912cccadd839aea24b132ca922147258,"@@ -1,40 +1,40 @@
-	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.foray.font.format.Panose soughtPanose) {
+	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.docx4j.fonts.foray.font.format.Panose soughtPanose) {
 
 		log.debug(""Looking for "" + soughtPanose);
 		
 		String resultingPanoseKey;
 		
 //		// First try panose space restricted to this font family
 //		2009 03 22 - we don't have physicalFontFamiliesMap any more		
 //		if (orignalKey!=null) {
 //			PhysicalFontFamily thisFamily = 
 //				physicalFontFamiliesMap.get( PhysicalFonts.getPhysicalFonts().get(orignalKey).getName() );					
 //			
 //			log.debug(""Searching within family:"" + thisFamily.getFamilyName() );
 //			
 //			resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, 
 //					thisFamily.getPhysicalFonts(), MATCH_THRESHOLD_INTRA_FAMILY);    
 //			if ( resultingPanoseKey!=null ) {
 //				log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
 //	        	fm.setPhysicalFont( PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey) );													
 //				return fm;
 //			}  else {
 //				log.warn(""No match in immediate font family"");
 //			}
 //		} else {
 //			log.debug(""originalKey was null."");
 //		}
 		
 		// Well, that failed, so search the whole space
 		
 		//fm.setDocumentFont(documentFontName); ???
 		resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, PhysicalFonts.getPhysicalFonts(),
 				MATCH_THRESHOLD); 
 		if ( resultingPanoseKey!=null ) {
 			log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
         	return PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey);													
 		}  else {
 			log.warn(""No match in panose space"");
 			return null;
 		}
 	}",NotBuggy,"Include source code for fop-fonts, as org.docx4j.fonts.fop.
Use FOP 1.0.
Move panose to org.docx4j.fonts.foray
Add LICENSE, NOTICE
",NotBuggy,"The commit message indicates a reorganization of code, including moving the `panose` class to a new package (`org.docx4j.fonts.foray`). The diff confirms this change by showing that the `getAssociatedPhysicalFont` method now uses `org.docx4j.fonts.foray.font.format.Panose` instead of `org.foray.font.format.Panose`. The commit message also mentions including source code for fop-fonts and using FOP 1.0, but this is not directly visible in the provided diff. The change in package suggests a restructuring rather than a bug fix.
**NotBuggy**"
docx4j,25396.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<BigInteger> createUi8(BigInteger value) {
+        return new JAXBElement<BigInteger>(_Ui8_QNAME, BigInteger.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message ""Move to maven multi-module project (Work in progress)"" indicates a structural change to the project, likely involving moving code into different modules or reorganizing the project structure to conform to a Maven multi-module setup.

The diff shows a change in a method that creates a JAXBElement. The original method `createPDir` created a JAXBElement for `P.Dir`, while the modified method `createUi8` creates a JAXBElement for `BigInteger`. The QName and class type have also changed. This change doesn't seem directly related to a bug fix but rather to a change in the data model or how JAXB elements are created. It's more likely related to adapting the code to the new Maven structure or a change in requirements. There's no specific error handling, logical correction, or exception handling improvement evident in the diff.

**NotBuggy**"
docx4j,14694.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,6 +1,7 @@
   public void delete(DiffXEvent e) {
-    if (e instanceof OpenElementEvent)
+    if (e instanceof OpenElementEvent) {
       push((OpenElementEvent)e, '-');
-    else if (e instanceof CloseElementEvent)
+    } else if (e instanceof CloseElementEvent) {
       pop();
+    }
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message:** ""update to diffx-0.7.4""

This commit message is quite generic. It simply states that the diffx library has been updated to version 0.7.4. It doesn't explicitly mention any bug fixes, new features, or specific issues addressed.

**Git Diff:**

```diff
--- a/src/main/java/uk/co/datumedge/diffx/format/TextDiff.java
+++ b/src/main/java/uk/co/datumedge/diffx/format/TextDiff.java
@@ -1,6 +1,7 @@
   public void delete(DiffXEvent e) {
-    if (e instanceof OpenElementEvent)
+    if (e instanceof OpenElementEvent) {
       push((OpenElementEvent)e, '-');
-    else if (e instanceof CloseElementEvent)
+    } else if (e instanceof CloseElementEvent) {
       pop();
+    }
   }
```

The diff shows a change in the `delete` method of the `TextDiff` class.  The original code had `if` and `else if` statements without curly braces. The updated code adds curly braces `{}` to both the `if` and `else if` blocks.

**Reasoning:**

The addition of curly braces around the `if` and `else if` blocks improves code clarity and maintainability. While the original code might have functioned correctly, it's generally considered good practice to always use curly braces, even for single-line blocks. This prevents potential errors if someone later adds more statements to the blocks without adding the braces.

However, this change doesn't necessarily indicate a bug fix. It's more likely a code style improvement or a defensive measure to prevent future bugs. The original code probably worked as intended, but the updated code is less prone to errors if modified later.

Given the generic commit message and the nature of the code change (adding curly braces), it's unlikely that this commit addresses a specific bug. It's more likely a general update or code style improvement.

**Conclusion:**

**NotBuggy**
"
docx4j,27318.json,fcdd0ece2e6c040ce5b870108f0e83bef14bce58,"@@ -1,7 +1,3 @@
     public String getDashstyle() {
-        if (dashstyle == null) {
-            return ""solid"";
-        } else {
-            return dashstyle;
-        }
+        return dashstyle;
     }",NotBuggy,"VML generated classes, based on ECMA 376 1ed (rather than earlier draft).  
Includes @XmlType namespace annotations from revs 940 and 1129.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""VML generated classes, based on ECMA 376 1ed (rather than earlier draft). Includes @XmlType namespace annotations from revs 940 and 1129"" suggests the following:

1.  **Schema Update:** The primary focus is on updating VML (Vector Markup Language) generated classes to align with the ECMA 376 1st edition standard. This implies a change in the underlying data structure or format being processed.
2.  **Namespace Annotations:** The commit also mentions incorporating `@XmlType` namespace annotations from specific revisions (940 and 1129). This likely involves adding or modifying XML-related annotations to the generated classes.

**Git Diff Analysis:**

The Git diff shows a change in the `getDashstyle()` method:

```diff
@@ -1,7 +1,3 @@
     public String getDashstyle() {
-        if (dashstyle == null) {
-            return ""solid"";
-        } else {
-            return dashstyle;
-        }
+        return dashstyle;
     }
```

The original code included a null check for the `dashstyle` field. If `dashstyle` was null, it would return ""solid""; otherwise, it would return the value of `dashstyle`. The modified code simply returns the value of `dashstyle` without any null check.

**Reasoning:**

The removal of the null check and default value suggests a potential bug fix or a change in the expected behavior. Here's why:

*   **Potential NullPointerException:** If `dashstyle` is null, the modified code will now throw a `NullPointerException` when `getDashstyle()` is called. This could be intentional if the code is now designed to handle null `dashstyle` values differently at a higher level or if the `dashstyle` field is now guaranteed to be non-null due to the schema update.
*   **Schema Change Implication:** The commit message mentions updating to ECMA 376 1ed. It's possible that the updated schema now mandates that `dashstyle` always has a value, eliminating the need for the null check.
*   **Implicit Bug Fix:** The original code's default return of ""solid"" when `dashstyle` was null could have masked an underlying issue where `dashstyle` was not being properly initialized. Removing the default value forces the code to handle the null case explicitly, potentially revealing and fixing a bug.

Given these points, the removal of the null check and default value in `getDashstyle()` suggests a bug fix or a change related to how null `dashstyle` values are handled, likely driven by the schema update.

**Conclusion:**

**Buggy**
"
docx4j,22538.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public void setExt(CTOfficeArtExtension value) {
-        this.ext = value;
+    public void setExtLst(CTOfficeArtExtensionList value) {
+        this.extLst = value;
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",NotBuggy,"The commit message ""More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd"" suggests an update to the Data Modeling Language (DML) based on a specific schema. The code diff shows a change from `setExt(CTOfficeArtExtension value)` to `setExtLst(CTOfficeArtExtensionList value)`. This indicates a change in the data structure, likely to accommodate a list of extensions instead of a single extension.

The change from `ext` to `extLst` and the corresponding type change from `CTOfficeArtExtension` to `CTOfficeArtExtensionList` strongly suggests a modification to align with the updated DML schema. While it's not explicitly a bug fix, the commit message indicates an update to be more complete based on a final schema. It's possible the previous implementation was incomplete or didn't fully adhere to the schema, which could be considered a bug or deficiency. However, without more context, it's difficult to definitively say if this change addresses a specific bug. The change seems more related to feature enhancement or schema compliance.

**NotBuggy**"
docx4j,8715.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public R createR() {
-        return new R();
+    public CTDefinedName createCTDefinedName() {
+        return new CTDefinedName();
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message ""Move to maven multi-module project (Work in progress)"" indicates a refactoring or restructuring of the project to a multi-module Maven project. The provided diff shows a change from `createR()` to `createCTDefinedName()` and the return type from `R` to `CTDefinedName`. This change seems to be related to adapting the code to the new project structure, possibly renaming classes or factories to fit within the new module organization. There is no indication of bug fixing in the provided diff.

**NotBuggy**"
docx4j,31212.json,ed8d15d352d0a2eaeac306e4ba9a3c83a3547558,"@@ -1,3 +1,3 @@
-        public BooleanDefaultTrue getSemiHidden() {
-            return semiHidden;
-        }

+    public BooleanDefaultTrue getSemiHidden() {
+        return semiHidden;
+    }",NotBuggy,"Change CT_FldChar to FldChar;
Style is freestanding, rather than an inner class of Styles.
(Other changes largely whitespace in license header)
",NotBuggy,"The commit message indicates a class name change from `CT_FldChar` to `FldChar` and mentions that it's now a freestanding class instead of an inner class of `Styles`. It also mentions whitespace changes in the license header. The provided diff shows a change in indentation for the `getSemiHidden()` method. This change doesn't seem related to the class name change or the class's location. It's more likely a formatting or style change. Since the commit message mentions whitespace changes, this diff aligns with that part of the message. There is no indication of a bug fix.
**NotBuggy**"
docx4j,1661.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,22 +1,48 @@
-   public static void main(final String args[]) throws IOException {
-      if (args.length == 0) {
-         System.err.println(""Must specify at least one file to view"");
-         System.exit(1);
-      }
-
-      boolean withSizes = false;
-      boolean newPOIFS = true;
-      for (int j = 0; j < args.length; j++) {
-         if (args[j].equalsIgnoreCase(""-size"") || args[j].equalsIgnoreCase(""-sizes"")) {
-            withSizes = true;
-         } else if (args[j].equalsIgnoreCase(""-old"") || args[j].equalsIgnoreCase(""-old-poifs"")) {
-            newPOIFS = false;
-         } else {
-            if(newPOIFS) {
-               viewFile(args[j], withSizes);
-            } else {
-               viewFileOld(args[j], withSizes);
+    public static void main(String[] args) throws Exception {
+        if (args.length == 0) {
+            System.err.println(""Must specify at least one file to dump"");
+            System.exit(1);
+        }
+        
+        boolean dumpProps = false, dumpMini = false;
+        for (int i = 0; i < args.length; i++) {
+            if (args[i].equalsIgnoreCase(""-dumprops"") ||
+                args[i].equalsIgnoreCase(""-dump-props"") ||
+                args[i].equalsIgnoreCase(""-dump-properties"")) {
+                dumpProps = true;
+                continue;
             }
-         }
-      }
+            if (args[i].equalsIgnoreCase(""-dumpmini"") ||
+                args[i].equalsIgnoreCase(""-dump-mini"") ||
+                args[i].equalsIgnoreCase(""-dump-ministream"") ||
+                args[i].equalsIgnoreCase(""-dump-mini-stream"")) {
+                dumpMini = true;
+                continue;
+            }
+            
+            System.out.println(""Dumping "" + args[i]);
+            FileInputStream is = new FileInputStream(args[i]);
+            NPOIFSFileSystem fs = new NPOIFSFileSystem(is);
+            is.close();
+
+            DirectoryEntry root = fs.getRoot();
+            File file = new File(root.getName());
+            file.mkdir();
+
+            dump(root, file);
+            
+            if (dumpProps) {
+                HeaderBlock header = getHeaderBlock(fs);
+                dump(fs, header.getPropertyStart(), ""properties"", file);
+            }
+            if (dumpMini) {
+                NPropertyTable props = getPropertyTable(fs);
+                int startBlock = props.getRoot().getStartBlock(); 
+                if (startBlock == POIFSConstants.END_OF_CHAIN) {
+                    System.err.println(""No Mini Stream in file"");
+                } else {
+                    dump(fs, startBlock, ""mini-stream"", file);
+                }
+            }
+        }
    }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",Buggy,"The commit message indicates a refactoring of the poifs package and the addition of password encryption functionality for loading and saving files. The diff shows changes to the `main` method, specifically how it handles command-line arguments and processes files. The original code had options for specifying sizes and using an old POIFS format. The updated code introduces options for dumping properties and the mini stream. The code now opens a file, creates a `NPOIFSFileSystem`, and then calls a `dump` method. It also retrieves the `HeaderBlock` and `PropertyTable` to dump properties and the mini stream if the corresponding flags are set. The addition of error handling for the case when the mini stream is not present suggests that the original code might have had issues when dealing with files without a mini stream.

**Buggy**
"
atmosphere,414.json,843374c2f23f04dfeca68025d80a9fc5c89846b5,"@@ -1,15 +1,15 @@
     public final static AtmosphereRequest wrap(HttpServletRequest request) {
         // Do not rewrap.
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
-            return AtmosphereRequestImpl.class.cast(request);
+            return (AtmosphereRequestImpl) request;
         }
 
         Builder b = new Builder();
         Enumeration<String> e = request.getAttributeNames();
         String s;
         while (e.hasMoreElements()) {
             s = e.nextElement();
             b.localAttributes.put(s, attributeWithoutException(request, s));
         }
         return b.request(request).build();
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Update code to JDK8"" suggests a general update to align the code with Java 8 standards or features. It doesn't explicitly mention any bug fixes.

**Git Diff Analysis:**

The diff shows a single change within the `wrap` method:

```diff
-            return AtmosphereRequestImpl.class.cast(request);
+            return (AtmosphereRequestImpl) request;
```

This change replaces the `Class.cast()` method with a direct cast `(AtmosphereRequestImpl) request`. This is a common change when migrating to newer Java versions, as direct casting can be more concise and potentially slightly more performant.

**Reasoning:**

1.  **No Bug Fix Indication:** The commit message focuses on a general update for JDK8 compatibility rather than addressing a specific bug.
2.  **Code Change is a Style/Compatibility Update:** The code change itself (replacing `Class.cast()` with a direct cast) is a typical modification made during Java version upgrades. It doesn't inherently suggest a bug was present and being fixed. It's more about using a more modern or preferred syntax.
3.  **No Error Handling or Logic Corrections:** The diff doesn't show any changes related to error handling, logical corrections, or exception handling. These types of changes would be strong indicators of a bug fix.

**Conclusion:**

Based on the commit message and the nature of the code change, it's unlikely that this commit represents a bug fix. It appears to be a stylistic/compatibility update for JDK8.

**NotBuggy**
"
atmosphere,1507.json,add336c1cc60c8189b00d21dd1a7b3408bc4754d,"@@ -1,6 +1,6 @@
     public void reconfigureInitParams(boolean reconfigureInitParams) {
         if (reconfigureInitParams) {
-            doInitParams(servletConfig);
+            doInitParams(servletConfig, reconfigureInitParams);
             doInitParamsForWebSocket(servletConfig);
         }
     }",NotBuggy,"Do not allow reloading the AsyncSupport from init-param
",Buggy,"The commit message ""Do not allow reloading the AsyncSupport from init-param"" suggests a restriction or prevention of reloading AsyncSupport configurations from initialization parameters. The code diff shows a modification in the `reconfigureInitParams` method where `doInitParams` is called with an additional parameter `reconfigureInitParams`. This change likely controls whether the initialization parameters are reloaded or not. The commit message indicates a bug fix or a change to prevent unintended reloading of configurations, which can be considered a bug fix.

**Buggy**"
atmosphere,1267.json,5241afdf818b484412c44d27d97a2c7e4dacbb71,"@@ -1,11 +1,11 @@
     private static void fallbackToManualAnnotatedClasses(Class<?> mainClass, AtmosphereFramework f, AnnotationHandler handler) {
         logger.warn(""Unable to detect annotations. Application may fail to deploy."");
         f.annotationScanned(true);
-        for (Class a : coreAnnotations) {
+        for (Class<?> a : coreAnnotations) {
             try {
                 handler.handleProcessor(loadClass(mainClass, a.getName()));
             } catch (Exception e) {
                 logger.trace("""", e);
             }
         }
     }",NotBuggy,"Refactor
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message.

The commit message is simply ""Refactor"". This suggests a change that improves the code's structure or readability without necessarily fixing a bug.

The code diff shows a change in the type of the `a` variable in the enhanced for loop from `Class` to `Class<?>`. This change makes the code more type-safe by explicitly stating that the `Class` object can represent any type. There's also a change in the exception handling, specifically using `logger.trace("""", e)` instead of `logger.error("""", e)`. This suggests a change in the logging level, potentially to reduce the verbosity of the logs when an exception occurs during annotation processing. The change from `logger.warn` to `logger.trace` indicates that the exception is not considered a critical error anymore, which is a sign of refactoring.

Reasoning:

1.  **Commit Message:** The commit message ""Refactor"" indicates a code improvement rather than a bug fix.
2.  **Type Safety:** Changing `Class` to `Class<?>` enhances type safety, a common refactoring practice.
3.  **Logging Level:** Changing `logger.warn` to `logger.trace` suggests a change in the severity of the logged message, which is also a common refactoring practice.

Conclusion:

**NotBuggy**
"
atmosphere,2813.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,9 +1,9 @@
         public void onMessage(WebSocket socket, String text) {
             super.onMessage(socket, text);
             LOGGER.trace(""onMessage(String) {} "", socket);
-            DefaultWebSocket g2w = DefaultWebSocket.class.cast(socket);
+            DefaultWebSocket g2w = (DefaultWebSocket) socket;
             org.atmosphere.websocket.WebSocket webSocket = (org.atmosphere.websocket.WebSocket) g2w.getUpgradeRequest().getAttribute(""grizzly.webSocket"");
             if (webSocket != null) {
                 webSocketProcessor.invokeWebSocketProtocol(webSocket, text);
             }
         }",NotBuggy,"Update code to JDK8
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Update code to JDK8"" suggests a general update to align the code with Java 8 standards. This could involve changes to syntax, use of new APIs, or removal of deprecated features. It doesn't explicitly mention fixing a bug.

**Code Diff Analysis:**

The code diff shows a change in how a `DefaultWebSocket` object is cast from a `WebSocket` object.

- **Original Code:** `DefaultWebSocket g2w = DefaultWebSocket.class.cast(socket);`
- **Modified Code:** `DefaultWebSocket g2w = (DefaultWebSocket) socket;`

The original code uses `DefaultWebSocket.class.cast(socket)` which is a type-safe casting method. The modified code uses a direct cast `(DefaultWebSocket) socket`.

**Reasoning:**

1.  **Type Casting Change:** The change from `DefaultWebSocket.class.cast(socket)` to `(DefaultWebSocket) socket` is likely a simplification made possible or preferred by Java 8. The original code is a more verbose way of doing a type cast. The new code is more concise.

2.  **No Error Handling:** There is no added error handling (e.g., try-catch blocks) or checks for null values that would suggest a bug fix related to unexpected input.

3.  **No Logic Correction:** The core logic of the `onMessage` method remains the same. The change only affects the way the `socket` object is cast to `DefaultWebSocket`.

4.  **Alignment with Commit Message:** The code change aligns with the commit message ""Update code to JDK8"" as it simplifies the casting syntax, which is a common practice when migrating to newer Java versions.

**Conclusion:**

The code change appears to be a stylistic update to align with Java 8 practices rather than a bug fix. Therefore, the answer is:

**NotBuggy**
"
atmosphere,588.json,b5b88b095c89dbb9914fd9ec8e0d999e2007fc55,"@@ -1,3 +1,7 @@
     public static AtmosphereResourceFactory resourceFactory() {
+        if (resourceFactoryDuplicate) {
+            throw new IllegalStateException(
+                    ""More than one instance has been stored. Universe cannot be used."");
+        }
         return resourceFactory;
     }",NotBuggy,"Throw exception if Universe methods are used when they are not reliable

The previous version causes a lot of logging about Universe not being reliable
even though you never use Universe. This changes the behavior so that you will
instead get an exception only when actually using the methods, when they will not
work properly.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message states that the previous version produced excessive logging even when Universe methods were not in use. This suggests an undesirable behavior that needed correction.
    *   The commit message explicitly mentions that the change introduces an exception when Universe methods are used in an unreliable state. This indicates a shift from logging to throwing an exception, which is a change in error handling.
    *   The phrase ""when they will not work properly"" suggests that the previous behavior was incorrect or unreliable, implying a bug.

2.  **Git Diff Analysis:**
    *   The diff introduces a check (`resourceFactoryDuplicate`) and throws an `IllegalStateException` if the check fails.
    *   The exception message ""More than one instance has been stored. Universe cannot be used."" clearly indicates a condition where the Universe methods would not function correctly.
    *   The code change prevents the use of `resourceFactory` if `resourceFactoryDuplicate` is true, suggesting a fix to prevent incorrect usage.

3.  **Relevance and Bug Fix Indication:**
    *   The commit message and the code change are highly relevant. The code implements the behavior described in the commit message (throwing an exception when Universe methods are used in an unreliable state).
    *   The change addresses the issue of excessive logging (by removing it) and prevents potentially incorrect behavior by throwing an exception. This strongly suggests a bug fix. The previous behavior was undesirable (excessive logging) and potentially led to incorrect results if the Universe methods were used when they were not reliable.

**Conclusion:**

**Buggy**
"
atmosphere,958.json,e1a548d5c483168f07249763b2b5e1971bfeaf1e,"@@ -1,5 +1,5 @@
-    protected void endRequest(AtmosphereResourceImpl r, boolean cancel) {
+    public void endRequest(AtmosphereResourceImpl r, boolean cancel) {
         if (completeLifecycle(r, cancel)) {
             config.framework().notify(Action.TYPE.CANCELLED, r.getRequest(false), r.getResponse(false));
         }
     }",NotBuggy,"Improve Logging, make method public
",NotBuggy,"The commit message ""Improve Logging, make method public"" suggests two changes: improved logging and making a method public.

The diff shows that the visibility of the `endRequest` method is changed from `protected` to `public`. There is no evidence of logging improvements in the provided diff.

The change in visibility from `protected` to `public` does not indicate a bug fix. It is a change in the API of the class.

**NotBuggy**"
atmosphere,2102.json,6603c01d44e431a43c5ae468feb76b09243d72d5,"@@ -1,35 +1,36 @@
     protected void retryInjection(AtmosphereFramework framework){
+        int maxTryPerCycle = maxTry;
         // Give another chance to injection in case we failed at first place. We may still fail if there is a strong
         // dependency between Injectable, e.g one depend on other, or if the Injectable is not defined at the right place
         // in META-INF/services/org/atmosphere/inject.Injectable
         Set<Field> fields = new HashSet<Field>();
         Object instance = null;
         final LinkedHashSet<Object> postponedMethodExecution = new LinkedHashSet<>(pushBackInjection);
-        while (!pushBackInjection.isEmpty() & maxTry-- > 0) {
+        while (!pushBackInjection.isEmpty() & maxTryPerCycle-- > 0) {
             Iterator<Object> t = new LinkedList(pushBackInjection).iterator();
             pushBackInjection.clear();
             while (t.hasNext()) {
                 instance = t.next();
                 fields.addAll(getInheritedPrivateFields(instance.getClass()));
                 try {
                     injectFields(fields, instance, framework, injectables);
                 } catch (IllegalAccessException e) {
                     logger.warn("""", e);
                 } finally {
                     fields.clear();
                 }
             }
         }
 
         if (!pushBackInjection.isEmpty()) {
             injectionFailed();
         }
 
         for (Object o : postponedMethodExecution) {
             try {
                 applyMethods(o, (Class<Object>) o.getClass());
             } catch (IllegalAccessException e) {
                 logger.warn("""", e);
             }
         }
     }",NotBuggy,"Re-try Injectable Injection: reset counter
",Buggy,"The commit message ""Re-try Injectable Injection: reset counter"" suggests a fix related to the retry mechanism for injectable injection. The code diff shows a modification where `maxTry` is copied to a local variable `maxTryPerCycle` and the loop condition uses `maxTryPerCycle` instead of `maxTry`. This change ensures that the original `maxTry` field is not modified during the retry attempts, effectively resetting the counter for each injection cycle or request. This addresses a potential bug where the retry counter might not be properly reset, leading to incorrect retry behavior.

**Buggy**"
atmosphere,3067.json,8c46e966a9a873af2cf4d6b4f7577d7975d62d40,"@@ -1,3 +1,3 @@
     public String getContainerName() {
-        return super.getContainerName() + "" using javax.servlet/3.0 and jsr356/WebSocket API"";
+        return super.getContainerName() + "" and jsr356/WebSocket API"";
     }",NotBuggy,"Cosmetic
",NotBuggy,"The commit message ""Cosmetic"" suggests a minor, superficial change. The diff shows a modification to the `getContainerName()` method, specifically altering the string that is returned. The phrase ""using javax.servlet/3.0"" has been removed from the string. This change doesn't seem to be related to fixing a bug, but rather to improve the appearance or clarity of the container name.

**NotBuggy**
"
atmosphere,2596.json,8c2994c415cab838ba6a4571eecb8934cf81c34b,"@@ -1,2 +1,2 @@
-                public void shutdown() {
-                }

+    public void shutdown() {
+    }",NotBuggy,"Refactor, no functional change
",Buggy,"The commit message states ""Refactor, no functional change,"" indicating that the commit aims to improve the code's structure or readability without altering its behavior.

The provided diff shows a change in indentation. This change does not introduce new functionality, fix a bug, or modify the program's behavior. It solely affects the code's formatting, aligning with the commit message's intention of refactoring without functional changes.

Therefore, the changes indicate a refactoring effort focused on code style rather than bug fixing.

**Buggy**"
atmosphere,415.json,b9625cac7eaae32687fa4f10939f9d6cbb9749e5,"@@ -1,63 +1,63 @@
     public final static AtmosphereRequest cloneRequest(HttpServletRequest request, boolean loadInMemory, boolean copySession, boolean isDestroyable, boolean createSession) {
         Builder b;
         HttpServletRequest r;
 
         Cookie[] cs = request.getCookies();
         Set<Cookie> hs = Collections.synchronizedSet(new HashSet());
         if (cs != null) {
             for (Cookie c : cs) {
                 hs.add(c);
             }
         }
 
         boolean isWrapped = false;
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
             b = AtmosphereRequestImpl.class.cast(request).b;
             isWrapped = true;
         } else {
             b = new Builder();
             b.request(request);
         }
 
-        HttpSession session = null;
+        HttpSession session = request.getSession(false);
         if (copySession) {
             session = request.getSession(createSession);
             if (session != null) {
                 session = new FakeHttpSession(session);
             } else {
                 session = new FakeHttpSession("""", null, System.currentTimeMillis(), -1);
             }
         }
 
         b.servletPath(request.getServletPath())
                 .pathInfo(request.getPathInfo())
                 .contextPath(request.getContextPath())
                 .requestURI(request.getRequestURI())
                 .requestURL(request.getRequestURL().toString())
                 .method(request.getMethod())
                 .serverName(request.getServerName())
                 .serverPort(request.getServerPort())
                 .remoteAddr(request.getRemoteAddr())
                 .remoteHost(request.getRemoteHost())
                 .remotePort(request.getRemotePort())
                 .destroyable(isDestroyable)
                 .cookies(hs)
                 .session(session)
                 .principal(request.getUserPrincipal())
                 .authType(request.getAuthType())
                 .isSSecure(request.isSecure());
 
         if (loadInMemory) {
             String s = (String) attributeWithoutException(request, FrameworkConfig.THROW_EXCEPTION_ON_CLONED_REQUEST);
             boolean throwException = s != null && Boolean.parseBoolean(s);
             r = new NoOpsRequest(throwException);
             if (isWrapped) {
                 load(b.request, b);
             } else {
                 load(request, b);
             }
             b.request(r);
         }
 
         return isWrapped ? AtmosphereRequestImpl.class.cast(request) : b.build();
     }",NotBuggy," #2157 use the original session by default when cloning a request
",Buggy,"The commit message indicates a change in how sessions are handled when cloning a request, specifically aiming to use the original session by default.

The code diff modifies the session handling logic within the `cloneRequest` method.

Here's a breakdown of the changes:

1.  **Original Code:**
    *   `HttpSession session = null;` - Initializes the session to null.
    *   `if (copySession) { ... }` -  Conditionally creates a new `FakeHttpSession` based on the `createSession` flag. If `createSession` is true, it gets the session from the request. If the session is not null, it wraps it in `FakeHttpSession`. If the session is null, it creates a new `FakeHttpSession`.

2.  **Modified Code:**
    *   `HttpSession session = request.getSession(false);` - Attempts to retrieve the existing session without creating a new one if one doesn't exist.
    *   `if (copySession) { ... }` - The logic inside this block remains mostly the same, except now it operates on the `session` obtained in the previous line.

The key change is the initialization of the `session` variable. Previously, it was `null` by default. Now, it attempts to retrieve the existing session using `request.getSession(false)`. This aligns with the commit message's intention to use the original session by default. The subsequent `if (copySession)` block then handles the creation of a `FakeHttpSession` if `copySession` is true.

This change addresses a potential issue where the original session was not being used when cloning a request, which could lead to unexpected behavior or data loss. The commit message and the code changes align, and the changes are designed to correct a potentially incorrect behavior.

**Buggy**
"
atmosphere,2073.json,894a09e40995ab99985763f1cdd1e05fed0fd18b,"@@ -1,3 +1,3 @@
-    public boolean supportedType(Type t) {
-        return false;
+    public Object injectable(AtmosphereResource resource) {
+        return null;
     }",NotBuggy,"Refactor Injection
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** ""Refactor Injection"" suggests a general code improvement or restructuring related to dependency injection. It doesn't explicitly mention fixing a bug.

2.  **Code Diff:** The code diff shows a significant change in the method signature.
    *   The original method `supportedType(Type t)` which returns a boolean is replaced with `injectable(AtmosphereResource resource)` which returns an Object.
    *   The original method always returned `false`.

3.  **Analysis:**
    *   The commit message indicates a refactoring, which is a general code improvement.
    *   The code diff shows a change in method signature and a change in the return value. The original method always returned `false`, which could have been a placeholder or a bug. The change in method signature and return type suggests a significant change in functionality.
    *   The original method always returning `false` could be a bug.

**Conclusion:**

**Buggy**
"
atmosphere,11.json,1894ff3002e438f057a2e01b781ba5b538d6ca2b,"@@ -1,10 +1,9 @@
     public Object broadcast() {
         try {
-            return (Object) b.broadcast(message).get();
-        } catch (InterruptedException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
-        } catch (ExecutionException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
+            return b.broadcast(message).get();
+        }
+        catch (Exception ex) {
+            logger.error(""failed to broadcast message: "" + message, ex);
         }
         return null;
     }",NotBuggy,"migrated from jul logging to SLF4J API.  converted most usages of Exception.printStackTrace() and calls to System.out and System.err to logging calls.  tests now run with logback as the logging implementation and write output to stdout and log file, adjust logback-test.xml in test resources directory as appropriate.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message indicates a migration from `java.util.logging` (JUL) to SLF4J and a shift from using `System.out/err` and `printStackTrace()` to proper logging. This is primarily a change in logging infrastructure and practices.

**Git Diff Analysis:**

The diff shows a change in the `broadcast()` method. Specifically, the `InterruptedException` and `ExecutionException` catch blocks have been consolidated into a single `Exception` catch block. The logging within the catch block has also been updated to use the new SLF4J logger. The log message now includes the failed message.

**Reasoning:**

1.  **Error Handling Improvement:** The original code had separate catch blocks for `InterruptedException` and `ExecutionException`, logging each individually. The modified code consolidates these into a single `Exception` catch block. While this might seem like a simplification, it ensures that *any* exception during the `broadcast` operation is caught and logged. This is an improvement in robustness, as it prevents unhandled exceptions from potentially crashing the application or leading to unexpected behavior.
2.  **Logging Enhancement:** The change from `LoggerUtils.getLogger().log(Level.SEVERE, null, ex)` to `logger.error(""failed to broadcast message: "" + message, ex)` is a clear improvement in logging. The new message provides more context (the failed message) and uses the appropriate log level (`error`).
3.  **Potential Bug Fix:** The consolidation of exception handling suggests that the original code might have been missing some exception types. By catching `Exception`, the code now handles a broader range of potential errors, which could prevent unexpected behavior. This, combined with the improved logging, makes it easier to diagnose and fix issues.

**Conclusion:**

The changes improve error handling and logging, which are often associated with bug fixes or preventative measures against potential bugs. The consolidation of exception handling suggests a broader approach to catching errors, which could address previously unhandled scenarios.

**Buggy**
"
atmosphere,2900.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,5 +1,5 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         CometContext ctx = CometEngine.getEngine().getCometContext(atmosphereCtx);
         resume(r.getRequest(false), ctx);
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message ""Update code to JDK8"" suggests a general update to align the codebase with JDK8 standards. The provided diff shows a change in the `complete` method's return type from `AsyncSupport` to `AsyncSupport<AtmosphereResourceImpl>`. This change appears to be a type parameterization, likely to improve type safety and take advantage of generics introduced in Java 5 and enhanced in later versions, including JDK8. This doesn't seem to be directly related to fixing a specific bug but rather an enhancement or modernization of the code.

**NotBuggy**
"
atmosphere,3053.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,15 +1,15 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         AtmosphereRequest req = r.getRequest(false);
         CountDownLatch latch = null;
 
         if (req.getAttribute(LATCH) != null) {
             latch = (CountDownLatch) req.getAttribute(LATCH);
         }
 
         if (latch != null) {
             latch.countDown();
         } else if (req.getAttribute(AtmosphereResourceImpl.PRE_SUSPEND) == null) {
             logger.trace(""Unable to resume the suspended connection"");
         }
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message ""Update code to JDK8"" suggests a general update to align the code with JDK8 standards. The diff shows a change in the `complete` method's return type from `AsyncSupport` to `AsyncSupport<AtmosphereResourceImpl>`. This change involves adding a generic type parameter to the `AsyncSupport` class, which is consistent with updating the code to be compatible with JDK8's generics features. The code logic itself remains unchanged; only the type signature is updated. This doesn't appear to be a bug fix, but rather an adaptation to newer Java features.

**NotBuggy**"
mockito,57.json,f00adf79925cd424394da4bb471c98326d3b5017,"@@ -1,3 +1,3 @@
-    public static <T> NewOngoingStubbing<T> when(T methodCall) {
+    public static <T> OngoingStubbing<T> when(T methodCall) {
         return MOCKITO_CORE.when(methodCall);
     }",NotBuggy,"In order to keep exported, client-facing interfaces fluent, renamed NewOngoingStubbing into OngoingStubbing. Had to change one interface name as well in the process.

--HG--
rename : src/org/mockito/internal/progress/OngoingStubbing.java => src/org/mockito/internal/progress/IOngoingStubbing.java
rename : src/org/mockito/stubbing/NewOngoingStubbing.java => src/org/mockito/stubbing/OngoingStubbing.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401516
",NotBuggy,"The commit message indicates a renaming of interfaces to maintain a fluent API. The diff shows a change from `NewOngoingStubbing` to `OngoingStubbing` in the `when` method. This aligns perfectly with the commit message's intention of renaming interfaces. There's no indication of bug fixing, error handling, or logical corrections. The changes are purely refactoring for API design.

**NotBuggy**"
mockito,855.json,87670dcd2a782a8218fa3c559a5da20c9e181476,"@@ -1,3 +1,3 @@
-    public Location getLocation() {
+    public LocationImpl getLocation() {
         return invocation.getLocation();
     }",NotBuggy,"Renamed Location so that we can hoist up an interface to the public api.

--HG--
rename : src/org/mockito/internal/debugging/Location.java => src/org/mockito/internal/debugging/LocationImpl.java
",NotBuggy,"The commit message indicates a renaming operation to facilitate the creation of a public API interface. The diff shows a change in the return type of the `getLocation()` method from `Location` to `LocationImpl`. This change directly reflects the renaming described in the commit message. The purpose is refactoring, not bug fixing.

**NotBuggy**"
mockito,1295.json,3ea1dd62f553eb5e9c1b4502552479848aa12e5a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""same("" + new ValuePrinter().appendValue(wanted).toString() + "")"";
+        return ""same("" + ValuePrinter.print(wanted) + "")"";
     }",NotBuggy,"Tidied up unit tests and the documentation.
",NotBuggy,"The commit message indicates a focus on tidying up unit tests and documentation. The provided code diff shows a change in the `toString()` method, specifically replacing `new ValuePrinter().appendValue(wanted).toString()` with `ValuePrinter.print(wanted)`.

Reasoning:

1.  **Relevance to Commit Message:** The code change itself doesn't directly relate to unit tests or documentation. It's a code refactoring within the `toString()` method. It's possible this change was done to improve readability or maintainability, which could be considered ""tidying up,"" but it's a stretch.
2.  **Bug Fix Indicators:** There are no explicit error handling updates, logical corrections, or exception handling improvements in the diff. The change appears to be a simplification or refactoring of how the `wanted` value is printed. There's no indication that the previous version was buggy or incorrect.
3.  **Refactoring:** The change replaces object creation and method chaining with a static method call. This could be a performance optimization or a style preference, but it doesn't inherently suggest a bug fix.

Conclusion: **NotBuggy**
"
mockito,20.json,f7c1b82139cf23a37f9775ee07c6d8ce0952650e,"@@ -1,3 +1,4 @@
     public static boolean booleanThat(Matcher<Boolean> matcher) {
-        return reportMatcher(matcher).returnFalse();
+        reportMatcher(matcher);
+        return false;
     }",NotBuggy,removed HandyReturnValues,Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** ""removed HandyReturnValues"" - This suggests the commit is focused on removing or refactoring code related to a class or concept called ""HandyReturnValues."" It doesn't explicitly mention a bug fix.

2.  **Code Diff:**
    *   The code modifies a method `booleanThat`.
    *   The original code `return reportMatcher(matcher).returnFalse();` calls `reportMatcher` and then calls `returnFalse()` on the result. This suggests that `reportMatcher` returns an object of type `HandyReturnValues` which has a method `returnFalse()`.
    *   The modified code `reportMatcher(matcher); return false;` calls `reportMatcher` and then simply returns `false`. This indicates that the `HandyReturnValues` object is no longer being used, and the return value is directly set to `false`.

3.  **Analysis:**
    *   The commit message aligns with the code diff. The code removes the usage of `HandyReturnValues`.
    *   The original code might have been unnecessarily complex, using an object (`HandyReturnValues`) just to return a boolean value. The change simplifies the code by directly returning `false`.
    *   It's possible that `HandyReturnValues` was causing issues or was deemed redundant, leading to its removal.
    *   The change ensures that the method always returns `false` after calling `reportMatcher`. This could be a correction if the previous logic was not always behaving as expected.

**Conclusion:**

The change simplifies the code and removes a potentially problematic or redundant class. It's plausible that the original implementation using `HandyReturnValues` had unintended side effects or was simply an over-engineered solution. The removal of `HandyReturnValues` and the direct return of `false` could be addressing a subtle bug or unexpected behavior.

**Buggy**
"
mockito,590.json,3658876bfe641290844c7378da991952bbfcaa05,"@@ -1,3 +1,3 @@
-    private String pluralize(int number) {
+    public static String pluralize(int number) {
         return number == 1 ? ""1 time"" : number + "" times"";
     }",NotBuggy,"Made certain things simpler + further refactoring to get rid of cause-based stack traces

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401294
",NotBuggy,"The commit message indicates simplification and refactoring, specifically mentioning the removal of cause-based stack traces. The provided code diff changes the `pluralize` method from a private instance method to a public static method. This change doesn't seem directly related to stack traces or error handling. It appears to be a refactoring step to make the method more accessible and potentially reusable. There is no indication of bug fixing.

**NotBuggy**"
mockito,976.json,f668427341272626209d0b21d1cb1e349a7186ad,"@@ -1,3 +1,3 @@
-    private boolean canRecoverFromFailure(VerificationMode verificationMode) {
-        return !(verificationMode instanceof AtMost || verificationMode instanceof Only || verificationMode instanceof NoMoreInteractions);
+    protected boolean canRecoverFromFailure(VerificationMode verificationMode) {
+        return !(verificationMode instanceof AtMost || verificationMode instanceof NoMoreInteractions);
     }",NotBuggy,"Added early completion on definite failure to timeout() too, by refactoring to combine after() and timeout() implementations
",NotBuggy,"The commit message indicates a refactoring to combine `after()` and `timeout()` implementations, and adding early completion on definite failure to `timeout()`. The diff shows a change in the visibility of the `canRecoverFromFailure` method from `private` to `protected`. This change in visibility doesn't seem directly related to the functionality described in the commit message. It's possible this change was necessary for the refactoring, but it's not immediately clear from the diff alone. The commit message mentions ""definite failure"", which hints at a bug fix or improvement in error handling. However, the provided diff doesn't give enough context to confirm this. The change in visibility itself doesn't indicate a bug fix.

**NotBuggy**
"
mockito,251.json,67e4b5884a1ff5f314cf633194e468131d0f2b53,"@@ -1,16 +1,16 @@
     public <T> T createMock(MockCreationSettings<T> settings, MockHandler handler) {
-        Class<? extends T> type = bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings));
+        Class<? extends T> type = createMockType(settings);
 
         Instantiator instantiator = Plugins.getInstantiatorProvider().getInstantiator(settings);
         try {
             T instance = instantiator.newInstance(type);
             MockMethodInterceptor mockMethodInterceptor = new MockMethodInterceptor(asInternalMockHandler(handler), settings);
             mocks.put(instance, mockMethodInterceptor);
             if (instance instanceof MockAccess) {
                 ((MockAccess) instance).setMockitoInterceptor(mockMethodInterceptor);
             }
             return instance;
         } catch (org.mockito.internal.creation.instance.InstantiationException e) {
             throw new MockitoException(""Unable to create mock instance of type '"" + type.getSimpleName() + ""'"", e);
         }
     }",NotBuggy,"Rework MockMaker.createMockType to take MockCreationSettings
",NotBuggy,"The commit message ""Rework MockMaker.createMockType to take MockCreationSettings"" suggests a refactoring of the `createMock` method within the `MockMaker` class. Specifically, it indicates that the `createMockType` method, which is responsible for generating the mock class, has been modified to accept `MockCreationSettings` as an argument.

The provided diff shows that the line `Class<? extends T> type = bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings));` has been replaced with `Class<? extends T> type = createMockType(settings);`. This confirms that the `createMockType` method is now being used and that it accepts `MockCreationSettings`.

The rest of the code in the `createMock` method remains largely unchanged, focusing on instantiation and interception. The change directly reflects the commit message's intention to modify the `createMockType` method's signature and usage. There is no indication of bug fixing, error handling, or logical corrections in the diff. The change appears to be a refactoring to improve the design or maintainability of the code.

**NotBuggy**"
mockito,1818.json,4ac04a1e40e015a457a3cd05e9bcb3b2ba27410d,"@@ -1,3 +1,3 @@
         public T should(VerificationMode mode) {
-            return verify(mock, mode);
+            return Mockito.verify(mock, mode);
         }",NotBuggy,#NAME?,Buggy,"The commit message is missing. However, the code change replaces `verify(mock, mode)` with `Mockito.verify(mock, mode)`. This suggests that the original code might have been calling a method `verify` that was either ambiguous or incorrect, and it has been corrected to explicitly call the `verify` method from the `Mockito` class. This is likely a bug fix, as the original code probably didn't work as intended.

**Buggy**"
mockito,1051.json,9eebf1cfb0062e36634fd6d5bcbc047af0ae8fcc,"@@ -1,3 +1,3 @@
-    public void wantedButNotInvoked(PrintableInvocation wanted) {
+    public void wantedButNotInvoked(DescribedInvocation wanted) {
         throw new WantedButNotInvoked(createWantedButNotInvokedMessage(wanted));
     }",NotBuggy,"Made the PrintableInvocation deprecated and created DescribedInvocation instead. I didn't like the PrintableInvocation that much, DescribedInvocation better describes the purpose and lives in a better package, too.
",NotBuggy,"The commit message indicates a refactoring and renaming of a class, `PrintableInvocation` to `DescribedInvocation`, for better clarity and organization. The code diff shows that the method `wantedButNotInvoked` now accepts `DescribedInvocation` instead of `PrintableInvocation`. This change directly reflects the renaming mentioned in the commit message. There's no indication of bug fixing, error handling, or logical correction. The change seems to be purely driven by design preference.

**NotBuggy**
"
mockito,1329.json,4bc732153e83b9490d6d94c2e27ea2f6ac9dc10c,"@@ -1,3 +1,3 @@
     public Object getMock() {
-        return mock;
+        return toInspect;
     }",NotBuggy,"Removed unnecessary type, we can use public type instead

- Instead of MockitoMock, we can simply use our public API: MockingDetails. It allows us to reuse / dogfood our public API and have cleaner codebase.
- Added new public API: mockingDetails.getMock(). I needed it to implement APIs for Spring Boot integration. Plus, it just make sense to add this method to make MockingDetails type more complete.
",NotBuggy,"The commit message indicates a refactoring effort to remove an unnecessary type (MockitoMock) and utilize the public API MockingDetails instead. The change in the diff involves modifying the return value of the `getMock()` method from `mock` to `toInspect`.

The commit message states that the change allows to reuse the public API and have a cleaner codebase. It also mentions the addition of a new public API `mockingDetails.getMock()` which was needed for Spring Boot integration.

The change in the diff itself doesn't directly indicate a bug fix. It seems to be more about aligning the internal implementation with the public API and providing a new API method. The renaming of the return value from `mock` to `toInspect` suggests a change in how the mock object is accessed or represented, but without further context, it's hard to say if it's addressing a specific bug.

Therefore, based on the commit message and the diff, the changes don't seem to be related to a bug fix.

**NotBuggy**
"
mockito,1759.json,5f41dde62faa4c3c381d29cb550243fa2378e8d0,"@@ -1,4 +1,4 @@
     public static byte[] aryEq(byte[] value) {
-        LastArguments.reportMatcher(new ArrayEquals(value));
+        LastArguments.instance().reportMatcher(new ArrayEquals(value));
         return null;
     }",NotBuggy,"some refactorings, enabled some ingnored tests

--HG--
rename : src/org/mockito/exceptions/NotAMockMethodException.java => src/org/mockito/exceptions/MissingMethodInvocationException.java
rename : src/org/mockito/internal/ExpectedInvocation.java => src/org/mockito/internal/InvocationWithMatchers.java
rename : src/org/mockito/internal/MockitoOperations.java => src/org/mockito/internal/MockitoState.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%4030
",NotBuggy,"The commit message indicates refactoring and enabling ignored tests. The diff shows a change in the `aryEq` method where `LastArguments.reportMatcher` is replaced with `LastArguments.instance().reportMatcher`. This change suggests a modification to how matchers are reported, potentially related to how arguments are handled or accessed. While it's a refactoring, it doesn't directly indicate a bug fix. The change seems to be related to accessing a singleton instance rather than fixing a specific error.

**NotBuggy**"
mockito,715.json,e3d77c7cb21eb5e3cc8f240f2e9cab3273a3a515,"@@ -1,3 +1,3 @@
     public Stubber doThrow(Class<? extends Throwable> toBeThrown, Class<? extends Throwable>... nextToBeThrown) {
-        return null;
+        return stubber().doThrow(toBeThrown, nextToBeThrown);
     }",NotBuggy,"Implemented doThrow varag syntax for lenient() mode
",NotBuggy,"The commit message ""Implemented doThrow varag syntax for lenient() mode"" suggests the addition of variable arguments (varargs) functionality to the `doThrow` method, specifically for lenient mode.

The provided code diff shows a change in the `doThrow` method. The original implementation `return null;` is replaced with `return stubber().doThrow(toBeThrown, nextToBeThrown);`. This change indicates the implementation of the varargs functionality by delegating the call to `stubber().doThrow` with the provided exception classes.

The commit message accurately describes the change in the code. The change itself doesn't directly indicate a bug fix, but rather an enhancement or new feature implementation. There is no error handling, logical correction, or exception-handling improvement in the diff.

**NotBuggy**
"
mockito,849.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,3 +1,3 @@
-    public Invocation getInvocation() {
+    public InvocationImpl getInvocation() {
         return this.invocation;
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message indicates a renaming of `Invocation` to `InvocationImpl` and a shift towards using the implementation class where possible. The provided diff shows a change from returning `Invocation` to returning `InvocationImpl`. This aligns perfectly with the commit message's intention to use the concrete implementation class (`InvocationImpl`) instead of the interface (`Invocation`) where applicable. The change doesn't inherently suggest a bug fix, but rather a refactoring or code cleanup effort.

**NotBuggy**"
mockito,810.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,8 +1,8 @@
-    public LocationImpl getLastLocation(List<Invocation> invocations) {
+    public LocationImpl getLastLocation(List<InvocationImpl> invocations) {
         if (invocations.isEmpty()) {
             return null;
         } else {
-            Invocation last = invocations.get(invocations.size() - 1);
+            InvocationImpl last = invocations.get(invocations.size() - 1);
             return last.getLocation();
         }
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message indicates a renaming of `Invocation` to `InvocationImpl` and a preference for using the implementation class where possible. The provided code diff shows a change in the `getLastLocation` method where the type of the `invocations` list and the `last` variable are changed from `Invocation` to `InvocationImpl`. This aligns perfectly with the commit message. The change doesn't inherently indicate a bug fix, but rather a refactoring to use the concrete implementation.

**NotBuggy**"
mockito,867.json,ccb369df05a9724ac9a8ab7abfd797b0ab01097e,"@@ -1,5 +1,4 @@
     public Invocation createInvocation(Object target, MockCreationSettings settings, Method method, Callable realMethod, Object... args) {
-        //TODO SZ - move real method away from InterceptedInvocation
-        InterceptedInvocation.RealMethod.FromCallable superMethod = new InterceptedInvocation.RealMethod.FromCallable(realMethod);
+        RealMethod.FromCallable superMethod = new RealMethod.FromCallable(realMethod);
         return MockMethodInterceptor.createInvocation(target, method, args, superMethod, settings);
     }",NotBuggy,"Moved inner class to top level

This way we can use it in other context like the new public API for creating instances of Invocation.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Moved inner class to top level"" indicates a refactoring operation. The rationale is to improve code reusability by making the inner class accessible from other parts of the codebase, specifically mentioning a new public API. This suggests a design improvement rather than a direct bug fix.

**Git Diff Analysis:**

The diff shows that `InterceptedInvocation.RealMethod.FromCallable` is changed to `RealMethod.FromCallable`. This confirms that the `RealMethod` class (or interface) has been moved from being an inner class of `InterceptedInvocation` to a top-level class. The code now directly references `RealMethod.FromCallable` without the `InterceptedInvocation.` prefix.

**Reasoning:**

The commit message and the diff align perfectly. The change is a refactoring to move an inner class to the top level, which allows it to be used in a new public API. There's no indication of error handling, logical corrections, or exception handling improvements. The change is purely about improving code structure and reusability. Therefore, it's unlikely that this commit is related to a bug fix.

**Conclusion:**

**NotBuggy**
"
mongo-java-driver,189.json,35bbbae5b9afb9a83c76c21bebb547f2b71426d3,"@@ -1,3 +1,3 @@
-    public DBObject findOne(DBObject o, DBObject fields, DBObject orderBy) {
+    public DBObject findOne(final DBObject o, final DBObject fields, final DBObject orderBy) {
         return findOne(o, fields, orderBy, getReadPreference());
     }",NotBuggy,"Added interfaces to aid maintaining backward compatibility in driver-compat.  Unimplemented methods throw exceptions.  Added QuickTour to driver-compat to drive out unimplemented functionality
",NotBuggy,"The commit message indicates the addition of interfaces for backward compatibility and the introduction of exceptions for unimplemented methods. It also mentions adding a QuickTour to test unimplemented functionality.

The provided code diff modifies the `findOne` method signature by adding the `final` keyword to the parameters. This change itself doesn't directly indicate a bug fix. It's more related to code style or preventing accidental modification of the parameters within the method. The diff doesn't show any exception handling, logical corrections, or other bug-related changes. The addition of `final` keyword doesn't seem to be related to the backward compatibility or unimplemented functionality mentioned in the commit message.

Therefore, based on the provided diff, the changes do not indicate a bug fix.

**NotBuggy**
"
mongo-java-driver,8694.json,bcfca4ff8ecacdb7b1df6fcbb9c5d044ed4c431e,"@@ -1,6 +1,7 @@
     private BsonDocument getCommand(final ConnectionDescription description) {
-        BsonDocument commandDocument = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
+        BsonDocument command = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
                                                .append(""index"", new BsonString(indexName));
-        appendWriteConcernToCommand(writeConcern, commandDocument, description);
-        return commandDocument;
+        putIfNotZero(command, ""maxTimeMS"", maxTimeMS);
+        appendWriteConcernToCommand(writeConcern, command, description);
+        return command;
     }",NotBuggy,"Added maxTimeMS support for index operations

CreateIndexOperation and DropIndexOperation now support
maxTimeMS

JAVA-2537
",NotBuggy,"The commit message indicates the addition of `maxTimeMS` support for index operations, specifically `CreateIndexOperation` and `DropIndexOperation`. The provided diff shows a modification to the `getCommand` method, which constructs a BsonDocument for a ""dropIndexes"" command. The change introduces a `putIfNotZero` call to add the `maxTimeMS` value to the command document if it's not zero. This aligns perfectly with the commit message's intention of adding `maxTimeMS` support. There is no indication of a bug fix.

**NotBuggy**"
mongo-java-driver,11486.json,9ae1e5cf7cebeab5ffec07a4ef4203f199ea5ac7,"@@ -1,3 +1,3 @@
-    public int getInsertedCount() {
-        throw getUnacknowledgedWriteException();
-    }

+            public List<BulkWriteUpsert> getUpserts() {
+                throw getUnacknowledgedWriteException();
+            }",NotBuggy,"Updated BulkWriteResult to follow the past tense convention in core for acknowledged operations.
",NotBuggy,"The commit message indicates a change to adhere to a past tense naming convention for acknowledged operations, specifically within `BulkWriteResult`. The provided diff snippet shows a method `getUpserts()` being modified to throw an `UnacknowledgedWriteException`. This suggests the method's behavior is being updated to reflect the state of an unacknowledged write operation, which aligns with the commit message's intention to handle acknowledged operations correctly. The change doesn't directly indicate a bug fix, but rather a refinement in how unacknowledged write operations are handled and reported, bringing the class into alignment with established conventions.

**NotBuggy**
"
mongo-java-driver,6838.json,aa2cfa6c3692bd4c26a52e673c0c1a1b52b2b26c,"@@ -1,3 +1,3 @@
-    public WriteResult getWriteResult() {
-        return writeResult;
+    public WriteConcernResult getWriteConcernResult() {
+        return writeConcernResult;
     }",NotBuggy,"Changed property type in WriteConcernException from WriteResult to WriteConcernResult

 JAVA-1395
",Buggy,"The commit message indicates a change in the property type within the `WriteConcernException` class, specifically from `WriteResult` to `WriteConcernResult`. The provided code diff confirms this change, showing the getter method being updated to reflect the new property type. This change suggests a potential bug fix or an improvement in type correctness, aligning the exception's data with the appropriate result type.

**Buggy**
"
mongo-java-driver,787.json,c5da37698cd4c57932a05dd0bdd51685d277ad49,"@@ -1,3 +1,7 @@
     private BsonDocument wrap(DBObject document, DBEncoder encoder) {
-        return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        if (encoder == null) {
+            return wrap(document);
+        } else {
+            return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        }
     }",NotBuggy,"driver-compat tests all passing except for field name validation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""driver-compat tests all passing except for field name validation"" suggests that the commit aims to address issues identified during driver compatibility testing. The phrase ""except for field name validation"" indicates a specific area of concern that the commit likely targets.

**Git Diff Analysis:**

The diff shows a modification to the `wrap` method. Specifically, it adds a null check for the `encoder` parameter.

Before:

```java
private BsonDocument wrap(DBObject document, DBEncoder encoder) {
    return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
}
```

After:

```java
private BsonDocument wrap(DBObject document, DBEncoder encoder) {
    if (encoder == null) {
        return wrap(document);
    } else {
        return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
    }
}
```

The added `if (encoder == null)` condition suggests that the original code might have been throwing a `NullPointerException` or behaving unexpectedly when the `encoder` was null. The fix introduces a conditional branch: if `encoder` is null, it calls `wrap(document)` (presumably another overload of the `wrap` method); otherwise, it proceeds with the original logic. This is a classic error-handling pattern.

**Reasoning:**

1.  **NullPointerException Prevention:** The null check strongly indicates a potential `NullPointerException` if `encoder` is null. This is a common bug scenario.
2.  **Error Handling:** The added `if` statement is a clear attempt to handle a potentially erroneous situation (null `encoder`).
3.  **Test-Driven Development:** The commit message mentions ""driver-compat tests,"" suggesting that the null check was likely added as a result of a failing test case that exposed the null `encoder` scenario.
4.  **Alignment with Commit Message:** While the commit message doesn't explicitly mention a `NullPointerException`, the fix addresses a potential issue uncovered during compatibility testing, which aligns with the overall goal of making the driver compatible. The ""field name validation"" part of the commit message might be related to what happens in the `wrap(document)` overload that's called when the encoder is null, or it could be a separate issue addressed in a different part of the commit (not shown in the diff).

**Conclusion:**

The code change introduces a null check to prevent a potential `NullPointerException`, which is a common type of bug. The commit message indicates that this change is related to compatibility testing, further suggesting that it's addressing a bug.

**Buggy**
"
mongo-java-driver,9253.json,679f5ff0d4d7578ad1dea71f7caf3bf54f920051,"@@ -1,3 +1,3 @@
-    public BsonDocument getCriteria() {
-        return criteria;
+    public BsonDocument getFilter() {
+        return filter;
     }",NotBuggy,"Replaced all uses of criteria with filter in public APIs
",NotBuggy,"The commit message indicates a renaming of a public API from ""criteria"" to ""filter"". The provided diff shows a direct replacement of `getCriteria()` with `getFilter()`. This change aligns perfectly with the commit message. There's no indication of bug fixes, error handling, or logical corrections. It's purely a renaming exercise.

**NotBuggy**
"
mongo-java-driver,4729.json,85caac9ec625d101e7d678276027ed8c063f66a6,"@@ -1,3 +1,3 @@
-    public Publisher<Void> insertOne(final TDocument document) {
+    public Publisher<InsertOneResult> insertOne(final TDocument document) {
         return insertOne(document, new InsertOneOptions());
     }",NotBuggy,"Insertions now return a result

Added InsertOneResult and InsertManyResult to insert operations

JAVA-1788
",NotBuggy,"The commit message ""Insertions now return a result"" and ""Added InsertOneResult and InsertManyResult to insert operations"" clearly indicates a change in the return type of insertion operations. The diff shows that the `insertOne` method's return type has been changed from `Publisher<Void>` to `Publisher<InsertOneResult>`. This aligns perfectly with the commit message, suggesting an enhancement rather than a bug fix. There's no indication of error handling, logical corrections, or exception handling improvements in the provided diff. The change is about providing more information after an insertion operation, which is a feature enhancement.

**NotBuggy**
"
mongo-java-driver,2768.json,84d8d7c95e0ebaa072717816471d4cf1c9539817,"@@ -1,13 +1,11 @@
     public T decode(final BsonReader reader, final DecoderContext decoderContext) {
-        if (decoderContext.hasCheckedDiscriminator()) {
-            if (!specialized) {
-                throw new CodecConfigurationException(""Cannot decode using an unspecialized generic ClassModel"");
-            }
-            InstanceCreator<T> instanceCreator = classModel.getInstanceCreator();
-            decodeProperties(reader, decoderContext, instanceCreator);
-            return instanceCreator.getInstance();
-        } else {
-            return getCodecFromDocument(reader, classModel.useDiscriminator(), classModel.getDiscriminatorKey(), registry,
-                    discriminatorLookup, this).decode(reader, DecoderContext.builder().checkedDiscriminator(true).build());
+        try {
+            return pojoCodec.decode(reader, decoderContext);
+        } catch (CodecConfigurationException e) {
+            throw new CodecConfigurationException(
+                    format(""An exception occurred when decoding using the AutomaticPojoCodec.%n""
+                            + ""Decoding into a '%s' failed with the following exception:%n%n%s%n%n""
+                            + ""A custom Codec or PojoCodec may need to be explicitly configured and registered to handle this type."",
+                            pojoCodec.getEncoderClass().getSimpleName(), e.getMessage()), e);
         }
     }",NotBuggy,"Added an automatic setting to the PojoCodecProvider

Automatically creates a PojoCodec for any unseen class that contains
serializable / deserializable properties.

JAVA-2567
",Buggy,"The commit message indicates the addition of an automatic setting to the `PojoCodecProvider`. The diff shows a change in the `decode` method. Specifically, the original logic is replaced by a `try-catch` block that attempts to decode using `pojoCodec.decode`. If a `CodecConfigurationException` occurs, it's caught, and a new `CodecConfigurationException` with a more informative message is thrown. This suggests that the original code might have been failing in certain scenarios, and the added `try-catch` block is a way to handle these failures more gracefully and provide better error messages. The change aims to improve error handling and provide more context when decoding fails, which is indicative of a bug fix or at least an improvement to handle potential issues.

**Buggy**"
mongo-java-driver,916.json,f148733678e3585a89dc2eb5b580cf37170c9396,"@@ -1,3 +1,3 @@
-                    public void apply(final GridFSFile gridFSFile) {
+                    public void accept(final GridFSFile gridFSFile) {
                         System.out.println(gridFSFile.getFilename());
                     }",NotBuggy,"Remove deprecated MongoIterable#forEach(Block<T>) method

JAVA-3194
",NotBuggy,"The commit message indicates the removal of a deprecated method `MongoIterable#forEach(Block<T>)`. The provided diff shows a change from `apply` to `accept`. This change suggests a migration from a deprecated interface/method to a newer, recommended one. While deprecation often stems from design flaws or potential issues, the diff itself doesn't directly indicate a bug fix. The change seems to be more about API modernization and adherence to updated standards.

**NotBuggy**"
mongo-java-driver,6716.json,59906debe95ea760891c8bae855bca239575dc72,"@@ -1,3 +1,3 @@
-    public boolean getRetryWrites() {
+    public Boolean getRetryWritesValue() {
         return retryWrites;
     }",NotBuggy,"Only apply retryWrites/compressors from connection string if set

JAVA-3057
JAVA-3058
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Only apply retryWrites/compressors from connection string if set"" suggests a conditional application of connection string options. This implies that previously, these options might have been applied unconditionally, potentially leading to incorrect behavior or unexpected configurations when the user didn't explicitly set them in the connection string.
    *   The JIRA tickets ""JAVA-3057"" and ""JAVA-3058"" likely contain more context about the specific issue being addressed.

2.  **Code Diff Analysis:**
    *   The code diff shows a simple renaming of the method `getRetryWrites()` to `getRetryWritesValue()`.  This change by itself doesn't directly indicate a bug fix. However, the renaming suggests a change in how the `retryWrites` value is handled or interpreted. It's possible the renaming is part of a larger change where the `retryWrites` value is now treated as a `Boolean` object instead of a primitive `boolean`, allowing it to be `null` if not explicitly set in the connection string. This would align with the commit message.

3.  **Connecting the Dots:**
    *   The commit message talks about conditionally applying options from the connection string. The code change, while seemingly minor, likely plays a role in implementing this conditional logic. The renaming to `getRetryWritesValue()` suggests that the method now returns a `Boolean` object, which can be `null`, indicating that the `retryWrites` option was not explicitly set in the connection string. This allows the code to avoid applying a default or incorrect value in such cases.

**Conclusion:**

The commit message and code diff, when considered together, strongly suggest a bug fix. The change likely addresses a scenario where connection options were being applied even when they weren't explicitly specified in the connection string, leading to unexpected behavior. The renaming of the method to `getRetryWritesValue()` is likely part of a larger change to handle the absence of the `retryWrites` option gracefully.

**Buggy**"
mongo-java-driver,3068.json,fa60415971bd69a1e4cfbfc57fdfa1ca17ec329b,"@@ -1,7 +1,7 @@
-    private void writeList(final BsonWriter writer, final List<Object> list, final EncoderContext encoderContext) {
+    private void writeIterable(final BsonWriter writer, final Iterable<Object> list, final EncoderContext encoderContext) {
         writer.writeStartArray();
         for (final Object value : list) {
             writeValue(writer, encoderContext, value);
         }
         writer.writeEndArray();
     }",NotBuggy,"When encoding a Document, encode all Iterable instances as BSON arrays, rather than just List instances.  This is consistent with the behavior of the DBObject encoder,
so this will make it easier to migrate from DBObject to Document

 JAVA-1761
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message states that the change is to encode all `Iterable` instances as BSON arrays, not just `List` instances, when encoding a `Document`. It also mentions consistency with `DBObject` encoder behavior and easing migration. This suggests a potential inconsistency or limitation in the previous implementation.

2.  **Code Diff Analysis:**
    *   The method name `writeList` is changed to `writeIterable`.
    *   The parameter type `List<Object>` is changed to `Iterable<Object>`.
    *   The code within the method remains the same, iterating over the provided collection and writing each value.

3.  **Relationship between Commit Message and Code Diff:** The code diff directly implements the change described in the commit message. By changing the method to accept `Iterable` instead of `List`, the code now handles a broader range of collection types.

4.  **Bug Fix Indication:** The commit message highlights an inconsistency in how different collection types were handled. The previous implementation only handled `List` instances, while other `Iterable` types were likely not encoded correctly as BSON arrays. This indicates a bug or at least an incomplete implementation. The change to `Iterable` fixes this by providing consistent behavior for all iterable collections.

**Conclusion:**

**Buggy**
"
mongo-java-driver,9860.json,5ef2985d8da07b58d49b4c517b0e7276e7366ccc,"@@ -1,3 +1,3 @@
     public void deleteOne(final Bson filter, final SingleResultCallback<DeleteResult> callback) {
-        delete(filter, false, callback);
+        deleteOne(filter, new DeleteOptions(), callback);
     }",NotBuggy,"Collation updates

Collation set on a per operation basis rather than globally.

JAVA-2241
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Collation updates: Collation set on a per operation basis rather than globally. JAVA-2241"" suggests a change in how collation is handled.  Specifically, it moves from a global setting to a per-operation setting.  This could be due to a bug where the global collation setting was not appropriate for all operations, or it could be an enhancement to provide more flexibility. The mention of ""JAVA-2241"" likely refers to a specific issue tracker entry that could provide more context.

**Git Diff Analysis:**

The diff shows a change in the `deleteOne` method.  The original code `delete(filter, false, callback)` is replaced with `deleteOne(filter, new DeleteOptions(), callback)`. This indicates that the `deleteOne` method now accepts `DeleteOptions`, which likely allows specifying collation (and potentially other options) on a per-operation basis. The original `delete` method probably used a global collation setting.

**Reasoning:**

The commit message and diff strongly suggest a change related to how collation is handled.  The move from a global setting to a per-operation setting implies that there was a limitation or problem with the global approach. It's possible that the global collation was causing incorrect or unexpected behavior in certain scenarios, which would constitute a bug. The change to pass `DeleteOptions` to `deleteOne` provides the ability to specify collation per operation, which is a more flexible and potentially correct approach.

**Conclusion:**

**Buggy**
"
mongo-java-driver,2518.json,50f7d6cc46dd3b94ed46fef91dde77844245fd04,"@@ -1,6 +1,6 @@
-    public boolean istInt64(final Object key) {
+    public RegularExpression getRegularExpression(final Object key, final RegularExpression defaultValue) {
         if (!containsKey(key)) {
-            return false;
+            return defaultValue;
         }
-        return get(key).isInt64();
+        return get(key).asRegularExpression();
     }",NotBuggy,"Added missing BsonDocument and BsonValue methods, and added test specs for both classes
",Buggy,"The commit message indicates the addition of methods and test specifications for `BsonDocument` and `BsonValue`. The provided diff snippet shows a change in the `getRegularExpression` method. Specifically, it changes the return value when the key is not found from `false` to a provided `defaultValue`. It also changes the return type from boolean to `RegularExpression` and calls `asRegularExpression()` on the value. This suggests an attempt to provide a default value when a key is missing, which is a common pattern in bug fixes or feature enhancements to handle missing data gracefully. The original code likely threw an exception or returned null, which could lead to errors. The change ensures a `RegularExpression` is always returned, preventing potential null pointer exceptions or unexpected behavior.

**Buggy**
"
mongo-java-driver,8113.json,7cc4be219a15b08c69dfba8d02c3e014f808b90d,"@@ -1,36 +1,19 @@
-    public <A> void read(
-            final ByteBuffer dst,
-            final long timeout, final TimeUnit unit,
-            final A attach, final CompletionHandler<Integer, ? super A> handler) {
-        checkReadOnly(dst);
-        if (!dst.hasRemaining()) {
-            completeWithZeroInt(attach, handler);
-            return;
-        }
-        group.startRead(
-                registeredSocket,
-                new ByteBufferSet(dst),
-                timeout, unit,
-                new LongConsumer() {
-                    @Override
-                    public void accept(final long c) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.completed((int) c, attach);
-                            }
-                        });
-                    }
-                },
-                new Consumer<Throwable>() {
-                    @Override
-                    public void accept(final Throwable e) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.failed(e, attach);
-                            }
-                        });
-                    }
-                });
-    }

+  public <A> void read(
+      ByteBuffer dst,
+      long timeout,
+      TimeUnit unit,
+      A attach,
+      CompletionHandler<Integer, ? super A> handler) {
+    checkReadOnly(dst);
+    if (!dst.hasRemaining()) {
+      completeWithZeroInt(attach, handler);
+      return;
+    }
+    group.startRead(
+        registeredSocket,
+        new ByteBufferSet(dst),
+        timeout,
+        unit,
+        c -> group.executor.submit(() -> handler.completed((int) c, attach)),
+        e -> group.executor.submit(() -> handler.failed(e, attach)));
+  }",NotBuggy,"Revendored tlschannel to 0.3.2

JAVA-3588
",NotBuggy,"The commit message ""Revendored tlschannel to 0.3.2"" indicates that the tlschannel library has been updated to version 0.3.2. The diff shows changes to the `read` method, specifically replacing anonymous inner classes with lambda expressions for the `LongConsumer` and `Consumer<Throwable>` interfaces. This change improves code readability and conciseness but doesn't inherently indicate a bug fix. The functionality of the code remains the same.

**NotBuggy**
"
mongo-java-driver,5025.json,24780d53705de06d511af7e5cae18081449c9401,"@@ -1,11 +1,6 @@
     public Publisher<ObjectId> uploadFromStream(final ClientSession clientSession, final String filename, final AsyncInputStream source,
                                                 final GridFSUploadOptions options) {
-        return new SingleResultObservableToPublisher<ObjectId>(
-                new Block<SingleResultCallback<ObjectId>>() {
-                    @Override
-                    public void apply(final SingleResultCallback<ObjectId> callback) {
-                        wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source), options,
-                                callback);
-                    }
-                });
+        return new SingleResultObservableToPublisher<>(
+                callback -> wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source),
+                        options, callback));
     }",NotBuggy,"Publisher<Success> => Publisher<Void>

Removed the Success enum to represent a successful operation. Idioms
have changed since the 1.0 version of the MongoDB Reactive Streams driver
and although null is an invalid value for a publisher. A Publisher<Void>
has come to represent empty publisher that only signals when a publisher has
either completed or errored.

Removed the Scala type alias of Completed to Void. This brings the
implementation inline with the driver it wraps.

JAVA-3398
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message clearly states a change in the return type of a method from `Publisher<Success>` to `Publisher<Void>`. It explains that `Publisher<Void>` is now used to represent an empty publisher that signals completion or error, aligning with the current idioms of the MongoDB Reactive Streams driver. It also mentions the removal of a Scala type alias to further align with the driver. The commit message references ""JAVA-3398,"" which is likely a task or issue tracker ID, suggesting a specific reason for the change.

2.  **Git Diff:** The diff shows a change in the `uploadFromStream` method. Specifically, it refactors the creation of the `SingleResultObservableToPublisher`. The core logic of calling `wrapped.uploadFromStream` remains the same, but the way the callback is handled is simplified using a lambda expression. The return type of the method is implicitly changed due to the change in the `SingleResultObservableToPublisher`.

3.  **Relevance and Bug Fix Indication:**

    *   The commit message explicitly states that the change is to align with the updated idioms of the MongoDB Reactive Streams driver. This suggests that the previous implementation might have been using an outdated or incorrect approach.
    *   The change from `Publisher<Success>` to `Publisher<Void>` indicates a shift in how successful operations are signaled. The old approach used a `Success` enum, while the new approach uses the completion signal of the `Publisher<Void>`. This suggests a potential issue with how success was previously handled.
    *   The simplification of the callback handling using a lambda expression doesn't directly indicate a bug fix, but it improves code readability and maintainability.
    *   The reference to ""JAVA-3398"" suggests that this change was driven by a specific issue or task, which could potentially be related to a bug or an improvement to address a specific problem.

**Conclusion:**

Based on the commit message and the code changes, the commit appears to be addressing a change in the API and how success is signaled, which could be related to a bug fix or an improvement to address a specific problem. Therefore, the changes indicate a bug fix.

**Buggy**"
junit5,948.json,a7507742dd8a690ebd1da503eadb4cbd2965fa9e,"@@ -1,3 +1,3 @@
-	public static MethodSource from(Method method) {
-		return new MethodSource(method);
+	public static MethodSource from(Method testMethod) {
+		return new MethodSource(testMethod);
 	}",NotBuggy,"Polish MethodSource (test source)
",NotBuggy,"The commit message ""Polish MethodSource (test source)"" suggests improvements or refinements to the `MethodSource` class, specifically within the context of test sources.

The code diff shows a change in the parameter name of the `from` method from `method` to `testMethod`. This is a minor change, likely for clarity and better naming conventions, especially since the context is test sources as indicated by the commit message. This change doesn't seem to address any specific bug or error. It's more about improving code readability and maintainability.

**NotBuggy**
"
junit5,387.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isFinal(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Move precondition checks from ModifierSupport to ReflectionUtils"" suggests a refactoring or code movement rather than a bug fix. It indicates that null checks (preconditions) are being relocated from `ModifierSupport` to `ReflectionUtils`.

2.  **Git Diff:** The diff shows that a `Preconditions.notNull(clazz, ""Class must not be null"")` check is being removed from the `isFinal` method in what appears to be a `ModifierSupport` class.

3.  **Alignment:** The commit message and the diff align. The diff shows the removal of a precondition check, which is consistent with the commit message's intention of moving such checks.

4.  **Bug Fix Indicators:** There is no direct evidence of a bug fix. The change seems to be related to code organization and potentially centralizing null checks in a utility class (`ReflectionUtils`). There's no indication of error handling being added, logic being corrected, or exceptions being handled better. The removal of the null check might even be considered a potential risk if `ReflectionUtils` doesn't handle null `clazz` appropriately, but without seeing the changes in `ReflectionUtils`, we can't conclude that.

**Conclusion:**

Based on the commit message and the diff, the change appears to be a refactoring or code movement. There's no clear indication of a bug being fixed.

**NotBuggy**
"
junit5,1662.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<URI> getSelectedUris() {
-		return selectedUris;
+		return this.selectedUris;
 	}",NotBuggy,"Polishing
",NotBuggy,"The commit message ""Polishing"" suggests minor improvements or refinements rather than a bug fix. The code change involves replacing `selectedUris` with `this.selectedUris`. This change is likely for code clarity or style consistency, explicitly referencing the class member variable. There's no indication of error handling, logical corrections, or exception handling improvements. Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
junit5,1670.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<String> getSelectedPackages() {
-		return selectedPackages;
+		return this.selectedPackages;
 	}",NotBuggy,"Polishing
",NotBuggy,"The commit message ""Polishing"" suggests minor improvements or refinements rather than a bug fix. The code change involves replacing `selectedPackages` with `this.selectedPackages`. This change enforces explicit use of the `this` keyword to access the instance variable. This is generally considered a code style improvement for clarity and doesn't directly address a bug.

**NotBuggy**
"
junit5,1710.json,75de66e17a98aa9627ad388338aacae8d8f17484,"@@ -1,3 +1,3 @@
-			public String generateDisplayNameForClass(Class<?> testClass) {
-				return replaceUnderscore(DEFAULT.generateDisplayNameForClass(testClass));
-			}

+		public String generateDisplayNameForClass(Class<?> testClass) {
+			return replaceUnderscores(super.generateDisplayNameForClass(testClass));
+		}",NotBuggy,"Document display name generation

Move pre-defined generators to DisplayNameGenerator interface.
",NotBuggy,"The commit message ""Document display name generation"" suggests changes related to how display names are generated, and the description mentions moving pre-defined generators to the `DisplayNameGenerator` interface.

The code diff shows a change in the `generateDisplayNameForClass` method. Specifically, it changes from calling `DEFAULT.generateDisplayNameForClass` and then replacing underscores to calling the superclass's `generateDisplayNameForClass` and then replacing underscores. This indicates a refactoring or modification of how display names are generated, possibly to inherit or extend the default behavior. The change from `replaceUnderscore` to `replaceUnderscores` suggests a correction or update in the method name.

The changes do not clearly indicate a bug fix. It's more likely a refactoring or enhancement of the display name generation process.

**NotBuggy**"
junit5,327.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,7 +1,7 @@
 	private DiscoverySelector selectMethod(List<Class<?>> classes, Method method) {
 		if (classes.size() == 1) {
 			return DiscoverySelectors.selectMethod(classes.get(0), method);
 		}
 		int lastIndex = classes.size() - 1;
-		return new NestedMethodSelector(classes.subList(0, lastIndex), classes.get(lastIndex), method);
+		return DiscoverySelectors.selectNestedMethod(classes.subList(0, lastIndex), classes.get(lastIndex), method);
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Introduce API to select and execute individual tests in inherited nested classes"" clearly indicates the introduction of new functionality related to test selection in nested classes. The subsequent bullet points detail the various aspects of this new API, including:

*   Moving nested element selectors to JUnit Platform.
*   Adding factory methods in `DiscoverySelectors` for building selectors.
*   Adding Javadoc and `@API` annotations.
*   Adding methods to `SelectorResolver`.
*   Adding unit tests.
*   Refactoring and adding `toString` methods.

The commit message suggests the addition of new features and improvements rather than fixing existing bugs.

**Git Diff Analysis:**

The provided Git diff shows a change in the `selectMethod` method. Specifically, it changes from directly instantiating a `NestedMethodSelector` to using a factory method `DiscoverySelectors.selectNestedMethod`.

`return new NestedMethodSelector(classes.subList(0, lastIndex), classes.get(lastIndex), method);`

is changed to

`return DiscoverySelectors.selectNestedMethod(classes.subList(0, lastIndex), classes.get(lastIndex), method);`

This change suggests an architectural improvement by centralizing the creation of `NestedMethodSelector` instances through a factory method. This could be for better control, consistency, or future extensibility. It doesn't inherently indicate a bug fix, but rather a change in how the object is created.

**Reasoning:**

The commit message focuses on introducing a new API for selecting and executing tests in nested classes. The diff shows a change in how `NestedMethodSelector` is instantiated, which aligns with the commit message's intention of introducing new functionality and potentially improving the API's design. There are no explicit mentions or indications of fixing a bug in either the commit message or the code diff. The change seems to be part of the new API implementation.

**Conclusion:**

NotBuggy
"
junit5,544.json,18fc3b8e2dc6be2be6b269aaa110df80430a9567,"@@ -1,9 +1,9 @@
-	public static <T> Predicate<T> excludeMatchingClasses(String pattern) {
+	public static <T> Predicate<T> excludeMatchingClasses(String patterns) {
 		// @formatter:off
-		return Optional.ofNullable(pattern)
+		return Optional.ofNullable(patterns)
 				.filter(StringUtils::isNotBlank)
 				.map(String::trim)
-				.map(ClassNamePatternFilterUtils::<T>createPredicateForNonBlankPattern)
+				.map(ClassNamePatternFilterUtils::<T>createPredicateFromPatterns)
 				.orElse(object -> true);
 		// @formatter:on
 	}",NotBuggy,"Polish contribution

See #2181
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** ""Polish contribution"" and a reference to issue #2181. ""Polish"" suggests improvements or refinements rather than a direct bug fix.

2.  **Code Diff:**
    *   The method `excludeMatchingClasses` now accepts `patterns` (plural) instead of `pattern` (singular).
    *   The method `createPredicateForNonBlankPattern` is replaced with `createPredicateFromPatterns`.

3.  **Reasoning:**
    *   The change from singular `pattern` to plural `patterns` suggests that the method now supports multiple patterns, which is an enhancement rather than a bug fix.
    *   The renaming of `createPredicateForNonBlankPattern` to `createPredicateFromPatterns` further supports the idea that the method's functionality has been generalized to handle multiple patterns.
    *   The commit message ""Polish contribution"" aligns with the code changes, indicating improvements and refinements.

**Conclusion:**

The changes indicate an enhancement to support multiple patterns rather than a bug fix.

**NotBuggy**"
junit5,1084.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,3 +1,3 @@
-	List<Class<?>> getEnclosingClasses() {
-		return enclosingClasses;
+	public List<Class<?>> getEnclosingClasses() {
+		return nestedClassSelector.getEnclosingClasses();
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Introduce API to select and execute individual tests in inherited nested classes"" suggests the introduction of new functionality rather than fixing a bug. The message describes adding new selectors for nested elements, factory methods, documentation, and unit tests. The overall theme is feature enhancement.

**Diff Analysis:**

The diff shows a modification in the `getEnclosingClasses()` method. Specifically, the visibility of the method is changed from package-private (no explicit modifier) to `public`. The original implementation `return enclosingClasses;` is replaced with `return nestedClassSelector.getEnclosingClasses();`. This change suggests that the enclosing classes are now being retrieved via a `nestedClassSelector` object, which implies a refactoring or redesign related to how nested classes are handled. The change in visibility to `public` also aligns with the commit message's intention of introducing a new API.

**Reasoning:**

The commit message focuses on introducing a new API for selecting and executing tests in inherited nested classes. The diff shows a change in the `getEnclosingClasses()` method, making it public and delegating the retrieval of enclosing classes to a `nestedClassSelector`. This aligns with the commit message's description of adding new selectors and API methods. The changes don't obviously indicate a bug fix. The code change seems to be part of implementing the new API functionality.

**Conclusion:**

NotBuggy
"
junit5,379.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isNotPrivate(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isNotPrivate(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message indicates a refactoring effort to move precondition checks from `ModifierSupport` to `ReflectionUtils`. The diff shows the removal of a null check (`Preconditions.notNull(clazz, ""Class must not be null"")`) from the `isNotPrivate` method in what appears to be the `ModifierSupport` class. This aligns with the commit message's intention to move precondition checks. While refactoring, moving precondition checks doesn't inherently indicate a bug fix. It's more about code organization and potentially centralizing these checks. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**
"
junit5,707.json,dff526f3f0dcebb101f40446f5d5df207160f7a4,"@@ -1,14 +1,16 @@
-	private CsvParserSettings buildParserSettings() {
+	private static CsvParserSettings createParserSettings(String delimiter, String lineSeparator, char quote,
+			String emptyValue) {
+
 		CsvParserSettings settings = new CsvParserSettings();
 		settings.getFormat().setDelimiter(delimiter);
 		settings.getFormat().setLineSeparator(lineSeparator);
 		settings.getFormat().setQuote(quote);
 		settings.getFormat().setQuoteEscape(quote);
 		settings.setEmptyValue(emptyValue);
 		settings.setAutoConfigurationEnabled(false);
 		// Do not use the built-in support for skipping rows/lines since it will
 		// throw an IllegalArgumentException if the file does not contain at least
 		// the number of specified lines to skip.
 		// settings.setNumberOfRowsToSkip(annotation.numLinesToSkip());
 		return settings;
 	}",NotBuggy,"Polish contribution and Csv[File]Source internals

See: #1958 and #1972
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Polish contribution and Csv[File]Source internals"" suggests general improvements and internal refactoring rather than a specific bug fix. The references to issues #1958 and #1972 would need to be examined to confirm if they relate to bug fixes.

2.  **Code Diff:** The code diff shows a change in how `CsvParserSettings` are created.
    *   The original code had a private method `buildParserSettings()`.
    *   The modified code introduces a new private static method `createParserSettings(String delimiter, String lineSeparator, char quote, String emptyValue)`.
    *   The new method takes parameters for delimiter, line separator, quote character, and empty value, making it more configurable and reusable.
    *   The method still sets `setAutoConfigurationEnabled(false)` and comments out the `setNumberOfRowsToSkip` method.

3.  **Reasoning:**
    *   The change from `buildParserSettings()` to `createParserSettings()` suggests a refactoring to improve the configurability and reusability of the `CsvParserSettings` object. This is a common practice for improving code quality and maintainability.
    *   The parameters added to the new method indicate that the settings can now be customized more easily, which could be related to addressing a specific requirement or use case.
    *   The commit message ""Polish contribution and Csv[File]Source internals"" aligns with the code changes, suggesting that the changes are part of a general improvement effort rather than a specific bug fix.
    *   The comment about skipping rows/lines and the `IllegalArgumentException` suggests that there might have been a bug related to handling files with fewer lines than expected. However, the code is commented out, so it's not directly fixing a bug in this commit.

**Conclusion:**

Based on the commit message and the code diff, the changes appear to be related to refactoring and improving the configurability of the `CsvParserSettings` object. Although there's a hint of a potential bug related to skipping rows, the code related to it is commented out. Therefore, the changes don't directly indicate a bug fix.

**NotBuggy**
"
junit5,389.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Member member) {
-		Preconditions.notNull(member, ""Member must not be null"");
 		return ReflectionUtils.isFinal(member);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",Buggy,"Okay, let's analyze this Git diff and commit message.

**Reasoning:**

1.  **Commit Message:** The commit message states that precondition checks are being moved from `ModifierSupport` to `ReflectionUtils`. This implies a refactoring or code reorganization, where validation logic is being shifted to a different utility class.

2.  **Git Diff:** The diff shows the removal of `Preconditions.notNull(member, ""Member must not be null"");` from the `isFinal` method in what appears to be a `ModifierSupport` class.

3.  **Alignment:** The code change aligns perfectly with the commit message. The precondition check (null check) is being removed from `ModifierSupport`. We don't have the corresponding change in `ReflectionUtils`, but the commit message suggests it's happening there.

4.  **Bug Fix Indication:** This change *could* be related to a bug fix, but it's more likely a refactoring. The removal of the null check might be because it's now handled in `ReflectionUtils`, or perhaps the check was deemed redundant or causing issues in certain scenarios. Without more context, it's difficult to definitively say. However, the act of moving validation logic is often done to address inconsistencies or edge cases that were previously missed.

5. **Error Handling:** The removal of the null check could be considered a change in error handling. It's possible that the original null check was too strict or was causing problems in legitimate use cases. Moving the check (presumably) to `ReflectionUtils` might allow for more flexible or context-aware error handling.

**Conclusion:**

Given the movement of precondition checks, it's plausible that this change addresses a subtle bug or edge case related to null inputs. Therefore, I'll lean towards classifying this as a bug fix.

**Buggy**
"
junit5,2167.json,26a780900f56fc083602fa2fddb644640b772117,"@@ -1,3 +1,3 @@
 	public static void assertTimeout(Duration timeout, Executable executable) {
-		assertTimeout(timeout, executable, () -> null);
+		AssertTimeout.assertTimeout(timeout, executable);
 	}",NotBuggy,"Move implementation of Assertions into separate classes (#559)

",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Move implementation of Assertions into separate classes"" suggests a refactoring or code reorganization effort. It indicates that the implementation of assertion methods is being moved into dedicated classes.

2.  **Git Diff:** The diff shows a change in the `assertTimeout` method. Specifically, the original line `assertTimeout(timeout, executable, () -> null);` is replaced with `AssertTimeout.assertTimeout(timeout, executable);`. This change implies that the actual implementation of the `assertTimeout` logic has been moved to a class named `AssertTimeout`. The original method now simply delegates the call to the new class.

3.  **Reasoning:**

    *   The commit message and the diff are consistent. The diff clearly shows a delegation to a new class, which aligns with the intention of moving the implementation.
    *   There is no indication of a bug fix. The change appears to be purely structural, aimed at improving code organization and maintainability. There are no error handling updates, logical corrections, or exception-handling improvements visible in the diff. The functionality of `assertTimeout` should remain the same after this refactoring.

**Conclusion:**

NotBuggy
"
junit5,526.json,afb80b5612afb4e6c99e0b2730c69da1fc581548,"@@ -1,10 +1,10 @@
-	private String determineSubpackageName(Path rootDir, Path classFile) {
-		Path relativePath = rootDir.relativize(classFile.getParent());
-		String pathSeparator = rootDir.getFileSystem().getSeparator();
+	private String determineSubpackageName(Path baseDir, Path classFile) {
+		Path relativePath = baseDir.relativize(classFile.getParent());
+		String pathSeparator = baseDir.getFileSystem().getSeparator();
 		String subpackageName = relativePath.toString().replace(pathSeparator, PACKAGE_SEPARATOR_STRING);
 		if (subpackageName.endsWith(pathSeparator)) {
 			// Workaround for JDK bug: https://bugs.openjdk.java.net/browse/JDK-8153248
 			subpackageName = subpackageName.substring(0, subpackageName.length() - pathSeparator.length());
 		}
 		return subpackageName;
 	}",NotBuggy,"Polish ClasspathScanner
",NotBuggy,"The commit message ""Polish ClasspathScanner"" suggests improvements or refinements to the `ClasspathScanner` class.

The code diff focuses on the `determineSubpackageName` method. The change involves renaming `rootDir` to `baseDir`, which is a simple renaming. The logic of the method remains the same. The method calculates the subpackage name based on the relative path between a base directory and a class file. It also includes a workaround for a known JDK bug related to path separators.

The changes do not indicate a bug fix. The renaming of the variable improves readability, and the existing workaround suggests that the original code already addressed the JDK bug.

**NotBuggy**"
junit5,393.json,ba5e223c4bb1720e59664723f27d97c373c39f74,"@@ -1,4 +1,4 @@
-	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classTester,
+	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classFilter,
 			Predicate<String> classNameFilter) {
-		return ReflectionUtils.findAllClassesInClasspathRoot(root, classTester, classNameFilter);
+		return ReflectionUtils.findAllClassesInClasspathRoot(root, classFilter, classNameFilter);
 	}",NotBuggy,"Polish JavaDoc for ReflectionSupport
",NotBuggy,"The commit message ""Polish JavaDoc for ReflectionSupport"" suggests improvements to the documentation of the `ReflectionSupport` class.

The provided diff shows a change in the parameter name from `classTester` to `classFilter` in the `findAllClassesInClasspathRoot` method. This change doesn't seem to be related to bug fixing but rather to code style and readability improvements. The functionality remains the same. The change aligns with the commit message, which indicates polishing.

**NotBuggy**"
junit5,1920.json,480a1b2c9b6e54a7016bc8a14ebdb7b7f98909d3,"@@ -1,3 +1,3 @@
 	public static void assertEquals(Short expected, short actual) {
-		assertEquals(expected, (Short) actual);
+		AssertEquals.assertEquals(expected, (Short) actual);
 	}",NotBuggy,"Add more overloaded variants of assertEquals-methods

#1710
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Add more overloaded variants of assertEquals-methods"" suggests the addition of new `assertEquals` methods with different parameter types. This is a feature enhancement rather than a bug fix.

2.  **Git Diff:** The diff shows a change in the `assertEquals(Short expected, short actual)` method. Specifically, it changes `assertEquals(expected, (Short) actual);` to `AssertEquals.assertEquals(expected, (Short) actual);`. This change suggests that the original implementation might have been calling the wrong `assertEquals` method (possibly a recursive call or a call to a less specific overload within the same class). The change explicitly calls `AssertEquals.assertEquals`, which likely resolves the ambiguity and ensures the correct method is invoked.

3.  **Reasoning:** The original code might have had an issue with method resolution, leading to unexpected behavior or even a stack overflow in certain scenarios if it was calling itself recursively. The change to `AssertEquals.assertEquals` clarifies the intended method call. This indicates that the original code was potentially buggy.

**Conclusion:**

**Buggy**
"
Essentials,258.json,e3fa24375342bdf58494c26cc125acb7d621cc87,"@@ -1,4 +1,4 @@
 	public boolean teleport(Entity entity)
 	{
-		throw new UnsupportedOperationException(Util.i18n(""notSupportedYet""));
+		throw new UnsupportedOperationException(_(""notSupportedYet""));
 	}",NotBuggy,"Switch to the new I18n class and format cleanup of all classes
",NotBuggy,"The commit message indicates a switch to a new I18n class and format cleanup. The code diff shows a change from `Util.i18n(""notSupportedYet"")` to `_(""notSupportedYet"")`. This change aligns with the commit message's intention to switch to a new I18n class. The change itself doesn't indicate a bug fix, but rather a code refactoring or update to use a new internationalization approach.

**NotBuggy**"
Essentials,496.json,f5bf895a0ef64905ab4b3ca0fbdbfcf92d09ee21,"@@ -1,6 +1,10 @@
 	public void setLastLocation(Location loc)
 	{
+		if (loc == null || loc.getWorld() == null)
+		{
+			return;
+		}
 		lastLocation = loc;
 		config.setProperty(""lastlocation"", loc);
 		config.save();
 	}",NotBuggy,"Don't set last location if it's invalid
",Buggy,"The commit message ""Don't set last location if it's invalid"" suggests a fix related to handling potentially invalid location data. The code diff introduces a check for `null` values for both the `Location` object itself and its associated `World`. If either is `null`, the method returns early, preventing the `lastLocation` from being updated and the configuration from being saved with invalid data. This aligns perfectly with the commit message, indicating a bug fix where invalid locations were previously being processed.

**Buggy**"
Essentials,431.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean setLeashHolder(Entity arg0)
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3 and mentions disabling vanilla fallback and using a new aliases file. The provided code diff shows a minor change in the `setLeashHolder` method, specifically removing a comment. This change doesn't seem directly related to bug fixing, error handling, or logical corrections. It appears to be a cleanup or minor adjustment. Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
Essentials,947.json,46476b36d99818dfeebc21120b5fda31b031e301,"@@ -1,7 +1,20 @@
 	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		final IText input = new TextInput(sender, ""custom"", true, ess);
 		final IText output = new KeywordReplacer(input, sender, ess);
 		final TextPager pager = new TextPager(output);
-		pager.showPage(commandLabel, args.length > 0 ? args[0] : null, null, sender);
+		String chapter = commandLabel;
+		String page;
+
+		if (commandLabel.equalsIgnoreCase(""customtext"") && args.length > 0 && !NumberUtil.isInt(commandLabel))
+		{
+			chapter = args[0];
+			page = args.length > 1 ? args[1] : null;
+		}
+		else
+		{
+			page = args.length > 0 ? args[0] : null;
+		}
+		
+		pager.showPage(chapter, page, null, sender);
 	}",NotBuggy,"Allow sending customtext chapters as a parameter instead of reading commandLabel
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Allow sending customtext chapters as a parameter instead of reading commandLabel"" suggests a change in how the `customtext` command handles chapter selection.  Previously, it seems the chapter was determined solely from the `commandLabel`. The new change allows specifying the chapter as a parameter.

**Git Diff Analysis:**

The code diff modifies the `run` method. Here's a breakdown:

1.  **Original Behavior:** The original code used `commandLabel` directly as the chapter name in `pager.showPage()`. The page number was taken from `args[0]` if it existed.

2.  **New Behavior:**
    *   It checks if `commandLabel` is ""customtext"" AND if there are arguments AND if the `commandLabel` is not an integer.
    *   If all these conditions are met, it assumes the first argument (`args[0]`) is the chapter name, and the second argument (`args[1]`) is the page number.
    *   Otherwise, it uses the original logic, taking the chapter from `commandLabel` and the page from `args[0]`.

**Reasoning:**

The code change introduces a conditional logic block to handle the case where the user wants to specify the chapter as a parameter to the `customtext` command. This addresses a limitation in the original implementation where the chapter was implicitly derived from the command label. The `NumberUtil.isInt(commandLabel)` check is interesting. This likely prevents the code from interpreting the command label as a chapter if it happens to be a number.

The change allows more flexibility in how the `customtext` command is used. It is likely that the original implementation was not flexible enough, and this change is designed to address this limitation. This is a feature enhancement rather than a bug fix.

**Conclusion:**

NotBuggy
"
Essentials,428.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public double getHealthScale()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3 and mentions disabling vanilla fallback and using a new aliases file. The provided code diff shows a minor change in the `getHealthScale()` method, specifically the removal of a comment.

Reasoning:

1.  **Commit Message Relevance:** The commit message primarily focuses on updating Bukkit and related configurations. The code diff doesn't directly reflect these changes. The removal of the comment seems unrelated to the core purpose of the commit.
2.  **Bug Fix Indicators:** The code change doesn't suggest any bug fixes. It's a minor modification that doesn't address any specific error or unexpected behavior. The removal of the comment doesn't indicate any error-handling updates, logical corrections, or exception-handling improvements.
3.  **Vanilla Fallback and Aliases File:** The commit message mentions disabling vanilla fallback and using a new aliases file. The code diff does not show any changes related to these functionalities.

Conclusion:
**NotBuggy**
"
Essentials,1851.json,7f65879cb6dc28084ecfff57b047941cc106ecf2,"@@ -1,4 +1,4 @@
 	public String getCurrencySymbol()
 	{
-		return config.getString(""currency-symbol"", ""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
+		return config.getString(""currency-symbol"", ""$"").concat(""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
 	}",NotBuggy,"Catch null eco symbol.
",Buggy,"The commit message ""Catch null eco symbol"" suggests that the commit aims to handle a null or potentially problematic ""eco symbol"" (likely related to currency).

The code diff modifies the `getCurrencySymbol()` method. Specifically, it changes:

```java
return config.getString(""currency-symbol"", ""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
```

to

```java
return config.getString(""currency-symbol"", ""$"").concat(""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
```

The change adds `.concat(""$"")` to the string obtained from the configuration. This ensures that even if `config.getString(""currency-symbol"", ""$"")` returns null or an empty string, the subsequent `.substring(0, 1)` call will not throw an exception.  By concatenating ""$"" to the retrieved string, the code guarantees that there's at least one character to extract via `substring(0, 1)`. This prevents a potential `StringIndexOutOfBoundsException` if the initial string was empty.

The commit message and the code change are related. The code change addresses a potential null or empty value for the currency symbol, which aligns with the intent to ""Catch null eco symbol"". The original code was likely throwing an exception when the currency symbol was empty. The modification prevents this exception, indicating a bug fix.

**Buggy**
"
Essentials,92.json,109c26fa8f0f015191409834b93e247d00706ae5,"@@ -1,13 +1,19 @@
 	public void onBlockBreak(final BlockBreakEvent event)
 	{
 		if (ess.getSettings().areSignsDisabled())
 		{
 			event.getHandlers().unregister(this);
 			return;
 		}
-
-		if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+		try
+		{
+			if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+			{
+				event.setCancelled(true);
+			}
+		}
+		catch (MaxMoneyException ex)
 		{
 			event.setCancelled(true);
 		}
 	}",NotBuggy,"[Breaking] Add exception when players have exceeded their account limit.
This might effect some plugins which hook Essentials for economy without using the API
",Buggy,"The commit message indicates that an exception is added when players exceed their account limit. The code diff shows a `try-catch` block being added around the `protectSignsAndBlocks` method call. The `catch` block specifically catches a `MaxMoneyException`. If this exception is caught, the block break event is cancelled. This aligns with the commit message, as the exception handling is directly related to the player's account limit. The addition of the `try-catch` block suggests that the original code did not handle the case where a player might exceed their account limit, which could be considered a bug.

**Buggy**"
Essentials,625.json,406295a5488f8b105325b7c8c918f257f484d911,"@@ -1,9 +1,9 @@
 	public void setProperty(final String path, final Location loc)
 	{
-		setProperty((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
-		setProperty((path == null ? """" : path + ""."") + ""x"", loc.getX());
-		setProperty((path == null ? """" : path + ""."") + ""y"", loc.getY());
-		setProperty((path == null ? """" : path + ""."") + ""z"", loc.getZ());
-		setProperty((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
-		setProperty((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
+		set((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
+		set((path == null ? """" : path + ""."") + ""x"", loc.getX());
+		set((path == null ? """" : path + ""."") + ""y"", loc.getY());
+		set((path == null ? """" : path + ""."") + ""z"", loc.getZ());
+		set((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
+		set((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
 	}",NotBuggy,"Updated Essentials to work with R5
",NotBuggy,"The commit message ""Updated Essentials to work with R5"" suggests that the changes are related to compatibility with a new version (R5) of Essentials or a related library. The code diff shows that `setProperty` is changed to `set`. This could be due to a change in the API of Essentials in version R5. This change is likely necessary to maintain compatibility with the new version. There is no indication of bug fixing.
**NotBuggy**"
Essentials,1533.json,51bd0d88f393dbd675d2227e84d24fe9d6313712,"@@ -1,9 +1,9 @@
 	public static void spawnmob(final IEssentials ess, final Server server, final User user, final List<String> parts, final List<String> data, int mobCount) throws Exception
 	{
 		final Block block = LocationUtil.getTarget(user.getBase()).getBlock();
 		if (block == null)
 		{
-			throw new Exception(_(""unableToSpawnMob""));
+			throw new Exception(tl(""unableToSpawnMob""));
 		}
 		spawnmob(ess, server, user.getSource(), user, block.getLocation(), parts, data, mobCount);
 	}",NotBuggy,"Future Java versions may not support _ as identifier.
",Buggy,"The commit message ""Future Java versions may not support _ as identifier"" suggests a change related to identifier naming conventions to ensure compatibility with future Java versions.

The code diff replaces `_(""unableToSpawnMob"")` with `tl(""unableToSpawnMob"")`. This change likely involves replacing a deprecated or problematic identifier (`_`) with a new one (`tl`) to avoid potential issues in future Java versions. The change itself doesn't inherently indicate a bug fix in the sense of correcting a functional error. Instead, it appears to be a proactive measure to address a potential compatibility issue.

**Buggy**
"
Essentials,429.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean isLeashed()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message indicates an update to Bukkit version 1.7.3-R0.3 and mentions disabling vanilla fallback and using a new aliases file. The provided code diff shows a minor change in the `isLeashed()` method, specifically removing a comment. This change doesn't seem directly related to bug fixing but rather a cleanup or minor adjustment during the Bukkit update. The removal of the comment doesn't inherently indicate a bug fix.

**NotBuggy**"
Essentials,2380.json,0f83fb9dceb0edac24600b4838166c09fa002c8a,"@@ -1,27 +1,27 @@
-	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
+	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws NotEnoughArgumentsException
 	{
 		if (args.length < 1)
 		{
 			throw new NotEnoughArgumentsException();
 		}
 
 		final List<Player> matches = server.matchPlayer(args[0]);
 
 		if (matches.isEmpty())
 		{
 			sender.sendMessage(""ￂﾧcThere are no players matching that name."");
 		}
 
 		for (Player p : matches)
 		{
 			try
 			{
 				final boolean toggle = EssentialsXMPP.getInstance().toggleSpy(p);
 				sender.sendMessage(""XMPP Spy "" + (toggle ? ""enabled"" : ""disabled"") + "" for "" + p.getDisplayName());
 			}
 			catch (Exception ex)
 			{
 				sender.sendMessage(""Error: "" + ex.getMessage());
 			}
 		}
 	}",NotBuggy,"More cleanup

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1570 e251c2fe-e539-e718-e476-b85c1f46cddb
",Buggy,"The commit message ""More cleanup"" is vague and doesn't provide much information about the changes.

The code diff shows the following changes:

1.  The `run` method signature is changed to declare that it throws `NotEnoughArgumentsException`.
2.  The `args` parameters are marked as `final`.

The addition of `throws NotEnoughArgumentsException` suggests that the code now explicitly declares an exception that can be thrown. This could be related to improving error handling or clarifying the method's behavior. The `final` keyword on the parameters indicates that the parameters are not modified within the method, which is a good practice for code clarity and preventing accidental modification.

The changes do not clearly indicate a bug fix, but the exception handling improvement could be related to fixing a potential issue where the method didn't handle insufficient arguments correctly.

**Buggy**"
Essentials,2816.json,55b083ddbf75a7e21d9bb55fafcd9fd88afa64c8,"@@ -1,3 +1,3 @@
-	public void onPlayerChat(final PlayerChatEvent event)
+	public void onPlayerChat(final AsyncPlayerChatEvent event)
 	{
 	}",NotBuggy,"CB #2289 Bukkit #1512

Change PlayerChatEvent to AsyncPlayerChatEvent
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""CB #2289 Bukkit #1512 Change PlayerChatEvent to AsyncPlayerChatEvent"" suggests a change in the event type used in the `onPlayerChat` method. The ""CB"" and ""Bukkit"" prefixes likely refer to issue trackers, indicating that this change is related to a reported issue or feature request in CraftBukkit or Bukkit.

2.  **Git Diff:** The diff shows a simple replacement of `PlayerChatEvent` with `AsyncPlayerChatEvent` in the method signature of `onPlayerChat`. This means the code is being updated to use the asynchronous version of the chat event.

3.  **Bug Fix or Feature/Improvement?**  The change from a synchronous event (`PlayerChatEvent`) to an asynchronous event (`AsyncPlayerChatEvent`) is a significant change in how chat events are handled.  Using `AsyncPlayerChatEvent` is generally preferred because it prevents the main server thread from being blocked during chat processing, which can improve server performance and responsiveness. Switching to `AsyncPlayerChatEvent` is often done to address performance issues or potential deadlocks that could occur when processing chat synchronously. While it might not be a direct ""bug fix"" in the sense of correcting incorrect logic, it addresses a potential architectural issue that could lead to problems. It's also possible that the original use of `PlayerChatEvent` was causing issues with certain plugins or server configurations.

**Conclusion:**

Given the change from a synchronous to an asynchronous event, and the likely performance implications, this change is most likely addressing a potential issue or improving the existing system. Therefore, it can be considered a bug fix.

**Buggy**
"
Essentials,1707.json,444ec11dc173b51c13859526718cd1e0935d3631,"@@ -1,4 +1,4 @@
 	public boolean canBuild(final Player base, final String group)
 	{
-		return hasPermission(base, ""essentials.build"");
+		return false;
 	}",NotBuggy,"Split Ess protect into EssProtect and EssAntiBuild
New permissions:
essentials.build - same as build: true
essentials.build.interact.<id> - whitelist single interact block
essentials.build.place.<id> - whitelist single place block
essentials.build.break.<id> - whitelist single break block
essentials.build.use.<id> - whitelist single use item
",Buggy,"The commit message indicates a refactoring of the ""Ess protect"" feature into ""EssProtect"" and ""EssAntiBuild"", along with the introduction of new, more granular permissions for controlling building actions. The provided code diff shows a change in the `canBuild` method where the return value is hardcoded to `false`. This change directly relates to the commit message's intention of controlling build permissions. The original code likely had a bug or unintended behavior where `canBuild` always returned `true` if the player had the `essentials.build` permission. The change to `return false` suggests a fix to properly implement the new permission system.

**Buggy**
"
Essentials,316.json,15c088e6dee3b470b7de80c95f7ec639613593e8,"@@ -1,4 +1,3 @@
 	public void recalculatePermissions()
 	{
-		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Bukkit Permission system
New config setting: use-bukkit-permissions
",Buggy,"The commit message ""Bukkit Permission system New config setting: use-bukkit-permissions"" suggests the introduction of a new configuration setting related to Bukkit permissions.

The provided code diff removes an `UnsupportedOperationException`. This removal suggests that the `recalculatePermissions` method, which was previously unimplemented, is now being implemented or at least prepared for implementation. This aligns with the commit message's intention to integrate with the Bukkit permission system. The removal of the exception indicates a step towards making the permission system functional, which could be considered a bug fix if the intention was for this method to be implemented.

**Buggy**
"
Essentials,814.json,ff46b39f2b42abbe4ce8dc39e564f3dc71a2a104,"@@ -1,4 +1,4 @@
-	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws Exception
+	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		sender.sendMessage(_(""helpConsole""));
 	}",NotBuggy,"Extract CommandSender to CommandSource, this should prevent Ess user object leaks.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Extract CommandSender to CommandSource, this should prevent Ess user object leaks"" strongly suggests a bug fix. The phrase ""prevent Ess user object leaks"" indicates a potential security or memory management issue related to how the `CommandSender` object (likely from the Essentials plugin, judging by ""Ess"") was being handled. Object leaks are a common type of bug. The change aims to address this by using `CommandSource` instead.

2.  **Git Diff:** The diff shows a simple change in the `run` method signature. The parameter `CommandSender sender` is replaced with `CommandSource sender`. This change aligns perfectly with the commit message's intention to use `CommandSource` instead of `CommandSender`. The fact that they are changing the type of the sender object to prevent leaks indicates that the old object was somehow causing issues.

3.  **Bug Fix Indication:** The commit message explicitly mentions preventing a user object leak, which is a bug. The code change directly supports this goal by replacing `CommandSender` with `CommandSource`. This suggests that the previous usage of `CommandSender` was the root cause of the leak.

**Conclusion:**

**Buggy**
"
commons-io,729.json,55bae88d398feed4a3008ec5e97eb8b85c983b1e,"@@ -1,3 +1,3 @@
     public static String toString(URI uri, Charset encoding) throws IOException {
-        return toString(uri.toURL(), toCharset(encoding));
+        return toString(uri.toURL(), Charsets.toCharset(encoding));
     }",NotBuggy,"[IO-318] Add Charset sister APIs to method that take a String charset name. FileUtils.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1307412 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Add Charset sister APIs to method that take a String charset name. FileUtils."" indicates the addition of `Charset` based APIs as alternatives to existing APIs that accept `String` for charset names.

The diff shows a change in the `toString` method where `toCharset(encoding)` is replaced with `Charsets.toCharset(encoding)`. This change suggests the use of a utility method (`Charsets.toCharset`) to handle `Charset` objects, which aligns with the commit message's intention of adding `Charset` sister APIs. The change itself doesn't inherently indicate a bug fix. It seems to be an enhancement or refactoring to provide a more convenient way to specify character encodings using `Charset` objects.

**NotBuggy**"
commons-io,677.json,6aa00766b9103d75c763160280429af8e1f0549e,"@@ -1,6 +1,6 @@
-    public static void readFully(InputStream input, byte[] buffer, int offset, int length) throws IOException {
-        int actual = read(input, buffer, offset, length);
+    public static void readFully(final InputStream input, final byte[] buffer, final int offset, final int length) throws IOException {
+        final int actual = read(input, buffer, offset, length);
         if (actual != length) {
             throw new EOFException(""Length to read: "" + length + "" actual: "" + actual);
         }
     }",NotBuggy,"Use final where possible.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1415850 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Use final where possible"" suggests a refactoring to improve code clarity and prevent unintended modifications of variables. The diff shows the addition of the `final` keyword to method parameters and a local variable within the `readFully` method. This change aligns perfectly with the commit message and doesn't indicate a bug fix. It's a straightforward application of the `final` keyword for better code practices.

**NotBuggy**"
commons-io,545.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(ClassNameMatcher m) {
+    public ValidatingObjectInputStream accept(final ClassNameMatcher m) {
         acceptMatchers.add(m);
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message ""Use final for parameters"" indicates an intention to add the `final` keyword to method parameters. The provided diff shows that the `final` keyword was indeed added to the `m` parameter of the `accept` method. This change doesn't inherently indicate a bug fix. It's more of a code style or defensive programming practice to prevent accidental modification of the parameter within the method. There's no error handling, logical correction, or exception handling improvement in the diff.

**NotBuggy**
"
commons-io,1030.json,7c111fecd837f22f1f8c1faa38dbef0f4a0e91ee,"@@ -1,5 +1,5 @@
-    public static <T extends FileVisitor<? super Path>> T visitFileTree(final Path directory, final T visitor)
+    public static <T extends FileVisitor<? super Path>> T visitFileTree(final T visitor, final Path directory)
             throws IOException {
         Files.walkFileTree(directory, visitor);
         return visitor;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor for counting longs and big integers. Add cleaning visitor.
",NotBuggy,"The commit message indicates the addition of `PathUtils` for operations on NIO `Path`, refactoring for counting longs and big integers, and adding a cleaning visitor. The provided diff shows a change in the order of arguments in the `visitFileTree` method, specifically swapping the `directory` and `visitor` arguments.

The change in argument order doesn't inherently suggest a bug fix. It could be a refactoring or an attempt to improve the API's usability. Without more context or information about the previous state of the code, it's difficult to determine if this change addresses a bug. The commit message mentions refactoring, which aligns with this change.

Therefore, based on the available information, it's not clear if this change is a bug fix.

**NotBuggy**"
commons-io,763.json,b7ec4f7e855ecb77c8bf42fdd97f123d4588edd2,"@@ -1,3 +1,3 @@
-    public <T extends Throwable> List<T> getCauseList(Class<T> clazz) {
+    public <T extends Throwable> List<T> getCauseList(final Class<T> clazz) {
         return (List<T>) causeList;
     }",NotBuggy,Use final.,NotBuggy,"The commit message ""Use final"" suggests that the change involves adding the `final` keyword to a variable, parameter, or field.

The provided diff shows that the `final` keyword has been added to the `clazz` parameter of the `getCauseList` method. This aligns with the commit message.

The addition of `final` to a parameter generally does not indicate a bug fix. It's a good practice to prevent accidental modification of the parameter within the method, but it doesn't directly address a bug.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
commons-io,1152.json,8db20da2ea53b8598e92fc0672482a74acb06845,"@@ -1,11 +1,7 @@
-    private static void validateMoveParameters(final File src, final File dest) throws FileNotFoundException {
-        if (src == null) {
-            throw new NullPointerException(""Source must not be null"");
-        }
-        if (dest == null) {
-            throw new NullPointerException(""Destination must not be null"");
-        }
-        if (!src.exists()) {
-            throw new FileNotFoundException(""Source '"" + src + ""' does not exist"");
+    private static void validateMoveParameters(final File source, final File destination) throws FileNotFoundException {
+        Objects.requireNonNull(source, ""source"");
+        Objects.requireNonNull(destination, ""destination"");
+        if (!source.exists()) {
+            throw new FileNotFoundException(""Source '"" + source + ""' does not exist"");
         }
     }",NotBuggy,"Use Objects.requireNonNull() instead of custom check.
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Use Objects.requireNonNull() instead of custom check"" clearly indicates a refactoring of the code. It suggests that the original code had a custom null check, which is now being replaced with the standard `Objects.requireNonNull()` method.

2.  **Code Diff:**
    *   The original code had explicit `if (src == null)` and `if (dest == null)` checks, throwing `NullPointerException` if either was null.
    *   The modified code uses `Objects.requireNonNull(source, ""source"")` and `Objects.requireNonNull(destination, ""destination"")`. This achieves the same null check but in a more concise and standard way.
    *   The variable names `src` and `dest` have been changed to `source` and `destination`. This is a minor change and doesn't necessarily indicate a bug fix.

3.  **Bug Fix Indication:** While the change improves code readability and potentially maintainability by using a standard library function, it doesn't inherently fix a bug. The original code already handled null `src` and `dest` values by throwing a `NullPointerException`. The new code does the same. The change is primarily about code style and using a more idiomatic approach.

**Conclusion:**

The change is a refactoring to use `Objects.requireNonNull()` for null checks instead of a custom implementation. It doesn't appear to address any specific bug.

**NotBuggy**"
commons-io,543.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(Pattern pattern) {
+    public ValidatingObjectInputStream accept(final Pattern pattern) {
         acceptMatchers.add(new RegexpClassNameMatcher(pattern));
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message ""Use final for parameters"" indicates a change to add the `final` keyword to method parameters. The provided diff shows exactly that: the `pattern` parameter in the `accept` method is now declared as `final Pattern pattern`. This change doesn't inherently indicate a bug fix. It's more of a code style or defensive programming practice to prevent accidental modification of the parameter within the method. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**
"
commons-io,336.json,46ab2532070e129ac54508bfb7a5be33a6b797bc,"@@ -1,12 +1,12 @@
     public int read() throws IOException {
         int c = EOF;
         while (reader != null) {
             c = reader.read();
             if (c == EOF) {
-                reader = readers.hasNext() ? readers.next() : null;
+                reader = nextReader();
             } else {
                 break;
             }
         }
         return c;
     }",NotBuggy,"[IO-458] Add a SequenceReader similar to java.io.SequenceInputStream.

Refactor commmon code in a private method.",NotBuggy,"The commit message indicates the addition of a `SequenceReader` and refactoring of common code into a private method. The provided diff shows a change in the `read()` method where `reader = readers.hasNext() ? readers.next() : null;` is replaced with `reader = nextReader();`. This suggests that the `nextReader()` method is the newly introduced private method containing the logic for retrieving the next reader in the sequence. The change itself doesn't inherently indicate a bug fix, but rather a refactoring to improve code organization and potentially re-use the logic for obtaining the next reader. Therefore, the change aligns with the commit message's description of refactoring.

**NotBuggy**
"
commons-io,475.json,1170279dc85c2bbdea594b053c56b379a72a2104,"@@ -1,53 +1,53 @@
         private String readLine() throws IOException {
 
             String line = null;
             int newLineMatchByteCount;
 
-            boolean isLastFilePart = (no == 1);
+            boolean isLastFilePart = no == 1;
 
             int i = currentLastBytePos;
             while (i > -1) {
 
                 if (!isLastFilePart && i < avoidNewlineSplitBufferSize) {
                     // avoidNewlineSplitBuffer: for all except the last file part we
                     // take a few bytes to the next file part to avoid splitting of newlines
                     createLeftOver();
                     break; // skip last few bytes and leave it to the next file part
                 }
 
                 // --- check for newline ---
                 if ((newLineMatchByteCount = getNewLineMatchByteCount(data, i)) > 0 /* found newline */) {
                     final int lineStart = i + 1;
                     int lineLengthBytes = currentLastBytePos - lineStart + 1;
 
                     if (lineLengthBytes < 0) {
                         throw new IllegalStateException(""Unexpected negative line length=""+lineLengthBytes);
                     }
                     byte[] lineData = new byte[lineLengthBytes];
                     System.arraycopy(data, lineStart, lineData, 0, lineLengthBytes);
 
                     line = new String(lineData, encoding);
 
                     currentLastBytePos = i - newLineMatchByteCount;
                     break; // found line
                 }
 
                 // --- move cursor ---
                 i -= byteDecrement;
 
                 // --- end of file part handling ---
                 if (i < 0) {
                     createLeftOver();
                     break; // end of file part
                 }
             }
 
             // --- last file part handling ---
             if (isLastFilePart && leftOver != null) {
                 // there will be no line break anymore, this is the first line of the file
                 line = new String(leftOver, encoding);
                 leftOver = null;
             }
 
             return line;
         }",NotBuggy,"Remove noisy and unnecessary parens. Especially painful in return statements: ""return (foo + bar);"" vs. ""return foo + bar;""

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1302748 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change to remove unnecessary parentheses, particularly in return statements. The diff shows the removal of parentheses in the line `boolean isLastFilePart = (no == 1);` which is now `boolean isLastFilePart = no == 1;`. This change aligns perfectly with the commit message's intention to remove unnecessary parentheses. There is no indication of bug fixing.

**NotBuggy**"
commons-io,1046.json,154cb7648ffde863b2548eba10b330c51a103c6a,"@@ -1,4 +1,4 @@
-    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attrs) throws IOException {
-        super.preVisitDirectory(dir, attrs);
+    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attributes) throws IOException {
+        super.preVisitDirectory(dir, attributes);
         return accept(dir) ? FileVisitResult.CONTINUE : FileVisitResult.SKIP_SUBTREE;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor to add PathUtils.copyDirectory(Path, Path, CopyOption...)
",NotBuggy,"The commit message indicates the addition of `PathUtils` for NIO `Path` operations, specifically mentioning `copyDirectory`. The provided diff shows a minor change in the `preVisitDirectory` method, renaming `attrs` to `attributes`. This change doesn't seem directly related to adding a `PathUtils` class or a `copyDirectory` method. It's a simple variable name change within an existing method, likely for clarity or consistency. There's no indication of bug fixing in this specific diff.

**NotBuggy**"
commons-io,627.json,3e947380d02dbd6d8f4e7590e3282498a16d6d32,"@@ -1,3 +1,3 @@
-    public static BufferedWriter asBufferedWriter(final Writer writer) {
+    public static BufferedWriter buffer(final Writer writer) {
         return writer instanceof BufferedWriter ? (BufferedWriter) writer : new BufferedWriter(writer);
     }",NotBuggy,"IO-233 IO-330 Add Methods for Buffering Streams/Writers To IOUtils

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1483859 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""IO-233 IO-330 Add Methods for Buffering Streams/Writers To IOUtils"" indicates the addition of methods for buffering streams and writers in the IOUtils class. The diff shows a renaming of the method `asBufferedWriter` to `buffer`. This change doesn't inherently indicate a bug fix. It seems to be more of a refactoring or renaming to align with the intended functionality of buffering streams/writers. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**"
commons-io,353.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,17 +1,17 @@
     public int read(final byte[] pBuffer, final int pOffset, final int pLength) throws IOException {
         int result = 0;
         IOException ioe = null;
         try {
             result = super.read(pBuffer, pOffset, pLength);
-        } catch (IOException pException) {
+        } catch (final IOException pException) {
             ioe = pException;
         }
         if (ioe != null) {
             noteError(ioe);
         } else if (result == -1) {
             noteFinished();
         } else if (result > 0) {
             noteDataBytes(pBuffer, pOffset, result);
         }
         return result;
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message states that the change is to consistently use the `final` modifier with local variables.

The diff shows that the local variable `pException` in the catch block is changed to be `final`. This aligns with the commit message. There is no indication of a bug fix in this change.

**NotBuggy**"
commons-io,754.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,12 +1,12 @@
     public static void writeChunked(final char[] data, final Writer output) throws IOException {
         if (data != null) {
             int bytes = data.length;
             int offset = 0;
             while (bytes > 0) {
-                int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
+                final int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
                 output.write(data, offset, chunk);
                 bytes -= chunk;
                 offset += chunk;
             }
         }
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message states that the changes consistently use the `final` modifier with local variables. The diff shows that the local variable `chunk` within the `writeChunked` method has been updated to include the `final` modifier. This change aligns with the commit message and doesn't indicate a bug fix.

**NotBuggy**"
commons-io,1022.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,4 +1,4 @@
     public static boolean directoryContentEquals(final Path path1, final Path path2, final int maxDepth,
-            LinkOption[] linkOptions, FileVisitOption... fileVisitOptions) throws IOException {
+            LinkOption[] linkOptions, FileVisitOption[] fileVisitOptions) throws IOException {
         return new RelativeSortedPaths(path1, path2, maxDepth, linkOptions, fileVisitOptions).equals;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",NotBuggy,"The commit message ""Implement directory content equality. Integrate feedback from Michael Osipov."" suggests the implementation of a new feature related to comparing the content of directories. The diff shows a minor change in the `directoryContentEquals` method signature, specifically in the type of the `fileVisitOptions` parameter, changing it from a varargs (`FileVisitOption...`) to an array (`FileVisitOption[]`).

This change is likely a direct result of the feedback mentioned in the commit message. While it's a modification to the method signature, it doesn't inherently indicate a bug fix. It could be a change to improve the API or address a specific use case. There's no error handling, logical correction, or exception handling improvement visible in the diff. The change is related to the method's input parameters, which is more about API design than bug fixing.

**NotBuggy**
"
commons-io,1020.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,34 +1,34 @@
     public static boolean directoryAndFileContentEquals(final Path path1, final Path path2,
-            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption... fileVisitOption)
+            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption[] fileVisitOption)
             throws IOException {
         // First walk both file trees and gather normalized paths.
         if (path1 == null && path2 == null) {
             return true;
         }
         if (path1 == null ^ path2 == null) {
             return false;
         }
         if (!Files.exists(path1) && !Files.exists(path2)) {
             return true;
         }
         final RelativeSortedPaths relativeSortedPaths = new RelativeSortedPaths(path1, path2, Integer.MAX_VALUE,
                 linkOptions, fileVisitOption);
         // If the normalized path names and counts are not the same, no need to compare contents.
         if (!relativeSortedPaths.equals) {
             return false;
         }
         // Both visitors contain the same normalized paths, we can compare file contents.
         final List<Path> fileList1 = relativeSortedPaths.relativeFileList1;
         final List<Path> fileList2 = relativeSortedPaths.relativeFileList2;
         for (Path path : fileList1) {
             final int binarySearch = Collections.binarySearch(fileList2, path);
             if (binarySearch > -1) {
                 if (!fileContentEquals(path1.resolve(path), path2.resolve(path), linkOptions, openOptions)) {
                     return false;
                 }
             } else {
                 throw new IllegalStateException(String.format(""Unexpected mismatch.""));
             }
         }
         return true;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",NotBuggy,"The commit message indicates the implementation of directory content equality and integration of feedback. The code changes involve modifying the `directoryAndFileContentEquals` method signature to accept `FileVisitOption[]` instead of `FileVisitOption...`. This change suggests a modification to the method's input parameters, potentially to accommodate different file visiting options. The rest of the code maintains the logic for comparing directory and file contents. There is no explicit indication of bug fixing.
**NotBuggy**"
wicket,134.json,2e1812d71cd5be231e57f82f7132f62295ef07c0,"@@ -1,4 +1,14 @@
 		public void detach(final IRequestCycle requestCycle)
 		{
 			delegate.detach(requestCycle);
+
+			if (logData == null)
+			{
+				ILogData delegateData;
+				if (delegate instanceof ILoggableRequestHandler)
+					delegateData = ((ILoggableRequestHandler)delegate).getLogData();
+				else
+					delegateData = new NoLogData();
+				logData = new DelegateLogData(delegateData);
+			}
 		}",NotBuggy,"WICKET-4074 RequestLogger needs a clear separation of concerns, courtesy of Emond Papegaaij


git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1183064 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message ""WICKET-4074 RequestLogger needs a clear separation of concerns, courtesy of Emond Papegaaij"" suggests a refactoring or improvement of the RequestLogger component in Wicket. The provided diff shows a modification to the `detach` method. Specifically, it adds a block of code that initializes `logData` if it's null. Inside this block, it checks if the `delegate` is an instance of `ILoggableRequestHandler`. If so, it retrieves the log data from the delegate; otherwise, it creates a `NoLogData` instance. Finally, it wraps the retrieved or created log data in a `DelegateLogData`.

The code change addresses a scenario where `logData` might be null during the detach phase. This could lead to issues if the RequestLogger relies on `logData` being initialized. The change ensures that `logData` is always initialized with either the delegate's log data (if available) or a default `NoLogData` instance. This initialization logic prevents potential NullPointerExceptions or unexpected behavior later in the request lifecycle. The commit message indicates a separation of concerns, and the code change ensures that the RequestLogger has its own log data even if the delegate doesn't explicitly provide it. This makes the RequestLogger more robust and independent.

**Buggy**
"
wicket,5717.json,83f3fa8231c877705edf3e7250a11fe18252ac1e,"@@ -1,5 +1,5 @@
-	public NumberTextField setMaximum(final Double maximum)
+	public NumberTextField<N> setMaximum(final N maximum)
 	{
 		this.maximum = maximum;
 		return this;
 	}",NotBuggy,"WICKET-3241 Add support for the new HTML 5 input types

Relax the type of (Number|Range)TextField.
Now it can be any subtype of java.lang.Number

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1053609 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""WICKET-3241 Add support for the new HTML 5 input types"" indicates a feature addition related to HTML 5 input types. The specific change involves relaxing the type constraint of `NumberTextField` to accept any subtype of `java.lang.Number`. This suggests an enhancement to support a wider range of number types, rather than a bug fix. The code change itself, modifying `setMaximum` to accept a generic type `N` extending `Number`, confirms this. There is no indication of error handling, logical corrections, or exception handling improvements.

**NotBuggy**"
wicket,8450.json,2ec1b5524431393274d459e2da2649dcf4446233,"@@ -1,4 +1,4 @@
 	public String getCharacterEncoding()
 	{
-		return null;
+		return characterEncoding;
 	}",NotBuggy,"WICKET-6063 Add support for WebSocketRequest#getUrl() and other properties which are available in the handshake request
",Buggy,"The commit message ""WICKET-6063 Add support for WebSocketRequest#getUrl() and other properties which are available in the handshake request"" indicates that the commit is adding support for certain properties in the `WebSocketRequest` class.

The diff shows a change in the `getCharacterEncoding()` method. The original implementation returns `null`, while the modified version returns `characterEncoding`. This suggests that the original implementation was incomplete or incorrect, and the change fixes it by returning the actual character encoding. This aligns with the commit message's intent to add support for properties. The original code returning `null` is likely a bug.

Therefore, the change indicates a bug fix.

**Buggy**
"
wicket,11326.json,d2be7fd368cdca02dd7481490de587bcc1a6e4cd,"@@ -1,13 +1,15 @@
-	protected OrderByLink newOrderByLink(String id, String property, ISortStateLocator stateLocator)
+	protected OrderByLink newOrderByLink(final String id, final String property,
+		final ISortStateLocator stateLocator)
 	{
-		return new OrderByLink(id, property, stateLocator, OrderByLink.VoidCssProvider.getInstance())
+		return new OrderByLink(id, property, stateLocator,
+			OrderByLink.VoidCssProvider.getInstance())
 		{
 			private static final long serialVersionUID = 1L;
 
 			@Override
 			protected void onSortChanged()
 			{
 				OrderByBorder.this.onSortChanged();
 			}
 		};
 	}",NotBuggy,"let eclipse do a final cleanup. No functional change

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1062370 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states ""let eclipse do a final cleanup. No functional change"". The diff shows changes to the `newOrderByLink` method signature, adding `final` keywords to the parameters. This is a common cleanup task performed by IDEs like Eclipse and doesn't change the functionality of the code. The addition of `final` to parameters is a non-functional change.

**NotBuggy**"
wicket,2324.json,d1710298c7e371f260299f732c58d0bf4d647161,"@@ -1,9 +1,9 @@
 	public boolean getStatelessHint(Component component)
 	{
-		if (this instanceof IBehaviorListener)
+		if (this instanceof IRequestListener)
 		{
 			// this behavior implements a callback interface, so it cannot be stateless
 			return false;
 		}
 		return true;
 	}",NotBuggy,"WICKET-6137 remove RequestListenerInterface
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** ""WICKET-6137 remove RequestListenerInterface"" suggests the removal or replacement of a specific interface named `RequestListenerInterface`.

2.  **Code Diff:** The code diff shows a change within the `getStatelessHint` method. The original code checks if `this` (presumably a behavior) is an instance of `IBehaviorListener`. The modified code checks if `this` is an instance of `IRequestListener`.

3.  **Analysis:**
    *   The commit message indicates the removal of `RequestListenerInterface`.
    *   The code diff replaces `IBehaviorListener` with `IRequestListener`.
    *   It's highly likely that `IRequestListener` is either a replacement for or related to the removed `RequestListenerInterface`. The change suggests a refactoring or update of how statelessness is determined for components based on request handling interfaces.
    *   The original code might have been incorrectly checking for `IBehaviorListener` when it should have been checking for `IRequestListener` to determine if the component is stateless. This would be a bug.

**Conclusion:**

**Buggy**
"
wicket,1688.json,9220f60d68e223ff1bf5d0dd5cd0f3112a1a2396,"@@ -1,4 +1,21 @@
 	public IResourceCachingStrategy getCachingStrategy()
 	{
+		if (resourceCachingStrategy == null)
+		{
+			final IResourceVersion resourceVersion;
+
+			if (application.usesDevelopmentConfig())
+			{
+				// use file last modified for resource cache keys
+				resourceVersion = new RequestCycleCachedResourceVersion(new LastModifiedResourceVersion());
+			}
+			else
+			{
+				// use md5 message digest for resource cache keys
+				resourceVersion = new CachingResourceVersion(new MessageDigestResourceVersion());
+			}
+			resourceCachingStrategy =
+				new FilenameWithVersionResourceCachingStrategy(resourceVersion);
+		}
 		return resourceCachingStrategy;
 	}",NotBuggy,"WICKET-3846 rework resource caching so it will by default work in clusters, also made the whole caching stuff pluggable and easier to extend

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1141298 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""WICKET-3846 rework resource caching so it will by default work in clusters, also made the whole caching stuff pluggable and easier to extend"" suggests a significant change to the resource caching mechanism in Wicket. The primary goals seem to be:
    *   Improved cluster compatibility.
    *   Increased pluggability and extensibility of the caching strategy.

2.  **Code Diff:** The code diff shows a change within the `getCachingStrategy()` method. It appears to be lazily initializing the `resourceCachingStrategy` if it's null. It also introduces conditional logic based on whether the application is in development mode (`application.usesDevelopmentConfig()`).

    *   **Development Mode:** If in development mode, it uses `LastModifiedResourceVersion` wrapped in `RequestCycleCachedResourceVersion`.
    *   **Production Mode:** If not in development mode, it uses `MessageDigestResourceVersion` wrapped in `CachingResourceVersion`.
    *   Finally, it creates a `FilenameWithVersionResourceCachingStrategy` using the chosen resource version.

3.  **Relevance and Bug Fix Indication:**

    *   The commit message mentions ""rework resource caching so it will by default work in clusters"". The code diff doesn't explicitly show cluster-specific code. However, using `MessageDigestResourceVersion` (likely MD5 hashing) in production mode is a common strategy for generating cache keys that are consistent across different servers in a cluster. This aligns with the cluster compatibility goal.
    *   The change introduces different caching strategies based on the application's configuration (development vs. production). This is a common pattern to avoid caching issues during development when resources are frequently modified. Using `LastModifiedResourceVersion` in development mode allows changes to be reflected quickly without needing to clear the cache.
    *   The introduction of `RequestCycleCachedResourceVersion` and `CachingResourceVersion` suggests an attempt to manage the caching lifecycle more explicitly, potentially addressing issues related to stale cache entries or incorrect cache invalidation.
    *   The commit message mentions ""pluggable and easier to extend"". The code diff shows a clear strategy pattern with the introduction of `IResourceVersion` and different implementations. This makes it easier to extend the caching mechanism with custom versioning strategies.

**Conclusion:**

The changes in the code diff align well with the commit message's goals of improving cluster compatibility and making the caching mechanism more pluggable. The introduction of different caching strategies based on the application's configuration, along with the explicit management of the caching lifecycle, suggests an attempt to address potential caching-related issues or bugs. Therefore, the changes indicate a bug fix or at least an improvement to prevent potential bugs.

**Buggy**
"
wicket,3339.json,d06f84d1b87011e5c152c5fb3f05ae50c1c58cda,"@@ -1,4 +1,4 @@
-			public Object getContainerRequest()
-			{
-				return null;
-			}

+				public Object getContainerRequest()
+				{
+					return null;
+				}",NotBuggy,"WICKET-5698 WebApplication#unmount() unmounts the whole compound mapper if some of its inner ones matches
",Buggy,"The commit message indicates a bug fix related to the `WebApplication#unmount()` method and its interaction with compound mappers. The provided code diff is minimal and doesn't seem directly related to unmounting or compound mappers. It only shows a change in indentation. Therefore, it's difficult to assess if this specific diff is a bug fix based solely on the provided information. However, given the commit message describes a bug fix, it's possible that this diff is a small part of a larger fix.

**Buggy**
"
wicket,11304.json,ed7c99768ab5764a76d77a1282d28b2f048fd1b8,"@@ -1,35 +1,43 @@
 			protected void populateItem(Item<IColumn<T, S>> item)
 			{
 				final IColumn<T, S> column = item.getModelObject();
 
-				WebMarkupContainer header = null;
+				WebMarkupContainer header;
 
 				if (column.isSortable())
 				{
 					header = newSortableHeader(""header"", column.getSortProperty(), stateLocator);
 				}
 				else
 				{
 					header = new WebMarkupContainer(""header"");
 				}
 
 				if (column instanceof IStyledColumn)
 				{
 					CssAttributeBehavior cssAttributeBehavior = new DataTable.CssAttributeBehavior()
 					{
 						private static final long serialVersionUID = 1L;
 
 						@Override
 						protected String getCssClass()
 						{
 							return ((IStyledColumn<?, S>)column).getCssClass();
 						}
 					};
 
 					header.add(cssAttributeBehavior);
 				}
 
+				if (column.getColspan() > 1) {
+					header.add(AttributeModifier.replace(""colspan"", column.getColspan()));
+				}
+
+				if (column.getRowspan() > 1) {
+					header.add(AttributeModifier.replace(""rowspan"", column.getRowspan()));
+				}
+
 				item.add(header);
 				item.setRenderBodyOnly(true);
 				header.add(column.getHeader(""label""));
 			}",NotBuggy,"WICKET-6095 Multiline headers in DataTable
",Buggy,"The commit message ""WICKET-6095 Multiline headers in DataTable"" suggests that the changes are related to handling multiline headers in the DataTable component. The diff shows modifications to the `populateItem` method, specifically adding `colspan` and `rowspan` attributes to the header based on the column's `getColspan()` and `getRowspan()` values. These changes directly address the ability to create multiline headers by allowing headers to span multiple columns or rows. The addition of `colspan` and `rowspan` attributes indicates that the previous implementation might have had issues with properly rendering headers that span multiple rows or columns, which can be considered a bug or missing feature.

**Buggy**"
wicket,3741.json,86fa5a7ac1e5e0aab3a2f47d423ffc53afb9f49c,"@@ -1,23 +1,23 @@
 	public BufferedWebResponse get(Object key)
 	{
 		BufferedWebResponse result = null;
 		Value value;
 		synchronized (this)
 		{
 			value = (Value)super.get(key);
 		}
 		if (value != null)
 		{
-			Duration elapsedTime =  Duration.between(value.creationTime, Instant.now());
-			if (lifetime.compareTo(elapsedTime) > 0)
+			Duration elapsedTime = Time.now().subtract(value.creationTime);
+			if (lifetime.greaterThan(elapsedTime))
 			{
 				result = value.response;
 			}
 			else
 			{
 				// expired, remove it
 				remove(key);
 			}
 		}
 		return result;
 	}",NotBuggy,"Revert ""Removed wicket-util date entities""

This reverts commit a8bb129af777670e6315142945155de3813d7b83.
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Revert ""Removed wicket-util date entities"""" indicates that a previous commit (a8bb129af777670e6315142945155de3813d7b83) is being undone.  Reverts often happen when the original change introduced a bug or had unintended consequences.

**Code Diff Analysis:**

The code diff shows a change in how the elapsed time is calculated.

*   **Original Code:** `Duration.between(value.creationTime, Instant.now());`
*   **Reverted Code:** `Time.now().subtract(value.creationTime);`

The change involves reverting from using `java.time.Duration.between` and `java.time.Instant.now()` to using `Time.now().subtract(value.creationTime)`.  The fact that this change was reverted suggests that the original removal of `wicket-util` date entities caused a problem. The original code likely introduced a bug, and this revert is intended to fix it.

**Reasoning:**

1.  **Revert Implies a Problem:** The commit message explicitly states that a previous commit is being reverted. This strongly suggests that the original commit introduced a bug or had undesirable side effects.
2.  **Date/Time Handling:** The code diff involves date/time calculations, which are often sources of bugs (e.g., incorrect time zones, off-by-one errors, etc.).
3.  **Potential Bug Fix:** The reversion of the date/time calculation method suggests that the original change was the cause of the problem.

**Conclusion:**

Based on the commit message and the code diff, it's highly likely that the reverted code was causing a bug. Therefore, the current commit is intended to fix that bug.

**Buggy**
"
wicket,6083.json,9e028af34cc432ddbffd6e09e82bf46d6226c0c8,"@@ -1,4 +1,5 @@
 				public boolean getDefaultFormProcessing()
 				{
+					// do not process the whole form
 					return false;
 				}",NotBuggy,"WICKET-6348 minor addition: update raw input when component is not contained in form
this wasn't done in 7.x either, but for consistency the raw input should be updated, so it is available when a validation fails
",Buggy,"The commit message ""WICKET-6348 minor addition: update raw input when component is not contained in form"" suggests a consistency fix related to how raw input is handled for components outside a form. The diff shows a change within the `getDefaultFormProcessing()` method, where a comment ""// do not process the whole form"" is added, and the method returns `false`.

Reasoning:
1. **Commit Message Alignment:** The commit message indicates a minor addition to update raw input for components not in a form. The code change, specifically setting `getDefaultFormProcessing()` to `false`, aligns with this by preventing the entire form from being processed, which is relevant when dealing with components outside the form.
2. **Bug Fix Indication:** The commit message mentions ""when a validation fails,"" suggesting that the original behavior might have led to incorrect or missing raw input data in certain validation scenarios. This implies a potential bug or inconsistency that is being addressed.
3. **Error Handling/Consistency:** The change aims to ensure consistency in raw input handling, which can be crucial for validation and data processing. Inconsistent behavior can be considered a bug.

Conclusion:
**Buggy**
"
wicket,4077.json,7c40e4dafa75973bf99b34567d977f310a5f02d0,"@@ -1,9 +1,9 @@
 	protected boolean shouldFilter(AppendingStringBuffer responseBuffer)
 	{
 		// To avoid reading the whole buffer for non-Ajax responses
 		// read just the first N chars. A candidate can start with:
 		// <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><ajax-response>
 		int min = Math.min(150, responseBuffer.length());
 		String firstNChars = responseBuffer.substring(0, min);
-		return firstNChars.contains(XmlAjaxResponse.START_ROOT_ELEMENT);
+		return firstNChars.contains(XmlPartialPageUpdate.START_ROOT_ELEMENT);
 	}",NotBuggy,"WICKET-5929 renamed *AjaxResponse to *PartialPageUpdate; moved all XML related encoding into XmlPartialPageUpdate
",NotBuggy,"The commit message indicates a renaming of `AjaxResponse` to `PartialPageUpdate` and a movement of XML-related encoding into `XmlPartialPageUpdate`. The provided code diff shows a change from `XmlAjaxResponse.START_ROOT_ELEMENT` to `XmlPartialPageUpdate.START_ROOT_ELEMENT`. This change directly reflects the renaming mentioned in the commit message. The change itself doesn't inherently indicate a bug fix, but rather a refactoring and reorganization of code. The renaming and moving of XML encoding suggest an improvement in code structure and maintainability, but not necessarily the correction of a bug.

**NotBuggy**"
wicket,39.json,64024788d5122cfe9a9b173b165ac7c069b7f665,"@@ -1,14 +1,14 @@
 		public boolean equals(Object obj)
 		{
 			if (this == obj)
 			{
 				return true;
 			}
 			if (obj instanceof QueryParameter == false)
 			{
 				return false;
 			}
 			QueryParameter rhs = (QueryParameter)obj;
-			return Objects.equal(getName(), rhs.getName()) &&
-				Objects.equal(getValue(), rhs.getValue());
+			return Objects.equal(getName(), rhs.getName())
+					&& Objects.equal(getValue(), rhs.getValue());
 		}",NotBuggy,"wip on decoupling and extracting wicket-request. also the request encoding is now properly propagated to url handling.

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@924375 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates work in progress on decoupling and extracting `wicket-request`, and also mentions that the request encoding is now properly propagated to URL handling. The code diff modifies the `equals` method of the `QueryParameter` class, specifically changing the formatting of the `return` statement.

The change in the `equals` method doesn't seem directly related to decoupling or request encoding propagation. It's a minor formatting change. Therefore, this change doesn't appear to be a bug fix.

**NotBuggy**"
wicket,10296.json,83740b9e5f9ca91ae185bc5683b6c94a5349b85b,"@@ -1,19 +1,20 @@
 	private Form<?> getCallbackForm()
 	{
 		Boolean insideModal = form.visitParents(ModalWindow.class,
 			new IVisitor<ModalWindow, Boolean>()
 			{
+				@Override
 				public void component(final ModalWindow object, final IVisit<Boolean> visit)
 				{
 					visit.stop(true);
 				}
 			});
 		if ((insideModal != null) && insideModal)
 		{
 			return form;
 		}
 		else
 		{
 			return form.getRootForm();
 		}
 	}",NotBuggy,"Add @Override to all overridden methods

This includes all places @Override is permitted in Java 6, since Wicket
6 requires Java 6.
",NotBuggy,"The commit message states the intention to add `@Override` annotations to all overridden methods, which is a good practice for code clarity and maintainability.

The provided code diff shows the addition of `@Override` to the `component` method within an anonymous inner class implementing the `IVisitor` interface. This aligns perfectly with the commit message. The change doesn't inherently indicate a bug fix but rather an improvement in code style and clarity. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**"
wicket,2209.json,ca03c252b5745bdccfb7c8cdffcc705846593ec1,"@@ -1,5 +1,6 @@
-	public void removePage(final String sessionId, final int pageId)
+	public void removePage(IPageContext context, IManageablePage page)
 	{
-		pagesCache.removePage(sessionId, pageId);
-		removePageData(sessionId, pageId);
+		getRequestData(context).remove(page);
+
+		getDelegate().removePage(context, page);
 	}",NotBuggy,"WICKET-6563 new IPageStore implementation

this closes #283
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""WICKET-6563 new IPageStore implementation"" suggests a new implementation or a significant change to the `IPageStore`. The phrase ""this closes #283"" indicates that this commit is intended to resolve a specific issue (likely a feature request or a bug report) tracked as issue #283.

2.  **Code Diff:** The code diff shows a change in the `removePage` method signature and its implementation.
    *   The original method `removePage(final String sessionId, final int pageId)` took a session ID and page ID as arguments.
    *   The modified method `removePage(IPageContext context, IManageablePage page)` now takes an `IPageContext` and an `IManageablePage` as arguments.
    *   The original implementation directly removed the page from `pagesCache` and then removed the page data.
    *   The new implementation retrieves request data based on the context, removes the page from that data, and then delegates the removal to another component (`getDelegate()`).

3.  **Analysis:**
    *   The change in method signature indicates a fundamental change in how pages are identified and removed. Instead of using session and page IDs, the new implementation uses an `IPageContext` and `IManageablePage` object. This suggests a shift towards a more object-oriented approach.
    *   The delegation to `getDelegate().removePage()` implies that the actual page removal logic might have been moved or refactored into another class or component.
    *   The change from directly removing from `pagesCache` to using `getRequestData(context).remove(page)` suggests a change in how pages are stored and managed within the session or request context.

4.  **Bug Fix Indication:** While the commit message indicates a new implementation, the changes in the `removePage` method suggest a potential fix for issues related to page removal. The original implementation might have had problems with concurrency, incorrect page removal, or other issues that the new implementation addresses. The change in method signature and the delegation pattern could be part of a larger effort to improve the robustness and correctness of page management. The fact that it closes #283 also suggests that it is fixing a reported issue.

**Conclusion:**

**Buggy**
"
wicket,4773.json,fe1e621081ebe22c22f6e5b5fa98d51e44d50d76,"@@ -1,4 +1,4 @@
 	public static <Z> CompoundPropertyModel<Z> of(IModel<Z> model)
 	{
-		return new CompoundPropertyModel<Z>(model);
+		return new CompoundPropertyModel<>(model);
 	}",NotBuggy,"Java 7 diamonds
",NotBuggy,"The commit message ""Java 7 diamonds"" suggests the commit is about using the diamond operator `<>` in Java code to reduce verbosity.

The diff shows a change from `new CompoundPropertyModel<Z>(model)` to `new CompoundPropertyModel<>(model)`. This change replaces the explicit type parameter `<Z>` with the diamond operator `<>`, which is a Java 7 feature. This aligns perfectly with the commit message. The change itself doesn't indicate a bug fix, but rather a stylistic improvement.

**NotBuggy**
"
vraptor4,1191.json,8757c99184f25ee826446209ff419f1120cf6131,"@@ -1,4 +1,4 @@
 	public void showAllMusicsAsHTTP() {
 		result.use(http()).body(""<p class=\""content\"">""+
-			dao.listAll().toString()+""</p>"");
+			musicDao.listAll().toString()+""</p>"");
 	}",NotBuggy,"Improving validation to musicjungle
",NotBuggy,"The commit message ""Improving validation to musicjungle"" suggests changes related to validation logic within the ""musicjungle"" application. However, the provided code diff replaces `dao.listAll()` with `musicDao.listAll()`. This change appears to be a simple renaming or refactoring of a data access object, not related to validation. There is no indication of bug fixing.

**NotBuggy**"
vraptor4,142.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,20 +1,20 @@
-		public String toString() {
+		public String getLink() {
 			Method method = null;
 
 			if (getMethodsAmountWithSameName() > 1) {
 				method = new Mirror().on(controller).reflect().method(methodName).withArgs(getClasses(args));
 				if (method == null && args.isEmpty()) {
 					throw new IllegalArgumentException(""Ambiguous method '"" + methodName + ""' on "" + controller + "". Try to add some parameters to resolve ambiguity, or use different method names."");
 				}
 			} else {
 				method = findMethodWithName(controller, methodName);
 			}
 
 			if(method == null) {
 				throw new IllegalArgumentException(
 					String.format(""There are no methods on %s named '%s' that receives args of types %s"",
 							controller, methodName, Arrays.toString(getClasses(args))));
 			}
 
 			return context.getContextPath() + router.urlFor(controller, method, getArgs(method));
 		}",NotBuggy,"Linker does not need to be a Map anymore
",Buggy,"The commit message ""Linker does not need to be a Map anymore"" doesn't seem to directly relate to the provided code diff. The diff focuses on the `toString` method (renamed to `getLink`) within a class that appears to be involved in routing or URL generation. The changes include:

1.  **Renaming `toString` to `getLink`:** This suggests a change in the intended use of the method, likely to explicitly represent retrieving a link rather than a general string representation.
2.  **Error Handling:** The code includes checks for ambiguous methods and methods that don't exist, throwing `IllegalArgumentException` in such cases. This indicates a focus on robustness and providing informative error messages.
3.  **Method Resolution:** The code attempts to find a suitable method based on the method name and arguments, handling cases where multiple methods with the same name exist.

The error handling and method resolution logic suggest that the original code might have had issues in these areas, potentially leading to incorrect or unexpected behavior. The changes aim to address these issues by providing more specific error messages and ensuring that the correct method is found. The renaming of the method suggests a clarification of its purpose, which could also be related to fixing a misunderstanding or misuse of the original method.

Based on the improved error handling and method resolution, it's likely that the changes address a bug or potential issue in the original code.

**Buggy**"
vraptor4,1047.json,634361fe0135b236651256d7c97945dec4f2c26d,"@@ -1,8 +1,8 @@
 	public void start() {
 		ControllerMethod method = controllerMethod.get();
-		stackStartingEvent.fire(new StackStarting(method));
+		interceptorsReadyEvent.fire(new InterceptorsReady(method));
 		LinkedList<InterceptorHandler> handlers = cache.getInterceptorHandlers();
 		internalStack.addFirst(handlers.iterator());
 		this.next(method, controllerInstance.get().getController());
 		internalStack.poll();
 	}",NotBuggy,"Renaming all events, to add some naming consistency
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Renaming all events, to add some naming consistency"" suggests a refactoring or code cleanup activity focused on improving the naming conventions of events within the codebase. It doesn't explicitly mention fixing a bug.

**Git Diff Analysis:**

The diff shows a single line change within the `start()` method:

- `stackStartingEvent.fire(new StackStarting(method));`
+ `interceptorsReadyEvent.fire(new InterceptorsReady(method));`

This change replaces the event `StackStartingEvent` with `InterceptorsReadyEvent`. The corresponding event object also changes from `StackStarting` to `InterceptorsReady`.

**Reasoning:**

The commit message aligns with the code change. The change involves renaming an event, which is consistent with the stated goal of adding naming consistency. There's no indication of error handling, logical corrections, or exception handling improvements. The change seems to be purely about renaming, which is a refactoring activity.  There's no evidence to suggest that the old event name was causing a bug.

**Conclusion:**

NotBuggy
"
vraptor4,1127.json,d63f531e2548c47d274077d9938be60c100d3caa,"@@ -1,5 +1,3 @@
 	private boolean isWebsocketRequest(HttpServletRequest request) {
-		// according to the Websocket spec (https://tools.ietf.org/html/rfc6455): The WebSocket Protocol 
-		// 5. The request MUST contain an |Upgrade| header field whose value MUST include the ""websocket"" keyword.
-	    return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
+		return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
 	}",NotBuggy,Moving comments within the method to the method's javadoc.,NotBuggy,"The commit message states that comments are being moved to the Javadoc. The diff shows that comments within the method body of `isWebsocketRequest` have been removed. There is no corresponding Javadoc change in the diff, but the removal of comments from the method body aligns with the intention described in the commit message. The change does not indicate a bug fix.

**NotBuggy**"
vraptor4,800.json,68bc254803a4e712c85d525df1319f1a6d68600c,"@@ -1,7 +1,7 @@
-	private File checkFile(File file) throws IOException {
+	private File checkFile(File file) throws FileNotFoundException {
 		if (!file.exists()) {
 			throw new FileNotFoundException(""File "" + file.getName() + ""doesn't exists"");
 		}
 		
 		return file;
 	}",NotBuggy,"Downgrading exception
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Downgrading exception"" suggests that the type of exception being thrown is being changed to a less specific or more general one. This could be done for various reasons, such as simplifying exception handling or aligning with the expected behavior of the calling code.

**Diff Analysis:**

The diff shows a change in the `checkFile` method's `throws` declaration. It's changing from `IOException` to `FileNotFoundException`.

**Reasoning:**

1.  **Exception Hierarchy:** `FileNotFoundException` is a subclass of `IOException`. This means that any code that catches `IOException` will also catch `FileNotFoundException`.

2.  **Specificity:** The original code threw a general `IOException` even when the specific problem was that the file was not found. The modified code now throws a more specific exception, `FileNotFoundException`, which accurately reflects the actual problem.

3.  **Potential Bug Fix:** While not a direct ""bug fix"" in the sense of correcting incorrect logic, this change improves the code's clarity and makes it easier for calling code to handle the specific ""file not found"" scenario. It's possible that the original, broader `IOException` was masking the true cause of an error, making debugging harder. By narrowing the exception type, the code becomes more maintainable and less prone to misinterpretation.

4.  **Error Handling:** This change directly impacts error handling. Code that specifically wants to handle the ""file not found"" case can now do so more easily by catching `FileNotFoundException`.

**Conclusion:**

The change improves the code's clarity and error handling by using a more specific exception type. While it might not be fixing a functional bug, it addresses a potential issue related to error handling and code maintainability. Therefore, I will classify this as a bug fix.

**Buggy**"
vraptor4,601.json,41ba6271a5f50496c4ada07f67c52b27ca15ef33,"@@ -1,3 +1,3 @@
 	public void indented() {
-		builder.setPrettyPrinting();
+		getGsonBuilder().setPrettyPrinting();
 	}",NotBuggy,"Uses protected getter for GsonBuilder to allow for extension
",NotBuggy,"The commit message ""Uses protected getter for GsonBuilder to allow for extension"" indicates a refactoring change to improve extensibility. The code change replaces direct access to the `builder` field with a call to `getGsonBuilder()`. This aligns with the commit message's intention of providing a protected getter. There's no indication of a bug fix in either the commit message or the code diff.

**NotBuggy**"
vraptor4,62.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,3 +1,3 @@
-		protected Map<Object, Linker> delegate() {
-			return Collections.emptyMap();
-		}

+	protected Map<Class<?>, Object> delegate() {
+		return Collections.emptyMap();
+	}",NotBuggy,"Linker does not need to be a Map anymore
",NotBuggy,"The commit message ""Linker does not need to be a Map anymore"" suggests a change in the data structure used for the linker. The diff shows that the `delegate()` method's return type has been changed from `Map<Object, Linker>` to `Map<Class<?>, Object>`. This indicates a refactoring or optimization of the linker implementation. The change in the return type suggests a potential modification in how linkers are stored or accessed. There is no explicit error handling, logical correction, or exception handling. The change seems to be related to data structure modification.

**NotBuggy**"
vraptor4,98.json,45b2bcef87bdd4c342f05b0134a3231749dc380d,"@@ -1,5 +1,5 @@
-	private <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
+	protected <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
 		if (args != null && args.length != 0) {
 			flash.includeParameters(DefaultControllerMethod.instanceFor(type, method), args);
 		}
 	}",NotBuggy,"Changing visibility of methods includeParametersInFlash and acceptsHttpGet
",NotBuggy,"The commit message states a change in the visibility of the methods `includeParametersInFlash` and `acceptsHttpGet`. The provided diff only shows a change in the visibility of the `includeParametersInFlash` method from `private` to `protected`. There is no indication of a bug fix. The change seems to be related to making the method accessible from subclasses, which is a refactoring or feature enhancement rather than a bug fix.

**NotBuggy**"
vraptor4,1124.json,68ec20ba9f131586d351821b75a9d3e7c842ef4c,"@@ -1,7 +1,8 @@
 	private URL getResource(String path) throws ServletException {
 		try {
 			return servletContext.getResource(path);
 		} catch (MalformedURLException e) {
-			throw new ServletException(""Something went wrong when trying to locate a beans.xml file"", e);
+			logger.error(""Something went wrong when trying to locate a beans.xml file"", e);
+			return null;
 		}
 	}",NotBuggy,"returning null instead of rethrowing exception
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states ""returning null instead of rethrowing exception"". This immediately suggests a change in error handling. Instead of propagating an exception up the call stack, the code is now designed to return `null` in an error scenario.

2.  **Code Diff:**
    *   The original code within the `try` block attempts to get a resource using `servletContext.getResource(path)`.
    *   If a `MalformedURLException` occurs, the original code threw a `ServletException`, wrapping the `MalformedURLException`. This would halt the execution flow and potentially cause the application to crash or misbehave.
    *   The modified code now catches the `MalformedURLException`, logs an error message using `logger.error()`, and then returns `null`.

3.  **Alignment and Bug Fix Indication:**
    *   The code change directly reflects the commit message. The exception is no longer re-thrown; instead, `null` is returned.
    *   This change is indicative of a bug fix because re-throwing the exception might have been causing unintended consequences, such as application crashes or incomplete processing. Returning `null` allows the calling code to handle the error gracefully, potentially providing a fallback mechanism or a more user-friendly error message. The original code's behavior of throwing a `ServletException` when a resource path was malformed could be considered a bug, as it might be too aggressive and disrupt the application's normal operation. The updated behavior provides a more controlled way to handle the error.

**Conclusion:**

**Buggy**
"
vraptor4,853.json,8fbc9e23ee5038a35159f408a961f98de86b562a,"@@ -1,6 +1,6 @@
 	public <V, K> CacheStore<K, V> createCacheWrapper(int capacity) {
 		Cache<K, V> guavaCache = CacheBuilder.newBuilder()
 			.maximumSize(capacity)
 			.build();
-		return new GuavaCacheWrapper<K,V>(guavaCache);
+		return new GuavaCacheWrapper<>(guavaCache);
 	}",NotBuggy,"squid:S1192, squid:S2293 - String literals should not be duplicated, The diamond operator should be used
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""squid:S1192, squid:S2293 - String literals should not be duplicated, The diamond operator should be used"" indicates two SonarQube rule violations are being addressed:

*   **squid:S1192 (String literals should not be duplicated):** This suggests that the commit aims to remove or refactor duplicated string literals in the code.
*   **squid:S2293 (The diamond operator should be used):** This indicates the commit replaces explicit type arguments in constructor calls with the diamond operator (`<>`) for improved readability and conciseness.

**Git Diff Analysis:**

The diff shows a single change:

```diff
-		return new GuavaCacheWrapper<K,V>(guavaCache);
+		return new GuavaCacheWrapper<>(guavaCache);
```

This change replaces the explicit type arguments `<K,V>` in the `GuavaCacheWrapper` constructor call with the diamond operator `<>`.

**Reasoning:**

The diff directly addresses the `squid:S2293` rule mentioned in the commit message. The change improves code readability by using the diamond operator, which is a standard practice in modern Java. The commit message does not mention any bug fixes, and the code change itself doesn't appear to be related to fixing any logical errors or unexpected behavior. It's purely a stylistic improvement. The other rule mentioned in the commit message, `squid:S1192`, is not reflected in the diff.

**Conclusion:**

The commit message and diff are consistent. The change addresses a code style issue (using the diamond operator). There's no indication of a bug fix.

**NotBuggy**"
vraptor4,645.json,fc1de5e5d28c1b21ea5c9e87120c1cfec3b1e1ee,"@@ -1,4 +1,4 @@
 	public Serializer recursive() {
-		this.serializee.setRecursive(true);
+		builder.getSerializee().setRecursive(true);
 		return this;
 	}",NotBuggy,"Reusing Serializee from builder
",NotBuggy,"The commit message ""Reusing Serializee from builder"" suggests a change in how the `Serializee` object is accessed or managed, specifically indicating that it's now being obtained from a builder.

The code diff shows that `this.serializee.setRecursive(true)` is changed to `builder.getSerializee().setRecursive(true)`. This confirms the commit message's intention of reusing `Serializee` from the builder.

This change doesn't inherently indicate a bug fix. It's more likely a refactoring or a change in the design where the `Serializee` object's lifecycle or management is now tied to the builder. There's no error handling, logical correction, or exception handling improvement visible in this diff.

**NotBuggy**
"
vraptor4,113.json,62fe19ddb7501c4371a8646e2f4316cb70ee0eaf,"@@ -1,7 +1,7 @@
 	public Object[] consumeParameters(ControllerMethod method) {
-		Object[] args = (Object[]) session.getAttribute(nameFor(method));
-		if (args != null) {
-			session.removeAttribute(nameFor(method));
-		}
+		String param = nameFor(method);
+		Object[] args = (Object[]) session.getAttribute(param);
+		session.removeAttribute(param);
+
 		return args;
 	}",NotBuggy,"Removing unnecessary statement
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states ""Removing unnecessary statement."" This suggests the removal of redundant or superfluous code.

2.  **Code Diff:**
    *   The original code had a conditional statement `if (args != null)` before removing the attribute from the session.
    *   The modified code removes this conditional statement, directly removing the attribute using `session.removeAttribute(param);`.

3.  **Analysis:**
    *   The removal of the `if (args != null)` check implies that the code was previously checking if the attribute existed before attempting to remove it.
    *   Removing the attribute regardless of its existence is generally safe in `HttpSession`, as `removeAttribute` will simply do nothing if the attribute doesn't exist.
    *   The original code might have been written defensively, but the check was unnecessary. Removing it simplifies the code.
    *   The introduction of the `param` variable to store the result of `nameFor(method)` and reuse it in both `getAttribute` and `removeAttribute` calls improves readability and maintainability. It also reduces the risk of subtle bugs if `nameFor(method)` had side effects or its result could change between calls.

4.  **Bug Fix Indication:**
    *   While the change simplifies the code and could be considered an improvement, it doesn't strongly suggest a bug fix. The original code, although slightly verbose, was not necessarily incorrect. The removal of the `if` statement doesn't inherently fix a flaw in the program's logic or error handling. It's more of a code cleanup or refactoring.

**Conclusion:**

NotBuggy
"
vraptor4,881.json,743c3c430bf1b46974df6ef038892a42ef97c667,"@@ -1,6 +1,6 @@
 	public Map<String, Collection<Message>> getGrouped() {
 		if (grouped == null) {
-			grouped = Multimaps.index(delegate, new GroupByCategory()).asMap();
+			grouped = index(delegate, groupByCategory).asMap();
 		}
 		return grouped;
 	}",NotBuggy,"Static instances and static import
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Static instances and static import"" suggests two main changes:

1.  **Static Instances:** Introduction or modification of static instances within the code.
2.  **Static Import:** Usage of static imports to simplify code by directly referencing static members of a class.

**Git Diff Analysis:**

```diff
@@ -1,6 +1,6 @@
 	public Map<String, Collection<Message>> getGrouped() {
 		if (grouped == null) {
-			grouped = Multimaps.index(delegate, new GroupByCategory()).asMap();
+			grouped = index(delegate, groupByCategory).asMap();
 		}
 		return grouped;
 	}
```

The diff shows a change within the `getGrouped()` method. Specifically, `Multimaps.index(delegate, new GroupByCategory()).asMap()` is replaced with `index(delegate, groupByCategory).asMap()`.

**Reasoning:**

1.  **Static Import:** The change from `Multimaps.index` to `index` strongly suggests the introduction of a static import. It implies that `index` is a static method imported from the `Multimaps` class (or another class where `index` is defined). This aligns with the ""static import"" part of the commit message.
2.  **Static Instance:** The change from `new GroupByCategory()` to `groupByCategory` suggests that `groupByCategory` is now a static instance of the `GroupByCategory` class. This aligns with the ""Static instances"" part of the commit message. The code is likely changed to use a pre-existing static instance instead of creating a new instance every time the `getGrouped()` method is called.

**Bug Fix Assessment:**

While the changes align with the commit message, they don't inherently indicate a bug fix. The refactoring to use a static instance and static import is more likely a performance optimization or code simplification. There's no explicit error handling, logical correction, or exception handling involved. The change aims to improve efficiency or readability rather than fix a flaw in the code's functionality.

**Conclusion:**

NotBuggy
"
vraptor4,935.json,7f19b84c6bed93f11e11501f35c21330780bd540,"@@ -1,8 +1,8 @@
-	private void addAll(Set<ConstraintViolation<Object>> errors) {
-		for (ConstraintViolation<Object> v : errors) {
+	public <T> void addAll(Set<ConstraintViolation<T>>  errors) {
+		for (ConstraintViolation<T> v : errors) {
 			String msg = interpolator.interpolate(v.getMessageTemplate(), new BeanValidatorContext(v), locale);
 			String category = v.getPropertyPath().toString();
 			add(new SimpleMessage(category, msg));
 			logger.debug(""added message {}={} for contraint violation"", category, msg);
 		}
 	}",NotBuggy,"Relaxing visibility
",NotBuggy,"The commit message ""Relaxing visibility"" suggests a change in the accessibility of a method or field, making it more accessible from other parts of the code.

The diff shows the following changes:

1.  `private` to `public`: The visibility of the `addAll` method is changed from `private` to `public`. This aligns with the commit message.
2.  `Set<ConstraintViolation<Object>>` to `Set<ConstraintViolation<T>>`: The parameter type of the `addAll` method is changed from `Set<ConstraintViolation<Object>>` to `Set<ConstraintViolation<T>>`. This makes the method more generic and reusable.

The changes do not indicate a bug fix. The visibility change and the introduction of generics suggest a refactoring or enhancement rather than a correction of faulty logic or error handling.

**NotBuggy**"
vraptor4,1163.json,b92a72ec752eea09e6e59da5b42aafae87e198c0,"@@ -1,3 +1,3 @@
 	public void add(Music music) {
-		session.save(music);
+		getSession().save(music);
 	}",NotBuggy,"Configures JPA and uses EntityManager instead of Hibernate Session;
",NotBuggy,"The commit message ""Configures JPA and uses EntityManager instead of Hibernate Session"" suggests a migration from Hibernate's Session API to JPA's EntityManager. The provided diff shows a change from `session.save(music)` to `getSession().save(music)`.

The change itself doesn't directly indicate a bug fix. It seems like the code is still using `save` method. The commit message indicates a configuration change and a shift towards using EntityManager. However, the diff doesn't show the complete picture of the JPA configuration or the EntityManager usage. It only shows a change in how the `save` method is being called. Without more context, it's difficult to determine if this change is related to fixing a bug. It could be part of a larger refactoring effort.

Therefore, based on the limited information, it's difficult to conclude that this commit fixes a bug.

**NotBuggy**
"
flink,31570.json,1ab79f65fabdb47d7d041557132b4978db4bc9e2,"@@ -1,4 +1,9 @@
 	public CatalogBaseTable copy() {
-		return new HiveCatalogView(
-			this.getOriginalQuery(), this.getExpandedQuery(), this.getSchema().copy(), new HashMap<>(this.getProperties()), getComment());
+		return new CatalogViewImpl(
+			getOriginalQuery(),
+			getExpandedQuery(),
+			getSchema().copy(),
+			new HashMap<>(getProperties()),
+			getComment()
+		);
 	}",NotBuggy,"[FLINK-12965][table][hive] unify catalog view implementations

This PR unified implementations of CatalogView.

This closes #8882.
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states that the PR unifies implementations of `CatalogView`. This suggests a refactoring or consolidation of code, rather than a direct bug fix. The phrase ""unify implementations"" implies that there were multiple ways of achieving the same thing, and this PR aims to standardize them.

2.  **Git Diff:** The diff shows a change within the `copy()` method of a class (presumably `HiveCatalogView`, based on the original code). The original code created a new `HiveCatalogView` object, while the modified code creates a `CatalogViewImpl` object instead. This change directly supports the commit message's claim of unifying implementations. The `HiveCatalogView` is being replaced by a more general `CatalogViewImpl`. This suggests that `HiveCatalogView` might have been a specific implementation that is now being replaced by a more unified approach.

3.  **Bug Fix Indicators:** There are no explicit error handling updates, logical corrections, or exception-handling improvements in the diff. The change appears to be a refactoring step to consolidate the implementations of `CatalogView`. The change doesn't seem to address any specific malfunction or incorrect behavior.

**Conclusion:**

The commit message and the code changes align with a refactoring effort to unify the implementations of `CatalogView`. There's no evidence to suggest that this change is related to fixing a bug.

**NotBuggy**
"
flink,32529.json,891075aefcfd702142c90cab51759b40765c5a5d,"@@ -1,3 +1,3 @@
 	public <T> T accept(QueryOperationVisitor<T> visitor) {
-		return visitor.visitSort(this);
+		return visitor.visit(this);
 	}",NotBuggy,"[hotfix][table-api] Renamed methods of TableOperationsVisitor
",NotBuggy,"The commit message indicates a renaming of methods in `TableOperationsVisitor`. The diff shows a method `visitSort` being renamed to `visit`. This aligns with the commit message and suggests a refactoring or renaming operation, not necessarily a bug fix.

**NotBuggy**"
flink,25786.json,8d67aa55f1c11e167a936f1f2a9d954db5960673,"@@ -1,13 +1,12 @@
 	public O returns(Class<OUT> typeClass) {
-		if (typeClass == null) {
-			throw new IllegalArgumentException(""Type class must not be null."");
-		}
+		requireNonNull(typeClass, ""type class must not be null"");
 		
 		try {
-			TypeInformation<OUT> ti = (TypeInformation<OUT>) TypeExtractor.createTypeInfo(typeClass);
-			return returns(ti);
+			return returns(TypeInformation.of(typeClass));
 		}
 		catch (InvalidTypesException e) {
-			throw new InvalidTypesException(""The given class is not suited for providing necessary type information."", e);
+			throw new InvalidTypesException(""Cannot infer the type information from the class alone."" +
+					""This is most likely because the class represents a generic type. In that case,"" +
+					""please use the 'returns(TypeHint)' method instead."", e);
 		}
 	}",NotBuggy,"[FLINK-2788] [apis] Add TypeHint class to allow type-safe generic type parsing

This closes #1744
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""[FLINK-2788] [apis] Add TypeHint class to allow type-safe generic type parsing"" suggests the introduction of a `TypeHint` class to handle generic type parsing in a type-safe manner. This implies that the previous method of handling generic types might have been problematic or lacking in type safety, which could be considered a bug or at least a deficiency.

**Git Diff Analysis:**

The diff focuses on the `returns(Class<OUT> typeClass)` method. Let's break down the changes:

1.  **Null Check:**
    *   The original code had a null check for `typeClass` with a custom `IllegalArgumentException`.
    *   The modified code replaces this with `requireNonNull(typeClass, ""type class must not be null"")`. This is a minor change, likely for code style or consistency, and doesn't directly indicate a bug fix.

2.  **Type Information Creation:**
    *   The original code used `TypeExtractor.createTypeInfo(typeClass)` to create `TypeInformation`.
    *   The modified code uses `TypeInformation.of(typeClass)`. This suggests a change in how type information is derived from the class.  It's possible that `TypeInformation.of` is a newer, more robust, or preferred method.

3.  **Exception Handling:**
    *   The original code caught `InvalidTypesException` and re-threw it with a generic message.
    *   The modified code catches the same exception but provides a much more specific and helpful error message: ""Cannot infer the type information from the class alone...please use the 'returns(TypeHint)' method instead."" This is a significant change. The new message clearly indicates a limitation of the `returns(Class)` method when dealing with generic types and guides the user to use the new `TypeHint` approach. This strongly suggests that the original method had issues with generic types, which the `TypeHint` is designed to address.

**Reasoning:**

The key indicator of a bug fix is the improved exception handling and the suggestion to use `TypeHint` for generic types. The original code likely failed or produced incorrect results when used with generic types due to limitations in the type extraction process. The new code explicitly acknowledges this limitation and directs users to the `TypeHint` class, which is the core of the commit message. The more descriptive exception message and the introduction of `TypeHint` strongly suggest that the previous implementation had a deficiency in handling generic types, which is now being addressed.

**Conclusion:**

**Buggy**
"
flink,3845.json,f728129bdb8c3176fba03c3e74c65ed254146061,"@@ -1,5 +1,5 @@
 	public int releaseMemory() {
-		// The pipelined subpartition does not react to memory release requests. The buffers will be
-		// recycled by the consuming task.
+		// The pipelined subpartition does not react to memory release requests.
+		// The buffers will be recycled by the consuming task.
 		return 0;
 	}",NotBuggy,"[FLINK-5169] [network] Make consumption of InputChannels fair
",NotBuggy,"The commit message indicates a change to ensure fair consumption of InputChannels. The code diff modifies a comment related to memory release in a pipelined subpartition. The original comment mentioned that buffers would be recycled by the consuming task, while the modified comment removes the first sentence and keeps the second sentence. This change doesn't seem directly related to fairness in InputChannel consumption. The change is only in a comment, and there is no error handling, logical correction, or exception handling improvement.
**NotBuggy**"
flink,12245.json,c401da36dfb5ff05527ed769babe16cbfdfc80be,"@@ -1,3 +1,3 @@
-	Optional<String> getPyExecutable() {
-		return Optional.ofNullable(pyExecutable);
+	Optional<String> getEntryPointScript() {
+		return Optional.ofNullable(entryPointScript);
 	}",NotBuggy,"[FLINK-16668][python] Improve PythonDriver to parse python dependency info from configuration

This closes #11682.
",NotBuggy,"The commit message indicates an improvement to the PythonDriver to parse Python dependency information from the configuration. The provided code diff changes `getPyExecutable()` to `getEntryPointScript()`. This change does not seem directly related to parsing Python dependency information. It looks more like a renaming of a method that retrieves the entry point script. There's no error handling, logical correction, or exception handling improvement visible in the diff. Therefore, it's unlikely to be a bug fix.

**NotBuggy**"
flink,291.json,4452be3a6c1c165b7d808cb4b779e6c2ecd69f01,"@@ -1,3 +1,3 @@
-	public long getMin() {
-		return (long) descriptiveStatistics.getMin();
-	}

+		public double getMin() {
+			return min;
+		}",NotBuggy,"[FLINK-12982][metrics] improve DescriptiveStatisticsHistogramStatistics performance

Instead of redirecting DescriptiveStatisticsHistogramStatistics calls to
DescriptiveStatistics, it takes a point-in-time snapshot using an own
UnivariateStatistic implementation that
a) calculates min, max, mean, and standard deviation in one go (as opposed to
   four iterations over the values array!)
b) caches pivots for the percentile calculation to speed up retrieval of
   multiple percentiles/quartiles

As a result, this roughly increases value retrieval performance by 120% when
accessing typical statistics in a metrics reporter, e.g. the InfluxDB reporter:
count, min, max, mean, stddev, p50, p75, p95, p98, p99, p999.
",Buggy,"The commit message indicates a performance improvement for `DescriptiveStatisticsHistogramStatistics` by avoiding multiple iterations over the values array and caching pivots for percentile calculations. The provided code diff shows a change in the `getMin()` method.

The original implementation retrieves the minimum value by calling `descriptiveStatistics.getMin()`, which likely involves iterating over the data. The modified implementation directly returns a cached `min` value. This aligns with the commit message's goal of improving performance by caching statistical values and avoiding redundant calculations. The change suggests that the original implementation was inefficient, which can be considered a bug or at least a performance bottleneck.

Therefore, the changes indicate a bug fix (performance bug).

**Buggy**"
flink,15309.json,d01d369701c8f79b06d60ee0f731392d3da9b912,"@@ -1,4 +1,4 @@
 	public GraphCsvReader fieldDelimiterEdges(String delimiter) {
-		this.EdgeReader.fieldDelimiter(delimiter);
+		this.edgeReader.fieldDelimiter(delimiter);
 		return this;
 	}",NotBuggy,"[FLINK-1520] [gelly] add types methods and make formatting changes to the graph csv reader

This squashes the following commits:

[FLINK-1520] [gelly] add named types methods for reading a Graph from CSV input,
with and without vertex/edge values. Change the examples and the tests accordingly.

[FLINK-1520] [gelly] corrections in Javadocs; updated documentation

This closes #1149
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""[FLINK-1520] \[gelly] add types methods and make formatting changes to the graph csv reader"" suggests the addition of new methods for reading graphs from CSV and some formatting changes. The sub-commits indicate adding methods for reading graphs with and without vertex/edge values and documentation updates.

2.  **Diff Analysis:**
    *   The diff shows a change from `this.EdgeReader.fieldDelimiter(delimiter);` to `this.edgeReader.fieldDelimiter(delimiter);`. This is a change in the case of the variable name, suggesting a correction of a typo or a naming convention issue. It's likely that `EdgeReader` was intended to be `edgeReader`, implying a bug fix.

3.  **Relevance Assessment:**
    *   The commit message mentions ""formatting changes,"" which could encompass variable naming conventions. However, the change from `EdgeReader` to `edgeReader` strongly suggests a correction of an incorrect variable name, which is a bug fix.

**Conclusion:**

The change in variable name from `EdgeReader` to `edgeReader` indicates a correction of a typo or naming convention issue, which is a bug fix.

**Buggy**"
flink,16953.json,1f7150fc6037406f0656de1babdbaa498c514bb5,"@@ -1,14 +1,15 @@
 	public <L, R> SingleOutputStreamOperator<R> select(
 			final OutputTag<L> timeoutOutputTag,
 			final PatternTimeoutFunction<T, L> patternTimeoutFunction,
 			final TypeInformation<R> outTypeInfo,
 			final PatternSelectFunction<T, R> patternSelectFunction) {
 		return CEPOperatorUtils.createTimeoutPatternStream(
 			inputStream,
 			pattern,
 			comparator,
 			clean(patternSelectFunction),
 			outTypeInfo,
 			timeoutOutputTag,
-			clean(patternTimeoutFunction));
+			clean(patternTimeoutFunction),
+			lateDataOutputTag);
 	}",NotBuggy,"[FLINK-9476][cep] Emit late elements in CEP as sideOutput

This closes #6104
",Buggy,"The commit message indicates a fix related to emitting late elements in CEP (Complex Event Processing) as side outputs. The code diff shows a modification in the `select` method where a `lateDataOutputTag` is being passed to the `createTimeoutPatternStream` method. This suggests that the change is indeed related to handling late data and directing it to a side output, which aligns with the commit message's intention of emitting late elements as side outputs. The addition of `lateDataOutputTag` strongly suggests a bug fix related to the handling of late events in CEP.

**Buggy**"
flink,19917.json,24eb47ac86299920aa137d5b6394a248ff58d19e,"@@ -1,3 +1,3 @@
-	protected int getLength() {
-		return 2;
+	public int getLength() {
+		return 4;
 	}",NotBuggy,"Renaming part 2 (core and java api)
",Buggy,"The commit message ""Renaming part 2 (core and java api)"" suggests a renaming operation, which doesn't align with the provided code diff. The code diff shows a change in the return value of the `getLength()` method from 2 to 4, and a change in the method's visibility from protected to public. This change is not related to renaming. The change in return value suggests a bug fix or a functional change.

**Buggy**"
flink,1138.json,033c69f9477c6352865e7e0da01296dd778ffe59,"@@ -1,5 +1,5 @@
-	public Class<FlatCombineFunction<IN, OUT>> getStubType() {
+	public Class<GroupCombineFunction<IN, OUT>> getStubType() {
 		@SuppressWarnings(""unchecked"")
-		final Class<FlatCombineFunction<IN, OUT>> clazz = (Class<FlatCombineFunction<IN, OUT>>) (Class<?>) FlatCombineFunction.class;
+		final Class<GroupCombineFunction<IN, OUT>> clazz = (Class<GroupCombineFunction<IN, OUT>>) (Class<?>) GroupCombineFunction.class;
 		return clazz;
 	}",NotBuggy,"[Flink-1780] Rename FlatCombineFunction to GroupCombineFunction

This closes #530
",NotBuggy,"The commit message indicates a renaming of `FlatCombineFunction` to `GroupCombineFunction`. The diff shows that the code indeed reflects this renaming. There are no error handling updates, logical corrections, or exception-handling improvements in the diff. The change is purely a renaming operation.

**NotBuggy**"
flink,39338.json,5cebfb76c717568ede8f7e8a82e9bbdf774c19ed,"@@ -1,12 +1,22 @@
 	private void finishAndReportAsync(Map<OperatorID, OperatorSnapshotFutures> snapshotFutures, CheckpointMetaData metadata, CheckpointMetrics metrics) {
+		final Future<?> channelWrittenFuture;
+		if (unalignedCheckpointEnabled) {
+			ChannelStateWriteResult writeResult = channelStateWriter.getWriteResult(metadata.getCheckpointId());
+			channelWrittenFuture = CompletableFuture.allOf(
+				writeResult.getInputChannelStateHandles(),
+				writeResult.getResultSubpartitionStateHandles());
+		} else {
+			channelWrittenFuture = FutureUtils.completedVoidFuture();
+		}
 		// we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit
 		executorService.execute(new AsyncCheckpointRunnable(
 			snapshotFutures,
 			metadata,
 			metrics,
+			channelWrittenFuture,
 			System.nanoTime(),
 			taskName,
 			closeableRegistry,
 			env,
 			asyncExceptionHandler));
 	}",NotBuggy,"[FLINK-16587][checkpointing] Spill the in-flight input and output buffers during checkpointing.
",Buggy,"The commit message indicates a change related to checkpointing, specifically spilling in-flight input and output buffers during checkpointing. The code diff introduces a new `channelWrittenFuture` which seems to be related to writing channel state handles, which aligns with the commit message's intention of handling in-flight data during checkpointing. The condition `unalignedCheckpointEnabled` suggests this is part of an optimization or a specific checkpointing strategy. The use of `ChannelStateWriteResult` and `channelStateWriter` further supports this interpretation. The code ensures that channel data is properly written before proceeding with the rest of the checkpointing process. This looks like a bug fix or improvement related to checkpointing.

**Buggy**"
flink,35096.json,9a6ca547d6bd261730c46519f6bffa0b699ec218,"@@ -1,3 +1,3 @@
-		public Iterable<UV> values() throws Exception {
-			return internalMap.values();
+		public Iterable<MV> values() throws Exception {
+			return new ValuesIterable(this.iterator());
 		}",NotBuggy,"[FLINK-13266][table] Relocate blink runtime classes to avoid class clashes
",Buggy,"The commit message indicates a relocation of Blink runtime classes to avoid class clashes. The provided code diff changes the return type of the `values()` method from `Iterable<UV>` to `Iterable<MV>` and modifies the return statement to return a `ValuesIterable` object created from the iterator of the class.

The change in return type and the introduction of `ValuesIterable` suggest a modification to how values are iterated, potentially to resolve a class clash issue as mentioned in the commit message. This could be a bug fix if the previous implementation was causing class loading or runtime errors due to the clashes.

Therefore, the changes indicate a bug fix.

**Buggy**"
flink,20306.json,bb76fda2444969c4e3a05e05485c43b533d8a499,"@@ -1,3 +1,3 @@
 	public boolean isImmutableType() {
-		return true;
+		return false;
 	}",NotBuggy,"Added direct array serializers for all primitives
",NotBuggy,"The commit message ""Added direct array serializers for all primitives"" suggests the addition of new functionality related to serializing primitive arrays. The provided code diff changes the return value of the `isImmutableType()` method from `true` to `false`. This change doesn't seem directly related to adding array serializers. It's possible that this change is part of a larger effort to implement the array serializers, and this specific modification is needed to allow the serialization logic to handle mutable array types. However, without more context, it's difficult to definitively say if this change is a bug fix. It's more likely a necessary adjustment to accommodate the new functionality. Therefore, I'll lean towards it not being a bug fix in isolation.

**NotBuggy**"
flink,22520.json,1a10fbef644ad32a3358711bfa5a167118186482,"@@ -1,3 +1,3 @@
-				public URL nextElement() {
-					return iter.next();
-				}

+			public URL nextElement() {
+				return iter.next();
+			}",NotBuggy,"[FLINK-11952][1/3] Make ChildFirstClassLoader a top-level class in flink-core
",NotBuggy,"The commit message indicates a refactoring change, specifically moving the `ChildFirstClassLoader` to a top-level class. The provided diff only shows a minor formatting change (indentation). This change doesn't seem to be related to fixing a bug, but rather to code organization.

**NotBuggy**"
flink,41571.json,8cdf2ff7e5817acc0c239ce31c098daf33d326b7,"@@ -1,3 +1,3 @@
-		public void setPendingTransaction(TXN pendingTransaction) {
+		public void setPendingTransaction(TransactionHolder<TXN> pendingTransaction) {
 			this.pendingTransaction = pendingTransaction;
 		}",NotBuggy,"[FLINK-7784] [kafka011-producer] Make TwoPhaseCommitSinkFunction aware of transaction timeouts.

TwoPhaseCommitSinkFunction allows to configure a transaction timeout. The
timeout can be used to log warnings if the transaction's age is appraoching
the timeout, and it can be used to swallow exceptions that are likely
irrecoverable. This commit also integrates these changes to the
FlinkKafkaProducer011.

This closes #4910.
",Buggy,"The commit message indicates that the changes are related to transaction timeouts in `TwoPhaseCommitSinkFunction` and `FlinkKafkaProducer011`. The goal is to log warnings and handle exceptions related to transaction timeouts.

The provided diff snippet shows a change in the `setPendingTransaction` method's parameter type from `TXN` to `TransactionHolder<TXN>`. This change suggests that the pending transaction is now wrapped in a `TransactionHolder`, which likely contains additional information such as the transaction's start time, allowing the system to track the transaction's age and implement the timeout-related logic described in the commit message. This change is directly related to the feature described in the commit message.

Based on the commit message and the code diff, the changes are related to handling transaction timeouts, which can be considered a bug fix or improvement related to error handling and resilience.

**Buggy**
"
eclipseJdt,25888.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,4 +1,4 @@
 public void goForHeaders(){
 	super.goForHeaders();
-	isFirst = true;
+	this.isFirst = true;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"The commit message indicates a cleanup operation focusing on code style and formatting: adding `this.` to fields, removing `this.` from message sends, and removing trailing whitespace. The provided diff shows the addition of `this.` to the `isFirst` field. This change aligns with the commit message's intention to add `this.` to fields for clarity and consistency. There's no indication of bug fixing, error handling, or logical correction in this specific diff. The change is purely stylistic.

**NotBuggy**"
eclipseJdt,20964.json,8245bd73c0aef7bf5f6de5def8941104b56db6f7,"@@ -1,3 +1,3 @@
 public int getNumberOfParameters() {
-	return fParameterTypes == null ? 0 : fParameterTypes.length;
+	return this.parameterTypes == null ? 0 : this.parameterTypes.length;
 }",NotBuggy,"*** empty log message ***
",Buggy,"The commit message is empty, which is not ideal, but we can still analyze the code diff.

The code diff shows a change from `fParameterTypes` to `this.parameterTypes`. This suggests a potential refactoring or correction of a variable name. It's possible that `fParameterTypes` was a typo or an incorrect reference, and `this.parameterTypes` is the correct field to access. Without more context, it's difficult to definitively say if this is a bug fix, but the change suggests a correction of some sort. It's likely that `fParameterTypes` was not correctly initialized or accessible, leading to incorrect behavior.

Reasoning:
1. The change involves correcting a field name from `fParameterTypes` to `this.parameterTypes`.
2. The use of `this` indicates that the intention is to access the class member `parameterTypes`.
3. The original code might have been referencing a non-existent or uninitialized field, potentially leading to incorrect behavior.
4. The change likely fixes an issue related to accessing the correct parameter types.

Conclusion: **Buggy**"
eclipseJdt,15535.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(SingleVariableDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message ""Add comment to empty blocks"" suggests that the change involves adding comments to empty code blocks.

The provided diff shows that a comment ""// default implementation: do nothing"" has been added to an empty `endVisit` method. This aligns perfectly with the commit message. There's no indication of a bug fix, error handling, or logical correction. The change is purely about adding a comment for clarity.

**NotBuggy**"
eclipseJdt,18964.json,527f8171257d427ef8c7c37dd3ea4724f920d7c5,"@@ -1,3 +1,3 @@
-boolean isKnownType(String qualifiedTypeName) {
+public boolean isKnownType(String qualifiedTypeName) {
 	return typeLocators.containsKey(qualifiedTypeName);
 }",NotBuggy,"Changes for API tools
",NotBuggy,"The commit message ""Changes for API tools"" is very generic and doesn't provide specific details about the changes made.

The code diff changes the visibility of the `isKnownType` method from package-private (default) to public. This change doesn't inherently indicate a bug fix. It's more likely a change in API design or accessibility. There's no error handling or logical correction in the code modification.

Therefore, based on the available information, the changes do not indicate a bug fix.

**NotBuggy**
"
eclipseJdt,5067.json,3c02a6af2c4b681d96ee9d070d231fb04b85b5fd,"@@ -1,3 +1,3 @@
 	public boolean isVarArgs() {
-		return (this.type.bits & IsVarArgs) != 0;
+		return this.type != null &&  (this.type.bits & IsVarArgs) != 0;
 	}",NotBuggy,"*** empty log message ***
",Buggy,"The commit message is empty, which is not ideal, but we can still analyze the code diff.

The code change adds a null check `this.type != null` before accessing `this.type.bits`. This suggests that `this.type` could potentially be null, and accessing `bits` on a null object would result in a `NullPointerException`. The added null check prevents this exception. This indicates a bug fix related to a potential `NullPointerException`.

Therefore, the change is a bug fix.

**Buggy**"
eclipseJdt,19335.json,f69c200469412ca3b93e80844c6c1f4e077169ac,"@@ -1,3 +1,3 @@
-protected OpenableElementInfo createElementInfo() {
+protected Object createElementInfo() {
 	return new OpenableElementInfo();
 }",NotBuggy,"*** empty log message ***
",NotBuggy,"The commit message is empty, which is not ideal. However, let's analyze the code diff.

The code changes the return type of the `createElementInfo` method from `OpenableElementInfo` to `Object`. This suggests a potential change in the class hierarchy or a move towards a more generic return type. Without more context, it's difficult to determine if this change is related to a bug fix. It could be a refactoring or a change in design. Since there is no context and the commit message is empty, I cannot confidently say that this is a bug fix.

**NotBuggy**
"
eclipseJdt,11399.json,f389464fbe74a4e5c3adfde8847a7200db37d80c,"@@ -1,3 +1,3 @@
 public int hash(int key) {
-	return (key & 0x7FFFFFFF) % keyTable.length;
+	return (key & 0x7FFFFFFF) % this.keyTable.length;
 }",NotBuggy,"HEAD - Use classfile pool to create class files
",Buggy,"The commit message ""Use classfile pool to create class files"" doesn't seem directly related to the code change. The code change involves modifying the `hash` function to use `this.keyTable.length` instead of `keyTable.length`. This suggests a potential bug fix where the code was incorrectly accessing the `keyTable`'s length, possibly due to scoping issues or incorrect object references. The change ensures that the hash function correctly uses the `keyTable` associated with the current object instance.

**Buggy**
"
eclipseJdt,20530.json,046b7e91db887d0087d68280c71a1c85744c622c,"@@ -1,13 +1,13 @@
 	public char[][] fullInclusionPatternChars() {
 
-		if (this.fullCharInclusionPatterns == UNINIT_PATTERNS) {
+		if (this.fullInclusionPatternChars == UNINIT_PATTERNS) {
 			int length = this.inclusionPatterns.length;
-			this.fullCharInclusionPatterns = new char[length][];
+			this.fullInclusionPatternChars = new char[length][];
 			IPath prefixPath = this.path.removeTrailingSeparator();
 			for (int i = 0; i < length; i++) {
-				this.fullCharInclusionPatterns[i] = 
+				this.fullInclusionPatternChars[i] = 
 					prefixPath.append(this.inclusionPatterns[i]).toString().toCharArray();
 			}
 		}
-		return this.fullCharInclusionPatterns;
+		return this.fullInclusionPatternChars;
 	}",NotBuggy,"*** empty log message ***
",Buggy,"The commit message is empty, which is not ideal for understanding the purpose of the change. However, by examining the diff, we can infer the intent.

The code change involves renaming `fullCharInclusionPatterns` to `fullInclusionPatternChars`. The condition `this.fullCharInclusionPatterns == UNINIT_PATTERNS` is changed to `this.fullInclusionPatternChars == UNINIT_PATTERNS`. The assignment `this.fullCharInclusionPatterns = new char[length][]` is changed to `this.fullInclusionPatternChars = new char[length][]`. Finally, the return statement changes from `this.fullCharInclusionPatterns` to `this.fullInclusionPatternChars`.

These changes strongly suggest a renaming refactoring to correct a typo or improve clarity. This kind of change is often done to fix a bug related to incorrect naming.

**Buggy**"
eclipseJdt,21417.json,f69aa90ed015a30d4053155fb28339fdbd4fbd07,"@@ -1,8 +1,5 @@
 	public void _ldiv(int pc) {
-		writeTabs();
-		buffer
-			.append(pc)
-			.append(Util.bind(""disassembler.tab"")) //$NON-NLS-1$
-			.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
+		dumpPcNumber(pc);
+		buffer.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
 		writeNewLine();
 	}",NotBuggy,"Improvement for disassembled string
",NotBuggy,"The commit message ""Improvement for disassembled string"" suggests a general enhancement rather than a specific bug fix.

The code diff modifies the `_ldiv` method. It replaces the `writeTabs()` and `Util.bind(""disassembler.tab"")` calls with a `dumpPcNumber(pc)` call. It seems like the change aims to improve the formatting or content of the disassembled string representation of the `_ldiv` instruction. There's no explicit error handling, logical correction, or exception handling improvement. The change appears to be a refactoring or enhancement of the output format.

**NotBuggy**"
eclipseJdt,26771.json,f7a77c5e8210599982f60934db59bb901b30cb8c,"@@ -1,9 +1,9 @@
-	public void buildStarting(ICompilationParticipantResult[] files, boolean isBatch){
+	public void buildStarting(BuildContext[] files, boolean isBatch){
 		// this gets called multiple times during a build.
 		// This gets called:
 		// 1) after ""aboutToBuild"" is called.
         // 2) everytime an incremental build occur because of newly generated files
         // this gets called.
 		if( _buildRound == 0 )
 			_isBatch = isBatch;
 	}",NotBuggy,"HEAD - improve compilation participant API
",NotBuggy,"The commit message ""improve compilation participant API"" suggests a general enhancement or modification of the API related to compilation participants. The code diff shows a change in the `buildStarting` method signature, specifically the type of the `files` parameter from `ICompilationParticipantResult[]` to `BuildContext[]`.

Reasoning:

1.  **API Improvement:** The change in the parameter type indicates an evolution or refinement of the API. It's likely that `BuildContext` provides more relevant or comprehensive information compared to `ICompilationParticipantResult` for the build process.
2.  **No direct bug fix indicators:** The change doesn't directly address error handling, logical errors, or exceptions. It's more about providing a better interface or data structure for compilation participants.
3.  **No error handling updates:** The diff doesn't include any changes related to error handling or exception management.
4.  **No logical corrections:** The code change doesn't seem to fix any logical errors.

Based on the analysis, the changes are related to API enhancement rather than fixing a specific bug.

**Conclusion: NotBuggy**"
eclipseJdt,15494.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(FieldDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message ""Add comment to empty blocks"" suggests that the change involves adding comments to empty code blocks.

The provided diff shows that a comment ""// default implementation: do nothing"" has been added to an empty `endVisit` method. This aligns perfectly with the commit message. The addition of a comment to an empty block doesn't inherently indicate a bug fix. It's more likely a stylistic or documentation improvement.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
eclipseJdt,16095.json,24c9e84b2ec4fea87b1bf2f444e5d541a0e3f479,"@@ -1,4 +1,4 @@
-	public boolean subtreeMatch(ASTMatcher matcher, Object other) {
+	final boolean subtreeMatch0(ASTMatcher matcher, Object other) {
 		// dispatch to correct overloaded match method
 		return matcher.match(this, other);
 	}",NotBuggy,"Reduce API footprint
",NotBuggy,"The commit message ""Reduce API footprint"" suggests an effort to minimize the public API surface of a class or component. The code diff shows a change in the method `subtreeMatch` to `subtreeMatch0`, and its visibility is likely changed to `private` or `protected` (although the diff doesn't explicitly show the visibility modifier, the renaming to include ""0"" strongly suggests this). This change aligns with the commit message because renaming a public method to include ""0"" and likely making it private reduces the API footprint. This change doesn't inherently indicate a bug fix. It's more related to API design and maintenance.

**NotBuggy**
"
eclipseJdt,19189.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,11 +1,11 @@
 private void rehash() {
-	QualifiedNameSet newSet = new QualifiedNameSet(elementSize * 2); // double the number of expected elements
+	QualifiedNameSet newSet = new QualifiedNameSet(this.elementSize * 2); // double the number of expected elements
 	char[][] current;
-	for (int i = qualifiedNames.length; --i >= 0;)
-		if ((current = qualifiedNames[i]) != null)
+	for (int i = this.qualifiedNames.length; --i >= 0;)
+		if ((current = this.qualifiedNames[i]) != null)
 			newSet.add(current);
 
 	this.qualifiedNames = newSet.qualifiedNames;
 	this.elementSize = newSet.elementSize;
 	this.threshold = newSet.threshold;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"The commit message indicates a cleanup operation, primarily focusing on code style improvements: adding `this.` to field accesses, removing `this.` from message sends (which isn't present in the diff), and removing trailing whitespace (also not visible in the diff).

The provided diff shows the addition of `this.` to field accesses like `elementSize` and `qualifiedNames`. This aligns with the commit message's intention to add `this.` to fields. There's no indication of bug fixing, error handling, or logical corrections in the code changes. The changes are purely stylistic.

**NotBuggy**"
eclipseJdt,21045.json,3b79f63e00cbebb1fe326313cd6905bd51bae92d,"@@ -1,3 +1,3 @@
 	public int getNameSourceEnd() {
-		return this.nameEnd;
+		return -1;
 	}",NotBuggy,"HEAD - 260257
",Buggy,"The commit message is missing, making it impossible to assess the alignment between the changes and the intended purpose of the commit. However, the code modification itself is straightforward: the method `getNameSourceEnd()` now always returns -1 instead of the value of `this.nameEnd`. Without knowing the context or the intended behavior, it's difficult to determine if this change is a bug fix or a new feature. It's possible that `this.nameEnd` was sometimes returning an incorrect value, and setting it to -1 is a way to signal that the name source end is not available or valid. This could be a bug fix.

**Buggy**"
commons-lang,2261.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,6 +1,6 @@
-    public static Short[] nullToEmpty(Short[] array) {
+    public static Short[] nullToEmpty(final Short[] array) {
         if (array == null || array.length == 0) {
             return EMPTY_SHORT_OBJECT_ARRAY;
         }
         return array;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the purpose of the change is to add the `final` modifier to method parameters. The diff shows that the `final` modifier has been added to the `array` parameter of the `nullToEmpty` method. This aligns with the commit message. Adding `final` to a method parameter prevents it from being reassigned within the method, which can help prevent accidental modification of the parameter. This change doesn't directly indicate a bug fix, but rather an improvement in code clarity and safety.

**NotBuggy**"
commons-lang,852.json,93a3b6b251429030ee0393a8483d2a225a7b0db2,"@@ -1,4 +1,4 @@
-    public ToStringBuilder<T> append(double value) {
+    public ToStringBuilder append(double value) {
         style.append(buffer, null, value);
         return this;
     }",NotBuggy,"Remove generics as they provide little value and get in the way

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@829635 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a refactoring to remove generics from the code. The diff shows a change in the `append` method signature from `ToStringBuilder<T>` to `ToStringBuilder`. This change aligns with the commit message's intention to remove generics. There's no indication of a bug fix in this change; it's purely a refactoring.

**NotBuggy**"
commons-lang,2074.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,3 +1,3 @@
-    public static final Processor getProcessor() {
+    public static Processor getProcessor() {
         return getProcessor(SystemUtils.OS_ARCH);
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Prevent redundant modifiers"" suggests the removal of unnecessary modifiers in the code. This could be related to code style improvements or potentially fixing a situation where redundant modifiers were causing unexpected behavior (though less likely).

**Git Diff Analysis:**

The diff shows the following change:

`-    public static final Processor getProcessor() {`
`+    public static Processor getProcessor() {`

The `final` modifier has been removed from the `getProcessor()` method declaration.

**Reasoning:**

1.  **Redundant Modifier:** The `final` modifier on a method prevents it from being overridden in subclasses. In this case, the removal of `final` suggests that it was deemed unnecessary. It's possible that the method wasn't intended to be overridden, but the `final` modifier wasn't strictly required, or perhaps a design change made it acceptable for the method to be overridden in the future.

2.  **Bug Fix Indication:** While removing a redundant modifier can improve code clarity and maintainability, it doesn't inherently indicate a bug fix. It's more likely a code cleanup or refactoring change. There's no error handling, logical correction, or exception handling improvement in the diff. The change is about removing a modifier that was deemed unnecessary.

**Conclusion:**

The change is about removing a redundant modifier, which is more of a code cleanup or refactoring task rather than a bug fix.

**NotBuggy**
"
commons-lang,1759.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,10 +1,10 @@
-    public static String reverseDelimited(String str, char separatorChar) {
+    public static String reverseDelimited(final String str, final char separatorChar) {
         if (str == null) {
             return null;
         }
         // could implement manually, but simple way is to reuse other,
         // probably slower, methods.
         String[] strs = split(str, separatorChar);
         ArrayUtils.reverse(strs);
         return join(strs, separatorChar);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the purpose of the change is to add the `final` modifier to method parameters.

The diff shows that the `final` modifier has been added to the `str` and `separatorChar` parameters of the `reverseDelimited` method.

The changes align with the commit message. Adding `final` to method parameters is generally a good practice that prevents accidental modification of the parameters within the method. This change does not seem to be related to fixing a bug, but rather to improving code quality and preventing potential future bugs.

**NotBuggy**"
commons-lang,772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static boolean isPrimitiveWrapper(Class<?> type) {
+    public static boolean isPrimitiveWrapper(final Class<?> type) {
         return wrapperPrimitiveMap.containsKey(type);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Add final modifier to method parameters"" indicates a change that adds the `final` keyword to method parameters.

The provided diff shows that the `final` keyword has been added to the `type` parameter of the `isPrimitiveWrapper` method.

The change aligns perfectly with the commit message. Adding `final` to method parameters is a common practice to prevent accidental modification of the parameter within the method and doesn't inherently indicate a bug fix.

**NotBuggy**"
commons-lang,1787.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String strip(String str) {
+    public static String strip(final String str) {
         return strip(str, null);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change to add the `final` modifier to method parameters. The diff shows that the `final` modifier was added to the `str` parameter of the `strip` method. This aligns with the commit message. The addition of `final` doesn't inherently indicate a bug fix. It's more of a code style or defensive programming practice to prevent accidental modification of the parameter within the method.

**NotBuggy**"
commons-lang,1773.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator( String str, String separator, int max ) {
+    public static String[] splitByWholeSeparator( final String str, final String separator, final int max ) {
         return splitByWholeSeparatorWorker(str, separator, max, false);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the purpose of the commit is to add the `final` modifier to method parameters. The diff shows that the `final` modifier has indeed been added to the `str`, `separator`, and `max` parameters of the `splitByWholeSeparator` method. This change does not seem to be related to fixing a bug. It is more likely a stylistic or code quality improvement.

**NotBuggy**"
commons-lang,1672.json,96c30e248d6d27a1df8e26be14aa10c633168cdb,"@@ -1,3 +1,3 @@
-    public static boolean isNoneBlank(CharSequence... css) {
+    public static boolean isNoneBlank(final CharSequence... css) {
       return !isAnyBlank(css);
     }",NotBuggy,"Use final consistently. 

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606051 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Use final consistently"" suggests an effort to improve code quality by adding the `final` keyword where appropriate. The diff shows the addition of the `final` keyword to the `css` parameter in the `isNoneBlank` method. This change doesn't inherently indicate a bug fix. It's more of a stylistic or preventative measure to ensure the parameter is not modified within the method, which aligns perfectly with the commit message.

**NotBuggy**"
commons-lang,186.json,dd74cf1da57ed432ce38631baebe6882419b01e6,"@@ -1,19 +1,18 @@
     public static int distance(final Class<?> child, final Class<?> parent) {
         if (child == null || parent == null) {
             return -1;
         }
 
         if (child.equals(parent)) {
             return 0;
         }
 
         final Class<?> cParent = child.getSuperclass();
         int d = BooleanUtils.toInteger(parent.equals(cParent));
 
         if (d == 1) {
             return d;
-        } else {
-            d += distance(cParent, parent);
-            return d > 0 ? d + 1 : -1;
         }
+        d += distance(cParent, parent);
+        return d > 0 ? d + 1 : -1;
     }",NotBuggy,"Statement unnecessarily nested within else clause.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that a statement was unnecessarily nested within an `else` clause. The diff shows that the `else` block has been removed, and the statement previously inside it is now executed unconditionally after the `if` block. This refactoring suggests that the original code might have had a logical flaw where the statement within the `else` block was not always executed when it should have been. The removal of the unnecessary `else` clause corrects this, indicating a bug fix.

**Buggy**"
commons-lang,2630.json,5292526e476ffbb19c6613a98464054236c86ace,"@@ -1,8 +1,8 @@
         public Character next() {
             if (hasNext == false) {
                 throw new NoSuchElementException();
             }
-            char cur = current;
+            final char cur = current;
             prepareNext();
             return Character.valueOf(cur);
         }",NotBuggy,"Add final modifier to local variables.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436770 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Add final modifier to local variables"" indicates a change that focuses on code style and immutability rather than fixing a bug. The diff shows the addition of the `final` keyword to the `cur` variable within the `next()` method. This change prevents the variable from being reassigned after its initial assignment, which can improve code readability and prevent unintended modifications. There is no indication of error handling, logical corrections, or exception handling improvements. Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
commons-lang,136.json,c79f82491a8bae9c3b9895f296a8519cd0dfe7ff,"@@ -1,15 +1,19 @@
     public static void removeFinalModifier(Field field) {
         Validate.isTrue(field != null, ""The field must not be null"");
 
         try {
             if (Modifier.isFinal(field.getModifiers())) {
                 Field modifiersField = Field.class.getDeclaredField(""modifiers"");
                 modifiersField.setAccessible(true);
-                modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                try {
+                    modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                } finally {
+                    modifiersField.setAccessible(false);
+                }
             }
         } catch (NoSuchFieldException ignored) {
             // The field class contains always a modifiers field
         } catch (IllegalAccessException ignored) {
              // The modifiers field is made accessible
         }
     }",NotBuggy,"removeFinalModifier does not clean up after itself.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1562985 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the `removeFinalModifier` method was not cleaning up after itself. Looking at the diff, the change involves adding a `finally` block to ensure that `modifiersField.setAccessible(false)` is always called, even if an exception occurs within the `try` block where the final modifier is removed. This suggests that the original code could leave the `modifiersField` accessible, potentially leading to unintended consequences later on. The addition of the `finally` block ensures that the accessibility flag is reset, thus addressing the cleanup issue described in the commit message. This is a bug fix because the original code had a resource leak (the accessible flag) that was not being properly managed.

**Buggy**"
commons-lang,1942.json,3728344459e91509c4b5b80d67a50c98063e08bd,"@@ -1,14 +1,14 @@
-    public static short max(final short[] array) {
+    public static short max(final short... array) {
         // Validates input
         validateArray(array);
     
         // Finds and returns max
         short max = array[0];
         for (int i = 1; i < array.length; i++) {
             if (array[i] > max) {
                 max = array[i];
             }
         }
     
         return max;
     }",NotBuggy,"LANG-1008: Change min/max methods in NumberUtils/IEEE754rUtils from array input parameters to varargs. This also closes #23 from github. Thanks to Thiago Andrade.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1593118 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change in the `min` and `max` methods in `NumberUtils` and `IEEE754rUtils` from array input parameters to varargs. The provided diff shows the change in the `max` method for short arrays, specifically changing the parameter type from `short[] array` to `short... array`. This aligns perfectly with the commit message. The change itself doesn't inherently indicate a bug fix, but rather a change in the method signature to allow for more flexible usage.

**NotBuggy**"
commons-lang,2067.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,4 +1,4 @@
-    private static final void init_X86_64Bit() {
+    private static void init_X86_64Bit() {
         Processor processor = new Processor(Processor.Arch.BIT_64, Processor.Type.X86);
         addProcessors(processor, ""x86_64"", ""amd64"", ""em64t"", ""universal"");
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"The commit message ""Prevent redundant modifiers"" suggests the removal of unnecessary modifiers in the code.

The diff shows that the `private static final void init_X86_64Bit()` method has been changed to `private static void init_X86_64Bit()`. The `final` modifier has been removed.

The removal of the `final` modifier aligns with the commit message, indicating that the modifier was redundant. This change doesn't inherently indicate a bug fix, but rather a code cleanup or optimization.

**NotBuggy**"
commons-lang,1223.json,9072a7615a54d5e753cf088326d9e14f1a5a6751,"@@ -1,7 +1,7 @@
     public EqualsBuilder append(final int lhs, final int rhs) {
         if (isEquals == false) {
             return this;
         }
-        isEquals = (lhs == rhs);
+        isEquals = lhs == rhs;
         return this;
     }",NotBuggy,"PMD: Useless parentheses

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1671669 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""PMD: Useless parentheses"" suggests the commit aims to remove unnecessary parentheses in the code.

The diff shows a change in the `append(int lhs, int rhs)` method of the `EqualsBuilder` class. The change involves removing parentheses around the expression `lhs == rhs`.

The removal of parentheses doesn't change the logic or functionality of the code. It simply improves readability by removing unnecessary characters. This aligns with the commit message, which indicates a style improvement rather than a bug fix. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**"
commons-lang,1772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator(String str, String separator) {
+    public static String[] splitByWholeSeparator(final String str, final String separator) {
         return splitByWholeSeparatorWorker( str, separator, -1, false ) ;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the purpose of the change is to add the `final` modifier to method parameters. The diff shows that the `final` modifier has been added to the `str` and `separator` parameters of the `splitByWholeSeparator` method. This aligns perfectly with the commit message. There is no indication of bug fixing.
**NotBuggy**"
elasticsearch,45328.json,bd7efa908ab69831757d45ca7519c286097777dc,"@@ -1,20 +1,20 @@
-    static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
+    public static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
         final String effectiveRouting;
         final int partitionOffset;
 
         if (routing == null) {
             assert(indexMetaData.isRoutingPartitionedIndex() == false) : ""A routing value is required for gets from a partitioned index"";
             effectiveRouting = id;
         } else {
             effectiveRouting = routing;
         }
 
         if (indexMetaData.isRoutingPartitionedIndex()) {
             partitionOffset = Math.floorMod(Murmur3HashFunction.hash(id), indexMetaData.getRoutingPartitionSize());
         } else {
             // we would have still got 0 above but this check just saves us an unnecessary hash calculation
             partitionOffset = 0;
         }
 
         return calculateScaledShardId(indexMetaData, effectiveRouting, partitionOffset);
     }",NotBuggy,"Add ability to split shards (#26931)

This change adds a new `_split` API that allows to split indices into a new
index with a power of two more shards that the source index.  This API works
alongside the `_shrink` API but doesn't require any shard relocation before
indices can be split.

The split operation is conceptually an inverse `_shrink` operation since we
initialize the index with a _syntetic_ number of routing shards that are used
for the consistent hashing at index time. Compared to indices created with
earlier versions this might produce slightly different shard distributions but
has no impact on the per-index backwards compatibility.  For now, the user is
required to prepare an index to be splittable by setting the
`index.number_of_routing_shards` at index creation time.  The setting allows the
user to prepare the index to be splittable in factors of
`index.number_of_routing_shards` ie. if the index is created with
`index.number_of_routing_shards: 16` and `index.number_of_shards: 2` it can be
split into `4, 8, 16` shards. This is an intermediate step until we can make
this the default. This also allows us to safely backport this change to 6.x.

The `_split` operation is implemented internally as a DeleteByQuery on the
lucene level that is executed while the primary shards execute their initial
recovery. Subsequent merges that are triggered due to this operation will not be
executed immediately. All merges will be deferred unti the shards are started
and will then be throttled accordingly.

This change is intended for the 6.1 feature release but will not support pre-6.1
indices to be split unless these indices have been shrunk before. In that case
these indices can be split backwards into their original number of shards.
",NotBuggy,"The commit message describes the addition of a new `_split` API for splitting indices into more shards. The provided code diff modifies the `generateShardId` method, making it public and adding a `static` keyword. While the commit message describes a new feature, the code change itself doesn't seem directly related to the core splitting logic. The change in access modifier and addition of `static` might be related to making the shard ID generation accessible from the new splitting API, but without more context, it's hard to say definitively if this change is a bug fix or just a utility function modification. The code change itself doesn't obviously indicate a bug fix.

**NotBuggy**
"
elasticsearch,38113.json,85c26d682a5a6fbaf345d66b9a783b78f4b1cc63,"@@ -1,19 +1,19 @@
             public boolean advanceExact(int doc) throws IOException {
                 if (bytesValues.advanceExact(doc)) {
                     count = bytesValues.docValueCount();
                     grow();
                     for (int i = 0; i < count; ++i) {
                         final BytesRef value = bytesValues.nextValue();
                         script.setNextAggregationValue(value.utf8ToString());
                         Object run = script.run();
-                        CollectionUtils.ensureNoSelfReferences(run);
+                        CollectionUtils.ensureNoSelfReferences(run, ""ValuesSource.BytesValues script"");
                         values[i].copyChars(run.toString());
                     }
                     sort();
                     return true;
                 } else {
                     count = 0;
                     grow();
                     return false;
                 }
             }",NotBuggy,"Call ensureNoSelfReferences() on _agg state variable after scripted metric agg script executions (#31044)

Previously this was called for the combine script only. This change checks for self references for
init, map, and reduce scripts as well, and adds unit test coverage for the init, map, and combine cases.

",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states that `ensureNoSelfReferences()` was previously only called for the `combine` script in scripted metric aggregations. The change extends this check to `init`, `map`, and `reduce` scripts as well. The message also mentions adding unit tests for `init`, `map`, and `combine` cases. This strongly suggests a potential bug where self-references in `init`, `map`, or `reduce` scripts were not being detected, leading to issues (e.g., infinite loops, stack overflows) during aggregation.

**Git Diff Analysis:**

The diff shows a modification within the `advanceExact` method. Specifically, the line:

```java
CollectionUtils.ensureNoSelfReferences(run);
```

has been changed to:

```java
CollectionUtils.ensureNoSelfReferences(run, ""ValuesSource.BytesValues script"");
```

This change adds a description to the `ensureNoSelfReferences` call. While the addition of a description itself isn't indicative of a bug fix, the context is crucial. The commit message indicates that this same check is being added to other script types (`init`, `map`, and `reduce` scripts) elsewhere in the codebase.  The fact that `ensureNoSelfReferences` is being called at all suggests a potential for self-referential data structures to be created within the scripts, which could lead to problems. The original code was missing this check for certain script types, which is now being addressed.

**Reasoning:**

The commit message explicitly states that the change addresses a missing check for self-references in certain scripted metric aggregation scripts. The Git diff confirms that a self-reference check is being added (or modified to include a description) in the code. The addition of unit tests further supports the idea that this change is intended to prevent a specific type of error related to self-referential data structures.  The original code's omission of this check for `init`, `map`, and `reduce` scripts constitutes a bug.

**Conclusion:**

**Buggy**
"
elasticsearch,38632.json,48b0908fc62a4047c8b254a3c3b52c973019561a,"@@ -1,3 +1,3 @@
-    Comparable<?>[] values() {
+    Comparable[] values() {
         return values;
     }",NotBuggy,"Make InternalComposite key comparable

Keys are compared in BucketSortPipelineAggregation so making key type (ArrayMap) implement Comparable. Maps are compared using the entry set's iterator so ordered maps order is maintain. For each entry first comparing key then value. Assuming all keys are strings. When comparing entries' values if type is not identical and\or type not implementing Comparable, throwing exception. Not implementing equals() and hashCode() functions as parent's ones are sufficient. Tests included.",Buggy,"The commit message indicates a change to make `InternalComposite` keys comparable, specifically for use in `BucketSortPipelineAggregation`. The message details the implementation strategy, including how maps are compared, the assumption that keys are strings, and how non-comparable values are handled. It also mentions the inclusion of tests.

The provided diff shows a change from `Comparable<?>[] values()` to `Comparable[] values()`. This change seems related to making the `InternalComposite` key comparable, as it refines the type of the `values` array to be specifically an array of `Comparable` objects, rather than an array of objects that *might* be comparable. This is a necessary step to ensure that the values within the composite key can be reliably compared.

The commit message describes a specific scenario (`BucketSortPipelineAggregation`) where comparable keys are needed. The code change directly supports this requirement by ensuring the key values are `Comparable`. This suggests the original code had a limitation or potential bug related to key comparison in that scenario. The change enforces that the values are comparable, which would address the bug.

Therefore, the changes indicate a bug fix.

**Buggy**
"
elasticsearch,12197.json,9e8cfbb40d7596eb56151fc8a30234fb336c0a43,"@@ -1,3 +1,3 @@
     public String getName() {
-        return ""xpack_ml_get_overall_buckets_action"";
+        return ""ml_get_overall_buckets_action"";
     }",NotBuggy,"[ML] Deprecate X-Pack centric ML endpoints (#36315)

This commit is part of our plan to deprecate and
ultimately remove the use of _xpack in the REST APIs.

Relates #35958",NotBuggy,"The commit message indicates a deprecation and removal of `_xpack` from REST APIs, specifically mentioning the replacement of `xpack` with `ml`. The provided code diff shows a change from `""xpack_ml_get_overall_buckets_action""` to `""ml_get_overall_buckets_action""`. This change directly reflects the intention stated in the commit message. There is no indication of a bug fix in the traditional sense (error handling, logical correction, exception handling). The change is a systematic renaming to remove the `xpack` prefix.

**NotBuggy**"
elasticsearch,39219.json,7bd2abe48af5a651b54bcd5bcb41c88e29390be0,"@@ -1,3 +1,3 @@
-    protected List<Bucket> getBucketsInternal() {
+    public List<Bucket> getBuckets() {
         return emptyList();
     }",NotBuggy,"Change Terms.Bucket to an interface (#24492)

This commit changes the Terms.Bucket abstract class to an interface, so
that it's easier for the Java High Level Rest Client to provide its own
implementation.

In its current state, the Terms.Bucket abstract class inherits from
InternalMultiBucketAggregation.InternalBucket which forces subclasses to
implement Writeable and exposes a public getProperty() method that relies
on InternalAggregation. This two points make it difficult for the Java
High Level Rest Client to implement the Terms and Terms.Bucket correctly.
This is also different from other MultiBucketsAggregation like Range
which are pure interfaces.

Changing Terms.Bucket to an interface causes a method clashes for the
`getBuckets()` method in InternalTerms. This is because:
 - InternalTerms implements Terms which declared a
 `List<Terms.Bucket> getBuckets()` method
 - InternalTerms extends InternalMultiBucketAggregation which declares a
 `List<? extends InternalBucket> getBuckets()` method
 - both overrides the MultiBucketsAggregation
 `List<? extends Bucket> getBuckets()` method

 There was no clashes before this change because Terms.Bucket extends
 InternalBucket and conformed to both declaration. With Terms.Bucket now
 an interface, the getBuckets() method in the Terms interface is changed
 to avoid method clash. This is a breaking change in the Java API but
 it's a straightforward change and the Terms multi bucket aggregation
 interface is also more coherent with the other Range, Histogram,
 Filters, AdjacencyMatrix etc that all return a `List<? extends Bucket>`.",Buggy,"The commit message indicates a refactoring effort to change `Terms.Bucket` from an abstract class to an interface to improve the Java High Level Rest Client's ability to provide its own implementation. The message highlights the difficulties in the current design due to inheritance from `InternalMultiBucketAggregation.InternalBucket` and the exposure of a public `getProperty()` method. It also mentions a method clash issue with `getBuckets()` in `InternalTerms` and explains how it was resolved by modifying the `getBuckets()` method in the `Terms` interface.

The provided diff snippet shows a change in a method signature from `protected List<Bucket> getBucketsInternal()` to `public List<Bucket> getBuckets()`. While the diff itself doesn't scream ""bug fix"", the commit message provides context. The change in access modifier (protected to public) and the removal of ""Internal"" from the method name suggest this is part of the refactoring to align with the interface change and resolve the method clash issue. The commit message explicitly states that this is a breaking change in the Java API.

Given the context from the commit message, the code change is part of a larger refactoring effort that addresses design issues that made it difficult for clients to implement the `Terms` aggregation correctly. While not a direct bug fix in the traditional sense, the refactoring aims to resolve underlying design problems that could lead to incorrect or difficult implementations, which can be considered a form of bug prevention.

**Buggy**
"
elasticsearch,61546.json,b70f925ccb735dc84d59598de06df6bf35bd4bdc,"@@ -1,3 +1,3 @@
-    Configuration getConfiguration() {
-        return configuration;
+    public String getMajor() {
+        return major;
     }",NotBuggy,"Support downloading JDKs with legacy version format (#51587)

",NotBuggy,"The commit message ""Support downloading JDKs with legacy version format"" suggests that the changes are related to handling older or non-standard JDK version formats. The provided code diff only shows a getter method being renamed from `getConfiguration()` to `getMajor()`. This change, by itself, doesn't provide enough context to determine if it's directly related to handling legacy version formats or fixing a bug. It could be part of a larger refactoring or a change in how the major version is accessed. Without more context, it's difficult to definitively say whether this change is a bug fix.

**NotBuggy**"
elasticsearch,39443.json,a0b004acb5965508782f5430c240f71580478832,"@@ -1,4 +1,4 @@
-    public IPv4RangeAggregatorFactory addRange(String key, String from, String to) {
+    public IPv4RangeAggregatorBuilder addRange(String key, String from, String to) {
         addRange(new Range(key, from, to));
         return this;
     }",NotBuggy,"Split AggregatorFactory into AggregatorBuilder and AggregatorFactory
",NotBuggy,"The commit message ""Split AggregatorFactory into AggregatorBuilder and AggregatorFactory"" suggests a refactoring or restructuring of the code related to `AggregatorFactory`. The provided diff shows a change in the return type of the `addRange` method from `IPv4RangeAggregatorFactory` to `IPv4RangeAggregatorBuilder`. This change aligns with the commit message, indicating a split or modification of the `AggregatorFactory` class. The change itself doesn't directly indicate a bug fix, but rather a structural modification. There is no indication of error handling, logical corrections, or exception handling improvements.

**NotBuggy**
"
elasticsearch,8046.json,2879e6717eedd6caad60e4fca3991ed4a9faad38,"@@ -1,11 +1,15 @@
     private static BytesReference filterSource(FetchSourceContext fetchSourceContext, BytesReference source) throws IOException {
+        if (fetchSourceContext.includes().length == 0 && fetchSourceContext.excludes().length == 0) {
+            return source;
+        }
+
         Set<String> includes = Set.of(fetchSourceContext.includes());
         Set<String> excludes = Set.of(fetchSourceContext.excludes());
 
         XContentBuilder builder =
             new XContentBuilder(XContentType.SMILE.xContent(), new BytesStreamOutput(source.length()), includes, excludes);
         XContentParser sourceParser = XContentHelper.createParser(NamedXContentRegistry.EMPTY,
             DeprecationHandler.THROW_UNSUPPORTED_OPERATION, source, XContentType.SMILE);
         builder.copyCurrentStructure(sourceParser);
         return BytesReference.bytes(builder);
     }",NotBuggy," Enrich processor configuration changes (#45466)

Enrich processor configuration changes:
* Renamed `enrich_key` option to `field` option.
* Replaced `set_from` and `targets` options with `target_field`.

The `target_field` option behaves different to how `set_from` and
`targets` worked. The `target_field` is the field that will contain
the looked up document.

Relates to #32789",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Enrich processor configuration changes"" describes a refactoring of the enrich processor's configuration options. Specifically, it mentions:

1.  Renaming `enrich_key` to `field`.
2.  Replacing `set_from` and `targets` with `target_field`.
3.  Explaining the new behavior of `target_field`.

The commit message suggests a change in the API or configuration of the enrich processor, likely to improve usability or consistency. The phrase ""Relates to #32789"" indicates a related issue or feature request that motivated these changes.

**Git Diff Analysis:**

The provided diff shows a modification to the `filterSource` method. The added code checks if both `includes` and `excludes` are empty in `fetchSourceContext`. If they are, the original `source` is returned directly.

**Reasoning:**

The commit message describes a refactoring of the enrich processor's configuration. The diff modifies how the source is filtered based on `fetchSourceContext`. The diff introduces a check to avoid unnecessary filtering when both includes and excludes are empty. This optimization could be related to a bug where filtering was performed even when it wasn't needed, potentially causing performance issues or incorrect results. The change in the diff seems to be an optimization to avoid unnecessary processing when no filtering is required. This could be a fix for a performance issue or an incorrect behavior when no includes or excludes are specified.

**Conclusion:**

The changes in the diff, combined with the commit message suggesting a refactoring and potential bug fix related to filtering, indicate that this commit likely addresses a bug or an inefficiency.

**Buggy**
"
elasticsearch,63479.json,df2acb3d9d81118e5dcf64979e0fa716beec8164,"@@ -1,10 +1,10 @@
     protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
                             ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, MULTIVALUE_MODE_FIELD)) {
+        if (MULTIVALUE_MODE_FIELD.match(currentFieldName)) {
             if (token == XContentParser.Token.VALUE_STRING) {
                 otherOptions.put(MULTIVALUE_MODE_FIELD, parser.text());
                 return true;
             }
         }
         return false;
     }",NotBuggy,"Remove some more usages of ParseFieldMatcher in favour of using ParseField directly

Relates to #19552
Relates to #22130
",NotBuggy,"The commit message indicates a refactoring effort to replace `ParseFieldMatcher` with `ParseField` directly. The diff shows that `parseFieldMatcher.match(currentFieldName, MULTIVALUE_MODE_FIELD)` is replaced by `MULTIVALUE_MODE_FIELD.match(currentFieldName)`. This change aligns with the commit message, indicating a direct substitution of one method call with another. There's no indication of bug fixing, error handling, or logical correction in this specific diff. It's purely a refactoring change as described in the commit message.

**NotBuggy**"
elasticsearch,26493.json,78f9af19c668886c682d5d1d74f5fb6f2a4c5e77,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(accessToken, refreshToken);
+        return Objects.hash(accessToken, refreshToken, realmName, username);
     }",NotBuggy,"Invalidate Token API enhancements - HLRC (#36362)

* Adds Invalidate Token API enhancements to HLRC

Relates: #35388
",NotBuggy,"The commit message ""Invalidate Token API enhancements - HLRC (#36362)"" indicates enhancements to the Invalidate Token API within the High-Level REST Client (HLRC). The provided code diff modifies the `hashCode` method of a class, adding `realmName` and `username` to the calculation.

Reasoning:

1.  **Commit Message Alignment:** The commit message mentions ""enhancements"" to the Invalidate Token API. Adding `realmName` and `username` to the `hashCode` method suggests that these fields are now relevant to the token invalidation process. This aligns with the ""enhancements"" mentioned in the commit message.
2.  **Bug Fix Indication:** The change to the `hashCode` method doesn't directly indicate a bug fix. It's more likely related to ensuring proper object comparison and identification when invalidating tokens, especially if `realmName` and `username` are now part of the token identification criteria. Without more context, it's hard to say if the previous `hashCode` implementation was buggy. It's possible that the previous implementation was insufficient for the enhanced API, but it wasn't necessarily incorrect.
3.  **Error Handling, Logical Corrections, Exception Handling:** The diff doesn't show any changes related to error handling, logical corrections, or exception handling.

Conclusion:

**NotBuggy**
"
elasticsearch,25327.json,92eb324776f26e0edf8b7957c81950890b8ce4d9,"@@ -1,9 +1,9 @@
     public CancelTasksResponse cancel(CancelTasksRequest cancelTasksRequest, RequestOptions options ) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(
             cancelTasksRequest,
             RequestConverters::cancelTasks,
             options,
-            parser -> CancelTasksResponse.fromXContent(parser),
+            CancelTasksResponse::fromXContent,
             emptySet()
         );
     }",NotBuggy,"REST high-level Client: remove deprecated API methods (#31200)

This commit removes all the API methods that accept a `Header` varargs
argument, in favour of the newly introduced API methods that accept a
`RequestOptions` argument.

Relates to #31069",NotBuggy,"Okay, let's analyze this Git diff and commit message.

The commit message states that deprecated API methods accepting `Header` varargs are being removed in favor of methods accepting `RequestOptions`. The provided diff shows a change in the `cancel` method. Specifically, it modifies how the `CancelTasksResponse` is parsed from the XContent. Instead of using a lambda `parser -> CancelTasksResponse.fromXContent(parser)`, it now directly uses a method reference `CancelTasksResponse::fromXContent`.

Reasoning:

1. **Commit Message Alignment:** The commit message focuses on removing deprecated APIs and using `RequestOptions`. The diff *doesn't* directly show the removal of a method with `Header` varargs. However, it *does* show a change related to how the response is parsed when using `RequestOptions`. This could be a consequence of the API change, where the response parsing needed to be adjusted for the new `RequestOptions` approach.

2. **Bug Fix Indication:** The change from a lambda to a method reference is primarily a code style improvement and doesn't inherently suggest a bug fix. It's more about making the code cleaner and potentially slightly more efficient.  There's no explicit error handling, logical correction, or exception handling improvement in this specific snippet.

3. **Context from Related Issue:** The commit message mentions ""Relates to #31069"". Without seeing that issue, it's hard to be 100% certain. However, based on the information at hand, the diff itself doesn't scream ""bug fix"". It seems more like a refactoring or adaptation related to the API change.

Conclusion:

**NotBuggy**
"
elasticsearch,44847.json,690136327fe2b08c14dcf7bcd140ceb6f10ae96f,"@@ -1,7 +1,6 @@
         public synchronized void reset() {
             super.reset();
-            version = UNKNOWN;
             fileDetails.clear();
             sourceThrottlingInNanos = UNKNOWN;
             targetThrottleTimeInNanos = UNKNOWN;
         }",NotBuggy,"Cleanup Deadcode o.e.indices (#44931)

* none of this is used anywhere",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Cleanup Deadcode o.e.indices (#44931)"" clearly states the intention of removing dead code within the `o.e.indices` package.  ""Dead code"" implies code that is no longer used or reachable, and removing it is generally a good practice for code maintainability and reducing potential confusion.

**Git Diff Analysis:**

The diff shows a modification within a `reset()` method. Specifically, the line `version = UNKNOWN;` is being removed. The comment ""* none of this is used anywhere"" strongly suggests that the removed line represents dead code.

**Reasoning:**

1.  **Alignment:** The commit message and the code change are perfectly aligned. The commit message states the removal of dead code, and the diff shows the removal of a line of code that is claimed to be unused.
2.  **Bug Fix Indication:** While removing dead code improves maintainability, it doesn't directly fix a bug in the sense of correcting incorrect behavior. Dead code, by definition, doesn't affect the program's execution because it's never reached. Removing it is more of a cleanup or refactoring task.

**Conclusion:**

The changes indicate a cleanup of dead code, which improves code quality but doesn't directly address a bug.

**NotBuggy**
"
elasticsearch,24653.json,28bb1a3ac0c529beb38cd3553b50e095a07df13b,"@@ -1,5 +1,5 @@
     public final GraphExploreResponse explore(GraphExploreRequest graphExploreRequest,
                                                              RequestOptions options) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(graphExploreRequest, GraphRequestConverters::explore,
-                options, GraphExploreResponse::fromXContext, emptySet());
+                options, GraphExploreResponse::fromXContent, emptySet());
     }",NotBuggy,"XPack/HLRC request/response compatibility tests (#34547)


Relates #34451 , #29827
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** ""XPack/HLRC request/response compatibility tests (#34547)"" suggests the commit is related to compatibility testing of the High-Level REST Client (HLRC) for X-Pack features. It also mentions related issues #34451 and #29827.

2.  **Git Diff:** The diff shows a single line change within the `explore` method. The change is from `GraphExploreResponse::fromXContext` to `GraphExploreResponse::fromXContent`. This indicates a change in how the `GraphExploreResponse` is parsed from the response. The method name change suggests a potential update to align with a change in the underlying data format or structure. `fromXContext` might have been deprecated or replaced by `fromXContent`.

3.  **Reasoning:**
    *   The change from `fromXContext` to `fromXContent` strongly suggests an adaptation to a change in the Elasticsearch API or data format. The older method might have been deprecated or removed, necessitating the update in the HLRC to maintain compatibility.
    *   The commit message mentioning ""compatibility tests"" supports the idea that this change is to ensure the HLRC continues to work correctly with a specific version of Elasticsearch.
    *   It's plausible that the original `fromXContext` was causing errors or failing to parse the response correctly, which would constitute a bug. The change to `fromXContent` would then be a fix.

**Conclusion:**

Based on the analysis, the change appears to be a correction to ensure compatibility with a specific Elasticsearch API or data format. The original method might have been causing errors or failing to parse the response correctly, which would constitute a bug.

**Buggy**"
elasticsearch,55784.json,7e3cd6a01931a95bfad785490f3319ed79f258f4,"@@ -1,5 +1,17 @@
-    public Status getStatus() {
-        return new Status(sliceId, total.get(), updated.get(), created.get(), deleted.get(), batch.get(), versionConflicts.get(),
-                noops.get(), bulkRetries.get(), searchRetries.get(), timeValueNanos(throttledNanos.get()), getRequestsPerSecond(),
-                getReasonCancelled(), throttledUntil());
+    public BulkByScrollTask.Status getStatus() {
+        return new BulkByScrollTask.Status(
+            sliceId,
+            total.get(),
+            updated.get(),
+            created.get(),
+            deleted.get(),
+            batch.get(),
+            versionConflicts.get(),
+            noops.get(),
+            bulkRetries.get(),
+            searchRetries.get(),
+            timeValueNanos(throttledNanos.get()),
+            getRequestsPerSecond(),
+            task.getReasonCancelled(),
+            throttledUntil());
     }",NotBuggy,"reindex: automatically choose the number of slices (#26030)

In reindex APIs, when using the `slices` parameter to choose the number of slices, adds the option to specify `slices` as ""auto"" which will choose a reasonable number of slices. It uses the number of shards in the source index, up to a ceiling. If there is more than one source index, it uses the smallest number of shards among them.

This gives users an easy way to use slicing in these APIs without having to make decisions about how to configure it, as it provides a good-enough configuration for them out of the box. This may become the default behavior for these APIs in the future.
",NotBuggy,"The commit message describes a new feature for reindex APIs where the number of slices can be automatically determined. The diff shows a change in the return type of the `getStatus()` method from `Status` to `BulkByScrollTask.Status`, and also a change in how the reason cancelled is retrieved (from `getReasonCancelled()` to `task.getReasonCancelled()`).

The change in return type suggests that the `getStatus()` method is now associated with a `BulkByScrollTask` which aligns with the reindex functionality. The change in how the reason cancelled is retrieved also seems related to the task-based nature of the reindex operation.

These changes do not obviously indicate a bug fix. They seem to be part of the new feature implementation.

**NotBuggy**
"
elasticsearch,33161.json,d8b4556e2dfa48b38baf614e3ac99196be07055c,"@@ -1,39 +1,12 @@
             public Scorer scorer(LeafReaderContext context) throws IOException {
                 Scorer subQueryScorer = subQueryWeight.scorer(context);
                 if (subQueryScorer == null) {
                     return null;
                 }
-                final LeafScoreFunction leafFunction = function.getLeafScoreFunction(context);
-                Scorer scriptScorer = new Scorer(this) {
-                    @Override
-                    public float score() throws IOException {
-                        int docId = docID();
-                        float subQueryScore = subQueryScoreMode == ScoreMode.COMPLETE ? subQueryScorer.score() : 0f;
-                        float score = (float) leafFunction.score(docId, subQueryScore);
-                        if (score == Float.NEGATIVE_INFINITY || Float.isNaN(score)) {
-                            throw new ElasticsearchException(
-                                ""script score query returned an invalid score: "" + score + "" for doc: "" + docId);
-                        }
-                        return score;
-                    }
-                    @Override
-                    public int docID() {
-                        return subQueryScorer.docID();
-                    }
-
-                    @Override
-                    public DocIdSetIterator iterator() {
-                        return subQueryScorer.iterator();
-                    }
-
-                    @Override
-                    public float getMaxScore(int upTo) {
-                        return Float.MAX_VALUE; // TODO: what would be a good upper bound?
-                    }
-                };
+                Scorer scriptScorer = makeScriptScorer(subQueryScorer, context, null);
 
                 if (minScore != null) {
                     scriptScorer = new MinScoreScorer(this, scriptScorer, minScore);
                 }
                 return scriptScorer;
             }",NotBuggy,"Add explanations to script score queries (#46693)

While function scores using scripts do allow explanations, they are only
creatable with an expert plugin. This commit improves the situation for
the newer script score query by adding the ability to set the
explanation from the script itself.

To set the explanation, a user would check for `explanation != null` to
indicate an explanation is needed, and then call
`explanation.set(""some description"")`.
",NotBuggy,"The commit message indicates an enhancement to the script score query by allowing users to set explanations from the script itself. The diff removes a large chunk of code related to `Scorer` and replaces it with a call to `makeScriptScorer`. This suggests a refactoring or abstraction of the scoring logic, likely to incorporate the new explanation functionality. The commit message and code changes align, indicating the addition of a feature rather than a bug fix.

**NotBuggy**"
checkstyle,1745.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,11 +1,11 @@
-    private static String getInstanceName(DetailAST aMethodCallAst)
+    private static String getInstanceName(DetailAST methodCallAst)
     {
         final String methodCallName =
-                FullIdent.createFullIdentBelow(aMethodCallAst).getText();
+                FullIdent.createFullIdentBelow(methodCallAst).getText();
         final int lastDotIndex = methodCallName.lastIndexOf('.');
         String instanceName = """";
         if (lastDotIndex != -1) {
             instanceName = methodCallName.substring(0, lastDotIndex);
         }
         return instanceName;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The commit message ""Prefixes, coding, #512"" is quite vague. Let's analyze the code diff.

The code diff shows a minor change in the method `getInstanceName`. The variable name `aMethodCallAst` is changed to `methodCallAst`. This is purely a variable renaming, likely for coding style consistency or readability. There are no logical changes, error handling updates, or exception handling improvements. The functionality of the code remains the same.

Given the lack of substantial changes and the vague commit message, it's unlikely that this commit fixes a bug.

**NotBuggy**
"
checkstyle,1737.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,4 +1,4 @@
-    public void setAllowedDistance(int aAllowedDistance)
+    public void setAllowedDistance(int allowedDistance)
     {
-        this.mAllowedDistance = aAllowedDistance;
+        this.allowedDistance = allowedDistance;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The commit message ""Prefixes, coding, #512"" suggests a focus on code style and potentially addressing an issue (indicated by #512).

The code diff shows a change in the parameter name from `aAllowedDistance` to `allowedDistance` and a corresponding change in the assignment to the member variable `mAllowedDistance` to `allowedDistance`. This indicates a change in naming convention, likely to remove the 'm' prefix for member variables and 'a' prefix for arguments.

This change doesn't appear to be directly related to fixing a bug. It's more about code style and consistency. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**
"
checkstyle,2819.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,10 +1,10 @@
     private static DetailAST findTokenWhichBeginsTheLine(DetailAST root) {
-        DetailAST tokenWhichBeginsTheLine;
+        final DetailAST tokenWhichBeginsTheLine;
         if (isUsingOfObjectReferenceToInvokeMethod(root)) {
             tokenWhichBeginsTheLine = findStartTokenOfMethodCallChain(root);
         }
         else {
             tokenWhichBeginsTheLine = root.getFirstChild().findFirstToken(TokenTypes.IDENT);
         }
         return tokenWhichBeginsTheLine;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message states that the change enforces `FinalLocalVariableCheck` on checkstyle code. The diff shows a local variable `tokenWhichBeginsTheLine` being made final. This aligns with the commit message and suggests that the change is to enforce a style check. There's no indication of a bug fix.

**NotBuggy**"
checkstyle,627.json,b8ca6a585b824e91b3b8c72dd5cc53c0eb0ab0f1,"@@ -1,11 +1,11 @@
     private void tagSuppressions(Collection<TextBlock> comments)
     {
         for (TextBlock comment : comments) {
             final int startLineNo = comment.getStartLineNo();
             final String[] text = comment.getText();
-            tagCoontLine(text[0], startLineNo, comment.getStartColNo());
+            tagCommentLine(text[0], startLineNo, comment.getStartColNo());
             for (int i = 1; i < text.length; i++) {
-                tagCoontLine(text[i], startLineNo + i, 0);
+                tagCommentLine(text[i], startLineNo + i, 0);
             }
         }
     }",NotBuggy,"Prefixes, checkstyle, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/TreeWalker.java
",Buggy,"The commit message ""Prefixes, checkstyle, #512"" suggests changes related to prefixes, checkstyle configurations, and issue #512. The diff shows a change in the method `tagSuppressions` within the `TreeWalker.java` file. Specifically, `tagCoontLine` is changed to `tagCommentLine`. This looks like a typo fix. The commit message doesn't explicitly mention a bug fix, but the code change suggests a correction of a typo, which can be considered a bug fix.

Reasoning:

1.  **Code Change:** The change from `tagCoontLine` to `tagCommentLine` strongly suggests a typo correction.
2.  **Commit Message:** The commit message mentions ""checkstyle,"" which might be related to code quality and typo fixes.
3.  **Bug Fix Indication:** Typo fixes are generally considered bug fixes.

Conclusion: **Buggy**"
checkstyle,2956.json,58219b9bd31c8f7fbc4f4bba0eeb811851f276b3,"@@ -1,12 +1,12 @@
-    private boolean isIgnoredParam(DetailAST aParamDef)
+    private boolean isIgnoredParam(DetailAST paramDef)
     {
         boolean result = false;
-        if (mIgnorePrimitiveTypes) {
-            final DetailAST parameterType = aParamDef.
+        if (ignorePrimitiveTypes) {
+            final DetailAST parameterType = paramDef.
                     findFirstToken(TokenTypes.TYPE).getFirstChild();
-            if (mPrimitiveDataTypes.contains(parameterType.getType())) {
+            if (primitiveDataTypes.contains(parameterType.getType())) {
                 result = true;
             }
         }
         return result;
     }",NotBuggy,"Prefixes, checks, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/checks/NewlineAtEndOfFileCheck.java
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**1. Commit Message Analysis:**

The commit message ""Prefixes, checks, #512"" suggests several things:

*   ""Prefixes"": This could refer to changes related to how prefixes are handled in the code.
*   ""checks"": This implies modifications or additions to validation checks within the code.
*   ""#512"": This likely refers to an issue tracker number (e.g., a bug report or feature request). The presence of an issue number strongly hints that the commit addresses a specific problem or enhancement.

**2. Code Diff Analysis:**

The code diff shows changes within the `isIgnoredParam` method of the `NewlineAtEndOfFileCheck.java` file. Let's break down the changes:

*   `private boolean isIgnoredParam(DetailAST aParamDef)` to `private boolean isIgnoredParam(DetailAST paramDef)`: This is a simple renaming of the parameter `aParamDef` to `paramDef`. This change by itself doesn't indicate a bug fix.
*   `if (mIgnorePrimitiveTypes)` to `if (ignorePrimitiveTypes)`: This is a change from accessing a member variable `mIgnorePrimitiveTypes` to `ignorePrimitiveTypes`. This suggests that the code might be refactoring to remove the `m` prefix.
*   `final DetailAST parameterType = aParamDef.findFirstToken(TokenTypes.TYPE).getFirstChild();` to `final DetailAST parameterType = paramDef.findFirstToken(TokenTypes.TYPE).getFirstChild();`: This change reflects the parameter renaming.
*   `if (mPrimitiveDataTypes.contains(parameterType.getType()))` to `if (primitiveDataTypes.contains(parameterType.getType()))`: This is a change from accessing a member variable `mPrimitiveDataTypes` to `primitiveDataTypes`. This suggests that the code might be refactoring to remove the `m` prefix.

**3. Reasoning and Conclusion:**

The changes in the code diff primarily involve renaming a parameter and removing the `m` prefix from member variables. While these changes might be part of a larger refactoring effort, they don't directly indicate a bug fix in themselves. However, the commit message including ""#512"" strongly suggests that this commit is related to a specific issue. It is possible that the issue involved some refactoring and renaming as part of the fix. Given the context of the commit message and the issue number, it is likely that these changes are related to addressing a bug or issue.

**Buggy**
"
checkstyle,1180.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,9 +1,9 @@
-    private boolean isEmptyLoop(DetailAST aAST, int aParentType)
+    private boolean isEmptyLoop(DetailAST ast, int parentType)
     {
-        return mAllowEmptyLoops
-            && (isEmptyBlock(aAST, aParentType, TokenTypes.LITERAL_FOR)
-                    || isEmptyBlock(aAST,
-                            aParentType, TokenTypes.LITERAL_WHILE)
-                            || isEmptyBlock(aAST,
-                                    aParentType, TokenTypes.LITERAL_DO));
+        return allowEmptyLoops
+            && (isEmptyBlock(ast, parentType, TokenTypes.LITERAL_FOR)
+                    || isEmptyBlock(ast,
+                            parentType, TokenTypes.LITERAL_WHILE)
+                            || isEmptyBlock(ast,
+                                    parentType, TokenTypes.LITERAL_DO));
     }",NotBuggy,"Prefixes, whitespace, #512
",NotBuggy,"The commit message ""Prefixes, whitespace, #512"" suggests changes related to code style (prefixes, whitespace) and potentially addresses issue #512.

The code diff shows the following changes:

1.  **Variable Naming:** `aAST` is changed to `ast` and `aParentType` is changed to `parentType`. This suggests a change in variable naming style, possibly to remove the ""a"" prefix.
2.  **Field Access:** `mAllowEmptyLoops` is changed to `allowEmptyLoops`. This suggests a change from accessing a member variable using `m` prefix to directly accessing it.

These changes are consistent with the commit message's claim of changes to prefixes and whitespace. The change in variable naming style and field access suggests a cleanup or refactoring effort. There's no explicit error handling, logical correction, or exception handling improvement in the diff. It's hard to determine if this commit fixes a bug without knowing the context of issue #512. However, based on the diff and commit message, the changes appear to be primarily stylistic and related to code cleanup.

**NotBuggy**"
checkstyle,2157.json,750b733bd124eaea34d4692a1bb8d0fd0fe90976,"@@ -1,4 +1,4 @@
-    public void setIgnoreName(boolean aValue)
+    public void setIgnoreName(boolean value)
     {
-        mIgnoreName = aValue;
+        ignoreName = value;
     }",NotBuggy,"Prefixes, naming, #512
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Prefixes, naming, #512"" suggests changes related to code style, specifically prefixes and naming conventions. The ""#512"" likely refers to an issue tracker number, possibly indicating a task to improve code quality.

2.  **Git Diff:**
    *   `public void setIgnoreName(boolean aValue)` changed to `public void setIgnoreName(boolean value)`: This change involves renaming the parameter `aValue` to `value`. This aligns with the ""naming"" part of the commit message.
    *   `mIgnoreName = aValue;` changed to `ignoreName = value;`: This change involves renaming the member variable `mIgnoreName` to `ignoreName`. This aligns with the ""prefixes"" and ""naming"" part of the commit message, likely removing the ""m"" prefix, which is a common coding style change.

3.  **Bug Fix Assessment:** The changes are purely stylistic and related to code cleanup. There's no indication of error handling, logical corrections, or exception handling improvements. The changes are focused on improving code readability and adhering to naming conventions.

**Conclusion:**

The changes are related to code style and naming conventions, with no indication of a bug fix.

**NotBuggy**"
checkstyle,2400.json,50c9bb69bcd93e6fbf272670e8b9d95f2f451c5f,"@@ -1,4 +1,4 @@
-    public void setAllowMarkerInterfaces(boolean aFlag)
+    public void setAllowMarkerInterfaces(boolean flag)
     {
-        mAllowMarkerInterfaces = aFlag;
+        allowMarkerInterfaces = flag;
     }",NotBuggy,"Prefixes, design, #512
",NotBuggy,"The commit message ""Prefixes, design, #512"" is vague and doesn't provide much context about the purpose of the change. The code diff shows a simple renaming of a variable from `aFlag` to `flag` and `mAllowMarkerInterfaces` to `allowMarkerInterfaces`. This change seems to be related to code style or design improvements, possibly to adhere to a specific naming convention. There's no indication of error handling, logical corrections, or exception handling improvements. Therefore, it's unlikely that this commit fixes a bug.

**NotBuggy**
"
checkstyle,1279.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,4 +1,4 @@
-    public void setEachLine(boolean aEachLine)
+    public void setEachLine(boolean eachLine)
     {
-        mEachLine = aEachLine;
+        this.eachLine = eachLine;
     }",NotBuggy,"Prefixes, whitespace, #512
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Prefixes, whitespace, #512"" suggests the following:

*   **Prefixes:** This could refer to changes related to how prefixes are handled in the code. However, without more context, it's difficult to say definitively.
*   **Whitespace:** This indicates that some whitespace cleanup or formatting changes were made.
*   **#512:** This likely refers to an issue tracker number (e.g., Jira, GitHub Issues). This strongly suggests that the commit is related to addressing a specific reported problem.

**Diff Analysis:**

The diff shows a very small change within the `setEachLine` method:

*   The parameter name `aEachLine` has been changed to `eachLine`.
*   The member variable access `mEachLine` has been changed to `this.eachLine`.

These changes appear to be primarily stylistic:

*   The parameter name change likely aims for better naming conventions (removing the ""a"" prefix).
*   The use of `this.eachLine` makes it explicit that the member variable is being accessed, which can improve readability.

**Reasoning:**

1.  **Issue Tracker Reference:** The presence of ""#512"" in the commit message strongly hints at a bug fix or feature implementation related to a specific issue.
2.  **Naming and Style:** The code changes themselves are minor and primarily related to code style and naming conventions. While not directly fixing a functional bug, they could be addressing a code quality issue raised in the context of issue #512.
3.  **Context is Key:** Without knowing the details of issue #512, it's hard to be certain. However, the combination of the issue tracker reference and the code changes suggests that this commit is addressing a specific problem, even if it's a minor one.

**Conclusion:**

Given the presence of the issue tracker reference and the code changes, it's likely that this commit is related to a bug fix or improvement, even if the changes themselves seem minor.

**Buggy**"
checkstyle,2496.json,16dac636b246c658cad31ee9b2e45c53b851d4f2,"@@ -1,4 +1,4 @@
-    public final void setSortImportsInGroupAlphabetically(boolean aValue)
+    public final void setSortImportsInGroupAlphabetically(boolean value)
     {
-        mSortImportsInGroupAlphabetically = aValue;
+        sortImportsInGroupAlphabetically = value;
     }",NotBuggy,"Prefixes, imports, #512
",NotBuggy,"The commit message ""Prefixes, imports, #512"" suggests changes related to prefixes, imports, and potentially issue #512.

The code diff shows a change in a setter method `setSortImportsInGroupAlphabetically`. Specifically, it changes the parameter name from `aValue` to `value` and the member variable name from `mSortImportsInGroupAlphabetically` to `sortImportsInGroupAlphabetically`.

These changes don't obviously indicate a bug fix. They seem more like renaming or refactoring, possibly to adhere to a coding style or convention. There's no error handling, logical correction, or exception handling improvement visible. The change in parameter name and member variable name could be related to a style guide or an effort to remove the 'm' prefix from member variables.

Therefore, based on the provided information, the changes do not appear to be related to a bug fix.

**NotBuggy**"
checkstyle,2285.json,5f68bb50fd191fec409f5edb90755a18fce3e0ac,"@@ -1,6 +1,12 @@
     public void beginTree(DetailAST rootAst)
     {
         immutableClassShortNames.clear();
-        final List<String> shortNames = getClassShortNames(immutableClassCanonicalNames);
-        immutableClassShortNames.addAll(shortNames);
+        final List<String> classShortNames =
+                getClassShortNames(immutableClassCanonicalNames);
+        immutableClassShortNames.addAll(classShortNames);
+
+        ignoreAnnotationShortNames.clear();
+        final List<String> annotationShortNames =
+                getClassShortNames(ignoreAnnotationCanonicalNames);
+        ignoreAnnotationShortNames.addAll(annotationShortNames);
     }",NotBuggy,"Visibility Modifier Check, ignore annotated fields
",NotBuggy,"The commit message ""Visibility Modifier Check, ignore annotated fields"" suggests that the change introduces a mechanism to ignore fields annotated with specific annotations during visibility checks.

The code diff shows the addition of logic to populate `ignoreAnnotationShortNames` from `ignoreAnnotationCanonicalNames`. This aligns with the commit message's intention to ignore annotated fields. The code retrieves short names of classes from canonical names, which is consistent with how immutable classes are handled. This suggests that the change is related to handling annotations similar to how immutable classes are handled, which is related to visibility checks.

The change seems to be adding functionality rather than fixing a bug.

**NotBuggy**"
checkstyle,1285.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,17 +1,17 @@
     private static DetailAST getWhitespaceFollowedNode(DetailAST ast) {
-        DetailAST whitespaceFollowedAst;
+        final DetailAST whitespaceFollowedAst;
         switch (ast.getType()) {
             case TokenTypes.TYPECAST:
                 whitespaceFollowedAst = ast.findFirstToken(TokenTypes.RPAREN);
                 break;
             case TokenTypes.ARRAY_DECLARATOR:
                 whitespaceFollowedAst = getArrayDeclaratorPreviousElement(ast);
                 break;
             case TokenTypes.INDEX_OP:
                 whitespaceFollowedAst = getIndexOpPreviousElement(ast);
                 break;
             default:
                 whitespaceFollowedAst = ast;
         }
         return whitespaceFollowedAst;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message indicates an enforcement of `FinalLocalVariableCheck` on Checkstyle code. The diff shows changes to the `getWhitespaceFollowedNode` method, specifically marking the `whitespaceFollowedAst` variable as `final`. This change aligns with the commit message's intention to enforce the `FinalLocalVariableCheck`. The change itself doesn't inherently indicate a bug fix but rather an enforcement of a coding style rule.

**NotBuggy**"
checkstyle,2860.json,1bd7f9bdf3dc678b70e79481971aebc50db69dc7,"@@ -1,4 +1,4 @@
-    public final void setAllowSamelineMultipleAnnotations(boolean aAllow)
+    public final void setAllowSamelineMultipleAnnotations(boolean allow)
     {
-        mAllowSamelineMultipleAnnotations = aAllow;
+        allowSamelineMultipleAnnotations = allow;
     }",NotBuggy,"Prefixes, annotation, #512
",NotBuggy,"The commit message ""Prefixes, annotation, #512"" is vague. The code diff shows a simple renaming of a variable `aAllow` to `allow` and `mAllowSamelineMultipleAnnotations` to `allowSamelineMultipleAnnotations`. This looks like a refactoring change to improve code readability and consistency, possibly related to coding style preferences (prefixes). There is no indication of a bug fix.

**NotBuggy**"
checkstyle,1965.json,eb5b977a475c2240db5efc83aec8973d734895eb,"@@ -1,13 +1,13 @@
         public Optional<FinalVariableCandidate> findFinalVariableCandidateForAst(DetailAST ast) {
-            Optional<FinalVariableCandidate> result = Optional.absent();
+            Optional<FinalVariableCandidate> result = Optional.empty();
             DetailAST storedVariable = null;
             final Optional<FinalVariableCandidate> candidate =
-                Optional.fromNullable(scope.get(ast.getText()));
+                Optional.ofNullable(scope.get(ast.getText()));
             if (candidate.isPresent()) {
                 storedVariable = candidate.get().variableIdent;
             }
             if (storedVariable != null && isSameVariables(storedVariable, ast)) {
                 result = candidate;
             }
             return result;
         }",NotBuggy,"Pull #3293: Replace Guava's Optional with Java's native (#3293)

",NotBuggy,"The commit message indicates a replacement of Guava's `Optional` with Java's native `Optional`. The diff shows changes related to `Optional` usage: `Optional.absent()` is replaced with `Optional.empty()`, and `Optional.fromNullable()` is replaced with `Optional.ofNullable()`. These changes are consistent with the commit message's intention to migrate to Java's native `Optional`. There's no indication of bug fixes in this diff. The changes are purely related to library replacement.

**NotBuggy**"
checkstyle,3762.json,b94bac0719851b65b59b4aafc5bd3ef029321ed7,"@@ -1,4 +1,4 @@
     public Charset getCharset()
     {
-        return mCharset;
+        return charset;
     }",NotBuggy,"Prefixes, api, #512
",NotBuggy,"The commit message ""Prefixes, api, #512"" is quite vague, but the diff shows a simple variable rename from `mCharset` to `charset` within the `getCharset()` method. This change doesn't inherently suggest a bug fix. It's more likely a refactoring or code style improvement. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**
"
lombok,2687.json,a05360a8eaba0de61f16f75816daf5a5af0a4567,"@@ -1,5 +1,4 @@
 	@Override public boolean visit(QualifiedNameReference node, ClassScope scope) {
-		setGeneratedBy(node, source);
-		applyOffsetExpression(node);
+		fixPositions(setGeneratedBy(node, source));
 		return super.visit(node, scope);
 	}",NotBuggy,ecj @Builder @Singular support for j.u. sets and maps.,Buggy,"The commit message indicates the addition of `@Builder` and `@Singular` support for `java.util` sets and maps in the `ecj` compiler. The provided diff modifies the `visit` method of a class that overrides `QualifiedNameReference` node visits. The change replaces two lines: `setGeneratedBy(node, source);` and `applyOffsetExpression(node);` with `fixPositions(setGeneratedBy(node, source));`.

The commit message does not directly suggest a bug fix. However, the change in the `visit` method, specifically the replacement of two separate calls with a single call that includes `fixPositions`, could be related to correcting an issue with how positions are handled for generated code. If the original code was generating incorrect positions for qualified name references, this change could be considered a bug fix. Without more context, it's difficult to definitively say. The `fixPositions` call suggests that the previous position information might have been incorrect, which points towards a bug fix.

**Buggy**
"
lombok,2499.json,fb401d4887895d1ebb8529d6323797f1bc8072a2,"@@ -1,3 +1,3 @@
-	public static EclipseSingularsRecipes get() {
-		return INSTANCE;
-	}

+				@Override public ReturnStatement get() {
+					return chain ? new ReturnStatement(new ThisReference(0, 0), 0, 0) : null;
+				}",NotBuggy,ecj: setter methods now return self(),Buggy,"The commit message ""ecj: setter methods now return self()"" indicates a change in the return type of setter methods to return the object itself (self or this). This is often done to enable method chaining (fluent interface).

The provided code diff changes a `get()` method. The original `get()` method returned a singleton instance. The modified `get()` method now returns a `ReturnStatement` which returns `this` if `chain` is true, otherwise it returns `null`. This change doesn't seem to be directly related to setter methods or the concept of returning `self()`. However, the change introduces a conditional return of `null`, which could be related to fixing a null pointer exception or handling a specific case where a return statement is not needed.

Given the limited context, it's difficult to definitively say if this is a bug fix. However, the introduction of a conditional `null` return suggests a potential bug fix related to handling cases where a return statement is not required.

**Buggy**
"
lombok,1375.json,5b4ab9dae8940973ce8b87c8c2a0388b768dac89,"@@ -1,4 +1,7 @@
-	@Override public java.util.List<Name> listFieldsToBeGenerated(JavacNode builderType, SingularData data) {
-		String p = data.getPluralName().toString();
-		return Arrays.asList(builderType.toName(p + ""$key""), builderType.toName(p + ""$value""));
+	@Override public java.util.List<Name> listMethodsToBeGenerated(SingularData data, JavacNode builderType) {
+		if (useGuavaInstead(builderType)) {
+			return guavaMapSingularizer.listMethodsToBeGenerated(data, builderType);
+		}
+		
+		return super.listMethodsToBeGenerated(data, builderType);
 	}",NotBuggy,added the ‘always use guava’ lombok.config option for @Singular @Builder.,NotBuggy,"The commit message indicates the addition of a configuration option ('always use guava') for `@Singular` and `@Builder` annotations, specifically related to Guava usage.

The code diff shows a modification in the `listMethodsToBeGenerated` method. It introduces a conditional check `useGuavaInstead(builderType)`. If this condition is true, it delegates the method call to `guavaMapSingularizer.listMethodsToBeGenerated(data, builderType)`. Otherwise, it calls the superclass's implementation.

The code change directly reflects the commit message's intent to conditionally use Guava based on the new configuration option. This doesn't inherently indicate a bug fix. It's adding new functionality or modifying existing behavior based on a configuration setting. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**"
lombok,2745.json,35691e83edffdadd5ef438793eec9c968e8bfd35,"@@ -1,9 +1,9 @@
-	public static TypeReference[] copyTypes(TypeReference[] refs) {
+	public static TypeReference[] copyTypes(TypeReference[] refs, ASTNode source) {
 		if ( refs == null ) return null;
 		TypeReference[] outs = new TypeReference[refs.length];
 		int idx = 0;
 		for ( TypeReference ref : refs ) {
-			outs[idx++] = copyType(ref);
+			outs[idx++] = copyType(ref, source);
 		}
 		return outs;
 	}",NotBuggy,"Massive change to the eclipse handlers: They now set the 'generatedBy' flag which we can use to patch eclipse in specific places to ignore generated nodes.
",NotBuggy,"The commit message indicates a significant change related to Eclipse handlers, specifically mentioning the introduction of a 'generatedBy' flag. This flag is intended to help patch Eclipse by allowing the system to ignore generated nodes in certain situations.

The provided code diff shows a modification to the `copyTypes` method. The change involves adding an `ASTNode source` parameter to the method signature and passing this `source` to the `copyType` method.

Reasoning:

1. **Relevance to Commit Message:** The code diff itself doesn't directly show the setting of the 'generatedBy' flag. However, the introduction of the `source` parameter suggests that the context (ASTNode) is being passed down to potentially influence how types are copied, possibly to mark them as generated based on the source. The commit message mentions patching eclipse to ignore generated nodes. Passing the ASTNode source to the copyType method could be a step in identifying and marking nodes as generated.

2. **Bug Fix Indication:** The change doesn't explicitly scream ""bug fix"". It looks more like a feature enhancement or a change to support the new 'generatedBy' flag functionality. There's no error handling, logical correction, or exception handling improvement visible in the diff. The change seems to be adding context to the type copying process.

Conclusion:

**NotBuggy**
"
lombok,2695.json,0234039babafa1098c337136a97f4e1f94e35855,"@@ -1,5 +1,5 @@
 	@Override public boolean visit(SingleMemberAnnotation node, BlockScope scope) {
 		setGeneratedBy(node, source);
-		applyOffset(node, newSourceStart, newSourceEnd);
+		applyOffset(node);
 		return super.visit(node, scope);
 	}",NotBuggy,"Removed statics and lots of parameters, reimplemented recalcSourcePosition
",NotBuggy,"The commit message indicates a refactoring effort, focusing on removing static elements and simplifying parameter lists, along with reimplementing `recalcSourcePosition`. The provided diff snippet shows a change in the `visit` method of the `SingleMemberAnnotation` class. Specifically, the `applyOffset` method call is modified, removing the `newSourceStart` and `newSourceEnd` parameters.

Reasoning:

1.  **Alignment with Commit Message:** The removal of parameters in the `applyOffset` method aligns with the commit message's claim of reducing the number of parameters.
2.  **Bug Fix Indication:** The change doesn't explicitly indicate a bug fix. It seems to be a part of the refactoring process mentioned in the commit message. There's no error handling, logical correction, or exception handling improvement visible in the provided diff. The change is more about simplifying the method signature.

Conclusion:

**NotBuggy**
"
lombok,962.json,4be46113e81292a88cd5fdb3a5ce18fbcffd570d,"@@ -1,7 +1,7 @@
-	public static LombokOptions replaceWithDelombokOptions(Context context) {
+	public static Javac6BasedLombokOptions replaceWithDelombokOptions(Context context) {
 		Options options = Options.instance(context);
 		context.put(optionsKey, (Options)null);
-		LombokOptions result = new LombokOptions(context);
+		Javac6BasedLombokOptions result = new Javac6BasedLombokOptions(context);
 		result.putAll(options);
 		return result;
 	}",NotBuggy,"Compiler options can now be specified for JDK 8 or JDK 6/7 or lower.

After this was finalized I realize that we might consider moving entirely
to String based options, instead of inferring the options from the
provided enum. This setup does have the benefit of throwing exceptions when
options are not present.
",NotBuggy,"The commit message indicates a change in how compiler options are specified, allowing for different options based on the JDK version. The diff shows a change in the return type of the `replaceWithDelombokOptions` method from `LombokOptions` to `Javac6BasedLombokOptions`. It also changes the type of the `result` variable. These changes suggest an adaptation to handle different compiler options, which aligns with the commit message. The commit message also hints at a potential future change to String based options, but the current diff doesn't reflect that directly. The changes don't explicitly indicate a bug fix, but rather an enhancement or refactoring to support different JDK versions.

**NotBuggy**"
lombok,244.json,eca219ee6433cd964f0549a114a791ca4eb9f0fa,"@@ -1,21 +1,20 @@
 	private <T> T readObject(JCTree tree, String fieldName, T defaultValue) {
 		Class<?> tClass = tree.getClass();
 		Map<String, Field> c = reflectionCache.get(tClass);
 		if (c == null) reflectionCache.put(tClass, c = new HashMap<String, Field>());
 		Field f = c.get(fieldName);
 		if (f == null) {
 			try {
-				f = tClass.getDeclaredField(fieldName);
+				f = Permit.getField(tClass, fieldName);
 			} catch (Exception e) {
 				return defaultValue;
 			}
-			f.setAccessible(true);
 			c.put(fieldName, f);
 		}
 		
 		try {
 			return (T) f.get(tree);
 		} catch (Exception e) {
 			return defaultValue;
 		}
 	}",NotBuggy,"eliminate ‘you are using private API’ warnings by streamlining all reflective access via a class that uses sun.misc.Unsafe to arrange access. From the nqzero permit-reflect library.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states the goal is to eliminate ""you are using private API"" warnings by streamlining reflective access using `sun.misc.Unsafe`. It also mentions the `nqzero permit-reflect` library. This suggests a change in how reflection is handled to avoid warnings related to accessing internal Java APIs.

**Git Diff Analysis:**

The diff shows a modification to the `readObject` method.  Here's a breakdown:

1.  **`f = tClass.getDeclaredField(fieldName);`  replaced with  `f = Permit.getField(tClass, fieldName);`**

    *   This is the core of the change. Instead of directly getting the declared field using `getDeclaredField`, it's now using a `Permit.getField` method. This strongly suggests that the `Permit` class (likely from the `nqzero permit-reflect` library mentioned in the commit message) is responsible for handling the reflective access in a safer or more controlled way, presumably using `sun.misc.Unsafe`.
2.  **`f.setAccessible(true);` removed**

    *   The removal of `f.setAccessible(true)` is significant.  Previously, the code was explicitly making the field accessible, which is a common source of ""private API"" warnings. The `Permit.getField` method likely handles accessibility internally, potentially using `Unsafe` to bypass normal access restrictions without triggering the warnings.

**Reasoning:**

The commit message and the diff align perfectly. The code is being modified to use a new `Permit` class (likely from the `nqzero permit-reflect` library) to manage reflective access. The removal of `f.setAccessible(true)` indicates that the `Permit` class is handling the accessibility issue, presumably using `sun.misc.Unsafe` as the commit message suggests.

The original code was likely triggering warnings because it was directly accessing private fields using reflection. The change aims to eliminate these warnings by using a more controlled and potentially ""safer"" approach via `sun.misc.Unsafe` (through the `Permit` class).

While the commit message doesn't explicitly state that this is a bug fix, the elimination of ""you are using private API"" warnings is often a desirable outcome, especially if those warnings were causing problems or instability. The change addresses a potential issue related to accessing internal APIs, which can be considered a form of bug fix or at least a preventative measure against future issues.

**Conclusion:**

**Buggy**
"
lombok,817.json,f07948d4959cfdb81bb0c540ebcbe879df0109fb,"@@ -1,3 +1,3 @@
 	@Override public int hashCode() {
-		return path.hashCode();
+		return eclipseIniPath.hashCode();
 	}",NotBuggy,"The installer is now waaay smarter on linuxes, and should support, out of the box and automatically, debian and debianesques like ubuntu.
",Buggy,"The commit message talks about improvements to the installer on Linux, specifically mentioning Debian and Ubuntu support. The provided code diff changes the `hashCode()` method of some class, replacing `path.hashCode()` with `eclipseIniPath.hashCode()`.

The change in the `hashCode()` method doesn't seem directly related to the installer's logic for handling different Linux distributions. It's a low-level change that could be related to performance or correctness of hash-based data structures, but it doesn't obviously address the installer's distribution-specific logic. It's possible that this change is part of a larger effort to improve the installer, but based on the provided information, it's difficult to establish a direct connection. The change could be fixing a bug related to how paths are hashed, which could indirectly affect the installer.

Given the limited context, it's hard to definitively say if this is a bug fix or not. However, the change in `hashCode()` suggests a potential correction of an incorrect or suboptimal implementation.

**Buggy**
"
lombok,2906.json,bb66465751b8f0b129e53e639854f95fcdb3dca8,"@@ -1,20 +1,12 @@
 	public static TypeReference createTypeReference(String typeName, Annotation source) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		long p = (long)pS << 32 | pE;
 		
-		TypeReference typeReference;
-		if (typeName.contains(""."")) {
-			
-			char[][] typeNameTokens = fromQualifiedName(typeName);
-			long[] pos = new long[typeNameTokens.length];
-			Arrays.fill(pos, p);
-			
-			typeReference = new QualifiedTypeReference(typeNameTokens, pos);
-		}
-		else {
-			typeReference = null;
-		}
+		char[][] typeNameTokens = fromQualifiedName(typeName);
+		long[] pos = new long[typeNameTokens.length];
+		Arrays.fill(pos, p);
 		
+		TypeReference typeReference = new QualifiedTypeReference(typeNameTokens, pos);
 		setGeneratedBy(typeReference, source);
 		return typeReference;
 	}",NotBuggy,"Custom log declaration
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message:** ""Custom log declaration""

**Diff Analysis:**

The code modifies the `createTypeReference` method. The original code had a conditional block that checked if the `typeName` contained a ""."". If it did, it created a `QualifiedTypeReference`. Otherwise, it set `typeReference` to `null`. The modified code removes this conditional block and always creates a `QualifiedTypeReference` regardless of whether the `typeName` contains a ""."".

**Reasoning:**

The original code had a potential bug where if the `typeName` did *not* contain a ""."", the `typeReference` would be set to `null`. The subsequent `setGeneratedBy` call would then likely result in a `NullPointerException`. The modified code fixes this by *always* creating a `QualifiedTypeReference`. This ensures that `typeReference` is never `null`, thus preventing the potential `NullPointerException`. The commit message ""Custom log declaration"" doesn't accurately reflect the change, which appears to be a bug fix related to handling type names without dots.

**Conclusion:**

**Buggy**
"
lombok,2557.json,519f95c2410c2e43c60e81233cad105552975606,"@@ -1,4 +1,6 @@
 	private static final boolean toBoolean(Object expr, boolean defaultValue) {
 		if (expr == null) return defaultValue;
+		if (expr instanceof FalseLiteral) return false;
+		if (expr instanceof TrueLiteral) return true;
 		return ((Boolean) expr).booleanValue();
 	}",NotBuggy,builder @Singleton now works fully in eclipse as well! feature done!,Buggy,"The commit message ""builder @Singleton now works fully in eclipse as well! feature done!"" suggests a feature enhancement related to the `@Singleton` annotation and its integration with Eclipse. The code diff introduces checks for `FalseLiteral` and `TrueLiteral` within the `toBoolean` method. This change seems to be related to handling boolean expressions correctly, which could be a fix for a bug where boolean values were not being interpreted correctly in the context of the `@Singleton` builder, especially within the Eclipse environment. The addition of these checks suggests a correction to how boolean literals are evaluated, indicating a bug fix.

Reasoning:

1.  **Commit Message Context:** The commit message indicates a feature completion related to `@Singleton` and Eclipse integration.
2.  **Code Change Analysis:** The code adds specific checks for `FalseLiteral` and `TrueLiteral` objects. This implies that the original code might have been incorrectly handling these literal types.
3.  **Bug Fix Indication:** The explicit handling of `FalseLiteral` and `TrueLiteral` suggests a correction to a previous error in boolean expression evaluation.

Conclusion: **Buggy**
"
lombok,2948.json,a31e9ffdac96737f5a441efa6c80a2394bba63d1,"@@ -1,3 +1,3 @@
-	public A getAst() {
+	public EclipseAST getAst() {
 		return ast;
 	}",NotBuggy,"use delared ast Type instead of generic one saves a lot of ""checkcasts"" in the byte code
",NotBuggy,"The commit message ""use declared ast Type instead of generic one saves a lot of ""checkcasts"" in the byte code"" suggests a performance optimization by changing the return type of the `getAst()` method from a generic type `A` to a more specific type `EclipseAST`. This change aims to reduce the number of `checkcast` operations in the bytecode, which can improve performance.

The diff shows that the return type of the `getAst()` method has indeed been changed from `A` to `EclipseAST`. This aligns perfectly with the commit message. There is no indication of a bug fix in the traditional sense (i.e., correcting incorrect behavior). Instead, it's an optimization.

**NotBuggy**
"
lombok,824.json,df84dffa62fc51231e9215d04b9751dba6c6cf4d,"@@ -1,17 +1,17 @@
 	private static String generateWriteErrorMessage() {
 		String osSpecificError;
 		
-		switch (EclipseFinder.getOS()) {
+		switch (IdeFinder.getOS()) {
 		default:
 		case MAC_OS_X:
 		case UNIX:
 			osSpecificError = "":\nStart terminal, go to the directory with lombok.jar, and run: sudo java -jar lombok.jar"";
 			break;
 		case WINDOWS:
 			osSpecificError = "":\nStart a new cmd (dos box) with admin privileges, go to the directory with lombok.jar, and run: java -jar lombok.jar"";
 			break;
 		}
 		
 		return "", probably because this installer does not have the access rights.\n"" +
 		""Try re-running the installer with administrative privileges"" + osSpecificError;
 	}",NotBuggy,"Massive update to how the installer works, and added netbeans to the installer.
",Buggy,"The commit message indicates a large update to the installer and the addition of NetBeans support. The provided diff focuses on generating a write error message, specifically tailoring instructions for different operating systems on how to run the installer with administrative privileges.

The change itself doesn't scream ""bug fix"", but the fact that the error message is being refined and made more specific for different OSes suggests that users might have been encountering issues with write permissions during installation. The original message might have been insufficient, leading to confusion or failed installations. The update provides clearer instructions, which can be seen as addressing a usability issue, which can be considered a bug.

Therefore, I conclude that this change is a bug fix.

**Buggy**
"
lombok,1913.json,143143276da646bcc9a195d827364f8499187a6a,"@@ -1,12 +1,14 @@
 	private void printValue(ConfigurationKey<?> key, Object value, boolean verbose, Collection<String> history) {
-		if (verbose) out.printf(""# %s\n"", key.getDescription());
+		if (verbose) out.printf(""# %s%n"", key.getDescription());
 		if (value == null) {
-			out.printf(""clear %s\n"", key.getKeyName());
+			out.printf(""clear %s%n"", key.getKeyName());
 		} else if (value instanceof List<?>) {
-			for (Object element : (List<?>)value) out.printf(""%s += %s\n"", key.getKeyName(), element);
+			List<?> list = (List<?>)value;
+			if (list.isEmpty()) out.printf(""clear %s%n"", key.getKeyName());
+			for (Object element : list) out.printf(""%s += %s%n"", key.getKeyName(), element);
 		} else {
-			out.printf(""%s = %s\n"", key.getKeyName(), value);
+			out.printf(""%s = %s%n"", key.getKeyName(), value);
 		}
 		if (!verbose) return;
-		for (String modification : history) out.printf(""# %s\n"", modification);
+		for (String modification : history) out.printf(""# %s%n"", modification);
 	}",NotBuggy,"[configuration] Have the ConfigurationApp print platform-specific newLine characters, and print clear for empty lists.
",Buggy,"Okay, let's analyze the provided commit message and Git diff.

**Commit Message:**

The commit message states two primary changes:

1.  ""Have the ConfigurationApp print platform-specific newLine characters"": This suggests a change from using a hardcoded newline character (like `\n`) to using the platform's specific newline character (e.g., `System.lineSeparator()` or `%n` in `printf`).
2.  ""print clear for empty lists"": This indicates that when a configuration key's value is an empty list, the application should print a ""clear"" command for that key.

**Git Diff Analysis:**

The diff shows modifications within the `printValue` method. Let's break down the changes:

*   `if (verbose) out.printf(""# %s%n"", key.getDescription());`:  The `\n` has been replaced with `%n`. This aligns with the first part of the commit message regarding platform-specific newline characters.
*   `if (value == null) { out.printf(""clear %s%n"", key.getKeyName()); }`: The `\n` has been replaced with `%n`.
*   `List<?> list = (List<?>)value; if (list.isEmpty()) out.printf(""clear %s%n"", key.getKeyName()); for (Object element : list) out.printf(""%s += %s%n"", key.getKeyName(), element);`: This block handles the case where the value is a list.  Critically, it *adds* a check for an empty list (`list.isEmpty()`) and, if the list is empty, prints `clear %s%n`. This directly addresses the second part of the commit message. The `\n` has been replaced with `%n`.
*   `else { out.printf(""%s = %s%n"", key.getKeyName(), value); }`: The `\n` has been replaced with `%n`.
*   `for (String modification : history) out.printf(""# %s%n"", modification);`: The `\n` has been replaced with `%n`.

**Reasoning:**

The changes in the diff directly correspond to the commit message. The replacement of `\n` with `%n` ensures platform-specific newline characters are used. The addition of the empty list check and the corresponding ""clear"" command output fulfill the second requirement of the commit message. The addition of the empty list check indicates that the previous behavior was not correct, and it was a bug.

**Conclusion:**

**Buggy**
"
lombok,1884.json,713822db9894cf99184aff57b3387f99846aa870,"@@ -1,7 +1,13 @@
 			@Override public Object parse(String value) {
 				try {
 					return Enum.valueOf(rawType, value);
 				} catch (Exception e) {
-					return Enum.valueOf(rawType, value.toUpperCase());
+					StringBuilder sb = new StringBuilder();
+					for (int i = 0; i < value.length(); i++) {
+						char c = value.charAt(i);
+						if (Character.isUpperCase(c) && i > 0) sb.append(""_"");
+						sb.append(Character.toUpperCase(c));
+					}
+					return Enum.valueOf(rawType, sb.toString());
 				}
 			}",NotBuggy,"Changed the lombok.nonNull.exceptionType configkey to be an enum, partly to enforce only sensible exceptions and partly to make it possible to have the message be ‘x is null’ when throwing IAEs.",Buggy,"The commit message indicates a change to the `lombok.nonNull.exceptionType` configuration key, converting it to an enum. The stated reasons are to enforce sensible exceptions and allow for a specific message format when throwing `IllegalArgumentException`s.

The provided diff shows a change within the `parse` method, specifically in the `catch` block that handles exceptions during enum value parsing. The original code attempted to convert the input `value` to uppercase and retry the `Enum.valueOf` method. The modified code now iterates through the input `value`, inserting underscores before uppercase characters (if not the first character) and converting each character to uppercase before appending it to a `StringBuilder`. This suggests an attempt to handle different naming conventions or formats for enum values provided as input.

The change addresses a potential issue where the input string does not directly match the enum's naming convention. The original code's simple `toUpperCase()` conversion might not be sufficient for all cases. The new logic attempts to convert the input string to a format that is more likely to match the enum's naming convention (e.g., converting ""illegalArgument"" to ""ILLEGAL_ARGUMENT""). This indicates a bug fix related to parsing enum values from strings.

**Buggy**
"
lombok,2541.json,09ea02e4f5752e615be2ff5177be1fb328702a5b,"@@ -1,11 +1,11 @@
-	private LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
+	public LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		LocalDeclaration tempVar = new LocalDeclaration(dollarFieldName, pS, pE);
 		setGeneratedBy(tempVar, source);
 		tempVar.initialization = initializer;
 		tempVar.type = type;
 		tempVar.type.sourceStart = pS; tempVar.type.sourceEnd = pE;
 		setGeneratedBy(tempVar.type, source);
 		tempVar.modifiers = Modifier.FINAL;
 		return tempVar;
 	}",NotBuggy,"added package-infos to lots of packages, updated the javadoc in these, and made lots of private/package private methods 'public' in packages that aren't actually public API. Tumi asked for this, and I can imagine this is useful for others who are hacking add-ons to lombok. The package-info files clarify that we aren't committing to long-term stability of anything except the stuff directly in the lombok package, lombok.experimental, and lombok.extern.
",NotBuggy,"The commit message indicates changes related to package-info files, Javadoc updates, and making private/package-private methods public. The stated reason is to facilitate add-on development for Lombok.

The provided diff shows a change in the visibility of the `createLocalDeclaration` method from `private` to `public`. This aligns with the commit message's description of making methods public for external use. There's no indication of bug fixing in either the commit message or the code diff. The change seems to be driven by a feature request to improve extensibility.

**NotBuggy**"
hibernate-search,521.json,1741d8fb1dd68d1fbe72e5650c3815e39e9426fa,"@@ -1,20 +1,19 @@
 	private TypeMetadata doGetTypeMetadataFor(Class<?> clazz, XClass xClass, ParseContext parseContext) {
-		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, configContext )
+		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, parseContext.getIndexManagerType(), configContext )
 				.boost( getBoost( xClass ) )
-				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) )
-				.analyzerReference( configContext.getDefaultLuceneAnalyzerReference() );
+				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) );
 
 		initializePackageLevelAnnotations( packageInfo( clazz ), configContext );
 
 		initializeClass(
 				typeMetadataBuilder,
 				true,
 				EMPTY_PREFIX,
 				parseContext,
 				configContext,
 				false,
 				null
 		);
 
 		return typeMetadataBuilder.build();
 	}",NotBuggy,"HSEARCH-2222 Change the way ScopedAnalyzer are managed

Also provide sensible default analyzer in the remote case.
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:** The commit message ""HSEARCH-2222 Change the way ScopedAnalyzer are managed. Also provide sensible default analyzer in the remote case."" suggests a modification in how analyzers, specifically `ScopedAnalyzer`, are handled. The mention of ""sensible default analyzer in the remote case"" hints at a potential issue where the default analyzer was not appropriate or missing in remote scenarios.

2.  **Diff Analysis:**
    *   The line `TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, configContext )` is changed to `TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, parseContext.getIndexManagerType(), configContext )`. This indicates that the `TypeMetadata.Builder` is now being initialized with `parseContext.getIndexManagerType()`. This likely influences the analyzer selection process.
    *   The line `.analyzerReference( configContext.getDefaultLuceneAnalyzerReference() )` is removed. This suggests that the default analyzer reference is no longer being directly set in this part of the code.

3.  **Connecting the Dots:**
    *   The removal of `configContext.getDefaultLuceneAnalyzerReference()` and the introduction of `parseContext.getIndexManagerType()` in the `TypeMetadata.Builder` constructor strongly suggest a change in how the analyzer is determined.
    *   The commit message's mention of ""sensible default analyzer in the remote case"" aligns with the code changes. It's plausible that the previous method of setting the default analyzer (`configContext.getDefaultLuceneAnalyzerReference()`) was not working correctly in remote scenarios, and the new approach using `parseContext.getIndexManagerType()` is intended to fix this.

**Conclusion:**

The changes in the code, combined with the commit message, indicate that the previous way of managing analyzers had a problem, especially in remote cases. The new approach aims to provide a more ""sensible default analyzer,"" which implies that the old default was not always appropriate. Therefore, this commit likely addresses a bug.

**Buggy**"
hibernate-search,5198.json,db76c736dbdcd54cfc605472cadcbb5f3a4051b1,"@@ -1,3 +1,3 @@
-	public JestClient getClient() {
+	public RestClient getClient() {
 		return client;
 	}",NotBuggy,"HSEARCH-2581 Use the official Elasticsearch REST client instead of Jest
",NotBuggy,"The commit message indicates a change in the Elasticsearch REST client used, from Jest to the official Elasticsearch REST client. The code diff shows that the return type of the `getClient()` method has been changed from `JestClient` to `RestClient`. This change aligns with the commit message. There is no indication of a bug fix in the code or the commit message.

**NotBuggy**"
hibernate-search,7079.json,6524d5233928c0ca691e79cc0f28a6b6116d3b1b,"@@ -1,3 +1,3 @@
 	public Address getAddress() {
-		return channel.getAddress();
+		return channelContainer.getChannel().getAddress();
 	}",NotBuggy,"HSEARCH-1459 Refactor ServiceManager and ServiceProvider
",NotBuggy,"The commit message indicates a refactoring of `ServiceManager` and `ServiceProvider`. The provided diff shows a change in how the `Address` is retrieved, switching from `channel.getAddress()` to `channelContainer.getChannel().getAddress()`. This change suggests a modification in the structure or relationship between the involved objects (`channel`, `channelContainer`). While refactoring can sometimes be related to fixing a bug, this specific change doesn't inherently indicate a bug fix. It seems more like an architectural adjustment or a change in how the address is accessed due to the refactoring. There's no explicit error handling, logical correction, or exception handling improvement visible in this diff.

**NotBuggy**
"
hibernate-search,2895.json,8c9a35568e74f8c4d21eda2a6f517a51c7fad55b,"@@ -1,4 +1,4 @@
-	public PropertySpatialMapping bottomGridLevel(int bottomGridLevel) {
-		spatial.put( ""bottomQuadTreeLevel"", bottomGridLevel );
+	public PropertySpatialMapping bottomSpatialHashLevel(int bottomSpatialHashLevel) {
+		spatial.put( ""bottomSpatialHashLevel"", bottomSpatialHashLevel );
 		return this;
 	}",NotBuggy,"HSEARCH-1327 Reword reference to Quad Tree for Spatial Hash in spatial module
",NotBuggy,"The commit message ""HSEARCH-1327 Reword reference to Quad Tree for Spatial Hash in spatial module"" indicates a renaming or rewording of a term related to spatial indexing, specifically changing ""Quad Tree"" to ""Spatial Hash"".

The provided code diff shows a change from `bottomGridLevel` to `bottomSpatialHashLevel` and `bottomQuadTreeLevel` to `bottomSpatialHashLevel`. This change directly reflects the intention stated in the commit message. The change is purely a renaming operation to use more accurate terminology. There is no indication of a bug fix in the code.

**NotBuggy**"
hibernate-search,5149.json,ee9e8d7bcbb74216bde5b521e9d7275cc944bbe7,"@@ -1,7 +1,7 @@
 	public void add(BulkableElasticsearchWork<?> work) {
 		currentBulkWorks.add( work );
-		if ( currentBulkWorks.size() >= MAX_BULK_SIZE ) {
+		if ( currentBulkWorks.size() >= maxBulkSize ) {
 			flushBulked();
 			flushBulk();
 		}
 	}",NotBuggy,"HSEARCH-2764 Set the minim bulk size to 1 for stream work orchestration

For stream works, we only submit single-work changesets, which means
the decision on whether to bulk the work or not will always happen
immediately after each work, when we only have one work to bulk.
Thus if we set the minimum to a value higher than 1, we would always
decide not to start a bulk (because there would always be only one
work to bulk), which would result in terrible performance.
",Buggy,"The commit message explains that setting the minimum bulk size to a value greater than 1 would result in performance issues for stream works. The code diff shows that the `add` method in `BulkableElasticsearchWork` class is modified to use `maxBulkSize` instead of `MAX_BULK_SIZE` for checking the current bulk size. This change allows the `maxBulkSize` to be configurable, and setting it to 1 would address the performance issue described in the commit message. The commit message indicates a bug fix related to bulk processing logic, and the code change aligns with the described fix.

**Buggy**"
hibernate-search,6283.json,359d8d7e1ab9ba2d9abc0aa8c4a16c7f61526996,"@@ -1,6 +1,6 @@
 	public void addDelete(String entityClassName, byte[] id) {
-		GenericRecord delete = new GenericData.Record( schemas.get( ""Delete"" ) );
+		GenericRecord delete = new GenericData.Record( protocol.getType( ""Delete"" ) );
 		delete.put( ""class"", entityClassName );
 		delete.put( ""id"", ByteBuffer.wrap( id ) );
 		operations.add( delete );
 	}",NotBuggy,"HSEARCH-757 Use Avro Protocol to implement proper schema relations

The previous solution was fragile and protocols accept several types with cross references
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""HSEARCH-757 Use Avro Protocol to implement proper schema relations"" suggests a change in how schema relations are handled, specifically by adopting the Avro protocol. The message also mentions that the previous solution was ""fragile"" and that protocols accept several types with cross-references. This strongly implies that the previous method of handling schema relations had issues, potentially leading to errors or unexpected behavior.

2.  **Code Diff:** The code diff shows a change in the `addDelete` method. Specifically, it replaces `schemas.get( ""Delete"" )` with `protocol.getType( ""Delete"" )`.

    *   `schemas.get(""Delete"")` likely refers to retrieving a schema named ""Delete"" from a map or similar data structure. This approach could be fragile if the schema is not properly managed or if there are inconsistencies in how schemas are defined and accessed.
    *   `protocol.getType(""Delete"")` suggests using the Avro protocol to obtain the ""Delete"" schema. This is consistent with the commit message and indicates a shift towards a more robust and standardized way of managing schemas. The Avro protocol is designed to handle schema evolution and cross-references, which addresses the ""fragile"" nature of the previous solution.

3.  **Bug Fix Indication:** The commit message explicitly states that the previous solution was fragile. The code change replaces a potentially fragile schema retrieval mechanism with a more robust Avro-based approach. This suggests that the change is intended to address a potential source of errors or instability, which aligns with the definition of a bug fix.

**Conclusion:**

**Buggy**
"
hibernate-search,1714.json,afb48fce1e1b306db8ec9f5f14fd5e9eba2afb0b,"@@ -1,3 +1,3 @@
 			public Function<HashSet<IndexedTypeIdentifier>, IndexedTypeSet> finisher() {
-				return IndexedTypesSets::fromSafeHashSet;
+				return IndexedTypeSets::fromSafeHashSet;
 			}",NotBuggy,"HSEARCH-1404 Rename IndexedTypesSets to IndexedTypeSets
",NotBuggy,"The commit message indicates a renaming operation from `IndexedTypesSets` to `IndexedTypeSets`. The provided diff shows a corresponding change in the code, where `IndexedTypesSets::fromSafeHashSet` is replaced with `IndexedTypeSets::fromSafeHashSet`. This aligns perfectly with the commit message. There's no indication of a bug fix, error handling, or logical correction. It's purely a renaming refactoring.

**NotBuggy**"
hibernate-search,6888.json,2bcf160fa7b80fa94734705d593e3cbb50ee7aa5,"@@ -1,50 +1,50 @@
 	private FetchingStrategy createCriteriaFetchingStrategy(
 			CacheMode cacheMode, int entityFetchSize, Integer maxResults) throws Exception {
 		Class<?> entityType = jobData.getEntityType( entityName );
 		Object upperBound = SerializationUtil.deserialize( serializedUpperBound );
 		Object lowerBound = SerializationUtil.deserialize( serializedLowerBound );
 		Set<Criterion> customQueryCriteria = jobData.getCustomQueryCriteria();
 
 		EntityTypeDescriptor typeDescriptor = jobData.getEntityTypeDescriptor( entityType );
 		IdOrder idOrder = typeDescriptor.getIdOrder();
 
 		return (session, lastCheckpointInfo) -> {
-			Criteria criteria = session.createCriteria( entityType );
+			Criteria criteria = new CriteriaImpl( entityType.getName(), session.unwrap( SessionImplementor.class ) );
 
 			// build orders for this entity
 			idOrder.addAscOrder( criteria );
 
 			// build criteria using job context data
 			customQueryCriteria.forEach( c -> criteria.add( c ) );
 
 			// build criteria using bounds
 			if ( upperBound != null ) {
 				criteria.add( idOrder.idLesser( upperBound ) );
 			}
 			if ( lastCheckpointInfo != null ) {
 				criteria.add( idOrder.idGreater( lastCheckpointInfo.getLastProcessedEntityId() ) );
 			}
 			else if ( lowerBound != null ) {
 				criteria.add( idOrder.idGreaterOrEqual( lowerBound ) );
 			}
 
 			if ( maxResults != null ) {
 				int remaining;
 				if ( lastCheckpointInfo != null ) {
 					remaining = maxResults - lastCheckpointInfo.getProcessedEntityCount();
 				}
 				else {
 					remaining = maxResults;
 				}
 				criteria.setMaxResults( remaining );
 			}
 
 			return criteria.setReadOnly( true )
 					.setCacheable( false )
 					.setLockMode( LockMode.NONE )
 					.setFlushMode( FlushMode.MANUAL )
 					.setCacheMode( cacheMode )
 					.setFetchSize( entityFetchSize )
 					.scroll( ScrollMode.FORWARD_ONLY );
 		};
 	}",NotBuggy,"HSEARCH-2910 JSR-352: Avoid logging warnings caused by internal usage of the legacy Criteria API
",Buggy,"The commit message indicates that the change aims to avoid logging warnings caused by internal usage of the legacy Criteria API. The diff replaces `session.createCriteria( entityType )` with `new CriteriaImpl( entityType.getName(), session.unwrap( SessionImplementor.class ) )`. This suggests that the original code was using a deprecated or internal method that caused warnings, and the change replaces it with a more appropriate implementation. This is likely a bug fix or a change to avoid future problems caused by the deprecated API.

**Buggy**"
hibernate-search,75.json,bb3f94df96e6e487ba231e4e201064cbb850aa02,"@@ -1,3 +1,3 @@
-	protected final void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
+	protected void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
 		luceneOptions.addNumericFieldToDocument( name, value, document );
 	}",NotBuggy,"HSEARCH-997 Adding short numeric bridge
",NotBuggy,"The commit message ""HSEARCH-997 Adding short numeric bridge"" suggests the addition of functionality related to handling short numeric values, likely for indexing purposes within a search context.

The provided code diff modifies the `applyToLuceneOptions` method by removing the `final` keyword. This change allows subclasses to override this method, potentially to provide specialized handling for numeric values, including short values.

The removal of `final` doesn't inherently indicate a bug fix. It's more likely a design decision to allow for greater flexibility and extensibility in how numeric values are processed. The change aligns with the commit message, as it could be a necessary step in implementing the ""short numeric bridge.""

Therefore, the change doesn't seem to be directly related to fixing a bug.

**NotBuggy**"
hibernate-search,5090.json,3bf6cd975dc1182ff31acb0eb5da642003664708,"@@ -1,3 +1,3 @@
-		public GetIndexMappingWork build() {
-			return new GetIndexMappingWork( this );
+		public GetIndexTypeMappingsWork build() {
+			return new GetIndexTypeMappingsWork( this );
 		}",NotBuggy,"HSEARCH-2581 Make GsonService part of the ElasticsearchWorkExecutionContext

And make JestAPIFormatter a utils class taking a GsonService as a parameter,
since it's only ever used where an execution context is available.
",Buggy,"The commit message indicates a refactoring of the `GsonService` and `JestAPIFormatter` classes within the context of Elasticsearch integration. The goal is to make `GsonService` part of the `ElasticsearchWorkExecutionContext` and to refactor `JestAPIFormatter` into a utility class that accepts a `GsonService` instance.

The provided diff shows a change from `GetIndexMappingWork` to `GetIndexTypeMappingsWork`. This change, while seemingly small, suggests a modification in how index mappings are retrieved, potentially to accommodate type-specific mappings or a broader range of mapping information. This could be related to a bug fix if the previous implementation (`GetIndexMappingWork`) was not correctly handling certain types of index mappings or was causing issues in specific scenarios.

Given the context of refactoring and the change in index mapping retrieval, it's plausible that this commit addresses a bug or limitation in the previous implementation.

**Buggy**"
hibernate-search,2698.json,45debe412916f6c6b4b4fbc84270dca378f0153d,"@@ -1,7 +1,7 @@
-	public Map<String, AnalyzerDef> build() {
-		Map<String, AnalyzerDef> result = new LinkedHashMap<>();
+	public SimpleLuceneAnalysisDefinitionRegistry build() {
+		SimpleLuceneAnalysisDefinitionRegistry registry = new SimpleLuceneAnalysisDefinitionRegistry();
 		for ( Map.Entry<String, LuceneAnalyzerDefinitionContextImpl> entry : definitions.entrySet() ) {
-			result.put( entry.getKey(), entry.getValue().build() );
+			registry.register( entry.getKey(), entry.getValue().build() );
 		}
-		return result;
+		return registry;
 	}",NotBuggy,"HSEARCH-2726 Store Lucene analyzer definitions in a dedicated class

This doesn't change much for now, but it will allow future commits
to add more information to the ""analysis definition registry"",
in particular normalizer definitions.
",NotBuggy,"The commit message indicates a refactoring to store Lucene analyzer definitions in a dedicated class to allow future extensions, particularly for normalizer definitions.

The code diff shows that the `build()` method's return type has changed from `Map<String, AnalyzerDef>` to `SimpleLuceneAnalysisDefinitionRegistry`. Instead of directly returning a map, it now creates a `SimpleLuceneAnalysisDefinitionRegistry` instance, registers the analyzer definitions using the `register` method, and returns the registry. This aligns with the commit message's intention to introduce a dedicated class for managing analyzer definitions. There is no indication of bug fixing in the code changes.

**NotBuggy**"
hibernate-search,4788.json,9451d18a3cf1f82dc134dcb6ddbd19ff771c1e03,"@@ -1,7 +1,7 @@
-		public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
-			if ( value == null ) {
-				return;
-			}
+	public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
+		if ( value == null ) {
+			return;
+		}
 
-			luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
-		}

+		luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
+	}",NotBuggy,"HSEARCH-2245 Handle Calendar and resolution in the Elasticsearch specific BridgeProvider

No tests yet as tests are blocked by HSEARCH-2246
",NotBuggy,"The commit message indicates changes related to handling `Calendar` and resolution in the Elasticsearch-specific `BridgeProvider`. The provided code diff only shows a minor formatting change (removing an empty line). This change doesn't seem to be related to handling `Calendar` or resolution. There's no error handling, logical correction, or exception handling improvement in the diff. Therefore, the diff doesn't indicate a bug fix.

**NotBuggy**"
hibernate-search,3749.json,0049cce1f0e0f6588e4f65bfa8f941267839a22e,"@@ -1,3 +1,3 @@
-		protected int size() {
-			return values.length;
-		}

+			public int size(int[] array) {
+				return array.length;
+			}",NotBuggy,"HSEARCH-2510 Avoid the extra iterable instantiation in CollectionHelper.iteratorFromArray

Because there will be someone who's not happy with that extra
instantiation, I'm sure.

Calls to array accessors seem to be inlined, so there shouldn't be much
execution overhead.
",NotBuggy,"The commit message indicates a performance optimization by avoiding an unnecessary object instantiation. The code diff shows the removal of a `size()` method within a class (presumably an anonymous class or a private inner class) and the introduction of a static `size(int[] array)` method. This change suggests that instead of creating an object with a `size()` method that accesses the array length, the code now directly calls a static method, passing the array as an argument. This avoids the object instantiation.

The commit message explicitly mentions avoiding an ""extra instantiation,"" and the code change directly reflects this goal. There's no indication of a bug fix in the traditional sense (no error handling, no logical correction, no exception handling). The change is purely aimed at improving performance by reducing object creation.

**NotBuggy**
"
hibernate-search,5464.json,ab4312a79532524ca5cf2bceaa1e173d4bd10630,"@@ -1,10 +1,8 @@
 		SearchResult scroll(String scrollId) {
-			try ( ServiceReference<ElasticsearchDialectProvider> dialectProvider =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchDialectProvider.class );
-					ServiceReference<ElasticsearchWorkProcessor> processor =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchWorkProcessor.class ) ) {
-				ElasticsearchWork<SearchResult> work = dialectProvider.get().getDialect().getWorkFactory()
+			try ( ServiceReference<ElasticsearchService> elasticsearchService =
+					getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchService.class ) ) {
+				ElasticsearchWork<SearchResult> work = elasticsearchService.get().getWorkFactory()
 						.scroll( scrollId, getScrollTimeout() ).build();
-				return processor.get().executeSyncUnsafe( work );
+				return elasticsearchService.get().getWorkProcessor().executeSyncUnsafe( work );
 			}
 		}",NotBuggy,"HSEARCH-2581 Make ElasticsearchService the single entry point for operations on the Elasticsearch server

This means in particular that the Elasticsearch service will manage the
bootstrapping of every component involved, which will allow us to avoid
circular dependencies in the next few commits.

Also, it seems cleaner, and it brings us one step closer to supporting
simulatenous access to multiple Elasticsearch clusters.
",NotBuggy,"The commit message indicates a refactoring effort to centralize Elasticsearch operations through `ElasticsearchService`. The diff shows that the code is being modified to use `ElasticsearchService` instead of directly accessing `ElasticsearchDialectProvider` and `ElasticsearchWorkProcessor`. The code now obtains an `ElasticsearchService` instance and uses it to create and execute the scroll work. This change aligns with the commit message's goal of making `ElasticsearchService` the single entry point. There is no indication of a bug fix in this change.
**NotBuggy**"
hibernate-search,1557.json,5cd16e6bac5161f228af80752581fd3bb0baa733,"@@ -1,4 +1,4 @@
 	public void initialize(Properties props, WorkerBuildContext context) {
 		// no-op
-		log.warn( ""initialized \""blackhole\"" backend. Index changes will be prepared but discarded!"" );
+		log.initializedBlackholeBackend();
 	}",NotBuggy,"HSEARCH-675 - Replace slf4j with JBoss Logging (except for archetype)
",NotBuggy,"Okay, let's analyze this Git diff and commit message.

**Commit Message Analysis:**

The commit message ""HSEARCH-675 - Replace slf4j with JBoss Logging (except for archetype)"" indicates a library replacement. The primary goal is to switch from SLF4j to JBoss Logging.

**Git Diff Analysis:**

The diff shows a change within the `initialize` method. Specifically, a logging statement is modified. The original statement `log.warn( ""initialized \""blackhole\"" backend. Index changes will be prepared but discarded!"" );` is replaced with `log.initializedBlackholeBackend();`.

**Reasoning:**

1.  **Library Replacement:** The commit message suggests a library replacement. The diff shows a change in the logging mechanism, which aligns with the commit message.
2.  **Logging Change:** The change involves replacing a direct `log.warn` call with a call to `log.initializedBlackholeBackend()`. This implies that the logging framework is being updated, and potentially pre-defined log messages are being used.
3.  **Bug Fix Indication:** While the change aligns with the commit message, it doesn't inherently indicate a bug fix. It's a refactoring/library update. The change from `log.warn` to `log.initializedBlackholeBackend()` is likely just a change in how the logging is handled after the library replacement. There's no indication of error handling updates, logical corrections, or exception handling improvements.

**Conclusion:**

NotBuggy
"
wildfly,13917.json,874e6032075a06b60e01bd96fe82d32a564bdc04,"@@ -1,2 +1,3 @@
-    protected void writeAttributes(final XMLExtendedStreamWriter writer, final SubsystemMarshallingContext context) throws XMLStreamException {
+    public void undeploy(final DeploymentUnit context) {
+
     }",NotBuggy,"AS7-4386 Add the option to configure a default distinct name for remote EJB's
",NotBuggy,"The commit message ""AS7-4386 Add the option to configure a default distinct name for remote EJB's"" suggests the addition of a configuration option related to remote EJB distinct names.

The provided code diff shows a seemingly unrelated change:

- `protected void writeAttributes(final XMLExtendedStreamWriter writer, final SubsystemMarshallingContext context) throws XMLStreamException {`
+ `public void undeploy(final DeploymentUnit context) {`
+
+ `}`

This diff replaces a method named `writeAttributes` with an empty `undeploy` method.  This change doesn't seem to be related to configuring a default distinct name for remote EJBs. The diff doesn't add any configuration options, modify EJB deployment logic, or deal with remote EJB naming. The change is more likely a refactoring or removal of code, and it's unclear if this is related to a bug fix. It's possible the original method was causing issues, but without more context, it's hard to say.

Given the disconnect between the commit message and the code change, and the lack of any obvious bug-related modifications in the diff, I'll assume this is not a bug fix.

**NotBuggy**
"
wildfly,7993.json,315a9c2129eb1ed4921ed4ea128e94c5014ff010,"@@ -1,8 +1,8 @@
-    static void addConnectorServiceConfigs(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
+    static void addConnectorServiceConfigurations(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
         if (model.hasDefined(CommonAttributes.CONNECTOR_SERVICE)) {
             final List<ConnectorServiceConfiguration> configs = configuration.getConnectorServiceConfigurations();
             for (Property prop : model.get(CommonAttributes.CONNECTOR_SERVICE).asPropertyList()) {
                 configs.add(createConnectorServiceConfiguration(context, prop.getName(), prop.getValue()));
             }
         }
     }",NotBuggy," [WFLY-12859]:  Refactoring code to centralize the configuration of the broker.
",NotBuggy,"The commit message indicates a refactoring to centralize broker configuration. The code diff shows a change in method name from `addConnectorServiceConfigs` to `addConnectorServiceConfigurations`. This change doesn't inherently indicate a bug fix. It's more likely a part of the refactoring process to improve code clarity or consistency. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**"
wildfly,21431.json,8da4de218c105bcf583f886e35a107b43708cdee,"@@ -1,38 +1,37 @@
     public void readElement(final XMLExtendedStreamReader reader, final ParseResult<JBossServiceXmlDescriptor> value) throws XMLStreamException {
         final JBossServiceXmlDescriptor serviceXmlDescriptor = new JBossServiceXmlDescriptor();
         final List<JBossServiceConfig> serviceConfigs = new ArrayList<JBossServiceConfig>();
         serviceXmlDescriptor.setServiceConfigs(serviceConfigs);
         value.setResult(serviceXmlDescriptor);
 
         final int count = reader.getAttributeCount();
         for(int i = 0; i < count; i++) {
-            final Attribute attribute = Attribute.of(reader.getAttributeName(i));
+            final QName attributeName = reader.getAttributeName(i);
+            final Attribute attribute = Attribute.of(attributeName);
             final String attributeValue = reader.getAttributeValue(i);
             switch(attribute) {
                 case MODE:
                     serviceXmlDescriptor.setControllerMode(JBossServiceXmlDescriptor.ControllerMode.of(attributeValue));
                     break;
-                default:
-                    throw unexpectedContent(reader);
             }
         }
 
         while (reader.hasNext()) {
             switch (reader.nextTag()) {
                 case COMMENT:
                     break;
                 case END_ELEMENT:
                     return;
                 case START_ELEMENT:
                     switch(Element.of(reader.getName())) {
                         case MBEAN:
                             serviceConfigs.add(parseMBean(reader));
                             break;
                         case UNKNOWN:
                             unexpectedContent(reader);
                             break;
                     }
                     break;
             }
         }
     }",NotBuggy,"Start to finish up parsed service deployment.  Testing, etc...
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Start to finish up parsed service deployment. Testing, etc..."" is quite vague. It suggests general work on parsing service deployments, including testing. The ""finish up"" part *could* imply addressing issues or bugs discovered during the process, but it's not explicit.

**Git Diff Analysis:**

The diff focuses on the `readElement` method, which appears to be part of an XML parsing process for JBoss service descriptors. Let's break down the changes:

1.  **Attribute Handling:**
    *   The code now retrieves the attribute name as a `QName` instead of directly creating an `Attribute` from it. This could be a change to handle namespaces or more complex attribute structures correctly.
    *   The `default` case in the `switch` statement for attributes is removed. This is significant. Previously, any unknown attribute would throw an `unexpectedContent` exception. Now, unknown attributes are silently ignored.
2.  **Element Handling:**
    *   The `UNKNOWN` case in the `switch` statement for elements calls `unexpectedContent(reader)`.

**Reasoning:**

The removal of the `default` case in the attribute handling switch statement is a strong indicator of a bug fix or at least a change in how the parser handles unexpected or unknown attributes. The original code would throw an exception if it encountered an attribute it didn't recognize. The new code simply ignores it.

This change suggests that the original code was too strict and was likely throwing exceptions when encountering valid but unexpected attributes in the XML. The change makes the parser more lenient. This is a common type of bug fix.

The change to use `QName` could be related to handling XML namespaces correctly, which is often a source of bugs in XML parsing.

**Conclusion:**

Based on the code changes, specifically the removal of the exception throwing for unknown attributes and the use of `QName`, it's likely that this commit addresses a bug related to how the XML parser handles unexpected or namespaced attributes. Therefore, I conclude:

**Buggy**
"
wildfly,9094.json,98592f3855adb79a6b9af100072db28f290929e5,"@@ -1,3 +1,3 @@
-    public Context getValue() throws IllegalStateException {
-        return context;
+    public ComponentView getValue() throws IllegalStateException, IllegalArgumentException {
+        return instance;
     }",NotBuggy,"Rework EE Component model for extensibility into EJBs and beyond
",NotBuggy,"The commit message ""Rework EE Component model for extensibility into EJBs and beyond"" suggests a structural change or refactoring of the EE Component model to make it more extensible. The provided diff shows a change in the return type of the `getValue()` method from `Context` to `ComponentView` and the addition of `IllegalArgumentException` to the throws clause.

Reasoning:

1.  **Return Type Change:** Changing the return type from `Context` to `ComponentView` indicates a significant change in the type of object being returned. This aligns with the ""rework"" aspect of the commit message, suggesting a modification to the component model's structure.
2.  **Exception Handling:** Adding `IllegalArgumentException` to the `throws` clause suggests that the method now has the potential to throw this exception under certain circumstances. This could be due to stricter validation or different handling of invalid arguments as part of the refactoring. The addition of an exception suggests that the previous implementation might have had a bug or a lack of proper error handling that is now being addressed.

Based on the changes, it's difficult to definitively say whether it's a bug fix. The addition of `IllegalArgumentException` suggests improved error handling, which could be in response to a previously unfixed bug. However, the primary focus seems to be on refactoring and extensibility.

Conclusion: **NotBuggy**
"
wildfly,1756.json,546d5ace217411e91be9573c4ddf0c4e88fc6654,"@@ -1,17 +1,19 @@
     private void parseBackups(XMLExtendedStreamReader reader, PathAddress cacheAddress, Map<PathAddress, ModelNode> operations) throws XMLStreamException {
 
-        PathAddress backupsAddress = cacheAddress.append(BackupsResourceDefinition.PATH);
+        PathAddress address = cacheAddress.append(BackupsResourceDefinition.PATH);
+        ModelNode operation = Util.createAddOperation(address);
+        operations.put(address, operation);
 
         while (reader.hasNext() && (reader.nextTag() != XMLStreamConstants.END_ELEMENT)) {
             XMLElement element = XMLElement.forName(reader.getLocalName());
             switch (element) {
                 case BACKUP: {
-                    this.parseBackup(reader, backupsAddress, operations);
+                    this.parseBackup(reader, address, operations);
                     break;
                 }
                 default: {
                     throw ParseUtils.unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5672 Implicitly added resources should be added by parent add operation instead of the parser
",Buggy,"The commit message indicates that resources implicitly added should be added by the parent add operation instead of the parser. The code diff shows a change in the `parseBackups` method. Specifically, it appears that an add operation is now being created for the `BackupsResourceDefinition.PATH` and added to the `operations` map. The `parseBackup` method is then called with the address of the backups resource. This change aligns with the commit message, suggesting a modification in how resources are added, potentially to correct an issue where resources were not being added correctly or were being added in the wrong order. This looks like a bug fix.

**Buggy**"
wildfly,14902.json,d9e37a5a768a6ba4f4a3fdd692cab4d5d6064667,"@@ -1,8 +1,8 @@
             public ManagedReference getReference() {
                 try {
-                    final Object value = objectFactoryClassInstance.getObjectInstance(name, null, null, environment);
+                    final Object value = new InitialContext().lookup(lookup);
                     return new ValueManagedReference(new ImmediateValue<Object>(value));
                 } catch (Exception e) {
                     throw new RuntimeException(e);
                 }
             }",NotBuggy,"Revert ""AS7-6121: re-implements subsystem 'lookup' binding service with a linkref, allowing Context#lookupLink() to return the link name""

This reverts commit 9ce9d0010c78fe957fc92b4180e53ea60702994d.
",Buggy,"The commit message indicates a reversion of a previous commit that reimplemented a subsystem. The diff shows that the code is reverting from using `objectFactoryClassInstance.getObjectInstance` to `new InitialContext().lookup(lookup)`. This suggests that the previous implementation might have introduced issues, prompting a return to the original approach. The try-catch block remains the same, indicating no changes in error handling. The change itself suggests a potential bug fix by reverting to a known working state.

**Buggy**"
wildfly,12438.json,92d027aaed28b83a6057d45690c0bbf8b5cecbaa,"@@ -1,4 +1,4 @@
         void close() {
-            this.clientMappingRegistry.removeListener(this);
+            this.listenerRegistration.close();
             this.clusterTopologyListeners.clear();
         }",NotBuggy,"Update usage of deprecated Group listener API.
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** ""Update usage of deprecated Group listener API."" This message clearly states that the purpose of the commit is to replace the usage of a deprecated API with a newer, presumably recommended, API.

2.  **Code Diff:**
    *   `- this.clientMappingRegistry.removeListener(this);`
    *   `+ this.listenerRegistration.close();`

    The code diff shows that a call to `removeListener` on `clientMappingRegistry` is being replaced with a call to `close` on `listenerRegistration`. This strongly suggests that `removeListener` is the deprecated API being replaced. The `listenerRegistration` object likely represents a registration object returned when the listener was initially added, and closing the registration is the new way to unregister the listener.

3.  **Bug Fix Indication:** While the commit message indicates a change due to API deprecation, it doesn't inherently imply a bug fix. Deprecation doesn't always mean the old API is broken, just that it's no longer the preferred way and might be removed in the future. However, using deprecated APIs can lead to bugs or compatibility issues down the line. Updating to the new API is a preventative measure.

4. **Error Handling/Logic Correction:** The change doesn't directly involve error handling or a logical correction in the traditional sense of fixing a flaw in the algorithm. It's more about adapting to a new API to avoid potential future issues.

**Conclusion:**

The change is to update a deprecated API. While this is a good practice, it doesn't indicate a bug fix in the sense of correcting an existing error. Therefore, the answer is **NotBuggy**."
wildfly,13536.json,b63c847db3fc4c885c809d837aee4361711166f8,"@@ -1,11 +1,11 @@
         void executeRuntime(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final String opName = operation.require(ModelDescriptionConstants.OP).asString();
             if (!opName.equals(ModelDescriptionConstants.READ_ATTRIBUTE_OPERATION)) {
-                throw MESSAGES.unknownOperations(opName);
+                throw EjbLogger.ROOT_LOGGER.unknownOperations(opName);
             }
 
             final TimerImpl timer = getTimer(context, operation);
 
             readAttribute(timer, context.getResult());
             context.completeStep(OperationContext.RollbackHandler.NOOP_ROLLBACK_HANDLER);
         }",NotBuggy,"[WFLY-2864] WildFly EJB module
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""[WFLY-2864] WildFly EJB module"" is quite generic. It indicates a change within the WildFly EJB module but doesn't specify the nature of the change (bug fix, feature, refactoring, etc.).

2.  **Code Diff:** The code diff shows a change in the `executeRuntime` method. Specifically, the exception thrown when an unknown operation is encountered has been modified.

    *   **Original Code:** `throw MESSAGES.unknownOperations(opName);`
    *   **Modified Code:** `throw EjbLogger.ROOT_LOGGER.unknownOperations(opName);`

    The change replaces a direct reference to `MESSAGES` with `EjbLogger.ROOT_LOGGER`. This suggests a change in how logging or error reporting is handled. It's likely that `EjbLogger` is a more centralized or standardized logging mechanism within the WildFly EJB module.

3.  **Reasoning:**

    *   The change involves exception handling. While not a direct bug fix in the sense of correcting a calculation or logic error, it addresses how errors are reported.
    *   The change likely aims to improve the clarity, consistency, or maintainability of error logging within the EJB module.
    *   The change from `MESSAGES` to `EjbLogger.ROOT_LOGGER` suggests a shift towards a more structured logging approach, potentially addressing a previous inconsistency or deficiency in error reporting.

**Conclusion:**

The code change, while small, indicates an improvement in error handling and logging. This kind of change is often made to address a deficiency in how errors were previously reported, which can be considered a bug fix in a broader sense.

**Buggy**
"
wildfly,9080.json,c4dceafe3f52e61a8bb318f10722eae193da8c62,"@@ -1,3 +1,3 @@
-    protected AtomicReference<ManagedReference> getInstanceReference() {
-        return instanceReference;
+    public Object getInstanceData(Object key) {
+        return instanceData.get(key);
     }",NotBuggy,"WFLY-2021 Change EE interceptors to be stateless

All state is now stored on the ComponentInstance, and can be setup during the post construct
phase. This largely removes the need for InterceptorFactory. At most a single instance of
each interceptor chain will be created per component, this should result in a massive decrease
in the amount of memory a component instance, and an instance reference allocates.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""WFLY-2021 Change EE interceptors to be stateless"" clearly indicates a refactoring effort focused on improving memory usage. The core idea is to make EE interceptors stateless, storing state on the `ComponentInstance` instead. This aims to reduce the number of interceptor instances created and, consequently, the memory footprint. The message highlights the removal of `InterceptorFactory` and the reduction in memory allocation per component instance.

**Git Diff Analysis:**

The diff shows a change from `getInstanceReference()` returning an `AtomicReference<ManagedReference>` to `getInstanceData(Object key)` returning a generic `Object`.  The original code used an `AtomicReference` to hold a `ManagedReference`, which likely represented a managed instance of the interceptor. The new code uses a `instanceData` which is likely a `Map` to store data associated with a given key.

**Reasoning:**

1.  **Refactoring for Performance:** The commit message explicitly states the goal of reducing memory usage. The code change supports this by suggesting a shift from managing interceptor instances directly with `AtomicReference<ManagedReference>` to storing instance data in a more generic way, potentially within a `Map` on the `ComponentInstance`.

2.  **Statelessness and Data Storage:** The move to stateless interceptors implies that any data previously held within the interceptor instance itself is now stored elsewhere (the `ComponentInstance`). The `getInstanceData` method provides a way to access this stored data using a key.

3.  **No Direct Bug Fix Indication:** While performance improvements can sometimes indirectly address bugs (e.g., memory leaks leading to crashes), the primary focus here is on optimization rather than fixing a specific, identified bug. The commit message doesn't mention any bug being resolved.

**Conclusion:**

The changes are related to a refactoring effort aimed at improving memory usage by making EE interceptors stateless. There's no explicit indication of a bug fix in the commit message or the code diff.

**NotBuggy**
"
wildfly,14713.json,45fe4a432cf42b096be038ece2ff80b69c9d930c,"@@ -1,3 +1,3 @@
-    public void rebind(final String name, final Object obj) throws NamingException {
-        throw MESSAGES.readOnlyNamingContext();
+    public void rebind(final String name, final Object object) throws NamingException {
+        rebind(parseName(name), object);
     }",NotBuggy,"Introduce writable naming context
",Buggy,"The commit message ""Introduce writable naming context"" suggests the introduction of functionality to allow modifications (writes) to the naming context.

The code diff shows that the `rebind` method, which is used to bind a name to an object in the naming context, was previously throwing an exception indicating a read-only context. The change replaces this with an implementation that actually performs the rebind operation. This aligns perfectly with the commit message, as it introduces the ability to write to the naming context. The previous implementation was clearly a limitation or a bug, as it prevented writing to the context. The new implementation fixes this by providing the intended functionality.

**Buggy**
"
wildfly,13160.json,8fec6d40a157018646ee131584fe5b88e31ef230,"@@ -1,53 +1,56 @@
     protected void parseRemote(final XMLExtendedStreamReader reader, List<ModelNode> operations) throws XMLStreamException {
         final int count = reader.getAttributeCount();
         final PathAddress ejb3RemoteServiceAddress = SUBSYSTEM_PATH.append(SERVICE, REMOTE);
         ModelNode operation = Util.createAddOperation(ejb3RemoteServiceAddress);
         final EnumSet<EJB3SubsystemXMLAttribute> required = EnumSet.of(EJB3SubsystemXMLAttribute.CONNECTOR_REF,
                 EJB3SubsystemXMLAttribute.THREAD_POOL_NAME);
         for (int i = 0; i < count; i++) {
             requireNoNamespaceAttribute(reader, i);
             final String value = reader.getAttributeValue(i);
             final EJB3SubsystemXMLAttribute attribute = EJB3SubsystemXMLAttribute.forName(reader.getAttributeLocalName(i));
             required.remove(attribute);
             switch (attribute) {
                 case CLIENT_MAPPINGS_CLUSTER_NAME:
                     EJB3RemoteResourceDefinition.CLIENT_MAPPINGS_CLUSTER_NAME.parseAndSetParameter(value, operation, reader);
                     break;
                 case CONNECTOR_REF:
                     EJB3RemoteResourceDefinition.CONNECTOR_REF.parseAndSetParameter(value, operation, reader);
                     break;
                 case THREAD_POOL_NAME:
                     EJB3RemoteResourceDefinition.THREAD_POOL_NAME.parseAndSetParameter(value, operation, reader);
                     break;
+                case EXECUTE_IN_WORKER:
+                    EJB3RemoteResourceDefinition.EXECUTE_IN_WORKER.parseAndSetParameter(value, operation, reader);
+                    break;
                 default:
                     throw unexpectedAttribute(reader, i);
             }
         }
         if (!required.isEmpty()) {
             throw missingRequired(reader, required);
         }
         // each profile adds it's own operation
         operations.add(operation);
 
         final Set<EJB3SubsystemXMLElement> parsedElements = new HashSet<EJB3SubsystemXMLElement>();
         while (reader.hasNext() && reader.nextTag() != XMLStreamConstants.END_ELEMENT) {
             EJB3SubsystemXMLElement element = EJB3SubsystemXMLElement.forName(reader.getLocalName());
             switch (element) {
                 case CHANNEL_CREATION_OPTIONS: {
                     if (parsedElements.contains(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS)) {
                         throw unexpectedElement(reader);
                     }
                     parsedElements.add(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS);
                     this.parseChannelCreationOptions(reader, ejb3RemoteServiceAddress, operations);
                     break;
                 }
                 case PROFILES: {
                     parseProfiles(reader, operations);
                     break;
                 }
                 default: {
                     throw unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5886 Remote EJB dispatches to thread pools twice
",Buggy,"The commit message ""WFLY-5886 Remote EJB dispatches to thread pools twice"" suggests a bug fix related to remote EJB calls being dispatched to thread pools more than once.

The diff adds `EXECUTE_IN_WORKER` to the attributes that can be parsed for the `remote` service configuration. This attribute likely controls whether the EJB execution happens in a worker thread. The addition of this attribute suggests a change in how remote EJB calls are handled, potentially addressing the double dispatch issue described in the commit message.

Given the commit message and the code changes, it's highly likely that this commit is a bug fix. The new `EXECUTE_IN_WORKER` attribute probably allows configuring whether the remote EJB calls are dispatched to a thread pool, thereby preventing the double dispatch.

**Buggy**"
wildfly,777.json,59f6d84cb689c50e44f68aeacd5a41d7e0056fd6,"@@ -1,6 +1,8 @@
     public void schedule(I id) {
-        BeanEntry<I> entry = this.factory.findValue(id);
-        if (entry != null) {
-            this.schedule(id, entry);
+        try (Batch batch = this.batcher.createBatch()) {
+            BeanEntry<I> entry = this.factory.findValue(id);
+            if (entry != null) {
+                this.schedule(id, entry);
+            }
         }
     }",NotBuggy,"WFLY-12587 Distributed web/SFSB expiration after failover schedules premature expiration
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""WFLY-12587 Distributed web/SFSB expiration after failover schedules premature expiration"" clearly states that there was a bug related to premature expiration of distributed web sessions or stateful session beans (SFSB) after a failover event. This strongly suggests a bug fix.

**Git Diff Analysis:**

The diff shows a change in the `schedule` method. The original code directly finds a `BeanEntry` and schedules it. The modified code introduces a `try-with-resources` block using a `Batch` object. This `Batch` object is created using `this.batcher.createBatch()`.

The addition of the `try-with-resources` block suggests that the `Batch` object manages some resources that need to be properly closed or released after the scheduling operation, even if an exception occurs. This is a common pattern for ensuring resource cleanup and preventing leaks.

**Reasoning:**

The commit message indicates a problem with premature expiration. The code change introduces a `Batch` object within a `try-with-resources` block. This suggests that the original code might have been missing a crucial step in managing the expiration process, possibly related to resource handling or synchronization. The `Batch` object likely ensures that all necessary actions related to scheduling and expiration are completed atomically or in a consistent manner, preventing premature expiration in distributed environments after a failover. The `Batch` object likely interacts with the distributed cache or session management system to ensure proper expiration behavior.

**Conclusion:**

The commit message describes a bug, and the code changes introduce a mechanism (the `Batch` object and `try-with-resources` block) that likely addresses the root cause of the premature expiration issue. Therefore, the changes indicate a bug fix.

**Buggy**
"
wildfly,16565.json,01a8fd7e3132ffed1bf87443991c7560051b2156,"@@ -1,3 +1,3 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getStatus().name()));
+                context.getResult().set(new ModelNode(ctx.isQueueNewRequests()));
             }",NotBuggy,"Add some more mod_cluster information
",Buggy,"The commit message ""Add some more mod_cluster information"" suggests that the commit aims to include additional details or data related to mod_cluster.

The code diff shows a change in the `handleNode` method. Specifically, the line `context.getResult().set(new ModelNode(ctx.getStatus().name()));` is replaced with `context.getResult().set(new ModelNode(ctx.isQueueNewRequests()));`.

The original code was setting the result to the name of the node's status. The modified code sets the result to a boolean value indicating whether the node is queuing new requests. This change seems to be providing different information about the mod_cluster node, which aligns with the commit message. It's not immediately apparent if this change is a bug fix. The original code might have been displaying incorrect or less useful information, and this change improves the information provided. It's possible that the previous status was misleading or incorrect, and the new boolean value provides a more accurate representation of the node's state.

Given that the change replaces one piece of information with another, and the commit message indicates adding more information, it's plausible that the original information was deemed incorrect or insufficient, suggesting a potential bug fix.

**Buggy**"
wildfly,8286.json,cc78bb76673be1d963bb01205cd4e5dcce1c3949,"@@ -1,41 +1,41 @@
     private ValueConfig parseValueValue(final XMLExtendedStreamReader reader) throws XMLStreamException {
         ValueConfig value = null;
         while (reader.hasNext()) {
             switch (reader.next()) {
                 case END_ELEMENT:
                     if (value == null)
                         throw new IllegalArgumentException(""Null value: "" + reader.getLocation());
                     return value;
                 case START_ELEMENT:
-                    switch (Element.of(reader.getName())) {
+                    switch (Element.of(reader.getLocalName())) {
                         case VALUE:
                             value = parseValue(reader);
                             break;
                         case INJECT:
                             value = parseInject(reader);
                             break;
                         case VALUE_FACTORY:
                             value = parseValueFactory(reader);
                             break;
                         case LIST:
                             value = parseList(reader);
                             break;
                         case SET:
                             value = parseSet(reader);
                             break;
                         case MAP:
                             value = parseMap(reader);
                             break;
                         default:
                             throw unexpectedElement(reader);
                     }
                     break;
                 case CHARACTERS:
                     StringValueConfig svc = new StringValueConfig();
                     svc.setValue(reader.getText());
                     value = svc;
                     break;
             }
         }
         throw unexpectedElement(reader);
     }",NotBuggy,"Allow for legacy MC bean deployment to be parsed as well.
",Buggy,"The commit message ""Allow for legacy MC bean deployment to be parsed as well"" suggests that the changes are related to parsing older versions of MC bean deployments. The diff focuses on the `parseValueValue` method, which is responsible for parsing value configurations from an XML stream. The key change is in the `START_ELEMENT` case, where `reader.getName()` is replaced with `reader.getLocalName()`. This indicates that the code was previously relying on the full qualified name of the XML element, but now it's using only the local name. This change is likely to handle XML documents where the namespace prefixes might be different or missing, which could be a characteristic of older or ""legacy"" deployments. The original code might have failed to parse these older deployments due to the namespace differences. Therefore, this change is likely a bug fix to allow parsing of legacy MC bean deployments.

**Buggy**"
wildfly,16563.json,dee5c658c668a3752e7273dc0b271b84b16aa753,"@@ -1,3 +1,7 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getUri().toString()));
+                ModelNode list = new ModelNode(ModelType.LIST);
+                for(String alias : ctx.getAliases()) {
+                    list.add(alias);
+                }
+                context.getResult().set(list);
             }",NotBuggy,"WFLY-4992 add nodes alises
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""WFLY-4992 add nodes alises"" suggests that the change is related to adding or modifying node aliases within the WildFly mod\_cluster subsystem. The use of ""add"" implies a new feature or enhancement rather than a bug fix. However, it could also be fixing an omission.

2.  **Code Diff:**
    *   The original code `context.getResult().set(new ModelNode(ctx.getUri().toString()));` sets the result to the URI of the node.
    *   The modified code introduces a `ModelNode` of type `LIST`. It iterates through the `ctx.getAliases()` and adds each alias to the list. Finally, it sets the result to this list.

3.  **Analysis:**
    *   The code change replaces the URI with a list of aliases. This suggests that the original code was incomplete or incorrect, as it only provided the URI and not the aliases. The change introduces the functionality to expose node aliases.
    *   The commit message aligns with the code changes. The code now includes aliases, which were previously missing.
    *   The change seems to address a missing feature or incomplete implementation rather than fixing a specific error or fault. It's more of an enhancement.

**Conclusion:**

**NotBuggy**
"
javaparser,4978.json,ee156d8b09ad78f5eb75e0cc1b8d74999f16cafe,"@@ -1,3 +1,3 @@
-		public Pos transform(Pos pos) {
+		public Position transform(Position pos) {
 			return lookup(pos).transform(pos);
 		}",NotBuggy,Apply recorded position mapping in a parser post-processing phase.,NotBuggy,"The commit message ""Apply recorded position mapping in a parser post-processing phase"" suggests a change related to position mapping during parsing.

The code diff shows a change from `Pos` to `Position` in the `transform` method's signature. This could be a renaming or type change related to how positions are handled. While it aligns with the general idea of position mapping, it doesn't inherently indicate a bug fix. It could be a refactoring or part of a larger feature implementation. There's no explicit error handling, logical correction, or exception handling improvement visible in this isolated diff.

**NotBuggy**
"
javaparser,6046.json,dcf4b43e5c2f05d22bad9567dcd98d9cdf707c82,"@@ -1,4 +1,4 @@
     public SymbolReference<ResolvedMethodDeclaration> solveMethod(String name, List<ResolvedType> argumentsTypes,
-                                                                  boolean staticOnly, TypeSolver typeSolver) {
-        return getContext().solveMethod(name, argumentsTypes, staticOnly, typeSolver);
+                                                                  boolean staticOnly) {
+        return getContext().solveMethod(name, argumentsTypes, staticOnly);
     }",NotBuggy,"#1910 Removed TypeSolver parameter from various methods.
",NotBuggy,"The commit message states that the `TypeSolver` parameter was removed from various methods. The provided diff shows that indeed the `TypeSolver` parameter was removed from the `solveMethod` method. This aligns with the commit message. The change itself doesn't inherently indicate a bug fix, but rather a refactoring or simplification of the API. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**
"
javaparser,166.json,adb78b277512189cc5f8ecb076d9bf8106f9e1e0,"@@ -1,3 +1,3 @@
-    public boolean isWhiteSpaceOrComment() {
-        return child instanceof Comment;
+    public final boolean isWhiteSpaceOrComment() {
+        return isWhiteSpace() || isComment();
     }",NotBuggy,"issue823: distinguish between spaces and comments
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""issue823: distinguish between spaces and comments"" suggests that the previous implementation incorrectly treated spaces and comments as the same thing, and this commit aims to differentiate them. This strongly hints at a bug fix.

**Diff Analysis:**

The code diff shows a change in the `isWhiteSpaceOrComment()` method.

*   **Original Code:**
    ```java
    public boolean isWhiteSpaceOrComment() {
        return child instanceof Comment;
    }
    ```
    This code only checks if the `child` is an instance of `Comment`. It doesn't consider whitespace at all. This confirms the issue described in the commit message.

*   **Modified Code:**
    ```java
    public final boolean isWhiteSpaceOrComment() {
        return isWhiteSpace() || isComment();
    }
    ```
    The modified code now explicitly checks if the element `isWhiteSpace()` or `isComment()`. This aligns perfectly with the commit message's intention to distinguish between spaces and comments. The addition of `isWhiteSpace()` addresses the original issue of not considering whitespace. The `final` keyword addition is a minor change that doesn't affect the bug fix aspect.

**Reasoning:**

The original code incorrectly identified only comments, neglecting whitespace. The commit message clearly states the intention to differentiate between spaces and comments. The modified code directly addresses this by adding a check for whitespace. This indicates that the previous implementation was flawed (a bug), and the current commit fixes it.

**Conclusion:**

**Buggy**
"
javaparser,5042.json,26b23bccac8c67aed1023f0b2c142c3a7f3d5f08,"@@ -1,23 +1,22 @@
     public List<ParseResult<CompilationUnit>> tryToParseParallelized(String startPackage) throws IOException {
         assertNotNull(startPackage);
         logPackage(startPackage);
         final Path path = packageAbsolutePath(root, startPackage);
-        ParallelParse parse = new ParallelParse(path, new ParallelParse.VisitFileCallback() {
-            @Override
-            public FileVisitResult process(Path file, BasicFileAttributes attrs) {
-                if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
-                    Path relative = root.relativize(file.getParent());
-                    try {
-                        tryToParse(relative.toString(), file.getFileName().toString(), new JavaParser(
-                                SourceRoot.this.javaParser.getParserConfiguration()));
-                    } catch (IOException e) {
-                        Log.error(e);
-                    }
+        ParallelParse parse = new ParallelParse(path, (file, attrs) -> {
+            if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
+                Path relative = root.relativize(file.getParent());
+                try {
+                    tryToParse(
+                            relative.toString(),
+                            file.getFileName().toString(),
+                            new JavaParser(parserConfiguration));
+                } catch (IOException e) {
+                    Log.error(e);
                 }
-                return FileVisitResult.CONTINUE;
             }
+            return FileVisitResult.CONTINUE;
         });
         ForkJoinPool pool = new ForkJoinPool();
         pool.invoke(parse);
         return getCache();
     }",NotBuggy,"Redo SourceRoot again
",NotBuggy,"The commit message ""Redo SourceRoot again"" is vague and doesn't provide specific details about the changes made.

The code diff primarily involves replacing an anonymous inner class with a lambda expression for the `VisitFileCallback` in the `ParallelParse` constructor. The functionality remains the same: it iterates through files, checks if they are Java files, and attempts to parse them. The exception handling (`Log.error(e)`) within the `try-catch` block remains unchanged. The core logic of parsing and error handling is consistent before and after the change. The change is primarily a refactoring to use a lambda expression instead of an anonymous inner class. There's no indication of a bug fix in this diff.

**NotBuggy**"
javaparser,3272.json,6d06ae94c521628da4bcd76675f3660fee8ebf76,"@@ -1,3 +1,3 @@
-    public ModuleProvidesStmtMetaModel getMetaModel() {
-        return JavaParserMetaModel.moduleProvidesStmtMetaModel;
+    public ModuleProvidesDirectiveMetaModel getMetaModel() {
+        return JavaParserMetaModel.moduleProvidesDirectiveMetaModel;
     }",NotBuggy,"Module statements -> directives
",NotBuggy,"The commit message ""Module statements -> directives"" suggests a renaming or refactoring of code related to module declarations, specifically changing ""statements"" to ""directives.""

The code diff shows a change from `ModuleProvidesStmtMetaModel` to `ModuleProvidesDirectiveMetaModel` and `moduleProvidesStmtMetaModel` to `moduleProvidesDirectiveMetaModel`. This aligns perfectly with the commit message, indicating a renaming of a class and its corresponding static field to use the term ""directive"" instead of ""statement.""

There's no indication of error handling, logical corrections, or exception handling. It's a straightforward renaming operation. Therefore, it's unlikely to be a bug fix.

**NotBuggy**
"
javaparser,225.json,c806b3355d14040857d754094f07595d7f84e02c,"@@ -1,3 +1,3 @@
-    boolean isProcessed() {
+    final boolean isProcessed() {
         return isProcessed;
     }",NotBuggy,"JavaDoc, newline at end of files and made methods final
",NotBuggy,"The commit message indicates changes related to JavaDoc, newline characters at the end of files, and making methods final. The provided diff shows a method being made final. This aligns with the commit message. There's no indication of bug fixes, error handling, or logical corrections in the diff. The change is purely related to code styling and access modifiers.

**NotBuggy**
"
javaparser,6917.json,37f742068fc5f198eef8c019d7f9673423c0b36a,"@@ -1,13 +1,13 @@
-    static String extractParameterName(CtBehavior method, int paramNumber) {
+    static Optional<String> extractParameterName(CtBehavior method, int paramNumber) {
         MethodInfo methodInfo = method.getMethodInfo();
         CodeAttribute codeAttribute = methodInfo.getCodeAttribute();
         if (codeAttribute != null) {
             LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute
                     .tag);
             if (attr != null) {
                 int pos = Modifier.isStatic(method.getModifiers()) ? 0 : 1;
-                return attr.variableName(paramNumber + pos);
+                return Optional.ofNullable(attr.variableName(paramNumber + pos));
             }
         }
-        return null;
+        return Optional.empty();
     }",NotBuggy,"Adding javadoc comment to explain why the name might not be available.

Changed return value from String to Optional to avoid null. Renamed tests to conform to code style.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message states three things:

1.  ""Adding javadoc comment to explain why the name might not be available."" - This suggests adding documentation to clarify a potential issue.
2.  ""Changed return value from String to Optional to avoid null."" - This is a significant change. Returning `Optional` instead of `String` avoids the possibility of returning `null`, which is a common source of `NullPointerException` and thus a potential bug.
3.  ""Renamed tests to conform to code style."" - This is a refactoring change and doesn't directly indicate a bug fix.

**Git Diff Analysis:**

The diff shows the following changes in the `extractParameterName` method:

1.  **Return Type Change:** The return type is changed from `String` to `Optional<String>`.
2.  **Null Handling:** The original code returned `null` when the parameter name couldn't be extracted. The modified code now returns `Optional.empty()` in the same scenario.
3.  **Optional Wrapping:** The successful return value `attr.variableName(paramNumber + pos)` is now wrapped in `Optional.ofNullable()`.

**Reasoning:**

The change from `String` to `Optional<String>` is a strong indicator of a bug fix or at least a bug prevention measure. Returning `null` is generally discouraged in modern Java development because it forces clients of the method to explicitly check for `null` to avoid `NullPointerException`. By using `Optional`, the code explicitly signals that the parameter name might not be available, and clients can handle this case in a more robust and type-safe way. The `Optional.ofNullable()` handles the case where `attr.variableName()` might return null.

The commit message explicitly mentions avoiding `null`, which aligns perfectly with the code changes. The original code's potential to return `null` can be considered a bug (or at least a potential source of bugs). The change to `Optional` addresses this.

**Conclusion:**

The changes clearly indicate a bug fix by preventing a potential `NullPointerException` and improving the robustness of the code.

**Buggy**
"
javaparser,535.json,621f1f76216a62bf1631fb337ba6c126c326a0cc,"@@ -1,5 +1,5 @@
     public String print(Node node) {
-        final PrettyPrintVisitor visitor = configuration.getVisitorFactory().apply(configuration);
+        final VoidVisitor<Void> visitor = configuration.getVisitorFactory().apply(configuration);
         node.accept(visitor, null);
-        return visitor.getSource();
+        return visitor.toString();
     }",NotBuggy,"Make toString's printer configuration accessible
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Make toString's printer configuration accessible"" suggests that the primary goal is to expose or allow access to the configuration used by the `toString` method (or a method that generates a string representation). It doesn't explicitly mention a bug fix, but rather an enhancement or refactoring related to configuration access.

**Git Diff Analysis:**

The diff shows changes within a `print` method that takes a `Node` as input and returns a String.

1.  **`PrettyPrintVisitor` to `VoidVisitor<Void>`:** The type of the `visitor` variable is changed from `PrettyPrintVisitor` to `VoidVisitor<Void>`. This suggests a change in the visitor's role or implementation. It could be a refactoring to use a more generic visitor type.
2.  **`visitor.getSource()` to `visitor.toString()`:** The method called to retrieve the string representation is changed from `getSource()` to `toString()`. This is a significant change. It implies that the `PrettyPrintVisitor` (now `VoidVisitor<Void>`) no longer has a `getSource()` method or that `toString()` is now the preferred way to get the string representation. This could be due to a change in the visitor's internal structure or a move towards standard Java practices.

**Reasoning:**

The change from `getSource()` to `toString()` is the most telling part of the diff.  It's possible that `getSource()` was removed or deprecated in favor of the standard `toString()` method.  This could be a bug fix if `getSource()` was not behaving as expected or was causing issues. The change in visitor type could be related to this refactoring. The commit message doesn't explicitly mention a bug, but the change in method used to obtain the string representation hints at a potential issue with the previous approach. It's plausible that `getSource()` had a bug or limitation that `toString()` resolves.

**Conclusion:**

**Buggy**
"
javaparser,6134.json,5d8d0cb7947efcf2282f7d056302dd905f348532,"@@ -1,3 +1,3 @@
-    public ClassDeclaration declaringType() {
+    public ResolvedClassDeclaration declaringType() {
         return classDeclaration;
     }",NotBuggy,"more work on converting to moved classes
",NotBuggy,"The commit message ""more work on converting to moved classes"" suggests a refactoring or code modernization effort, specifically focusing on classes that have been moved or renamed. The diff shows a change in the return type of the `declaringType()` method from `ClassDeclaration` to `ResolvedClassDeclaration`. This aligns with the commit message, as it indicates a change related to class types, potentially due to classes being moved or refactored. There's no explicit indication of a bug fix in the traditional sense (e.g., fixing a wrong calculation, handling an exception, etc.). It's more about adapting the code to a new class structure.

**NotBuggy**
"
javaparser,5700.json,e15d5a8aa84a34917e4ef47135dfc62e58ea7650,"@@ -1,3 +1,3 @@
-    public List<Type> getFormalArgumentTypes() {
+    public List<ResolvedType> getFormalArgumentTypes() {
         return formalArgumentTypes;
     }",NotBuggy,"correcting module core to use new classes
",NotBuggy,"The commit message ""correcting module core to use new classes"" suggests a change to incorporate new classes into the core module. The provided diff shows a change in the return type of the `getFormalArgumentTypes` method from `List<Type>` to `List<ResolvedType>`. This indicates that the code is being updated to use a new class `ResolvedType` instead of `Type`. This change aligns with the commit message. There is no explicit indication of a bug fix in the traditional sense (error handling, logical correction, exception handling). However, the change could be part of a larger refactoring or improvement effort to address a previous design flaw or limitation. It's difficult to definitively say whether it's a bug fix without more context.

**NotBuggy**
"
javaparser,1394.json,9a62aa11f05a3eb80b8d11a756ff63be0b259b95,"@@ -1,3 +1,3 @@
     public Integer visit(MethodCallExpr n, Void arg) {
-        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0);
+        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0) * 31 + (n.getComment().accept(this, arg));
     }",NotBuggy,"Use superclass fields for hashcode generation too
",Buggy,"The commit message ""Use superclass fields for hashcode generation too"" suggests that the commit aims to improve the hashcode generation of a class by including fields from its superclass.

The provided diff modifies the `visit` method of a class that seems to be a visitor for Java's `MethodCallExpr`. The change involves adding `(n.getComment().accept(this, arg))` to the hashcode calculation.

The commit message does not explicitly mention the `getComment()` method, nor does it clearly state that the change relates to including superclass fields. However, it's possible that `getComment()` is a field inherited from a superclass, or that including the comment contributes to a more complete hashcode that considers inherited properties.

Given the information, it's difficult to definitively say whether this is a bug fix. It could be an enhancement to improve hashcode distribution. However, the fact that a field (`getComment()`) is now included in the hashcode calculation suggests that it was previously missing, potentially leading to hash collisions. This could be considered a bug fix.

Therefore, I will assume that the missing field in hashcode generation was a bug.

**Buggy**"
javaparser,640.json,ca2adb511d3e8c8b73c63a5da6cb3b8e49fe5564,"@@ -1,3 +1,3 @@
     public ResolvedIntersectionType resolve() {
-        return getSymbolResolver().resolve(this, ResolvedIntersectionType.class);
+        return getSymbolResolver().toResolvedType(this, ResolvedIntersectionType.class);
     }",NotBuggy,"divide SymbolResolver.resolve in two methods
",NotBuggy,"The commit message ""divide SymbolResolver.resolve in two methods"" suggests a refactoring where the `resolve` method in `SymbolResolver` is split into two methods.

The provided diff shows a change in the `resolve()` method of an unspecified class. It replaces a call to `getSymbolResolver().resolve(this, ResolvedIntersectionType.class)` with `getSymbolResolver().toResolvedType(this, ResolvedIntersectionType.class)`.

Based on the commit message and the diff, it seems like the `resolve` method in `SymbolResolver` was indeed divided, and the original call was replaced with a call to a new method `toResolvedType`. This is a refactoring change and does not directly indicate a bug fix. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**
"
javaparser,3650.json,54abb2027d67c0efc1c82712175e06d7eed2593b,"@@ -1,5 +1,2 @@
     public void ifSynchronizedStmt(Consumer<SynchronizedStmt> action) {
-        if (isSynchronizedStmt()) {
-            action.accept(asSynchronizedStmt());
-        }
     }",NotBuggy,"Be more like Java Symbol Solver (for ifType)
",Buggy,"The commit message ""Be more like Java Symbol Solver (for ifType)"" suggests an effort to align the current code's behavior with that of Java Symbol Solver, specifically concerning the `ifType`. The code diff removes a conditional check `if (isSynchronizedStmt())` before executing `action.accept(asSynchronizedStmt())`. This change implies that the original code might have been incorrectly skipping the execution of the action in certain scenarios, which is now corrected to align with Java Symbol Solver's behavior. This correction addresses a potential bug where the action was not always executed when it should have been.

**Buggy**"
javaparser,5313.json,356ae8dcc930e7ff6e418c30f49c922c08279786,"@@ -1,3 +1,3 @@
     public static CompilationUnit parseResource(final ClassLoader classLoader, final String path, Charset encoding) throws IOException {
-        return handleResult(parser.parseResource(classLoader, path, encoding));
+        return handleResult(newParser().parseResource(classLoader, path, encoding));
     }",NotBuggy,"Make StaticJavaParser thread safe
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Make StaticJavaParser thread safe"" suggests that the previous implementation of `StaticJavaParser` was not thread-safe, which is a bug. The goal is to fix a concurrency issue.

2.  **Git Diff:** The diff shows a change in the `parseResource` method. Instead of using a static `parser` field (which would be shared across threads and thus a source of thread-safety issues), it now calls `newParser()` to create a new instance of the parser for each call. This eliminates the shared mutable state and makes the method thread-safe.

3.  **Alignment:** The code change directly addresses the issue described in the commit message. By creating a new parser instance for each call, the code avoids potential race conditions and data corruption that could occur when multiple threads access and modify the same parser instance concurrently.

4.  **Bug Fix Indication:** The commit message explicitly states the intention to fix a thread-safety issue. The code changes align with this intention by removing the shared static parser instance. Thread-safety issues are generally considered bugs.

**Conclusion:**

**Buggy**
"
javaparser,946.json,9a03bc6291c871b2d705a17b4cad923bc954ba84,"@@ -1,3 +1,3 @@
     public Integer visit(final AnnotationMemberDeclaration n, final Void arg) {
-        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
+        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().hashCode()) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
     }",NotBuggy,"Revert ""Create a Modifier node""
",Buggy,"The commit message ""Revert ""Create a Modifier node"""" indicates that the changes are undoing a previous commit that created a ""Modifier node."" The diff shows a change in how the `n.getModifiers()` part is handled within the `visit` method. Specifically, it reverts from `n.getModifiers().accept(this, arg)` back to `n.getModifiers().hashCode()`. This suggests that the previous change involving `accept(this, arg)` on the modifiers was problematic and is now being reverted. Reverting a change often indicates that the original change introduced a bug or unintended behavior.

**Buggy**"
intellij-community,24004.json,9c7a04d5af8b287ea37ac662bb86d527e0253acd,"@@ -1,3 +1,9 @@
   protected ActionToolbar createToolbar() {
-    return null;
+    ActionGroup actionGroup = createLeftToolbarActionGroup();
+    if (actionGroup != null) {
+      return ActionManager.getInstance().createActionToolbar(""TextEditorWithPreview"", actionGroup, true);
+    }
+    else {
+      return null;
+    }
   }",NotBuggy,"IDEA-CR-48269: PY-34819 TextEditorWithPreview and Markdown SplitFileEditor changed according to UX/UI team recommendations

* Right action group customization added in TextEditorWithPreview
* Gutter alignment turned off for the left toolbar
* Reserved spaces removed from the toolbars

GitOrigin-RevId: bb53affaa3ace7e1191757631ed9756fa2c22b6c",NotBuggy,"The commit message indicates changes related to UI/UX improvements in `TextEditorWithPreview`, including right action group customization, gutter alignment adjustments, and removal of reserved spaces from toolbars.

The provided code diff shows a modification in the `createToolbar()` method. Previously, it simply returned `null`. Now, it creates an `ActionToolbar` based on an `ActionGroup` obtained from `createLeftToolbarActionGroup()`. If `createLeftToolbarActionGroup()` returns `null`, the method still returns `null`. This change aligns with the ""Right action group customization added in TextEditorWithPreview"" part of the commit message, as it introduces a mechanism to create and display a toolbar with actions. The code change itself doesn't directly indicate a bug fix, but rather an enhancement or feature addition. There's no error handling improvement, logical correction, or exception handling update.

**NotBuggy**"
intellij-community,33570.json,62eeca9c70c26cc115590b8ddfdab4d4cfa83eb7,"@@ -1,20 +1,19 @@
   public ProjectTemplate[] createTemplates(String group, WizardContext context) {
     Collection<Pair<URL, ClassLoader>> urls = myGroups.getValue().get(group);
     List<ProjectTemplate> templates = new ArrayList<ProjectTemplate>();
     for (Pair<URL, ClassLoader> url : urls) {
       try {
         List<String> children = UrlUtil.getChildrenRelativePaths(url.first);
         for (String child : children) {
           if (child.endsWith(ZIP)) {
             URL templateUrl = new URL(url.first.toExternalForm() + ""/"" + child);
-            String name = getTemplateName(child);
-            templates.add(new LocalArchivedTemplate(name, templateUrl, url.second));
+            templates.add(new LocalArchivedTemplate(templateUrl, url.second));
           }
         }
       }
       catch (IOException e) {
         LOG.error(e);
       }
     }
     return templates.toArray(new ProjectTemplate[templates.size()]);
   }",NotBuggy,"new project wizard: adding project templates
",NotBuggy,"The commit message ""new project wizard: adding project templates"" suggests the addition of project templates to a new project wizard.

The code diff shows modifications within the `createTemplates` method. Specifically, the `LocalArchivedTemplate` constructor is now called with `templateUrl` and `url.second` instead of `name, templateUrl, url.second`. The `name` variable, which was previously derived from `child` using `getTemplateName(child)`, is no longer used.

The change suggests a refactoring or simplification of how `LocalArchivedTemplate` objects are created. The removal of `name` from the constructor call could indicate a change in how the template name is handled or that the template name is no longer required during the object's creation.

There is an `IOException` catch block, which is good practice, but it doesn't necessarily indicate a bug fix. The core change seems to be related to template creation and not directly related to fixing a bug.

Based on the analysis, the changes align with the commit message, and there's no strong indication of a bug fix.

**NotBuggy**"
intellij-community,15453.json,ccb245867bc3b8f973f3fe9db7fd9a02a1169f49,"@@ -1,3 +1,3 @@
   public VirtualFile refreshAndFindFileByPath(@NotNull String path) {
-    return VfsImplUtil.refreshAndFindFileByPath(this, path);
+    return isValid(path) ? VfsImplUtil.refreshAndFindFileByPath(this, path) : null;
   }",NotBuggy,"[vfs] early .jar path validation (EA-95201)
",Buggy,"The commit message indicates an early validation of `.jar` paths, and the code change adds a check `isValid(path)` before proceeding with `refreshAndFindFileByPath`. This suggests that the previous implementation might have been attempting to process invalid paths, potentially leading to errors or unexpected behavior. The added validation acts as a safeguard, preventing the execution of the function with invalid paths. This is a bug fix.

**Buggy**"
intellij-community,27721.json,3fee046d2007f647e44c0a3744adfba605aad47d,"@@ -1,3 +1,3 @@
           public Dimension getPreferredSize() {
-            return new Dimension(super.getPreferredSize().width, JBUI.scale(44));
+            return new Dimension(super.getPreferredSize().width, JBUIScale.scale(44));
           }",NotBuggy,"[vcs-log] cleanup RootCellRenderer

GitOrigin-RevId: bba216626193ac756cd133b8ae9bc4e577abea42
",NotBuggy,"The commit message indicates a cleanup of `RootCellRenderer`. The diff shows a change from `JBUI.scale(44)` to `JBUIScale.scale(44)`. This looks like a simple refactoring or renaming of a method within the same utility class or a similar one. It doesn't appear to be related to fixing a bug, but rather improving the code's structure or readability.

**NotBuggy**"
intellij-community,25643.json,3b06cfe23b19153bd54c129cb9eba8cae7b4654e,"@@ -1,10 +1,9 @@
-          public Builder setModuleName(
-              int index, String value) {
-            if (value == null) {
+            public Builder setTypeId(String value) {
+              if (value == null) {
     throw new NullPointerException();
   }
-  ensureModuleNameIsMutable();
-            moduleName_.set(index, value);
-            
-            return this;
-          }

+  bitField0_ |= 0x00000001;
+              typeId_ = value;
+              
+              return this;
+            }",NotBuggy,"external compiler: compile scope based on build targets
",Buggy,"Okay, let's analyze the provided Git diff and commit message.

**Commit Message Analysis:**

The commit message ""external compiler: compile scope based on build targets"" suggests that the changes are related to how the external compiler determines the scope of compilation. This likely involves modifying the compiler's behavior to consider build targets when deciding which files or modules to compile.

**Diff Analysis:**

The diff shows a change in a `Builder` class, specifically modifying a method. The original method `setModuleName` is being replaced by `setTypeId`. The null check remains, but the logic for updating the field changes:

*   **Original:** `ensureModuleNameIsMutable(); moduleName_.set(index, value);` This suggests the code was dealing with a list of module names, and this method was setting a module name at a specific index.
*   **New:** `bitField0_ |= 0x00000001; typeId_ = value;` This indicates that the code is now setting a single `typeId_` field and using a bit field to track if the field has been set.

**Relevance Assessment:**

The diff itself doesn't directly reveal a bug fix. However, the change from handling a list of module names to a single `typeId` suggests a potential correction in how the compiler identifies the target to compile. It's possible that the original code incorrectly handled multiple module names when only one was expected, or that the logic for indexing into the module name list was flawed. The change to using a single `typeId` could be a simplification or a correction of a previous error.

Given the commit message and the code change, it's plausible that the original code had a bug related to how it determined the compilation scope based on build targets. The change to using a single `typeId` could be a way to fix this bug by ensuring that the compiler focuses on the correct target.

**Conclusion:**

**Buggy**
"
intellij-community,21829.json,8f877cd64f6e456fa8b4bb6cc67a65a473358191,"@@ -1,3 +1,3 @@
   public boolean isDeclarationWriteAccess(PsiElement element) {
-    return element instanceof PyTargetExpression;
+    return element instanceof PyTargetExpression || element.getParent() instanceof PyDelStatement;
   }",NotBuggy,"detect 'del x' as write access (PY-9784)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""detect 'del x' as write access (PY-9784)"" suggests that the code change is related to correctly identifying write access in Python code, specifically when the `del` statement is used. The ""PY-9784"" likely refers to a specific issue or bug report in a project management system. The commit message indicates that the previous implementation was not correctly identifying `del x` as a write access, which is a bug.

**Git Diff Analysis:**

The diff shows a modification to the `isDeclarationWriteAccess` method.

*   **Original Code:** `return element instanceof PyTargetExpression;`
*   **Modified Code:** `return element instanceof PyTargetExpression || element.getParent() instanceof PyDelStatement;`

The original code only considered `PyTargetExpression` (e.g., `x = 5`) as a write access. The modified code adds a condition to also consider the parent of the element being a `PyDelStatement` (e.g., `del x`) as a write access. This means that deleting a variable is now correctly recognized as a write operation.

**Reasoning:**

The commit message clearly states that the change is to ""detect 'del x' as write access,"" implying that it was not being detected previously. The code change directly addresses this by adding a check for `PyDelStatement`. This indicates a bug fix because the previous implementation was incomplete or incorrect in determining write access. The addition of the `PyDelStatement` check corrects this oversight.

**Conclusion:**

**Buggy**
"
intellij-community,20356.json,9f970709265d9f147b917bd74bd19667f5b8536d,"@@ -1,5 +1,5 @@
-  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<String> nameConsumer) {
+  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<? super String> nameConsumer) {
     String name =
       SchemeNameGenerator.getUniqueName(preferredName, schemeName -> getModel().containsScheme(schemeName, isProjectScheme));
     mySchemesCombo.startEdit(name, isProjectScheme, nameConsumer);
   }",NotBuggy,"bounded wildcards in API
",NotBuggy,"The commit message ""bounded wildcards in API"" suggests a change related to the use of bounded wildcards in the API.

The code diff shows a change in the `editNewSchemeName` method signature. Specifically, the type of the `nameConsumer` parameter has been changed from `Consumer<String>` to `Consumer<? super String>`. This change introduces a lower-bounded wildcard.

The original code only accepted a `Consumer` that consumes `String` objects. The modified code accepts a `Consumer` that consumes `String` objects or any supertype of `String`. This change increases the flexibility of the API by allowing consumers that can handle more general types.

This change doesn't appear to be directly related to fixing a bug. It's more likely a refactoring or enhancement to improve the API's flexibility. The use of a bounded wildcard suggests an attempt to make the API more generic and reusable.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
intellij-community,9028.json,571f34ea65d8637b1c3b3f8022ba8956a9eaaa59,"@@ -1,6 +1,6 @@
       private void process(@NotNull CaretEvent event) {
         Caret caret = event.getCaret();
-        if (caret == myCaretModel.getPrimaryCaret()) {
+        if (caret == caretModel.getPrimaryCaret()) {
           applyFocusMode(caret);
         }
       }",NotBuggy,"extract FocusModeModel
",NotBuggy,"The commit message ""extract FocusModeModel"" suggests a refactoring operation, specifically extracting a class or component related to focus mode. The provided code diff shows a change from `myCaretModel.getPrimaryCaret()` to `caretModel.getPrimaryCaret()`. This change indicates that `myCaretModel` has been replaced with `caretModel`. This could be part of the extraction process, where the caret model is now accessed through a different object or a more generic interface. There's no indication of bug fixing in this change. It's a simple variable name change, likely due to the refactoring.

**NotBuggy**
"
intellij-community,5202.json,32ebeeb487dc2bb7d5327285e8cc7930f92a22ff,"@@ -1,10 +1,9 @@
-  private Presentation update(AnAction action) {
+  private Presentation update(AnAction action, UpdateStrategy strategy) {
     if (myUpdatedPresentations.containsKey(action)) {
       return myUpdatedPresentations.get(action);
     }
 
-    AnActionEvent event = createActionEvent(action);
-    Presentation presentation = doUpdate(myModalContext, action, event) ? event.getPresentation(): null;
+    Presentation presentation = strategy.update.fun(action);
     myUpdatedPresentations.put(action, presentation);
     return presentation;
   }",NotBuggy,"update menu and toolbar actions with timeout, display non-updated actions using their default/previous presentation
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""update menu and toolbar actions with timeout, display non-updated actions using their default/previous presentation"" suggests several things:

1.  **Timeout:** There's a new mechanism to handle timeouts during the update of menu and toolbar actions. This implies that previously, actions might have been hanging or blocking the UI.
2.  **Default/Previous Presentation:** If an action fails to update (possibly due to the timeout), the system now displays the action using its default or previous presentation. This suggests a fix for a situation where actions might have disappeared or become unusable when an update failed.

**Code Diff Analysis:**

The code diff shows a change in the `update` method:

1.  **`UpdateStrategy` Parameter:** A new `UpdateStrategy` parameter has been added to the `update` method. This strongly suggests that the update process is now configurable or has different strategies for updating actions.
2.  **`strategy.update.fun(action)`:** The core update logic is now delegated to `strategy.update.fun(action)`. This indicates that the actual update process is now encapsulated within the `UpdateStrategy`.
3.  **Removal of `createActionEvent` and `doUpdate`:** The original code directly created an `AnActionEvent` and called `doUpdate`. This direct approach has been replaced by the more flexible `UpdateStrategy`.

**Reasoning:**

The commit message and code diff align well. The introduction of `UpdateStrategy` likely allows for the implementation of the timeout mechanism described in the commit message. The strategy can handle the timeout and, if the update fails within the timeout, return a default or previous presentation.

The original code likely had issues with actions hanging during updates, potentially leading to UI freezes or unusable actions. The new `UpdateStrategy` provides a way to mitigate this by introducing a timeout and a fallback mechanism (default/previous presentation). This strongly suggests a bug fix related to handling slow or failing action updates.

**Conclusion:**

**Buggy**
"
intellij-community,15839.json,3d7fcfef434c97bb682789e4d4aabbd56e856816,"@@ -1,14 +1,14 @@
-  private void setSelectedEditor(final FileEditor editor) {
+  private void setSelectedEditor(@NotNull FileEditor editor) {
     final EditorWithProviderComposite composite = getEditorComposite(editor);
     if (composite == null) return;
 
     final FileEditor[] editors = composite.getEditors();
     for (int i = 0; i < editors.length; i++) {
       final FileEditor each = editors[i];
       if (editor == each) {
         composite.setSelectedEditor(i);
         composite.getSelectedEditor().selectNotify();
         break;
       }
     }
   }",NotBuggy,"cleanup
",NotBuggy,"The commit message ""cleanup"" suggests general improvements or refactoring. The diff shows a change from `FileEditor editor` to `@NotNull FileEditor editor`. This indicates the addition of a nullability annotation, which is a form of code cleanup and aims to prevent potential NullPointerExceptions. There are no explicit bug fixes or error handling changes.

**NotBuggy**"
intellij-community,27442.json,e253d6a0dc44dfd8cea8853fbd6c2bb6de9c0038,"@@ -1,5 +1,5 @@
-  public void applyChanges(@NotNull JavaModuleExtensionImpl modified) {
+  public void applyChanges(@NotNull JpsJavaModuleExtensionImpl modified) {
     setLanguageLevel(modified.myLanguageLevel);
     setOutputUrl(modified.myOutputUrl);
     setTestOutputUrl(modified.myTestOutputUrl);
   }",NotBuggy,"new project model: more java-specific properties added to the model
",NotBuggy,"The commit message ""new project model: more java-specific properties added to the model"" suggests enhancements to the project model with Java-specific properties.

The provided code diff shows a change in the parameter type of the `applyChanges` method from `JavaModuleExtensionImpl` to `JpsJavaModuleExtensionImpl`. This change doesn't directly indicate a bug fix. It seems more like a part of the new project model implementation, possibly involving renaming or refactoring of classes related to Java module extensions. The change itself doesn't introduce error handling, logical corrections, or exception handling improvements. It's more of a structural or API change.

Therefore, based on the provided information, the changes do not indicate a bug fix.

**NotBuggy**
"
intellij-community,19676.json,8440df0fda18a651251e13ac1b5a693c85d1a617,"@@ -1,26 +1,26 @@
     private static void appendNode(Node node, StringBuilder sb) {
       if (node.target == ERROR_NODE) {
         sb.append(node.method);
         return;
       }
       else if (node.target != null) {
         appendNode(node.target, sb);
         sb.append('.');
       }
       sb.append(node.method).append('(');
       boolean first = true;
-      for (Object arg : (node.args == null ? ArrayUtil.EMPTY_OBJECT_ARRAY : node.args)) {
+      for (Object arg : (node.args == null ? ArrayUtilRt.EMPTY_OBJECT_ARRAY : node.args)) {
         if (first) first = false;
         else sb.append(',').append(' ');
         if (arg instanceof Node) {
           appendNode((Node)arg, sb);
         }
         else if (arg instanceof String) {
           sb.append('\""').append(StringUtil.escapeStringCharacters((String)arg)).append('\""');
         }
         else if (arg instanceof Number) {
           sb.append(arg);
         }
       }
       sb.append(')');
     }",NotBuggy,"prefer ArrayUtilRt

GitOrigin-RevId: f4b11d48949267d778f6cdab4ef8ac60eb85aa2e
",NotBuggy,"The commit message ""prefer ArrayUtilRt"" suggests a change to use `ArrayUtilRt` instead of `ArrayUtil`. The diff shows that `ArrayUtil.EMPTY_OBJECT_ARRAY` has been replaced with `ArrayUtilRt.EMPTY_OBJECT_ARRAY`. This change aligns with the commit message. There is no indication of a bug fix in this change. It seems to be a preference or optimization.

**NotBuggy**"
intellij-community,21985.json,b5c92ae0ba1ee1964a0e18b02eb49ceafaf8679e,"@@ -1,16 +1,16 @@
-  public PyType getCallableType(@NotNull Callable callable, @NotNull TypeEvalContext context) {
+  public PyType getCallableType(@NotNull PyCallable callable, @NotNull TypeEvalContext context) {
     if (callable instanceof PyFunction) {
       final String qualifiedName = callable.getQualifiedName();
       if (qualifiedName != null && qualifiedName.startsWith(""PyQt"")){
         final QualifiedName name = QualifiedName.fromDottedString(qualifiedName);
         final String qtVersion = name.getComponents().get(0);
         final String docstring = ((PyFunction)callable).getDocStringValue();
         if (docstring != null && docstring.contains(""[signal]"")) {
           final PyClass aClass = PyClassNameIndex.findClass(qtVersion + ""."" + ourQtBoundSignal, callable.getProject());
           if (aClass != null)
             return new PyClassTypeImpl(aClass, false);
         }
       }
     }
     return null;
   }",NotBuggy,"Rename com.jetbrains.python.psi.Callable to PyCallable so as not to clash with java.util.concurrent.Callable
",NotBuggy,"The commit message indicates a renaming operation to avoid naming conflicts. The diff shows that `com.jetbrains.python.psi.Callable` is renamed to `PyCallable`. This change doesn't seem to be directly related to fixing a bug but rather to prevent potential naming conflicts with `java.util.concurrent.Callable`. There's no error handling, logical correction, or exception handling improvement in the diff.

**NotBuggy**"
intellij-community,13487.json,47f3db3350684a82b8dead8a0b4f7c961e0ed685,"@@ -1,3 +1,3 @@
           public boolean hasLine() {
-            return ((bitField0_ & 0x00000040) == 0x00000040);
+            return instance.hasLine();
           }",NotBuggy,"protobuf version advanced to 3.0.0
",NotBuggy,"The commit message ""protobuf version advanced to 3.0.0"" indicates a dependency update. The code change replaces a direct bitwise operation `((bitField0_ & 0x00000040) == 0x00000040)` with a call to `instance.hasLine()`. This suggests the code was modified to align with the updated protobuf library. The change itself doesn't obviously indicate a bug fix, but rather an adaptation to a new API or behavior in protobuf 3.0.0. It's possible the old bitwise operation was no longer the correct way to check for the presence of the ""line"" field after the protobuf update. However, without more context, it's difficult to definitively say if this change was specifically addressing a bug. It's more likely a compatibility update.

**NotBuggy**
"
intellij-community,26056.json,4f5720f30aaff736459a6dc2d23912bf00f0d745,"@@ -1,18 +1,18 @@
   private void configureStartingScriptPanel(final Project project, final PyConsoleOptionsProvider.PyConsoleSettings optionsProvider) {
-    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myStartScript), project, PythonFileType.INSTANCE) {
+    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myCustomStartScript), project, PythonFileType.INSTANCE) {
       @Override
       protected EditorEx createEditor() {
         final EditorEx editor = super.createEditor();
         editor.setVerticalScrollbarVisible(true);
         return editor;
       }
 
       @Override
       protected boolean isOneLineMode() {
         return false;
       }
     };
     myStartingScriptPanel.setLayout(new BorderLayout());
     myStartingScriptPanel.add(myEditorTextField, BorderLayout.CENTER);
     myConsoleSettings = optionsProvider;
   }",NotBuggy,"Setting to remove separator line in console (PY-2573).
",NotBuggy,"The commit message indicates a change related to removing a separator line in the console, specifically referencing issue PY-2573. The provided diff focuses on the configuration of the starting script panel in the Python console settings.

The code changes involve modifying how the `EditorTextField` is initialized, specifically changing `optionsProvider.myStartScript` to `optionsProvider.myCustomStartScript`. This suggests a shift from a general start script to a custom start script.

While the commit message mentions removing a separator line, the code diff doesn't directly reflect this. The changes are related to the starting script configuration, which might indirectly influence the console's appearance, but the connection isn't explicit. The change from `myStartScript` to `myCustomStartScript` suggests a feature enhancement or configuration change rather than a bug fix. It's possible the original script was not customizable, and this change allows for a custom script, which then allows the user to remove the separator line.

Given the information, it's difficult to definitively say if this is a bug fix. However, the change in script source suggests a configuration update or new feature rather than a correction of faulty behavior.

**NotBuggy**"
hector,1607.json,8751ec479084eca0622c2f26f6c0b02577ad2a15,"@@ -1,4 +1,4 @@
   public String system_update_keyspace(KsDef ks_def)
-      throws InvalidRequestException, TException {
+      throws InvalidRequestException, TException, SchemaDisagreementException {
     return client.system_update_keyspace(ks_def);
   }",NotBuggy,"v0.8 specific changes for bringing up to date with latest cassandra trunk
",Buggy,"The commit message ""v0.8 specific changes for bringing up to date with latest cassandra trunk"" indicates a synchronization effort with the main Cassandra trunk for the v0.8 branch. The diff shows the addition of `SchemaDisagreementException` to the `throws` clause of the `system_update_keyspace` method. This suggests that the method now explicitly declares that it can throw a `SchemaDisagreementException`, which likely wasn't handled or declared before. This addition is likely a consequence of changes in the Cassandra trunk related to schema management and disagreement handling. The change aligns with the commit message's intention of bringing the v0.8 branch up to date. It's plausible that the original code lacked proper handling or declaration of this exception, which could lead to unexpected behavior or crashes. Thus, the change can be considered a bug fix.

**Buggy**
"
hector,2275.json,747116cab7950e8466a5b7b89c223866472dab49,"@@ -1,21 +1,21 @@
       public Column execute(Cassandra.Client cassandra) throws HectorException {
         try {          
           if ( queryLogger.isDebugEnabled() ) {
-            queryLogger.debug(""---------\nColumnFamily: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
+            queryLogger.debug(""---------\nColumnFamily multiget: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
           }
           long startTime = System.nanoTime();
-          Map<ByteBuffer, List<ColumnOrSuperColumn>> rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
+          rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
               ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
           long duration = System.nanoTime() - startTime;
 
-
+          
           if ( queryLogger.isDebugEnabled() ) {
             queryLogger.debug(""Execution took {} microseconds on host {}\n----------"", duration/1000, getCassandraHost());
           }
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }
         hasValues = true;
 
         return null;
       }",NotBuggy,"initial cut at automatic multiget
",NotBuggy,"The commit message ""initial cut at automatic multiget"" suggests the introduction of a new feature related to multi-get operations.

The code diff shows changes within the `execute` method, specifically related to `cassandra.multiget_slice`. The debug log message is updated to include ""multiget"" in the output. The `rows` variable is assigned the result of the `multiget_slice` call.

Based on the commit message and the code diff, the changes introduce a new functionality (multiget). There is no explicit error handling, logical correction, or exception handling improvement in the diff. The code diff aligns with the commit message, which describes the initial implementation of the multiget feature.

**NotBuggy**"
hector,1823.json,a54d8e467f611b1e3467ad27145004668efc0d6d,"@@ -1,5 +1,5 @@
   public HColumn<N, V> setValue(V value) {
     notNull(value, ""value is null"");
-    this.value = value;
+    this.column.value = valueSerializer.toByteBuffer(value);
     return this;
   }",NotBuggy,"huge efficiency improvement in hcolumimpl. tied a lot closer to thrift Column, but the efficiency gains are worth it and we are still behind an interface
",NotBuggy,"The commit message indicates a significant efficiency improvement in `HColumnImpl` by tying it closer to the Thrift `Column` representation. The code diff shows that the `setValue` method is modified to directly use the `valueSerializer` to convert the value to a `ByteBuffer` and assign it to the `column.value`. This change aligns with the commit message's claim of tying closer to the Thrift `Column` (which uses `ByteBuffer` for values) and improving efficiency by avoiding intermediate object creation or conversion steps later on. While not explicitly a bug fix, the change suggests an optimization to address a previous inefficiency.

**NotBuggy**"
hector,2272.json,229041fa14667b7d0ea1064511055e96713b230e,"@@ -1,13 +1,13 @@
   private void maybeExecuteSlice(N columnName) {
     if ( columnNames == null ) {
       columnNames = new HashSet<N>();
     }
     if ( columns == null ) {
-      columns = new HashMap<N, Column>();
+      columns = new HashMap<N, HColumn<N,ByteBuffer>>();
     }
     if ( columns.get(columnName) == null ) {
       columnNames.add(columnName);
       activeSlicePredicate.setColumnNames(columnNames);
       doExecuteSlice();
     }    
   }",NotBuggy,"added more innards to hcolumnfamily, test coverage for such
",NotBuggy,"The commit message indicates the addition of ""more innards"" to `HColumnFamily` and increased test coverage. The code diff shows a change in the type of the `columns` field from `HashMap<N, Column>` to `HashMap<N, HColumn<N,ByteBuffer>>`. This type change suggests a modification to accommodate a more specific or enhanced column representation within the `HColumnFamily`. While the commit message doesn't explicitly mention a bug fix, the change in the data structure could be related to addressing an issue with the previous implementation or improving its functionality. The change itself doesn't scream ""bug fix"", and the commit message doesn't suggest it either.

**NotBuggy**"
hector,516.json,50daddbde6d764dd15c3c1fc6b74da2ea8da240d,"@@ -1,12 +1,12 @@
   public <T> AbstractComposite setComponent(int index, T value,
       Serializer<T> s, String comparator, boolean inclusive) {
     serialized = null;
 
     while (components.size() <= index) {
       components.add(null);
     }
-    components.set(index, new Component(value, s, comparator, inclusive));
+    components.set(index, new Component(value, null, s, comparator, inclusive));
 
     return this;
 
   }",NotBuggy,"Made deserialization of components lazy and occur at get() time
",NotBuggy,"The commit message states that the deserialization of components is made lazy and occurs at get() time. The provided diff modifies the `setComponent` method. Specifically, it changes how the `Component` object is constructed.

The original code:
`components.set(index, new Component(value, s, comparator, inclusive));`

The modified code:
`components.set(index, new Component(value, null, s, comparator, inclusive));`

The change replaces the first argument in the `Component` constructor from `value` to `null`. It's highly likely that the `Component` constructor now handles the `value` lazily, possibly storing the serializer `s` and the actual value `value` to be deserialized later when `get()` is called. This aligns with the commit message, suggesting a performance optimization by deferring deserialization. This change doesn't look like a bug fix.

**NotBuggy**"
hector,1926.json,12bad5fac77e564df933c1893fd3210e16152326,"@@ -1,27 +1,27 @@
 	public void copy() throws HectorException {
 		if (this.cf == null) {
 			throw new HectorException(""Unable to clone row with null column family"");
 		}
 		if (this.rowKey == null) {
 			throw new HectorException(""Unable to clone row with null row key"");
 		}
 		if (this.destinationKey == null) {
 			throw new HectorException(""Unable to clone row with null clone key"");
 		}
 
 		ColumnFamilyTemplate<K, ByteBuffer> template = new ThriftColumnFamilyTemplate<K, ByteBuffer>(this.keyspace, this.cf, this.keySerializer, this.bs);
 		Mutator<K> mutator = HFactory.createMutator(this.keyspace, this.keySerializer, new BatchSizeHint(1, this.mutateInterval));
 		ColumnFamilyUpdater<K, ByteBuffer> updater = template.createUpdater(this.destinationKey, mutator);
 
-		SliceQuery<K, ByteBuffer, V> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.valueSerializer).
+		SliceQuery<K, ByteBuffer, ByteBuffer> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.bs).
 						setColumnFamily(this.cf).
 						setKey(this.rowKey);
 
-		ColumnSliceIterator<K, ByteBuffer, V> iterator = new ColumnSliceIterator<K, ByteBuffer, V>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
+		ColumnSliceIterator<K, ByteBuffer, ByteBuffer> iterator = new ColumnSliceIterator<K, ByteBuffer, ByteBuffer>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
 		while (iterator.hasNext()) {
-			HColumn<ByteBuffer, V> column = iterator.next();
-			updater.setValue(column.getName(), column.getValue(), this.valueSerializer);
+			HColumn<ByteBuffer, ByteBuffer> column = iterator.next();
+			updater.setValue(column.getName(), column.getValue(), this.bs);
 		}
 
 		template.update(updater);
 	}",NotBuggy,"Changed class to use ByteBuffer as the column value serializer to accomodate CF with different value types
",Buggy,"The commit message indicates a change to use `ByteBuffer` as the column value serializer to accommodate column families with different value types.

The code diff shows the following changes:

1.  `ColumnFamilyTemplate<K, ByteBuffer>` is used, which aligns with using `ByteBuffer` for column values.
2.  `SliceQuery<K, ByteBuffer, V>` is changed to `SliceQuery<K, ByteBuffer, ByteBuffer>`, which aligns with using `ByteBuffer` for column values.
3.  `ColumnSliceIterator<K, ByteBuffer, V>` is changed to `ColumnSliceIterator<K, ByteBuffer, ByteBuffer>`, which aligns with using `ByteBuffer` for column values.
4.  `HColumn<ByteBuffer, V>` is changed to `HColumn<ByteBuffer, ByteBuffer>`, which aligns with using `ByteBuffer` for column values.
5.  `updater.setValue(column.getName(), column.getValue(), this.valueSerializer)` is changed to `updater.setValue(column.getName(), column.getValue(), this.bs)`, which aligns with using `ByteBuffer` for column values.

The changes in the code diff align with the commit message. The original code used a generic type `V` for the column value, which was replaced with `ByteBuffer`. This suggests that the original code had a limitation where it could only handle column families with a specific value type. The change to `ByteBuffer` allows the code to handle column families with different value types, which indicates a bug fix or enhancement to address a limitation in the original code.

**Buggy**"
hector,2776.json,6f80d0e241cea8f85ccab3514c6b45fdd9239856,"@@ -0,0 +1,3 @@
+  public void setUseThriftFramedTransport(boolean useThriftFramedTransport) {
+    this.useThriftFramedTransport = useThriftFramedTransport;
+  }",NotBuggy,"added framed transport to CHC
",NotBuggy,"The commit message ""added framed transport to CHC"" indicates the addition of a feature related to Thrift's framed transport. The provided code snippet shows a setter method `setUseThriftFramedTransport` which suggests the introduction of a configuration option to enable or disable the use of Thrift's framed transport.

The code directly implements the feature described in the commit message. There's no indication of a bug fix in the code or the commit message. The change introduces a new functionality rather than correcting an existing error.

**NotBuggy**
"
hector,22.json,1af2d5ee84680a2abfe86f1710cce6569d2d5b7a,"@@ -1,3 +1,3 @@
-  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfSuperMapDef) {
-    this.cfBaseMapDef = cfSuperMapDef;
+  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfBaseMapDef) {
+    this.cfBaseMapDef = cfBaseMapDef;
   }",NotBuggy,"refactored ClassCacheMgr to make a leaner more readable class and prevent bloat, and make parallel work a bit easier
",NotBuggy,"The commit message indicates a refactoring effort focused on improving the `ClassCacheMgr` class by making it leaner, more readable, preventing bloat, and facilitating parallel work.

The provided diff shows a simple change: renaming the parameter `cfSuperMapDef` to `cfBaseMapDef` in the `setCfBaseMapDef` method.

Reasoning:

1.  **Alignment with Commit Message:** The parameter renaming itself doesn't directly indicate a bug fix. It could be part of the ""making it more readable"" aspect of the refactoring. The change is very small and localized.
2.  **Bug Fix Indicators:** There's no error handling, logical correction, or exception handling improvement in the diff. The change is purely a renaming operation.
3.  **Refactoring:** The commit message mentions refactoring, and renaming variables for clarity is a common refactoring practice.

Conclusion: **NotBuggy**
"
hector,927.json,8996eec24a06cfc68a8ef691674d2c187af07b60,"@@ -1,14 +1,11 @@
   public static Subject loginService(String serviceName) throws LoginException {
-    LoginContext loginCtx = new LoginContext(serviceName,
-        new CallbackHandler() {
-          // as we use .keytab file there is no need to specify any options in
-          // callback
+    LoginContext loginCtx = new LoginContext(serviceName, new CallbackHandler() {
+          // as we use .keytab file there is no need to specify any options in callback
           public void handle(Callback[] callbacks) throws IOException,
               UnsupportedCallbackException {
           }
         });
 
     loginCtx.login();
-
     return loginCtx.getSubject();
   }",NotBuggy,"Edit sample files and better jadavocs
",NotBuggy,"The commit message ""Edit sample files and better javadocs"" indicates changes to sample files and improvements to Javadoc comments.

The provided code diff focuses on the `loginService` method. The changes involve removing an unnecessary newline character. This change does not seem to be related to fixing a bug. It's more of a code formatting or style improvement. The comment is also slightly reworded, but the functionality remains the same.

Based on the analysis, the changes do not indicate a bug fix.

**NotBuggy**"
hector,1382.json,6c42b8d05ffc0aef7de1c5e5e67c1ded7e4f45d4,"@@ -1,11 +1,5 @@
   public <N> MutationResult deleteCounter(final K key, final String cf, final N counterColumnName, 
       final Serializer<N> nameSerializer) {
-    return new MutationResultImpl(keyspace.doExecute(new KeyspaceOperationCallback<Void>() {
-        @Override
-        public Void doInKeyspace(KeyspaceService ks) throws HectorException {
-          ks.removeCounter(keySerializer.toByteBuffer(key), ThriftFactory.createColumnPath(cf, counterColumnName, 
-              nameSerializer));
-          return null;
-        }
-    }));
+    addCounterDeletion(key,cf,counterColumnName,nameSerializer);
+    return execute();
   }",NotBuggy,"initial cleanup to support #354
",NotBuggy,"The commit message indicates an initial cleanup to support a feature request (#354). The code diff shows a refactoring of the `deleteCounter` method. The original implementation directly executed the counter deletion within a `KeyspaceOperationCallback`. The modified code extracts the counter deletion logic into a separate `addCounterDeletion` method and uses an `execute` method, suggesting a change in how mutations are handled, possibly batching or deferring execution.

The change doesn't explicitly indicate a bug fix. It's more of a structural change to facilitate a new feature. There's no error handling, logical correction, or exception handling improvement visible in the diff. The change seems to be about improving the architecture or preparing for future enhancements.

**NotBuggy**
"
hector,2389.json,da2edd4944bc071a11859fb61f76df5112b06a80,"@@ -1,14 +1,11 @@
       public Map<ByteBuffer,List<ColumnOrSuperColumn>> execute(Cassandra.Client cassandra) throws HectorException {
-        Map<ByteBuffer,List<ColumnOrSuperColumn>> cosc = new LinkedHashMap<ByteBuffer, List<ColumnOrSuperColumn>>();
         try {          
           List<K> keyList = new ArrayList<K>();
           Iterators.addAll(keyList, keys.iterator());
-          cosc = cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
+          return cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
               (workingSlicePredicate == null ? activeSlicePredicate.setColumnNames(columnValueSerializers.keySet()).toThrift() : workingSlicePredicate.toThrift()),              
             ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }        
-
-        return cosc;
       }",NotBuggy,"remove unnecessary allocation
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states ""remove unnecessary allocation."" This suggests an optimization effort to reduce memory usage by eliminating an object that was being created but not effectively used.

2.  **Code Diff:**
    *   The code diff shows that the line `Map<ByteBuffer,List<ColumnOrSuperColumn>> cosc = new LinkedHashMap<ByteBuffer, List<ColumnOrSuperColumn>>();` is removed. This line was allocating a new `LinkedHashMap`.
    *   The `return cosc;` statement at the end of the method is also removed.
    *   The `cassandra.multiget_slice` call, which was previously assigning its result to `cosc`, is now directly returned.

3.  **Analysis:**
    *   The original code allocated a `LinkedHashMap` named `cosc`, assigned the result of `cassandra.multiget_slice` to it, and then returned `cosc`.
    *   The modified code directly returns the result of `cassandra.multiget_slice`.
    *   This indicates that the `cosc` variable was unnecessary. The `cassandra.multiget_slice` method's result was being assigned to `cosc` and then immediately returned. The allocation of `cosc` was redundant.

4.  **Bug Fix or Optimization?** While not a bug in the sense of incorrect behavior, the original code had an inefficiency. Removing the unnecessary allocation is an optimization. It improves performance (slightly) and reduces memory footprint. It's a fix for a suboptimal implementation.

**Conclusion:**

**Buggy** (in the sense of fixing a suboptimal implementation)
"
hector,1732.json,f8ad7a5aa29b297d3f6281a4d061f8dd9da0da4e,"@@ -1,5 +1,5 @@
-  public  RangeSubSlicesQuery<SN,N,V> setSuperColumn(SN sc) {
+  public RangeSubSlicesQuery<SN, N, V> setSuperColumn(SN sc) {
     Assert.notNull(sc, ""sc can't be null"");
     superColumn = sc;
     return this;
   }",NotBuggy,"Refactor: Extract interface from RangeSlicesQuery, RangeSubSlicesQuery, SuperSlicesQuery, SubSliceQuery and SuperSliceQuery and move them to the api package
",NotBuggy,"The commit message indicates a refactoring operation, specifically extracting interfaces from several query classes and moving them to the `api` package. The provided code diff shows a minor change in the `RangeSubSlicesQuery` class, where a space is removed. This change doesn't seem to be directly related to fixing a bug but rather to code formatting or style consistency. The primary action described in the commit message is interface extraction and movement, which isn't reflected in the given diff. Therefore, the change doesn't indicate a bug fix.

**NotBuggy**"
hector,587.json,ea13a02750c3c4dba496c54ee459a0f2e749ee43,"@@ -1,19 +1,6 @@
   public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(""HLockManagerConfigurator [keyspaceName="");
-    builder.append(keyspaceName);
-    builder.append("", lockManagerCF="");
-    builder.append(lockManagerCF);
-    builder.append("", rowsCacheEnabled="");
-    builder.append(rowsCacheEnabled);
-    builder.append("", locksTTLInMillis="");
-    builder.append(locksTTLInMillis);
-    builder.append("", backOffRetryDelayInMillis="");
-    builder.append(backOffRetryDelayInMillis);
-    builder.append("", replicationFactor="");
-    builder.append(replicationFactor);
-    builder.append("", numberOfLockObserverThreads="");
-    builder.append(numberOfLockObserverThreads);
-    builder.append(""]"");
-    return builder.toString();
+    return ""HLockManagerConfigurator [keyspaceName="" + keyspaceName + "", lockManagerCF="" + lockManagerCF
+        + "", rowsCacheEnabled="" + rowsCacheEnabled + "", locksTTLInMillis="" + locksTTLInMillis
+        + "", backOffRetryDelayInMillis="" + backOffRetryDelayInMillis + "", replicationFactor="" + replicationFactor
+        + "", numberOfLockObserverThreads="" + numberOfLockObserverThreads + "", maxSelectSize="" + maxSelectSize + ""]"";
   }",NotBuggy,"Made ack size configurable.  The first 10 elements is more than
sufficient to guarantee ordered locking.  ",NotBuggy,"The commit message ""Made ack size configurable. The first 10 elements is more than sufficient to guarantee ordered locking."" indicates that the commit introduces a configuration option for the acknowledgement size, likely related to locking mechanisms. The message also suggests that a specific value (10) is sufficient for ordered locking.

The provided diff modifies the `toString()` method of the `HLockManagerConfigurator` class. It changes the way the string representation of the object is constructed, switching from using a `StringBuilder` to string concatenation. More importantly, it adds `maxSelectSize` to the output of the `toString()` method.

Based on the commit message and the diff, the `maxSelectSize` parameter is newly added and is likely the configurable ack size mentioned in the commit message. The change to the `toString()` method is simply to include this new parameter in the string representation of the configuration. There is no indication of bug fixing in the code change. The change is an enhancement to include the new parameter in the `toString()` method.

**NotBuggy**"
hector,24.json,c6fd6f3fa896a6f616cb2332dacbf9491316dfb0,"@@ -1,3 +1,3 @@
-  public Map<Object, CFMappingDef<? extends T, I>> getDerivedClassMap() {
+  public Map<Object, CFMappingDef<? extends T>> getDerivedClassMap() {
     return derivedClassMap;
   }",NotBuggy,"a POJO's ID was genericized in most places as <I> but this doesn't make sense when starting to use complex types as the key - so it has been removed
",Buggy,"The commit message indicates a change to the generic type of a POJO's ID, removing the `<I>` type parameter. The provided code diff shows a change in the return type of the `getDerivedClassMap()` method, specifically removing the `<I>` type parameter from `CFMappingDef<? extends T, I>`. This change aligns perfectly with the commit message, indicating that the generic type `I` has been removed. The commit message suggests that the original generic type was not suitable for complex types, implying a potential issue or limitation with the previous implementation. This suggests that the change is a correction or improvement to address this limitation. Therefore, the changes indicate a bug fix.

**Buggy**"
hector,2693.json,b0dc2661d56bb80142771050c0124ea013fe4e31,"@@ -1,7 +1,9 @@
-      public List<TokenRange> execute(Cassandra.Client cassandra) throws HectorException {
+      public String execute(Cassandra.Client cassandra) throws HectorException {
         try {
-          return cassandra.describe_ring(keyspace);
+          log.info(""in execute with client {}"", cassandra);
+          return cassandra.describe_cluster_name();
         } catch (Exception e) {
           throw xtrans.translate(e);
         }
+
       }",NotBuggy,"Refactor: Seperate the thrift classes from the Cluster interface so it doesn't import any thrift specific classes and create a ThriftCluster implementation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Refactor: Seperate the thrift classes from the Cluster interface so it doesn't import any thrift specific classes and create a ThriftCluster implementation"" suggests a refactoring effort. The primary goal is to decouple the `Cluster` interface from Thrift-specific classes, implying a design improvement rather than a bug fix. The introduction of a `ThriftCluster` implementation further supports this refactoring narrative.

**Git Diff Analysis:**

The diff shows a change in the `execute` method.

1.  **Return Type Change:** The return type of the `execute` method has changed from `List<TokenRange>` to `String`.
2.  **Functionality Change:** Instead of calling `cassandra.describe_ring(keyspace)`, it now calls `cassandra.describe_cluster_name()`.
3.  **Logging Addition:** A log statement `log.info(""in execute with client {}"", cassandra);` has been added.

**Reasoning:**

The change in return type and the method called within the `execute` function suggests a significant change in functionality. While the commit message describes a refactoring, the code changes indicate a shift in what the `execute` method *does*. The original method likely retrieved token ranges, while the new method retrieves the cluster name. This could be part of the refactoring, but it's a functional change, not just a structural one. The added log statement doesn't directly indicate a bug fix, but it could be for debugging purposes related to the change in functionality. The exception handling remains the same, suggesting no specific bug fix related to exception handling.

Given the change in functionality, it's possible that the original implementation had an issue or was not providing the correct information, which this change addresses. It's also possible that this is simply a change in requirements, but without more context, it's difficult to say for sure.

**Conclusion:**

The changes in the `execute` method's return type and the function it calls suggest a functional change that could be related to correcting an issue with the previous implementation. Therefore, I will classify this as a bug fix.

**Buggy**
"
cucumber-jvm,1199.json,669dd85aad21b1c030d21bf3def4d770ca4f891e,"@@ -1,3 +1,3 @@
-    void execute(Scenario scenario) throws Throwable {
+    void execute(TestCaseState scenario) {
         delegate.execute(scenario);
     }",NotBuggy,"[Core] Rename Scenario to TestCaseState
",NotBuggy,"The commit message ""Rename Scenario to TestCaseState"" suggests a renaming refactoring. The code diff shows that the parameter type `Scenario` in the `execute` method has been renamed to `TestCaseState`. This change aligns with the commit message and appears to be a simple renaming operation. There is no indication of a bug fix, error handling, or logical correction.

**NotBuggy**"
cucumber-jvm,565.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,7 +1,7 @@
     public boolean equals(Object o) {
         if (this == o) return true;
         if (o == null || getClass() != o.getClass()) return false;
-        CucumberLocation that = (CucumberLocation) o;
+        GherkinMessagesLocation that = (GherkinMessagesLocation) o;
         return line == that.line &&
             column == that.column;
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",NotBuggy,"The commit message indicates the introduction of a new module (`gherkin-messages`) to support the Gherkin `Rule` keyword. The provided diff shows a change in the `equals` method of a class, renaming `CucumberLocation` to `GherkinMessagesLocation`. This suggests a direct relationship between the commit message and the code change, as the new module likely includes a class representing the location of elements within the Gherkin `Rule`. The renaming suggests that the class is now specifically associated with the `gherkin-messages` module and its functionality. There's no explicit indication of a bug fix in the commit message or the diff. The change appears to be part of the new feature implementation.

**NotBuggy**"
cucumber-jvm,978.json,365f45a2df0bff71567bd457d663185f5f3cc788,"@@ -1,11 +1,11 @@
     private List<Node> reversedPoppedNodes(List<Node> currentStack, List<Node> newStack) {
         for (int i = 0; i < currentStack.size() && i < newStack.size(); i++) {
             if (!currentStack.get(i).equals(newStack.get(i))) {
                 return currentStack.subList(i, currentStack.size());
             }
         }
         if (newStack.size() < currentStack.size()) {
             return currentStack.subList(newStack.size(), currentStack.size());
         }
-        return Collections.emptyList();
+        return emptyList();
     }",NotBuggy,"[Plugin] Add TestSourceParsed event (#1895)

Cucumber scenarios and individual examples in a scenario outline are compiled
into pickles. These pickles are wrapped by a `TestCase`. As such
Cucumbers internal representation lacks any hierarchy. I.e. once compiled
into a a pickle a scenario is no longer associated with a feature file.

However consumers of Cucumbers output generally expect results to be reported
in hierarchical fashion. This event allows test cases to be associated with
 with a `Node` in the hierarchy. The `TestSourceParsed` event provides this information. 

A node is an element in a tree. It has a name, keyword and location. A node can optionally be container of yet more nodes. 

```
└── TestSourceParsed
      ├── Node
      └── Container
      │ ├── Node
      │ └── Node
      └── Container
        └── Container
           ├── Node
           └── Node
```

Note that this information is intentionally abstract. To recreate a a facsimile of the original feature file it is still recommended to use the AST produced by a Gherkin parser. And to avoid conflict with other tools is recommended that this parser is shaded.

Closes: #1870",NotBuggy,"The commit message describes the addition of a `TestSourceParsed` event to provide hierarchical information about test cases, associating them with nodes in a tree structure. The code diff changes `Collections.emptyList()` to `emptyList()`. This change is likely an optimization or a style preference, as both achieve the same result of returning an empty list. The change itself doesn't suggest a bug fix, and the commit message focuses on adding a new feature rather than correcting an existing issue.

**NotBuggy**"
cucumber-jvm,700.json,4d2aa2b173a286d766da8ada1546733fd26d5664,"@@ -1,7 +1,7 @@
-    private List<Class<?>> findClassesForUris(List<URI> baseUris, String basePackageName, Predicate<Class<?>> classFilter) {
+    private List<Class<?>> findClassesForUris(List<URI> baseUris, String packageName, Predicate<Class<?>> classFilter) {
         return baseUris.stream()
-            .map(baseUri -> findClassesForUri(baseUri, basePackageName, classFilter))
+            .map(baseUri -> findClassesForUri(baseUri, packageName, classFilter))
             .flatMap(Collection::stream)
             .distinct()
             .collect(toList());
     }",NotBuggy,"[Core] Use consistent naming in scanners

From a consumer the package being scanned is the `packageName` rather
then the `basePackageName` which only makes sense ocne the package is
compared to some other package.
",NotBuggy,"The commit message indicates a change in naming convention from `basePackageName` to `packageName` for consistency. The diff shows that `basePackageName` is indeed replaced with `packageName` in the `findClassesForUris` method. This change aligns with the commit message and aims to improve code readability and maintainability by using a more consumer-centric term. There is no indication of a bug fix in this change.

**NotBuggy**"
cucumber-jvm,608.json,4f2ded527763a56e55bc6713e56ba6fa6ef9b359,"@@ -1,3 +1,3 @@
-    static InjectionProvider<?>[] setUpInjectionProviders(final String resourceName) {
-        return new CucumberNeedleConfiguration(resourceName).getInjectionProviders();
+    static InjectionProvider<?>[] setUpInjectionProviders() {
+        return new CucumberNeedleConfiguration().getInjectionProviders();
     }",NotBuggy,"[Needle] Move ObjectFactory to io.cucumber.needle
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""[Needle] Move ObjectFactory to io.cucumber.needle"" suggests a refactoring or reorganization of the codebase, specifically moving the `ObjectFactory` to a different package (`io.cucumber.needle`).

2.  **Git Diff:** The diff shows a change in the `setUpInjectionProviders` method. The original version took a `resourceName` as an argument, while the modified version takes no arguments. The `CucumberNeedleConfiguration` constructor also changes from taking `resourceName` to no arguments.

3.  **Reasoning:**
    *   The commit message indicates a package move, which isn't directly reflected in the diff. However, the change in the `setUpInjectionProviders` method and the `CucumberNeedleConfiguration` constructor *could* be related to the package move. It's possible that the `resourceName` is now handled differently after the move, perhaps being configured within the `CucumberNeedleConfiguration` class itself or obtained through a different mechanism.
    *   The change in method signature from `setUpInjectionProviders(final String resourceName)` to `setUpInjectionProviders()` suggests that the `resourceName` is no longer needed as an explicit parameter. This could be because it's now being handled internally within the `CucumberNeedleConfiguration` class.
    *   Without more context, it's difficult to definitively say if this is a bug fix. It's more likely a refactoring related to the package move. It's possible that the old way of passing `resourceName` was causing issues, but there's no direct evidence of that in the provided information.

**Conclusion:**

NotBuggy
"
cucumber-jvm,468.json,f2eeac155f0893001bda9bf3a0161d151e2eff38,"@@ -1,3 +1,3 @@
-        Type getType() {
-            return type;
-        }

+    public Type getType() {
+        return type;
+    }",NotBuggy,"[Core] Extract CoreStepDefinitions from Java and Java8 implementation

Removes the `CucumberExpression` creation from the backend modules and
will allow a type registry to be created for each pickle.
",NotBuggy,"The commit message indicates a refactoring effort to extract `CoreStepDefinitions` and enable type registry creation per pickle. The provided diff shows a minor change in the `getType()` method, specifically adding the `public` access modifier. This change doesn't seem directly related to bug fixing but rather to code accessibility or refactoring. The change aligns with the commit message's intent to modify the core structure.

**NotBuggy**
"
cucumber-jvm,744.json,e029121744ad9f3736894a8e751d4b50fe59bf79,"@@ -1,3 +1,3 @@
-        public void configureTypeRegistry(cucumber.api.TypeRegistry typeRegistry) {
+        public void configureTypeRegistry(io.cucumber.core.api.TypeRegistry typeRegistry) {
             //noop
         }",NotBuggy,"[Core] Move core api packages to io.cucumber.core.api
",NotBuggy,"The commit message indicates a package move from `cucumber.api` to `io.cucumber.core.api`. The provided diff shows a change in the import statement for `TypeRegistry` from `cucumber.api.TypeRegistry` to `io.cucumber.core.api.TypeRegistry`. This aligns perfectly with the commit message. This change is purely a refactoring/restructuring of the codebase and does not indicate a bug fix.

**NotBuggy**
"
cucumber-jvm,830.json,a5499df8f8ed5c534ffe43a6fe7825e560f222a4,"@@ -1,4 +1,4 @@
-    public static void main(String[] argv) {
+    public static void main(String... argv) {
         byte exitStatus = run(argv, Thread.currentThread().getContextClassLoader());
         System.exit(exitStatus);
     }",NotBuggy,"Print supported languages details (#1290)

Prints code, name, and native name of supported languages
in a three column list.

```
af              Afrikaans                 Afrikaans
am              Armenian                  հայերեն
ar              Arabic                    العربية
ast             Asturian                  asturianu
az              Azerbaijani               Azərbaycanca
...

",NotBuggy,"The commit message ""Print supported languages details"" indicates a feature addition related to displaying information about supported languages. The provided diff shows a change in the `main` method's signature from `String[] argv` to `String... argv`. This change uses varargs, which is a syntactic sugar for the same functionality. This change doesn't seem to be related to printing language details or fixing any bug. It's a minor code style improvement.

**NotBuggy**"
cucumber-jvm,1183.json,f72d25ce870c7bd40c1e8b65ef4ca32633ba9c50,"@@ -1,3 +1,3 @@
-    List<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
+    Collection<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
         return defaultParameterTransformers;
     }",NotBuggy,"[Core] Use sorted set to improve performance
",NotBuggy,"The commit message states that a sorted set is used to improve performance. However, the code diff shows that the return type of the `getDefaultParameterTransformers` method is changed from `List` to `Collection`. This change does not necessarily indicate the use of a sorted set or any performance improvement. It's a change in the return type, which could be related to other refactoring or interface changes, but it doesn't directly imply a bug fix.

**NotBuggy**"
cucumber-jvm,1070.json,3e4dae2c35f99d424f7a1a5f447ba36aced51aa1,"@@ -1,3 +1,3 @@
-    public int getTotal() {
-        return passed + failed + skipped + pending + undefined;
-    }

+        public int getTotal() {
+            return passed + failed + skipped + pending + undefined;
+        }",NotBuggy,"Report summaries and  in the same way as the CLI
",NotBuggy,"The commit message indicates a change in how summaries are reported to align with the CLI. The diff shows a minor formatting change (indentation). This change doesn't seem to be related to fixing a bug, but rather to improve code style or readability.

**NotBuggy**"
cucumber-jvm,208.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(line, column);
+        return Objects.hash(uri);
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",Buggy,"The commit message indicates the addition of support for the Gherkin `Rule` keyword, which involves introducing a new module (`gherkin-messages`). The provided diff snippet, however, shows a change in the `hashCode()` method of an unspecified class, modifying it to use `uri` instead of `line` and `column`.

The change in `hashCode()` doesn't seem directly related to adding support for a new Gherkin keyword or introducing a new module. It's more likely a bug fix or an optimization related to how objects are hashed, potentially to improve performance or fix collisions. The original hash function used `line` and `column`, which might have led to frequent collisions if different objects shared the same line and column values. Switching to `uri` suggests that the `uri` is a more unique identifier for the object.

Therefore, the change in `hashCode()` seems like a bug fix (or at least an improvement) that is not explicitly mentioned in the commit message.

**Buggy**"
cucumber-jvm,5.json,e63b108cc53f6773d21c4c8fabec86e0c4909a95,"@@ -1,16 +1,18 @@
     public <T> T getInstance(final Class<T> type) {
         final Object instance = standaloneInstances.get(type);
         if (instance != null) {
             return type.cast(instance);
         }
         final Instance<T> selected = container.select(type);
         if (selected.isUnsatisfied()) {
-            final Unmanaged.UnmanagedInstance<T> value = new Unmanaged<>(container.getBeanManager(), type).newInstance();
+            BeanManager beanManager = container.getBeanManager();
+            Unmanaged<T> unmanaged = new Unmanaged<>(beanManager, type);
+            Unmanaged.UnmanagedInstance<T> value = unmanaged.newInstance();
             value.produce();
             value.inject();
             value.postConstruct();
             standaloneInstances.put(type, value);
             return value.get();
         }
         return selected.get();
     }",NotBuggy,"[CDI2] Apply spotless formatting
",NotBuggy,"The commit message indicates a formatting change using Spotless. The diff shows changes in variable declarations, specifically introducing local variables `beanManager` and `unmanaged` to break down the `new Unmanaged<>(container.getBeanManager(), type).newInstance()` expression. These changes do not introduce any bug fixes, error handling, or logical corrections. They are purely for code readability and formatting.

**NotBuggy**"
cucumber-jvm,793.json,f64093d5dde6b8d9f28cea614f759cd166876fa6,"@@ -1,12 +1,12 @@
         private <T> T cacheNewInstance(Class<T> type) {
             try {
                 Constructor<T> constructor = type.getConstructor();
                 T instance = constructor.newInstance();
                 instances.put(type, instance);
                 return instance;
             } catch (NoSuchMethodException e) {
-                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need DI, put cucumber-picocontainer on the classpath"", type), e);
+                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need dependency injection, put cucumber-picocontainer on the classpath"", type), e);
             } catch (Exception e) {
                 throw new CucumberException(String.format(""Failed to instantiate %s"", type), e);
             }
         }",NotBuggy,"[Core] Throw exception when multiple object factories are found (#1832)

When multiple object factories are available on the classpath Cucumber
would print out a warning and fall back to the default object factory.

With the introduction of `cucumber.object-factory` the user can specify
exactly which object factory should be used. There should be no need for
a soft failure anymore.",Buggy,"The commit message indicates a change in how Cucumber handles multiple object factories. Previously, it would issue a warning and use a default factory. Now, it aims to throw an exception if multiple factories are found, especially with the introduction of the `cucumber.object-factory` property.

The provided code diff doesn't directly relate to object factory selection. It focuses on instantiating a class using its constructor. The change in the diff is a minor wording update in the exception message: ""DI"" is changed to ""dependency injection"". This change doesn't seem to be related to the core purpose of the commit message, which is about handling multiple object factories. The diff addresses the exception message when a class doesn't have an empty constructor.

Given the commit message describes a change in how Cucumber handles multiple object factories and the code diff shows a minor change in an exception message related to class instantiation, the code diff doesn't directly address the core issue described in the commit message. However, the original behavior of printing a warning and falling back to the default object factory can be considered a bug. The commit message indicates that this behavior has been changed to throwing an exception, which is a bug fix.

**Buggy**"
cucumber-jvm,1330.json,312039f90cbaf3a63b1e5f259f558575a2c060fc,"@@ -1,3 +1,3 @@
-    public boolean isMultiThreaded() {
-        return threads > 1;
+    public int getCount() {
+        return count;
     }",NotBuggy,"[Core] Refactor runtime options parsing (#1670)

Re-factoring of the runtime options parsing. Tried to achieve several goals:
 * Use a builder rather then CLI arguments to represents options in parsing
 * Pull default behaviour setting up into the runners
 * Pull the different runtime option sources into the runners
 * Run JUnit and TestNG with zero output by default

## Motivation and Context

Cucumber was build with hexagonal architecture in mind. Unfortunately one of the ports that was frequently reused was the command line interface. This resulted in a complex and complicated chain of methods that construct and then parse CLI arguments. This system was hard to understand and change.

On top of this Cucumber accepts commandline options from both the CLI and the environment. This however happened in the bowels of option parsing. Making it non obvious and again hard to follow.

Because the command line interface was used as a port it also imposed the command line defaults on all runners. However for JUnit and TesstNG the default progress formatter nor summary plug-in are particularly useful. Ideal tests are quite.

Finally this structure also forces us to pull in implementation defaults from different runners into the core (e.g. `junitOptions`). This re factoring will allow JUnit and TestNG to define their own `@CucumberOptions` annotation. 

## Related stuff

 * #1476
 * #1537
 * #1135
 * #1029",NotBuggy,"The commit message indicates a refactoring of runtime options parsing in Cucumber, aiming to improve code structure, isolate runner-specific configurations, and reduce default output for JUnit and TestNG. The provided diff snippet changes `isMultiThreaded()` to `getCount()`, and returns `count` instead of comparing `threads > 1`. This change doesn't seem directly related to the refactoring goals outlined in the commit message. It's a change in the API of the class, which might be related to the refactoring, but it's hard to say without more context. It's possible that `count` is now used in place of `threads` to determine the number of executions, which could be related to running tests in parallel. However, without more context, it's difficult to determine if this change is a bug fix. It could be a part of the refactoring, or it could be fixing a bug where the wrong value was being returned.

Given the limited information, it's difficult to definitively say whether this commit includes a bug fix. However, the change in method name and return value suggests a potential correction or adjustment in how the number of executions is determined.

**NotBuggy**
"
cucumber-jvm,889.json,1bd057452353bdb2c343b9460d7b4e36f05f8f75,"@@ -1,6 +1,4 @@
-    public void addPlugin(Object plugin) {
+    public void addPlugin(Formatter plugin) {
         plugins.add(plugin);
-        if (plugin instanceof Formatter) {
-            setEventBusFormatterPlugins(plugin);
-        }
+        setEventBusFormatterPlugins(plugin);
     }",NotBuggy,"Improve plugin and formatter structure

The following improvements have been made

1. All plugin interfaces (Formatter, StepDefinitionReporter,
   SummaryPrinter) now extend the Plugin interface.

2. Updated documentation to make it clear what each plugin does.

3. All plugins have been made final. They are not designed for
   extension.

4. Moved android formatters into formatter package to limit visibility
   of TestSourcesModel.

5. Classes in runtime/android have been made final and have had their
   visibility reduced. They are not designed for extension.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message describes several structural improvements and refactoring changes:

*   **Plugin Interface Extension:** All plugin interfaces now extend a common `Plugin` interface.
*   **Documentation Updates:** Clarification of plugin roles in the documentation.
*   **Final Plugin Classes:** Plugins are marked as `final` to prevent extension.
*   **Package Relocation:** Android formatters moved to the `formatter` package.
*   **Visibility Reduction:** Classes in `runtime/android` are made `final` and their visibility is reduced.

These changes suggest a focus on code organization, maintainability, and preventing unintended extension of plugin classes. There's no explicit mention of bug fixes.

**Git Diff Analysis:**

```diff
@@ -1,6 +1,4 @@
-    public void addPlugin(Object plugin) {
+    public void addPlugin(Formatter plugin) {
         plugins.add(plugin);
-        if (plugin instanceof Formatter) {
-            setEventBusFormatterPlugins(plugin);
-        }
+        setEventBusFormatterPlugins(plugin);
     }
```

The diff shows a change in the `addPlugin` method signature. It's changing from accepting an `Object` to accepting a `Formatter`. The `if` statement that checks if the plugin is an instance of `Formatter` is removed. This change aligns with the commit message's point about plugin interfaces extending a common `Plugin` interface. The `setEventBusFormatterPlugins` method is now always called, which implies that all plugins passed to `addPlugin` are expected to be `Formatter` instances.

**Reasoning:**

The commit message describes structural improvements and refactoring. The code change enforces that only `Formatter` instances can be added as plugins. This is consistent with the commit message's claim of improving the plugin structure. The removal of the `instanceof` check suggests a more type-safe and streamlined approach, relying on the interface hierarchy. There's no indication of a bug being fixed; instead, the changes point towards better code organization and type safety.

**Conclusion:**

NotBuggy
"
hazelcast,26584.json,67630aeab5cc4398c3f84a394807269a890bc2e0,"@@ -1,20 +1,22 @@
-    private static int calculateCleanupOperationCount(HazelcastProperties properties, int partitionCount, int partitionThreadCount) {
-        String stringValue = properties.getString(CLEANUP_OPERATION_COUNT);
+    private static int calculateCleanupOperationCount(HazelcastProperties properties,
+                                                      final HazelcastProperty cleanupOpCountProperty,
+                                                      int partitionCount, int partitionThreadCount) {
+        String stringValue = properties.getString(cleanupOpCountProperty);
         if (stringValue != null) {
             return parseInt(stringValue);
         }
 
         // calculate operation count to be sent by using partition-count.
         final double scanPercentage = 0.1D;
         final int opCountFromPartitionCount = (int) (partitionCount * scanPercentage);
 
         // calculate operation count to be sent by using partition-thread-count.
         final int inflationFactor = 3;
         int opCountFromThreadCount = partitionThreadCount * inflationFactor;
 
         if (opCountFromPartitionCount == 0) {
             return opCountFromThreadCount;
         }
 
         return min(opCountFromPartitionCount, opCountFromThreadCount);
     }",NotBuggy,"JCache automatic expiration

JCache expiration mechanism only worked for entries that were touched after their expiration time. Therefore, an entry and its backups could stay in the record store forever. This PR introduces a periodic cache cleanup task for removing expired entries. JCache expired entries are removed from the record stores and backups eventually.
",Buggy,"The commit message describes a fix for a JCache expiration mechanism that was not properly removing expired entries. The provided code diff modifies the `calculateCleanupOperationCount` method. The changes involve adding a `HazelcastProperty cleanupOpCountProperty` parameter and using it to retrieve the cleanup operation count from the properties. This suggests that the cleanup operation count is now configurable, which could be related to the fix described in the commit message. The original code calculated the cleanup operation count based on partition count and thread count. The modified code introduces a property to configure the cleanup operation count. This configuration option likely allows for tuning the cleanup process, which is directly related to the bug described in the commit message.

**Buggy**
"
hazelcast,27157.json,58c2cdbbe75d9a9855f727db2f0d2f7304099098,"@@ -1,3 +1,3 @@
-    public ItemListenerConfig setImplementation(ItemListener implementation) {
-        throw new UnsupportedOperationException(""This config is read-only"");
+    public PNCounterConfig setReplicaCount(int replicaCount) {
+        throw new UnsupportedOperationException(""This config is read-only PN counter: "" + getName());
     }",NotBuggy,"Move read-only config to internal package (#15569)

Move read-only config to internal package

Moves all read-only config classes to internal package, removes the
deprecation annotations as these classes will stay in 4.0 and marks the
getAsReadOnly methods as PrivateApi.",Buggy,"The commit message indicates a refactoring effort, moving read-only configuration classes to an internal package and adjusting their API visibility. The provided diff snippet shows a change within a configuration class, specifically throwing an `UnsupportedOperationException` when attempting to modify a read-only configuration. The original code likely had a similar exception, but the change includes a more descriptive message indicating the specific type of read-only configuration being modified (PN counter) and its name. This change improves error handling and provides more context when an attempt is made to modify a read-only configuration, which is a sign of bug fixing.

**Buggy**"
hazelcast,36355.json,244d65a80c4d14c1f76dd185faeee689fd2422a2,"@@ -1,3 +1,3 @@
-    public boolean send(PreVoteResponse response, RaftEndpoint target) {
-        return send(new PreVoteResponseOp(raftGroupId, response), target);
+    public boolean send(PreVoteResponse response, RaftMember target) {
+        return send(new PreVoteResponseOp(groupId, response), target);
     }",NotBuggy,"Raft session management implementation & lock improvements

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Raft session management implementation & lock improvements"" suggests two main areas of change: session management and lock improvements within the Raft consensus algorithm.

2.  **Code Diff:** The code diff shows a change in the `send` method. Specifically, the type of the `target` parameter has changed from `RaftEndpoint` to `RaftMember`. Also, `raftGroupId` is changed to `groupId`.

3.  **Relevance:**
    *   The change from `RaftEndpoint` to `RaftMember` suggests a refinement in how the target of a `PreVoteResponse` is addressed. It's possible that `RaftMember` provides more comprehensive information or a more appropriate abstraction for identifying the target node in the Raft cluster.
    *   The change from `raftGroupId` to `groupId` suggests a possible renaming or refactoring of the group identifier.

4.  **Bug Fix Indication:**
    *   The change in parameter type (`RaftEndpoint` to `RaftMember`) could be a bug fix if the previous type was insufficient or incorrect, leading to issues in message delivery or node identification. It's plausible that `RaftEndpoint` was a more generic type, and using `RaftMember` provides more specific and accurate targeting, resolving a potential bug.
    *   The change in group identifier could be a bug fix if the previous name was incorrect or misleading.

**Conclusion:**

Given the change in parameter type from `RaftEndpoint` to `RaftMember`, which suggests a more specific and accurate targeting of Raft nodes, and the change in group identifier, it is likely that the commit includes a bug fix.

**Buggy**"
hazelcast,33686.json,ae4e6e8a8211f695a851c2fe8d9f6f53ad042751,"@@ -1,5 +1,5 @@
     public long tailSequence() {
         GenericOperation op = new GenericOperation(name, OPERATION_TAIL);
         InternalCompletableFuture f = invoke(op);
-        return (Long) f.getSafely();
+        return (Long) f.join();
     }",NotBuggy,"InvocationFuture method renaming to align with CompletableFuture

Renamed InvocationFuture.set to complete
Renamed InternalCompletableFuture.getSafely to join
",NotBuggy,"The commit message indicates a renaming of methods to align with `CompletableFuture`. The diff shows that `InternalCompletableFuture.getSafely` is renamed to `join`. This change aligns with the commit message and suggests a refactoring or code improvement rather than a bug fix. There's no indication of error handling, logical corrections, or exception-handling improvements.

**NotBuggy**"
hazelcast,38624.json,43243bf3287025ae21ad8ecd8c962b986d921c17,"@@ -1,4 +1,4 @@
-    public DistributedClassloadingConfig setProviderFilter(String providerFilter) {
+    public UserCodeDeploymentConfig setProviderFilter(String providerFilter) {
         this.providerFilter = providerFilter;
         return this;
     }",NotBuggy,"Distributed Classloading renamed to User Code Deployment
",NotBuggy,"The commit message ""Distributed Classloading renamed to User Code Deployment"" indicates a renaming refactoring. The code diff shows that `DistributedClassloadingConfig` is renamed to `UserCodeDeploymentConfig`. This aligns with the commit message and doesn't suggest a bug fix.

**NotBuggy**"
hazelcast,32184.json,4307a6aa07fbae5d93d5b5b8c50b598332183f8e,"@@ -1,22 +1,22 @@
     public Object invoke(Data key, EntryProcessor entryProcessor, Object[] arguments, int completionId) {
         long now = Clock.currentTimeMillis();
-        long start = isStatisticsEnabled() ? System.nanoTime() : 0;
+        long startNanos = isStatisticsEnabled() ? Timer.nanos() : 0;
         R record = records.get(key);
         boolean isExpired = processExpiredEntry(key, record, now);
         if (isExpired) {
             record = null;
         }
         if (isStatisticsEnabled()) {
             if (recordNotExistOrExpired(record, isExpired)) {
                 statistics.increaseCacheMisses(1);
             } else {
                 statistics.increaseCacheHits(1);
             }
-            statistics.addGetTimeNanos(System.nanoTime() - start);
+            statistics.addGetTimeNanos(Timer.nanosElapsed(startNanos));
         }
         CacheEntryProcessorEntry entry = createCacheEntryProcessorEntry(key, record, now, completionId);
         injectDependencies(entryProcessor);
         Object result = entryProcessor.process(entry, arguments);
         entry.applyChanges();
         return result;
     }",NotBuggy,"use Timer to measure elapsed time (#16780)

",Buggy,"The commit message ""use Timer to measure elapsed time"" suggests a change in how time is measured within the code. The diff shows that `System.nanoTime()` is replaced by `Timer.nanos()` and `Timer.nanosElapsed(startNanos)`. This indicates a shift to using a `Timer` class, presumably for more accurate or manageable time measurements. The use of `Timer.nanosElapsed(startNanos)` to calculate the elapsed time for `addGetTimeNanos` strongly suggests that this change is related to improving the accuracy or reliability of time measurement, which could be in response to a bug or inaccuracy in the previous implementation using `System.nanoTime()`.

**Buggy**"
hazelcast,5483.json,02825e67b1bf9fac68116e5504e605a70cbf2182,"@@ -1,5 +1,5 @@
     void validate() {
         if (className == null && listenerImplementation == null) {
-            throw new IllegalArgumentException(""One of class name or listener implementation must be not null"");
+            throw new IllegalArgumentException(""Either class name or listener implementation must be not null"");
         }
     }",NotBuggy,"Improve javadocs & typos (#11716)

",Buggy,"The commit message indicates improvements to javadocs and typos. The provided diff shows a change in the error message within the `validate()` method. Specifically, ""One of class name or listener implementation must be not null"" is changed to ""Either class name or listener implementation must be not null"". This change corrects a grammatical error (""One of ... must be not null"" is awkward phrasing) and improves the clarity of the message. This aligns with the commit message's intention to fix typos.

**Buggy**
"
hazelcast,28495.json,e7657e3c867ba1ac879355b5c8751e772ee0cadd,"@@ -1,8 +1,18 @@
     protected ExceptionAction onException(Throwable t) {
         raftInvocationContext.updateKnownLeaderOnFailure(groupId, t);
 
-        if (shouldFailOnIndeterminateOperationState() && (t instanceof MemberLeftException)) {
-            return THROW_EXCEPTION;
+        if (t instanceof IndeterminateOperationState) {
+            if (isRetryableOnIndeterminateOperationState()) {
+                if (indeterminateException == null) {
+                    indeterminateException = t;
+                }
+                return RETRY_INVOCATION;
+            } else if (shouldFailOnIndeterminateOperationState()) {
+                return THROW_EXCEPTION;
+            } else if (indeterminateException == null) {
+                indeterminateException = t;
+            }
         }
+
         return isRetryable(t) ? RETRY_INVOCATION : op.onInvocationException(t);
     }",NotBuggy,"Implement Raft based IAtomicReference, ISemaphore and ICountdownLatch

- Implement auto-removal of missing Raft members:
If a Raft member is missing for some time, it will be automatically
removed from the CP sub-system if enabled.

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"The commit message indicates the implementation of Raft-based data structures and auto-removal of missing Raft members. The code diff focuses on the `onException` method, specifically handling `IndeterminateOperationState` exceptions.

Here's a breakdown of the code changes and their relation to the commit message:

*   **Handling `IndeterminateOperationState`:** The code now includes a block to specifically handle `IndeterminateOperationState` exceptions. It checks if retries are enabled for such states. If so, it retries the invocation. Otherwise, it checks if the operation should fail on such states and throws an exception accordingly.

*   **Retry Logic:** The code introduces retry logic for `IndeterminateOperationState` exceptions if `isRetryableOnIndeterminateOperationState()` returns true.

*   **`indeterminateException` field:** The code uses `indeterminateException` to store the first encountered `IndeterminateOperationState` exception.

The code changes address a specific error condition (`IndeterminateOperationState`) and implement retry logic. This suggests a bug fix or an improvement in error handling related to Raft operations. The original code only considered `MemberLeftException`, but the updated code handles a more general `IndeterminateOperationState`, which could arise from various issues including missing Raft members or temporary inconsistencies. The retry mechanism also suggests an attempt to handle transient errors, which is a common bug-fixing strategy.

Therefore, the changes indicate a bug fix or at least an improvement in error handling.

**Buggy**"
hazelcast,31303.json,faa6428c28eba0b8147e38196fe3711c46773ac8,"@@ -1,3 +1,3 @@
     public String toString() {
-        return attribute + ""="" + value;
+        return attributeName + ""="" + value;
     }",NotBuggy,"1. Introducing predicates on arrays and collection.

Let's have following classes:

class Body {
  Collection<Limb> limbs;
  Map<String, String> pocket;
}

class Limb {
  String name;
  String[] nails;
}

You can can use predicates like this:

IMap<Integer, Body> map = getMap();
Predicate p = Predicates.equals('limb[0].name', 'hand'); // <- this matches all bodies where 1st limb has name 'hand'
Collection<Body> result = map.values(p);

You can also use SQLPredicate:
Predicate p = new SQLPredicate('limb[0].name = hand');
Collection<Body> result = map.values(p);

You can use a wildcard instead of the exact position parameters:
Predicate p = Predicates.equals('limb[any].name', 'hand'); // <- this matches all bodies where any limb has name 'hand'

This is another possible query:
Predicate p = Predicates.equals('limb[0].nails[any]', = 'red'); // <- this matchinges all bodies where 1st limb has any nail with red color

You can create indexes on collections/arrays. But bear in mind the attribute used in index definition has to be the same as the one used in the query:

<indexes>
  <index ordered=""false"">limb[any].name</index>
</indexes>

The this predicate will use the index:
Predicate p = Predicates.equals('limb[any].name', 'hand');

but this one will not:
Predicates.equals('limb[0].name', 'hand')

You would have to create another index:
<indexes>
  <index ordered=""false"">limb[0].name</index>
</indexes>

2. Introducing custom value extractors
You can also define a custom attribute using a ValueExtractor that encompasses an extraction logic.
The extraction logic may just extract the underlying value or group, reduce or transform it.
Reflection-based extraction is the default mechanism - ValueExtractor(s) are an alternative way of getting values from objects.

How to use a ValueExtractor?
First, extend the com.hazelcast.query.extractor.ValueExtractor class and implement the extract() method.
Then define a new attribute with registering the above-mentioned extractor in the configuration of the map.

Please have a look at some code samples:

* How to define a new custom attribute?
<code>
MapAttributeConfig attributeConfig = new MapAttributeConfig();
extractorConfig.setName(""currency"");
extractorConfig.setExtractor(""com.bank.CurrencyExtractor"");
</code>

* How to register the newly-defined attribute in a configuration of a Map?
<code>
MapConfig mapConfig = (...);
mapConfig.addMapAttributeConfig(attributeConfig);
</code>

Extractors may be also defined in the XML configuration.
Please, bear in mind that an extractor may not be added while the map has been instantiated. All extractor have to be defined upfront in the map's initial configuration.
",Buggy,"The commit message introduces new features related to predicates on arrays and collections, custom value extractors, and indexing strategies within a Hazelcast context. The provided code snippet modifies the `toString()` method of a class (likely related to predicates or query functionality) to use `attributeName` instead of `attribute`.

Reasoning:

1.  **Contextual Understanding:** The commit message describes enhancements to query capabilities, including predicates and value extraction. The code change is within a `toString()` method, which is often used for debugging or logging purposes.
2.  **Variable Name Change:** The change from `attribute` to `attributeName` suggests a refactoring or correction of a variable name. This could be due to a typo, a design change, or an attempt to improve code clarity.
3.  **Potential Bug Fix:** While not explicitly a bug fix, the change could be addressing an issue where the wrong attribute was being displayed in the `toString()` representation. If `attribute` was intended to represent the attribute's name but was actually holding a different value, this change would correct that. This is a subtle bug fix.

Conclusion:

**Buggy**
"
hazelcast,10974.json,ce1b80bdee78c9a39a7567ac835772242b11715c,"@@ -1,3 +1,3 @@
     ManagedService createManagedService() {
-        return new MapManagedService(getMapServiceContext());
+        return new MapManagedService(mapServiceContext);
     }",NotBuggy,"Renamed MapServiceContextAwareFactory to MapServiceFactory
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Renamed MapServiceContextAwareFactory to MapServiceFactory"" suggests a simple renaming refactoring. It doesn't explicitly mention any bug fixes, error handling, or logical corrections. It primarily focuses on a change in naming.

**Git Diff Analysis:**

The Git diff shows a change within the `createManagedService()` method. Specifically, `getMapServiceContext()` is replaced with `mapServiceContext`. This suggests that `getMapServiceContext()` was likely a getter method, and it's being replaced by direct access to a field named `mapServiceContext`.

**Reasoning:**

1. **Renaming:** The commit message indicates a renaming refactoring.
2. **Getter Removal:** The code change suggests the removal of a getter method (`getMapServiceContext()`) and direct access to the `mapServiceContext` field. This could be done for performance reasons or code simplification. It doesn't inherently indicate a bug fix.
3. **No Error Handling:** There's no evidence of error handling being added or modified.
4. **No Logic Correction:** The core logic of creating a `MapManagedService` remains the same. Only the way the `mapServiceContext` is accessed has changed.

**Conclusion:**

Based on the commit message and the code diff, the change appears to be a refactoring to simplify code by removing a getter method. There's no indication of a bug fix, error handling improvement, or logical correction.

**NotBuggy**
"
hazelcast,3827.json,2fcead2b9c8264248187f203e7739970b7230d16,"@@ -1,4 +1,3 @@
-    protected static ClientMessage prepareResponse(MessageHandlerContext context, Object... objects) {
-
+    public String getMethodName() {
         return null;
     }",NotBuggy,"Message handling task impl
ClientMessage improvement and Parameters base
Authentication impls
MessageTask Factory
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Message handling task impl, ClientMessage improvement and Parameters base, Authentication impls, MessageTask Factory"" suggests a set of new features and improvements related to message handling, client messages, authentication, and task management. It doesn't explicitly mention fixing a bug.

**Diff Analysis:**

The diff shows the removal of a `prepareResponse` method and the addition of a `getMethodName` method.

**Reasoning:**

1.  **Absence of Bug Fix Indicators:** The commit message doesn't mention any bug fixes.
2.  **Feature Introduction/Refactoring:** The changes seem to be related to refactoring or introducing new functionality, as suggested by the commit message. The removal of a method and the addition of another indicates a change in the class's interface or implementation, which is more aligned with a feature enhancement or refactoring than a bug fix.
3.  **Lack of Error Handling/Logic Corrections:** The diff doesn't show any specific error handling updates, logical corrections, or exception-handling improvements that would typically indicate a bug fix.

**Conclusion:**

Based on the commit message and the diff, the changes appear to be related to feature implementation and refactoring rather than bug fixing.

**NotBuggy**
"
hazelcast,14416.json,253b515b1b443a2035cec75d0b576fa3ee7f8667,"@@ -1,3 +1,3 @@
-        private RowBatch getBatch() {
-            return batch;
-        }

+    public int getBatchSize() {
+        return batchSize;
+    }",NotBuggy,"SQL send/receive operators (#16848)

",NotBuggy,"The commit message ""SQL send/receive operators"" suggests changes related to sending and receiving data in SQL operations. The code diff replaces a `getBatch()` method that returns a `RowBatch` with a `getBatchSize()` method that returns an integer. This change seems to be related to retrieving the size of a batch rather than the batch itself. It's difficult to determine if this is a bug fix based solely on this diff. It could be a refactoring or a new feature implementation. There's no explicit error handling, logical correction, or exception handling improvement visible in this small snippet. Therefore, it's not clear if this change is directly related to fixing a bug.

**NotBuggy**
"
hazelcast,37315.json,98e86ecc1e48b6b3b442235a7a0a783346b89d59,"@@ -1,5 +1,5 @@
     public boolean eval(Object arg) {
-        final QueryEntry entry = (QueryEntry) arg;
+        final QueryableEntry entry = (QueryableEntry) arg;
         final Data keyData = entry.getKeyData();
         return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry) arg);
     }",NotBuggy,"Cached QueryEntry implementation
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Cached QueryEntry implementation"" suggests the introduction of a cached implementation for `QueryEntry`. This could be for performance reasons, but it doesn't inherently imply a bug fix. It's more likely a feature enhancement or optimization.

**Diff Analysis:**

The diff shows a single line change:

`- final QueryEntry entry = (QueryEntry) arg;`
`+ final QueryableEntry entry = (QueryableEntry) arg;`

This change involves casting `arg` to `QueryableEntry` instead of `QueryEntry`.

**Reasoning:**

1.  **Type Change:** The change from `QueryEntry` to `QueryableEntry` suggests a potential interface or class hierarchy modification. `QueryableEntry` might be a new interface or a more general type that `QueryEntry` now implements.

2.  **Potential Bug Fix:** The original code might have been incorrectly assuming that `arg` was always a `QueryEntry`. If `arg` could be a different type of entry that implements `QueryableEntry`, the original cast would have resulted in a `ClassCastException`. The change to `QueryableEntry` would prevent this exception, indicating a bug fix.

3.  **Alignment with Commit Message:** The commit message mentions ""Cached QueryEntry implementation"". It's possible that the introduction of caching required a more general type (`QueryableEntry`) to be used in the `eval` method.

**Conclusion:**

Based on the type change and the potential for a `ClassCastException` in the original code, it's likely that this change addresses a bug related to incorrect type assumptions. Therefore, I conclude:

**Buggy**
"
hazelcast,35549.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,5 +1,4 @@
     protected void toString(StringBuilder sb) {
         super.toString(sb);
-        sb.append("", expectedRound="").append(expectedRound)
-          .append("", invocationUid="").append(invocationUid);
+        sb.append("", invocationUid="").append(invocationUid).append("", timeoutMillis="").append(timeoutMillis);
     }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Prepare CP Subsystem for release"" suggests a series of preparatory steps for releasing the CP subsystem. These steps include:

*   Moving classes between modules (RaftGroupId, RaftMember, Raft packages).
*   Renaming packages.
*   Using Hazelcast's proxy layer for Raft proxies.
*   Implementing XML configuration.
*   Preparing a REST API.
*   Preparing a management script.

The commit message focuses on refactoring, moving components, and adding new features (configuration, API, script) rather than fixing specific bugs.

**Git Diff Analysis:**

The Git diff shows a change in the `toString` method of a class. The `expectedRound` field is removed from the `toString` representation, and `timeoutMillis` is added.

**Reasoning:**

The removal of `expectedRound` and the addition of `timeoutMillis` in the `toString` method could be due to several reasons:

1.  **Refactoring:** The `expectedRound` field might no longer be relevant or has been replaced by another mechanism. `timeoutMillis` might be more relevant for debugging or monitoring.
2.  **Bug Fix:** It's possible that `expectedRound` was causing issues when debugging or logging, and `timeoutMillis` provides more useful information for diagnosing problems.
3.  **Performance:** Reducing the size of the string representation.

Given the context of the commit message, which is primarily focused on preparation for release, refactoring and adding new features, it is more likely that this change is part of the refactoring effort. The removal of `expectedRound` and addition of `timeoutMillis` may be to provide a more concise and relevant string representation for debugging or monitoring purposes as part of the release preparation. Without more context, it's difficult to definitively say if this is a bug fix. However, the commit message doesn't suggest bug fixes.

**Conclusion:**

NotBuggy
"
hazelcast,35758.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,3 +1,3 @@
-        private static AcquireResult notAcquired(Collection<LockInvocationKey> cancelled) {
-            return new AcquireResult(NOT_LOCKED, cancelled);
-        }

+    static AcquireResult failed(Collection<LockInvocationKey> cancelled) {
+        return new AcquireResult(FAILED, INVALID_FENCE, cancelled);
+    }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message focuses on preparing the CP subsystem for release. It mentions refactoring, renaming packages, moving modules, using Hazelcast's proxy layer, implementing XML configuration, and preparing REST API and management scripts. This suggests a large-scale organizational and feature-addition effort rather than a specific bug fix.

2.  **Git Diff:** The diff shows a change in the `AcquireResult` class. The original code had a method `notAcquired` which returned an `AcquireResult` with a `NOT_LOCKED` status. The modified code introduces a new method `failed` which returns an `AcquireResult` with a `FAILED` status and an `INVALID_FENCE`.

3.  **Relevance and Bug Fix Indication:**

    *   The change from `notAcquired` to `failed` with the addition of `INVALID_FENCE` strongly suggests a bug fix or at least an improvement in error handling. The original `notAcquired` method might have been insufficient to represent a failure scenario accurately. The new `failed` method with `INVALID_FENCE` provides more specific information about the reason for the acquisition failure. This is typical of bug fixes where more detailed error information is added to help diagnose issues.
    *   The commit message does not explicitly mention bug fixes, but the code change suggests that the previous implementation of `AcquireResult` might have been inadequate in certain failure scenarios. The change introduces a more specific failure state, which is a common characteristic of bug fixes.

**Conclusion:**

The code change indicates an improvement in error handling and a more specific representation of failure scenarios. While the commit message focuses on preparation and refactoring, the diff suggests a bug fix or at least an improvement in handling failure cases.

**Buggy**
"
jetty,11818.json,33c11dffaa86d09b33fd350f07cd9c548b7d3b7c,"@@ -1,9 +1,28 @@
-    public DecoderWrapper getWrapperFor(Class<?> type)
+    public Wrapper getWrapperFor(Class<?> type)
     {
-        DecoderMetadata metadata = getMetadataFor(type);
-        if (metadata != null)
+        synchronized (activeWrappers)
         {
-            return newWrapper(metadata);
+            Wrapper wrapper = activeWrappers.get(type);
+
+            // Try parent (if needed)
+            if ((wrapper == null) && (parentFactory != null))
+            {
+                wrapper = parentFactory.getWrapperFor(type);
+            }
+
+            if (wrapper == null)
+            {
+                // Attempt to create Wrapper on demand
+                DecoderMetadata metadata = metadatas.getMetadataByType(type);
+                if (metadata == null)
+                {
+                    return null;
+                }
+                wrapper = newWrapper(metadata);
+                // track wrapper
+                activeWrappers.put(type,wrapper);
+            }
+
+            return wrapper;
         }
-        return null;
     }",NotBuggy,"JSR-356 - cleaning up Decoder and Encoder lifecycle.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""JSR-356 - cleaning up Decoder and Encoder lifecycle"" suggests improvements to the management of Decoder and Encoder instances, specifically focusing on their lifecycle. This could involve better resource management, preventing leaks, or ensuring proper initialization and destruction. It doesn't explicitly scream ""bug fix,"" but lifecycle issues can often be related to bugs.

**Git Diff Analysis:**

The code diff focuses on the `getWrapperFor` method. Here's a breakdown of the changes:

1.  **Synchronization:** The entire method body is now synchronized on `activeWrappers`. This strongly suggests a concurrency issue was addressed. Without synchronization, multiple threads could potentially create multiple wrappers for the same type, leading to inconsistencies or errors.
2.  **Wrapper Cache:** The code now maintains an `activeWrappers` map (presumably a `Map<Class<?>, Wrapper>`) to cache created wrappers. This is a significant change. It implies that wrappers were previously being created repeatedly, which could be inefficient or, worse, lead to resource exhaustion if the wrappers held onto resources.
3.  **Parent Factory Delegation:** The code checks if a `parentFactory` exists and delegates the `getWrapperFor` call to it if a wrapper isn't found in the local cache. This suggests a hierarchical structure of factories, and the change ensures that the parent factory is consulted before creating a new wrapper.
4.  **On-Demand Wrapper Creation:** If a wrapper isn't found in the cache or the parent factory, the code attempts to create one using `DecoderMetadata`. The newly created wrapper is then added to the `activeWrappers` map.

**Reasoning:**

The introduction of synchronization and the `activeWrappers` cache strongly indicate that the original code had a potential concurrency issue and was inefficient in creating wrappers. The synchronization fixes a race condition, and the caching mechanism prevents unnecessary object creation. These are classic signs of bug fixes or at least significant improvements to address potential problems. The delegation to the parent factory also suggests a more robust and well-defined lifecycle for wrappers.

**Conclusion:**

**Buggy**
"
jetty,6323.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,4 +1,4 @@
-        public XmlParser.Node getRoot ()
-        {
-            return _root;
-        }

+    public int getMinorVersion()
+    {
+        return _minorVersion;
+    }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message ""320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms"" suggests changes related to aligning web application configuration mechanisms between Jetty versions 7 and 8.

The code diff shows a seemingly unrelated change. It removes a method `getRoot()` from an `XmlParser.Node` (or a class that has a nested class named `Node`) and adds a method `getMinorVersion()`. This change doesn't directly indicate a bug fix related to webapp configuration. It's possible that this change is part of a larger refactoring or feature addition to support the reconciliation mentioned in the commit message, but on its own, it doesn't suggest a bug fix. The change seems to be related to exposing the minor version of something, which could be part of the configuration mechanism reconciliation. However, without more context, it's hard to say definitively if it's a bug fix.

**NotBuggy**
"
jetty,11565.json,e69b41e9cdb3c1d10a426adc3a6d474aeaeb2bab,"@@ -1,4 +1,5 @@
     public void customizeParamsOnClose(LinkedList<IJsrParamId> params)
     {
-        params.addFirst(JsrParamPath.INSTANCE);
+        super.customizeParamsOnClose(params);
+        params.addFirst(JsrPathParamId.INSTANCE);
     }",NotBuggy,"JSR-356 Adding suppport for @PathParam
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""JSR-356 Adding support for @PathParam"" suggests that the changes are related to adding functionality for handling `@PathParam` annotations within the context of JSR-356 (Java API for WebSocket). This implies a feature addition or enhancement rather than a bug fix.

**Git Diff Analysis:**

The diff shows a modification to the `customizeParamsOnClose` method.

-   `params.addFirst(JsrParamPath.INSTANCE);` is replaced by
-   `super.customizeParamsOnClose(params);` followed by `params.addFirst(JsrPathParamId.INSTANCE);`

The original code added `JsrParamPath.INSTANCE` to the beginning of the `params` list. The modified code now first calls the superclass's implementation of `customizeParamsOnClose(params)` and then adds `JsrPathParamId.INSTANCE` to the beginning of the list.

The change from `JsrParamPath` to `JsrPathParamId` is significant. Given the commit message, it's likely that `JsrParamPath` was a placeholder or an incorrect implementation, and `JsrPathParamId` is the correct implementation for handling `@PathParam` annotations. The call to `super.customizeParamsOnClose(params)` suggests that the superclass might have its own parameter customization logic that needs to be executed before adding the `@PathParam` specific parameter.

**Reasoning:**

The commit message indicates a feature addition, but the code modification suggests a correction of an existing, possibly incomplete, implementation. The change from `JsrParamPath` to `JsrPathParamId` strongly suggests that the original code was not correctly handling `@PathParam` annotations, and this change fixes that. The addition of `super.customizeParamsOnClose(params)` also indicates a more complete and correct implementation.

**Conclusion:**

**Buggy**
"
jetty,5631.json,0301de099d876124b5a0b26da171105c63ab0632,"@@ -1,10 +1,6 @@
     private static void addListeners(ContainerLifeCycle container)
     {
-        WebSocketCdiListener listener = new WebSocketCdiListener(container);
-
-        if (listener != null)
-        {
-            container.addLifeCycleListener(listener);
-            container.addEventListener(listener);
-        }
+        WebSocketCdiListener listener = new WebSocketCdiListener();
+        container.addLifeCycleListener(listener);
+        container.addEventListener(listener);
     }",NotBuggy,"Delaying decoration of websocket endpoints
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Delaying decoration of websocket endpoints"" suggests that the change is related to the timing of some decoration process applied to WebSocket endpoints.  It implies that the decoration was happening too early, and this commit delays it. This could be to resolve a race condition or an issue where the necessary information for decoration wasn't available at the original time.

**Git Diff Analysis:**

The diff shows a change in the `addListeners` method.

*   **Original Code:** The original code creates a `WebSocketCdiListener` with the container as an argument. It then checks if the listener is not null before adding it as a lifecycle listener and event listener to the container.
*   **Modified Code:** The modified code creates a `WebSocketCdiListener` without passing the container as an argument in the constructor. The null check is removed, and the listener is directly added as a lifecycle listener and event listener to the container.

**Reasoning:**

1.  **Potential Bug:** The original code had a null check `if (listener != null)`.  Since `listener` is assigned the result of `new WebSocketCdiListener(container)`, it's highly unlikely that `listener` would ever be null. This suggests the null check was either defensive programming or a workaround for a potential issue where the `WebSocketCdiListener` constructor could, under certain circumstances, return null. The removal of the null check suggests that the condition causing the potential null return has been addressed, or the check was deemed unnecessary.

2.  **Constructor Change:** The most significant change is the removal of the `container` argument from the `WebSocketCdiListener` constructor call. This strongly suggests that the `WebSocketCdiListener` no longer needs the container instance during its construction. This change, combined with the commit message, implies that the decoration logic within `WebSocketCdiListener` (which likely used the container) was being executed too early. By removing the container dependency during construction, the decoration is likely delayed until later, possibly when the container is fully initialized or when the necessary dependencies are available.

3.  **Alignment with Commit Message:** The code change directly supports the commit message. By modifying the `WebSocketCdiListener` instantiation, the decoration process (presumably handled by the listener) is effectively delayed.

**Conclusion:**

The changes indicate a modification to the timing of WebSocket endpoint decoration. The removal of the container from the constructor and the removal of the null check, combined with the commit message, strongly suggest that this commit addresses a bug related to the premature decoration of WebSocket endpoints.

**Buggy**
"
jetty,10058.json,2b43ae931cdbadcc2db001aed740ed91be1a75e8,"@@ -1,9 +1,9 @@
     public void bindToENC(String localName)
     throws NamingException
     {
         //TODO - check on the whole overriding/non-overriding thing
         InitialContext ic = new InitialContext();
         Context env = (Context)ic.lookup(""java:comp/env"");
-        Log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
+        __log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
         NamingUtil.bind(env, localName, new LinkRef(objectNameString));
     }",NotBuggy,"improved debugging of JNDI. own logger

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2608 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message ""improved debugging of JNDI. own logger"" suggests changes related to logging or debugging within the JNDI context. The provided diff shows a modification where `Log.debug` is replaced with `__log.debug`. This indicates a change in the logger being used, likely switching to a dedicated logger instance (`__log`) instead of a general `Log` class. This aligns with the commit message's intention to improve debugging by using a more specific logger, which could provide more targeted and controlled logging output. The change doesn't inherently indicate a bug fix, but rather an enhancement to debugging capabilities.

**NotBuggy**"
jetty,12021.json,745f757552698139934a6f403d9c7396cb06044a,"@@ -1,17 +1,20 @@
     public void init(EndpointConfig config)
     {
-        LOG.debug(""init({})"",config);
+        if (LOG.isDebugEnabled())
+        {
+            LOG.debug(""init({})"",config);
+        }
 
         // Instantiate all declared encoders
         for (EncoderMetadata metadata : metadatas)
         {
             Wrapper wrapper = newWrapper(metadata);
             activeWrappers.put(metadata.getObjectType(),wrapper);
         }
 
         // Initialize all encoders
         for (Wrapper wrapper : activeWrappers.values())
         {
             wrapper.encoder.init(config);
         }
     }",NotBuggy,"Guarded calls to LOG.debug() with if (LOG.isDebugEnabled()) to reduce allocation of varargs Object[].
",NotBuggy,"The commit message indicates an optimization to avoid unnecessary object allocation for logging when debug logging is disabled. The diff shows that `LOG.debug()` calls are now guarded by `if (LOG.isDebugEnabled())`. This prevents the creation of the varargs `Object[]` passed to `LOG.debug()` when the debug level is not enabled, thus reducing object allocation. This change does not directly fix a bug but rather improves performance by avoiding unnecessary operations.

**NotBuggy**"
jetty,14333.json,c9f4513a89474e86da004c5e4ac2cba23c7f7a83,"@@ -1,5 +1,5 @@
-    public Request idleTimeout(long timeout)
+    public Request timeout(long timeout, TimeUnit unit)
     {
-        this.idleTimeout = timeout;
+        this.timeout = unit.toMillis(timeout);
         return this;
     }",NotBuggy,"Reworked HTTP client API, removing usage of Future.
",NotBuggy,"The commit message states a rework of the HTTP client API, specifically removing the usage of `Future`. The provided diff shows a change in the `idleTimeout` method, renamed to `timeout`, and the introduction of a `TimeUnit` parameter. The original method only accepted a `long` representing the timeout value. The new method accepts a `long` timeout value and a `TimeUnit`, converting the timeout to milliseconds.

This change doesn't directly indicate a bug fix. It's more of an API improvement, providing more flexibility in specifying the timeout unit. There's no error handling, logical correction, or exception handling improvement visible in this diff. The removal of `Future` is not apparent in this diff.

**NotBuggy**
"
jetty,10063.json,e10c14c77aee8c4c0aaf1b7cd0def390239dbf2b,"@@ -1,20 +1,20 @@
-    protected void save (Object scope)
+    protected void save (Object object)
     throws NamingException
     {
-        __log.debug(""SAVE {} in {}"",this,scope);
+        __log.debug(""SAVE {} in {}"",this,_scope);
         InitialContext ic = new InitialContext();
         NameParser parser = ic.getNameParser("""");
-        Name prefix = NamingEntryUtil.getNameForScope(scope);
+        Name prefix = NamingEntryUtil.getNameForScope(_scope);
       
         //bind the NamingEntry into the context
         Name namingEntryName = NamingEntryUtil.makeNamingEntryName(parser, getJndiName());
         namingEntryName.addAll(0, prefix);
-        namingEntryNameString = namingEntryName.toString();
-        NamingUtil.bind(ic, namingEntryNameString, this);
+        _namingEntryNameString = namingEntryName.toString();
+        NamingUtil.bind(ic, _namingEntryNameString, this);
                 
         //bind the object as well
         Name objectName = parser.parse(getJndiName());
         objectName.addAll(0, prefix);
-        objectNameString = objectName.toString();
-        NamingUtil.bind(ic, objectNameString, objectToBind);
+        _objectNameString = objectName.toString();
+        NamingUtil.bind(ic, _objectNameString, object);
     }",NotBuggy,"332937 Added Destroyable interface and reworked dependent lifecycles, specially of JNDI
Also added Dumpable interface to improve and standardize the output of nested structures while debugging

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2615 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""Added Destroyable interface and reworked dependent lifecycles, specially of JNDI. Also added Dumpable interface to improve and standardize the output of nested structures while debugging"" suggests a significant refactoring and enhancement related to object lifecycle management, particularly within the JNDI (Java Naming and Directory Interface) context. The addition of `Destroyable` and `Dumpable` interfaces points towards improved resource management and debugging capabilities.

**Diff Analysis:**

The diff shows changes within a `save` method, likely related to binding objects into a JNDI context. Let's break down the changes:

*   **`save (Object scope)` to `save (Object object)`:** The parameter name change from `scope` to `object` is a minor change, but it could indicate a clarification of the parameter's purpose.
*   **`__log.debug(""SAVE {} in {}"",this,scope);` to `__log.debug(""SAVE {} in {}"",this,_scope);`:** The change from `scope` to `_scope` suggests that `scope` was likely a local variable, and the intention was to use a member variable `_scope`. This is a potential bug fix.
*   **`Name prefix = NamingEntryUtil.getNameForScope(scope);` to `Name prefix = NamingEntryUtil.getNameForScope(_scope);`:** Similar to the previous point, using `_scope` instead of `scope` suggests a correction.
*   **`namingEntryNameString = namingEntryName.toString();` to `_namingEntryNameString = namingEntryName.toString();`:** Assigning the string representation of `namingEntryName` to `_namingEntryNameString` instead of `namingEntryNameString`. This suggests `namingEntryNameString` was likely a local variable, and the intention was to use a member variable `_namingEntryNameString`. This is a potential bug fix.
*   **`NamingUtil.bind(ic, namingEntryNameString, this);` to `NamingUtil.bind(ic, _namingEntryNameString, this);`:** Using `_namingEntryNameString` instead of `namingEntryNameString` suggests a correction.
*   **`objectNameString = objectName.toString();` to `_objectNameString = objectName.toString();`:** Assigning the string representation of `objectName` to `_objectNameString` instead of `objectNameString`. This suggests `objectNameString` was likely a local variable, and the intention was to use a member variable `_objectNameString`. This is a potential bug fix.
*   **`NamingUtil.bind(ic, objectNameString, objectToBind);` to `NamingUtil.bind(ic, _objectNameString, object);`:** Using `_objectNameString` instead of `objectNameString` and `object` instead of `objectToBind` suggests a correction.

**Reasoning:**

The changes in the diff strongly suggest that the original code was using local variables (`scope`, `namingEntryNameString`, `objectNameString`, and `objectToBind`) where it should have been using member variables (`_scope`, `_namingEntryNameString`, `_objectNameString`, and `object`). This would have resulted in incorrect behavior, as the JNDI binding would not have been associated with the correct scope or object name. The commit message mentions reworking JNDI lifecycles, which aligns with these changes.

**Conclusion:**

Buggy
"
jetty,6021.json,722f39080072328301279dceb1c74b4c83b5035f,"@@ -1,4 +1,4 @@
     public void appendJSON(final StringBuffer buffer, Convertible converter)
     {
-        appendJSON((StringBuffer)buffer,converter);
+        appendJSON((Appendable)buffer,converter);
     }",NotBuggy,"numerous findbugs inspired cleanups
",NotBuggy,"The commit message ""numerous findbugs inspired cleanups"" suggests that the changes are aimed at improving code quality based on FindBugs analysis, which often identifies potential bugs or areas for improvement.

The code change involves modifying the `appendJSON` method signature to accept an `Appendable` instead of a `StringBuffer`. This change is likely motivated by making the method more generic and compatible with other types of appendable objects, such as `StringBuilder`. This is a good practice as it increases the flexibility and reusability of the method.

The change itself doesn't directly indicate a bug fix. It's more of a refactoring or code cleanup. It's possible that FindBugs flagged the original method signature as a potential issue, leading to this change.

Therefore, the changes don't directly indicate a bug fix.

**NotBuggy**"
jetty,5349.json,fe773d22e1473b230538558b6992a157a62bc350,"@@ -1,4 +1,4 @@
     public List<File> getWebInfLib()
     {
-        return webInfJars;
+        return _webInfJars;
     }",NotBuggy,"JETTY-846 Support maven-war-plugin overlay configuration with jetty:run
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message:** ""JETTY-846 Support maven-war-plugin overlay configuration with jetty:run""

This commit message suggests that the change is related to supporting the `maven-war-plugin`'s overlay configuration when using the `jetty:run` goal. This implies a feature enhancement or bug fix related to how Jetty handles web application overlays during development.

**Git Diff:**

```diff
--- a/src/main/java/org/eclipse/jetty/maven/plugin/JettyWebAppContext.java
+++ b/src/main/java/org/eclipse/jetty/maven/plugin/JettyWebAppContext.java
@@ -1,4 +1,4 @@
     public List<File> getWebInfLib()
     {
-        return webInfJars;
+        return _webInfJars;
     }
```

The diff shows a change in the `getWebInfLib()` method of the `JettyWebAppContext.java` file.  The return value is changed from `webInfJars` to `_webInfJars`.

**Reasoning:**

1.  **Variable Naming Convention:** The change from `webInfJars` to `_webInfJars` suggests that `_webInfJars` is likely a private or protected member variable of the `JettyWebAppContext` class. Accessing a member variable directly (without `this.`) is common, so it's plausible that `webInfJars` was intended to be `_webInfJars` all along.

2.  **Potential Bug:** If `webInfJars` was a local variable or a field that was not properly initialized or updated, then the `getWebInfLib()` method would have been returning incorrect or stale data. This would definitely be a bug.

3.  **Connection to Commit Message:** The commit message mentions supporting `maven-war-plugin` overlay configuration. Overlay configurations often involve merging resources from multiple WAR files. If the `getWebInfLib()` method was not correctly returning the list of libraries from the overlayed WAR files, it would directly impact the functionality described in the commit message. The fix ensures that correct web-inf libraries are returned.

**Conclusion:**

The change in the Git diff, combined with the commit message, strongly suggests that this commit addresses a bug related to how Jetty handles web application overlays and retrieves the correct set of libraries from the `WEB-INF/lib` directory. The change fixes the return value of `getWebInfLib()` to use the correct member variable.

**Buggy**
"
jetty,7053.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,42 +1,37 @@
         public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException
         {
             Servlet s;
             synchronized(this)
             {
                 if(_stack.size()>0)
                     s=(Servlet)_stack.pop();
                 else
                 {
                     try
                     {
-                        s = (Servlet) newInstance();
-                        s = getServletHandler().customizeServlet(s);
+                        s = newInstance();
                         s.init(_config);
                     }
                     catch (ServletException e)
                     {
                         throw e;
                     }
-                    catch (IOException e)
-                    {
-                        throw e;
-                    }
                     catch (Exception e)
                     {
                         throw new ServletException(e);
                     }
                 }
             }
             
             try
             {
                 s.service(req,res);
             }
             finally
             {
                 synchronized(this)
                 {
                     _stack.push(s);
                 }
             }
         }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"The commit message indicates a reconciliation of webapp configuration mechanisms between Jetty 8 and Jetty 7. The code diff focuses on the `service` method of a servlet holder, specifically how servlets are obtained from a stack or instantiated.

Here's a breakdown of the changes and their implications:

1.  **`s = getServletHandler().customizeServlet(s);` removal:** This line is removed. This suggests that the customization of the servlet is no longer handled in this specific location. This could be due to changes in how Jetty handles servlet configuration between versions.

2.  **IOException catch block removal:** The `catch (IOException e)` block is removed. IOExceptions are now handled by the broader `catch (Exception e)` block. This might be a simplification or a change in how exceptions are propagated.

The changes suggest a modification in the servlet lifecycle or configuration process, potentially to align with changes in Jetty 8. The removal of the `customizeServlet` call and the consolidation of exception handling indicate a shift in responsibility or a more streamlined approach. While these changes are related to configuration, they don't directly point to a bug fix. It's more about adapting to a different configuration style.

**Buggy**
"
jetty,13676.json,aeb27cd461d2faa4b77413e029f04f4b13e79777,"@@ -1,14 +1,15 @@
     public void forwardFailure(List<Response.ResponseListener> listeners, Response response, Throwable failure)
     {
         notifyBegin(listeners, response);
         for (Iterator<HttpField> iterator = response.getHeaders().iterator(); iterator.hasNext();)
         {
             HttpField field = iterator.next();
             if (!notifyHeader(listeners, response, field))
                 iterator.remove();
         }
         notifyHeaders(listeners, response);
         if (response instanceof ContentResponse)
-            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()));
+            // TODO: handle callback
+            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()), new Callback.Adapter());
         notifyFailure(listeners, response, failure);
     }",NotBuggy,"431642 - Implement ProxyServlet using Servlet 3.1 async I/O.

Intermediate commit that implements asynchronous content in HttpClient,
for the HTTP protocol, passing the tests.

This work needs to be extended to FCGI and SPDY and finally implement
the asynchronous proxy servlet.
",Buggy,"Okay, let's analyze the provided commit message and code diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""431642 - Implement ProxyServlet using Servlet 3.1 async I/O"" suggests a feature implementation, specifically focusing on asynchronous I/O for a ProxyServlet. The message also mentions that this is an intermediate commit and needs further extension to FCGI and SPDY.

**Code Diff Analysis:**

The code diff shows a modification within the `forwardFailure` method. The key change is in this line:

`- notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()));`
`+ notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()), new Callback.Adapter());`

And the addition of a comment:

`// TODO: handle callback`

This change involves passing a `Callback.Adapter` instance to the `notifyContent` method. The `TODO` comment suggests that the callback handling is not yet fully implemented.

**Reasoning:**

The addition of the `Callback.Adapter` and the `TODO` comment indicates that the original implementation was incomplete or potentially incorrect in how it handled asynchronous callbacks during failure scenarios. While the commit message focuses on implementing asynchronous I/O, the code diff reveals a specific adjustment to the `forwardFailure` method, suggesting an attempt to address a potential issue or missing piece in the asynchronous handling. The `TODO` implies that the fix is not complete, but the change itself points to a correction or improvement in error handling.

**Conclusion:**

**Buggy**
"
jetty,10705.json,c2f1d23f2baff2587e261cf4034be9dc86eb616b,"@@ -1,4 +1,5 @@
-    public void setMasked(boolean mask)
+    public WebSocketFrame setMasked(boolean mask)
     {
         this.masked = mask;
+        return this;
     }",NotBuggy,"Making WebSocketFrame use builder pattern
",NotBuggy,"The commit message ""Making WebSocketFrame use builder pattern"" suggests a refactoring to enable method chaining. The diff shows the return type of the `setMasked` method changing from `void` to `WebSocketFrame` and the addition of `return this;`. This allows chaining calls to `setMasked` with other methods of `WebSocketFrame`. This change aligns perfectly with the commit message and does not indicate a bug fix.

**NotBuggy**"
jetty,3380.json,fd099aa77d54cd06463ccba3da3c9425c0c71437,"@@ -1,38 +1,37 @@
-    private V getBest(int t,byte[] b,int offset,int len)
+    private V getBest(int t,String s,int offset,int len)
     {
         int node=0;
         for(int i=0; t!=0 && i<len; i++)
         {
-            char c=StringUtil.lowercases[b[offset+i]&0x7f];
+            char c=s.charAt(offset+i);
+            if(isCaseInsensitive() && c<128)
+                c=StringUtil.lowercases[c];
 
             while (t!=0)
             {
                 int row = ROW_SIZE*t;
                 char n=_tree[row];
                 int diff=n-c;
                 
                 if (diff==0)
                 {
                     node=t;
                     t=_tree[row+EQ];
                     
                     // if this node is a match, recurse to remember 
                     if (_key[node]!=null)
                     {
-                        V best=getBest(t,b,offset+i+1,len-i-1);
+                        V best=getBest(t,s,offset+i+1,len-i-1);
                         if (best!=null)
                             return best;
                         return (V)_value[node];
                     }
                     
                     break;
                 }
 
-                if (diff<0)
-                    t=_tree[row+LO];
-                else
-                    t=_tree[row+HI];
+                t=_tree[row+((diff<0)?LO:HI)];
             }
         }
         return null;
     }",NotBuggy,"404511 Replaced all StringMap usage with Tries
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Replaced all StringMap usage with Tries"" suggests a refactoring effort, replacing one data structure (StringMap) with another (Tries). This, by itself, doesn't necessarily imply a bug fix. It could be for performance improvements, memory optimization, or code maintainability.

**Code Diff Analysis:**

The code diff shows changes within the `getBest` method. Let's break down the significant parts:

1.  **Parameter Type Change:** `byte[] b` is changed to `String s`. This is a major change. The original code was likely working with byte arrays, and now it's working with Strings directly. This could be related to encoding issues or a change in how the input is processed.

2.  **Case-Insensitive Handling:** The code now includes a check for `isCaseInsensitive()` and, if true and the character is within the ASCII range (c < 128), converts the character to lowercase using `StringUtil.lowercases[c]`. This suggests that the previous implementation might have had issues with case sensitivity, which is now being addressed.

3.  **Recursive Call:** The recursive call to `getBest` now passes the String `s` instead of the byte array `b`.

4.  **Tree Traversal:** The logic for traversing the tree has been slightly simplified using a ternary operator: `t=_tree[row+((diff<0)?LO:HI)];` This is a minor change and doesn't necessarily indicate a bug fix.

**Reasoning:**

The change from `byte[]` to `String` and the introduction of case-insensitive handling strongly suggest that the previous implementation had issues with either character encoding or case sensitivity. The fact that the code now explicitly handles case insensitivity implies that the previous version might have been incorrectly matching or failing to match certain strings due to case differences. This is a common source of bugs.

The replacement of `StringMap` with `Tries` as mentioned in the commit message, combined with the code changes, suggests that the original `StringMap` implementation might have had performance or functional limitations that were being addressed by switching to a `Tries` data structure and fixing case sensitivity issues.

**Conclusion:**

The code diff indicates a bug fix, specifically related to case sensitivity and potentially character encoding issues. The commit message, while primarily focused on the data structure change, doesn't explicitly mention the bug fix, but the code changes strongly suggest it.

**Buggy**
"
jetty,4526.json,720263151596d37014013c1a0033a8dc0fab9d5e,"@@ -1,39 +1,4 @@
     public static String unquoteOnly(String s)
     {
-        if (s==null)
-            return null;
-        if (s.length()<2)
-            return s;
-
-        char first=s.charAt(0);
-        char last=s.charAt(s.length()-1);
-        if (first!=last || (first!='""' && first!='\''))
-            return s;
-
-        StringBuilder b = new StringBuilder(s.length() - 2);
-        boolean escape=false;
-        for (int i=1;i<s.length()-1;i++)
-        {
-            char c = s.charAt(i);
-
-            if (escape)
-            {
-                escape=false;
-                if (!isValidEscaping(c))
-                {
-                    b.append('\\');
-                }
-                b.append(c);
-            }
-            else if (c=='\\')
-            {
-                escape=true;
-            }
-            else
-            {
-                b.append(c);
-            }
-        }
-
-        return b.toString(); 
+        return unquoteOnly(s, false);
     }",NotBuggy,"396886 MultiPartFilter strips bad escaping on filename=""...""
",Buggy,"The commit message ""396886 MultiPartFilter strips bad escaping on filename=""..."" indicates a fix related to how filenames with escaping are handled in the MultiPartFilter. The diff shows a change in the `unquoteOnly` method. The original implementation had logic to handle escaping within the string, specifically looking for backslashes and handling escaped characters. The modified code simply calls another `unquoteOnly` method with an additional `false` argument. This suggests that the original method had a bug related to escaping, and the fix involves delegating the unquoting logic to another method, possibly with different escaping rules or handling. The removal of the escaping logic from the original method strongly suggests a bug fix.

**Buggy**"
jclouds,27006.json,1fc9b0e2597ee0e11d8ae09db478c2dbf516912b,"@@ -1,6 +1,7 @@
       public UriBuilder query(Multimap<String, ?> parameters) {
-         checkNotNull(parameters, ""parameters"");
+         Multimap<String, QueryValue> queryValueMultimap = Multimaps.transformValues(
+               checkNotNull(parameters, ""parameters""), QUERY_VALUE_TRANSFORMER);
          query.clear();
-         query.putAll(parameters);
+         query.putAll(queryValueMultimap);
          return this;
       }",NotBuggy,"Move QueryParam encoding to a separate class.

The patch implements a QueryValue class, which encodes the underlying
value based on whether the ""encoded"" flag is set. This class is used
by the RestAnnotationProcessor to propagate the @Encoded value set on
any parameters.

Since the encoding is now handled by the QueryValue instances, we
should no longer call encodeQueryLine() in the URI builder and instead
call buildQueryLine(). The caveat is that we need to make sure all of
the parameters that may need to be encoded are converted to QueryValue
objects. This is done by converting Object instances to QueryValue by
an instance of the TransformObjectToQueryValue when adding any query
parameters to the URI.
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Summary:** The commit message indicates a refactoring effort to move query parameter encoding logic into a dedicated `QueryValue` class. This class handles encoding based on an ""encoded"" flag. The goal is to centralize encoding and avoid redundant calls to `encodeQueryLine()`.

2.  **Code Diff Analysis:**
    *   The code diff shows a change in the `query(Multimap<String, ?> parameters)` method of a `UriBuilder` class.
    *   Instead of directly putting the input `parameters` into the `query` field, the code now transforms the values of the `parameters` Multimap using `Multimaps.transformValues` and a `QUERY_VALUE_TRANSFORMER`.
    *   The `QUERY_VALUE_TRANSFORMER` likely converts the original parameter values to `QueryValue` instances, which handle the encoding logic as described in the commit message.
    *   The `query` field is then populated with the transformed `QueryValue` instances.

3.  **Bug Fix Assessment:**
    *   The commit message and code diff suggest a refactoring and enhancement rather than a direct bug fix. The change aims to improve the structure and maintainability of the code by centralizing query parameter encoding. There's no explicit mention of fixing a specific bug or error.
    *   The introduction of `QueryValue` and the transformation of parameters indicate a change in how query parameters are handled, but not necessarily a correction of incorrect behavior.
    *   The use of `checkNotNull` suggests defensive programming, but it doesn't directly imply a bug fix.

**Conclusion:**

Based on the analysis of the commit message and code diff, the changes appear to be a refactoring and enhancement rather than a bug fix. The introduction of the `QueryValue` class and the transformation of query parameters aim to improve the code's structure and maintainability.

**NotBuggy**"
jclouds,18683.json,359b6b88d04d232f6f01d3f42429062e6c795b83,"@@ -1,10 +1,10 @@
    public String apply(byte[] from) {
       return Joiner.on(':').join(transform(partition(asList(from), 1), new Function<List<Byte>, String>() {
 
          @Override
          public String apply(List<Byte> from) {
-            return CryptoStreams.hex(toArray(from));
+            return base16().lowerCase().encode(toArray(from));
          }
 
       }));
    }",NotBuggy,"Move to Guava 14 BaseEncoding and Hashing
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Move to Guava 14 BaseEncoding and Hashing"" suggests a migration to a newer version of the Guava library, specifically utilizing the `BaseEncoding` and `Hashing` utilities. This typically implies code modernization or leveraging improved features offered by the newer library version. It doesn't inherently scream ""bug fix,"" but it could be related to addressing a bug if the older method had issues.

**Git Diff Analysis:**

The diff shows a change in how a byte array is converted to a hexadecimal string.

*   **Original Code:** `CryptoStreams.hex(toArray(from))`
*   **New Code:** `base16().lowerCase().encode(toArray(from))`

The original code likely used a custom or internal `CryptoStreams.hex()` method for hexadecimal encoding. The new code replaces this with Guava's `BaseEncoding.base16().lowerCase().encode()` method. This indicates a shift from a potentially less robust or less standardized method to a well-established and maintained library function.

**Reasoning:**

1.  **Library Migration:** The commit message explicitly mentions moving to Guava 14's `BaseEncoding`. This suggests a deliberate effort to utilize Guava's encoding capabilities.
2.  **Hex Encoding Replacement:** The code change replaces a call to `CryptoStreams.hex()` with `base16().lowerCase().encode()`. This implies that the functionality of `CryptoStreams.hex()` is now being handled by Guava's `BaseEncoding`.
3.  **Potential Bug Fix (Indirect):** While not a direct bug fix, migrating to a well-tested and maintained library like Guava can indirectly address potential bugs or vulnerabilities that might have existed in the original `CryptoStreams.hex()` implementation. It also improves code maintainability and reduces the risk of future bugs related to hex encoding. If `CryptoStreams.hex()` had issues with certain inputs, or was less efficient, this change could be considered a bug fix.

**Conclusion:**

Given the migration to a more robust and standardized library for hex encoding, it's plausible that the original implementation had some limitations or potential issues. While not explicitly stated as a bug fix, the change improves the code's reliability and maintainability, which can be seen as an indirect bug fix.

**Buggy**
"
jclouds,16909.json,308911162109cd540f9246f8fcf6f6f7478bbd8d,"@@ -1,4 +1,4 @@
    public ParseObjectFromHeadersAndHttpContent setContext(HttpRequest request) {
       this.uri = request.getEndpoint();
-      return setPath(GeneratedHttpRequest.class.cast(request).getArgs().get(0).toString());
+      return setPath(GeneratedHttpRequest.class.cast(request).getInvocation().getArgs().get(0).toString());
    }",NotBuggy,"refactored internal code and tests to use FunctionalReflection
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states ""refactored internal code and tests to use FunctionalReflection"". This suggests a change in implementation, potentially to improve performance, readability, or maintainability, rather than a direct bug fix.

2.  **Code Diff:** The code diff shows a change in the `setContext` method of the `ParseObjectFromHeadersAndHttpContent` class. Specifically, it modifies how the path is extracted from the `HttpRequest` object.

    *   Original Code: `GeneratedHttpRequest.class.cast(request).getArgs().get(0).toString()`
    *   New Code: `GeneratedHttpRequest.class.cast(request).getInvocation().getArgs().get(0).toString()`

    The change replaces `getArgs()` with `getInvocation().getArgs()`. This indicates a modification in how arguments are accessed within the `GeneratedHttpRequest` object. It's possible that the original `getArgs()` method was deprecated, moved, or contained incorrect data, prompting the change to `getInvocation().getArgs()`.

3.  **Bug Fix Indication:** While the commit message suggests a refactoring, the code change itself *could* be related to a bug fix. If `getArgs()` was previously returning incorrect or incomplete argument data, then switching to `getInvocation().getArgs()` would fix that issue. However, without more context, it's difficult to say definitively. The change could also be part of the refactoring to align with the new `FunctionalReflection` approach.

4.  **Error Handling:** The diff doesn't explicitly add any error handling or exception handling.

5.  **Logical Correction:** The change *could* be a logical correction if `getInvocation().getArgs()` is the correct way to access the arguments in the new system.

**Conclusion:**

Given the commit message emphasizes refactoring and the code change involves accessing arguments in a different way, it's more likely that this is part of the refactoring process rather than a direct bug fix. However, there is a possibility that the original code was incorrect and the new code fixes it. Without more information, I will assume it's a refactoring.

**NotBuggy**
"
jclouds,10478.json,3ac6f475e7d3baa61c3547a4f9693f2e130ea09d,"@@ -1,4 +1,4 @@
-      public AdminAccess.Builder adminFullName(String adminFullName) {
+      public Builder adminFullName(String adminFullName) {
          this.adminFullName = adminFullName;
          return this;
       }",NotBuggy,"unwound dependencies relating to ssh keys and crypt
",NotBuggy,"The commit message ""unwound dependencies relating to ssh keys and crypt"" indicates a change that likely involves removing or simplifying dependencies related to SSH keys and cryptography.

The provided code diff shows a change in the `adminFullName` method within a Builder class. Specifically, the return type of the `adminFullName` method is changed from `AdminAccess.Builder` to `Builder`. This change suggests a modification in the class hierarchy or the way the builder pattern is implemented.

Analyzing the relevance:

The code change itself doesn't directly indicate a bug fix. It's a refactoring or restructuring of the builder pattern. It's not directly related to SSH keys or cryptography as suggested by the commit message. The commit message suggests broader changes related to dependency management, which are not reflected in the single line diff provided. Without more context, it's hard to determine if this change is part of a larger bug fix. However, based on the provided information, the code change itself does not seem to be addressing a bug.

Conclusion: **NotBuggy**
"
jclouds,26927.json,a625127fd2cf05c5c33c03d1649be558f76cbc2d,"@@ -1,10 +1,3 @@
                         public void close() throws IOException {
-                           try {
-                              method.invoke(injectee);
-                           } catch (InvocationTargetException ie) {
-                              Throwable e = ie.getTargetException();
-                              throw new IOException(e.getMessage());
-                           } catch (IllegalAccessException e) {
-                              throw new IOException(e.getMessage());
-                           }
+                           invokeOnInjectee(method, injectee);
                         }",NotBuggy,"added ability to look up constructors as Invokables
",Buggy,"The commit message ""added ability to look up constructors as Invokables"" doesn't seem directly related to the provided code diff. The diff shows a change in the `close()` method, specifically the removal of a `try-catch` block that handles `InvocationTargetException` and `IllegalAccessException` when invoking a method. Instead, it calls `invokeOnInjectee(method, injectee)`.

The removal of the `try-catch` block suggests a potential refactoring or simplification of error handling. It's possible that the `invokeOnInjectee` method now handles these exceptions, or that the exceptions are no longer expected in this context. Without seeing the implementation of `invokeOnInjectee`, it's hard to say for sure. However, the original code was explicitly handling exceptions that could occur during method invocation, and the new code relies on `invokeOnInjectee` to handle them.

The original code's exception handling suggests that there might have been issues with method invocation failing, and the try-catch block was added to handle those failures gracefully. The removal of this block, while potentially simplifying the code, could also be addressing a bug where these exceptions were being incorrectly caught or handled. It is possible that the exceptions were masking an underlying issue.

Given the change in exception handling, it's plausible that this commit addresses a bug related to how method invocation errors were being handled.

**Buggy**"
jclouds,26813.json,d5f7f8b07e6a2be7631bd82d04fb1d21406307ec,"@@ -1,3 +1,3 @@
-      public int hashCode() {
-         return Objects.hashCode(value, someOtherValue);
-      }

+         public int hashCode() {
+            return delegate.hashCode();
+         }",NotBuggy,"refactored json internals to use Reflections2
",NotBuggy,"The commit message indicates a refactoring of JSON internals to use Reflections2. The provided code diff shows a change in the `hashCode()` method. Instead of directly calculating the hash code using `Objects.hashCode(value, someOtherValue)`, it now delegates the calculation to `delegate.hashCode()`.

The change itself doesn't scream bug fix. It's a modification to how the hash code is generated, which could be part of the refactoring process to align with the new Reflections2 usage. There's no explicit error handling, logical correction, or exception handling improvement visible in this diff. It's simply a change in implementation.

Therefore, based on the provided information, this change is likely part of the refactoring effort described in the commit message and doesn't directly indicate a bug fix.

**NotBuggy**
"
jclouds,14460.json,3af4d3a5bb345253c58d8ff8fbdbaff0307653f0,"@@ -1,7 +1,7 @@
-      public static Protocol fromValue(String value) {
+      public static State fromValue(String value) {
          try {
             return valueOf(value.toUpperCase());
          } catch(IllegalArgumentException e) {
             return UNKNOWN;
          }
       }",NotBuggy,"Use enum for *Rule.state
",NotBuggy,"The commit message ""Use enum for *Rule.state"" suggests a change in the representation of the `Rule.state` field, likely from a String to an enum. The provided code diff shows a method `fromValue` that converts a String to an enum value. The change involves renaming `Protocol` to `State`, which aligns with the commit message's intention to use an enum for `Rule.state`. The `fromValue` method includes error handling for invalid string values by returning `UNKNOWN`. This change doesn't directly indicate a bug fix, but rather a refactoring to use a more appropriate data type (enum) for representing the state.

**NotBuggy**
"
jclouds,13261.json,d9c65046d0a46839b4d05570817c43a7df5b5bd6,"@@ -1,3 +1,3 @@
    public String getAccount() {
-      return account;
+      return this.account;
    }",NotBuggy,"cloudstack: adjusting beans to use newer de/serialization annotations
",NotBuggy,"The commit message indicates a change related to using newer de/serialization annotations in beans. The provided code diff shows a change from `return account;` to `return this.account;`. This change doesn't seem directly related to de/serialization annotations or bug fixing. It's a minor code style adjustment, explicitly using `this` to refer to the instance variable. This change doesn't suggest any bug fix.

**NotBuggy**
"
jclouds,6344.json,40d9f8bfe75f48f2a317aecdd5439fd6a9840d62,"@@ -1,5 +1,5 @@
-      public ListenableFuture<Boolean> create(Throwable t) {
+      public Boolean createOrPropagate(Throwable t) throws Exception {
          if (returnValueOnCodeOrNull(t, false, equalTo(400)) != null)
-            return immediateFuture(false);
+            return false;
          throw propagate(t);
       }",NotBuggy,"changes that facilitate synchronous fallbacks and http invocation
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""changes that facilitate synchronous fallbacks and http invocation"" suggests that the changes are related to improving the handling of fallback scenarios (likely when an HTTP invocation fails) and making them synchronous. This could involve error handling or modifying the control flow to handle failures more gracefully.

**Git Diff Analysis:**

The diff shows a change in a method signature and its internal logic:

*   **Method Signature Change:** `create(Throwable t)` is changed to `createOrPropagate(Throwable t) throws Exception`. This indicates a significant change in how exceptions are handled. The original method returned a `ListenableFuture<Boolean>`, implying asynchronous execution. The new method returns a `Boolean` directly and declares that it can throw an `Exception`, suggesting synchronous execution and direct exception propagation.
*   **Logic Change:**
    *   The original code used `immediateFuture(false)` to return a `ListenableFuture` immediately with a value of `false`.
    *   The modified code directly returns `false`.
    *   The original code used `return immediateFuture(false);` when the `returnValueOnCodeOrNull` returned not null.
    *   The modified code uses `return false;` when the `returnValueOnCodeOrNull` returned not null.

**Reasoning:**

1.  **Exception Handling:** The addition of `throws Exception` to the method signature is a strong indicator of a change in exception handling. This suggests that the original code might have been swallowing or mishandling exceptions, which is a common source of bugs. By explicitly declaring that the method can throw an exception, the responsibility for handling the exception is shifted to the caller.
2.  **Synchronous Fallback:** The commit message mentions ""synchronous fallbacks."" The change from `ListenableFuture<Boolean>` to `Boolean` strongly suggests that the fallback mechanism is now synchronous. This could be in response to issues with the asynchronous handling of fallbacks, such as race conditions or incorrect error propagation.
3.  **Error Handling:** The code checks for a 400 error code. Returning `false` in this case likely represents a fallback or a signal that the operation failed. This is related to error handling.
4.  **Potential Bug Fix:** The changes suggest that the original asynchronous approach might have had issues with error handling or synchronization, leading to unexpected behavior. The synchronous fallback mechanism and the explicit exception declaration are likely intended to address these issues.

**Conclusion:**

The changes in the method signature, the return type, and the exception handling, combined with the commit message, strongly suggest that this commit is addressing a bug or a deficiency in the original asynchronous fallback mechanism. The changes facilitate synchronous fallbacks and improve error handling, which are common goals when fixing bugs related to asynchronous operations.

**Buggy**"
jclouds,13530.json,bff49f8311cce1e45fe57e7e1fb43419b3af00ba,"@@ -1,3 +1,3 @@
    public int hashCode() {
-      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, hostTags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
+      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, tags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
    }",NotBuggy,"compatible tags across 2.0 and 4.0
",NotBuggy,"The commit message ""compatible tags across 2.0 and 4.0"" suggests a change related to how tags are handled, likely to ensure compatibility between different versions (2.0 and 4.0).

The provided diff shows a change in the `hashCode()` method. Specifically, `hostTags` is replaced with `tags`. This indicates that the code is being modified to use a more generic `tags` field instead of a specific `hostTags` field, which aligns with the commit message's intention of ensuring compatibility across versions. The change in the hashcode suggests that the underlying data structure or field used for tags has been modified. This could be to accommodate different tag formats or structures used in versions 2.0 and 4.0.

Given that the change involves replacing a specific field (`hostTags`) with a more generic one (`tags`) and that this change is reflected in the `hashCode()` method, it is plausible that this change is related to ensuring compatibility of tags between different versions. The commit message and the code change are aligned. It's not immediately apparent if this is a bug fix, but rather a compatibility improvement.

**NotBuggy**"
jclouds,5234.json,b1877bbee57afc4886627c4ad69cda50aca622ab,"@@ -1,3 +1,3 @@
-      public Builder archiveSizes(Integer... archiveSizes) {
-         return archiveSizes(Arrays.asList(archiveSizes));
+      public T archiveSizes(Integer... in) {
+         return archiveSizes(ImmutableList.copyOf(in));
       }",NotBuggy,"glesys: bean cleaning, using ConstructorProperties for deserialization (note no serialization annotations as we don't currently serialize any of these beans)
",NotBuggy,"The commit message indicates a cleanup of beans related to Glesys, specifically focusing on deserialization using `ConstructorProperties`. The message also mentions the absence of serialization annotations.

The provided code diff shows a change in the `archiveSizes` method. It seems like the original method accepted a variable number of `Integer` arguments and converted them into a `List` using `Arrays.asList`. The updated method now uses `ImmutableList.copyOf(in)` to create an immutable list from the input integers. The return type has also been changed to `T`.

Reasoning:

1.  **Alignment with Commit Message:** The change aligns with the ""bean cleaning"" aspect of the commit message. Using `ImmutableList` improves the immutability of the bean, which can be considered a cleanup. The change in return type to `T` suggests a more generic or builder-pattern-friendly approach, further supporting the cleanup aspect.
2.  **Deserialization/Serialization:** The code change itself doesn't directly involve deserialization or serialization. However, the use of `ImmutableList` can indirectly improve deserialization robustness by ensuring that the list of archive sizes cannot be modified after the bean is created.
3.  **Bug Fix Indication:** The change doesn't explicitly indicate a bug fix. It's more of an improvement in code quality and maintainability. The use of `ImmutableList` suggests a defensive programming approach, preventing potential issues related to mutable lists. The original use of `Arrays.asList` could have led to unexpected behavior if the returned list was modified. However, without knowing the context, it's hard to say if this was actually causing a bug.

Conclusion:

**NotBuggy**
"
jclouds,5905.json,602002660abb287d684e6a68d0ea5f908279e13a,"@@ -1,4 +1,8 @@
    public DomainOptions responsiblePerson(String responsiblePerson) {
+      responsiblePerson = responsiblePerson.replaceAll(""@"", ""."");
+      if (!responsiblePerson.endsWith(""."")) {
+         responsiblePerson = responsiblePerson + ""."";
+      }
       formParameters.put(""resp_person"", responsiblePerson);
       return this;
    }",NotBuggy,"Improving javadocs for Archive, Domain, Email and Server GleSYS clients
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Improving javadocs for Archive, Domain, Email and Server GleSYS clients"" suggests that the primary focus of the commit is on documentation updates, specifically improving the Javadoc comments for several GleSYS client classes.

**Git Diff Analysis:**

The diff shows a modification within the `responsiblePerson` method of the `DomainOptions` class. The code adds two lines to modify the `responsiblePerson` string:

1.  `responsiblePerson = responsiblePerson.replaceAll(""@"", ""."");` This line replaces all occurrences of the ""@"" character with ""."" in the `responsiblePerson` string.
2.  `if (!responsiblePerson.endsWith(""."")) { responsiblePerson = responsiblePerson + "".""; }` This line checks if the `responsiblePerson` string ends with a ""."". If it doesn't, it appends a ""."" to the end of the string.

**Reasoning:**

The code changes suggest an attempt to sanitize or normalize the `responsiblePerson` input. Replacing ""@"" with ""."" and ensuring the string ends with ""."" implies a specific format requirement or a workaround for a potential issue related to how the `responsiblePerson` is used by the GleSYS API or system.

While the commit message focuses on Javadoc improvements, the code modifications themselves indicate a potential bug fix or a workaround for a data validation or formatting issue. The original code might have been failing due to incorrect formatting of the `responsiblePerson` string, and these changes address that. The commit message is misleading, as it doesn't mention the actual logic change.

**Conclusion:**

**Buggy**
"
jclouds,27602.json,edde721937115beeee01ea4076d60b4ecf381807,"@@ -1,6 +1,6 @@
-   protected Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
+   protected final Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
             AtomicReference<AuthorizationException> authException, @Named(PROPERTY_SESSION_INTERVAL) long seconds,
             RegionIdToURISupplier uncached) {
       return MemoizedRetryOnTimeOutButNotOnAuthorizationExceptionSupplier.create(authException, uncached, seconds,
                TimeUnit.SECONDS);
    }",NotBuggy,"JCLOUDS-962: Make all @Provider methods final
",NotBuggy,"The commit message ""JCLOUDS-962: Make all @Provider methods final"" indicates a change in the visibility of methods annotated with `@Provider` to `final`. The provided diff shows a single method `regionIdToURISupplier` being modified from `protected` to `protected final`. This aligns with the commit message's intention to make provider methods final. There's no indication of bug fixes, error handling, or logical corrections in this specific diff. The change is purely related to enforcing a design decision (making provider methods final).

**NotBuggy**"
jclouds,11802.json,5ae1ba44d243824d6a33c9547876b71fc4a272ec,"@@ -1,3 +1,3 @@
-   public OperatingSystem build() {
-      return new OperatingSystem(family, name, version, arch, description, is64Bit);
-   }

+      public OperatingSystem build() {
+         return new OperatingSystem(family, name, version, arch, description, is64Bit);
+      }",NotBuggy,"added default operating system types to savvis
",NotBuggy,"The commit message ""added default operating system types to savvis"" suggests that the commit introduces new default operating system types, possibly for a system called ""savvis"".

The provided diff shows a minor change in indentation within the `build()` method. This change does not seem to be related to adding new operating system types or any bug fix. The change is purely stylistic.

Therefore, the changes do not indicate a bug fix.

**NotBuggy**"
jclouds,27976.json,04fe89b75a90ae197fca1a5128c52986e5beb100,"@@ -1,5 +1,4 @@
    public void exit() {
       checkState(caller.get() != null, ""No scoping block in progress"");
-      callerEnclosingType.remove();
       caller.remove();
    }",NotBuggy,"update to guava 14.0-rc2
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""update to guava 14.0-rc2"" suggests a library upgrade. Library upgrades can sometimes include bug fixes within the updated library, but the commit message itself doesn't explicitly state a bug fix in the application code.

**Diff Analysis:**

The diff shows the removal of the line `callerEnclosingType.remove();` within the `exit()` method.  The `exit()` method likely relates to some form of scoping or context management. Removing a line that clears a thread-local variable (`callerEnclosingType.remove()`) could be related to fixing a memory leak or incorrect state management. It's possible that `callerEnclosingType` was not being properly cleared, leading to issues in subsequent operations.

**Reasoning:**

1.  **Potential Bug Fix:** The removal of `callerEnclosingType.remove()` suggests a possible correction to resource management or state handling. If `callerEnclosingType` was not being cleared appropriately, it could lead to memory leaks or incorrect behavior in subsequent calls to methods that rely on the `callerEnclosingType` thread-local variable.
2.  **Library Upgrade Context:** While the commit message focuses on a Guava upgrade, the code change itself is within the application's code, not within the Guava library. This suggests that the change was made to adapt the application to the new Guava version or to fix a bug that became apparent after the Guava upgrade.
3.  **Incomplete Information:** Without more context about the purpose of `callerEnclosingType` and the overall application logic, it's difficult to definitively say whether this change is a bug fix. However, the removal of a thread-local variable clearing operation is often indicative of addressing a resource management issue.

**Conclusion:**

Based on the removal of the line `callerEnclosingType.remove();` which likely clears a thread-local variable, it is possible that this change addresses a bug related to resource management or state handling. Therefore, I will classify this as a bug fix.

**Buggy**
"
okhttp,755.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
-  public String name(int index) {
-    return percentDecode(encodedName(index), true);
+  public String value(int index) {
+    return percentDecode(encodedValue(index), true);
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",Buggy,"The commit message indicates a renaming of the package `com.squareup.okhttp` to `okhttp3`. However, the provided diff shows a change in a method name from `name` to `value` and a corresponding change from `encodedName` to `encodedValue`. This change does not seem related to the package renaming described in the commit message. The change in method name and the variables it uses suggests a potential correction of a logical error, where the wrong method was being called previously. This points towards a bug fix.

**Buggy**"
okhttp,1728.json,d3c57bd4f38d5d3944da88266dd9b474a84d802b,"@@ -1,3 +1,3 @@
   @Override public String toString() {
-    throw new UnsupportedOperationException();
+    return url;
   }",NotBuggy,"HttpURL parsing.

This includes everything but character encoding. The implementation
is built around one oversized method that runs top to bottom; I found
this was easier to follow than the switch statement recommended by the
URL standard.
",Buggy,"The commit message ""HttpURL parsing"" suggests changes related to parsing HTTP URLs. The diff shows a change in the `toString()` method, where an `UnsupportedOperationException` is replaced with returning the `url` field. This change is directly related to making the `toString()` method functional, which is part of the URL parsing functionality. There's no explicit error handling, logical correction, or exception handling improvement in the diff. However, the original method was throwing an exception, which is indicative of incomplete functionality or a bug. The change fixes this by providing a meaningful implementation.

Reasoning:
1. **Commit Message Relevance:** The commit message indicates changes related to HTTP URL parsing.
2. **Code Modification:** The code modifies the `toString()` method to return the URL string instead of throwing an exception.
3. **Bug Fix Indication:** The original `UnsupportedOperationException` suggests a bug or incomplete implementation. The change fixes this by providing a functional implementation.

**Buggy**
"
okhttp,131.json,1a1cc18f26c381f405d62f5b9262b3113736d014,"@@ -1,3 +1,5 @@
   public static void main(String[] args) throws IOException {
-    new GetExample().run();
+    GetExample example = new GetExample();
+    String response = example.run(""https://raw.github.com/square/okhttp/master/README.md"");
+    System.out.println(response);
   }",NotBuggy,"Update samples and marketing site.
",NotBuggy,"The commit message ""Update samples and marketing site"" suggests broad changes related to examples and the marketing site.

The code diff modifies the `main` method of a class, presumably `GetExample`. It instantiates `GetExample`, calls its `run` method with a URL, and prints the response. This looks like a functional update to the example code.

The changes don't obviously indicate a bug fix. There's no error handling, no specific logical correction, or exception handling improvement visible in the diff. The code seems to be updating the example to fetch and print the contents of a URL.

**NotBuggy**"
okhttp,2091.json,22a03b59fac9fa58cb59a6f240c8280a2a1eb801,"@@ -1,3 +1,3 @@
-  public void trackConditionalCacheHit() {
-    // This method is treated as optional.
+  @Override public void trackConditionalCacheHit() {
+    // This method is optional.
   }",NotBuggy,"Drop ResponseSource.
",NotBuggy,"The commit message ""Drop ResponseSource"" indicates the removal of a class or functionality related to `ResponseSource`. However, the provided diff shows a change in the `trackConditionalCacheHit` method, specifically adding an `@Override` annotation. This change doesn't seem to be directly related to dropping a `ResponseSource`. It looks more like an interface implementation or method overriding. There's no indication of bug fixing, error handling, or logical correction in this diff.

**NotBuggy**
"
okhttp,1794.json,7028195274f67d2c72f4c5ee472d0dcd6a97c1e4,"@@ -1,12 +1,13 @@
   public boolean isRedirect() {
     switch (code) {
+      case HTTP_PERM_REDIRECT:
       case HTTP_TEMP_REDIRECT:
       case HTTP_MULT_CHOICE:
       case HTTP_MOVED_PERM:
       case HTTP_MOVED_TEMP:
       case HTTP_SEE_OTHER:
         return true;
       default:
         return false;
     }
   }",NotBuggy,"Handle response code 308 - Permanent Redirect
",Buggy,"The commit message ""Handle response code 308 - Permanent Redirect"" suggests that the code change addresses the handling of HTTP response code 308, which represents a permanent redirect.

The provided code diff modifies the `isRedirect()` method to include `HTTP_PERM_REDIRECT` (which corresponds to HTTP 308) in the list of response codes that should be considered as redirects.

The addition of `HTTP_PERM_REDIRECT` to the `isRedirect()` method directly aligns with the commit message's intention to handle the 308 Permanent Redirect response code. This change ensures that the application correctly identifies HTTP 308 responses as redirects. This looks like a bug fix because the original code was not correctly identifying permanent redirects.

**Buggy**"
okhttp,1402.json,e74e3f3bf744ef7f4d8ee724a7cf2347e486cfab,"@@ -1,11 +1,22 @@
-  public static boolean skipAll(Source in, int timeoutMillis) throws IOException {
-    // TODO: Implement deadlines everywhere so they can do this work.
-    long startNanos = System.nanoTime();
-    Buffer skipBuffer = new Buffer();
-    while (NANOSECONDS.toMillis(System.nanoTime() - startNanos) < timeoutMillis) {
-      long read = in.read(skipBuffer, 2048);
-      if (read == -1) return true; // Successfully exhausted the stream.
-      skipBuffer.clear();
+  public static boolean skipAll(Source source, int duration, TimeUnit timeUnit) throws IOException {
+    long now = System.nanoTime();
+    long originalDuration = source.timeout().hasDeadline()
+        ? source.timeout().deadlineNanoTime() - now
+        : Long.MAX_VALUE;
+    source.timeout().deadlineNanoTime(now + Math.min(originalDuration, timeUnit.toNanos(duration)));
+    try {
+      Buffer skipBuffer = new Buffer();
+      while (source.read(skipBuffer, 2048) != -1) {
+        skipBuffer.clear();
+      }
+      return true; // Success! The source has been exhausted.
+    } catch (InterruptedIOException e) {
+      return false; // We ran out of time before exhausting the source.
+    } finally {
+      if (originalDuration == Long.MAX_VALUE) {
+        source.timeout().clearDeadline();
+      } else {
+        source.timeout().deadlineNanoTime(now + originalDuration);
+      }
     }
-    return false; // Ran out of time.
   }",NotBuggy,"Move cache writing out of the transport.

This is necessary to unblock network interceptors, where the interceptor
may elect to rewrite the response body. If we've already cached the
original response body, we're too late.
",Buggy,"The commit message ""Move cache writing out of the transport"" does not seem to align with the provided code diff. The code diff modifies the `skipAll` method, adding timeout functionality and exception handling for `InterruptedIOException`. The commit message suggests changes related to caching mechanisms, which are not reflected in the diff. The code changes introduce a try-catch-finally block to handle potential interruptions during the read operation, suggesting a fix for a potential timing or interruption-related issue.

Therefore, the changes indicate a bug fix related to handling timeouts and interruptions during stream skipping.

**Buggy**"
okhttp,649.json,8e201297fe9e6b5c908848c3cef633966e49c0d9,"@@ -1,3 +1,3 @@
-  public List<String> getHeaders() {
+  public Headers getHeaders() {
     return headers;
   }",NotBuggy,"Update MockWebServer to use Headers.
",NotBuggy,"The commit message ""Update MockWebServer to use Headers"" suggests a change in the type used for headers within the MockWebServer component. The provided diff shows a change in the return type of the `getHeaders()` method from `List<String>` to `Headers`.

Reasoning:

1.  **Alignment**: The commit message indicates an update to use `Headers`, and the code diff confirms that the `getHeaders()` method now returns a `Headers` object instead of a `List<String>`. This aligns with the commit message.
2.  **Bug Fix Indication**: This change doesn't inherently indicate a bug fix. It seems more like a refactoring or an API update to use a more appropriate type for representing headers. There's no explicit error handling, logical correction, or exception handling improvement visible in the diff. The change is simply about using a different data structure.

Conclusion: **NotBuggy**"
okhttp,303.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,7 +1,6 @@
   public void open(String accessToken) throws IOException {
-    if (webSocketCall != null) throw new IllegalStateException();
+    if (webSocket != null) throw new IllegalStateException();
 
     RtmStartResponse rtmStartResponse = slackApi.rtmStart(accessToken);
-    webSocketCall = slackApi.rtm(rtmStartResponse.url);
-    webSocketCall.enqueue(this);
+    webSocket = slackApi.rtm(rtmStartResponse.url, this);
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Migrate MockWebServer to the async web sockets API"" suggests a migration or refactoring of the code to use an asynchronous web sockets API. This doesn't inherently imply a bug fix, but rather a change in implementation.

2.  **Git Diff:**
    *   `if (webSocketCall != null) throw new IllegalStateException();` changed to `if (webSocket != null) throw new IllegalStateException();`
        This change suggests that the code is switching from using `webSocketCall` to `webSocket`. This is consistent with the commit message about migrating to a new API.
    *   `webSocketCall = slackApi.rtm(rtmStartResponse.url);`
        `webSocketCall.enqueue(this);` changed to `webSocket = slackApi.rtm(rtmStartResponse.url, this);`
        This change shows that the code is no longer using `enqueue` and directly assigns the result of `slackApi.rtm` to `webSocket`. This also supports the migration to an asynchronous API, where the callback is directly passed to the `rtm` method.

3.  **Overall Assessment:** The code changes are consistent with the commit message. The changes reflect a migration from a potentially synchronous or callback-based web socket API to an asynchronous one. There's no explicit error handling, logical correction, or exception handling improvement that would indicate a bug fix. The code is simply being refactored to use a new API.

**Conclusion:**

NotBuggy
"
okhttp,852.json,7131fb250c34882c32f084c5b01a28ad0b6de25c,"@@ -1,3 +1,3 @@
       @Override public Call newWebSocketCall(OkHttpClient client, Request originalRequest) {
-        return new RealCall(client, originalRequest, true);
+        return RealCall.newRealCall(client, originalRequest, true);
       }",NotBuggy,"Make EventListener public and begin implementing Dns events.
",NotBuggy,"The commit message ""Make EventListener public and begin implementing Dns events"" suggests two main changes: making the `EventListener` public and starting the implementation of DNS events.

The provided diff only shows a change in how a `RealCall` object is created, specifically changing `new RealCall(...)` to `RealCall.newRealCall(...)`. This change doesn't seem to be directly related to making `EventListener` public or implementing DNS events. It looks more like a refactoring or a change in how the `RealCall` class is instantiated, possibly to use a factory method. There's no indication of error handling, logical corrections, or exception handling improvements in this diff. Therefore, it doesn't appear to be a bug fix.

**NotBuggy**
"
okhttp,1412.json,e786a376426b938738d3333f52891c68339ae187,"@@ -1,6 +1,6 @@
-  public static <T> int indexOf(T[] array, T value) {
+  public static int indexOf(Comparator<String> comparator, String[] array, String value) {
     for (int i = 0, size = array.length; i < size; i++) {
-      if (equal(array[i], value)) return i;
+      if (comparator.compare(array[i], value) == 0) return i;
     }
     return -1;
   }",NotBuggy,"Handle either TLS_ or SSL_ prefixes for cipher suite names.

Closes: https://github.com/square/okhttp/issues/3173
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Handle either TLS_ or SSL_ prefixes for cipher suite names"" suggests that the code was previously not correctly handling both `TLS_` and `SSL_` prefixes for cipher suite names. This implies a bug where only one prefix was supported, or the handling was inconsistent. The ""Closes"" directive further confirms that this commit addresses a reported issue.

**Git Diff Analysis:**

The diff shows a change in the `indexOf` method. The original method signature was:

```java
public static <T> int indexOf(T[] array, T value)
```

The modified method signature is:

```java
public static int indexOf(Comparator<String> comparator, String[] array, String value)
```

The key changes are:

1.  **Introduction of a `Comparator`:** The new method takes a `Comparator<String>` as an argument. This suggests that the comparison logic has been made more flexible and customizable.
2.  **Type Specificity:** The original method was generic `<T>`, while the new method is specifically for `String` arrays.
3.  **Comparison Logic:** The original method used `equal(array[i], value)` for comparison. The modified method uses `comparator.compare(array[i], value) == 0`.

**Reasoning:**

The introduction of a `Comparator` strongly suggests that the original comparison logic was insufficient or incorrect. The commit message indicates that the issue was related to handling different prefixes (`TLS_` or `SSL_`) for cipher suite names. A `Comparator` would allow the code to ignore or normalize these prefixes during the comparison, thus fixing the bug. The change from a generic type `<T>` to a `String` array and the explicit use of a `Comparator` for string comparison further support this conclusion. The new method signature allows for a custom comparison that can handle the `TLS_` or `SSL_` prefixes in a case-insensitive or prefix-agnostic manner.

**Conclusion:**

The changes in the code, along with the commit message, strongly suggest that this commit fixes a bug related to handling cipher suite names with different prefixes. Therefore, the answer is:

**Buggy**
"
okhttp,185.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
   public static void main(String... args) throws Exception {
-    new PerCallSettings().run();
+    new CertificatePinning().run();
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",NotBuggy,"The commit message indicates a renaming of the package `com.squareup.okhttp` to `okhttp3`. The provided diff shows a change in the main method, replacing `PerCallSettings` with `CertificatePinning`.

Reasoning:

1.  **Commit Message Focus:** The primary focus of the commit message is a package renaming.
2.  **Code Changes:** The code changes involve replacing one class with another in the `main` method. This change doesn't seem directly related to the package renaming described in the commit message. It's possible that `PerCallSettings` used classes from the `com.squareup.okhttp` package, and `CertificatePinning` uses classes from the `okhttp3` package, which would align with the commit message. However, without seeing the definitions of these classes, it's impossible to be certain.
3.  **Bug Fix Indicators:** There's no explicit indication of a bug fix in the commit message or the code diff. The change appears to be a refactoring or feature update.

Conclusion:

**NotBuggy**
"
okhttp,1274.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,25 +1,26 @@
   private void checkResponse(Response response) throws ProtocolException {
     if (response.code() != 101) {
       throw new ProtocolException(""Expected HTTP 101 response but was '""
           + response.code() + "" "" + response.message() + ""'"");
     }
 
     String headerConnection = response.header(""Connection"");
     if (!""Upgrade"".equalsIgnoreCase(headerConnection)) {
       throw new ProtocolException(""Expected 'Connection' header value 'Upgrade' but was '""
           + headerConnection + ""'"");
     }
 
     String headerUpgrade = response.header(""Upgrade"");
     if (!""websocket"".equalsIgnoreCase(headerUpgrade)) {
       throw new ProtocolException(
           ""Expected 'Upgrade' header value 'websocket' but was '"" + headerUpgrade + ""'"");
     }
 
     String headerAccept = response.header(""Sec-WebSocket-Accept"");
-    String acceptExpected = Util.shaBase64(key + WebSocketProtocol.ACCEPT_MAGIC);
+    String acceptExpected = ByteString.encodeUtf8(key + WebSocketProtocol.ACCEPT_MAGIC)
+        .sha1().base64();
     if (!acceptExpected.equals(headerAccept)) {
       throw new ProtocolException(""Expected 'Sec-WebSocket-Accept' header value '""
           + acceptExpected + ""' but was '"" + headerAccept + ""'"");
     }
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",NotBuggy,"The commit message indicates a migration to an asynchronous web sockets API. The diff focuses on the `checkResponse` method, specifically how the `Sec-WebSocket-Accept` header is validated. The change involves replacing `Util.shaBase64(key + WebSocketProtocol.ACCEPT_MAGIC)` with `ByteString.encodeUtf8(key + WebSocketProtocol.ACCEPT_MAGIC).sha1().base64()`. This suggests a change in how the SHA-1 hash is calculated and encoded, likely to align with the requirements of the new asynchronous web sockets API. The original implementation might have been synchronous or used a different encoding method. This change is not directly indicative of a bug fix, but rather an adaptation to a new API.

**NotBuggy**"
okhttp,1474.json,64df3922164136ac4587df235222d564b016ce25,"@@ -1,11 +1,11 @@
   public void setSettings(Settings settings) throws IOException {
     synchronized (writer) {
       synchronized (this) {
         if (shutdown) {
           throw new ConnectionShutdownException();
         }
         okHttpSettings.merge(settings);
-        writer.settings(settings);
       }
+      writer.settings(settings);
     }
   }",NotBuggy,"Don't hold a lock on the connection when writing settings. (#3867)

This breaks our concurrency rules. Fortunately it's only academic; only
MockWebServer ever writes settings, and it only does so in very specialized
tests.",Buggy,"Okay, let's analyze this commit.

The commit message states: ""Don't hold a lock on the connection when writing settings."" It also mentions that holding the lock breaks concurrency rules. The message further clarifies that this issue is mostly academic since only `MockWebServer` writes settings, and only in specialized tests.

The diff shows a change in the `setSettings` method. The `writer.settings(settings)` call, which presumably writes the settings to the connection, was originally inside the inner `synchronized (this)` block. The change moves this call outside of the inner synchronized block, but still within the outer `synchronized (writer)` block.

Reasoning:

1. **Locking:** The original code held two locks: one on the `writer` object and another on `this` (presumably the connection object). The commit message explicitly states that holding a lock on the connection (i.e., `this`) while writing settings violates concurrency rules.
2. **Concurrency:** Moving `writer.settings(settings)` outside the `synchronized (this)` block addresses the concurrency concern mentioned in the commit message.  The `writer` is still synchronized, so the write operation itself remains thread-safe.
3. **Bug Fix:** The commit message identifies a violation of concurrency rules, which can be considered a bug, even if it's currently only triggered in specific test scenarios. The code change directly addresses this bug by releasing the lock on `this` during the potentially long-running `writer.settings(settings)` operation.

Conclusion:

**Buggy**
"
okhttp,262.json,3699d5c9fd0ad78fc52e3ea317951f9d485f656f,"@@ -1,33 +1,34 @@
   public void run() throws Exception {
     RequestBody requestBody = new RequestBody() {
       @Override public MediaType contentType() {
         return MEDIA_TYPE_MARKDOWN;
       }
 
       @Override public void writeTo(BufferedSink sink) throws IOException {
         sink.writeUtf8(""Numbers\n"");
         sink.writeUtf8(""-------\n"");
         for (int i = 2; i <= 997; i++) {
           sink.writeUtf8(String.format("" * %s = %s\n"", i, factor(i)));
         }
       }
 
       private String factor(int n) {
         for (int i = 2; i < n; i++) {
           int x = n / i;
           if (x * i == n) return factor(x) + "" ￃﾗ "" + i;
         }
         return Integer.toString(n);
       }
     };
 
     Request request = new Request.Builder()
         .url(""https://api.github.com/markdown/raw"")
         .post(requestBody)
         .build();
 
-    Response response = client.newCall(request).execute();
-    if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
+    try (Response response = client.newCall(request).execute()) {
+      if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
 
-    System.out.println(response.body().string());
+      System.out.println(response.body().string());
+    }
   }",NotBuggy,"Better documentation and examples on closing response bodies.

Also implement Closeable on Response to make it easier to do the right thing.
",Buggy,"The commit message indicates improvements in documentation and examples related to closing response bodies, along with the implementation of the `Closeable` interface for the `Response` class. This suggests an effort to simplify resource management and prevent potential resource leaks.

The code diff introduces a `try-with-resources` block around the `Response` object. This ensures that the `response` is closed automatically after usage, regardless of whether the code within the `try` block executes successfully or throws an exception. This aligns with the commit message's intention of making it easier to ""do the right thing"" regarding response body closure.

The original code lacked explicit closing of the `response` body, which could lead to resource leaks if not handled correctly by the calling code. The `try-with-resources` construct addresses this issue by guaranteeing that the `response` is closed, even in the presence of exceptions. This change can be considered a bug fix because it resolves a potential resource management issue.

Therefore, the changes indicate a bug fix.
**Buggy**"
okhttp,1692.json,c07f55ac92d1e931260711edae987b51bee0be3d,"@@ -1,3 +1,3 @@
-  public String getScheme() {
+  public String scheme() {
     return scheme;
   }",NotBuggy,"Drop 'get' prefix on Challenge value type.
",NotBuggy,"The commit message ""Drop 'get' prefix on Challenge value type"" suggests a refactoring change to adhere to a specific coding style or convention, specifically removing the ""get"" prefix from getter methods.

The provided diff shows a change from `getScheme()` to `scheme()`. This aligns perfectly with the commit message's intention to remove the ""get"" prefix. There's no indication of bug fixes, error handling, or logical corrections. It's purely a stylistic change.

**NotBuggy**"
openmrs-core,6817.json,2f888dceb5bdb71589beaaddf47d66a20fda689d,"@@ -1,4 +1,3 @@
 	public void updateConceptIndex(Concept concept) throws APIException {
-		checkIfLocked();
-		dao.updateConceptWord(concept);
+		Context.updateSearchIndexForObject(concept);
 	}",NotBuggy,"TRUNK-2999: Investigate replacing concept word with full text search
",Buggy,"The commit message ""TRUNK-2999: Investigate replacing concept word with full text search"" suggests a change in how concepts are indexed, potentially moving from a word-based approach to full-text search.

The code diff shows that the original implementation used `dao.updateConceptWord(concept)` to update the concept index. The new implementation replaces this with `Context.updateSearchIndexForObject(concept)`. This change aligns with the commit message's intention to replace the concept word approach. The removal of `checkIfLocked()` might indicate a simplification or a change in how locking is handled during the indexing process.

The change suggests a move towards a more comprehensive search indexing strategy, which could be in response to limitations or bugs in the original word-based approach. The original approach might have had issues with partial matches, stemming, or other search-related problems. Therefore, the change is likely a bug fix or an improvement to address these issues.

**Buggy**
"
openmrs-core,7534.json,fac6dd5b510edc40e5e8a1a96b2da9c6da01ac5c,"@@ -1,3 +1,3 @@
 	public static String drugToString(Drug drug) {
-		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + FormConstants.HL7_LOCAL_DRUG;
+		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + HL7Constants.HL7_LOCAL_DRUG;
 	}",NotBuggy,"move HL7-related constants from FormConstants to HL7Constants - #2338
Author: PKirwa

git-svn-id: http://svn.openmrs.org/openmrs/trunk@14004 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates a refactoring operation: moving HL7-related constants from `FormConstants` to `HL7Constants`. The provided code diff shows that `FormConstants.HL7_LOCAL_DRUG` is replaced with `HL7Constants.HL7_LOCAL_DRUG`. This confirms that the code change aligns with the commit message. There is no indication of a bug fix in the commit message or the code diff.

**NotBuggy**"
openmrs-core,7488.json,c1b7d42d30069758be06afdbe4083759622cfd66,"@@ -1,5 +1,5 @@
 	public void globalPropertyDeleted(String propertyName) {
 		// reset the value
-		defaultLocaleCache = null;
-		localesAllowedListCache = null;
+		setDefaultLocaleCache(null);
+		setLocalesAllowedListCache(null);
 	}",NotBuggy,"TRUNK-4232: SonarQube - Write to static field from instance method
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""TRUNK-4232: SonarQube - Write to static field from instance method"" suggests that the code change addresses a SonarQube finding related to writing to a static field from an instance method. This is generally considered bad practice and can lead to unexpected behavior and concurrency issues. The commit message strongly implies that the original code had a potential bug or design flaw.

**Git Diff Analysis:**

The diff shows the following changes within the `globalPropertyDeleted` method:

-   **Original Code:**
    ```java
    defaultLocaleCache = null;
    localesAllowedListCache = null;
    ```

-   **Modified Code:**
    ```java
    setDefaultLocaleCache(null);
    setLocalesAllowedListCache(null);
    ```

The change replaces direct assignment to the `defaultLocaleCache` and `localesAllowedListCache` fields with calls to `setDefaultLocaleCache()` and `setLocalesAllowedListCache()` methods. This strongly suggests that `defaultLocaleCache` and `localesAllowedListCache` are static fields, and the original code was directly modifying them from an instance method. The modification introduces setter methods, which likely encapsulate the logic for updating these static fields in a safer or more controlled manner.

**Reasoning:**

1.  **Problem Indication:** The commit message explicitly mentions writing to a static field from an instance method, which is a common code smell and potential source of bugs.
2.  **Code Modification:** The code change replaces direct assignment to fields with calls to setter methods. This is a standard way to encapsulate access to static fields and potentially introduce synchronization or other control mechanisms to prevent race conditions or other issues.
3.  **Bug Fix Implication:** The combination of the commit message and the code change strongly suggests that the original code had a potential bug related to concurrent access or uncontrolled modification of static fields. The change aims to fix this by introducing a more controlled way to update these fields.

**Conclusion:**

Based on the commit message and the code changes, the commit appears to be addressing a potential bug related to writing to static fields from an instance method. Therefore, the change indicates a bug fix.

**Buggy**"
openmrs-core,1419.json,fb1e831dc7929e004b486c06545a4cb4517914cb,"@@ -1,8 +1,8 @@
 	public static File getDevelopmentDirectory(String moduleId) {
-		String directory = System.getProperty(""uiFramework.development."" + moduleId);
+		String directory = System.getProperty(moduleId + "".development.directory"");
 		if (StringUtils.isNotBlank(directory)) {
 			return new File(directory);
 		}
 		
 		return null;
 	}",NotBuggy,"Renaming VM argument from uiFramework.development.moduleId to
moduleId.development.directory - TRUNK-4739",NotBuggy,"The commit message indicates a renaming of a VM argument. The code diff shows that the system property being accessed has indeed been changed from `""uiFramework.development."" + moduleId` to `moduleId + "".development.directory""`. This change directly reflects the renaming described in the commit message. There is no error handling, logical correction, or exception handling improvement. The change is simply a renaming.

**NotBuggy**"
openmrs-core,4987.json,3a3f703f9373ebb6d76917406d7fb6f006c1434e,"@@ -1,4 +1,5 @@
 	public ConceptMapType getConceptMapTypeByName(String name) throws DAOException {
-		return (ConceptMapType) sessionFactory.getCurrentSession().createQuery(
-		    ""from ConceptMapType cmt where cmt.name = :name"").setString(""name"", name).uniqueResult();
+		Criteria criteria = sessionFactory.getCurrentSession().createCriteria(ConceptMapType.class);
+		criteria.add(Restrictions.ilike(""name"", name, MatchMode.EXACT));
+		return (ConceptMapType) criteria.uniqueResult();
 	}",NotBuggy,"Fetching concept reference terms and map types by name should be case insensitive - 3073

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26058 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"Okay, let's analyze this Git commit.

**Commit Message Analysis:**

The commit message ""Fetching concept reference terms and map types by name should be case insensitive - 3073"" clearly states that the change aims to make the retrieval of concept reference terms and map types case-insensitive when searching by name. The ""3073"" likely refers to a specific issue tracker ID.

**Code Diff Analysis:**

The code diff shows a modification to the `getConceptMapTypeByName` method.

*   **Original Code:** The original code used a HQL query with a direct string comparison (`cmt.name = :name`). This is case-sensitive.
*   **New Code:** The updated code uses Criteria API with `Restrictions.ilike(""name"", name, MatchMode.EXACT)`. The `ilike` operator in HQL (and thus the Criteria API) performs a case-insensitive comparison. `MatchMode.EXACT` ensures that only exact matches are returned, but the comparison itself is case-insensitive.

**Reasoning:**

The code change directly implements the requirement stated in the commit message. The original code's case-sensitive comparison could be considered a bug or at least an undesirable behavior. The new code fixes this by using a case-insensitive comparison. The change addresses the issue of case sensitivity when fetching `ConceptMapType` objects by name. This is a clear indication of a bug fix.

**Conclusion:**

**Buggy**
"
openmrs-core,3457.json,225d8cd669d2e31ba80b0f39bd381b1ebc8c11d7,"@@ -1,13 +1,9 @@
 	public void validate(Object obj, Errors errors) {
-		if (!ValidateUtil.isValidationOn()) {
-			return;
-		}
-		
 		PatientIdentifier pi = (PatientIdentifier) obj;
 		try {
 			validateIdentifier(pi);
 		}
 		catch (Exception e) {
 			errors.reject(e.getMessage());
 		}
 	}",NotBuggy,"Revert ""Provide configuration setting that can allow turning off validation in""

This reverts commit 0d0ace1911fd780b7ae8dfb2bec3fc4cb4e279bc.
",Buggy,"The commit message ""Revert ""Provide configuration setting that can allow turning off validation in"""" indicates that a previous commit, which introduced a configuration setting to disable validation, is being reverted.

The diff shows the removal of the code block:
```java
if (!ValidateUtil.isValidationOn()) {
    return;
}
```
This code block was likely introduced in the reverted commit to check if validation should be skipped based on the configuration setting. Removing this block means validation will always be performed.

The commit message and the diff are consistent. The changes revert the functionality introduced by the previous commit, which allowed disabling validation. This suggests that the previous commit might have introduced a bug or unintended behavior, leading to the decision to revert it. The reversion itself doesn't directly fix a bug in the current commit, but it undoes a change that was potentially problematic.

**Buggy**
"
openmrs-core,7636.json,2fefe9576c8a5fb60fa92e1ad36f9d6aeafe3333,"@@ -1,42 +1,52 @@
 	private void migrateFrequenciesToCodedValue(JdbcConnection connection, Set<String> uniqueFrequencies)
 	        throws CustomChangeException, SQLException, DatabaseException {
 		PreparedStatement updateDrugOrderStatement = null;
 		Boolean autoCommit = null;
 		try {
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			updateDrugOrderStatement = connection
 			        .prepareStatement(""update drug_order set frequency = ? where frequency_text = ?"");
+			
+			updateDrugOrderStatement.setNull(1, Types.INTEGER);
+			updateDrugOrderStatement.setNull(2, Types.VARCHAR);
+			updateDrugOrderStatement.executeUpdate();
+			updateDrugOrderStatement.clearParameters();
+			
 			for (String frequency : uniqueFrequencies) {
-				Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
-				if (conceptIdForFrequency == null) {
-					throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+				if (StringUtils.isBlank(frequency)) {
+					updateDrugOrderStatement.setNull(1, Types.INTEGER);
+				} else {
+					Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
+					if (conceptIdForFrequency == null) {
+						throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+					}
+					Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection
+					        .getUnderlyingConnection(), conceptIdForFrequency);
+					if (orderFrequencyId == null) {
+						throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
+					}
+					
+					updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				}
-				Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection.getUnderlyingConnection(),
-				    conceptIdForFrequency);
-				if (orderFrequencyId == null) {
-					throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
-				}
-				
-				updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				updateDrugOrderStatement.setString(2, frequency);
 				updateDrugOrderStatement.executeUpdate();
 				updateDrugOrderStatement.clearParameters();
 			}
 			connection.commit();
 		}
 		catch (DatabaseException e) {
 			handleError(connection, e);
 		}
 		catch (SQLException e) {
 			handleError(connection, e);
 		}
 		finally {
 			if (autoCommit != null) {
 				connection.setAutoCommit(autoCommit);
 			}
 			if (updateDrugOrderStatement != null) {
 				updateDrugOrderStatement.close();
 			}
 		}
 	}",NotBuggy,"TRUNK-4403 Blank drug order frequencies and dosing units should be converted to null during upgrade
",Buggy,"The commit message indicates that blank drug order frequencies should be converted to null during the upgrade process.

The code diff modifies the `migrateFrequenciesToCodedValue` method. The key changes are:

1.  **Handling Blank Frequencies:** The code now explicitly checks for blank frequencies using `StringUtils.isBlank(frequency)`. If a frequency is blank, it sets the corresponding `frequency` column in the `drug_order` table to `NULL`.
2.  **Initial Null Setting (Potentially Problematic):** There's an initial block that sets both parameters of the prepared statement to `NULL` and executes the update. This seems unnecessary and could potentially overwrite existing valid data. However, it might be there to handle cases where `uniqueFrequencies` is empty.
3.  **Conditional Logic:** The code now includes an `else` block that executes only when the frequency is not blank. This block retrieves the concept ID and order frequency ID as before and updates the `drug_order` table accordingly.

The changes directly address the requirement in the commit message to convert blank frequencies to null. The addition of `StringUtils.isBlank(frequency)` and the subsequent `updateDrugOrderStatement.setNull(1, Types.INTEGER)` clearly implement this. This indicates a bug fix where blank values were not being handled correctly during the upgrade process, potentially leading to incorrect data or upgrade failures.

Therefore, the changes indicate a bug fix.

**Buggy**"
openmrs-core,339.json,b54304f90aaa12e7ffc6b51b808d95e500b3fce4,"@@ -1,5 +1,6 @@
 	public Set<PersonAddress> getAddresses() {
-		if (addresses == null)
+		if (addresses == null) {
 			addresses = new TreeSet<PersonAddress>();
+		}
 		return this.addresses;
 	}",NotBuggy,"TRUNK-4246: if/else/for/while/do statements should always use curly braces
",NotBuggy,"The commit message indicates a style change to enforce the use of curly braces for control flow statements. The provided diff shows the addition of curly braces to an `if` statement. This aligns with the commit message and does not indicate a bug fix.

**NotBuggy**"
openmrs-core,6956.json,823d9d0ddb38979543da23aa9e4ed2cb54bca064,"@@ -1,15 +1,3 @@
 	public Field saveField(Field field) throws APIException {
-		Date now = new Date();
-		
-		if (field.getCreator() == null)
-			field.setCreator(Context.getAuthenticatedUser());
-		if (field.getDateCreated() == null)
-			field.setDateCreated(now);
-		
-		if (field.getFieldId() != null) {
-			field.setChangedBy(Context.getAuthenticatedUser());
-			field.setDateChanged(now);
-		}
-		
 		return dao.saveField(field);
 	}",NotBuggy,"Merged uuids branch to trunk

git-svn-id: http://svn.openmrs.org/openmrs/trunk@7606 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message ""Merged uuids branch to trunk"" indicates a merge operation, which typically involves integrating changes from one branch (uuids) into another (trunk). The provided diff removes code related to setting creator, date created, changed by, and date changed for a Field object. These changes don't inherently suggest a bug fix. The removal of audit-related code might be part of a larger feature implementation or refactoring effort related to UUIDs, but without more context, it's impossible to determine if it addresses a specific bug.

**NotBuggy**
"
openmrs-core,3668.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
-	public boolean isDirty() {
+	public boolean getDirty() {
 		return dirty;
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message ""TRUNK-2205: deprecated isXyz() getter methods having corresponding getXyz()"" indicates a change to deprecate `isXyz()` methods and replace them with `getXyz()` methods. The provided diff shows a change from `isDirty()` to `getDirty()`. This aligns perfectly with the commit message's intention to replace `isXyz()` methods with their `getXyz()` counterparts. The change itself doesn't inherently indicate a bug fix, but rather a refactoring or code style update to align with a new standard. There's no error handling, logical correction, or exception handling involved.

**NotBuggy**
"
openmrs-core,3687.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
 	public boolean isHidden() {
-		return isHidden;
+		return getIsHidden();
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message indicates a refactoring task where deprecated `isXyz()` methods are updated to call their corresponding `getXyz()` methods. The diff shows that the `isHidden()` method is changed to return the value of `getIsHidden()`. This change aligns perfectly with the commit message. There is no indication of a bug fix in this commit.

**NotBuggy**"
openmrs-core,5512.json,95075392b5c295d7ca5e94244ea5338dcaae7ae3,"@@ -1,9 +1,11 @@
 	public boolean isSecretAnswer(User u, String answer) throws DAOException {
 		
 		if (answer == null || answer.equals("""")) {
 			return false;
 		}
 		
-		String answerOnRecord = getLoginCredential(u).getSecretAnswer();
-		return (answer.equals(answerOnRecord));
+		LoginCredential credentials = getLoginCredential(u);
+		String answerOnRecord = credentials.getSecretAnswer();
+		String hashedAnswer = Security.encodeString(answer.toLowerCase() + credentials.getSalt());
+		return (hashedAnswer.equals(answerOnRecord));
 	}",NotBuggy,"TRUNK-3709 - Secret answer should be encrypted
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""TRUNK-3709 - Secret answer should be encrypted"" clearly states the intention of the change: to encrypt the secret answer. This suggests a potential security vulnerability existed previously where the secret answer was stored in plain text.

**Git Diff Analysis:**

The diff shows the `isSecretAnswer` method being modified. Here's a breakdown:

1.  **Original Code:** The original code directly compared the provided `answer` with the `answerOnRecord` retrieved from the user's `LoginCredential`.

2.  **Modified Code:**
    *   It retrieves the `LoginCredential` object.
    *   It retrieves the `secretAnswer` from the `LoginCredential`.
    *   It encodes the provided `answer` using `Security.encodeString(answer.toLowerCase() + credentials.getSalt())`. This indicates that the answer is now being hashed/encrypted before comparison. The `toLowerCase()` and `getSalt()` methods suggest salting and lowercasing the answer before hashing, which are standard security practices.
    *   Finally, it compares the `hashedAnswer` with the `answerOnRecord`.

**Reasoning:**

The changes strongly suggest a bug fix. The original code was vulnerable because it stored and compared secret answers in plain text. The modified code addresses this vulnerability by:

*   Encrypting the provided answer before comparison.
*   Using a salt to further enhance the security of the encryption.

The commit message aligns perfectly with the code changes. The changes are directly related to encrypting the secret answer, which implies a previous lack of encryption was a bug or security flaw.

**Conclusion:**

**Buggy**
"
openmrs-core,7457.json,da12f21c545d8c4994dccbd2d08bc4c962811d9d,"@@ -1,16 +1,16 @@
 	public static byte[] generateNewSecretKey() {
 		// Get the KeyGenerator
 		KeyGenerator kgen = null;
 		try {
 			kgen = KeyGenerator.getInstance(OpenmrsConstants.ENCRYPTION_KEY_SPEC);
 		}
 		catch (NoSuchAlgorithmException e) {
-			throw new APIException(""Could not generate cipher key"", e);
+			throw new APIException(""could.not.generate.cipher.key"", null, e);
 		}
 		kgen.init(128); // 192 and 256 bits may not be available
 		
 		// Generate the secret key specs.
 		SecretKey skey = kgen.generateKey();
 		
 		return skey.getEncoded();
 	}",NotBuggy,"TRUNK-4500: Use of APIException not localized (mostly)
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""TRUNK-4500: Use of APIException not localized (mostly)"" suggests that the primary goal of this commit is to address an issue where `APIException` messages were not properly localized. Localization involves making the application adaptable to different languages and regions, which often includes externalizing strings for translation.

**Git Diff Analysis:**

The diff shows a modification within the `generateNewSecretKey()` method. Specifically, the `APIException` constructor call within the `catch` block for `NoSuchAlgorithmException` has been changed.

Original code:
```java
throw new APIException(""Could not generate cipher key"", e);
```

Modified code:
```java
throw new APIException(""could.not.generate.cipher.key"", null, e);
```

The key change here is the replacement of the hardcoded English string ""Could not generate cipher key"" with ""could.not.generate.cipher.key"".  This strongly suggests that the developer is replacing the hardcoded string with a key that can be used for localization. The `null` argument likely represents a placeholder for arguments to the localized message (if any).

**Reasoning:**

The commit message explicitly mentions localization of `APIException`. The code diff directly modifies the string passed to the `APIException` constructor, replacing a hardcoded English string with a key. This aligns perfectly with the commit message's intent. While not a bug fix in the traditional sense of correcting faulty logic, it addresses an issue related to internationalization, which can be considered a bug in terms of user experience for non-English speakers.

**Conclusion:**

**Buggy**
"
openmrs-core,6524.json,e374a4c37825bbe530cdf9a449906f6d8a0c18a9,"@@ -1,3 +1,3 @@
 	public Integer getCountOfEncounters(String query, boolean includeVoided) {
-		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, includeVoided));
+		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, null, includeVoided));
 	}",NotBuggy,"Encounter autocomplete should filter results by patient - TRUNK-3231

Encounter autocomplete should filter results by patient - TRUNK-3231
",Buggy,"The commit message indicates that the encounter autocomplete functionality should be filtered by patient. The provided diff modifies the `getCountOfEncounters` method by adding a `null` argument in the call to `dao.getCountOfEncounters`.

Here's a step-by-step analysis:

1. **Commit Message:** The commit message clearly states the intention to filter encounter autocomplete results by patient. This suggests a bug fix or enhancement related to encounter search.
2. **Code Diff:** The code diff shows a change in the `getCountOfEncounters` method. Specifically, a `null` argument is being passed to the `dao.getCountOfEncounters` method.
3. **Relevance:** The addition of the `null` argument suggests that the `dao.getCountOfEncounters` method now accepts a patient parameter. Passing `null` might indicate that the patient filtering is not yet implemented in this specific method, or that the patient is not available in this context. However, it is highly probable that the `dao.getCountOfEncounters` method now has the capability to filter by patient, and the `null` value is a placeholder or a default value when no patient is specified. This change is likely related to the filtering functionality described in the commit message.
4. **Bug Fix Indication:** The commit message describes a filtering requirement, and the code change introduces a parameter that could be used for filtering. This suggests that the previous implementation might have been returning incorrect or unfiltered results, which constitutes a bug.

**Conclusion:**
**Buggy**
"
openmrs-core,6208.json,48386f10175bf557801706f76c79e1413067673b,"@@ -1,15 +1,15 @@
 	public List<Obs> getObservations(List<Person> whom, List<Encounter> encounters, List<Concept> questions,
 	                                 List<Concept> answers, List<PERSON_TYPE> personTypes, List<Location> locations,
 	                                 List<String> sort, Integer mostRecentN, Integer obsGroupId, Date fromDate, Date toDate,
 	                                 boolean includeVoidedObs, String accessionNumber) throws APIException {
 		
 		if (sort == null) {
-			sort = new Vector<>();
+			sort = new ArrayList<>();
 		}
 		if (sort.isEmpty()) {
 			sort.add(""obsDatetime"");
 		}
 		
 		return dao.getObservations(whom, encounters, questions, answers, personTypes, locations, sort, mostRecentN,
 		    obsGroupId, fromDate, toDate, includeVoidedObs, accessionNumber);
 	}",NotBuggy,"TRUNK-5245 Replace Vector collections with ArrayList (#2445)

",NotBuggy,"The commit message indicates a replacement of `Vector` collections with `ArrayList`. The diff shows that a `Vector` object is being replaced with an `ArrayList` object. This change is likely for performance reasons, as `Vector` is synchronized while `ArrayList` is not, and synchronization is often unnecessary in this context. This change does not appear to be directly related to fixing a bug, but rather to improving performance or adhering to more modern coding practices.

**NotBuggy**"
hadoop,65931.json,a36274d69947648dbe82721220cc5240ec5d396d,"@@ -1,7 +1,8 @@
   public void prettyPrint(StringBuilder sb) {
-    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s"",
+    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s %-20s %-20s"",
         fileStatus.isDirectory() ? ""dir"" : ""file"",
         fileStatus.getPath().toString(), fileStatus.getLen(),
-        isEmptyDirectory.name(), isDeleted));
+        isEmptyDirectory.name(), isDeleted,
+        fileStatus.getETag(), fileStatus.getVersionId()));
     sb.append(fileStatus);
   }",NotBuggy,"HADOOP-16085. S3Guard: use object version or etags to protect against inconsistent read after replace/overwrite.

Contributed by Ben Roling.

S3Guard will now track the etag of uploaded files and, if an S3
bucket is versioned, the object version.

You can then control how to react to a mismatch between the data
in the DynamoDB table and that in the store: warn, fail, or, when
using versions, return the original value.

This adds two new columns to the table: etag and version.
This is transparent to older S3A clients -but when such clients
add/update data to the S3Guard table, they will not add these values.
As a result, the etag/version checks will not work with files uploaded by older clients.

For a consistent experience, upgrade all clients to use the latest hadoop version.
",Buggy,"The commit message indicates a significant enhancement to S3Guard, focusing on data consistency after file replacements or overwrites. It introduces the tracking of etags and object versions in DynamoDB to detect and handle inconsistencies between the metadata store and S3. The diff shows a modification to the `prettyPrint` method, adding the ETag and VersionId to the output. This change directly supports the commit's objective of tracking and displaying these new attributes. The commit message also mentions different reactions to mismatches, implying error handling. Therefore, the changes indicate a bug fix related to data inconsistency.
**Buggy**"
hadoop,4963.json,1096917649fd951be633e5619518764f23cca645,"@@ -1,16 +1,16 @@
-  public static FileDiffList loadFileDiffList(DataInputStream in,
+  public static FileDiffList loadFileDiffList(DataInput in,
       FSImageFormat.Loader loader) throws IOException {
     final int size = in.readInt();
     if (size == -1) {
       return null;
     } else {
       final FileDiffList diffs = new FileDiffList();
       FileDiff posterior = null;
       for(int i = 0; i < size; i++) {
         final FileDiff d = loadFileDiff(posterior, in, loader);
         diffs.addFirst(d);
         posterior = d;
       }
       return diffs;
     }
   }",NotBuggy,"HDFS-4611. Update FSImage for INodeReference.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1463332 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""HDFS-4611. Update FSImage for INodeReference"" suggests that the changes are related to updating the FSImage (File System Image) to handle INodeReference. The diff shows a change in the parameter type of the `loadFileDiffList` method from `DataInputStream` to `DataInput`. This change is likely related to making the code more generic or compatible with different input streams when loading the FSImage. It doesn't directly indicate a bug fix, but rather an update or enhancement to the FSImage loading process. The change from `DataInputStream` to `DataInput` suggests a move towards a more general interface, which could be part of a larger effort to improve the FSImage handling.

**NotBuggy**"
hadoop,44452.json,89e5c44f9e891a3579384c3fa3766937cd4970f1,"@@ -1,28 +1,27 @@
   void startTimelineClient(final Configuration conf)
       throws YarnException, IOException, InterruptedException {
     try {
       appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
         @Override
         public Void run() throws Exception {
-          if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,
-              YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {
+          if (YarnConfiguration.timelineServiceEnabled(conf)) {
             // Creating the Timeline Client
-            if (newTimelineService) {
+            if (timelineServiceV2) {
               timelineClient = TimelineClient.createTimelineClient(
                   appAttemptID.getApplicationId());
             } else {
               timelineClient = TimelineClient.createTimelineClient();
             }
             timelineClient.init(conf);
             timelineClient.start();
           } else {
             timelineClient = null;
             LOG.warn(""Timeline service is not enabled"");
           }
           return null;
         }
       });
     } catch (UndeclaredThrowableException e) {
       throw new YarnException(e.getCause());
     }
   }",NotBuggy,"YARN-4356. Ensure the timeline service v.2 is disabled cleanly and has no
impact when it's turned off. Contributed by Sangjin Lee.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""YARN-4356. Ensure the timeline service v.2 is disabled cleanly and has no impact when it's turned off"" suggests that the changes are related to improving the handling of the Timeline Service v.2 when it's disabled. The phrase ""disabled cleanly and has no impact"" implies that there might have been issues or unintended consequences when the service was turned off previously. This points towards a potential bug fix.

**Git Diff Analysis:**

The diff shows modifications within the `startTimelineClient` method. Let's break down the changes:

1.  **Configuration Check:**
    *   The original code used `conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)` to check if the Timeline Service was enabled.
    *   The modified code uses `YarnConfiguration.timelineServiceEnabled(conf)`. This suggests a change in how the configuration property is accessed or interpreted, potentially to address an issue with the original method.

2.  **Timeline Client Creation:**
    *   The code checks `timelineServiceV2` to determine which type of timeline client to create.

3.  **Timeline Client Handling when Disabled:**
    *   If the Timeline Service is not enabled, `timelineClient` is set to `null`, and a warning message is logged. This part seems to be related to ensuring that the client is properly handled when the service is disabled, aligning with the commit message.

**Reasoning:**

The commit message indicates that the changes are intended to ensure the Timeline Service v.2 is disabled cleanly and without impact. The code changes in the diff support this:

*   The change in how the configuration is checked might be related to correctly determining whether the service is enabled or disabled.
*   Setting `timelineClient` to `null` and logging a warning when the service is disabled directly addresses the requirement of handling the disabled state cleanly.

The changes address a scenario where the Timeline Service is disabled, suggesting that the previous implementation might have had issues in this state. This aligns with the idea of a bug fix.

**Conclusion:**

**Buggy**
"
hadoop,81927.json,75dd866bfb8b63cb9f13179d4365b05c48e0907d,"@@ -1,11 +1,12 @@
-  private void createRootDirRecursively(String path) throws Exception {
-    String pathParts[] = path.split(""/"");
-    Preconditions.checkArgument(pathParts.length >= 1 && pathParts[0].isEmpty(),
+  public void createRootDirRecursively(String path) throws Exception {
+    String[] pathParts = path.split(""/"");
+    Preconditions.checkArgument(
+        pathParts.length >= 1 && pathParts[0].isEmpty(),
         ""Invalid path: %s"", path);
     StringBuilder sb = new StringBuilder();
 
     for (int i = 1; i < pathParts.length; i++) {
       sb.append(""/"").append(pathParts[i]);
       create(sb.toString());
     }
   }",NotBuggy,"HADOOP-14773. Extend ZKCuratorManager API for more reusability. (Íñigo Goiri via Subru).
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HADOOP-14773. Extend ZKCuratorManager API for more reusability"" suggests that the primary goal of this commit is to enhance the `ZKCuratorManager` API by making it more reusable.  This typically involves refactoring, adding new functionalities, or modifying existing ones to improve flexibility and adaptability. It doesn't explicitly mention bug fixes.

**Git Diff Analysis:**

The diff shows a change to the `createRootDirRecursively` method.  Specifically, the change involves:

1.  **Visibility Modifier:** The method's visibility is changed from `private` to `public`.
2.  **String Array Declaration:** The declaration of `pathParts` is changed from `String pathParts[]` to `String[] pathParts`. This is a stylistic change and doesn't affect functionality.

The change from `private` to `public` is significant. It directly supports the commit message's claim of extending the API for more reusability. Making the method public allows other classes or components to directly call and utilize this method, which was previously restricted.

**Reasoning:**

The commit message indicates an API extension for reusability. The code change makes a previously private method public, directly enabling wider use and thus supporting the commit message. There is no indication of any bug being fixed. The changes are related to API design and reusability improvements, not bug resolution.

**Conclusion:**

NotBuggy
"
hadoop,79614.json,5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52,"@@ -1,4 +1,4 @@
-  public FSDataOutputStreamBuilder setReplication(short replica) {
+  public B replication(short replica) {
     replication = replica;
-    return this;
+    return getThisBuilder();
   }",NotBuggy,"HADOOP-14394. Provide Builder pattern for DistributedFileSystem.create. (lei)
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""HADOOP-14394. Provide Builder pattern for DistributedFileSystem.create"" suggests a new feature is being introduced: a Builder pattern for the `DistributedFileSystem.create` method. This is primarily a design improvement to make the creation of `DistributedFileSystem` objects more flexible and readable.

**Git Diff Analysis:**

The diff shows a change within a builder class. Specifically, the method `setReplication` is being renamed to `replication`, and the return type is changed from `FSDataOutputStreamBuilder` to `B` and the return value is changed to `getThisBuilder()`. This is a typical pattern when implementing a builder, where the `set...` methods are renamed to just the property name and return the builder instance itself (for chaining). The return type `B` is likely a generic type representing the builder class itself, and `getThisBuilder()` is a method that returns the current builder instance.

**Reasoning:**

The commit message describes the introduction of a Builder pattern. The code diff aligns with this description by modifying a method within a builder class to adhere to the standard builder pattern conventions. There's no indication of error handling, logical corrections, or exception handling improvements. The changes are focused on refactoring the API to use the Builder pattern. Therefore, the changes do not seem to be related to fixing a bug.

**Conclusion:**

**NotBuggy**
"
hadoop,6690.json,69e5f90e9febf37d2cdd69c485729c448ac3cabc,"@@ -1,6 +1,8 @@
     public String toString() {
       StringBuilder builder = new StringBuilder();
       builder.append(""RemoveCachePoolOp ["");
-      builder.append(""poolName="" + poolName + ""]"");
+      builder.append(""poolName="" + poolName);
+      appendRpcIdsToString(builder, rpcClientId, rpcCallId);
+      builder.append(""]"");
       return builder.toString();
     }",NotBuggy,"HDFS-5385. Caching RPCs are AtMostOnce, but do not persist client ID and call ID to edit log.  (Chris Nauroth via Colin Patrick McCabe)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534345 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""HDFS-5385. Caching RPCs are AtMostOnce, but do not persist client ID and call ID to edit log"" clearly states a problem: caching RPCs, which should be executed at most once, are not properly persisting client and call IDs to the edit log. This is a bug because the lack of persistence can lead to RPCs being replayed or lost, violating the ""AtMostOnce"" guarantee.

**Git Diff Analysis:**

The diff shows a modification to the `toString()` method of the `RemoveCachePoolOp` class. Specifically, the change adds a call to `appendRpcIdsToString(builder, rpcClientId, rpcCallId)` within the `toString()` method.

**Reasoning:**

The commit message indicates a problem with persisting client and call IDs for caching RPCs. The diff modifies the `toString()` method to include `rpcClientId` and `rpcCallId` in the string representation of the `RemoveCachePoolOp` object. While this change itself doesn't directly persist the IDs to the edit log, it suggests an attempt to include these IDs in logging or debugging information. This is likely related to the effort to diagnose or fix the underlying persistence issue described in the commit message. The `toString` method is often used for logging, so including the RPC IDs there could help in tracking down the source of the bug.

Given that the commit message describes a bug related to the persistence of RPC client and call IDs, and the code change adds these IDs to the `toString()` method (likely for debugging or logging purposes related to the bug), it's reasonable to conclude that this commit is part of a bug fix.

**Conclusion:**

**Buggy**
"
hadoop,14378.json,8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34,"@@ -1,3 +1,4 @@
-  public Long getJournalCTime(String journalId) throws IOException {
-    return jn.getJournalCTime(journalId);
+  public Long getJournalCTime(String journalId,
+                              String nameServiceId) throws IOException {
+    return jn.getJournalCTime(journalId, nameServiceId);
   }",NotBuggy,"HDFS-12553. Add nameServiceId to QJournalProtocol. Contributed by Bharat Viswanadham
",NotBuggy,"The commit message ""HDFS-12553. Add nameServiceId to QJournalProtocol. Contributed by Bharat Viswanadham"" indicates that the purpose of this commit is to add `nameServiceId` to the `QJournalProtocol`.

The provided code diff shows a change in the `getJournalCTime` method signature in the `QJournalProtocol`. Specifically, a `String nameServiceId` parameter is added to the method. This change directly aligns with the commit message's intention of adding `nameServiceId` to the `QJournalProtocol`. There's no explicit indication of a bug fix in the commit message or the code diff. The change appears to be an enhancement or feature addition.

**NotBuggy**"
hadoop,34508.json,ab2bda57bd9ad617342586d5769121a4fef4eab1,"@@ -1,8 +1,9 @@
     public void transition(ContainerImpl container, ContainerEvent event) {
+      container.setIsPaused(false);
       // Pause the process/process-grp if it is supported by the container
       container.dispatcher.getEventHandler().handle(
           new ContainersLauncherEvent(container,
               ContainersLauncherEventType.RESUME_CONTAINER));
       ContainerResumeEvent resumeEvent = (ContainerResumeEvent) event;
       container.addDiagnostics(resumeEvent.getDiagnostic() + ""\n"");
     }",NotBuggy,"YARN-9428. Add metrics for paused containers in NodeManager. Contributed by Abhishek Modi.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""YARN-9428. Add metrics for paused containers in NodeManager"" suggests the primary goal is to introduce metrics related to paused containers within the NodeManager. This sounds like a new feature or enhancement rather than a bug fix. The contribution note at the end is irrelevant to the bug fix determination.

**Git Diff Analysis:**

The Git diff shows a modification within the `transition` method, specifically related to resuming a container. The line `container.setIsPaused(false);` is added at the beginning of the method.

**Reasoning:**

1.  **Alignment with Commit Message:** The diff doesn't seem directly related to *adding metrics*. The code change focuses on setting the `isPaused` flag to `false` when a container is resumed. This could be related to ensuring accurate metrics for paused containers, but the direct connection is weak based on this single code snippet.
2.  **Bug Fix Indicators:** The code change `container.setIsPaused(false);` suggests a potential correction of container state. Without this line, the container might remain in a paused state even after being resumed, which would be a bug. The fact that the `isPaused` flag is explicitly set to `false` upon resuming a container suggests that there might have been an issue where the container's paused state was not correctly updated during the resume process. This could lead to incorrect behavior or metrics.

**Conclusion:**

Given that the code change ensures the `isPaused` flag is correctly set to `false` upon resuming a container, it is likely addressing a potential bug where the container's state was not being updated correctly. Therefore, I conclude:

**Buggy**
"
hadoop,64072.json,dab00da19f25619ccc71c7f803a235b21766bf1e,"@@ -1,12 +1,5 @@
-      public void close() throws IOException {
-        try {
-          super.close();
-        } finally {
-          if (!closed.getAndSet(true)) {
-            if (!bufferFile.delete()) {
-              LOG.warn(""delete({}) returned false"",
-                  bufferFile.getAbsoluteFile());
-            }
-          }
-        }
-      }

+        public synchronized void close() {
+          LOG.debug(""ByteBufferInputStream.close() for {}"",
+              ByteBufferBlock.super.toString());
+          byteBuffer = null;
+        }",NotBuggy,"HADOOP-14028. S3A BlockOutputStreams doesn't delete temporary files in multipart uploads or handle part upload failures.
Contributed by Steve Loughran.

(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)
",Buggy,"The commit message indicates that the changes address issues in S3A BlockOutputStreams related to temporary file deletion during multipart uploads and handling part upload failures. The provided diff snippet shows a change in the `close()` method of a class, seemingly related to buffer management.

The original `close()` method attempts to delete a `bufferFile` and logs a warning if the deletion fails. The modified `close()` method simply sets `byteBuffer` to null. This change suggests that the original implementation had a flaw in how it handled buffer files, potentially leading to resource leaks or incomplete cleanup after an operation. The new implementation appears to be a simplified approach to releasing the buffer, which could be a fix for the issues described in the commit message.

Therefore, the changes indicate a bug fix.

**Buggy**"
hadoop,79999.json,2b452b4e6063072b2bec491edd3f412eb7ac21f3,"@@ -1,16 +1,13 @@
   void init() {
     if (used.get() < 0) {
       used.set(0);
+      if (!shouldFirstRefresh) {
+        // Skip initial refresh operation, so we need to do first refresh
+        // operation immediately in refresh thread.
+        initRefeshThread(true);
+        return;
+      }
       refresh();
     }
-
-    if (refreshInterval > 0) {
-      refreshUsed = new Thread(new RefreshThread(this),
-          ""refreshUsed-"" + dirPath);
-      refreshUsed.setDaemon(true);
-      refreshUsed.start();
-    } else {
-      running.set(false);
-      refreshUsed = null;
-    }
+    initRefeshThread(false);
   }",NotBuggy,"HDFS-14986. ReplicaCachingGetSpaceUsed throws ConcurrentModificationException. Contributed by Aiphago.
",Buggy,"The commit message indicates a fix for a `ConcurrentModificationException` in `ReplicaCachingGetSpaceUsed`. The code diff modifies the `init` method, specifically how the refresh thread is initialized.

Here's a breakdown of the changes and how they relate to the potential bug:

1. **Initial `used` value check:** The code checks if `used.get() < 0` and sets it to 0. This part seems like initialization and not directly related to concurrency issues.

2. **Conditional `refresh()` call:** The code now includes a conditional check `if (!shouldFirstRefresh)`. If `shouldFirstRefresh` is false, it calls `initRefeshThread(true)` and returns.  This suggests a change in the initialization sequence of the refresh thread, potentially to avoid a race condition or concurrent modification during the initial setup.

3. **`initRefeshThread(false)` call:** The original code had a conditional block based on `refreshInterval > 0` to start the refresh thread. This block is now replaced with a call to `initRefeshThread(false)` regardless of the `refreshInterval`. This indicates a change in how the refresh thread is managed, which could be related to preventing the `ConcurrentModificationException`.

The changes suggest that the original initialization sequence of the refresh thread might have been causing a `ConcurrentModificationException`, possibly due to a race condition or improper synchronization. The new code modifies the initialization order and potentially introduces a flag (`shouldFirstRefresh`) to control the initial refresh behavior. These modifications are likely aimed at resolving the concurrency issue mentioned in the commit message.

**Buggy**
"
hadoop,9493.json,1594b472bb9df7537dbc001411c99058cc11ba41,"@@ -1,3 +1,3 @@
-  public float getVolumeDataDensity() {
+  public double getVolumeDataDensity() {
     return volumeDataDensity;
   }",NotBuggy,"HDFS-9543. DiskBalancer: Add Data mover. Contributed by Anu Engineer.
",Buggy,"The commit message ""HDFS-9543. DiskBalancer: Add Data mover. Contributed by Anu Engineer."" indicates the addition of a data mover feature to the DiskBalancer in HDFS. The provided code diff changes the return type of the `getVolumeDataDensity()` method from `float` to `double`.

Reasoning:

1.  **Type Change:** The change from `float` to `double` suggests a need for higher precision in representing the volume data density. This could be due to calculations or measurements where `float` was insufficient, potentially leading to inaccuracies or data loss.

2.  **Bug Fix Indication:** While not a direct bug fix in the sense of correcting a logical error, the change in data type can be considered a bug fix if the previous `float` type was causing precision issues or incorrect calculations.  It's a preventative measure against potential bugs related to data representation.

3.  **Relevance to Commit Message:** The commit message focuses on adding a data mover. The data mover likely uses `getVolumeDataDensity()` to make decisions about data movement. Therefore, ensuring the accuracy of `getVolumeDataDensity()` by using `double` is relevant to the overall goal of the commit.

Conclusion:
**Buggy**
"
hadoop,37365.json,9875325d5c63f343809907d06bf48a298035a611,"@@ -1,28 +1,28 @@
   public static ReservationAllocationStateProto buildStateProto(
       ReservationAllocation allocation) {
     ReservationAllocationStateProto.Builder builder =
         ReservationAllocationStateProto.newBuilder();
 
-    builder.setAcceptanceTimestamp(allocation.getAcceptanceTime());
+    builder.setAcceptanceTime(allocation.getAcceptanceTime());
     builder.setContainsGangs(allocation.containsGangs());
     builder.setStartTime(allocation.getStartTime());
     builder.setEndTime(allocation.getEndTime());
     builder.setUser(allocation.getUser());
     ReservationDefinitionProto definitionProto = convertToProtoFormat(
         allocation.getReservationDefinition());
     builder.setReservationDefinition(definitionProto);
 
     for (Map.Entry<ReservationInterval, Resource> entry :
         allocation.getAllocationRequests().entrySet()) {
       ResourceAllocationRequestProto p =
           ResourceAllocationRequestProto.newBuilder()
           .setStartTime(entry.getKey().getStartTime())
           .setEndTime(entry.getKey().getEndTime())
           .setResource(convertToProtoFormat(entry.getValue()))
           .build();
       builder.addAllocationRequests(p);
     }
 
     ReservationAllocationStateProto allocationProto = builder.build();
     return allocationProto;
   }",NotBuggy,"YARN-4340. Add list API to reservation system. (Sean Po via wangda)
",Buggy,"The commit message ""YARN-4340. Add list API to reservation system"" indicates the addition of a list API to the reservation system.

The provided code diff focuses on the `buildStateProto` method, which converts a `ReservationAllocation` object into its protobuf representation (`ReservationAllocationStateProto`). The changes involve renaming `setAcceptanceTimestamp` to `setAcceptanceTime` in the protobuf builder. This change seems to be a correction or update to align with the correct field name in the protobuf definition. This could be due to a typo or an outdated field name.

Given that the commit message describes adding a list API, and the code diff shows a modification in how reservation allocation state is serialized (specifically, correcting a field name), it's unlikely that the code diff directly implements the list API functionality. However, the correction of the field name suggests that there was an issue with the existing code, which can be considered a bug fix.

**Buggy**"
hadoop,10353.json,d1d4e16690cc85f7f22fbead9cf596260819b561,"@@ -1,3 +1,3 @@
   boolean sendImmediately() {
-    return readyToSend;
+    return readyToSend && monotonicNow() - ibrInterval >= lastIBR;
   }",NotBuggy,"HDFS-9710. DN can be configured to send block receipt IBRs in batches.
",NotBuggy,"The commit message indicates a feature addition: the ability for DataNodes to send Inter-Block Reports (IBRs) in batches, configurable by an interval. The code change modifies the `sendImmediately()` method. The original implementation simply returned the value of `readyToSend`. The modified implementation adds a condition based on `monotonicNow() - ibrInterval >= lastIBR`. This condition checks if the time elapsed since the last IBR is greater than or equal to the configured interval (`ibrInterval`). This ensures that IBRs are not sent too frequently, supporting the batching functionality described in the commit message. The change is directly related to the feature being implemented and doesn't appear to be fixing a bug.

**NotBuggy**
"
hadoop,60643.json,a81144daa012e13590725f67f53e35ef84a6f1ec,"@@ -1,26 +1,23 @@
-          public Iterator<ApplicationTimeoutMapProto> iterator() {
-            return new Iterator<ApplicationTimeoutMapProto>() {
-              private Iterator<ApplicationTimeoutType> iterator =
-                  applicationTimeouts.keySet().iterator();
+      public Iterator<StringStringMapProto> iterator() {
+        return new Iterator<StringStringMapProto>() {
+          private Iterator<String> iterator = schedulingProperties.keySet()
+              .iterator();
 
-              @Override
-              public boolean hasNext() {
-                return iterator.hasNext();
-              }
+          @Override
+          public boolean hasNext() {
+            return iterator.hasNext();
+          }
 
-              @Override
-              public ApplicationTimeoutMapProto next() {
-                ApplicationTimeoutType key = iterator.next();
-                return ApplicationTimeoutMapProto.newBuilder()
-                    .setTimeout(applicationTimeouts.get(key))
-                    .setApplicationTimeoutType(
-                        ProtoUtils.convertToProtoFormat(key))
-                    .build();
-              }
+          @Override
+          public StringStringMapProto next() {
+            String key = iterator.next();
+            return StringStringMapProto.newBuilder()
+                .setValue(schedulingProperties.get(key)).setKey(key).build();
+          }
 
-              @Override
-              public void remove() {
-                throw new UnsupportedOperationException();
-              }
-            };
-          }

+          @Override
+          public void remove() {
+            throw new UnsupportedOperationException();
+          }
+        };
+      }",NotBuggy,"YARN-7666. Introduce scheduler specific environment variable support in ApplicationSubmissionContext for better scheduling placement configurations. (Sunil G via wangda)

Change-Id: I0fd826490f5160d47d42af2a9ac0bd8ec4e959dc
",NotBuggy,"The commit message indicates the introduction of scheduler-specific environment variable support in ApplicationSubmissionContext. The code diff shows a change in the iterator implementation, specifically modifying it to handle `StringStringMapProto` instead of `ApplicationTimeoutMapProto`. The iterator now iterates over keys of type `String` from `schedulingProperties` instead of `ApplicationTimeoutType` from `applicationTimeouts`. The `next()` method now constructs a `StringStringMapProto` with key-value pairs from `schedulingProperties`. These changes align with the commit message, as they introduce support for scheduler-specific properties represented as strings. The changes do not appear to be related to fixing a bug, but rather to adding a new feature.

**NotBuggy**"
hadoop,14879.json,1b5451bf054c335188e4cd66f7b4a1d80013e86d,"@@ -1,8 +1,7 @@
     public String getLongUsage() {
-      TableListing listing = AdminHelper.getOptionDescriptionListing();
-      listing.addRow(""<path>"", ""The path of the directory ""
-          + ""from which the erasure coding policy will be unset."");
-      return getShortUsage() + ""\n""
-          + ""Unset the erasure coding policy for a directory.\n\n""
-          + listing.toString();
+      return getShortUsage() + ""\n"" +
+          ""Get the list of supported erasure coding codecs and coders.\n"" +
+          ""A coder is an implementation of a codec. A codec can have "" +
+          ""different implementations, thus different coders.\n"" +
+          ""The coders for a codec are listed in a fall back order.\n"";
     }",NotBuggy,"HDFS-11794. Add ec sub command -listCodec to show currently supported ec codecs. Contributed by SammiChen.
",NotBuggy,"The commit message indicates the addition of a new subcommand, `listCodec`, to display supported erasure coding codecs. The provided diff modifies the `getLongUsage` method, changing the description to reflect the new functionality of listing codecs and coders. The original code was related to unsetting erasure coding policies, which is unrelated to the new functionality. The change in description suggests the addition of a new feature rather than fixing a bug.

**NotBuggy**"
RxJava,6339.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,105 +1,109 @@
         void drain() {
             if (getAndIncrement() != 0) {
                 return;
             }
 
             int missed = 1;
 
             final Subscriber<? super R> downstream = this.downstream;
             final SimpleQueue<T> queue = this.queue;
             final AtomicThrowable error = this.error;
             Iterator<? extends R> iterator = this.currentIterator;
             long requested = this.requested.get();
             long emitted = this.emitted;
             final int limit = prefetch - (prefetch >> 2);
             boolean canRequest = sourceMode != QueueFuseable.SYNC;
 
             for (;;) {
                 if (cancelled) {
                     queue.clear();
                     clearCurrentSuppressCloseError();
                 } else {
                     boolean isDone = upstreamDone;
                     if (error.get() != null) {
                         downstream.onError(error.get());
                         cancelled = true;
                         continue;
                     }
 
                     if (iterator == null) {
                         T t;
 
                         try {
                             t = queue.poll();
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         boolean isEmpty = t == null;
 
                         if (isDone && isEmpty) {
                             downstream.onComplete();
                             cancelled = true;
                         }
                         else if (!isEmpty) {
                             if (canRequest && ++consumed == limit) {
                                 consumed = 0;
                                 upstream.request(limit);
                             }
 
                             Stream<? extends R> stream;
                             try {
                                 stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                                 iterator = stream.iterator();
 
                                 if (iterator.hasNext()) {
                                     currentIterator = iterator;
                                     currentCloseable = stream;
                                 } else {
                                     iterator = null;
                                 }
                             } catch (Throwable ex) {
+                                Exceptions.throwIfFatal(ex);
                                 trySignalError(downstream, ex);
                             }
                             continue;
                         }
                     }
                     if (iterator != null && emitted != requested) {
                         R item;
 
                         try {
                             item = Objects.requireNonNull(iterator.next(), ""The Stream.Iterator returned a null value"");
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         if (!cancelled) {
                             downstream.onNext(item);
                             emitted++;
 
                             if (!cancelled) {
                                 try {
                                     if (!iterator.hasNext()) {
                                         iterator = null;
                                         clearCurrentRethrowCloseError();
                                     }
                                 } catch (Throwable ex) {
+                                    Exceptions.throwIfFatal(ex);
                                     trySignalError(downstream, ex);
                                 }
                             }
                         }
 
                         continue;
                     }
                 }
 
                 this.emitted = emitted;
                 missed = addAndGet(-missed);
                 if (missed == 0) {
                     break;
                 }
                 requested = this.requested.get();
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates the addition of missing `throwIfFatal` calls. The diff shows that `Exceptions.throwIfFatal(ex)` is added in multiple catch blocks. This suggests that the original code was missing a crucial check for fatal exceptions, which could lead to unexpected behavior or crashes. Adding these checks is a bug fix because it addresses a potential vulnerability in the error handling logic.

**Buggy**"
RxJava,6463.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,13 +1,13 @@
         public T poll() {
             if (iterator == null) {
                 return null;
             }
             if (!once) {
                 once = true;
             } else {
                 if (!iterator.hasNext()) {
                     return null;
                 }
             }
-            return ObjectHelper.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
+            return Objects.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff shows exactly that change in the `poll()` method. This change doesn't inherently indicate a bug fix. It's more likely a code modernization or standardization effort, replacing a custom utility with a standard Java library function. The functionality remains the same: ensuring that the iterator doesn't return null values.

**NotBuggy**"
RxJava,6464.json,af17c6e268d0e2fe4675feb46c19a29227cb135b,"@@ -1,3 +1,10 @@
         public boolean isEmpty() {
-            return iterator == null || !iterator.hasNext();
+            Iterator<T> it = iterator;
+            if (it != null) {
+                if (!once || it.hasNext()) {
+                    return false;
+                }
+                clear();
+            }
+            return true;
         }",NotBuggy,"3.x: [Java 8] Add Observable operators + cleanup (#6797)

",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: [Java 8] Add Observable operators + cleanup (#6797)"" suggests the following:

*   **Feature Addition:** The primary focus is adding new ""Observable operators,"" likely related to reactive programming concepts in Java 8.
*   **Cleanup:** The commit also mentions ""cleanup,"" which could encompass various code improvements, including refactoring, removing dead code, or addressing minor issues.
*   **Java 8:** The changes are specifically targeted for Java 8 compatibility or leveraging Java 8 features.

**Git Diff Analysis:**

The diff shows a modification to the `isEmpty()` method. Let's break down the changes:

*   **Original Code:**

    ```java
    public boolean isEmpty() {
        return iterator == null || !iterator.hasNext();
    }
    ```

    This code simply checks if the iterator is null or if it has no more elements.

*   **Modified Code:**

    ```java
    public boolean isEmpty() {
        Iterator<T> it = iterator;
        if (it != null) {
            if (!once || it.hasNext()) {
                return false;
            }
            clear();
        }
        return true;
    }
    ```

    The modified code introduces several changes:

    1.  **Local Variable:** It assigns the `iterator` to a local variable `it`. This might be done for thread safety or to avoid potential issues if the `iterator` field is modified concurrently.
    2.  **Null Check:** It explicitly checks if `it` (the iterator) is null.
    3.  **`once` Condition:** It introduces a condition `!once || it.hasNext()`. The `once` variable is not defined in the provided diff, but its presence suggests that the logic is dependent on whether the iterator is intended to be used only once.
    4.  **`clear()` Call:** If `it` is not null and `!once || it.hasNext()` is false, the code calls `clear()`. This indicates a potential resource management or state cleanup operation.

**Reasoning:**

The changes in the `isEmpty()` method are more than just a simple cleanup. The introduction of the `once` condition and the `clear()` call suggest that there might have been a bug or an incorrect behavior related to how the iterator was being used or managed. Specifically, the `clear()` call implies that resources associated with the iterator were not being released or that the state was not being properly reset in certain scenarios. The `once` flag suggests a specific use case where the iterator should only be used once. The original code did not account for this scenario, potentially leading to issues if the iterator was accessed after it was supposed to be ""empty"".

The changes address a specific edge case or condition that was not handled correctly in the original code. This indicates a bug fix.

**Conclusion:**

**Buggy**
"
RxJava,6454.json,a249f4f1241f1c0ebe8e03dc80bdd16e29962c53,"@@ -1,23 +1,3 @@
     protected void subscribeActual(Subscriber<? super T> s) {
-        Iterator<T> iterator;
-        try {
-            iterator = stream.iterator();
-
-            if (!iterator.hasNext()) {
-                EmptySubscription.complete(s);
-                closeSafely(stream);
-                return;
-            }
-        } catch (Throwable ex) {
-            Exceptions.throwIfFatal(ex);
-            EmptySubscription.error(ex, s);
-            closeSafely(stream);
-            return;
-        }
-
-        if (s instanceof ConditionalSubscriber) {
-            s.onSubscribe(new StreamConditionalSubscription<T>((ConditionalSubscriber<? super T>)s, iterator, stream));
-        } else {
-            s.onSubscribe(new StreamSubscription<>(s, iterator, stream));
-        }
+        subscribeStream(s, stream);
     }",NotBuggy,"3.x: [Java 8] Add blockingStream & flatMapStream to Flowable (#6779)

",NotBuggy,"The commit message indicates the addition of `blockingStream` and `flatMapStream` to `Flowable` for Java 8. The provided diff shows a change in the `subscribeActual` method. The original implementation used an iterator to process the stream, with explicit error handling for iterator creation and empty stream scenarios. The modified code replaces this with a call to `subscribeStream`.

The original code includes a try-catch block to handle potential exceptions when obtaining the iterator from the stream. It also handles the case where the iterator is initially empty. The new code delegates the subscription logic to `subscribeStream`. It's not possible to determine from this diff alone whether `subscribeStream` provides equivalent error handling and empty stream handling. However, the removal of explicit error handling suggests a potential refactoring or consolidation of error handling logic. Without knowing the implementation of `subscribeStream`, it's difficult to definitively say if this change is related to a bug fix. However, the simplification of the `subscribeActual` method suggests a potential refactoring to address a previous issue or improve code maintainability.

Therefore, based on the code modification, it is not possible to determine if the change indicates a bug fix.

**NotBuggy**"
RxJava,5791.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static int getIntProperty(boolean enabled, String key, int defaultNotFound, int defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return Integer.parseInt(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: Add missing throwIfFatal calls (#6801)"" strongly suggests a bug fix. The phrase ""missing throwIfFatal calls"" implies that the code was previously lacking proper handling of fatal exceptions, which could lead to unexpected application termination or other critical issues.

**Code Diff Analysis:**

The code diff shows the addition of `Exceptions.throwIfFatal(ex);` within a `catch` block. This line is crucial for handling `Throwable` exceptions. Here's why:

1.  **Exception Handling:** The `try-catch` block is designed to catch potential exceptions during the `Integer.parseInt(value)` operation.
2.  **Fatal Exception Handling:** The `Exceptions.throwIfFatal(ex)` call is specifically designed to re-throw exceptions that are considered fatal (e.g., `OutOfMemoryError`, `StackOverflowError`). These exceptions typically indicate severe problems that the application cannot recover from, and attempting to continue execution could lead to unpredictable behavior or data corruption.
3.  **Missing Check:** The commit message indicates that this `throwIfFatal` call was previously missing. Without it, a fatal exception caught by the `catch` block would be incorrectly handled as a non-fatal exception, potentially leading to the application continuing in an unstable state.

**Reasoning:**

The addition of `Exceptions.throwIfFatal(ex)` addresses a critical oversight in the original code. Without this check, fatal exceptions would be masked, potentially leading to application instability or data corruption. The commit message explicitly states that these calls were missing, confirming that this change is intended to correct a bug.

**Conclusion:**

**Buggy**
"
RxJava,6341.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,7 +1,8 @@
         void clearCurrentSuppressCloseError() {
             try {
                 clearCurrentRethrowCloseError();
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 RxJavaPlugins.onError(ex);
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message ""3.x: Add missing throwIfFatal calls"" suggests that the commit aims to add `throwIfFatal` calls in certain locations.

The provided diff shows that a `Exceptions.throwIfFatal(ex)` call is added within a `catch` block that catches `Throwable`. This aligns with the commit message's intention to add missing `throwIfFatal` calls. The addition of `throwIfFatal` within a `catch` block that handles `Throwable` indicates a defensive programming practice to prevent fatal errors from being silently caught and potentially causing unpredictable behavior. This is a common practice when addressing potential bugs related to error handling.

Thus, the changes indicate a bug fix.

**Buggy**"
RxJava,6378.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,23 +1,24 @@
     protected void subscribeActual(Observer<? super R> observer) {
         if (source instanceof Supplier) {
             Stream<? extends R> stream = null;
             try {
                 @SuppressWarnings(""unchecked"")
                 T t = ((Supplier<T>)source).get();
                 if (t != null) {
                     stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                 }
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 EmptyDisposable.error(ex, observer);
                 return;
             }
 
             if (stream != null) {
                 ObservableFromStream.subscribeStream(observer, stream);
             } else {
                 EmptyDisposable.complete(observer);
             }
         } else {
             source.subscribe(new FlatMapStreamObserver<>(observer, mapper));
         }
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: Add missing throwIfFatal calls (#6801)"" suggests that the change involves adding `throwIfFatal` calls in certain code paths. The phrase ""missing"" implies that these calls were previously absent, potentially leading to unhandled fatal errors. This strongly hints at a bug fix.

**Git Diff Analysis:**

The diff shows a modification within the `subscribeActual` method. Specifically, a `try-catch` block is present to handle potential exceptions thrown by `mapper.apply(t)`. Inside the `catch` block, the line `Exceptions.throwIfFatal(ex);` has been added.

**Reasoning:**

1.  **`throwIfFatal` Significance:** The `Exceptions.throwIfFatal(ex)` call is crucial for preventing fatal exceptions (like `OutOfMemoryError` or `StackOverflowError`) from being caught and potentially masked by the application. If a fatal exception occurs, the application should typically terminate to avoid unpredictable behavior.

2.  **Missing Call Implication:** The commit message indicates that this `throwIfFatal` call was previously missing. This means that if a fatal exception occurred within the `try` block, it might have been incorrectly handled, potentially leading to application instability or incorrect error reporting.

3.  **Bug Fix Indication:** The addition of `throwIfFatal` addresses the potential for mishandling fatal exceptions, which directly relates to fixing a bug. The code is now more robust in the face of fatal errors.

**Conclusion:**

The commit message and the code diff clearly indicate that the change is a bug fix. The addition of the `throwIfFatal` call addresses a potential vulnerability where fatal exceptions could be incorrectly handled.

**Buggy**"
RxJava,5792.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static boolean getBooleanProperty(boolean enabled, String key, boolean defaultNotFound, boolean defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return ""true"".equals(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: Add missing throwIfFatal calls (#6801)"" suggests that the change involves adding `throwIfFatal` calls. The phrase ""missing"" implies that these calls were previously absent, potentially leading to issues when fatal exceptions occurred. This strongly hints at a bug fix.

**Git Diff Analysis:**

The diff shows a change within the `getBooleanProperty` method. Specifically, a `try-catch` block is present, and inside the `catch` block, `Exceptions.throwIfFatal(ex)` is added.

**Reasoning:**

1.  **Error Handling:** The addition of `Exceptions.throwIfFatal(ex)` within the `catch` block indicates an improvement in error handling.  Without this call, a fatal exception caught in this block might not have been properly propagated, potentially leading to unexpected behavior or masking the underlying issue.

2.  **Bug Fix Indication:** The commit message explicitly states that the `throwIfFatal` calls were ""missing."" This, combined with the code change adding the call within a `catch` block, strongly suggests that the change is intended to address a bug related to the handling of fatal exceptions.  The previous code could have suppressed fatal exceptions, leading to unpredictable or incorrect program behavior.

**Conclusion:**

Based on the commit message and the code changes, the addition of `Exceptions.throwIfFatal(ex)` appears to be a bug fix related to the proper handling of fatal exceptions.

**Buggy**"
RxJava,6466.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,56 +1,56 @@
         public void run(long n) {
             long emitted = 0L;
             Iterator<T> iterator = this.iterator;
             ConditionalSubscriber<? super T> downstream = this.downstream;
 
             for (;;) {
 
                 if (cancelled) {
                     clear();
                     break;
                 } else {
                     T next;
                     try {
-                        next = ObjectHelper.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
+                        next = Objects.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (downstream.tryOnNext(next)) {
                         emitted++;
                     }
 
                     if (cancelled) {
                         continue;
                     }
 
                     try {
                         if (!iterator.hasNext()) {
                             downstream.onComplete();
                             cancelled = true;
                             continue;
                         }
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (emitted != n) {
                         continue;
                     }
                 }
 
                 n = get();
                 if (emitted == n) {
                     if (compareAndSet(n, 0L)) {
                         break;
                     }
                     n = get();
                 }
             }
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a straightforward replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`.

The diff shows that `ObjectHelper.requireNonNull` is indeed replaced with `Objects.requireNonNull` within the `run` method. This change doesn't inherently indicate a bug fix. It's more likely a code modernization or standardization effort, possibly to leverage the standard Java library. The functionality remains the same: checking for null values and throwing a `NullPointerException` if a null is encountered. There are no error handling updates, logical corrections, or exception handling improvements that would suggest a bug fix.

**NotBuggy**"
RxJava,7468.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArray(SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: Use more appropriate operators when delegating to Flowable ops"" suggests that the change involves using better operators when delegating operations to Flowable. The phrase ""more appropriate operators"" hints at a potential correction of a suboptimal or incorrect implementation, which could be related to a bug or performance issue. The message also mentions removing unused classes, which could be a consequence of the operator change.

**Diff Analysis:**

The diff shows a change in the `mergeArray` method. Specifically, the `flatMapSingle` operator's `maxConcurrency` argument is being modified.

*   **Original Code:** `Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, sources.length);`
*   **Modified Code:** `Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, Math.max(1, sources.length));`

The change replaces `sources.length` with `Math.max(1, sources.length)`. This ensures that the `maxConcurrency` argument is at least 1, even when the `sources` array is empty.

**Reasoning:**

1.  **Potential Bug:** If `sources.length` is 0, the original code would have passed 0 as the `maxConcurrency` argument to `flatMapSingle`. This could lead to unexpected behavior or errors in the `flatMapSingle` operator, as a concurrency of 0 might be invalid or have unintended consequences. The change to `Math.max(1, sources.length)` addresses this by ensuring a minimum concurrency of 1.

2.  **Operator Appropriateness:** The commit message mentions ""more appropriate operators."" While the operator itself (`flatMapSingle`) isn't changing, the arguments passed to it are being adjusted to be more suitable for all input scenarios, including the case where the input array is empty.

3.  **Error Handling/Prevention:** The change can be seen as a form of error prevention. By ensuring a minimum concurrency of 1, the code avoids a potentially problematic scenario where the concurrency is 0.

**Conclusion:**

The change addresses a potential issue where the `maxConcurrency` argument to `flatMapSingle` could be 0, which might lead to unexpected behavior. The use of `Math.max(1, sources.length)` ensures a minimum concurrency of 1, preventing this issue. This indicates a bug fix or at least a preventative measure against a potential bug.

**Buggy**
"
RxJava,8113.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromStream(@NonNull Stream<T> stream) {
-        ObjectHelper.requireNonNull(stream, ""stream is null"");
+        Objects.requireNonNull(stream, ""stream is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromStream<>(stream));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The code diff shows exactly that: a direct replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull` in the `fromStream` method. This change doesn't inherently indicate a bug fix. It's more likely a code modernization or standardization effort, replacing a custom utility with a standard Java library function. There's no error handling, logical correction, or exception handling improvement apparent in this change.

**NotBuggy**"
RxJava,8410.json,f1a795da0d4cdfb3133206ad8e3887bdb8ba948b,"@@ -1,3 +1,3 @@
     public final <R> Maybe<R> concatMapSingle(@NonNull Function<? super T, ? extends SingleSource<? extends R>> mapper) {
-        return flatMapSingleElement(mapper);
+        return flatMapSingle(mapper);
     }",NotBuggy,"3.x: Swap Maybe.flatMapSingle and Maybe.flatMapSingleElement (#6891)

* Deleted Maybe.flatMapSingle

And replaced tests with flatMapSingle().toSingle()

* Renamed Maybe.flatMapSingleElement to flatMapSingle

* Deleted unused MaybeFlatMapSingle operator

* Renamed operator MaybeFlatMapSingleElement to MaybeFlatMapSingle

* Add Ignore to XFlatMapTest.maybeSingle
",NotBuggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Commit Message Analysis:**

The commit message ""3.x: Swap Maybe.flatMapSingle and Maybe.flatMapSingleElement (#6891)"" suggests a renaming and swapping of two methods: `Maybe.flatMapSingle` and `Maybe.flatMapSingleElement`. It also mentions deleting the original `Maybe.flatMapSingle` and an unused operator `MaybeFlatMapSingle`. The tests were updated to use `flatMapSingle().toSingle()`.

**Git Diff Analysis:**

The provided diff snippet shows a single line change within the `concatMapSingle` method. It replaces a call to `flatMapSingleElement(mapper)` with `flatMapSingle(mapper)`. This change directly reflects the renaming and swapping described in the commit message.

**Reasoning:**

1.  **Renaming and Swapping:** The diff confirms that `flatMapSingleElement` is being replaced by `flatMapSingle`, aligning with the commit message's intention to swap the methods.
2.  **Method Deletion:** The commit message states that `Maybe.flatMapSingle` was deleted. While the diff doesn't show the deletion itself, the renaming implies that the original `flatMapSingle` was removed or renamed to `flatMapSingleElement` and then `flatMapSingleElement` was renamed to `flatMapSingle`.
3.  **Test Updates:** The commit message mentions updating tests to use `flatMapSingle().toSingle()`. This suggests that the behavior of the methods might have been slightly different, requiring adjustments in the tests to accommodate the change.

**Bug Fix Indication:**

While the commit message and diff primarily describe a refactoring (renaming and swapping), the fact that tests needed to be updated *could* indicate a bug fix. It's possible that the original implementation had subtle differences that were causing issues, and the renaming/swapping was done to correct this. However, without more context, it's difficult to say for sure.

Given the information available, it's more likely that this is a refactoring for clarity or consistency within the API. There is no clear indication of a bug being fixed.

**Conclusion:**

**NotBuggy**
"
RxJava,8112.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromCompletionStage(@NonNull CompletionStage<T> stage) {
-        ObjectHelper.requireNonNull(stage, ""stage is null"");
+        Objects.requireNonNull(stage, ""stage is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromCompletionStage<>(stage));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The diff shows exactly that change in the `fromCompletionStage` method. This change doesn't inherently indicate a bug fix. It's more likely a code modernization or standardization effort, replacing a custom null-check utility with the standard Java library's equivalent. There's no error handling update, logical correction, or exception-handling improvement visible in the diff.

**NotBuggy**"
RxJava,7469.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArrayDelayError(@NonNull SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message ""Use more appropriate operators when delegating to Flowable ops"" suggests an improvement in how operations are delegated to Flowable. The phrase ""more appropriate operators"" implies that the previous operators were potentially incorrect or suboptimal, which could have led to unexpected behavior or bugs.

2.  **Git Diff Analysis:**
    *   The diff modifies the `mergeArrayDelayError` method. Specifically, it changes the `sources.length` argument in the `flatMapSingle` operator to `Math.max(1, sources.length)`.
    *   The original code directly used `sources.length` as the `maxConcurrency` argument to `flatMapSingle`. If `sources.length` was zero, this would pass zero as the `maxConcurrency`.
    *   The javadoc for `flatMapSingle` states that `maxConcurrency` is the ""maximum number of single sources that may be subscribed to concurrently."" It's reasonable to assume that a `maxConcurrency` of 0 is invalid or has unintended consequences (e.g., preventing any subscriptions).
    *   The change `Math.max(1, sources.length)` ensures that `maxConcurrency` is always at least 1, even if the input array `sources` is empty. This prevents the potential issue of passing an invalid concurrency value to `flatMapSingle`.

3.  **Connecting the Dots:**
    *   The commit message indicates the use of ""more appropriate operators,"" and the code change ensures a minimum concurrency of 1. This suggests the original code had a flaw where a zero-length source array could lead to incorrect behavior in `flatMapSingle` due to an invalid `maxConcurrency` value. The change addresses this potential issue.

**Conclusion:**

The code change addresses a scenario where a zero-length input array could lead to incorrect behavior due to an invalid `maxConcurrency` value being passed to `flatMapSingle`. This constitutes a bug fix.

**Buggy**"
RxJava,6762.json,ecfb6b741d411de9d060b1c4b1fcd90ff7530854,"@@ -1,3 +1,4 @@
     static AutoCloseable toAutoCloseable(@NonNull Disposable disposable) {
+        Objects.requireNonNull(disposable, ""disposable is null"");
         return disposable::dispose;
     }",NotBuggy,"3.x: Improve Javadocs of Completable and some others (#6809)

",Buggy,"The commit message indicates improvements to Javadocs. However, the provided diff shows the addition of `Objects.requireNonNull(disposable, ""disposable is null"");`. This suggests a null check is being added, which is a defensive programming practice and often indicates a potential bug fix related to handling null `disposable` objects. The addition of a null check implies that the code might have previously been vulnerable to a `NullPointerException` if `disposable` was null.

Therefore, the changes indicate a bug fix.

**Buggy**"
junit4,587.json,83a25d440d118b7d10779a884d35f31045949e27,"@@ -1,3 +1,3 @@
             public int compare(T o1, T o2) {
-                return fSorter.compare(describeChild(o1), describeChild(o2));
+                return sorter.compare(describeChild(o1), describeChild(o2));
             }",NotBuggy,"Treatments for parallel execution
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** ""Treatments for parallel execution"" suggests modifications made to improve or handle parallel execution scenarios. This could involve thread safety, synchronization, or other concurrency-related adjustments.

2.  **Code Diff:**
    ```diff
    --- a/src/java/org/apache/commons/collections4/comparators/TransformingComparator.java
    +++ b/src/java/org/apache/commons/collections4/comparators/TransformingComparator.java
    @@ -1,3 +1,3 @@
                public int compare(T o1, T o2) {
    -                return fSorter.compare(describeChild(o1), describeChild(o2));
    +                return sorter.compare(describeChild(o1), describeChild(o2));
                }
    ```

    *   The diff shows a simple variable name change from `fSorter` to `sorter`.

3.  **Analysis:**

    *   The commit message talks about ""Treatments for parallel execution,"" but the code change seems to be a simple renaming of a variable.
    *   It's possible that `fSorter` was a field that wasn't properly initialized or thread-safe, and renaming it to `sorter` implies a fix related to proper initialization or access within a parallel execution context. However, without more context or surrounding code, it's difficult to confirm this.
    *   It is also possible that `fSorter` was a typo and `sorter` was the intended name.

4.  **Conclusion:**

Given the limited information, it's difficult to definitively say whether this is a bug fix. However, the commit message suggests changes related to parallel execution, and the variable renaming *could* be part of a larger fix to ensure thread safety or proper initialization in a multithreaded environment. Therefore, I will lean towards it being a bug fix.

**Buggy**"
junit4,522.json,627b85abfd9c1f6c69b14f9d33fb292868fe6826,"@@ -1,3 +1,4 @@
-    public static void assertThrows(Class<? extends Throwable> expectedThrowable, ThrowingRunnable runnable) {
-        expectThrows(expectedThrowable, runnable);
+    public static <T extends Throwable> T assertThrows(Class<T> expectedThrowable,
+            ThrowingRunnable runnable) {
+        return assertThrows(null, expectedThrowable, runnable);
     }",NotBuggy,"rename expectThrows() to assertThrows()
",NotBuggy,"The commit message ""rename expectThrows() to assertThrows()"" indicates a renaming operation. The provided diff shows that `expectThrows` is not being renamed. Instead, a new `assertThrows` method is introduced that calls another `assertThrows` method. This suggests the original method might be deprecated or refactored. The change doesn't inherently indicate a bug fix, but rather a change in API design or method signature.

**NotBuggy**"
junit4,313.json,5501ad5d546427b228b208da15cec997e0b13cc8,"@@ -1,3 +1,3 @@
-	public RememberingListener listener() {
+	public RunListener listener() {
 		return new RememberingListener();
 	}",NotBuggy,"Finished Max javadoc
",NotBuggy,"The commit message ""Finished Max javadoc"" indicates that the commit is related to documentation updates for Max.

The code diff shows a change from `RememberingListener listener()` to `RunListener listener()`. This change suggests a modification in the type of listener being returned by the `listener()` method. It's a change in the code's functionality, not directly related to documentation. The commit message doesn't accurately reflect the code change. This change doesn't seem to be related to a bug fix.

**NotBuggy**"
junit4,738.json,86b9346e4c8fa263810dbd55a2775081194b1d6f,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return uniqueId.hashCode();
+        return fUniqueId.hashCode();
     }",NotBuggy,"Ensure serialization compatibility where possible

Not possible for org.junit.runner.Result
",Buggy,"The commit message ""Ensure serialization compatibility where possible"" suggests a change made to maintain compatibility during serialization, which often involves dealing with object state and versioning.

The provided code diff modifies the `hashCode()` method in a class, changing `uniqueId.hashCode()` to `fUniqueId.hashCode()`. This change is highly likely related to serialization compatibility. It suggests that the field name `uniqueId` might have been changed to `fUniqueId` in a previous version, and this change in `hashCode()` ensures that the hash code remains consistent even after deserialization if the field name has changed. This is a common practice to maintain compatibility across different versions of a serialized object.

Given the commit message and the code change, it's reasonable to infer that this change is related to fixing a potential issue where the hash code might be different after deserialization due to a field name change, which would break compatibility. Therefore, this change is likely a bug fix.

**Buggy**
"
junit4,271.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     private String missingExceptionMessage() {
-        String expectation= StringDescription.toString(fMatcherBuilder.build());
+        String expectation= StringDescription.toString(matcherBuilder.build());
         return format(missingExceptionMessage, expectation);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a refactoring of field names by removing the 'f' prefix, introducing a new constant `LEGACY_CODING_STYLE`, and modifying the existing `CODING_STYLE`.

The provided code diff shows the removal of the 'f' prefix from `fMatcherBuilder` to `matcherBuilder` within the `missingExceptionMessage` method. This change aligns with the commit message's intention to remove the 'f' prefix from field names.

The diff only shows the renaming of a field. There's no indication of error handling, logical corrections, or exception handling improvements. Therefore, the change doesn't seem to be related to fixing a bug.

**NotBuggy**
"
junit4,657.json,fbb1efed56f2b495d68a598d5345cbc95b118d35,"@@ -1,3 +1,3 @@
     public boolean isStatic() {
-        return Modifier.isStatic(fMethod.getModifiers());
+        return Modifier.isStatic(getModifiers());
     }",NotBuggy,"Remove duplicate code.
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Remove duplicate code"" suggests that the primary goal of the commit is to eliminate redundancy in the codebase. This often implies that a piece of code was unnecessarily repeated, and the commit aims to consolidate it into a single, reusable component.

**Git Diff Analysis:**

The diff shows a change within the `isStatic()` method. The original code was:

```java
return Modifier.isStatic(fMethod.getModifiers());
```

The modified code is:

```java
return Modifier.isStatic(getModifiers());
```

The change replaces `fMethod.getModifiers()` with `getModifiers()`. This strongly suggests that `fMethod` was an unnecessary indirection. The method likely already had access to its own modifiers via `getModifiers()`, and using `fMethod.getModifiers()` was redundant and potentially error-prone if `fMethod` was not correctly initialized or pointed to the wrong method.

**Reasoning:**

1.  **Code Duplication:** The original code likely duplicated the functionality of accessing the method's modifiers. The `getModifiers()` method is the standard way to retrieve the modifiers of the current method.
2.  **Potential Bug:** The original code could have introduced a bug if `fMethod` was not properly initialized or if it referenced a different method than intended. In such cases, `fMethod.getModifiers()` would return the modifiers of the wrong method, leading to incorrect behavior of the `isStatic()` method.
3.  **Simplification and Correctness:** The modified code simplifies the logic and eliminates the potential for errors related to `fMethod`. It directly uses the correct way to access the method's modifiers.

**Conclusion:**

The commit message and the diff strongly suggest that the change is a bug fix. The original code was redundant and potentially incorrect due to the unnecessary use of `fMethod.getModifiers()`. The modified code eliminates the redundancy and ensures that the `isStatic()` method correctly determines if the method is static.

**Buggy**"
junit4,498.json,9ac4cc842d70dc7ef266f4d677d90d9b177e4be6,"@@ -1,5 +1,5 @@
     static public void assertEquals(String message, long expected, long actual) {
         if (expected != actual) {
-            failNotEquals(message, expected, actual);
+            failNotEquals(message, Long.valueOf(expected), Long.valueOf(actual));
         }
     }",NotBuggy,"Use explicit auto-boxing.
",NotBuggy,"The commit message ""Use explicit auto-boxing"" suggests a change related to converting primitive types to their corresponding wrapper objects.

The provided diff shows a modification in the `assertEquals` method where `expected` and `actual` (both of type `long`) are now explicitly converted to `Long` objects using `Long.valueOf()`. This aligns perfectly with the commit message's intention of using explicit auto-boxing.

The change doesn't directly indicate a bug fix. It's more of a style or performance optimization (avoiding implicit auto-boxing which might have a slight overhead). There's no error handling, logical correction, or exception handling improvement.

Therefore, the changes are not related to fixing a bug.

**NotBuggy**"
junit4,163.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     protected void describeMismatchSafely(T item, Description description) {
         description.appendText(""message "");
-        fMatcher.describeMismatch(item.getMessage(), description);
+        matcher.describeMismatch(item.getMessage(), description);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a refactoring of field names by removing the 'f' prefix and updates to coding styles. The code diff shows a change from `fMatcher` to `matcher`. This aligns with the commit message's intention to remove the 'f' prefix from field names. There's no indication of a bug fix in this change.

**NotBuggy**"
junit4,267.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     private boolean isAnyExceptionExpected() {
-        return fMatcherBuilder.expectsThrowable();
+        return matcherBuilder.expectsThrowable();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a refactoring of field names by removing the 'f' prefix, the addition of a constant `LEGACY_CODING_STYLE`, and a change to `CODING_STYLE`. The provided diff shows the removal of the 'f' prefix from `fMatcherBuilder`, renaming it to `matcherBuilder`. This change aligns with the commit message's intention to remove the 'f' prefix from field names. There's no indication of a bug fix in this specific diff. The change seems to be purely stylistic.

**NotBuggy**"
junit4,663.json,02c328028b4d32c15bbf0becc9838e54ecbafcbf,"@@ -1,7 +1,11 @@
     public Runner safeRunnerForClass(Class<?> testClass) {
         try {
-            return runnerForClass(testClass);
+            Runner runner = runnerForClass(testClass);
+            if (runner != null) {
+                configureRunner(runner);
+            }
+            return runner;
         } catch (Throwable e) {
             return new ErrorReportingRunner(testClass, e);
         }
     }",NotBuggy,"Add Ordering, Orderable and @OrderWith (#1130)

* Add Ordering, Orderable and @OrderWith.

These APIs allow arbitrary ordering of tests, including randomization.
",NotBuggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Add Ordering, Orderable and @OrderWith (#1130)"" suggests the introduction of new features related to test ordering.  The description clarifies that these additions enable arbitrary ordering of tests, including randomization.  This sounds like a feature enhancement rather than a bug fix.

**Git Diff Analysis:**

The diff shows a modification within the `safeRunnerForClass` method.  Specifically, it appears to be adding a configuration step to the `Runner` instance *after* it's created by `runnerForClass` but *before* it's returned.  The added code includes a null check on the runner before configuring it.

```diff
@@ -1,7 +1,11 @@
     public Runner safeRunnerForClass(Class<?> testClass) {
         try {
-            return runnerForClass(testClass);
+            Runner runner = runnerForClass(testClass);
+            if (runner != null) {
+                configureRunner(runner);
+            }
+            return runner;
         } catch (Throwable e) {
             return new ErrorReportingRunner(testClass, e);
         }
     }
```

**Reasoning:**

1.  **Feature Enhancement vs. Bug Fix:** The commit message clearly indicates a feature addition (test ordering).

2.  **Code Modification:** The code modification adds a configuration step (`configureRunner`) to the `Runner` instance. The addition of a null check (`if (runner != null)`) suggests that `runnerForClass` *might* return null in some cases. If `runnerForClass` could return null before this change, and that would cause a NullPointerException later on, then this change could be considered a bug fix. However, without more context on what `runnerForClass` does, it is difficult to say for sure.

3.  **Alignment:** The code changes seem to be related to the feature described in the commit message. The `configureRunner` method likely sets up the ordering based on the new annotations/interfaces.

**Conclusion:**

The changes seem to be primarily focused on implementing the new test ordering feature. While the added null check *could* be addressing a potential bug, the overall context suggests it's more about ensuring the new feature works correctly under all circumstances. Therefore, I will classify this as a feature enhancement rather than a bug fix.

**NotBuggy**
"
junit4,165.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     public void describeTo(Description description) {
         description.appendText(""exception with cause "");
-        description.appendDescriptionOf(fMatcher);
+        description.appendDescriptionOf(matcher);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a refactoring of field names by removing the 'f' prefix, the addition of `LEGACY_CODING_STYLE`, and a change to `CODING_STYLE`. The provided diff shows the removal of the 'f' prefix from `fMatcher`, renaming it to `matcher`. This aligns with the commit message's description of removing the 'f' prefix from field names. There's no indication of bug fixes in the provided diff.
**NotBuggy**"
junit4,788.json,b01900fddca5ff54258e621025ccdbbc75f880f1,"@@ -1,8 +1,8 @@
-    FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
+    static FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
             throws FilterFactoryNotCreatedException {
         try {
             return filterFactoryClass.getConstructor().newInstance();
         } catch (Exception e) {
             throw new FilterFactoryNotCreatedException(e);
         }
     }",NotBuggy,"FilterFactoryFactory renamed to FilterFactories and its methods made static.
ClassUtil renamed to Classes for consistency.
",NotBuggy,"The commit message indicates a renaming of `FilterFactoryFactory` to `FilterFactories` and making its methods static, along with renaming `ClassUtil` to `Classes`. The provided diff snippet shows a method `createFilterFactory` being made static. This aligns with the commit message. There is no indication of a bug fix in the changes.
**NotBuggy**"
junit4,702.json,95bd62b2ffd80394d1a17e9078a4d79478423030,"@@ -1,3 +1,3 @@
 			protected void notifyListener(RunListener each) throws Exception {
-				each.testIgnoredReason(description, e);
+				each.testAssumptionFailed(description, e);
 			};",NotBuggy,"Start separating the idea of failed assumptions from ignored tests
",NotBuggy,"The commit message ""Start separating the idea of failed assumptions from ignored tests"" suggests a change in how test results are categorized and reported. The code diff shows a change from `testIgnoredReason` to `testAssumptionFailed`. This indicates a semantic correction in how the test runner reports different kinds of test failures. It's not directly fixing a bug in the sense of incorrect computation or unexpected behavior, but rather refining the reporting of test outcomes to be more precise. The change aligns with the commit message by differentiating between ignored tests and failed assumptions.

**NotBuggy**"
junit4,640.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     public Class<?> getDeclaringClass() {
-        return fMethod.getDeclaringClass();
+        return method.getDeclaringClass();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message indicates a refactoring of field names by removing the 'f' prefix and introduces changes related to coding styles. The provided code diff shows a change from `fMethod` to `method`. This aligns with the commit message's intention to remove the 'f' prefix from field names. There is no indication of bug fixing in the traditional sense (error handling, logical corrections, exception handling). The change appears to be a stylistic refactoring.

**NotBuggy**"
junit4,348.json,a72b0dbef4b01e8ad0b832d9a579dd7fabd5a071,"@@ -1,3 +1,3 @@
-	public static Executioner methods() {
-		return new ParallelExecutioner(false, true);
+	public static Computer methods() {
+		return new ParallelComputer(false, true);
 	}",NotBuggy,"Clean up parallel execution naming.
",NotBuggy,"The commit message ""Clean up parallel execution naming"" suggests a refactoring or renaming of classes or methods related to parallel execution.

The diff shows a change from `Executioner` to `Computer` and `ParallelExecutioner` to `ParallelComputer`. This aligns with the commit message, indicating a renaming or refactoring for clarity or consistency in naming.

There's no indication of bug fixes, error handling, or logical corrections. It appears to be a purely cosmetic change focused on improving the naming conventions within the parallel execution components.

**NotBuggy**"
facebook-android-sdk,3514.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,5 +1,5 @@
-    public static final Set<LoggingBehavior> getLoggingBehaviors() {
+    public static Set<LoggingBehavior> getLoggingBehaviors() {
         synchronized (loggingBehaviors) {
             return Collections.unmodifiableSet(new HashSet<LoggingBehavior>(loggingBehaviors));
         }
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** ""Facebook Android SDK 4.0"" - This commit message is very generic and doesn't provide any specific information about the changes made. It simply indicates a version update for the Facebook Android SDK.

2.  **Git Diff:**

    *   The diff shows a change in the return type of the `getLoggingBehaviors()` method. It changes from `public static final Set<LoggingBehavior>` to `public static Set<LoggingBehavior>`. The `final` keyword is removed.

3.  **Reasoning:**

    *   The removal of the `final` keyword from the method signature doesn't inherently indicate a bug fix. It suggests a change in the design or API of the SDK. Removing `final` means subclasses can now override this method. This is more likely a feature enhancement or API modification rather than a bug fix.
    *   Without more context or a more descriptive commit message, it's difficult to definitively say whether this change is related to a bug fix. It could be part of a larger refactoring or feature implementation.

**Conclusion:**

Based on the limited information and the nature of the change (removing `final`), it's unlikely that this commit directly addresses a bug. The change seems more related to API design or feature enhancement.

**NotBuggy**"
facebook-android-sdk,1334.json,bc17a04007ed19a340cfb2753c07f927e514b5ad,"@@ -1,7 +1,7 @@
     private void logAuthorizationMethodStart(String method) {
         Bundle bundle = newAuthorizationLoggingBundle(pendingRequest.getAuthId());
         bundle.putLong(EVENT_PARAM_TIMESTAMP, System.currentTimeMillis());
         bundle.putString(EVENT_PARAM_METHOD, method);
 
-        appEventsLogger.logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
+        getAppEventsLogger().logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
     }",NotBuggy,"Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Test Plan: - Built ZIP

Reviewers: mingfli

Reviewed By: mingfli

CC: subodh, platform-diffs@lists

Differential Revision: https://phabricator.fb.com/D986742
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Facebook Android SDK 3.5.1"" and the provided link to the changelog suggest a version update rather than a specific bug fix. The message focuses on the SDK update itself.

2.  **Code Diff:** The code diff shows a change in the `logAuthorizationMethodStart` method. Specifically, `appEventsLogger.logSdkEvent` is changed to `getAppEventsLogger().logSdkEvent`. This indicates that the way the `appEventsLogger` is accessed has been modified. It's possible that `appEventsLogger` was previously a direct field access, and now it's being accessed through a getter method (`getAppEventsLogger()`).

3.  **Reasoning:**
    *   The change from direct field access to a getter method could be for several reasons:
        *   **Encapsulation:**  Improving encapsulation by hiding the `appEventsLogger` field and controlling access through a getter.
        *   **Lazy Initialization:** The getter might be responsible for lazy initialization of the `appEventsLogger`. This could be a performance optimization or a way to avoid null pointer exceptions if the logger isn't always needed.
        *   **Thread Safety:** The getter could be synchronized to ensure thread-safe access to the `appEventsLogger`.
    *   Without more context, it's difficult to say definitively if this change is a bug fix. It *could* be fixing a null pointer exception if `appEventsLogger` wasn't being properly initialized before. However, it could also be a refactoring or an improvement in the SDK's architecture.

**Conclusion:**

Based on the information available, it's not clear if this change is directly related to fixing a bug. The commit message suggests a version update, and the code change could be a refactoring or improvement. Therefore, I will classify it as **NotBuggy**.
"
facebook-android-sdk,2354.json,2921bd7e2a268d0e08d10c13f90f8381d27c24ca,"@@ -1,8 +1,11 @@
     public void add(Activity activity) {
+        if (InternalSettings.isUnityApp()) {
+            return;
+        }
         if (Thread.currentThread() != Looper.getMainLooper().getThread()) {
             throw new FacebookException(""Can't add activity to CodelessMatcher on non-UI thread"");
         }
         this.activitiesSet.add(activity);
         delegateMap.clear();
         startTracking();
     }",NotBuggy,"Delegate event matching to unity sdk

Summary:
Codeless event matching should be handled in Unity SDK due to the special tree hierarchy in Unity apps. This diff does 2 things:
1. Send event mappin to Unity on parsing the app settings.
2. Skip event matching for unity apps.

Differential Revision: D10215680

fbshipit-source-id: 28b2128619aa220cd5cf178d7024464d5e126bd3
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message Analysis:**
    *   The commit message clearly states the purpose of the change: delegating event matching to the Unity SDK for Unity applications.
    *   It explains the rationale: Unity's unique tree hierarchy necessitates handling codeless event matching within the Unity SDK.
    *   It outlines two specific actions: sending event mappings to Unity and skipping event matching in the current code for Unity apps.

2.  **Diff Analysis:**
    *   The diff shows a modification to the `add(Activity activity)` method.
    *   A conditional statement `if (InternalSettings.isUnityApp()) { return; }` is added at the beginning of the method.
    *   This condition effectively prevents the rest of the method from executing if the application is a Unity app.

3.  **Alignment and Bug Fix Indication:**
    *   The code change directly implements the second action described in the commit message: ""Skip event matching for unity apps.""
    *   The commit message suggests that the previous event matching logic was not working correctly for Unity apps due to their specific tree hierarchy. By skipping the original logic and delegating it to the Unity SDK, the change aims to correct this behavior.
    *   The change is intended to fix a problem with how codeless event matching was handled in Unity apps, indicating a bug fix.

**Conclusion:**

**Buggy**
"
facebook-android-sdk,1430.json,8d168dd203acf2e068a6b6f6029e9baaea992538,"@@ -1,4 +1,3 @@
                     public void onBackgroundProcessingStarted() {
-                        view.findViewById(R.id.com_facebook_login_activity_progress_bar)
-                                .setVisibility(View.VISIBLE);
+                        progressBar.setVisibility(View.VISIBLE);
                     }",NotBuggy,"Facebook Android SDK 4.28
",Buggy,"The commit message ""Facebook Android SDK 4.28"" is quite generic and doesn't provide specific details about the changes made.

The code diff shows a change in how the progress bar's visibility is set. Instead of using `view.findViewById(R.id.com_facebook_login_activity_progress_bar)` to locate the progress bar every time, it now uses `progressBar`. This suggests that `progressBar` is likely a cached reference to the progress bar view.

Reasoning:

1. **Lack of Specificity in Commit Message:** The commit message is too general. A good commit message should describe *why* the change was made.
2. **Code Change Implies Optimization/Refactoring:** The code change suggests that the original code was inefficient because it was repeatedly calling `findViewById` to get a reference to the progress bar. The new code likely caches the reference, which is an optimization. This could be a fix for a performance issue, which can be considered a bug.

Conclusion: **Buggy**
"
facebook-android-sdk,2780.json,9dd41692e4bc962ab7bc56c5a17679be80ab50bf,"@@ -1,4 +1,7 @@
     private static String getUserAgent() {
-        // TODO port: construct user agent string with version
-        return ""FBAndroidSDK"";
+        if (userAgent == null) {
+            userAgent = String.format(""%s.%s"", USER_AGENT_BASE, FacebookSdkVersion.BUILD);
+        }
+
+        return userAgent;
     }",NotBuggy,"Added UserAgent, Migration Bundle

Summary: Adding version string in UserAgent and current sdk Migration Bundle.

Test Plan:
- Unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang, jacl

Reviewed By: mingfli

CC: msdkexp@

Differential Revision: https://phabricator.fb.com/D583727

Task ID: 994912
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Added UserAgent, Migration Bundle"" suggests the addition of a UserAgent string and a Migration Bundle. The description elaborates that the UserAgent now includes a version string.

2.  **Git Diff:** The diff shows a modification to the `getUserAgent()` method.
    *   The original code had a `TODO` comment indicating that the UserAgent string construction was incomplete. It simply returned a static string ""FBAndroidSDK"".
    *   The modified code initializes the `userAgent` with a formatted string that includes `USER_AGENT_BASE` and `FacebookSdkVersion.BUILD`. This implies the original implementation was missing version information, which is now being added.
    *   The code also adds a check to ensure the `userAgent` is only initialized once.

3.  **Reasoning:**
    *   The `TODO` comment in the original code clearly indicates an incomplete implementation.
    *   The commit message states that the change adds a version string to the UserAgent.
    *   The diff confirms that the UserAgent string is now constructed with version information from `FacebookSdkVersion.BUILD`.
    *   The original implementation returning a hardcoded string ""FBAndroidSDK"" can be considered a bug or at least an incomplete feature. The new implementation addresses this by including the version.

**Conclusion:**

The changes address an incomplete implementation by adding version information to the UserAgent string. This can be considered a bug fix or an enhancement to correct a missing feature.

**Buggy**
"
facebook-android-sdk,2061.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-    public List<String> getDeclinedPermissions() {
+    public Set<String> getDeclinedPermissions() {
         return this.declinedPermissions;
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message ""Facebook Android SDK 4.0"" is very generic and doesn't provide specific information about the changes.

The code diff shows a change in the return type of the `getDeclinedPermissions()` method from `List<String>` to `Set<String>`. This change suggests that the order of declined permissions is not important, and using a `Set` might provide better performance or prevent duplicate entries.

Since the commit message is vague and the code change doesn't clearly indicate a bug fix, it's difficult to determine if this change addresses a specific bug. It's more likely a general improvement or API update.

**NotBuggy**
"
facebook-android-sdk,775.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-        public int describeContents() {
-            return 0;
-        }

+    public int describeContents() {
+        return 0;
+    }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message ""Facebook Android SDK 4.0"" is very generic and doesn't provide specific details about the changes. The diff shows a minor formatting change: adjusting the indentation of the `describeContents()` method. There's no indication of a bug fix, error handling, or logical correction. The change seems purely stylistic.

**NotBuggy**"
facebook-android-sdk,477.json,0086a52b843d60753942a20573fcdde4c9a3e948,"@@ -1,4 +1,4 @@
-        public Builder setObjectType(final LikeView.ObjectType objectType) {
-            this.objectType = objectType == null ? LikeView.ObjectType.UNKNOWN : objectType;
+        public Builder setObjectType(final String objectType) {
+            this.objectType = objectType;
             return this;
         }",NotBuggy,"Facebook Android SDK 4.1
",Buggy,"The commit message ""Facebook Android SDK 4.1"" is very generic and doesn't provide specific details about the changes made.

The code diff shows a change in the `setObjectType` method within a `Builder` class. The original code accepted a `LikeView.ObjectType` enum, handling null input by defaulting to `LikeView.ObjectType.UNKNOWN`. The modified code now accepts a `String` directly, without any null check or default value assignment.

Reasoning:

1. **Type Change:** The parameter type of `setObjectType` has changed from a specific enum (`LikeView.ObjectType`) to a general `String`. This could be due to a change in the API or a design decision to allow more flexibility in object type specification.
2. **Null Handling Removal:** The original code explicitly handled null input by defaulting to `LikeView.ObjectType.UNKNOWN`. The new code does not have any null check. This might introduce a potential issue if a null `objectType` is passed, as it could lead to a `NullPointerException` later in the code where this value is used.  The removal of the null check suggests a potential bug fix or a change in how null values are handled elsewhere. It's possible the calling code is now expected to handle null values, or that a null value is now acceptable in this context.
3. **Lack of Context:** Without more context about the surrounding code and the intended use of `objectType`, it's difficult to definitively say if this change is a bug fix. However, the removal of explicit null handling is a common source of bugs.

Given the removal of the null check and the potential for a `NullPointerException` if a null `String` is passed as `objectType`, this change *could* be related to fixing a bug where null values were not being handled correctly. It's also possible that this change introduces a new bug.

Buggy
"
facebook-android-sdk,818.json,845ba874cc5552f4deccf7d6262d351e4bbd72e4,"@@ -1,4 +1,5 @@
-        public void writeToParcel(Parcel parcel, int flags) {
-            parcel.writeString(actionText);
-            parcel.writeLong(publishDate.getTime());
+        public void writeToParcel(Parcel parcel, int i) {
+            parcel.writeString(callId.toString());
+            parcel.writeParcelable(requestIntent, 0);
+            parcel.writeInt(requestCode);
         }",NotBuggy,"Facebook Android SDK 3.5 (see https://developers.facebook.com/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5

Test Plan: - Release testing

Reviewers: mingfli

Reviewed By: mingfli

CC: platform-diffs@lists, subodh, dreiss

Differential Revision: https://phabricator.fb.com/D935263
",NotBuggy,"The commit message ""Facebook Android SDK 3.5"" indicates a release of the Facebook Android SDK, version 3.5. The provided diff shows changes to the `writeToParcel` method of an unspecified class. The original code wrote `actionText` (a String) and `publishDate` (a Date) to the Parcel. The modified code writes `callId` (a UUID), `requestIntent` (a Parcelable), and `requestCode` (an integer) to the Parcel.

The change in the `writeToParcel` method suggests a modification to how data is serialized for inter-process communication or persistence. Without more context about the class containing this method, it's difficult to determine if this change is a bug fix. However, the change itself doesn't scream ""bug fix"". It looks more like a change in the data structure being serialized. The commit message also indicates a new release, which often involves feature additions and refactoring.

Therefore, based on the available information, it's unlikely that this change is a bug fix.

**NotBuggy**
"
facebook-android-sdk,2702.json,d4a9d0c92085e732231e5bbb09f8e1f1fe292f6a,"@@ -1,3 +1,11 @@
-    public static Request newMyFriendsRequest(Session session, Callback callback) {
-        return new Request(session, MY_FRIENDS, null, null, callback);
+    public static Request newMyFriendsRequest(Session session, final GraphUserListCallback callback) {
+        Callback wrapper = new Callback() {
+            @Override
+            public void onCompleted(Response response) {
+                if (callback != null) {
+                    callback.onCompleted(typedListFromResponse(response, GraphUser.class), response);
+                }
+            }
+        };
+        return new Request(session, MY_FRIENDS, null, null, wrapper);
     }",NotBuggy,"Minor Request API changes

Summary:
- Added execute variants to Request static helpers
- Made callbacks strongly typed for Request statics
- Renamed execute -> executeAndWait
- Added instance executeAndWait/executeAsync to Request and RequestBatch

Test Plan:
- added unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang

Reviewed By: mingfli

Differential Revision: https://phabricator.fb.com/D600294

Task ID: 1801701
",Buggy,"Okay, let's analyze this Git diff and commit message to determine if it indicates a bug fix.

**Analysis:**

1.  **Commit Message:** The commit message ""Minor Request API changes"" suggests enhancements or modifications to the Request API. The summary elaborates on these changes, including adding execute variants, strongly typing callbacks, renaming methods, and adding instance methods.

2.  **Code Diff:** The code diff shows a change in the `newMyFriendsRequest` method. Specifically, the `Callback` type is changed to `GraphUserListCallback`. A wrapper `Callback` is created to adapt the new `GraphUserListCallback` to the old `Callback` interface. Inside the wrapper, `typedListFromResponse` is called to convert the response to a list of `GraphUser` objects.

3.  **Bug Fix Inference:** The change in callback type and the introduction of a wrapper suggest that the original `Callback` was not providing the expected type information. The `typedListFromResponse` method indicates that the response needed to be explicitly converted to a list of `GraphUser` objects. This strongly suggests that the previous implementation was either not type-safe or was returning the wrong type, which could be considered a bug. The change ensures that the callback receives a strongly typed list of `GraphUser` objects.

**Conclusion:**

Based on the code diff and the commit message, the changes indicate that the previous implementation had a potential type-related issue. The introduction of the `GraphUserListCallback` and the wrapper with `typedListFromResponse` strongly suggests a bug fix related to type safety and data conversion.

**Buggy**
"
facebook-android-sdk,2571.json,1ebea4d8e02aff0bc96cad34c959ab680421f8eb,"@@ -1,7 +1,9 @@
     private static void cancelCurrentTask() {
-        if (currentFuture != null) {
-            currentFuture.cancel(false);
-        }
+        synchronized (currentFutureLock) {
+            if (currentFuture != null) {
+                currentFuture.cancel(false);
+            }
 
-        currentFuture = null;
+            currentFuture = null;
+        }
     }",NotBuggy,"Facebook Android SDK 4.27
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** ""Facebook Android SDK 4.27"" - This commit message is quite generic and doesn't explicitly mention any bug fixes. It simply indicates a version update of the Facebook Android SDK.

2.  **Code Diff Analysis:**
    *   The code change involves adding a `synchronized` block around the code that accesses and modifies `currentFuture`.
    *   Specifically, the `cancelCurrentTask` method now synchronizes on `currentFutureLock` before checking if `currentFuture` is not null, canceling it, and then setting `currentFuture` to null.

3.  **Interpretation:**
    *   The addition of the `synchronized` block suggests a potential concurrency issue. Without synchronization, multiple threads could potentially access and modify `currentFuture` simultaneously, leading to race conditions. For example, one thread might check if `currentFuture` is not null, while another thread cancels and sets it to null in between, leading to a `NullPointerException` or other unexpected behavior.
    *   The synchronization aims to prevent these race conditions by ensuring that only one thread can access and modify `currentFuture` at a time. This strongly suggests that the original code was susceptible to concurrent modification issues, which is a type of bug.

**Conclusion:**

The code change introduces synchronization to prevent concurrent access to a shared resource (`currentFuture`). This indicates a fix for a potential race condition, which is a type of bug.

**Buggy**"
facebook-android-sdk,2275.json,2edb0d361bce159d052355395d6f3dd443c29a6b,"@@ -1,38 +1,38 @@
     private static synchronized void initialize() {
         try {
             FetchedAppSettings settings = FetchedAppSettingsManager.queryAppSettings(
                     FacebookSdk.getApplicationId(), false);
             if (settings == null) {
                 return;
             }
             String eventFilterResponse = settings.getRestrictiveDataSetting();
             if (!eventFilterResponse.isEmpty()) {
                 JSONObject jsonObject = new JSONObject(eventFilterResponse);
 
-                deprecatedParams.clear();
+                deprecatedParamFilters.clear();
 
                 Iterator<String> keys = jsonObject.keys();
                 while (keys.hasNext()) {
                     String key = keys.next();
                     JSONObject json = jsonObject.getJSONObject(key);
                     if (json != null) {
                         if (json.optBoolean(""is_deprecated_event"")) {
                             deprecatedEvents.add(key);
                         } else {
                             JSONArray deprecatedParamJsonArray = json
                                     .optJSONArray(""deprecated_param"");
-                            DeprecatedParam deprecatedParam = new DeprecatedParam(key,
+                            DeprecatedParamFilter deprecatedParamFilter = new DeprecatedParamFilter(key,
                                     new ArrayList<String>());
                             if (deprecatedParamJsonArray != null) {
-                                deprecatedParam.deprecateParams = Utility
+                                deprecatedParamFilter.deprecateParams = Utility
                                         .convertJSONArrayToList(deprecatedParamJsonArray);
                             }
-                            deprecatedParams.add(deprecatedParam);
+                            deprecatedParamFilters.add(deprecatedParamFilter);
                         }
                     }
                 }
             }
         } catch (Exception e) {
             /* swallow */
         }
     }",NotBuggy,"Refactor: better naming and several small changes

Reviewed By: ZebingZong

Differential Revision: D19523661

fbshipit-source-id: 664f47c474172136f9ef6e2f7f157f428cd8f5f6
",NotBuggy,"The commit message indicates a refactoring with better naming and several small changes. The diff shows the following changes:

1.  `deprecatedParams.clear()` is changed to `deprecatedParamFilters.clear()`.
2.  `DeprecatedParam deprecatedParam` is changed to `DeprecatedParamFilter deprecatedParamFilter`.
3.  `deprecatedParam.deprecateParams` is changed to `deprecatedParamFilter.deprecateParams`.
4.  `deprecatedParams.add(deprecatedParam)` is changed to `deprecatedParamFilters.add(deprecatedParamFilter)`.

These changes suggest a renaming of variables and classes, which aligns with the commit message's ""better naming"" aspect. There's no explicit bug fix indicated in the code. The `try-catch` block remains unchanged, and there are no added error handling or logical corrections.

**NotBuggy**"
facebook-android-sdk,2617.json,426267f3dcc9daadcf383ec1896bffa1d7d46852,"@@ -1,9 +1,7 @@
         public String toKey() {
             switch (this) {
-                case ADDRESS_DETECTION:
-                case APP_EVENT_PREDICTION: return ""fc3"";
                 case MTML_ADDRESS_DETECTION: return ""address_detect"";
                 case MTML_APP_EVENT_PREDICTION: return ""app_event_pred"";
             }
             return ""Unknown"";
         }",NotBuggy,"Remove Non-MTML model

Summary: Same as the title

Reviewed By: jiangcici

Differential Revision: D20863645

fbshipit-source-id: e34e7b2a6a3aaddbf82b543c46eda834339da7ae
",NotBuggy,"The commit message ""Remove Non-MTML model"" indicates the removal of models that are not related to MTML. The code diff shows the removal of `ADDRESS_DETECTION` and `APP_EVENT_PREDICTION` cases from the `switch` statement. These cases are likely the non-MTML models that the commit message refers to. The removal of these cases suggests that they were either deprecated, replaced, or no longer needed. This change does not directly indicate a bug fix, but rather a cleanup or refactoring of the code.

**NotBuggy**"
facebook-android-sdk,304.json,b2fa42b8fca13e5f44025fb76b270ef8b9733eb8,"@@ -1,20 +1,26 @@
     private static void validate(ShareContent content, Validator validator)
             throws FacebookException {
         if (content == null) {
             throw new FacebookException(""Must provide non-null content to share"");
         }
 
         if (content instanceof ShareLinkContent) {
             validator.validate((ShareLinkContent) content);
         } else if (content instanceof SharePhotoContent) {
             validator.validate((SharePhotoContent) content);
         } else if (content instanceof ShareVideoContent) {
             validator.validate((ShareVideoContent) content);
         } else if (content instanceof ShareOpenGraphContent) {
             validator.validate((ShareOpenGraphContent) content);
         } else if (content instanceof ShareMediaContent) {
             validator.validate((ShareMediaContent) content);
         } else if (content instanceof ShareCameraEffectContent) {
             validator.validate((ShareCameraEffectContent) content);
+        } else if (content instanceof ShareMessengerOpenGraphMusicTemplateContent) {
+            validator.validate((ShareMessengerOpenGraphMusicTemplateContent) content);
+        } else if (content instanceof ShareMessengerMediaTemplateContent) {
+            validator.validate((ShareMessengerMediaTemplateContent) content);
+        } else if (content instanceof ShareMessengerGenericTemplateContent) {
+            validator.validate((ShareMessengerGenericTemplateContent) content);
         }
     }",NotBuggy,"Facebook Android SDK 4.29
",NotBuggy,"The commit message ""Facebook Android SDK 4.29"" indicates a version update or release of the Facebook Android SDK. The diff adds validation for `ShareMessengerOpenGraphMusicTemplateContent`, `ShareMessengerMediaTemplateContent`, and `ShareMessengerGenericTemplateContent`. These additions suggest the introduction of new features or support for new content types within the SDK. The code changes involve adding new `else if` blocks to handle these new content types, ensuring they are validated using the provided `Validator`. This doesn't inherently indicate a bug fix, but rather an extension of the SDK's functionality.

**NotBuggy**"
titan,4308.json,53441fa4ba940ba38ebbe2710ce9041819722020,"@@ -1,3 +1,3 @@
-    public boolean isValidDataType(Class<?> clazz) {
-        return true;
+    public boolean isQNF() {
+        return false;
     }",NotBuggy,"Added the notion of Query-Normal-Form (QNF) to property constraints and queries in Titan. Now, all queries inside titan have to conform to QNF which makes it easier to deal with conditions because one can precondition on QNF and then process accordingly. NOT YET TESTED.
",NotBuggy,"The commit message indicates a significant change related to query processing in Titan, introducing the concept of Query-Normal-Form (QNF). The code diff, however, shows a very small change: renaming a method from `isValidDataType` to `isQNF` and changing its return value from `true` to `false`.

Here's a step-by-step analysis:

1. **Commit Message Context:** The commit message describes a system-wide change affecting all queries. It suggests a structural modification to how queries are handled within Titan.

2. **Code Diff Relevance:** The code diff is extremely small and doesn't reflect the scope of changes described in the commit message.  Renaming a method and hardcoding its return value to `false` doesn't implement the concept of QNF or enforce it on queries.  It's a tiny, isolated change.

3. **Bug Fix Indicators:** The code change itself doesn't directly indicate a bug fix. It's more like a placeholder or a very preliminary step towards a larger feature.  The change from `true` to `false` *could* be interpreted as disabling a previously incorrect validation, but without more context, it's impossible to say for sure. The ""NOT YET TESTED"" part in the commit message further suggests that this is not a bug fix, but rather a work in progress.

4. **Alignment:** The code change is *very weakly* aligned with the commit message. The method name `isQNF` hints at the intended functionality, but the implementation is trivial and incomplete. The commit message promises a significant structural change, while the code provides only a superficial modification.

5. **Conclusion:** Given the minimal code change and the ""NOT YET TESTED"" disclaimer, it's unlikely that this commit represents a bug fix. It's more likely an initial step in a larger feature implementation.

**NotBuggy**
"
titan,1229.json,ba36d65be185a738d418b86746aad625361bb71b,"@@ -1,3 +1,3 @@
     private static int getLimit(long limitAndValuePos) {
-        return (int)(limitAndValuePos>>>32);
+        return (int)(limitAndValuePos>>>32l);
     }",NotBuggy,"Bugifixing of backend change
",Buggy,"Okay, let's analyze the provided Git diff and commit message to determine if the changes indicate a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message ""Bugifixing of backend change"" explicitly states that the commit is intended to fix a bug. This is a strong indicator that the diff should contain a bug fix.

2.  **Code Diff Analysis:**
    *   The code change involves a right bit shift operation (`>>>`) on a `long` variable (`limitAndValuePos`).
    *   The original code `(int)(limitAndValuePos>>>32)` performs a right bit shift by 32 bits.
    *   The modified code `(int)(limitAndValuePos>>>32l)` performs a right bit shift by 32 bits, but the `32` is now explicitly a `long` literal (`32l`).

3.  **Potential Bug:**
    *   In Java, without the `l` suffix, `32` is treated as an `int`. While shifting by an `int` value works, explicitly using a `long` literal might be necessary in some contexts to avoid potential issues with implicit type conversions or unexpected behavior when dealing with large bit shift values.
    *   The original code might have been causing unexpected results due to integer promotion rules or other subtle issues related to the interaction between `int` and `long` types during the bit shift operation. By explicitly specifying `32l`, the code ensures that the shift amount is treated as a `long,` potentially resolving a bug related to incorrect bit shifting.

**Conclusion:**

The commit message explicitly indicates a bug fix, and the code change involves a modification to a bit shift operation that could potentially resolve issues related to type conversions or unexpected behavior. Therefore, the changes suggest a bug fix.

**Buggy**"
titan,4809.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
-	public int getNumCompactions() {
-		return numberOfCompactions;
-	}

+    public int getNumCompactions() {
+        return numberOfCompactions;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message ""big fatty refactor -- tests do not pass"" indicates a large-scale refactoring effort, but the tests are failing. The provided diff shows a minor change in whitespace. This change is unlikely to be the cause of failing tests after a refactor, but it's also not indicative of a bug fix. The change itself doesn't introduce or resolve any logical errors. It's just a formatting update.

**NotBuggy**
"
titan,4181.json,9737d31b23256e071d7e761a53819feff896b472,"@@ -1,6 +1,9 @@
     public QueryProfiler addNested(String groupName) {
+        //Flatten out AND/OR nesting
+        if (groupName.equals(AND_QUERY) || groupName.equals(OR_QUERY)) return this;
+
         int nextId = (subMetricCounter++);
         MutableMetrics nested = new MutableMetrics(metrics.getId()+"".""+groupName+""_""+nextId,groupName);
         metrics.addNested(nested);
         return new TP3ProfileWrapper(nested);
     }",NotBuggy,"Flattening profile metrics
",Buggy,"Okay, let's analyze the provided commit message and Git diff to determine if the changes indicate a bug fix.

**Commit Message Analysis:**

The commit message ""Flattening profile metrics"" suggests that the change aims to simplify or restructure the way profile metrics are organized, likely to avoid deep nesting.

**Code Diff Analysis:**

The code diff introduces a conditional statement at the beginning of the `addNested` method:

```java
if (groupName.equals(AND_QUERY) || groupName.equals(OR_QUERY)) return this;
```

This code checks if the `groupName` is either `AND_QUERY` or `OR_QUERY`. If it is, the method immediately returns the current `QueryProfiler` instance without creating a new nested metric. This effectively prevents the creation of nested metrics for ""AND"" and ""OR"" query groups.

**Reasoning:**

The commit message and code diff align well. The code change directly implements the ""flattening"" mentioned in the commit message by preventing the creation of nested metrics for specific query types (AND/OR).

The fact that the code is explicitly preventing the creation of nested metrics for `AND_QUERY` and `OR_QUERY` suggests that the previous behavior (allowing nesting) was likely problematic. Perhaps it led to overly complex metric structures or incorrect aggregation. This indicates a bug fix or at least a workaround for a design flaw. The comment ""//Flatten out AND/OR nesting"" further supports this.

**Conclusion:**

**Buggy**
"
titan,816.json,eab917b78fa300d8e6d81a543d4f1738f945b5b2,"@@ -1,3 +1,3 @@
-    public ByteBuffer getSliceEnd() {
+    public StaticBuffer getSliceEnd() {
         return sliceEnd;
     }",NotBuggy,"Refactored titan-core to use custom Buffer interfaces for better abstraction and thread safety instead of ByteBuffer.

INTERMEDIATE COMMIT - DOES NOT BUILD
",NotBuggy,"Okay, let's analyze the provided Git diff and commit message to determine if it indicates a bug fix.

**Reasoning:**

1.  **Commit Message:** The commit message states a refactoring effort to replace `ByteBuffer` with custom `Buffer` interfaces for improved abstraction and thread safety. This suggests a design improvement rather than a direct bug fix.

2.  **Git Diff:** The diff shows a change in the return type of the `getSliceEnd()` method from `ByteBuffer` to `StaticBuffer`. This aligns with the commit message's intention to replace `ByteBuffer`.

3.  **Bug Fix Indicators:** There's no explicit indication of a bug fix in either the commit message or the code diff. The changes seem to be focused on refactoring and improving the codebase's design and thread safety. There are no error handling updates, logical corrections, or exception-handling improvements.

**Conclusion:**

The changes are consistent with a refactoring effort as described in the commit message. There's no evidence to suggest that this commit addresses a specific bug.

**NotBuggy**
"
titan,3913.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
         public boolean hasNext() {
-            return nextEdge!=null;
+            return nextEdge != null;
         }",NotBuggy,"big fatty refactor -- tests do not pass.
",Buggy,"The commit message indicates a large refactoring, and the tests are failing. The diff shows a minor change in the `hasNext()` method, specifically adding a space between `!=` and `null`. This change, while small, could be related to the failing tests, especially if the original code was causing issues due to subtle parsing or comparison problems. Since the commit message explicitly states that tests are failing after a refactor, and the diff shows a change to a core method, it is likely that this commit is an attempt to fix a bug introduced during the refactoring.

**Buggy**
"
titan,2659.json,d5662482340f61d370cde27fece75d7ecb6b16e4,"@@ -1,3 +1,3 @@
-                public boolean apply(@Nullable InternalRelation internalRelation) {
-                    return internalRelation.getType().equals(key);
-                }

+                    public boolean apply(@Nullable TitanVertex vertex) {
+                        return query.matches(vertex);
+                    }",NotBuggy,"Intermediate commit on indexing and complete refactor
",NotBuggy,"The commit message ""Intermediate commit on indexing and complete refactor"" suggests broad changes related to indexing and a significant refactoring effort.

The provided code diff shows a change in a method's implementation. Specifically, the `apply` method's logic and signature have been altered. The original code filters `InternalRelation` objects based on their type, comparing it to a `key`. The modified code filters `TitanVertex` objects using a `query`. The change from `InternalRelation` to `TitanVertex` and the introduction of a `query` object strongly suggest a shift in the underlying data model or query mechanism.

The commit message mentions refactoring, which aligns with the substantial code modification seen in the diff. The change doesn't explicitly indicate a bug fix, but rather a change in functionality or implementation strategy. There's no error handling, logical correction, or exception handling improvement visible in the diff.

**NotBuggy**
"
titan,526.json,d4056cb5a0ec5764ba5d0bd7454c68062d576ba1,"@@ -1,4 +1,4 @@
-        public Builder supportedMappings(Mapping... maps) {
+        public Builder supportedStringMappings(Mapping... maps) {
             supportedMappings.addAll(Arrays.asList(maps));
             return this;
         }",NotBuggy,"Finised streamlining test cases and query semantics across the different indexing backends. Added additional test coverage for Lucene.
SOLR does not yet work.
",NotBuggy,"The commit message indicates streamlining test cases and query semantics, and adding test coverage for Lucene. The diff shows a method name change from `supportedMappings` to `supportedStringMappings`. This change doesn't seem directly related to bug fixing, but rather a refactoring or renaming. It's possible this renaming was done to clarify the purpose of the method or to better align with the overall design as part of the streamlining effort. There's no explicit error handling, logical correction, or exception handling improvement visible in this diff.

**NotBuggy**
"
titan,5594.json,31b68d5c462b46d42f4ca7d3b3f63923a6d5a1c7,"@@ -1,12 +1,12 @@
     public static ScanMetrics hbaseRemove(String titanPropertiesPath, String indexName, String relationType)
             throws InterruptedException, IOException, ClassNotFoundException {
         Properties p = new Properties();
         FileInputStream fis = null;
         try {
             fis = new FileInputStream(titanPropertiesPath);
             p.load(fis);
             return hbaseRemove(p, indexName, relationType);
         } finally {
-            IOUtils.close(fis);
+            IOUtils.closeQuietly(fis);
         }
     }",NotBuggy,"Remove elasticsearch-hadoop-mr dependency

titan-hadoop has depended on this artifact since
042d04148489f4650e232682388f24ace709a95b, but it was only actually
used for its IOUtils.close static helper method.  Titan already has a
bunch of alternative providers of static helper methods with
equivalent functionality.
",Buggy,"The commit message indicates the removal of a dependency because a utility class from that dependency was being used, and an alternative was available within the project. The diff shows a change from `IOUtils.close(fis)` to `IOUtils.closeQuietly(fis)`. This change suggests a bug fix because `closeQuietly` usually handles null input streams and exceptions during the closing process, preventing potential NullPointerExceptions or IOExceptions from propagating. This indicates an improvement in error handling, which is a sign of a bug fix.

**Buggy**"
titan,2713.json,51075f047f33c09c17c33f7e61fcf859c815ed52,"@@ -1,5 +1,6 @@
-    protected void verifyAccess() {
-        if (isRemoved()) {
-            throw InvalidElementException.removedException(this);
-        }
+    protected final void verifyAccess() {
+        return;
+//        if (isRemoved()) {
+//            throw InvalidElementException.removedException(this);
+//        }
     }",NotBuggy,"Updated to new TP3
",Buggy,"The commit message ""Updated to new TP3"" is vague and doesn't provide specific details about the changes made.

The code diff shows that the `verifyAccess` method has been modified. The original implementation checked if the element was removed and threw an exception if it was. The updated implementation now simply returns without performing any checks, and the original code is commented out.

Reasoning:
1. The commit message doesn't give any context about why the `verifyAccess` method was modified.
2. The original implementation of `verifyAccess` was likely in place to prevent access to removed elements, which could lead to unexpected behavior or errors.
3. The updated implementation disables this check, which could potentially introduce bugs if removed elements are accessed.
4. Disabling a check that throws an exception when a removed element is accessed is likely a bug fix.

Conclusion: **Buggy**"
titan,2620.json,aead96abd7152d9843e25578fd640dbbdaa8b765,"@@ -1,3 +1,3 @@
     public Iterable<Edge> getEdges() {
-        return new VertexCentricEdgeIterable(getVertices());
+        return new VertexCentricEdgeIterable(getInternalVertices(),RelationCategory.EDGE);
     }",NotBuggy,"Intermediate big bang commit
",Buggy,"The commit message ""Intermediate big bang commit"" is vague and doesn't provide specific information about the changes made.

The code modifies the `getEdges()` method to use `getInternalVertices()` and `RelationCategory.EDGE` in the `VertexCentricEdgeIterable` constructor. The original code used `getVertices()`. Without more context, it's difficult to determine if this change is a bug fix or a new feature. However, the change suggests a refinement in how edges are retrieved, potentially addressing an issue with the original implementation. It's plausible that `getVertices()` was returning an incorrect set of vertices for edge retrieval, and `getInternalVertices()` provides a more accurate set. The addition of `RelationCategory.EDGE` further suggests a more specific filtering or categorization of edges, which could be related to correcting a previous oversight.

Given the lack of detail in the commit message, and the code change that seems to refine the edge retrieval process, it's possible that this commit addresses a bug related to incorrect or incomplete edge retrieval.

**Buggy**
"
titan,4872.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,12 +1,12 @@
-	public static final long[] arrayDifference(long[] arr, long[] subset) {
-		long[] res = new long[arr.length-subset.length];
-		int pos = 0;
-		for (int i=0;i<arr.length;i++) {
-			if (!Longs.contains(subset, arr[i])) {
-				res[pos]=arr[i];
-				pos++;
-			}
-		}
-		assert pos==res.length;
-		return res;
-	}

+    public static final long[] arrayDifference(long[] arr, long[] subset) {
+        long[] res = new long[arr.length - subset.length];
+        int pos = 0;
+        for (int i = 0; i < arr.length; i++) {
+            if (!Longs.contains(subset, arr[i])) {
+                res[pos] = arr[i];
+                pos++;
+            }
+        }
+        assert pos == res.length;
+        return res;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message ""big fatty refactor -- tests do not pass"" indicates a large-scale refactoring effort, but the tests are failing. The provided diff shows a minor change in the formatting of the `arrayDifference` method. The code logic remains the same. There are no error handling updates, logical corrections, or exception-handling improvements. The changes do not indicate a bug fix.

**NotBuggy**"
titan,4164.json,7f94323100a007d1f346b163485b14a43a53dd4c,"@@ -1,3 +1,3 @@
     public int getLimit() {
-        return getBranchFactor();
+        return this.limit;
     }",NotBuggy,"Updated optimizers to include PropertyStep. Reworked current logic. Tests to be added.
",Buggy,"The commit message ""Updated optimizers to include PropertyStep. Reworked current logic. Tests to be added."" indicates a change in the optimizers, potentially including a new `PropertyStep`. The phrase ""Reworked current logic"" suggests modifications to existing code. The diff shows a change in the `getLimit()` method, where `getBranchFactor()` is replaced with `this.limit`. This looks like a bug fix, where the wrong value was being returned.

**Buggy**
"
titan,2594.json,e1ac9ae34518829d69f60234525b2133b31a88e2,"@@ -1,4 +1,4 @@
     public InternalVertex getOtherPartitionVertex(TitanVertex partitionedVertex, long otherPartition) {
         Preconditions.checkArgument(isPartitionedVertex(partitionedVertex));
-        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getID(), otherPartition));
+        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getLongId(), otherPartition));
     }",NotBuggy,"Partial Faunus refactoring
",Buggy,"The commit message ""Partial Faunus refactoring"" suggests a general code improvement or restructuring.

The code diff modifies the `getOtherPartitionVertex` method. Specifically, it changes `partitionedVertex.getID()` to `partitionedVertex.getLongId()`. This indicates that the original code might have been using the wrong method to retrieve the vertex ID, which could lead to incorrect vertex retrieval or processing. This correction strongly suggests that the original code had a bug related to how it was retrieving the vertex ID.

Therefore, the changes indicate a bug fix.

**Buggy**
"
