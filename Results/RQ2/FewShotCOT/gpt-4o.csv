Project,File,CommitHash,Diff,Decision,CommitMessage,Detection,Explanation
spring-boot,9232.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,7 +1,7 @@
 	public static String template(TemplateEngine engine, String name, Map<String, ?> model)
 			throws IOException, CompilationFailedException, ClassNotFoundException {
-		Writable writable = getTemplate(name).make(model);
+		Writable writable = getTemplate(engine, name).make(model);
 		StringWriter result = new StringWriter();
 		writable.writeTo(result);
 		return result.toString();
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"The code change specifically addresses an issue mentioned in the commit message. The method template was previously ignoring the engine argument; instead of using a default engine or incorrect method, it now uses getTemplate with the engine parameter, aligning it with the commit message's description of the bug.

This change suggests a logical correction in the handling of the parameters, confirming an oversight or bug was present and is now fixed. Therefore, the modifications in the diff are consistent with a bug fix as described in the commit message.

Therefore, the assessment for this change is **Buggy**."
spring-boot,9233.json,ed15f742fd4eacc14b06908112ac4ca6ae4c0f90,"@@ -1,17 +1,16 @@
-	private static Template getTemplate(String name) throws CompilationFailedException,
+	private static Template getTemplate(TemplateEngine engine, String name) throws CompilationFailedException,
 			ClassNotFoundException, IOException {
-		GStringTemplateEngine engine = new GStringTemplateEngine();
 
 		File file = new File(""templates"", name);
 		if (file.exists()) {
 			return engine.createTemplate(file);
 		}
 
 		ClassLoader classLoader = GroovyTemplate.class.getClassLoader();
 		URL resource = classLoader.getResource(""templates/"" + name);
 		if (resource != null) {
 			return engine.createTemplate(resource);
 		}
 
 		return engine.createTemplate(name);
 	}",Buggy,"Fix bug in GroovyTemplate convenience

It was ignoring the engine argument in the 3 arg version
of template().",Buggy,"The commit message indicates a fix to address ignoring the engine argument in a specific method version. The diff modifies getTemplate to accept a TemplateEngine argument, replacing the hardcoded use of GStringTemplateEngine. This aligns changes with the message, ensuring the passed engine is used. The answer is **Buggy**."
spring-boot,510.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,3 +1,3 @@
-		public CouchbaseEnvironment couchbaseEnvironment() throws Exception {
-			return createEnvironment(this.properties);
+		public DefaultCouchbaseEnvironment couchbaseEnvironment() throws Exception {
+			return initializeEnvironmentBuilder(this.properties).build();
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",NotBuggy,"The provided diff makes changes to the `couchbaseEnvironment` method by replacing the call to `createEnvironment` with `initializeEnvironmentBuilder` followed by `.build()`. This implies that the method now returns a `DefaultCouchbaseEnvironment` instead of whatever type `createEnvironment` was initially returning. 

This change aligns with the commit message, which mentions making the socket connect timeout configurable. Although the method name `initializeEnvironmentBuilder` suggests steps consistent with setting up an environment where configurations like timeouts could be applied, the diff itself doesn't explicitly show changes related to socket timeout customization.

However, the commit message also mentions another issue: the inability to easily extend `CouchbaseConfiguration` for customization. This change may address that, as the diff suggests the setup of a builder pattern, which typically makes customization or extension more flexible.

The actual socket timeout customization indicated in the commit message appears to be part of a broader set of changes not shown in this diff. The diff seems to facilitate more flexible configurations, which is consistent with the commit narrative, possibly making it easier for developers to extend and modify settings like timeouts.

Since the core focus of the diff seems to resolve issues related to configurability rather than a direct bug (unless extending the configuration itself was considered a bug), the specific code changes shown here are more about enhancing functionality rather than fixing bugs in existing logic.

Therefore, assessing the reformulation of configurability, I conclude with **NotBuggy** for this specific code snippet, as there is no direct indication of addressing erroneous logic or execution faults in the diff itself."
spring-boot,516.json,99ae6dac5321a741d93ff5187fafb94c295a6928,"@@ -1,22 +1,23 @@
-		protected CouchbaseEnvironment createEnvironment(CouchbaseProperties properties) {
+		protected DefaultCouchbaseEnvironment.Builder initializeEnvironmentBuilder(CouchbaseProperties properties) {
 			CouchbaseProperties.Endpoints endpoints = properties.getEnv().getEndpoints();
 			CouchbaseProperties.Timeouts timeouts = properties.getEnv().getTimeouts();
 			DefaultCouchbaseEnvironment.Builder builder = DefaultCouchbaseEnvironment
 					.builder().connectTimeout(timeouts.getConnect())
 					.kvEndpoints(endpoints.getKeyValue())
 					.kvTimeout(timeouts.getKeyValue())
 					.queryEndpoints(endpoints.getQuery())
 					.queryTimeout(timeouts.getQuery()).viewEndpoints(endpoints.getView())
+					.socketConnectTimeout(timeouts.getSocketConnect())
 					.viewTimeout(timeouts.getView());
 			CouchbaseProperties.Ssl ssl = properties.getEnv().getSsl();
 			if (ssl.getEnabled()) {
 				builder.sslEnabled(true);
 				if (ssl.getKeyStore() != null) {
 					builder.sslKeystoreFile(ssl.getKeyStore());
 				}
 				if (ssl.getKeyStorePassword() != null) {
 					builder.sslKeystorePassword(ssl.getKeyStorePassword());
 				}
 			}
-			return builder.build();
+			return builder;
 		}",Buggy,"Customize Couchbase's socket connect timeout

Our Windows build is failing currently because the couchbase server does
not handle a socket connection within a second (the default). This commit
adds a property to customize this option and set it to 10 sec in the
sample.

While investigating this issue, it turns out that while
`CouchbaseConfiguration` is public, it is not really possible to extend
it in user's configuration. This commit fixes this problem and add a test
that demonstrates how it can be used.

Closes gh-5657",NotBuggy,"The commit message describes customizing the socket connect timeout for Couchbase and fixing an extendability issue with `CouchbaseConfiguration`. 

The code diff shows modifications to add a `.socketConnectTimeout()` method call to configure the socket connect timeout based on provided properties. This aligns with customizing the socket connect timeout as mentioned in the commit message. 

Additionally, the method has been renamed from `createEnvironment` to `initializeEnvironmentBuilder` and now returns a `DefaultCouchbaseEnvironment.Builder` instead of a fully built environment object. This change can make it easier for users to extend and further customize the builder configuration, which addresses the issue of extendability as mentioned in the commit.

While the commit message describes a functional improvement and customizability enhancement, there is no indication that the actual adjustments address a specific bug unrelated to functionality or extendability. The extensions and configurations in this commit are intended to allow users more flexibility and are not directly fixing an error in behavior.

Overall, the changes align with the commit message, and though they improve functionality and enhance customizability, they do not necessarily indicate a bug fix as per the typical definition (i.e., correcting erroneous behavior).

Concluding, the analysis of code changes, messaging, and the purpose indicates that the source code modification does not primarily focus on resolving an error but is more about providing enhancements and flexibility. Thus, the conclusion for this commit is **NotBuggy**."
spring-boot,6313.json,1a2186e6efed5c53d7c50548de7b30b9a6dcd7f7,"@@ -1,6 +1,6 @@
 		private boolean startsWithArgumentClassName(String message) {
-			Predicate<Object> startsWith = (argument) -> argument != null
-					&& message.startsWith(argument.getClass().getName());
+			Predicate<Object> startsWith = (argument) -> startsWithArgumentClassName(
+					message, argument);
 			return startsWith.test(this.argument)
 					|| Stream.of(this.additionalArguments).anyMatch(startsWith);
 		}",Buggy,"Attempt to fix lambda error detection on JDK 9

Update `LambdaSafe` to also detect `ClassCastException` messages that
start with ""module/name"".

See gh-11584",Buggy,"The commit message indicates that the change aims to improve the detection of `ClassCastException` messages, particularly for lambda errors, by considering messages that start with ""module/name"". 

In the diff provided, the logic for checking whether a message starts with a class name has been refactored. Originally, the predicate `startsWith` checked if `message` starts with the class name of an `argument`, performed directly within the lambda expression. Now, `startsWith` uses a method `startsWithArgumentClassName(message, argument)`. Although the content of `startsWithArgumentClassName(message, argument)` is not shown, the change suggests improved modularity and possibly the handling of edge cases.

However, from this diff alone, we cannot conclusively infer whether it addresses the specific issue related to JDK 9 and messages starting with ""module/name"" unless we see the implementation of `startsWithArgumentClassName(message, argument)`.

Despite this, the change likely involves a bug fix attempt because:
- The commit message specifies an ""attempt to fix"" an error detection.
- JDK-specific issues (such as those arising from system changes in JDK 9) often involve nuanced handling and can cause defects in code execution paths if not properly addressed.

Given these elements, these changes suggest an attempt to address a specific bug related to error detection in certain environments.

In conclusion, the effort to modify behavior and attempt to fix a known issue aligns with the nature of a bug fix, and therefore, this change is **Buggy**."
spring-boot,2051.json,4b4dc28a869e2f8b988f6ac6ea8a31c274477da5,"@@ -1,9 +1,10 @@
 	private void logError(ServerRequest request, ServerResponse response, Throwable throwable) {
 		if (logger.isDebugEnabled()) {
 			logger.debug(request.exchange().getLogPrefix() + formatError(throwable, request));
 		}
-		if (response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
+		if (HttpStatus.resolve(response.rawStatusCode()) != null
+				&& response.statusCode().equals(HttpStatus.INTERNAL_SERVER_ERROR)) {
 			logger.error(request.exchange().getLogPrefix() + ""500 Server Error for "" + formatRequest(request),
 					throwable);
 		}
 	}",Buggy,"Support non-standard error codes with AbstractErrorWebExceptionHandler

Fixes gh-16691",Buggy,"The commit message indicates support for non-standard error codes. In the code, the status code comparison changed. It now checks if the raw status code can be resolved into a standard HttpStatus before comparing against INTERNAL_SERVER_ERROR. This prevents log errors for unknown status codes, avoiding potential misclassification of errors. The answer is **Buggy**."
guava,10860.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,13 +1,15 @@
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message indicates a bug fix in the `Optional#presentInstances` method, and the diff shows changes within an anonymous iterator class used in the method. Here's the step-by-step analysis:

1. **Introduction of Field**: A new private field `iterator` is added to store the iterator obtained from the `optionals` list, ensuring it's not null by using `checkNotNull()`. 

2. **Correctness and Null Check**: `checkNotNull` hints at a possible bug fix regarding nullability scenarios that weren't previously handled. By ensuring that the iterator is checked for null, the risk of encountering a `NullPointerException` during iteration is mitigated.

3. **Functionality**: The `computeNext()` method iterates over `Optional` instances. If an `Optional` is present, it returns the contained value; otherwise, it continues.

4. **Bug Relation**: Before the change, the iterator may not have been explicitly checked for null, and this could lead to runtime exceptions if `optionals.iterator()` returned null. By expressly adding the check and using a local iterator field, the method ensures safe iteration over the optionals.

Thus, the modifications in this commit are directly aligned with the commit message indicating a bug fix. The main change ensures robust handling of potential null scenarios during iteration, which is quintessential for addressing bugs.

Conclusively, these changes indicate a bug fix, and therefore the conclusion is **Buggy**."
guava,10861.json,81f0a77431bf90bf02543b999577400c7c13aa13,"@@ -1,19 +1,20 @@
-  public static <T> Iterable<T> presentInstances(Iterable<Optional<T>> optionals) {
+  public static <T> Iterable<T> presentInstances(final Iterable<Optional<T>> optionals) {
     checkNotNull(optionals);
-    final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
     return new Iterable<T>() {
       @Override public Iterator<T> iterator() {
         return new AbstractIterator<T>() {
+          private final Iterator<Optional<T>> iterator = checkNotNull(optionals.iterator());
+
           @Override protected T computeNext() {
             while (iterator.hasNext()) {
               Optional<T> optional = iterator.next();
               if (optional.isPresent()) {
                 return optional.get();
               }
             }
             return endOfData();
           }
         };
       };
     };
   }",Buggy,"Fix bug in Optional#presentInstances.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=26472796",Buggy,"The commit message mentions a bug fix in `Optional#presentInstances`. The code change relocates the instantiation of the iterator from before the return of the `Iterable<T>` to within the `AbstractIterator<T>` constructor. This modification potentially addresses a bug where successive calls to the `iterator` method on an `Iterable` could return an exhausted iterator rather than a fresh one, leading to incorrect behavior after the first iteration.

By moving the iterator instantiation into the `iterator` method, each call gets a fresh iterator, which is the expected behavior for an `Iterable`. This change aligns with the nature of fixing a bug in the `presentInstances` method.

Thus, the modifications indicate the fixing of a bug in handling iterators properly. The conclusion is **Buggy**."
guava,14951.json,803f44ac9c6a3457388394e952132fe18449cb9f,"@@ -1,4 +1,6 @@
     @Override public void clear() {
-      linkedEntries.removeAll(createEntries(delegate()));
+      for (V value : delegate) {
+        linkedEntries.remove(createEntry(value));
+      }
       delegate.clear();
     }",Buggy,"Fix performance problem in LinkedHashMultimap.removeAll(key), as documented in http://code.google.com/p/guava-libraries/issues/detail?id=371&start=100

R=kak
DELTA=3  (2 added, 0 deleted, 1 changed)


Revision created by MOE tool push_codebase.
MOE_MIGRATION=177089


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@93 8138a162-5c33-11de-8abc-d1c337b90d21",NotBuggy,"The commit message states a performance problem in `LinkedHashMultimap.removeAll(key)`. The change modifies the `clear` method, improving removal efficiency by iterating over the delegate and removing each entry individually from `linkedEntries`, rather than attempting to remove all at once. This suggests a performance optimization rather than a bug fix, resolving a potentially inefficient operation. Since the purpose is performance improvement and not correcting incorrect behavior, the conclusion is **NotBuggy**."
guava,9470.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,3 +1,5 @@
   public boolean hasEdgeConnecting(N nodeU, N nodeV) {
-    return !edgesConnecting(nodeU, nodeV).isEmpty();
+    checkNotNull(nodeU);
+    checkNotNull(nodeV);
+    return nodes().contains(nodeU) && successors(nodeU).contains(nodeV);
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"The diff indicates a change in the method `hasEdgeConnecting` to fix a bug. Previously, it relied on `edgesConnecting(nodeU, nodeV).isEmpty()` which could throw an exception if either `nodeU` or `nodeV` was not part of the graph. The revised implementation includes two checks using `checkNotNull` to ensure that `nodeU` and `nodeV` are not null, which can prevent NullPointerExceptions. Moreover, it adds logic to check if `nodeU` is part of the graph using `nodes().contains(nodeU)` and if there is a direct connection from `nodeU` to `nodeV` using `successors(nodeU).contains(nodeV)`. 

This change directly addresses the problem mentioned in the commit message: it prevents the method from throwing exceptions if the nodes are not present in the graph. Since the modifications include error-handling improvements and logical corrections to prevent exceptions, they align well with the described bug fix.

Therefore, the evaluation of the changes indicates that the commit fixes a bug as described. The conclusion is **Buggy**."
guava,9471.json,2ee7f9da69308c56d5af71267e8b797cedaf31ba,"@@ -1,7 +1,7 @@
   public boolean hasEdgeConnecting(EndpointPair<N> endpoints) {
     checkNotNull(endpoints);
     if (!isOrderingCompatible(endpoints)) {
       return false;
     }
-    return !edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty();
+    return hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV());
   }",Buggy,"AbstractNetwork: fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.

RELNOTES=Fix bug in AbstractNetwork.hasEdgeConnecting() causing it to throw if either endpoint was not in the graph.  Originally reported as GitHub issue #3721.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=282846559",Buggy,"The commit message indicates a bug fix in the `AbstractNetwork.hasEdgeConnecting()` method. The provided code diff changes the implementation from calling `!edgesConnecting(endpoints.nodeU(), endpoints.nodeV()).isEmpty()` to directly calling `hasEdgeConnecting(endpoints.nodeU(), endpoints.nodeV())`. 

This modification is likely addressing the issue described in the commit message. Previously, the method might have thrown exceptions when either of the endpoints was not present in the graph, as the `edgesConnecting` method might not handle such cases correctly. Instead, the updated line delegates the task to the `hasEdgeConnecting(nodeU, nodeV)` method, which presumably includes the necessary checks to safely determine the presence of an edge, even if the nodes are not in the graph.

The commit message and the diff are consistent in addressing a bug fix related to handling exceptions when endpoints are not in the graph. Therefore, the series of changes correspond well to the described fix.

Hence, the changes indicate a bug fix. The conclusion is **Buggy**."
guava,18902.json,314727b81a4f45210ab53ebd46b695150ea36f8c,"@@ -1,14 +1,11 @@
   public static long copy(Readable from, Appendable to) throws IOException {
     CharBuffer buf = CharBuffer.allocate(BUF_SIZE);
     long total = 0;
-    while (true) {
-      int r = from.read(buf);
-      if (r == -1) {
-        break;
-      }
+    while (from.read(buf) != -1) {
       buf.flip();
-      to.append(buf, 0, r);
-      total += r;
+      to.append(buf);
+      total += buf.remaining();
+      buf.clear();
     }
     return total;
   }",Buggy,"Fix a bug with CharStreams.copy not clearing its buffer after reading.

Guava issue 1061: http://code.google.com/p/guava-libraries/issues/detail?id=1061
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=31793143",Buggy,"The commit message indicates a bug fix in the `CharStreams.copy` method related to buffering issues. The original code does not clear its buffer after reading, potentially leading to incorrect data being copied upon subsequent reads. 

Examining the changes:
- The bug fix replaces an iterative read that manually tracks how much was read (`int r`) and marks the buffer for writing with a simplified loop.
- After each buffer append (`to.append(buf)`), the `buf.clear()` method is now called, ensuring the buffer is reset for the next read cycle.
- This update prevents potential data mishandling from residual data in the buffer not being properly cleared after each read into the buffer, an oversight corrected by adding `buf.clear()`.

Given the inherently problematic nature of not clearing the buffer, the modifications presented directly correlate with the stated bug fix. These changes address a method's logical flaw that would otherwise allow old data to persist in the buffer, which could lead to copying unintended repeated content.

Thus, the changes correspond to solving a specific bug as mentioned in the commit message. The commit indeed fixes the issue outlined with effective buffer management in the method `copy`, aligning with the operation of eradicating the persistence of residual data in subsequent operations.

The final assessment of this commit is **Buggy**."
guava,12172.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,3 +1,3 @@
     @Override public int size() {
-      return safeIntFactorial(inputList.size());
+      return IntMath.factorial(inputList.size());
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"The commit suggests replacing a custom factorial calculation method with IntMath.factorial. This could address a size calculation bug, as IntMath likely handles large integers more accurately. The mention of a size calculation error further suggests a potential bug in the original method. Therefore, replacing safeIntFactorial with a well-tested library method is consistent with the commit message's intention to fix a size-related bug. Given the context and the commit message, the change is aimed at correcting an arithmetic error due to incorrect handling of large values.

The use of IntMath.possibly indicates reliance on a tested library for better reliability or performance, hinting strongly that the previous implementation might've had inaccuracies.

Taking all these details into account, the refactor is likely to address pre-existing issues efficiently and align with the stated bug fix objective, leading the final assessment toward a bug-related correction.

The final decision is **Buggy**."
guava,12171.json,843f8d8fc37133e6870d5b2200838219a55eb83e,"@@ -1,32 +1,25 @@
-    static <E> int calculateSize(List<E> sortedInputList,
-        Comparator<? super E> comparator) {
-      try {
-        long permutations = 1;
-        int n = 1;
-        int r = 1;
-        for (; n < sortedInputList.size(); n++, r++) {
-          int comparison = comparator.compare(sortedInputList.get(n - 1),
-              sortedInputList.get(n));
-          // The list is sorted, this is an invariant.
-          checkState(comparison <= 0);
-          if (comparison < 0) {
-            // We move to the next non-repeated element.
-            permutations *= binomialCoefficient(n, r);
-            r = 0;
-
-            // Return early if we have more than MAX_VALUE permutations.
-            if (!isPositiveInt(permutations)) {
-              return Integer.MAX_VALUE;
-            }
+    private static <E> int calculateSize(
+        List<E> sortedInputList, Comparator<? super E> comparator) {
+      long permutations = 1;
+      int n = 1;
+      int r = 1;
+      while (n < sortedInputList.size()) {
+        int comparison = comparator.compare(
+            sortedInputList.get(n - 1), sortedInputList.get(n));
+        if (comparison < 0) {
+          // We move to the next non-repeated element.
+          permutations *= binomial(n, r);
+          r = 0;
+          if (!isPositiveInt(permutations)) {
+            return Integer.MAX_VALUE;
           }
         }
-        permutations *= binomialCoefficient(n, r);
-        if (!isPositiveInt(permutations)) {
-          return Integer.MAX_VALUE;
-        }
-        return (int) permutations;
-      } catch (IllegalArgumentException e) {
-        // Overflow. Fall back to max size.
+        n++;
+        r++;
+      }
+      permutations *= binomial(n, r);
+      if (!isPositiveInt(permutations)) {
         return Integer.MAX_VALUE;
       }
+      return (int) permutations;
     }",Buggy,"Nuke 85 lines of code in Collections2 by adopting common.math. Which apparently fixed a size calculation bug that wasn't found due to a bad test that thought C(34,14) > MAX_VALUE.  (?)

Also snuck a call to IntMath.pow() into cartesianProduct.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=28310475",Buggy,"This commit refactors `calculateSize` by adopting a different mathematical library, which is likely more robust and precise. The change intends to fix a size calculation issue caused by potential overflow or incorrect arithmetic, which might yield erroneous results. The commit message mentions correcting a bug related to size calculation that aligns with changes where `binomialCoefficient` is replaced by `binomial`, which could imply updates to utilize a more accurate or efficient math function. The change also corrects error checking logic, previously using a try-catch for handling `IllegalArgumentException`, now streamlined without catching exceptions, possibly due to using methods that better manage range and overflow issues inherently.

Given these updates focus on fixing the calculation logic and aim at addressing an erroneous size result due to mathematical overflow, the changes are consistent with the commit message indicating a bug fix. Therefore, this change is marked as **Buggy**."
guava,1857.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,9 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     valueType = (Class<V>) stream.readObject();
-    setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        WellBehavedMap.wrap(new EnumMap<V, K>(valueType)));
+    setDelegates(new EnumMap<K, V>(keyType), new EnumMap<V, K>(valueType));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",NotBuggy,"The commit message indicates the removal of `WellBehavedMap` as it is no longer needed due to a bug fix in Java 7. The change in the diff involves directly using `EnumMap` without wrapping it with `WellBehavedMap`. This suggests the `WellBehavedMap` was created to work around a past Java bug which no longer exists, thus simplifying the code. 

Removing `WellBehavedMap` suggests that the previous workaround for the Java bug is now unnecessary, aligning with the commit message about simplifying work around `EnumMap` usage.

Since the change isn't fixing a present bug but rather cleaning up outdated, workaround code, this is not directly addressing a bug introduced by the current codebase. Instead, it addresses a structural simplification due to the evolution of the Java environment. Therefore, the nature of this change is more about removing technical debt rather than a straightforward bug fix in typical product functionality.

Conclusion: **NotBuggy**."
guava,13304.json,a0f748e78516c8c80c945f6a4c4fae752d11bff5,"@@ -1,8 +1,7 @@
   private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {
     stream.defaultReadObject();
     keyType = (Class<K>) stream.readObject();
     setDelegates(
-        WellBehavedMap.wrap(new EnumMap<K, V>(keyType)),
-        new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
+        new EnumMap<K, V>(keyType), new HashMap<V, K>(keyType.getEnumConstants().length * 3 / 2));
     Serialization.populateMap(this, stream);
   }",NotBuggy,"Delete WellBehavedMap.

The bug it worked around was fixed in Java 7:
https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6312706

And I don't believe it was ever present under Android:
https://android.googlesource.com/platform/libcore/+/fdb2704414a9ed92394ada0d1395e4db86889465/luni/src/main/java/java/util/EnumMap.java#146

I haven't looked for the absolute first version ever of GWT, but the still very old GWT file doesn't have the bug, either.

Deleting this class will simplify some work I'm doing around our EnumMap usage for j2cl.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=233418344",NotBuggy,"The commit message explains the removal of `WellBehavedMap` due to a bug that was fixed in Java 7 and not present in Android or GWT. The code change reflects this by removing the `WellBehavedMap.wrap` call, thus directly using the `EnumMap`. The change is not fixing a new bug in the code but rather cleaning up legacy code due to external fixes. Therefore, the answer is **NotBuggy**."
guava,22245.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,7 +1,7 @@
   public static RegularImmutableAsList<Object> instantiate(SerializationStreamReader reader)
       throws SerializationException {
-    @SuppressWarnings(""unchecked"") // serialization is necessarily type unsafe
-    ImmutableCollection<Object> delegateCollection = (ImmutableCollection) reader.readObject();
-    ImmutableList<?> delegateList = (ImmutableList<?>) reader.readObject();
-    return new RegularImmutableAsList<Object>(delegateCollection, delegateList);
+    ArrayList<Object> elements = new ArrayList<Object>();
+    Collection_CustomFieldSerializerBase.deserialize(reader, elements);
+    ImmutableList<Object> delegate = ImmutableList.copyOf(elements);
+    return new RegularImmutableAsList<Object>(delegate, delegate);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"The commit message indicates a fix for serialization issues related to the `RegularImmutableAsList` class, specifically when the `ImmutableList` is included in GWT's serialization policy while `ImmutableSet` is not. The code changes replace the deserialization process of `ImmutableCollection` and `ImmutableList` with a new method of deserializing into an `ArrayList<Object>`, followed by creating an `ImmutableList<Object>` using `ImmutableList.copyOf(elements)`. This change makes the serialization process more robust and aligns with the commit message, which talks about fixing serialization errors. The problem seems related to type-safety issues during serialization, which the update addresses by changing the approach.

Thus, the modifications align with the commit message, indicating they indeed fix a bug related to serialization.

The answer is **Buggy**."
guava,22246.json,c5231cfd14de78cd50099e095798fba6726cbe9e,"@@ -1,5 +1,4 @@
   public static void serialize(SerializationStreamWriter writer, RegularImmutableAsList<?> instance)
       throws SerializationException {
-    writer.writeObject(instance.delegateCollection());
-    writer.writeObject(instance.delegateList());
+    Collection_CustomFieldSerializerBase.serialize(writer, instance);
   }",Buggy,"fixes serialization errors when serializing a
RegularImmutableAsList

the issue occur when ImmutableList is included in gwt's serialization
policy and ImmutableSet not and a list created by ImmutableSet.asList
needs to be serialized

Fixes #2614

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=137724815",Buggy,"The commit message explains that this change addresses serialization errors—specifically, issues with serializing a `RegularImmutableAsList`. The code change involves replacing two `writeObject` calls with a single call to `Collection_CustomFieldSerializerBase.serialize`.

Let's analyze this step by step:

1. **Commit Message vs. Code Diff:**
   - The commit message discusses fixing serialization errors related to `RegularImmutableAsList` caused by inconsistencies in GWT's serialization policy.
   - The code diff shows a change in the serialization method, where direct writing of delegate collection and list objects is replaced by a call to `Collection_CustomFieldSerializerBase.serialize`.

2. **Logic and Error Handling:**
   - The previous implementation attempted to serialize two components of the `RegularImmutableAsList` individually, which may have led to errors if `ImmutableSet` and other components weren't serializable as expected.
   - The new implementation suggests that `Collection_CustomFieldSerializerBase.serialize` abstracts these details, presumably handling the serialization correctly given the context.

3. **Problem and Solution Alignment:**
   - The message specifies a scenario involving `ImmutableList` and `ImmutableSet` that could lead to errors during serialization, which aligns with the changes made in the code.
   - The change to the use of `Collection_CustomFieldSerializerBase.serialize` suggests a consolidated or more generalized approach to ensure proper serialization, likely avoiding the issues mentioned.

Based on this analysis, the changes indeed seem to address and fix a bug in serialization logic as explained in the commit message. Thus, this diff reflects a bug fix and the conclusion is **Buggy**."
guava,11944.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,20 +1,26 @@
   private Type resolveTypeArgsForSubclass(Class<?> subclass) {
-    if (runtimeType instanceof Class) {
+    // If both runtimeType and subclass are not parameterized, return subclass
+    // If runtimeType is not parameterized but subclass is, process subclass as a parameterized type
+    // If runtimeType is a raw type (i.e. is a parameterized type specified as a Class<?>), we
+    // return subclass as a raw type
+    if (runtimeType instanceof Class
+        && ((subclass.getTypeParameters().length == 0)
+            || (getRawType().getTypeParameters().length != 0))) {
       // no resolution needed
       return subclass;
     }
     // class Base<A, B> {}
     // class Sub<X, Y> extends Base<X, Y> {}
     // Base<String, Integer>.subtype(Sub.class):
 
     // Sub<X, Y>.getSupertype(Base.class) => Base<X, Y>
     // => X=String, Y=Integer
     // => Sub<X, Y>=Sub<String, Integer>
     TypeToken<?> genericSubtype = toGenericType(subclass);
     @SuppressWarnings({""rawtypes"", ""unchecked""}) // subclass isn't <? extends T>
     Type supertypeWithArgsFromSubtype =
         genericSubtype.getSupertype((Class) getRawType()).runtimeType;
     return new TypeResolver()
         .where(supertypeWithArgsFromSubtype, runtimeType)
         .resolveType(genericSubtype.runtimeType);
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message describes a bug fix in handling generic type resolution, detailing an inconsistency where a non-generic call results in a raw type rather than a parameterized one. The change primarily revolves around the method `resolveTypeArgsForSubclass`.

Here's the breakdown of the diff:

1. **Conditions for Returning the Subclass:**
   - The added conditions check the parameterization of `runtimeType` and `subclass`.
   - If both `runtimeType` and `subclass` aren't parameterized, it simply returns `subclass`, which is straightforward and maintains existing behavior.

2. **Handling Raw Types:**
   - The added comments and logic explicitly address cases where `runtimeType` is a raw type but `subclass` isn't. This indicates careful handling of parameterized types versus raw types.

3. **Additional Explanatory Comments:**
   - The comments provide context on how type resolution is managed, particularly when dealing with superclasses and subclasses of generic types.

The modification directly aligns with the described problem in the commit message. It addresses the incorrect handling of generic subtypes, which would previously result in incorrect raw types. The detailed analysis of `runtimeType` and `subclass` and the added handling ensure that parameterization is correctly resolved, thus fixing the described bug.

Thus, the changes indicate a bug fix for the issue with generic type handling described in the commit message. Therefore, the conclusion is **Buggy**."
guava,11937.json,946ebccece2c45cce528cdc6d2db1aa31d2cfa73,"@@ -1,24 +1,26 @@
   static <T> TypeToken<? extends T> toGenericType(Class<T> cls) {
     if (cls.isArray()) {
       Type arrayOfGenericType =
           Types.newArrayType(
               // If we are passed with int[].class, don't turn it to GenericArrayType
               toGenericType(cls.getComponentType()).runtimeType);
       @SuppressWarnings(""unchecked"") // array is covariant
       TypeToken<? extends T> result = (TypeToken<? extends T>) of(arrayOfGenericType);
       return result;
     }
     TypeVariable<Class<T>>[] typeParams = cls.getTypeParameters();
     Type ownerType =
-        cls.isMemberClass() ? toGenericType(cls.getEnclosingClass()).runtimeType : null;
+        cls.isMemberClass() && !Modifier.isStatic(cls.getModifiers())
+            ? toGenericType(cls.getEnclosingClass()).runtimeType
+            : null;
 
-    if ((typeParams.length > 0) || (ownerType != cls.getEnclosingClass())) {
+    if ((typeParams.length > 0) || ((ownerType != null) && ownerType != cls.getEnclosingClass())) {
       @SuppressWarnings(""unchecked"") // Like, it's Iterable<T> for Iterable.class
       TypeToken<? extends T> type =
           (TypeToken<? extends T>)
               of(Types.newParameterizedTypeWithOwner(ownerType, cls, typeParams));
       return type;
     } else {
       return of(cls);
     }
   }",Buggy,"Currently, SomeClass<?>.getSubType(SubClass<?, OtherT>) returns SubClass<?, OtherT>.
However, if we do NotGeneric.getSubtype(SubClass<OtherT>), we get SubClass as a raw type instead of
SubClass<OtherT>.

If we fix that bug, then as it turns out we weren't generating proper owners for static classes, so fix that too
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=119755147",Buggy,"The commit message describes a bug related to how subtypes and owners for static classes are generated. The code changes address these issues:

1. **Generating Proper Owners for Static Classes**: 
   - The modified code adds a condition to check if a class is static before determining its owner type. Specifically, it checks `!Modifier.isStatic(cls.getModifiers())` to ensure that only non-static member classes have an owner type. This prevents static classes from having inappropriate owner types, which aligns with the second part of the commit message about generating proper owners for static classes.

2. **Subtype Handling**:
   - The condition in the `if` statement is modified to consider whether an ownerType is different from `cls.getEnclosingClass()`. This suggests an improved handling of the parameterized types that includes their owners, which could be related to the first part of the commit message regarding subtypes.

Given these code adjustments, the modifications appear to address the bugs mentioned in the commit message. These changes fix specific logical handling of member classes and subtypes, aligning with the stated intention to correct subtype and owner generation issues.

**Conclusion**: The changes made in this commit align with the description in the commit message and effectively address a bug related to subtype handling and owner determination for static classes. Therefore, the overall assessment is **Buggy**."
guava,15642.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,31 +1,30 @@
     int crossOverUp(int index, E x) {
       if (index == 0) {
         queue[0] = x;
         return 0;
       }
       int parentIndex = getParentIndex(index);
       E parentElement = elementData(parentIndex);
       if (parentIndex != 0) {
-        // This is a guard for the case of the childless uncle. No checks are
-        // performed for childlessness (even if we could check it), but since
-        // it is the minimum sibling that is moved from ""max"" to ""min"" half
-        // of the heap, and only if x is larger, and this is at the bottom
-        // edge of the heap, the heap invariant is still preserved.
+        // This is a guard for the case of the childless uncle.
+        // Since the end of the array is actually the middle of the heap,
+        // a smaller childless uncle can become a child of x when we
+        // bubble up alternate levels, violating the invariant.
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
-        if (uncleIndex != parentIndex) {
+        if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, parentElement) < 0) {
             parentIndex = uncleIndex;
             parentElement = uncleElement;
           }
         }
       }
       if (ordering.compare(parentElement, x) < 0) {
         queue[index] = parentElement;
         queue[parentIndex] = x;
         return parentIndex;
       }
       queue[index] = x;
       return index;
     }",Buggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message refers to fixing a ""childless uncle"" bug that causes heap corruption. The provided diff modifies the condition within the `crossOverUp` method. Here's the analysis of the changes:

1. **Understanding the Original Bug:**
   - The ""childless uncle"" bug is explained in the comments. It seems related to incorrect handling when an element is moving within a binary heap (queue), potentially leading to heap property violations. 
   - The term ""childless uncle"" suggests the issue occurs when navigating the heap structure, particularly at points where a node (uncle) without children is involved.

2. **Code Analysis:**
   - **Original Code:**
     - The comment hints at a guard for handling cases involving nodes on the ""bottom edge"" of the heap, possibly where an uncle node doesn’t have children but indirectly lets a smaller node violate heap properties.
   - **Modified Code:**
     - The modification explicitly checks if the uncle node has no left child by using `getLeftChildIndex(uncleIndex) >= size`.
     - This change ensures that only nodes truly without children are considered, thus preserving heap invariants when rearranging elements.

3. **Reasoning about the Bug Fix:**
   - The added check ensures a node is considered a childless uncle only if it actually has no children available in the array (`>= size`). The previous logic overlooked scenarios where a node incorrectly appeared as a valid candidate based on structure rather than actual childless status.
   - This consideration prevents incorrectly designating an uncle node during heap modifications, avoiding corruption by maintaining proper parent-child relationships.

4. **Conclusion:**
   - The modification explicitly addresses heap consistency during node swaps (bubble up), fixing a specific bug case. The aim is to correct structural invariants during element comparison and placement in the array-based heap.

Given this detailed breakdown, the changes indeed align with the commit message's intent to fix a specific bug causing heap corruption. Therefore, this change can be concluded as **Buggy**."
guava,15629.json,dd96b4930181fc70bd7c90beded3a272f251db35,"@@ -1,32 +1,28 @@
   @VisibleForTesting MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
+    E actualLastElement = elementData(size);
+    int lastElementAt = heapForIndex(size)
+        .getCorrectLastElement(actualLastElement);
     E toTrickle = elementData(size);
     queue[size] = null;
-    Heap heap = heapForIndex(index);
-    // We consider elementData(index) a ""hole"", and we want to fill it
-    // with the last element of the heap, toTrickle.
-    // Since the last element of the heap is from the bottom level, we
-    // optimistically fill index position with elements from lower levels,
-    // moving the hole down. In most cases this reduces the number of
-    // comparisons with toTrickle, but in some cases we will need to bubble it
-    // all the way up again.
-    int vacated = heap.fillHoleAt(index);
-    // Try to see if toTrickle can be bubbled up min levels.
-    int bubbledTo = heap.bubbleUpAlternatingLevels(vacated, toTrickle);
-    if (bubbledTo == vacated) {
-      // Could not bubble toTrickle up min levels, try moving
-      // it from min level to max level (or max to min level) and bubble up
-      // there.
-      return heap.tryCrossOverAndBubbleUp(index, vacated, toTrickle);
-    } else {
-      return (bubbledTo < index)
-          ? new MoveDesc<E>(toTrickle, elementData(index))
-          : null;
+    MoveDesc<E> changes = fillHole(index, toTrickle);
+    if (lastElementAt < index) {
+      // Last element is moved to before index, swapped with trickled element.
+      if (changes == null) {
+        // The trickled element is still after index.
+        return new MoveDesc<E>(actualLastElement, toTrickle);
+      } else {
+        // The trickled element is back before index, but the replaced element
+        // has now been moved after index.
+        return new MoveDesc<E>(actualLastElement, changes.replaced);
+      }
     }
+    // Trickled element was after index to begin with, no adjustment needed.
+    return changes;
   }",NotBuggy,"Fix a second case of ""childless uncle"" bug causing heap corruption.

Revision created by MOE tool push_codebase.
MOE_MIGRATION=1027


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@275 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The commit message indicates a fix for a specific bug involving ""childless uncle,"" which suggests an issue with heap structure manipulation that led to heap corruption. Analyzing the code diff:

1. **Variable Changes and Logic Adjustments**:
   - The original `removeAt` method included logic to manage heap operations, filling a hole left by a removed element and trying to bubble up a trickled element within the heap.
   - The revised method introduces a variable `actualLastElement` and a new logic to handle the movement of the last element correctly, suggesting addressing an issue in element removal and movement logic.

2. **Element Operations**:
   - The change involves extracting `actualLastElement` and using `lastElementAt` to determine its correct moved position. The changes made include more precise handling of element positions and swaps within the heap, fixing conditions where heap corruption might occur by incorrectly ordering or swapping elements.

3. **Conditional Checks**:
   - The logic for bubbling up and cross-over attempts in the heap has been replaced by a simplified handling using `fillHole` and checking positions relative to the index.
   - This new implementation gives more control and makes explicit checks against positions to avoid incorrect heaps caused by mishandling elements that could lead to the ""childless uncle"" scenario.

The modifications align with fixing a bug. The revised logic appears to prevent incorrect operations that would result in heap corruption. The commit's content indicates a fix for a specific logical error (the ""childless uncle"" bug), making this commit a clear example of resolving bugs in data structure manipulation.

Thus, the changes are consistent with the commit message and indicate a bug fix. The assessment concludes with **Buggy**."
guava,12247.json,0007cb257b593400097f37cf5fa9c8d5fb598c62,"@@ -1,7 +1,9 @@
     SetBuilderImpl<E> review() {
       int targetTableSize = chooseTableSize(distinct);
       if (targetTableSize * 2 < hashTable.length) {
         hashTable = rebuildHashTable(targetTableSize, dedupedElements, distinct);
+        maxRunBeforeFallback = maxRunBeforeFallback(targetTableSize);
+        expandTableThreshold = (int) (DESIRED_LOAD_FACTOR * targetTableSize);
       }
       return hashFloodingDetected(hashTable) ? new JdkBackedSetBuilderImpl<E>(this) : this;
     }",Buggy,"Fix #3570 by resetting expandTableThreshold and
maxRunBeforeFallback after resizing the hashTable.

Fixes #3571

[]

RELNOTES=Fixed a bug in `ImmutableSet.Builder` that could lead to infinite loops when building multiple sets from the same builder.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=264648412",Buggy,"The commit message indicates a fix for a bug related to `ImmutableSet.Builder` that could cause infinite loops. By examining the diff, we see that after resizing the `hashTable`, it now properly resets `maxRunBeforeFallback` and `expandTableThreshold`. Resetting these variables to their appropriate values after a hash table resize prevents incorrect hash table behavior, which could lead to issues like infinite loops.

This change clearly addresses an error in the logic where critical thresholds weren't reset, potentially leading to inappropriate fallback operations or unnecessary set builder switches, aligning perfectly with the bug description in the commit message.

Therefore, this commit is indeed addressing a bug, and I conclude with **Buggy**."
guava,11603.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,7 +1,6 @@
   public Stopwatch start() {
-    checkState(!isRunning,
-        ""This stopwatch is already running; it cannot be started more than once."");
+    checkState(!isRunning, ""This stopwatch is already running."");
     isRunning = true;
     startTick = ticker.read();
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",NotBuggy,"The commit message specifies documentation fixes and internal cleanup. The code change involves modifying an exception message to remove potentially misleading information about starting a stopwatch more than once, aligning with the commit message goal of refining internal messages without functional code. This update doesn't imply fixing a bug but instead improves clarity and correctness of the documentation or messages. Overall, this aligns with the nature of a documentation-related change rather than a bug fix.

The conclusion is **NotBuggy**."
guava,11604.json,9129e5e50020f84c7a859726b42ce211d2e37e5f,"@@ -1,8 +1,7 @@
   public Stopwatch stop() {
     long tick = ticker.read();
-    checkState(isRunning,
-        ""This stopwatch is already stopped; it cannot be stopped more than once."");
+    checkState(isRunning, ""This stopwatch is already stopped."");
     isRunning = false;
     elapsedNanos += tick - startTick;
     return this;
   }",NotBuggy,"Miscellaneous documentation fixes and internal cleanups.
- Remove potentially misleading ""it cannot be started/stopped more than once"" from Stopwatch exception messages.
- Fix TreeTraverser ASCII art.
- Remove inaccurate @GwtCompatible annotation from FeatureUtil.
- Fix mistaken duplicate words (""the the,"" etc.).
- Strip *all* of benchmark running instructions from CharMatcherBenchmark, and preemptively strip instructions from internal escapers benchmarks.
- Remove test <echo> from ant section of pom.xml.
- Add @Nullable to StandardBaseEncoding's other constructor's paddingChar parameter.
- Remove stale comment from Futures. (UninterruptibleFuture is long dead.)
- Update name of Truth GWT module.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=42036409",NotBuggy,"The commit message indicates miscellaneous documentation fixes and internal cleanups, which aligns with the provided code diff. The change modifies an exception message in the `Stopwatch` class to remove potentially misleading information. Specifically, it removes the phrase ""it cannot be stopped more than once,"" focusing solely on stating that the stopwatch is already stopped. This change doesn't correct any bugs related to logic or functionality but instead clarifies documentation within the exception message.

Since the change aligns well with the commit message focusing on documentation and cleanup rather than functionality modification or correction of a logical error, there is no indication of a bug fix.

**NotBuggy**"
guava,81.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",NotBuggy,"The commit message outlines a language change to replace ""whitelist"" and ""blacklist"" with less loaded terms, without indicating functional alterations. The diff confirms this language adjustment, swapping ""whitelisted"" with ""allowed"" in comments. There's no bug fix. The answer is **NotBuggy**."
guava,10879.json,0cd4e9faa1360da4a343f84cb275d6eda0c5e732,"@@ -1,19 +1,19 @@
   private static @Nullable Object getJLA() {
     try {
       /*
        * We load sun.misc.* classes using reflection since Android doesn't support these classes and
        * would result in compilation failure if we directly refer to these classes.
        */
       Class<?> sharedSecrets = Class.forName(SHARED_SECRETS_CLASSNAME, false, null);
       Method langAccess = sharedSecrets.getMethod(""getJavaLangAccess"");
       return langAccess.invoke(null);
     } catch (ThreadDeath death) {
       throw death;
     } catch (Throwable t) {
       /*
-       * This is not one of AppEngine's whitelisted classes, so even in Sun JDKs, this can fail with
+       * This is not one of AppEngine's allowed classes, so even in Sun JDKs, this can fail with
        * a NoClassDefFoundError. Other apps might deny access to sun.misc packages.
        */
       return null;
     }
   }",NotBuggy,"Fix nonpublic identifiers and documentation that use ""whitelist"" or ""blacklist"" to use less problematic terms.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=199834981",NotBuggy,"The commit message mentions replacing terms like ""whitelist"" with less problematic ones, which aligns with the code change. The diff shows a documentation comment modification, changing ""whitelisted"" to ""allowed"". There are no logical changes, corrections, or error-handling improvements suggesting a bug fix. Given the purpose of moving towards inclusive language without altering behavior or fixing errors, this update does not indicate a bug fix.

Consequently, based on the analysis, the conclusion is **NotBuggy**."
guava,21458.json,eb3a9f447715b05c18179bf6313dfd28851bb56e,"@@ -1,18 +1,29 @@
   public void addListener(Runnable listener, Executor executor) {
     checkNotNull(listener, ""Runnable was null."");
     checkNotNull(executor, ""Executor was null."");
-    Listener oldHead = listeners;
-    if (oldHead != Listener.TOMBSTONE) {
-      Listener newNode = new Listener(listener, executor);
-      do {
-        newNode.next = oldHead;
-        if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
-          return;
-        }
-        oldHead = listeners; // re-read
-      } while (oldHead != Listener.TOMBSTONE);
+    // Checking isDone and listeners != TOMBSTONE may seem redundant, but our contract for
+    // addListener says that listeners execute 'immediate' if the future isDone(). However, our
+    // protocol for completing a future is to assign the value field (which sets isDone to true) and
+    // then to release waiters, followed by executing afterDone(), followed by releasing listeners.
+    // That means that it is possible to observe that the future isDone and that your listeners
+    // don't execute 'immediately'.  By checking isDone here we avoid that.
+    // A corollary to all that is that we don't need to check isDone inside the loop because if we
+    // get into the loop we know that we weren't done when we entered and therefore we aren't under
+    // an obligation to execute 'immediately'.
+    if (!isDone()) {
+      Listener oldHead = listeners;
+      if (oldHead != Listener.TOMBSTONE) {
+        Listener newNode = new Listener(listener, executor);
+        do {
+          newNode.next = oldHead;
+          if (ATOMIC_HELPER.casListeners(this, oldHead, newNode)) {
+            return;
+          }
+          oldHead = listeners; // re-read
+        } while (oldHead != Listener.TOMBSTONE);
+      }
     }
     // If we get here then the Listener TOMBSTONE was set, which means the future is done, call
     // the listener.
     executeListener(listener, executor);
   }",Buggy,"Fix a bug in AbstractFuture.addListener where we failed to follow the contract precisely.

ListenableFuture.addListener says that listeners should execute 'immediately' when the future is done.  However, because the old version of addListener only checked the listeners field, it is possible to observe that the future is done and that your listener would not execute immediately.  This cl fixes that by adding an explicit check for isDone.

RELNOTES=Close a short race where you could observe that an AbstractFuture was done, but that listeners wouldn't execute immediately.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=212281545",Buggy,"The commit message indicates that the addListener method in AbstractFuture did not correctly follow the contract that listeners should execute immediately when the future is marked as done. 

The diff addresses this by:
- Adding a check for `isDone()` at the beginning of the method. This ensures that if the future is already done when `addListener` is called, the listener will be executed immediately. Previously, this may not have happened because the method only checked the `listeners` field for the TOMBSTONE value.
  
The comments within the code clarify the purpose of the changes, explaining that checking `isDone()` prevents the issue where the future is done, but the listeners are not executed immediately.

Since this change directly addresses a race condition as described in the commit message, and it alters the logic to correctly adhere to the contract, it aligns with the message's stated intention of fixing a bug.

Thus, the answer is **Buggy**."
guava,22829.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,4 +1,4 @@
     public void invalidate(Object key) {
-      key = checkNotNull(key);
+      checkNotNull(key);
       localCache.remove(key);
     }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",NotBuggy,"The code change deals with maintaining the null check on `key` but modifies how it's handled. Originally, `key` was reassigned after `checkNotNull(key)`, but now the check is a standalone call before using `key`. This change aligns with handling situations where `key` might be null, ensuring a `NullPointerException` is properly thrown if `key` is indeed null.

However, this doesn't indicate a bug fix in the traditional sense of resolving an erroneous behavior in production code. This is more about improving code quality by adhering to best practices and establishing contract expectations (i.e., the key should not be null). As the commit message specifically states that these are ""fix Error Prone errors,"" it suggests the change is meant to improve code practices flagged by the Error Prone tool, rather than fixing a functional bug.

Thus, the review concludes with **NotBuggy**."
guava,22803.json,0664d966535b95328b684b471ac05c481b55f297,"@@ -1,19 +1,19 @@
   public V get(Object key) {
-    key = checkNotNull(key);
+    checkNotNull(key);
     Timestamped<V> value = cachingHashMap.get(key);
 
     if (value == null) {
       statsCounter.recordMisses(1);
       return null;
     } else if (!isExpired(value)) {
       statsCounter.recordHits(1);
       value.updateTimestamp();
       return value.getValue();
     } else {
       statsCounter.recordEviction();
       statsCounter.recordMisses(1);
       alertListenerIfPresent(key, value.getValue(), RemovalCause.EXPIRED);
       cachingHashMap.remove(key);
       return null;
     }
   }",NotBuggy,"Fix Error Prone errors in GWT code.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=115110183",NotBuggy,"The change in the diff is from reassigning `key` to checking `key` with `checkNotNull(key)`, eliminating the reassignment. This aligns with fixing an Error Prone warning where variables are reassigned. This does not relate to fixing a logical or runtime bug, but rather to improving code readability and correctness by adhering to best practices highlighted by tools like Error Prone.

The commit message specifies that the change is aimed at fixing errors related to Error Prone checks in GWT code. Error Prone is a static analysis tool that catches common Java mistakes, so the change is about improving the code quality by addressing these flagged issues rather than fixing a runtime bug.

Thus, given that this caters to code quality improvements rather than resolving functionality bugs, the appropriate conclusion is **NotBuggy**."
guava,21488.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,12 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
       V value = getDone(this);
-      builder.append(""SUCCESS, result=["").append(value).append(""]"");
+      builder.append(""SUCCESS, result=["").append(userObjectToString(value)).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED""); // shouldn't be reachable
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message mentions a fix for stack overflow issues in the `AbstractFuture.toString()` method. The code change modifies how a value is appended to a `StringBuilder` from directly calling `value.toString()` to using a `userObjectToString(value)` method. This change indicates an attempt to handle recursive or problematic `toString` calls more safely, aiming to mitigate stack overflow issues caused by self-referential structures or values with problematic `toString` implementations.

This aligns with the commit message's description of addressing stack overflow scenarios in a manner similar to what `AbstractCollection.toString` does. The modified approach suggests handling the value more carefully, consistent with preventing stack overflows from simple recursive cases.

Given the relevance and details in the change and commit message, it signifies a bug fix aimed at improving robustness against stack overflows.

**Buggy**"
guava,10179.json,c0c117a04ab47f2fb380311efec97a7d9851cd6e,"@@ -1,11 +1,11 @@
   protected String pendingToString() {
     Object localValue = value;
     if (localValue instanceof SetFuture) {
-      return ""setFuture=["" + ((SetFuture) localValue).future + ""]"";
+      return ""setFuture=["" + userObjectToString(((SetFuture) localValue).future) + ""]"";
     } else if (this instanceof ScheduledFuture) {
       return ""remaining delay=[""
           + ((ScheduledFuture) this).getDelay(TimeUnit.MILLISECONDS)
           + "" ms]"";
     }
     return null;
   }",Buggy,"Fix some simple cases of stack overflow in AbstractFuture.toString()

This is a partial fix for people doing weird things and can easily be circumvented by people adding some indirection, but this is also what AbstractCollection.toString does so it seems like there is a decent precedent for a partial solution to this problem.

It doesn't appear to be possible to implement a full fix without resorting to things like threadlocals or examining the callstack, and this problem doesn't seem important enough to justify a solution like that.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=181325137",Buggy,"The commit message describes fixing stack overflow cases in `AbstractFuture.toString()`. The diff modifies `pendingToString()` by calling `userObjectToString` for `SetFuture` instances instead of directly invoking `.toString()` on the future object. 

This change suggests a concern for avoiding potential recursive `.toString()` calls, which could lead to a stack overflow, especially if there's cyclic dependency or improper handling elsewhere. Using `userObjectToString` likely introduces safer handling or checks to mitigate these risks.

This aligns with the commit message's implication of addressing stack overflow issues. The message also details limitations in achieving a complete solution, making the approach similar to `AbstractCollection.toString`.

Given these details, the change indicates a bug fix related to stack overflow problems in the `.toString()` method, and thus I conclude with **Buggy**."
guava,10881.json,c95fc106ff08d0652b5d661cab9a73f38697eccc,"@@ -1,3 +1,12 @@
   private static Method getSizeMethod() {
-    return getJlaMethod(""getStackTraceDepth"", Throwable.class);
+    try {
+      Method getStackTraceDepth = getJlaMethod(""getStackTraceDepth"", Throwable.class);
+      if (getStackTraceDepth == null) {
+        return null;
+      }
+      getStackTraceDepth.invoke(getJLA(), new Throwable());
+      return getStackTraceDepth;
+    } catch (UnsupportedOperationException | IllegalAccessException | InvocationTargetException e) {
+      return null;
+    }
   }",Buggy,"Throwables#lazyStackTrace unsupported on IBM JDK 6

Fix addresses a problem occurred on IBM JDK 6. To ensure
that we can call underlying method, Throwables#getSizeMethod,
to get  stack size, first a smoke test  with dummy exception.
Once method executed with success, it is safe to return it for
further usage.

Fixes #2947

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=169910803",Buggy,"The provided diff makes adjustments to the `getSizeMethod` function, which aims to make it more robust against potential issues on IBM JDK 6. Here's the analysis of the changes:

1. **Original Functionality**: Previously, `getSizeMethod` simply attempted to retrieve a method using `getJlaMethod(""getStackTraceDepth"", Throwable.class)` and returned it directly. There was no handling for any potential failures.

2. **Current Functionality**:
   - The updated method first attempts to retrieve the `getStackTraceDepth` method.
   - A check exists to see if `getStackTraceDepth` is non-null. If null, it directly returns null.
   - Then, a ""smoke test"" is performed: it attempts to invoke the `getStackTraceDepth` method using a new `Throwable` to ensure it can be called successfully.
   - If any exceptions (`UnsupportedOperationException`, `IllegalAccessException`, `InvocationTargetException`) are thrown during this process, the function returns null, effectively indicating that the method isn't usable.

3. **Reason for Changes**: The commit message suggests that this fix addresses a specific issue observed on IBM JDK 6. The method may exist but can be unsupported in some environments, or it might be inaccessible or cause unexpected behavior if called improperly.

4. **Conclusion**:
   - The changes involve introducing exception handling and validation via a ""dummy"" invocation. This modification enhances robustness and appears directly geared towards addressing a bug where `getSizeMethod()` could incorrectly assume the usability of the method it retrieves.
   - Additionally, the use of multiple catch blocks ensures that any problematic attempt to invoke `getStackTraceDepth` is gracefully handled, indicating an improvement in error management.

Overall, the alterations to this method are indicative of a bug fix. The additional logic checks prevent the possible misbehavior from premature invocation of an unsupported or inaccessible method on certain Java Virtual Machine (JVM) implementations, specifically IBM JDK 6. Therefore, the changes align with the commit message, suggesting this is indeed a bug fix.

**Buggy**"
guava,11748.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,7 +1,7 @@
-    static Object forLookup(Type t) {
+    static TypeVariableKey forLookup(Type t) {
       if (t instanceof TypeVariable) {
         return new TypeVariableKey((TypeVariable<?>) t);
       } else {
         return null;
       }
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",NotBuggy,"The diff highlights a change in the return type of the `forLookup` method from `Object` to `TypeVariableKey`. This change is in line with the simplified adjustment mentioned in the commit message, aiming to enhance type clarity and correctness.

Here's my reasoning:

1. **Commit Message Analysis**: 
   - The commit message outlines multiple changes, but the relevant point here is about declaring a more specific return type instead of `Object`, which aligns with the change in the diff.
   - It mentions correcting calls with a wrong type, but does not suggest that this specific change was about bug correction.

2. **Code Changes**: 
   - The change from `Object` to `TypeVariableKey` suggests an improvement in how types are handled in the code by restricting the return type to better reflect the actual usage. It doesn't inherently indicate a bug fix but rather a refinement in the code for clarity and correctness.
   - The method’s functionality remains unchanged; it's a signature change that helps with type safety and clarity.

3. **Context and Intent**:
   - This adjustment implies an effort to simplify and make the code more maintainable and less error-prone by leveraging Java generics more effectively.
   - The commit message indicates this is primarily a simplification rather than a bug fix. It aligns with the note in the diff about the simplification and preference for specificity in return types.

Based on the above points, the commit primarily addresses code simplification and does not suggest a bug fix directly related to the adjusted code portion. Thus, the final conclusion is **NotBuggy**."
guava,11957.json,4362a4529306ea43e177fae2d0457e794dd77cd6,"@@ -1,24 +1,24 @@
     private int collectTypes(K type, Map<? super K, Integer> map) {
-      Integer existing = map.get(this);
+      Integer existing = map.get(type);
       if (existing != null) {
         // short circuit: if set contains type it already contains its supertypes
         return existing;
       }
       int aboveMe = getRawType(type).isInterface()
           ? 1 // interfaces should be listed before Object
           : 0;
       for (K interfaceType : getInterfaces(type)) {
         aboveMe = Math.max(aboveMe, collectTypes(interfaceType, map));
       }
       K superclass = getSuperclass(type);
       if (superclass != null) {
         aboveMe = Math.max(aboveMe, collectTypes(superclass, map));
       }
       /*
        * TODO(benyu): should we include Object for interface?
        * Also, CharSequence[] and Object[] for String[]?
        *
        */
       map.put(type, aboveMe + 1);
       return aboveMe + 1;
     }",NotBuggy,"Fix calls to contains(Object) and get(Object) that pass a value of apparently the wrong type.
Tighten up some generics in other cases to make the type, which was correct after all, look more correct.

BUGS:

AbstractBiMapTester:
- Fix inv.entrySet().contains(...) check, which was using the forward entry instead of the reverse.
- Fix getMap().get(v) call to be an inv.get(v) call.
- Use |reversed| instead of |entry| consistently for clarity.

TypeToken:
- Call map.get(K) instead of map.get(TypeCollector).
(Presumably this was just an optimization and not necessary for correctness?)

SIMPLIFICATIONS:

TypeResolver:
- forLookup always returns a (nullable) TypeVariableKey. Declare that return type instead of plain Object.
(benyu@: I feel like we may have talked about this, with your expressing a preference for the Object type. But I can find no record of the discussion, so I could be making that up. If you do prefer Object, I won't push for the change.)
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=108994208",Buggy,"The commit message indicates bug fixes in the usage of map methods like `contains(Object)` and `get(Object)` with potentially incorrect object types. The diff shows a specific change in the `collectTypes` method where `map.get(this)` is updated to `map.get(type)`. This change rectifies the method call to use the correct key type (`K`) for the `map.get()` operation. 

This aligns with the commit message, as using `this` was presumably an incorrect usage that might lead to bugs due to mismatched key types in the map. Correcting the method call to use the parameter `type` instead of `this` ensures that the map interactions are consistent with the intended logic and data types.

The mention of optimizing and fixing type-related issues in the commit message corresponds to this code change. As the original use of `this` could potentially lead to logical errors or mismatches in map key lookups, this adjustment does indeed reflect a bug fix.

Thus, the analysis of this change and its alignment with the commit message concludes with **Buggy**."
guava,10943.json,3fbaf56f390db1a7c442ceaeab2c3a8bc1c8fd79,"@@ -1,29 +1,43 @@
     @Override protected String computeNext() {
       while (offset != -1) {
         int start = offset;
         int end;
 
         int separatorPosition = separatorStart(offset);
         if (separatorPosition == -1) {
           end = toSplit.length();
           offset = -1;
         } else {
           end = separatorPosition;
           offset = separatorEnd(separatorPosition);
         }
 
         while (start < end && trimmer.matches(toSplit.charAt(start))) {
           start++;
         }
         while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
           end--;
         }
 
         if (omitEmptyStrings && start == end) {
           continue;
         }
 
+        if (limit == 1) {
+          // The limit has been reached, return the rest of the string as the
+          // final item.  This is tested after empty string removal so that
+          // empty strings do not count towards the limit.
+          end = toSplit.length();
+          offset = -1;
+          // Since we may have changed the end, we need to trim it again.
+          while (end > start && trimmer.matches(toSplit.charAt(end - 1))) {
+            end--;
+          }
+        } else {
+          limit--;
+        }
+
         return toSplit.subSequence(start, end).toString();
       }
       return endOfData();
     }",Buggy,"Fixed bug/532.
Add warnings to README.

These should be the last changes for Guava release 08.


Revision created by MOE tool push_codebase.
MOE_MIGRATION=


git-svn-id: https://guava-libraries.googlecode.com/svn/trunk@160 8138a162-5c33-11de-8abc-d1c337b90d21",Buggy,"The described commit message references a bug fix but directly addresses adding warnings to the README, not detailing specifics about coding adjustments. In the provided diff, the code update introduces a change to handle a limit condition in a method, guaranteeing that when a limit on splits is specified, the remaining string is correctly treated as one piece.

Key changes:
- Added checks for the limit, altering behavior when the limit is reached by appending the remainder of the string as a single element.
- Adjusted trimming logic to ensure it's correctly applied even after the limit logic is enforced.
- This change ensures correct string splitting respecting specified limits and modifies operation sequences properly, suggesting it resolves a relevant functionality issue.

Additionally, the update aligns with handling edge cases such as the presence of empty strings not counting toward the limit, which could be a logical error previously unhandled.

Considering the diff and relating it back to the commit, this code change appears a correction of specific functionality in a method that might address Bug 532. Therefore, the answer should be **Buggy**."
guava,15629.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,27 +1,34 @@
   MoveDesc<E> removeAt(int index) {
     checkPositionIndex(index, size);
     modCount++;
     size--;
     if (size == index) {
       queue[size] = null;
       return null;
     }
     E actualLastElement = elementData(size);
-    int lastElementAt = heapForIndex(size).getCorrectLastElement(actualLastElement);
+    int lastElementAt = heapForIndex(size).swapWithConceptuallyLastElement(actualLastElement);
+    if (lastElementAt == index) {
+      // 'actualLastElement' is now at 'lastElementAt', and the element that was at 'lastElementAt'
+      // is now at the end of queue. If that's the element we wanted to remove in the first place,
+      // don't try to (incorrectly) trickle it. Instead, just delete it and we're done.
+      queue[size] = null;
+      return null;
+    }
     E toTrickle = elementData(size);
     queue[size] = null;
     MoveDesc<E> changes = fillHole(index, toTrickle);
     if (lastElementAt < index) {
       // Last element is moved to before index, swapped with trickled element.
       if (changes == null) {
         // The trickled element is still after index.
         return new MoveDesc<E>(actualLastElement, toTrickle);
       } else {
         // The trickled element is back before index, but the replaced element
         // has now been moved after index.
         return new MoveDesc<E>(actualLastElement, changes.replaced);
       }
     }
     // Trickled element was after index to begin with, no adjustment needed.
     return changes;
   }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message indicates two bugs in the `MinMaxPriorityQueue` class, primarily addressing issues with `removeAt(int)` potentially removing the wrong element and an iteration bug. The diff provided addresses the first bug by modifying the logic in the `removeAt(int index)` method.

Here's a step-by-step breakdown of the changes:

1. **Initial Validations and Setup**: The initial code performs boundary checks and decreases the queue size, which remains unchanged.

2. **Handling Removed Element**: The pre-existing logic checks if the removed element is at the end of the queue. If true, it clears that position and returns early, which remains unchanged.

3. **Swapping and Correcting Element Placement**: 
   - The original code used `getCorrectLastElement`, which is replaced with `swapWithConceptuallyLastElement`, a more precise method name indicating a swap operation.
   - A new condition is introduced: if `lastElementAt` equals the index of the element being removed, it now directly deletes the swapped element without further unnecessary operations, preventing incorrect element movements.

4. **Trickling Logic**:
   - The changes ensure that if the trickled element was moved to its own index due to the swap, it prevents unnecessary trickle operations.
   - If elements need trickling to maintain heap properties, it continues to use `fillHole`.

5. **Additional Bug Indications**: The changes ensure correctly handling edge cases where elements change positions due to heap restructuring, addressing an incorrect removal scenario.

These modifications align perfectly with the described bug fix for incorrect element removal, ensuring that the right element gets removed, and unnecessary operations are minimized. The changes focus on logical corrections pertinent to bug fixes. The answer is **Buggy**."
guava,15643.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,16 +1,16 @@
-    int getCorrectLastElement(E actualLastElement) {
+    int swapWithConceptuallyLastElement(E actualLastElement) {
       int parentIndex = getParentIndex(size);
       if (parentIndex != 0) {
         int grandparentIndex = getParentIndex(parentIndex);
         int uncleIndex = getRightChildIndex(grandparentIndex);
         if (uncleIndex != parentIndex && getLeftChildIndex(uncleIndex) >= size) {
           E uncleElement = elementData(uncleIndex);
           if (ordering.compare(uncleElement, actualLastElement) < 0) {
             queue[uncleIndex] = actualLastElement;
             queue[size] = uncleElement;
             return uncleIndex;
           }
         }
       }
       return size;
     }",NotBuggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message describes fixing two bugs, and the diff shows a name change and a potential fix for one of them. Specifically, the method name change implies a logic shift from merely identifying a ""correct"" last element to actively swapping. The description and linked issue strongly indicate this addresses the first bug (incorrect element removal). The lack of context around iterating elements during removal suggests the diff partially covers the commit message. Nevertheless, the altered method suggests correcting internal logic, likely fixing a bug. The answer is **Buggy**."
guava,15653.json,2ef955163b3d43e7849c1929ef4e5d714b93da96,"@@ -1,21 +1,23 @@
     public void remove() {
       checkRemove(canRemove);
       checkModCount();
       canRemove = false;
       expectedModCount++;
       if (cursor < size()) {
         MoveDesc<E> moved = removeAt(cursor);
         if (moved != null) {
           if (forgetMeNot == null) {
             forgetMeNot = new ArrayDeque<E>();
             skipMe = new ArrayList<E>(3);
           }
-          forgetMeNot.add(moved.toTrickle);
+          if (!containsExact(skipMe, moved.toTrickle)) {
+            forgetMeNot.add(moved.toTrickle);
+          }
           skipMe.add(moved.replaced);
         }
         cursor--;
       } else { // we must have set lastFromForgetMeNot in next()
         checkState(removeExact(lastFromForgetMeNot));
         lastFromForgetMeNot = null;
       }
     }",Buggy,"Fix two bugs in MinMaxPriorityQueue (introduced in [] First is a bug in removeAt(int) that sometimes causes the wrong element to be removed. Second is a bug that sometimes causes certain elements to be iterated over more than once if elements were removed during iteration.

Reported externally at https://github.com/google/guava/issues/2658

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=140382230",Buggy,"The commit message mentions two specific bug fixes in the `MinMaxPriorityQueue`. The provided diff is a part of the code modifications intended to address these bugs.

1. **Inspecting the Code Changes**:
   - The modified code takes place in the `remove()` method.
   - The change introduces a condition to check `containsExact(skipMe, moved.toTrickle)` prior to adding `moved.toTrickle` to `forgetMeNot`. This prevents duplicate elements from being added unnecessarily, ensuring elements that are moved during removal aren’t mistakenly re-added if they were already marked to be skipped.

2. **Relevance to the Commit Message**:
   - The commit message speaks of a fix to an issue where certain elements could be iterated over more than once if they were removed during iteration. This aligns with the problem addressed by the condition `if (!containsExact(skipMe, moved.toTrickle))`, which prevents duplicate entries and unnecessary iterations.
   - This correction helps maintain the integrity of the iteration process after an element is removed, consistent with the described bug fix in the commit message.

Considering these observations, the changes in the code directly relate to one of the stated bugs in the commit message, particularly addressing the iteration issue. Therefore, the conclusion for this diff is **Buggy**."
guava,11915.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,19 +1,20 @@
   final TypeToken<T> rejectTypeVariables() {
     new TypeVisitor() {
       @Override void visitTypeVariable(TypeVariable<?> type) {
         throw new IllegalArgumentException(
             runtimeType + ""contains a type variable and is not safe for the operation"");
       }
       @Override void visitWildcardType(WildcardType type) {
         visit(type.getLowerBounds());
         visit(type.getUpperBounds());
       }
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }
       @Override void visitGenericArrayType(GenericArrayType type) {
         visit(type.getGenericComponentType());
       }
     }.visit(runtimeType);
     return this;
   }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The changes made in the code are aimed at fixing a bug in the `rejectTypeVariables()` method. Here's what the analysis of the diff shows:

1. **Understanding the Code Changes:**
    - The commit adds a new line in the `visitParameterizedType()` method, calling `visit(type.getOwnerType());`.
    - Previously, the method was only visiting the actual type arguments of a parameterized type. 

2. **Reason for the Change:**
    - The commit message explains that there was a bug where owner type's type variables weren't accounted for during validation.
    - If an owner type has type variables, and those weren't visited, it could lead to scenarios where the presence of type variables wasn't correctly identified, violating safety guarantees.
    - This can be significant in inner classes where the owner class's type parameters may affect the inner class's type variables directly.

3. **Alignment with Commit Message:**
    - The commit message clearly describes the nature of the bug.
    - It provides an illustrative example showing how the bug would manifest.
    - The fix in the code directly addresses this by ensuring that owner types are examined for any type variables, thereby removing the oversight.

Since the code changes align accurately with the commit message and address a specific bug related to type variable rejection in generic type handling, it is certain that this change indicates a bug fix.

The final assessment is **Buggy**."
guava,11913.json,953e11352bbcb297987c81d1158f0c46f16cc054,"@@ -1,3 +1,4 @@
       @Override void visitParameterizedType(ParameterizedType type) {
         visit(type.getActualTypeArguments());
+        visit(type.getOwnerType());
       }",Buggy,"Fixed a bug in rejectTypeVariable() where we failed to account for the owner type's type variables. For example:

abstract class From<K> {
class To<V> {
TypeToken<To<V>> type() {
return new TypeToken<To<V>>(getClass()) {};
}
}

static <K> TypeToken<From<K>.To<?>> cheat() {
return new From<K>().new To<String>() {}.type();
}

TypeToken<From<String>.To<?>> notReallyString = TheClass.<String>cheat();
notReallyString.rejectTypeVariables(); // Should throw
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=45444012",Buggy,"The commit message explains a bug related to `rejectTypeVariable()` not accounting for the owner type's type variables in parameterized types. The provided code example illustrates this oversight. The fix involves modifying `visitParameterizedType` by adding `visit(type.getOwnerType())`. This ensures that the owner type's type variables are now considered, preventing the bug where such type variables would not be rejected appropriately.

Considering the changes and the commit message, the alterations align well with the described issue and its resolution. The fix is clearly about correcting a specific logical oversight, improving how type variables are processed.

Therefore, the answer is **Buggy**."
guava,17172.json,0330f712f56cc53633874e95bdc1203cf881eb85,"@@ -1,10 +1,10 @@
     public V get(@Nullable Object key) {
       if (key instanceof Range) {
         Range<?> range = (Range<?>) key;
         RangeMapEntry<K, V> rangeMapEntry = entriesByLowerBound.get(range.lowerBound);
-        if (rangeMapEntry.getKey().equals(range)) {
+        if (rangeMapEntry != null && rangeMapEntry.getKey().equals(range)) {
           return rangeMapEntry.getValue();
         }
       }
       return null;
     }",Buggy,"Fix bug in TreeRangeMap.asMapOfRanges().get(absentRange)
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=38651843",Buggy,"The diff addresses a bug in the `get` method of a map-like data structure working with ranges. The original code assumes that `entriesByLowerBound.get(range.lowerBound)` will always return a non-null value. However, if the range is not present in the map, `entriesByLowerBound.get` returns null, leading to a `NullPointerException` when `rangeMapEntry.getKey().equals(range)` is called. 

The modification introduces a null check (`rangeMapEntry != null`) before attempting to access methods on `rangeMapEntry`. This ensures that the code accommodates absent ranges gracefully and avoids a potential `NullPointerException`.

The commit message, indicating a fix for a bug in `TreeRangeMap.asMapOfRanges().get(absentRange)`, directly aligns with the changes in the diff, which handle scenarios where a range might be absent.

Therefore, the changes represent a bug fix in the code. The answer is **Buggy**."
guava,21153.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void startTask() {
-      lock.lock();
-      try {
-        if (isShutdown()) {
+      synchronized (lock) {
+        if (shutdown) {
           throw new RejectedExecutionException(""Executor already shutdown"");
         }
         runningTasks++;
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message mentions a fix for a signaling bug in `newDirectExecutorService()`. The code change switches from an explicit `lock` with `lock.lock()` and `lock.unlock()` to a synchronized block on `lock`. This change targets synchronization and signaling issues more efficiently. The original issue hinted in the commit message involves a scenario where a thread waiting on termination might not be correctly notified on shutdown due to inadequate synchronization mechanisms. By transitioning to using the `synchronized` block, the code can leverage Java's implicit monitor locks which employ strategies such as biased locking, potentially reducing contention and providing a solution to the issue described.

This aligns with fixing the signaling bug as described in the commit message, wherein a thread waiting on a condition might be improperly managed. The application of `synchronized` effectively changes how threads waiting on the condition are handled, addressing the described bug.

Considering the nature of the changes and their alignment with the indicated bug fix in the commit message, the analysis concludes this as a bug fix.

**Buggy**"
guava,21154.json,1a5b0b9caaa5b6aaef77c9e7ace4f8cac4025738,"@@ -1,11 +1,8 @@
     private void endTask() {
-      lock.lock();
-      try {
-        runningTasks--;
-        if (isTerminated()) {
-          termination.signalAll();
+      synchronized (lock) {
+        int numRunning = --runningTasks;
+        if (numRunning == 0) {
+          lock.notifyAll();
         }
-      } finally {
-        lock.unlock();
       }
     }",Buggy,"Fix a signaling bug in newDirectExecutorService()  Also switch to implicit
monitors to take advantage of biased locking.

If thread A calls awaitTermination while there are no running tasks, then
Thread B calls shutdown(). Thread A will never be woken up.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=87280837",Buggy,"The commit message describes a bug in newDirectExecutorService() concerning signaling and thread termination, with a focus on a scenario where threads could hang. The code changes replace a `ReentrantLock` with synchronized blocks and use `notifyAll()` instead of `signalAll()`. This change addresses thread notification behavior, which is consistent with fixing a signaling bug as described. The transition to using implicit monitors (synchronized blocks) can also help with performance through biased locking, aligning with the commit's intent to improve efficiency. Thus, the changes clearly relate to fixing the described bug. 

Therefore, the changes align with the commit message and address the bug in question.

**Buggy**"
guava,11437.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,25 +1,21 @@
   public String collapseFrom(CharSequence sequence, char replacement) {
-    int first = indexIn(sequence);
-    if (first == -1) {
-      return sequence.toString();
-    }
-
-    // TODO(kevinb): see if this implementation can be made faster
-    StringBuilder builder = new StringBuilder(sequence.length())
-        .append(sequence.subSequence(0, first))
-        .append(replacement);
-    boolean in = true;
-    for (int i = first + 1; i < sequence.length(); i++) {
+    // This implementation avoids unnecessary allocation.
+    int len = sequence.length();
+    for (int i = 0; i < len; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        if (!in) {
-          builder.append(replacement);
-          in = true;
+        if (c == replacement
+            && (i == len - 1 || !matches(sequence.charAt(i + 1)))) {
+          // a no-op replacement
+          i++;
+        } else {
+          StringBuilder builder = new StringBuilder(len)
+              .append(sequence.subSequence(0, i))
+              .append(replacement);
+          return finishCollapseFrom(sequence, i + 1, len, replacement, builder, true);
         }
-      } else {
-        builder.append(c);
-        in = false;
       }
     }
-    return builder.toString();
+    // no replacement needed
+    return sequence.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",NotBuggy,"The changes in the `collapseFrom` method optimize it by avoiding unnecessary `StringBuilder` allocations. The method is modified to iterate over the character sequence without immediate allocation and only allocates a `StringBuilder` if a replacement is necessary.

The commit message states that the update optimizes the `collapseFrom` method to allocate memory only when necessary, which is precisely what the code change accomplishes. The original code always allocated a `StringBuilder` when a match was found, even if no further replacements were required, leading to unnecessary allocations. The new code checks for actual replacements and only then creates a `StringBuilder`.

This commit doesn't indicate a bug fix but rather a performance optimization. The commit message aligns well with the code changes, and since the main purpose here is optimization and not fixing an error, the conclusion is **NotBuggy**."
guava,11403.json,b4cf74aa3a56751f25ae09eb79b68e0656d40618,"@@ -1,21 +1,17 @@
-  public String trimAndCollapseFrom(CharSequence sequence, char replacement) {
-    int first = negate().indexIn(sequence);
-    if (first == -1) {
-      return """"; // everything matches. nothing's left.
-    }
-    StringBuilder builder = new StringBuilder(sequence.length());
-    boolean inMatchingGroup = false;
-    for (int i = first; i < sequence.length(); i++) {
+  private String finishCollapseFrom(
+      CharSequence sequence, int start, int end, char replacement,
+      StringBuilder builder, boolean inMatchingGroup) {
+    for (int i = start; i < end; i++) {
       char c = sequence.charAt(i);
       if (matches(c)) {
-        inMatchingGroup = true;
-      } else {
-        if (inMatchingGroup) {
+        if (!inMatchingGroup) {
           builder.append(replacement);
-          inMatchingGroup = false;
+          inMatchingGroup = true;
         }
+      } else {
         builder.append(c);
+        inMatchingGroup = false;
       }
     }
     return builder.toString();
   }",NotBuggy,"Optimize collapseFrom and trimAndCollapseFrom so that they
only allocate if necessary.  Add a few tests.  Remove the GWT
workarounds for GWT bug 4491 since it appears to be fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=39757514",NotBuggy,"The commit message discusses optimizing the `collapseFrom` and `trimAndCollapseFrom` methods by minimizing unnecessary string allocations. 

Examining the code changes:
1. **Method Change**: The logic handling was slightly restructured. Previously, new allocations could occur in the `trimAndCollapseFrom` method whether necessary or not. The reorganization into `finishCollapseFrom` suggests resource allocation and management improvements, aligning with the message about optimizing for less allocation.
2. **In-loop Logic**: The diff modifies checks for `inMatchingGroup` in a way that seems to avoid unnecessary state changes in the loop, which could lead to optimizations in processing the sequence.
3. **Function Signature**: The newly defined function may assist in reducing overall allocations by reusing `StringBuilder` more efficiently.

The commit message doesn't mention any bugs being fixed, which is consistent with the changes seen in the diff. The adjustments primarily focus on performance improvements and optimizations, not bug fixes.

Thus, based on the evidence presented in the diff and the stated goal within the commit message, the modifications are performance-oriented rather than aimed at addressing a bug. Therefore, the changes are **NotBuggy**."
guava,22907.json,9bf42862f877a0ed8234bd484aa0089a3773ccd4,"@@ -1,11 +1,12 @@
   private void addDoneString(StringBuilder builder) {
     try {
-      builder.append(""SUCCESS, result=["").append(getDone(this)).append(""]"");
+      V value = getDone(this);
+      builder.append(""SUCCESS, result=["").append(value).append(""]"");
     } catch (ExecutionException e) {
       builder.append(""FAILURE, cause=["").append(e.getCause()).append(""]"");
     } catch (CancellationException e) {
       builder.append(""CANCELLED"");
     } catch (RuntimeException e) {
       builder.append(""UNKNOWN, cause=["").append(e.getClass()).append("" thrown from get()]"");
     }
   }",Buggy,"ImmediateFuture.toString() implementation to provide the returned value or exception.
Provides for all ImmediateFutures implementation the same syntax that AbstractFuture uses.

It's especially convenient during debugging of mocked asynchronous gRPC services.

Fixes bug in AbstractFuture#toString in GWT that caused wrong representation:
""Class$S413@205[status=SUCCESS, result=[FAILURE, cause=[Class$S6]]"" due to ongoing
builder being already called when getDone() throws exception.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165925433",Buggy,"The commit message highlights a bug fix related to the string representation of `AbstractFuture` in GWT (Google Web Toolkit), where the string format was incorrect due to premature invocation of the builder in the `getDone()` method.

The code change addresses this by:
- Separating the retrieval of the result from the appending logic. Previously, the method `getDone(this)` was called directly within the `append()` method, which could lead to incorrect string formats in case of an exception.
- Introducing a new local variable `V value` that holds the result of `getDone(this)`. This allows the result to be stored separately before appending it to the `StringBuilder`.
- Maintaining the original behavior by appending the `value` within the `try` block to ensure that if an `ExecutionException` is thrown, the catch blocks append the appropriate failure or cancelation messages.

The changes improve error handling within the `toString` implementation by correctly managing how exceptions are caught and represented in the string output. This aligns with the commit message, which describes improvements to exception handling and resultant string representation accuracy. 

Given these observations, the diff indicates a corrective patch to address a specific issue with string representation, thus marking it as a bug fix.

Therefore, the assessment is **Buggy**."
pmd,8491.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,17 +1,17 @@
     public Object visit(ASTMethodDeclarator node, Object data) {
         Scope scope = node.getScope();
         Map params = scope.getVariableDeclarations(true);
         for (Iterator i = params.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration)i.next();
             List usages = (List)params.get(decl);
             for (Iterator j = usages.iterator();j.hasNext();) {
                 NameOccurrence occ = (NameOccurrence)j.next();
-                if (occ.isOnLeftHandSide()) {
+                if (occ.isOnLeftHandSide() && (occ.getNameForWhichThisIsAQualifier() == null)) {
                     RuleContext ctx = (RuleContext)data;
                     String msg = MessageFormat.format(getMessage(), new Object[] {decl.getImage()});
                     ctx.getReport().addRuleViolation(createRuleViolation(ctx, decl.getLine(), msg));
                 }
             }
         }
         return super.visit(node, data);
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The code change adds a condition to the if statement in the visit method, which improves the logic for detecting rule violations. The additional condition, `occ.getNameForWhichThisIsAQualifier() == null`, ensures that the logic only considers occurrences that are not qualifiers for another name. This likely fixes a false positive or logical oversight in the original rule violation detection logic.

By refining the criteria for when a variable occurrence is considered on the left-hand side (and thus relevant for further processing), the change addresses a specific bug in the logic. This aligns with the commit message stating ""Several bug fixes,"" indicating that this adjustment is part of those corrections.

Therefore, given the context and the nature of the code modification, it represents a bug fix. The conclusion is **Buggy**."
pmd,8745.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,20 +1,20 @@
     public Object visit(ASTMethodDeclaration node, Object data) {
         if (node.isAbstract()) {
             return data;
         }
 
         List returnNodes = new ArrayList();
-        node.findChildrenOfType(ASTReturnStatement.class, returnNodes);
+        node.findChildrenOfType(ASTReturnStatement.class, returnNodes, false);
         if (returnNodes.size() > 1) {
             RuleContext ctx = (RuleContext)data;
             for (Iterator i = returnNodes.iterator(); i.hasNext();) {
                 SimpleNode problem = (SimpleNode)i.next();
                 // skip the last one, it's OK
                 if (!i.hasNext()) {
                     continue;
                 }
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, problem.getBeginLine()));
             }
         }
         return data;
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates several bug fixes. The diff specifies a change in the method call to `findChildrenOfType`. Originally, this method was called with two arguments, implicitly assuming a third argument's default value. The refactoring introduces a `false` value as a third explicit argument. This adjustment likely addresses a condition where the method's behavior needs to work correctly with a specific parameter value.

By updating the method call, it suggests that the previous implementation had an unintended or incorrect behavior, likely leading to unresolved issues with detecting certain return statements within a method declaration. This seems to adjust the logic for how child nodes are found, which matches the commit message's claim of fixing bugs.

Therefore, given the relationship between the change and the commit message, these modifications align with the concept of fixing a bug related to handling return statement nodes within a method's abstract syntax tree. Hence, the answer is **Buggy**."
pmd,1151.json,384cd4f3d9a15a09b664c728ad42e03321428d7a,"@@ -1,15 +1,18 @@
-    private void findChildrenOfType(Node node, Class targetType, List results) {
+    private void findChildrenOfType(Node node, Class targetType, List results, boolean descendIntoNestedClasses) {
         if (node.getClass().equals(targetType)) {
             results.add(node);
         }
+        if (node.getClass().equals(ASTClassBody.class) && !descendIntoNestedClasses) {
+            return;
+        }
         for (int i=0; i<node.jjtGetNumChildren(); i++) {
-            Node child = (Node)node.jjtGetChild(i);
+            Node child = node.jjtGetChild(i);
             if (child.jjtGetNumChildren()>0) {
-                findChildrenOfType(child, targetType, results);
+                findChildrenOfType(child, targetType, results, descendIntoNestedClasses);
             } else {
                 if (child.getClass().equals(targetType)) {
                     results.add(child);
                 }
             }
         }
     }",Buggy,"Several bug fixes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1376 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit introduces a boolean parameter, `descendIntoNestedClasses`, to the `findChildrenOfType` method. This change allows controlling whether or not to descend into nested class bodies when searching for nodes of a specific type. Here's a detailed breakdown of the changes:

1. **Parameter Addition**:
   - The method `findChildrenOfType` now includes an additional parameter, `descendIntoNestedClasses`. This parameter is used to decide if the method should continue the search within nested class definitions.

2. **Logic Addition**:
   - An additional condition checks if the current node is an instance of `ASTClassBody` and if `descendIntoNestedClasses` is false. If both conditions are true, the method returns early, effectively skipping the nested classes.

3. **Recursive Call Update**:
   - The recursive call to `findChildrenOfType` has been updated to pass the new `descendIntoNestedClasses` parameter, ensuring the intended behavior is applied consistently throughout recursive calls.

These changes look like they're intended to fix a bug related to unwanted or unexpected traversal into nested classes during node type searches. Developers might encounter issues or errors if the system doesn't handle or expect nested class structures properly. The modifications address this by introducing controlled behavior, preventing traversal into nested class bodies when it's not desired.

Therefore, the changes directly indicate a bug fix to enhance the functionality of node searching in nested class environments. The commit message's vague ""Several bug fixes"" aligns, as the change resolves a potential issue with nested class recursion control. Thus, the answer is **Buggy**."
pmd,7973.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,10 +1,4 @@
     public boolean isOnRightHandSide() {
         SimpleNode node = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
-        if (node instanceof ASTExpression) {
-            SimpleNode parent = (SimpleNode) node.jjtGetParent();
-            if (node.jjtGetNumChildren() == 3) {
-                return true;
-            }
-        }
-        return false;
+        return node instanceof ASTExpression && node.jjtGetNumChildren() == 3;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a fix for ""ImmutableFieldRule"" false positives related to static fields, but the diff changes `isOnRightHandSide`. The modification combines conditions into a single return statement, not addressing the mentioned false positives related to static fields.

However, this change alone doesn't suggest a fix for false positives about static fields. It's a simplification or refactoring, not directly linked to static field handling.

No evidence suggests a direct correction of false positives explicitly for fields. Thus, the actual bug fix isn't here.

The decision is **NotBuggy**."
pmd,8886.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,18 +1,18 @@
     public Object visit(ASTUnmodifiedClassDeclaration node, Object data) {
         Map vars = node.getScope().getVariableDeclarations();
         for (Iterator i = vars.keySet().iterator(); i.hasNext();) {
             VariableNameDeclaration decl = (VariableNameDeclaration) i.next();
-            if (!decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
+            if (decl.getAccessNodeParent().isStatic() || !decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
                 continue;
             }
+
             int result = initializedInConstructor((List)vars.get(decl));
             if (result == MUTABLE) {
             	continue;
             }
-            if ((result == IMMUTABLE) ||
-                ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
+            if (result == IMMUTABLE || ((result == CHECKDECL) && initializedInDeclaration(decl.getAccessNodeParent()))) {
                 ((RuleContext) data).getReport().addRuleViolation(createRuleViolation((RuleContext) data, decl.getLine(), MessageFormat.format(getMessage(), new Object[]{decl.getImage()})));
             }
         }
         return super.visit(node, data);
     }",Buggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The changes in the diff are related to fixing false positives for static fields by modifying the condition in a PMD rule. Specifically, the line:

```java
if (!decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
```

is changed to:

```java
if (decl.getAccessNodeParent().isStatic() || !decl.getAccessNodeParent().isPrivate() || decl.getAccessNodeParent().isFinal()) {
```

By adding `decl.getAccessNodeParent().isStatic()`, the code now correctly recognizes static fields and handles them according to the specific conditions set for detecting immutable field rule violations. This adjustment prevents static fields from incorrectly triggering the rule, which aligns perfectly with the commit message claiming to fix false positives.

The code diff shows evidence of improving the logic to prevent such errors, thus indicating it is a bug fix.

The commit message also mentions an unrelated fix about a version number, but this diff focuses on the part related to false positives for static fields.

Considering this analysis, the appropriate conclusion, as it genuinely addresses a reported bug related to false positives, is **Buggy**."
pmd,8888.json,a405d23dfb9e574e2b2ef23f1f45d548a738ed3b,"@@ -1,31 +1,31 @@
     private int initializedInConstructor(List usages) {
         int rc = MUTABLE, initCount = 0;
         boolean setInConstructor = false;
 		boolean foundUsage = false;
 
         for (Iterator j = usages.iterator(); j.hasNext();) {
         	foundUsage = true;
-        	NameOccurrence occurance = (NameOccurrence)j.next();
-            if (occurance.isOnLeftHandSide()) {
-            	SimpleNode node = occurance.getLocation();
+        	NameOccurrence occ = (NameOccurrence)j.next();
+            if (occ.isOnLeftHandSide()) {
+            	SimpleNode node = occ.getLocation();
             	if (node.getFirstParentOfType(ASTConstructorDeclaration.class) != null) {
             		setInConstructor = true;
             		initCount++;
  				}
  				else {
  					if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
  						initCount++;
  					}
  				}
  			}
         }
         if (!foundUsage || (initCount == 0)) {
         	rc = CHECKDECL;
         }
         else {
         	if ((initCount == 1) && setInConstructor) {
         		rc = IMMUTABLE;
         	}
         }
         return rc;
     }",NotBuggy,"Fixed bug 1050173 - ImmutableFieldRule no longer reports false positives for static fields.  Also fixed version number in PMD.java, argh.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2994 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit fixes a bug associated with the class ImmutableFieldRule to prevent false positives for static fields. However, in the provided diff, the only change is renaming the variable `occurance` to `occ`, which is a simple refactor for readability or consistency purposes. This modification itself doesn't address the logic or behavior of the code in terms of resolving the stated bug.

The commit message also mentions a version number fix in PMD.java, but no relevant changes are shown in this diff. The diff provided does not include any alterations that directly fix the bug mentioned regarding false positives or affect the logic related to the bug description.

Since the diff doesn't contain changes indicating any error-handling updates, logical corrections, exception-handling improvements, or other signs of bug-related changes associated with addressing false positives for static fields, the review of the diff itself does not point towards a bug fix.

Thus, based on the diff provided, the conclusion for this specific change is **NotBuggy**."
pmd,8249.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,4 +1,4 @@
 		public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
 			parent.addType(Type.getType(desc));
-			return annotationVisitor;
+			return parent.annotationVisitor;
 		}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug related to `acceptType` functionality, specific to fields and local variables. The code change modifies the return value of the `visitAnnotation` method, changing from `annotationVisitor` to `parent.annotationVisitor`. 

Here's the step-by-step analysis:

1. **Context**: The method `visitAnnotation` deals with handling annotations in a visitor pattern, a common technique in parsing or processing serialized data (like bytecode).

2. **Change**: The diff changes the returned object from `annotationVisitor` (a local or possibly inherited variable) to `parent.annotationVisitor`.

3. **Implication of Change**: 
   - **Consistency**: If `parent.annotationVisitor` is intended to be used consistently within this context, then the change fixes a logical issue where the wrong annotation visitor instance might have been used.
   - **Scoping/Bug**: The issue might have arisen because the method was not accessing the properly scoped or expected instance, thereby potentially missing some annotations or incorrectly processing them, especially in fields or local variables as mentioned in the commit message.

4. **Relation to Commit Message**: 
   - The change aligns with the commitment of fixing a bug in how annotations are processed via `acceptType` in the context of fields and local variables. It ensures that the correct visitor instance tied to the `parent` is always used, potentially resolving errors related to wrong annotation processing.

5. **Assessment**: Given that the commit addresses a specific issue related to incorrect instance usage regarding annotations, the change is a direct fix to the specified problem described in the commit message.

Conclusively, the change fixes a bug by ensuring consistent usage of the `parent.annotationVisitor`, aligning with the commit message's context of processing annotations correctly. 

The assessment is **Buggy**."
pmd,8247.json,222563dc32c51c8f77a1b06ba69077463a525479,"@@ -1,5 +1,5 @@
 	private void extractSignature(String sig) {
 		if (sig != null) {
-			new SignatureReader(sig).acceptType(sigVisitor);
+			new SignatureReader(sig).accept(sigVisitor);
 		}
 	}",Buggy,"bug fix: acceptType only works for fields and locals


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4822 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The changes in the diff show a modification from `acceptType(sigVisitor)` to `accept(sigVisitor)`. Given the context of the commit message mentioning a bug fix, it indicates that the method `acceptType` wasn't correctly handling certain cases or inputs. The switch to `accept` likely corrects the handling logic for `sigVisitor`, suggesting a bug fix related to how signatures are processed.

**Buggy**"
pmd,6069.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,8 +1,8 @@
     public void jjtClose() {
-        if ((children == null) || (children.length == 0)) {
+        if (beginLine == -1 && (children == null || children.length == 0)) {
             beginLine = parser.token.beginLine;
             beginColumn = parser.token.beginColumn;
         }
         endLine = parser.token.endLine;
         endColumn = parser.token.endColumn;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The diff modifies the condition in the `jjtClose` method. Originally, it only checked if the `children` array was null or empty. The change introduces an additional check, `beginLine == -1`, ensuring that the `beginLine` and `beginColumn` are only set if they haven't been initialized yet. 

This change implies that fields beginLine and beginColumn may have been incorrectly initialized under certain conditions (such as when the method is called again with existing children). The added condition helps prevent wrongly setting the positional information due to potential parser reuse or nodes being evaluated earlier.

The commit message, ""Fixed bug 583047 - columns were wrong on ASTName nodes"", directly aligns with this update as it addresses initialization that could lead to inaccuracies in node positions, effectively fixing incorrect ASTName nodes' positions. Therefore, the changes in the code indicate a bug fix.

The answer is **Buggy**."
pmd,6068.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,6 +1,6 @@
     public void jjtOpen() {
-        if (parser.token.next != null) {
+        if (beginLine == -1 && parser.token.next != null) {
             beginLine = parser.token.next.beginLine;
             beginColumn = parser.token.next.beginColumn;
         }
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The change in the code introduces a condition to check if `beginLine` is -1 before assigning new values to `beginLine` and `beginColumn`. This modification likely ensures that these variables are only set if they haven't been previously set (indicating an uninitialized or default state). 

The commit message explicitly states that it fixes bug 583047, where ""columns were wrong on ASTName nodes."" The correlation between the message and code indicates that this check prevents overwriting the `beginLine` and `beginColumn` fields when they already have valid values, which likely maintains the correct position information within the AST nodes.

This alignment between the commit message and the code change suggests that the update addresses an issue of incorrect column information, which is indicative of a bug fix. Therefore, the conclusion is **Buggy**."
pmd,1134.json,51ab4ae563f9875328e56161bff3eb97331b10cd,"@@ -1,11 +1,3 @@
     public int getBeginLine() {
-        if (beginLine != -1) {
-            return beginLine;
-        } else {
-            if ((children != null) && (children.length > 0)) {
-                return ((SimpleNode) children[0]).getBeginLine();
-            } else {
-                throw new RuntimeException(""Unable to determine begining line of Node."");
-            }
-        }
+        return beginLine;
     }",Buggy,"Fixed bug 583047 - columns were wrong on ASTName nodes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@2011 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message states that a bug related to columns in ASTName nodes was fixed. However, the code change involves simplifying the `getBeginLine()` method by removing the logic that checks if `beginLine` is `-1` and, if so, attempts to retrieve the beginning line from the first child node. The new code simply returns `beginLine`, assuming it is always set correctly.

This change addresses a situation where the previous logic could have led to a `RuntimeException` if `beginLine` was not set and there were no children, or if the children did not correctly provide the beginning line. Simplifying the method to always return `beginLine` suggests that there might have been a bug related to incorrect line calculations when relying on child nodes or when `beginLine` wasn't properly initialized before this change.

Overall, given the context described in the commit message and the modification to ensure `getBeginLine()` returns the stored `beginLine` consistently, this change likely fixes the described bug. Therefore, the analysis concludes with **Buggy**."
pmd,2689.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,7 +1,7 @@
     public int getLineCount(Mark mark, Match match) {
         TokenEntry endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount());
         if (endTok.equals(TokenEntry.EOF)) {
             endTok = get(mark.getIndexIntoTokenArray() + match.getTokenCount() - 1);
         }
-        return endTok.getBeginLine() - mark.getBeginLine();
+        return endTok.getBeginLine() - mark.getBeginLine() - 1;
     }",Buggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The change in the diff modifies the logic that calculates line count by subtracting an additional 1. This suggests correcting off-by-one error in line count calculation, aligning with the commit message's reference to fixing a logic bug in the ""source code slice"". The adjustment implies the previous implementation was consistently overestimating line counts by one line. Given these factors, it's clear this is a bug fix. Therefore, the final assessment is **Buggy**."
pmd,2580.json,e249deb0879da58e78f31eafbafc3992661b5142,"@@ -1,32 +1,32 @@
     public void findMatches(int min) {
        /*
          Assign sort codes to all the pooled code. This should speed
          up sorting them.
        */
         int count = 1;
         for (Iterator iter = pool.keySet().iterator(); iter.hasNext();) {
            TokenEntry token = (TokenEntry)iter.next();
            token.setSortCode(count++);
         }
 
         MarkComparator mc = new MarkComparator(cpdListener, code);
         Collections.sort(marks, mc);
 
         MatchCollector coll = new MatchCollector(marks, code, mc);
         matches = coll.collect(min);
         Collections.sort(matches);
 
         for (Iterator i = matches(); i.hasNext();) {
             Match match = (Match)i.next();
             for (Iterator occurrences = match.iterator(); occurrences.hasNext();) {
                 Mark mark = (Mark)occurrences.next();
-                SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                 match.setLineCount(tokens.getLineCount(mark, match));
                 if (!occurrences.hasNext()) {
                     int start = mark.getBeginLine();
-                    int end = mark.getBeginLine()-1 + tokens.getLineCount(mark, match);
+                    int end = start + match.getLineCount() - 1;
+                    SourceCode sourceCode = (SourceCode)source.get(mark.getTokenSrcID());
                     match.setSourceCodeSlice(sourceCode.getSlice(start, end));
                 }
             }
         }
     }",NotBuggy,"Fixed bug in the 'source code slice' logic


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1705 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message states a fix in the 'source code slice' logic, and the code change supports this statement. The problem it addresses is the calculation of the `end` variable by making it dependent on the `match.getLineCount()`, which is a logical correction from using `mark.getBeginLine() - 1 + tokens.getLineCount(mark, match)`. By ensuring that `end` correctly reflects the start line plus the line count minus one, it accurately delineates the slice boundaries for the source code. Additionally, the placement of the `SourceCode` retrieval is corrected to occur after computing `start` and `end`, likely to ensure it operates with accurate boundaries. Therefore, these changes seem to be aligned with fixing incorrect logic related to source code slicing.

**Buggy**"
pmd,961.json,1ee8cc4e8daf001172f2a3aa1a8c2a89706e988f,"@@ -1,32 +1,32 @@
     public static MetricKey<ASTAnyTypeDeclaration> of(final Metric<ASTAnyTypeDeclaration> metric, final String name) {
         return new MetricKey<ASTAnyTypeDeclaration>() {
             @Override
             public String name() {
                 return name;
             }
 
 
             @Override
             public Metric<ASTAnyTypeDeclaration> getCalculator() {
                 return metric;
             }
 
 
             @Override
             public boolean supports(ASTAnyTypeDeclaration node) {
                 return metric.supports(node);
             }
 
 
             @Override
             public boolean equals(Object obj) {
                 return obj == this;
             }
 
 
             @Override
             public int hashCode() {
-                return metric.hashCode() * 31 + name.hashCode();
+                return (metric != null ? metric.hashCode() * 31 : 0) + (name != null ? name.hashCode() : 0);
             }
         };
     }",Buggy,Fix bug in key hashcode with null names or metric,Buggy,"The commit message indicates a bug fix related to handling null values in the hash code computation for keys with potential null components. The diff changes the hashCode method to add checks for null for both `metric` and `name`. This prevents a `NullPointerException` if either field is null and modifies the hash code logic accordingly. This type of change is directly aligned with fixing a bug concerning null handling, as described in the commit message.

The changes in the code indicate an error-handling update to address null safety, which clearly fits the context of a bug fix. Thus, the appropriate conclusion is **Buggy**."
pmd,5595.json,64ca9b6f01f9b946a456f6de05eada2374906d6a,"@@ -1,3 +1,6 @@
     public EcmascriptNode getFinallyBlock() {
-	return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
+        if (!isFinally()) {
+            return null;
+        }
+        return (EcmascriptNode) jjtGetChild(jjtGetNumChildren() - 1);
     }",Buggy,"pmd: fix #1141 ECMAScript: getFinallyBlock() is buggy.
fix #1142 ECMAScript: getCatchClause() is buggy.",Buggy,"The diff shows a logical correction in the `getFinallyBlock` method. The modification adds a check with `isFinally()` to determine if the block is indeed a `finally` block before returning it. If it's not a `finally` block, it returns `null`. This prevents potentially incorrect behavior when the method incorrectly assumed the presence of a `finally` block.

The commit message indicates that this change addresses a bug in the `getFinallyBlock()` method (`fix #1141 ECMAScript: getFinallyBlock() is buggy`). The added condition prevents the method from returning a node that is not actually a `finally` block, which aligns with the description of fixing a bug.

Therefore, considering the context provided by the commit message and the code changes, these modifications are consistent with addressing and fixing a bug in the method's logic. 

The conclusion is **Buggy**, as the change is intended to correct a previously erroneous behavior in the code."
pmd,376.json,a3d5e7e1921e2c1c10f6a1e60fdfc19983732097,"@@ -1,3 +1,3 @@
     public void setClasspathRef(Reference r) {
-        createClasspath().setRefid(r);
+        createLongClasspath().setRefid(r);
     }",Buggy,"Fixed nested classpath bug


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1901 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fixed ""nested classpath bug."" The change replaces `createClasspath()` with `createLongClasspath()`, suggestive of addressing the bug by using a potentially more appropriate or correctly functioning method concerning classpath setting.

The commit addresses a bug related to setting classpaths—likely with nested classpath issues—and aligns with the message. This suggests the original code caused incorrect or unintended classpath configuration when dealing with nested contexts.

Therefore, the changes correspond with the bug fix described in the commit message, and the refactoring directly pertains to resolving a bug.

The answer is **Buggy**."
pmd,7138.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,7 +1,11 @@
-    private String getPackageName(String importName) {
+    public String getPackageName() {
+        String importName = getImportedName();
+        if (isImportOnDemand) {
+            return importName;
+        }
         if (importName.indexOf('.') == -1) {
             return """";
         }
         int lastDot = importName.lastIndexOf('.');
         return importName.substring(0, lastDot);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug fix related to detecting on-demand imports from the same package. The code diff reflects changes consistent with this fix:

1. The method `getPackageName` is modified to be public, suggesting its importance in broader scope usage, potentially aiding with the detection of import issues.

2. The logic now explicitly checks if an import is on-demand (`isImportOnDemand`). If it is, the method directly returns `importName`. This suggests the function now properly distinguishes on-demand imports, which is central to addressing the bug described in the commit message.

The changes directly address and fix the issue stated in the commit message by introducing a condition to handle on-demand imports appropriately. Thus, the modifications align with the commit description as a bug fix.

The answer is **Buggy**."
pmd,8736.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,19 +1,19 @@
     public Object visit(ASTImportDeclaration node, Object data) {
-        ImportWrapper wrapper = new ImportWrapper(node.getImportedNameNode().getImage(), node.getImportedNameNode().getImage(), node.getImportedNameNode());
+        ImportWrapper wrapper = new ImportWrapper(node.getImportedName(), node.getImportedName(), node.getImportedNameNode());
 
         // blahhhh... this really wants to be ASTImportDeclaration to be polymorphic...
         if (node.isImportOnDemand()) {
             if (importOnDemandImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 importOnDemandImports.add(wrapper);
             }
         } else {
             if (singleTypeImports.contains(wrapper)) {
                 addViolation(data, node.getImportedNameNode(), node.getImportedNameNode().getImage());
             } else {
                 singleTypeImports.add(wrapper);
             }
         }
         return data;
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message states that a bug related to on-demand imports from the same package is fixed. The code change replaces `getImportedNameNode().getImage()` with `getImportedName()`, ensuring the correct import name is used. This adjustment likely aids in correctly identifying and processing the specific import scenario described in the commit message.

The usage of `node.getImportedName()` might handle the case where the node represents an import on demand (a wildcard import like `import package.*;`) more effectively, which aligns with the intention of spotting a problematic import pattern.

Therefore, the changes seem directly related to correcting logic to handle a specific import case, which aligns with the commit message regarding bug fixing. This modification is indicative of a bug fix.

Final assessment: **Buggy**."
pmd,8913.json,fc511ec111d90e8f861c187a7e012652c80fcc4d,"@@ -1,6 +1,6 @@
     public Object visit(ASTImportDeclaration node, Object o) {
-        if (node.getImportedNameNode().getImage().indexOf(""junit"") != -1) {
+        if (node.getImportedName().indexOf(""junit"") != -1) {
             junitImported = true;
         }
         return super.visit(node, o);
     }",Buggy,"Fixed a bug in ImportFromSamePackage; now it catches the case where a class has an on-demand import for the same package it is in.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3729 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message claims a fix for detecting on-demand imports for the same package. However, the code change concerns identifying ""junit"" in imports, changing a method call from `getImportedNameNode().getImage()` to `getImportedName()`. It may not noticeably affect locating same-package on-demand imports.

The modification potentially corrects a specific function oversight, although it doesn't seem directly related to the stated problem but could indirectly make detection more reliable if `getImportedName()` provides a more accurate name representation or prevents an error.

Without further context about why `getImportedName()` is more suitable, it's challenging to conclusively connect this change with the described bug fix, particularly as it focuses on ""junit"" detection behavior.

Given the limited context and without clear evidence of addressing same-package import issues, this change, addressing potential image retrieval inconsistencies, tentatively alludes to a bug fix, assuming the existing functionality was flawed.

Therefore, considering the possibility of an underlying execution error related to this manipulation, the answer is **Buggy**, albeit with reservations about the precision of the fix regarding the commit message's specific statement."
pmd,4678.json,fb25329e0d9ec5d632667df393f7c1b25a698e68,"@@ -1,42 +1,42 @@
     public Object visit(ASTUserClass node, Object data) {
 
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
-        // baz = String.escapeSignleQuotes(...);
-        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
-        for (ASTAssignmentExpression a : assignmentCalls) {
-            findSanitizedVariables(a);
-            findSelectContainingVariables(a);
-        }
-
         final List<ASTFieldDeclaration> fieldExpr = node.findDescendantsOfType(ASTFieldDeclaration.class);
         for (ASTFieldDeclaration a : fieldExpr) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
         // String foo = String.escapeSignleQuotes(...);
         final List<ASTVariableDeclaration> variableDecl = node.findDescendantsOfType(ASTVariableDeclaration.class);
         for (ASTVariableDeclaration a : variableDecl) {
             findSanitizedVariables(a);
             findSelectContainingVariables(a);
         }
 
+        // baz = String.escapeSignleQuotes(...);
+        final List<ASTAssignmentExpression> assignmentCalls = node.findDescendantsOfType(ASTAssignmentExpression.class);
+        for (ASTAssignmentExpression a : assignmentCalls) {
+            findSanitizedVariables(a);
+            findSelectContainingVariables(a);
+        }
+
         // Database.query(...) check
         final List<ASTMethodCallExpression> potentialDbQueryCalls = node
                 .findDescendantsOfType(ASTMethodCallExpression.class);
 
         for (ASTMethodCallExpression m : potentialDbQueryCalls) {
             if (!Helper.isTestMethodOrClass(m) && Helper.isMethodName(m, DATABASE, QUERY)) {
                 reportStrings(m, data);
                 reportVariables(m, data);
             }
         }
-        
+
         safeVariables.clear();
         selectContainingVariables.clear();
 
         return data;
     }",NotBuggy,Bug fix to improve detection of concatenated vars,Buggy,"The commit message indicates a bug fix related to the detection of concatenated variables. The diff shows the reordering of code blocks within the `visit` method of `ASTUserClass`. Specifically, the block handling `ASTAssignmentExpression` was moved below another block handling `ASTVariableDeclaration`.

No changes were made to the logic within the individual helper methods (`findSanitizedVariables`, `findSelectContainingVariables`), meaning this change doesn't directly alter the logic used for detecting concatenated variables but changes the order in which these expressions are processed.

The reordering suggests an implicit dependency or preference on the sequence of detecting declarations versus assignments. If the original ordering led to missed detections due to processing order, this could indeed be a valid bug fix aimed at improving detection accuracy for concerns such as variable concatenation, especially if the variable must be recognized or processed before others in specific node processing.

Therefore, given the context of improving variable detection related to concatenation, this change is likely a bug fix addressing an issue with how the tool traverses and analyzes nodes for concatenated variables.

The evaluation concludes with **Buggy**, as the change aligns with the commit message indicating an improvement to variable detection that potentially resolves a bug in the analysis sequence."
pmd,7859.json,15ff7ca9447619240ec96fdb32f7974e1ecf5730,"@@ -1,34 +1,43 @@
-    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, final int parameterCount) {
+    private MethodNameDeclaration createBuiltInMethodDeclaration(final String methodName, String... parameterTypes) {
         ASTMethodDeclaration methodDeclaration = new ASTMethodDeclaration(JavaParserTreeConstants.JJTMETHODDECLARATION);
         methodDeclaration.setPublic(true);
         methodDeclaration.setScope(this);
 
         ASTMethodDeclarator methodDeclarator = new ASTMethodDeclarator(JavaParserTreeConstants.JJTMETHODDECLARATOR);
         methodDeclarator.setImage(methodName);
         methodDeclarator.setScope(this);
 
         ASTFormalParameters formalParameters = new ASTFormalParameters(JavaParserTreeConstants.JJTFORMALPARAMETERS);
         formalParameters.setScope(this);
 
         methodDeclaration.jjtAddChild(methodDeclarator, 0);
         methodDeclarator.jjtSetParent(methodDeclaration);
         methodDeclarator.jjtAddChild(formalParameters, 0);
         formalParameters.jjtSetParent(methodDeclarator);
 
+        int parameterCount = parameterTypes.length;
         for (int i = 0; i < parameterCount; i++) {
             ASTFormalParameter formalParameter = new ASTFormalParameter(JavaParserTreeConstants.JJTFORMALPARAMETER);
             formalParameters.jjtAddChild(formalParameter, i);
             formalParameter.jjtSetParent(formalParameters);
 
             ASTType type = new ASTType(JavaParserTreeConstants.JJTTYPE);
             formalParameter.jjtAddChild(type, 0);
             type.jjtSetParent(formalParameter);
+            ASTReferenceType referenceType = new ASTReferenceType(JavaParserTreeConstants.JJTREFERENCETYPE);
+            type.jjtAddChild(referenceType, 0);
+            referenceType.jjtSetParent(type);
+            ASTClassOrInterfaceType classOrInterfaceType = new ASTClassOrInterfaceType(JavaParserTreeConstants.JJTCLASSORINTERFACETYPE);
+            classOrInterfaceType.setImage(parameterTypes[i]);
+            referenceType.jjtAddChild(classOrInterfaceType, 0);
+            classOrInterfaceType.jjtSetParent(referenceType);
+
             ASTVariableDeclaratorId variableDeclaratorId = new ASTVariableDeclaratorId(JavaParserTreeConstants.JJTVARIABLEDECLARATORID);
             variableDeclaratorId.setImage(""arg"" + i);
             formalParameter.jjtAddChild(variableDeclaratorId, 1);
             variableDeclaratorId.jjtSetParent(formalParameter);
         }
 
         MethodNameDeclaration mnd = new MethodNameDeclaration(methodDeclarator);
         return mnd;
     }",Buggy,Fixes #1490 [java] PMD Error while processing - NullPointerException,Buggy,"The commit message states that it fixes a NullPointerException related to PMD error processing. Let's examine the diff for any changes relating to null handling or improvements that could address a null-pointer issue:

1. **Method Signature Change**: 
   - The method signature changed from using `int parameterCount` to `String... parameterTypes`. This allows specifying parameter types directly, which were previously unhandled, potentially leading to missing data or null references.

2. **Handling Parameter Types**:
   - In the for-loop, `parameterCount` is now derived from `parameterTypes.length`. Ensuring `parameterTypes` isn't null would prevent NullPointerException.
   - For each parameter, changes include creating AST nodes based on the parameter type strings (`parameterTypes[i]`). This detail wasn't handled previously and may prevent processing errors related to missing metadata expected by PMD.

3. **Additional AST Node Handling**:
   - The introduction of `ASTReferenceType` and `ASTClassOrInterfaceType` nodes, populated with specific types, ensures the method correctly manages method declarations. This can be pivotal in preventing processing errors if incorrect assumptions about AST structure caused previous failures.

These changes directly address potential mishandling of method information that could lead to null references during AST processing by tools like PMD.

The changes directly align with the commit message's intent to fix a NullPointerException, indicating a bug fix related to code processing. Therefore, this indicates a **Buggy** commit."
pmd,10015.json,8694a26cafe730add04695ec1917cd51bd4bbc4e,"@@ -1,4 +1,3 @@
     public TokenManager createTokenManager(Reader source) {
-	return new PLSQLTokenManager(source);
-
+        return new PLSQLTokenManager(IOUtil.skipBOM(source));
     }",Buggy,Fix #1167 Error while processing PLSQL file with BOM,Buggy,"The commit message indicates a fix for an error processing PLSQL files with a BOM (Byte Order Mark). The code change addresses this by using `IOUtil.skipBOM(source)` when creating a `PLSQLTokenManager`, which suggests an intention to skip the BOM from the source reader.

Analyzing the code:
- The original method directly passed the `source` to `PLSQLTokenManager`.
- The updated method now processes the `source` with `IOUtil.skipBOM(source)` before passing it to `PLSQLTokenManager`.

This change directly addresses the issue described in the commit message by ensuring the BOM is skipped, which likely resolves errors related to BOM presence. This is a typical solution for handling BOM in text processing systems, matching the description of an error fix.

Given that the code change aligns perfectly with the commit message, and that it specifically handles the mentioned error scenario, the modifications indeed indicate a bug fix.

Conclusively, the diff and the commit message describe a precise bug fix operation, so the assessment is **Buggy**."
pmd,3080.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,12 +1,12 @@
     public void publish(LogRecord logRecord) {
-        System.out.println(logRecord.getMessage());
+        System.out.println(FORMATTER.format(logRecord));
         if (logRecord.getThrown() != null) {
             // Use the same channel, to make sure that the stacktrace comes
             // after the message on the console (using printStackTrace
             // directly messes things up)
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             System.out.println(stringWriter.toString());
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message states ""Fixed formatting problems in loggers,"" and the code change replaces a simple log message printout with a formatted version using FORMATTER. This aligns with the message, addressing a formatting issue rather than a bug. The change improves output quality but doesn't fix a functional error. Hence, the answer is **NotBuggy**."
pmd,3084.json,6619e19d3558abf87dbf0b3f0047e355d593043b,"@@ -1,25 +1,25 @@
     public void publish(LogRecord logRecord) {
         //Map the log levels from java.util.logging to Ant
         int antLevel;
         Level level = logRecord.getLevel();
         if (level == Level.FINEST)
             antLevel = Project.MSG_DEBUG;   //Shown when -debug is supplied to Ant
         else if (level == Level.FINE || level == Level.FINER || level == Level.CONFIG)
             antLevel = Project.MSG_VERBOSE; //Shown when -verbose is supplied to Ant
         else if (level == Level.INFO)
             antLevel = Project.MSG_INFO;    //Always shown
         else if (level == Level.WARNING)
             antLevel = Project.MSG_WARN;    //Always shown
         else if (level == Level.SEVERE)
             antLevel = Project.MSG_ERR;     //Always shown
         else
             throw new IllegalStateException(""Unknown logging level"");   //shouldn't get ALL or NONE
         
-        antTask.log(logRecord.getMessage(), antLevel);
+        antTask.log(FORMATTER.format(logRecord), antLevel);
         if (logRecord.getThrown() != null) {
             StringWriter stringWriter = new StringWriter();
             PrintWriter printWriter = new PrintWriter(stringWriter, true);
             logRecord.getThrown().printStackTrace(printWriter);
             antTask.log(stringWriter.toString(), antLevel);
         }
     }",NotBuggy,"Fixed formatting problems in loggers

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@5905 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates a focus on fixing formatting problems in loggers. The code change modifies the way log messages are formatted before logging them via `antTask.log`. Previously, the message was directly passed to the `log` method. With this change, a `FORMATTER` is used to format the `LogRecord` before it is logged. This likely addresses a formatting issue where messages were not being logged correctly or consistently in the desired format.

Such changes typically relate to presentation or readability improvements in logs, rather than addressing a functional bug in the logic of the application (such as addressing incorrect logic, handling exceptions, etc.). Therefore, the change is more aligned with cosmetic or usability enhancements as described in the commit message.

Thus, this change does not align with a functional bug fix but rather a formatting improvement, as indicated in the commit message.

The assessment is **NotBuggy**."
pmd,7444.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
-    public boolean isArrayDeference() {
-        return isArrayDeference;
+    public boolean isArrayDereference() {
+        return isArrayDereference;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message mentions fixing a specific bug related to SimplifyConditional and null checks. However, the actual code change shown in the diff is merely a correction of a method name from `isArrayDeference` to `isArrayDereference`. This change corrects a typographical error but does not relate directly to the logic or behavior described in the commit message concerning null checks and `instanceof`. 

While the correction itself is valid and improves code readability, it does not align with the described bug fix related to functional logic in the commit message. This suggests that the diff may not fully encompass the changes described, or that the commit message describes other changes not included in the provided diff.

Given the provided information, the actual modification is a naming correction without functional impact, rather than a bug fix impacting behavior based on logical conditions as described in the commit message.

Therefore, the conclusion based on the diff alone is **NotBuggy**."
pmd,7443.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,3 +1,3 @@
     public void setIsArrayDereference() {
-        isArrayDeference = true;
+        isArrayDereference = true;
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message states a bug fix related to null checks and instanceof handling for array dereferences. However, the code change only corrects a spelling error in the property name from `isArrayDeference` to `isArrayDereference`. This modification addresses a typo in the code but does not directly correlate with the described bug fix regarding conditional checks and array dereference logic.

Based on the provided code diff, the change doesn’t reflect the commit message focus on simplifying conditionals around null checks and array dereferencing in conjunction with instanceof operations. Instead, it solely corrects a typographical error in a variable name, which, although necessary, isn’t indicative of a logic bug fix as described.

Therefore, the conclusion is **NotBuggy**."
pmd,9037.json,dc453cf103787e06a70385cb4eab1917f6286446,"@@ -1,43 +1,43 @@
     public Object visit(ASTStatementExpression node, Object data) {
         if (node.jjtGetNumChildren() != 3
                 || !(node.jjtGetChild(0) instanceof ASTPrimaryExpression)
                 || !(node.jjtGetChild(1) instanceof ASTAssignmentOperator)
                 || (((ASTAssignmentOperator) (node.jjtGetChild(1))).isCompound())
                 || !(node.jjtGetChild(2) instanceof ASTExpression)
                 || node.jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
                 || node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetNumChildren() == 0
         ) {
             return super.visit(node, data);
         }
 
         SimpleNode lhs = (SimpleNode) node.jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(lhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         SimpleNode rhs = (SimpleNode) node.jjtGetChild(2).jjtGetChild(0).jjtGetChild(0).jjtGetChild(0);
         if (!(rhs instanceof ASTName)) {
             return super.visit(node, data);
         }
 
         if (!lhs.getImage().equals(rhs.getImage())) {
             return super.visit(node, data);
         }
 
         if (lhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = lhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         if (rhs.jjtGetParent().jjtGetParent().jjtGetNumChildren() > 1) {
             Node n = rhs.jjtGetParent().jjtGetParent().jjtGetChild(1);
-            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDeference()) {
+            if (n instanceof ASTPrimarySuffix && ((ASTPrimarySuffix) n).isArguments() || ((ASTPrimarySuffix) n).isArrayDereference()) {
                 return super.visit(node, data);
             }
         }
 
         addViolation(data, node);
         return super.visit(node, data);
     }",NotBuggy,"Fixed bug 1242544 - SimplifyConditional no longer flags null checks that precede an instanceof involving an array dereference.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3728 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates that the bug fix involves addressing issues related to null checks and array dereference operations. The code diff reflects two primary changes: 

1. **Typographical Fix**: 
   - Corrects the misspelled method name from `isArrayDeference()` to `isArrayDereference()`. This correction is crucial, as it potentially corrects a logical error where the original code was incorrectly trying to invoke a method due to the misspelling. 

2. **Code Logic Assessment**: 
   - Based on the commit message, SimplifyConditional seems to have had an issue with flagging null checks incorrectly when dealing with arrays. However, from the diff, the logic remains largely unchanged except for the ensured invocation of the correct method due to the spelling fix. Therefore, no additional changes to the logic that evaluates null checks and array dereference operations are directly visible in the diff provided.

The commit message suggests significant internal logic related to object dereferencing and instanceof usage that no longer flags specific null checks incorrectly. Although only a typographical correction is visible, the surrounding logic probably addresses the bug as per the commit's intent.

Considering this context, even though the change appears minor (correction of a method name), it can have a significant impact if it corrects the method invocation logic. Therefore, the diff aligns with the commit message describing a bug fix in handling certain scenarios involving null checks and array dereferencing.

The conclusion is **Buggy** due to the fix related to a potentially critical logic validator, inferred from the methodological correction and the commit explanation."
pmd,8010.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,11 +1,19 @@
     public JavaTypeDefinition getGenericType(final String parameterName) {
-        final TypeVariable<?>[] typeParameters = clazz.getTypeParameters();
-        for (int i = 0; i < typeParameters.length; i++) {
-            if (typeParameters[i].getName().equals(parameterName)) {
-                return getGenericType(i);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            final TypeVariable<?>[] typeParameters = currTypeDef.clazz.getTypeParameters();
+            for (int i = 0; i < typeParameters.length; i++) {
+                if (typeParameters[i].getName().equals(parameterName)) {
+                    return currTypeDef.getGenericType(i);
+                }
             }
         }
 
-        throw new IllegalArgumentException(""No generic parameter by name "" + parameterName
-                                                   + "" on class "" + clazz.getSimpleName());
+        // throw because we could not find parameterName
+        StringBuilder builder = new StringBuilder(""No generic parameter by name "").append(parameterName);
+        for (JavaTypeDefinition currTypeDef = this; currTypeDef != null; currTypeDef = currTypeDef.enclosingClass) {
+            builder.append(""\n on class "");
+            builder.append(clazz.getSimpleName());
+        }
+
+        throw new IllegalArgumentException(builder.toString());
     }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"The changes made to the method `getGenericType` appear to fix a bug related to handling nested classes when the outer class is generic. 

Here is the step-by-step reasoning:

1. **Original Flaw**: Previously, the code only checked the type parameters of the current class (`clazz`). If the outer class was generic and the intended type parameter was declared there, it would not be found, leading to an incorrect result or exception.

2. **Changes Made**:
   - The updated code introduces a loop that iterates over the current class and its enclosing classes (`currTypeDef`).
   - For each level of the class hierarchy, the type parameters are retrieved and checked.
   - This ensures that if the type parameter is in any enclosing generic class, it will be found and returned correctly.
   - The exception message construction has been refined to provide clearer feedback by listing all the classes checked.

3. **Bug Fix Description**: The modification clearly addresses an issue where nested classes did not correctly identify generic types if those were declared in an outer class. The bug fix allows better handling of inherited and nested generic parameters, thus aligning with the commit message that mentioned a bug with generics in nested classes.

Given the context and nature of these modifications, they indicate a bug fix related to the incorrect handling of generic parameters in nested classes.

Thus, the conclusion is **Buggy**."
pmd,8270.json,87bbe9e5b34ddf7a66a3d5b462991cbb47746d80,"@@ -1,3 +1,3 @@
         public void visitInnerClassType(String name) {
-            parent.parseClassName(name);
+            // parent.parseClassName(name);
         }",Buggy,"Java, typedef: fix a bug with nested classes if the outter class is generic",Buggy,"The commit message indicates a bug fix related to nested classes in a context where the outer class is generic. In the code diff, a method `parseClassName` invocation is commented out within the `visitInnerClassType` method. 

Here's a step-by-step analysis:

1. **Context Understanding**: The `visitInnerClassType` method seems related to processing or visiting inner classes. Commenting out this line suggests a condition where calling `parseClassName` was incorrect or unnecessary, particularly when dealing with generic outer classes.

2. **Alignment with Commit Message**: The commit message specifies a bug fix related to nested classes and generic outer classes. This change directly relates to nested class processing, which aligns with the commit message.

3. **Potential Bug Indicators**:
    - **Error Avoidance**: Commenting out this method could prevent an error or bug when parsing class names for nested classes with generic outer types. This can be due to complications that generics introduce in type names compared to non-generic classes.
    - **Logical Correction**: If `parseClassName(name)` was previously causing incorrect behavior or exceptions under certain conditions (e.g., when encountering a generic outer class), this change exemplifies a logical correction.

In conclusion, the commented-out line represents a bug fix addressing nested class handling specifically when the outer class is generic. This update aligns with the commit message indicating a corrective change. Therefore, the assessment is **Buggy**."
pmd,7974.json,0b90fd01542a5a765cbb72e0b0c90185168d2331,"@@ -1,31 +1,32 @@
     public boolean isOnLeftHandSide() {
         // I detest this method with every atom of my being
         Node primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(
-                    ""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
-                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
+                    ""Found a NameOccurrence ("" + location + "") that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = ""
+                            + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent()
+                            + "" (location line "" + location.getBeginLine() + "" col "" + location.getBeginColumn() + "")"");
         }
 
         if (isStandAlonePostfix(primaryExpression)) {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         return !isCompoundAssignment(primaryExpression);
     }",Buggy,"[java] Fix processing error with Cast + MethodReference

Occurred in https://github.com/spring-projects/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/support/DefaultListableBeanFactory.java#L746",NotBuggy,"The commit message indicates a fix for a processing error related to casting and method references. Analyzing the diff, the primary change is in the error message of a runtime exception. Previously, the message did not include the exact location details of the `NameOccurrence`. The updated code adds the specific name occurrence along with its line and column numbers in the message. This doesn't change any logic but improves the error reporting for debugging.

While more detailed error messages could aid in identifying where processing errors occur, the essence of this modification doesn't directly correlate with fixing a bug like a logic or algorithmic error. Enhancements in error messages are typically improvements rather than bug fixes unless the inadequate messages caused misdiagnoses affecting functionality.

Thus, given the focus of the commit message and the nature of the code change, it is more about improving error diagnostics rather than fixing a processing error as described.

Conclusion: **NotBuggy**."
pmd,7846.json,9a940e0cc57577c730a54adf2582612581cb05af,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""Variable "" + node.getImage() + "":"" + node.getBeginLine();
+        return ""Variable symbol "" + node.getImage() + "" line "" + node.getBeginLine();
     }",Buggy,"fixed bug 660069; thx to mcclain looney for the bug report


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1323 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a bug has been fixed. The change in the code updates the `toString()` method, modifying how information about a node is displayed. This change improves the clarity or correctness of the output string by providing more detailed information, which suggests the previous message may have been ambiguous or incorrect. Although the nature of the bug 660069 is not explicitly detailed, this kind of string modification often resolves issues with logging, debugging, or display outputs that mislead or confuse users or developers.

Therefore, given that the change is to provide more precise information output, it's reasonable to attribute this as a bug fix dealing with output description consistency. Consequently, the conclusion is **Buggy**."
pmd,2946.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,4 +1,3 @@
         public String toString() {
-            SimpleNode n = (SimpleNode) node.jjtGetChild(1);
-            return n.getImage();
+            return node.getMethodName();
         }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message states that a specific bug related to ""NullAssignment"" and final fields has been fixed. However, the provided code diff shows a change in how a method name is obtained in the `toString()` method.

Let's analyze the changes:

1. **Original Code:**
   - It casts `node.jjtGetChild(1)` to `SimpleNode` and gets the image of this node, which is returned as a string. The logic involves navigating the tree structure and fetching an image from a child node.

2. **Modified Code:**
   - The code now directly returns `node.getMethodName()`. This change presumes that the `node` object has a method named `getMethodName()` which directly provides the needed string representation.

The commit message does not appear to be directly related to this code change. The modification shifts from using a child node's image to directly calling a method, which may simplify or optimize the process but does not match the message about handling null assignments or final fields related issues.

Therefore, the changes in the code diff do not address or reflect the bug in ""NullAssignment"" as described in the commit message. As such, the code changes do not indicate a bug fix related to the commit message.

The appropriate conclusion here is **NotBuggy**."
pmd,9191.json,4383ac357979bf353947351cac11d9586398ffbd,"@@ -1,15 +1,20 @@
     public Object visit(ASTNullLiteral node, Object data) {
-        if (lookUp(node) instanceof ASTStatementExpression) {
-            Node n = lookUp(node);
+        if (get5thParent(node) instanceof ASTStatementExpression) {
+            ASTStatementExpression n = (ASTStatementExpression)get5thParent(node);
+
+            if (isAssignmentToFinalField(n)) {
+                return data;
+            }
+
             if (n.jjtGetNumChildren() > 2 && n.jjtGetChild(1) instanceof ASTAssignmentOperator) {
                 RuleContext ctx = (RuleContext) data;
                 ctx.getReport().addRuleViolation(createRuleViolation(ctx, node));
             }
-        } else if (lookUp2(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp2(node), data, node);
-        } else if (lookUp(node) instanceof ASTConditionalExpression) {
-            checkTernary((ASTConditionalExpression)lookUp(node), data, node);
+        } else if (get4thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get4thParent(node), data, node);
+        } else if (get5thParent(node) instanceof ASTConditionalExpression) {
+            checkTernary((ASTConditionalExpression)get5thParent(node), data, node);
         }
 
         return data;
     }",Buggy,"Fixed bug 1235300 - NullAssignment no longer flags assignments to final fields.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3672 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message indicates a fix where NullAssignment no longer flags assignments to final fields. Analyzing the code changes in the diff:

1. **Original Functionality**: The initial code was flagging any occurrence of a null literal within certain expressions without checking if it was assigning a value to a final field.

2. **Code Change Analysis**:
   - The refactoring introduces specific checks for parent expressions by using methods `get4thParent` and `get5thParent` instead of `lookUp` and `lookUp2`.
   - It effectively adds a filter with `isAssignmentToFinalField(n)`, which skips further processing if the node is an assignment to a final field.

3. **Implication of Changes**: The new condition `isAssignmentToFinalField(n)` directly enforces the commit message claim, ensuring that null assignments to final fields are no longer flagged.

4. **Logical Correction**: This is a logical correction ensuring the tool behaves correctly according to the specifications mentioned, which is to ignore assignments to final fields in the rule being modified.

Based on the reasoning above, the changes indeed reflect a bug fix as per the commit message, because they refine the logic to only flag incorrect null assignments, skipping valid scenarios involving final fields. 

Thus, the correct conclusion is **Buggy**."
pmd,7976.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,3 +1,3 @@
-    private boolean postFixWithExceptions(SimpleNode primaryExpression) {
+    private boolean isStandAlonePostfix(SimpleNode primaryExpression) {
         return primaryExpression instanceof ASTPostfixExpression && primaryExpression.jjtGetParent() instanceof ASTStatementExpression && thirdChildHasDottedName(primaryExpression);
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message mentions a ""predecrement problem,"" but the diff shows a method name change from `postFixWithExceptions` to `isStandAlonePostfix`. This change does not address a pre-decrement or any other logical issue relating to decrement operations. Instead, it appears to be a refactor for clarity or readability. There is no evidence in the provided diff that an actual bug related to pre-decrement operations is being fixed.

Without more context, such as other parts of the code or additional changes related to decrement logic, it is difficult to conclude that this change directly addresses a specific bug.

Therefore, based on the provided diff and description, the conclusion is **NotBuggy**."
pmd,7974.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,32 +1,32 @@
     public boolean isOnLeftHandSide() {
         SimpleNode primaryExpression;
         if (location.jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent();
         } else if (location.jjtGetParent().jjtGetParent() instanceof ASTPrimaryExpression) {
             primaryExpression = (SimpleNode) location.jjtGetParent().jjtGetParent().jjtGetParent();
         } else {
             throw new RuntimeException(""Found a NameOccurrence that didn't have an ASTPrimary Expression as parent or grandparent.  Parent = "" + location.jjtGetParent() + "" and grandparent = "" + location.jjtGetParent().jjtGetParent());
         }
 
-        if (postFixWithExceptions(primaryExpression))  {
+        if (isStandAlonePostfix(primaryExpression))  {
             return true;
         }
 
         if (primaryExpression.jjtGetNumChildren() <= 1) {
             return false;
         }
 
         if (!(primaryExpression.jjtGetChild(1) instanceof ASTAssignmentOperator)) {
             return false;
         }
 
         if (isPartOfQualifiedName() /* or is an array type */) {
             return false;
         }
 
         if (isCompoundAssignment(primaryExpression)) {
             return false;
         }
 
         return true;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message indicates the resolution of a ""predecrement problem,"" but the changes in the code focus solely on method renaming without affecting logic. The modification replaces `postFixWithExceptions` with `isStandAlonePostfix`, suggesting improved naming for clarity, yet no functional fixes readily discernible. Despite refining readability or alignment, no bugs are explicitly rectified. Hence, the answer is **NotBuggy**."
pmd,8888.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,30 +1,30 @@
     private int initializedInConstructor(List usages, Set allConstructors) {
         int rc = MUTABLE, methodInitCount = 0;
         boolean foundUsage = false;
         Set consSet = new HashSet();
-        
+
         for (Iterator j = usages.iterator(); j.hasNext();) {
             foundUsage = true;
             NameOccurrence occ = (NameOccurrence)j.next();
-            if (occ.isOnLeftHandSide()) {
+            if (occ.isOnLeftHandSide() || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreDecrementExpression || occ.getLocation().jjtGetParent().jjtGetParent().jjtGetParent() instanceof ASTPreIncrementExpression) {
                 SimpleNode node = occ.getLocation();
                 SimpleNode constructor = (SimpleNode)node.getFirstParentOfType(ASTConstructorDeclaration.class);
                 if (constructor != null) {
                     consSet.add(constructor);
                 } else {
                     if (node.getFirstParentOfType(ASTMethodDeclaration.class) != null) {
                         methodInitCount++;
                     }
                 }
             }
         }
         if (!foundUsage || ((methodInitCount == 0) && consSet.isEmpty())) {
             rc = CHECKDECL;
         } else {
             allConstructors.removeAll(consSet);
             if (allConstructors.isEmpty() && (methodInitCount == 0)) {
                 rc = IMMUTABLE;
             }
         }
         return rc;
     }",Buggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message mentions fixing a ""predecrement problem."" The code change adds a condition to check for `ASTPreDecrementExpression` and `ASTPreIncrementExpression` while iterating over `usages`. These expressions can change the value of a variable, potentially causing the program logic to fail if not properly accounted for.

Before this commit, calls to methods like `isOnLeftHandSide()` might not cover pre-increment or pre-decrement expressions that modify a variable. By adding these additional checks, the function better captures cases where variable values change unexpectedly.

This change aligns with the commit message by addressing handling of expressions that modify variable state, which could lead to inaccurate behavior if left unchecked.

Therefore, this commit can be concluded as addressing a bug or logical error concerning the specific way pre-increment and pre-decrement expressions were not previously considered.

The conclusion is **Buggy**."
pmd,8892.json,78583ee897aa40fd474c85d59cccee016dfadfe0,"@@ -1,6 +1,5 @@
     private Set findAllConstructors(ASTUnmodifiedClassDeclaration node) {
-        List results = node.findChildrenOfType(ASTConstructorDeclaration.class);
-        HashSet set = new HashSet();
-        set.addAll(results);
+        Set set = new HashSet();
+        set.addAll(node.findChildrenOfType(ASTConstructorDeclaration.class));
         return set;
     }",NotBuggy,"Fixed a predecrement problem


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3005 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message mentions a ""predecrement problem,"" but the provided code diff doesn't show any predecrement (i.e., use of --operator) or related corrections. Instead, it simplifies code by directly initializing the `set` with the results of `findChildrenOfType`. No change here suggests a bug fix related to the predecrement, as neither predecrement nor decrement operations are present in the code snippet. This modification looks to be more of a refactor or simplification rather than a bug fix. 

Therefore, based on the information given, the answer is **NotBuggy**."
pmd,6536.json,1942e94cec6b6a30532716d14da5c7cd0cd8f075,"@@ -1,15 +1,15 @@
     private boolean isUnbalanced(String image, String pattern) {
         int occurance = 0;
-        int index = image.lastIndexOf(""="");
+        int index = image.indexOf(""="");
         index = image.indexOf(pattern, index + 1);
         while (index >= 0) {
             occurance++;
             index = image.indexOf(pattern, index + 1);
         }
 
         if ((occurance % 2) != 0) {
             return true;
         }
 
         return false;
     }",Buggy,Bug fix,Buggy,"To analyze the change:

1. The method `isUnbalanced` counts occurrences of a pattern following the last index of ""="" in the original code, and checks if the number of occurrences is odd.
2. The change updates the code to begin searching for the pattern after the first occurrence of ""="" rather than the last.

Reasoning for the change indicating a bug fix:

- **Behavioral Impact**: Originally, the code would miss patterns that appear before the last ""="". If it matters that patterns earlier in the string are counted, the original code might result in false negatives or positives for the ""unbalanced"" state.
- **Logical Correctness**: Fixing the start index to after the first ""="" ensures capturing all valid instances of the pattern placed after the initial key-value delimiter.
  
Given these details, the change aligns with the commit message indicating that this is a bug fix. The previous logic might have led to errors in determining the ""unbalanced"" status when patterns appeared before the last ""="". Thus, the analysis concludes with **Buggy**."
pmd,358.json,6156ba5de5f004b96bceb0cc8e44191c6876c7c9,"@@ -1,23 +1,23 @@
     private void validate() throws BuildException {
         if (formatters.isEmpty() && !printToConsole) {
             throw new BuildException(""No formatter specified; and printToConsole was false"");
         }
 
         for (Iterator i = formatters.iterator(); i.hasNext();) {
             Formatter f = (Formatter) i.next();
-            if (f.isToFileNull()) {
+            if (f.isNoOutputSupplied()) {
                 throw new BuildException(""Formatter toFile attribute is required"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', or '1.5'"");
         }
     }",Buggy,"Applied a patch from Wouter Zelle to clean up the Ant Formatter class, fix a TextRenderer bug, and make toConsole cleaner.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4292 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The commit message mentions fixing a bug in TextRenderer and cleaning up the Ant Formatter class. The code change involves a rename from `isToFileNull()` to `isNoOutputSupplied()`. This suggests a shift in logic check, altering how formatter output absence is determined. Given no apparent change in logic, this likely clarifies semantics rather than fixing a bug. Without evidence explicitly pertaining to a bug related to `TextRenderer` or different behavior due to this change, the diff doesn't directly resolve a reported bug. Therefore, the answer is **NotBuggy**."
pmd,412.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,12 +1,10 @@
     private void logRulesUsed(RuleSets rules) {
         log(""Using these rulesets: "" + ruleSetFiles, Project.MSG_VERBOSE);
 
         RuleSet[] ruleSets = rules.getAllRuleSets();
-        for (int j = 0; j < ruleSets.length; j++) {
-            RuleSet ruleSet = ruleSets[j];
-
+        for (RuleSet ruleSet : ruleSets) {
             for (Rule rule: ruleSet.getRules()) {
                 log(""Using rule "" + rule.getName(), Project.MSG_VERBOSE);
             }
         }
     }",NotBuggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",NotBuggy,"The diff indicates a change from an array indexing-based for-loop to an enhanced for-loop (for-each syntax) over `ruleSets`. This change enhances readability but does not involve any functional bug fix. Since the commit message refers to fixing a bug about path resolution in an Ant task, which is unrelated to the provided code changes, the changes in the code do not directly relate to the commit message. They reflect a refactor for cleaner code rather than a bug fix. Thus, the provided code diff does not align with the bug-fixing aspect mentioned in the commit message.

The answer is **NotBuggy**."
pmd,358.json,604a45d941cf2042c5c548490ef4401ad54080c9,"@@ -1,19 +1,31 @@
     private void validate() throws BuildException {
         // TODO - check for empty Formatters List here?
         for (Formatter f: formatters) {
             if (f.isNoOutputSupplied()) {
                 throw new BuildException(""toFile or toConsole needs to be specified in Formatter"");
             }
         }
 
         if (ruleSetFiles == null) {
             if (nestedRules.isEmpty()) {
                 throw new BuildException(""No rulesets specified"");
             }
             ruleSetFiles = getNestedRuleSetFiles();
         }
 
+        // convert relative paths and substitute env variables/properties
+        final StringBuffer sb = new StringBuffer();
+        for(String s: ruleSetFiles.split("","")) {
+            Path p = new Path(getProject());
+            p.setPath(getProject().replaceProperties(s));
+            if (sb.length() > 0) {
+                sb.append(',');
+            }
+            sb.append(p);
+        }
+        ruleSetFiles = sb.toString();
+
         if (!targetJDK.equals(""1.3"") && !targetJDK.equals(""1.4"") && !targetJDK.equals(""1.5"") && !targetJDK.equals(""1.6"") && !targetJDK.equals(""1.7"") && !targetJDK.equals(""jsp"")) {
             throw new BuildException(""The targetjdk attribute, if used, must be set to either '1.3', '1.4', '1.5', '1.6', '1.7' or 'jsp'"");
         }
     }",Buggy,"Fixed bug 1943204 - Ant task: <ruleset> path should be relative to Ant basedir


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6441 51baf565-9d33-0410-a72c-fc3788e3496d",Buggy,"The commit message states that the change fixes a bug related to the `<ruleset>` path being relative to the Ant `basedir`. The code update includes new logic within the `validate` method that processes `ruleSetFiles`. This section converts each path, replacing environment variables and properties, and appears to handle paths concerning the project. This aligns with fixing the bug, as it adjusts how paths are interpreted relative to the project base directory. These changes relate directly to a described problem, suggesting a pre-existing bug in path handling within the Ant task.

Thus, the changes and the commit message are consistent, indicating a bug fix. The answer is **Buggy**."
pmd,6753.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,8 +1,10 @@
     public Object visit(ASTPrimaryExpression node, Object data) {
-        if (isForeignAttributeOrMethod(node) && (isAttributeAccess(node)
-            || isMethodCall(node) && isForeignGetterSetterCall(node))) {
-
-            ((MutableInt) data).increment();
+        if (isForeignAttributeOrMethod(node)) {
+            if (isAttributeAccess(node)) {
+                ((MutableInt) data).increment();
+            } else {
+                ((MutableInt) data).add(countForeignGetterSetterCalls(node));
+            }
         }
         return super.visit(node, data);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"The commit message indicates a fix related to ATFD (Access to Foreign Data) calculation. The change in the code revolves around how method call chains are considered when determining ATFD.

Here's the step-by-step analysis of the diff:

1. **Original Code Behavior:**
   - The original condition had a compound `if` statement that checked:
     - `isForeignAttributeOrMethod(node)`
     - Whether it is an attribute access or a method call that is a foreign getter/setter call.
   - If all conditions were satisfied, the foreign attribute/method count was incremented by one.

2. **Modified Code Behavior:**
   - The new code separates the checks:
     - If it is an attribute access, it increments the count by one as before.
     - If it is not an attribute access, it assumes it must be a method call and uses `countForeignGetterSetterCalls(node)` to determine how many to add to the count.

3. **Understanding the Change:**
   - The update correctly considers method call chains by invoking `countForeignGetterSetterCalls(node)`, presumably counting the number of method calls in the chain that qualify as accessing foreign attributes.
   - This change aligns with the commit message stating that ""Method call chains are now considered,"" which suggests an enhancement of the logic to accurately capture foreign data accesses in method chains—previously missed.

4. **Context:**
   - Fixing issue #1910, as noted in the commit, indicates that this was a recognized problem.

5. **Conclusion:**
   - The logical correction in the code indicates that the previous implementation didn't effectively account for method call chains, which was likely a bug or overlooked scenario in the ATFD calculation.

Hence, the code changes reflect a logical correction to appropriately account for all cases of foreign data access, corresponding directly with the commit's stated intention to fix the issue and adjust ATFD calculations. Therefore, the changes are consistent with a bug fix.

The answer is **Buggy**."
pmd,6754.json,1ef5079b29401bdb0df6e773f0160fa54f779018,"@@ -1,6 +1,4 @@
     private boolean isForeignGetterSetterCall(ASTPrimaryExpression node) {
-
         String methodOrAttributeName = getMethodOrAttributeName(node);
-
-        return methodOrAttributeName != null && StringUtils.startsWithAny(methodOrAttributeName, ""get"", ""is"", ""set"");
+        return isForeignGetterSetterCall(methodOrAttributeName);
     }",Buggy,"[java] ATFD calculation problem

* Fixes #1910
* Method call chains are now considered",Buggy,"The commit message states a fix for an ATFD (Access to Foreign Data) calculation problem and specifically mentions that method call chains are now considered. The diff shows a refactor in the `isForeignGetterSetterCall` method, replacing inline logic with a call to a method (likely overloaded or newly implemented elsewhere) titled `isForeignGetterSetterCall(String)`.

1. **Commit Message Analysis**: 
   - The message highlights fixing an issue with the ATFD calculation by considering method call chains.

2. **Code Diff Analysis**: 
   - The inline logic checking for method or attribute names starting with ""get"", ""is"", or ""set"" has been replaced with a method call to `isForeignGetterSetterCall`, likely a more comprehensive implementation.
   - The update suggests inclusion of a sophisticated mechanism to detect method or attribute patterns, possibly handling chaining or more complex logic beyond simple prefix checks.

3. **Relevance to Message**:
   - Aligns well, as this structural change seems intended to address incorrect ATFD calculation due to overlooked patterns involving method call chains.
   - The modification implies deeper logic handling that fits the described enhancement for functionality.

The source code changes reflect an alignment with the mentioned bug fix around accurately calculating ATFD by possibly enhancing the logic for recognizing getter/setter chains. Therefore, the conclusion is **Buggy**."
hbase,27505.json,d589b7238257c78dc8616e47ab5bb3e2309a5237,"@@ -1,3 +1,3 @@
-    public Throwable getCause() {
+    public synchronized Throwable getCause() {
       return cause;
     }",Buggy,"HBASE-19847 Fix findbugs and error-prone warnings in hbase-thrift (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>",Buggy,"The commit message indicates fixing warnings associated with findbugs and error-prone tools. The change synchronizes the `getCause()` method, which addresses potential thread-safety warnings. Synchronization can prevent race conditions in concurrent environments, which can be flagged as a bug by static analysis tools. Thus, this change aligns with the commit message relating to fixing warnings, especially those concerning potential bugs due to concurrency issues.

The changes indicate a focus on improving the robustness and thread safety of the code, which is related to resolving potential bugs or warnings identified by analysis tools.

Therefore, the conclusion is **Buggy**."
hbase,7228.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,8 +1,8 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (now < nextRefillTime) {
       return 0;
     }
     nextRefillTime = now + super.getTimeUnitInMillis();
     return limit;
   }",NotBuggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message states the fix of a ""refill bug,"" and the provided code diff shows the removal of an unused parameter `available` from the `refill` method. This suggests that the method's logic erroneously took or was influenced by an incorrect parameter that may have contributed to a bug previously introduced in ""HBASE-13686."" By updating this signature, the method simplifies its logic, which could correct a misbehavior related to how values were refilled if the unused parameter was leading to incorrect operations within a broader application context.

The update also maintains the logic around checking `now` against `nextRefillTime`, ensuring the refill mechanics by `limit` stay intact without additional unnecessary input.

Given that this change clearly addresses what is purported to be a bug related to method behavior (as indicated by the commit message), it aligns with fixing incorrect or undesired behavior in the application.

Thus, the assessment based on the commit message and the code change provided leads to the conclusion: **Buggy**."
hbase,7525.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,16 +1,16 @@
   public synchronized boolean canExecute(final long amount) {
-    long refillAmount = refill(limit, avail);
+    long refillAmount = refill(limit);
     if (refillAmount == 0 && avail < amount) {
       return false;
     }
     // check for positive overflow
     if (avail <= Long.MAX_VALUE - refillAmount) {
       avail = Math.max(0, Math.min(avail + refillAmount, limit));
     } else {
       avail = Math.max(0, limit);
     }
     if (avail >= amount) {
       return true;
     }
     return false;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message mentions a bug fix related to a ""refill bug."" The diff shows that the method `refill` initially took two parameters, `limit` and `avail`, but now only takes `limit`. This indicates a possible correction in how the refilling logic should compute `refillAmount`, which likely was dependent only on `limit`. The change ensures the method logic aligns with the intended refill operation, which was likely addressing an incorrect parameter usage from the earlier implementation causing the bug.

These changes indicate the patch addresses a specific operational bug, as evidenced by the correction in the parameters of the `refill` method. 

Thus, the conclusion is **Buggy**."
hbase,7389.json,84a50393ee56d09abb68f54b44b64f5279bd33c9,"@@ -1,15 +1,15 @@
-  public long refill(long limit, long available) {
+  public long refill(long limit) {
     final long now = EnvironmentEdgeManager.currentTime();
     if (nextRefillTime == -1) {
       // Till now no resource has been consumed.
       nextRefillTime = EnvironmentEdgeManager.currentTime();
       return limit;
     }
 
     long delta = (limit * (now - nextRefillTime)) / super.getTimeUnitInMillis();
     if (delta > 0) {
       this.nextRefillTime = now;
-      return Math.min(limit, available + delta);
+      return Math.min(limit, delta);
     }
     return 0;
   }",Buggy,HBASE-13888 Fix refill bug from HBASE-13686 (Guanghao Zhang),Buggy,"The commit message indicates a bug fix related to a refill mechanism. Here's the analysis of the code change:

1. **Function Signature Change**: The method `refill` signature is altered from `refill(long limit, long available)` to `refill(long limit)`, removing the `available` parameter. This is an immediate indication that the previous logic using `available` was incorrect or unnecessary.

2. **Logic Adjustment**: Inside the method, the return statement is adjusted from `Math.min(limit, available + delta)` to `Math.min(limit, delta)`. The original logic added `delta` to `available`, but with the removal of `available`, the logic now only considers `delta`.

3. **Bug Fix and Logic Correction**: The change likely indicates that `available` was improperly used in previous logic, leading to erroneous refill calculations. By removing the `available` parameter and its usage, the update directly ties the calculation to `delta`, improving the correctness of the resource refill logic.

Therefore, this alteration aligns with the commit message concerning a bug fix, as the code motion targets correcting the refill mechanism's behavior by eliminating an inaccurate parameter and its faulty application.

Concluding the analysis, this modification is indeed a bug fix. The answer is **Buggy**."
hbase,1189.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,5 +1,4 @@
   public static void main(String[] args) throws Exception {
-    if (conf == null) conf = HBaseConfiguration.create();
-    int ret = ToolRunner.run(conf, new ReplicationSyncUp(), args);
+    int ret = ToolRunner.run(HBaseConfiguration.create(), new ReplicationSyncUp(), args);
     System.exit(ret);
   }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,"The commit message mentions fixing a ""findbugs"" error. The affected section of the code is the initialization of a configuration object (`conf`). Originally, the configuration was checked for nullity before initialization, but this check is removed now, and the configuration creation is directly passed as a parameter to `ToolRunner.run()`. This change suggests that the prior code may have had a redundant or unsafe `null` check, which could have triggered a findbugs warning related to potential null dereference or incorrect lazy initialization logic.

Given that the commit message specifically entails resolving a findbugs error, which commonly deals with potential bugs in code due to possible runtime issues, this change aligns with fixing such a possible defect or inefficiency.

Hence, the change directly relates to what the commit message states, indicating that an actual bug related to configuration initialization and potential mishandling was fixed.

Therefore, the conclusion is **Buggy**."
hbase,1193.json,2beda62a10f0828eb10cec28b0ba53246cd0b671,"@@ -1,3 +1,3 @@
     public Configuration getConfiguration() {
-      return conf;
+      return getConf();
     }",Buggy,"HBASE-20083 Fix findbugs error for ReplicationSyncUp
",Buggy,"The commit message refers to fixing a FindBugs error, which typically involves addressing potential defects in code, such as bad practices or potential runtime issues. In this diff:

- The method `getConfiguration()` previously returned the `conf` field directly.
- The change modifies it to return the result of `getConf()`. 

This alteration suggests that `getConf()` might include additional logic, validation, or encapsulation that direct access to the `conf` field bypasses. This kind of change often addresses encapsulation issues or lazy initialization needs, which are common concerns highlighted by static analysis tools like FindBugs.

Given that the purpose of the change aligns with the commit's indication of addressing a FindBugs error, this suggests a bug-related correction concerning object management or ensuring consistent access patterns.

Therefore, the modifications are consistent with the intent to fix a bug-like issue, based on the tools involved.

The conclusion is **Buggy**."
hbase,5737.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,7 +1,10 @@
   private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {
     // If meta, move it last otherwise other unassigns fail because meta is not
     // online for them to update state in. This is dodgy. Needs to be made more
     // robust. See TODO below.
-    if (hri.isMetaRegion()) regions.addLast(hri);
-    else regions.addFirst(hri);
+    if (hri.isMetaRegion()) {
+      regions.addLast(hri);
+    } else {
+      regions.addFirst(hri);
+    }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The provided diff shows changes to add braces to an `if-else` statement for better readability and compliance with coding standards. This type of modification is consistent with addressing Checkstyle errors and does not alter the logic of the code or fix any bugs. Considering the commit message specifically mentions fixing Checkstyle errors and making adjustments to ensure Checkstyle can enforce style violations, the changes are related to code quality improvement rather than bug fixes.

Therefore, the code modification aligns with the commit message as it is primarily focused on improving code style and formatting without indicating any bugs being fixed.

The answer is **NotBuggy**."
hbase,5705.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,8 +1,11 @@
   static Set<Address> getOnlineServers(final MasterServices master) {
     Set<Address> onlineServers = new HashSet<Address>();
-    if (master == null) return onlineServers;
+    if (master == null) {
+      return onlineServers;
+    }
+
     for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {
       onlineServers.add(server.getAddress());
     }
     return onlineServers;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates fixing Checkstyle errors, which are typically related to code formatting, style, and conventions rather than functional bugs. The code diff shows that a single-line `if` statement was expanded to include braces, likely to comply with Checkstyle rules for code clarity and readability. These changes align with the commit message as they address styling issues rather than logical or functional errors in the code.

Given that this update pertains to code style improvements rather than fixing a bug, the conclusion is **NotBuggy**."
hbase,5782.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,4 +1,7 @@
   public boolean isOnline() {
-    if (this.rsGroupInfoManager == null) return false;
+    if (this.rsGroupInfoManager == null) {
+      return false;
+    }
+
     return this.rsGroupInfoManager.isOnline();
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates that the changes relate to fixing Checkstyle errors and enabling Checkstyle to fail on violations. Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard or set of rules.

In the given diff, the code modification involves altering the formatting of an `if` statement to adhere to a coding style guideline by placing the `return false;` statement inside a block with braces, ensuring consistency and readability.

This change is related to style and code quality improvements rather than fixing logic errors or bugs in the program. There are no error-handling updates, logical corrections, exception-handling improvements, or other signs of a direct bug fix.

Therefore, this change aligns with the commit message regarding Checkstyle error fixes and does not involve a bug fix. The appropriate conclusion is **NotBuggy**."
hbase,5736.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,16 +1,19 @@
   private List<RegionInfo> getRegions(final Address server) {
     LinkedList<RegionInfo> regions = new LinkedList<>();
     for (Map.Entry<RegionInfo, ServerName> el :
         master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {
-      if (el.getValue() == null) continue;
+      if (el.getValue() == null) {
+        continue;
+      }
+
       if (el.getValue().getAddress().equals(server)) {
         addRegion(regions, el.getKey());
       }
     }
     for (RegionStateNode state : master.getAssignmentManager().getRegionsInTransition()) {
       if (state.getRegionLocation().getAddress().equals(server)) {
         addRegion(regions, state.getRegionInfo());
       }
     }
     return regions;
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The commit message indicates that the changes are meant to fix Checkstyle errors in the `hbase-rsgroup` module—not to fix a bug. The code diff shows an alteration in formatting (the addition of braces for the `if` statement), which aligns with Checkstyle requirements for coding standards to improve code readability and maintainability. There is no functional change or logic correction present in this commit. Therefore, the modifications are consistent with the commit message and do not indicate a bug fix.

**NotBuggy**"
hbase,5743.json,c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad,"@@ -1,58 +1,61 @@
   public boolean balanceRSGroup(String groupName) throws IOException {
     ServerManager serverManager = master.getServerManager();
     AssignmentManager assignmentManager = master.getAssignmentManager();
     LoadBalancer balancer = master.getLoadBalancer();
 
     synchronized (balancer) {
       // If balance not true, don't run balancer.
-      if (!((HMaster) master).isBalancerOn()) return false;
+      if (!((HMaster) master).isBalancerOn()) {
+        return false;
+      }
+
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().preBalanceRSGroup(groupName);
       }
       if (getRSGroupInfo(groupName) == null) {
         throw new ConstraintException(""RSGroup does not exist: ""+groupName);
       }
       // Only allow one balance run at at time.
       Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);
       if (groupRIT.size() > 0) {
         LOG.debug(""Not running balancer because "" + groupRIT.size() + "" region(s) in transition: "" +
           StringUtils.abbreviate(
               master.getAssignmentManager().getRegionStates().getRegionsInTransition().toString(),
               256));
         return false;
       }
       if (serverManager.areDeadServersInProgress()) {
         LOG.debug(""Not running balancer because processing dead regionserver(s): "" +
             serverManager.getDeadServers());
         return false;
       }
 
       //We balance per group instead of per table
       List<RegionPlan> plans = new ArrayList<>();
       for(Map.Entry<TableName, Map<ServerName, List<RegionInfo>>> tableMap:
           getRSGroupAssignmentsByTable(groupName).entrySet()) {
         LOG.info(""Creating partial plan for table "" + tableMap.getKey() + "": ""
             + tableMap.getValue());
         List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());
         LOG.info(""Partial plan for table "" + tableMap.getKey() + "": "" + partialPlans);
         if (partialPlans != null) {
           plans.addAll(partialPlans);
         }
       }
       long startTime = System.currentTimeMillis();
       boolean balancerRan = !plans.isEmpty();
       if (balancerRan) {
         LOG.info(""RSGroup balance "" + groupName + "" starting with plan count: "" + plans.size());
         for (RegionPlan plan: plans) {
           LOG.info(""balance "" + plan);
           assignmentManager.moveAsync(plan);
         }
         LOG.info(""RSGroup balance "" + groupName + "" completed after "" +
             (System.currentTimeMillis()-startTime) + "" seconds"");
       }
       if (master.getMasterCoprocessorHost() != null) {
         master.getMasterCoprocessorHost().postBalanceRSGroup(groupName, balancerRan);
       }
       return balancerRan;
     }
   }",NotBuggy,"HBASE-19601 Fixed Checkstyle errors in hbase-rsgroup and enabled Checkstyle to fail on violations
",NotBuggy,"The changes in the diff are focused on improving code clarity and conformity to checkstyle rules. This is evident from:

1. Braces: A conditional statement `if (!((HMaster) master).isBalancerOn()) return false;` was modified to use braces for the return statement. This change addresses checkstyle concerns regarding control structures without braces and improves readability and maintainability.

2. Message Context: The commit message explicitly mentions fixing checkstyle errors in a specific module (`hbase-rsgroup`) and enabling checkstyle to fail on violations. There is no indication of fixing functional bugs or logical corrections within the code itself.

3. No Changes Identified as Functional: The code functionality remains the same as there are no changes in logic, algorithms, or error-handling mechanisms.

Based on the commit message and the changes, the focus is clearly on addressing code style issues, not fixing a bug in the functional behavior of the application.

Therefore, the conclusion is **NotBuggy**."
hbase,3328.json,d272ac908ceb4696e05431066ae02d953fa6fc9d,"@@ -1,30 +1,30 @@
   protected void chore() {
-    if (!connected) {
+    if (!isConnected()) {
       return;
     }
 
     List<ServerName> sns = generateDeadServersListToSend();
     if (sns.isEmpty()) {
       // Nothing to send. Done.
       return;
     }
 
     final long curTime = EnvironmentEdgeManager.currentTime();
     if (lastMessageTime > curTime - messagePeriod) {
       // We already sent something less than 10 second ago. Done.
       return;
     }
 
     // Ok, we're going to send something then.
     lastMessageTime = curTime;
 
     // We're reusing an existing protobuf message, but we don't send everything.
     // This could be extended in the future, for example if we want to send stuff like the
     //  hbase:meta server name.
     publisher.publish(ClusterMetricsBuilder.newBuilder()
       .setHBaseVersion(VersionInfo.getVersion())
       .setClusterId(master.getMasterFileSystem().getClusterId().toString())
       .setMasterName(master.getServerName())
       .setDeadServerNames(sns)
       .build());
   }",NotBuggy,"HBASE-20069 fix existing findbugs errors in hbase-server; ADDENDUM Address review
",NotBuggy,"The commit message suggests fixing findbugs errors. The code diff changes the conditional check from a boolean field, `connected`, to a method call, `isConnected()`. This potentially addresses a findbugs error related to encapsulation, direct field access, or synchronization issues that static analysis tools like FindBugs might flag, particularly if `connected` is private or involves multi-threading scenarios.

By using a method, we improve encapsulation, potentially introduce synchronization (if necessary), and could also allow the method to contain additional logic to determine the connected state, beyond what a simple boolean flag allows.

Thus, the change aligns with the commit message about fixing existing findbugs errors, which indicates that the change may be correcting a logical oversight or enhancing the robustness of the condition being checked. However, without specific information directly tying this change to a bug or a runtime error, it is primarily aligning with code quality or best practice improvements.

Given these points, the changes indicate an enhancement for code robustness rather than directly addressing a functional bug. Therefore, my conclusion is **NotBuggy**."
hbase,36406.json,aeffca497bf36ea12f89a5f92d2f918b010741fc,"@@ -1,6 +1,6 @@
   public void writeTo(Object object, Class<?> type, Type genericType,
       Annotation[] annotations, MediaType mediaType,
       MultivaluedMap<String, Object> httpHeaders, OutputStream outStream)
       throws IOException, WebApplicationException {
-    outStream.write(object.toString().getBytes());
+    outStream.write(Bytes.toBytes(object.toString()));
   }",Buggy,"HBASE-19846 Fix findbugs and error-prone warnings in hbase-rest (branch-2)

Signed-off-by: tedyu <yuzhihong@gmail.com>
",NotBuggy,"The commit message indicates that the change aims to address findbugs and error-prone warnings in a specific module. The change in the diff is focused on converting an object to a byte array before writing it to an OutputStream. The original code uses `object.toString().getBytes()`, which relies on the platform's default character encoding. The updated code uses `Bytes.toBytes(object.toString())`, which likely specifies a particular charset, possibly ASCII or UTF-8, ensuring consistent behavior across different environments.

By changing how the string is converted to bytes, this modification addresses a potential issue flagged by static analysis tools related to character encoding, which can lead to inconsistent behavior if the local default character encoding differs from what is expected. This is an improvement in code robustness and would fall under fixing warnings as stated in the commit message.

Since the modification addresses a warning related to encoding issues rather than fixing a runtime error or logic flaw, the change is primarily about improving the correctness and portability of the code rather than fixing an existing bug.

**NotBuggy**"
hbase,34490.json,2413a10e6718afaf74185078f0320f2e9e47d273,"@@ -1,47 +1,48 @@
   public List<InputSplit> getSplits(JobContext context) throws IOException {
     boolean closeOnFinish = false;
 
     // Just in case a subclass is relying on JobConfigurable magic.
     if (table == null) {
       initialize(context);
       closeOnFinish = true;
     }
 
     // null check in case our child overrides getTable to not throw.
     try {
       if (getTable() == null) {
         // initialize() must not have been implemented in the subclass.
         throw new IOException(INITIALIZATION_ERROR);
       }
     } catch (IllegalStateException exception) {
       throw new IOException(INITIALIZATION_ERROR, exception);
     }
 
     try {
       List<InputSplit> splits = oneInputSplitPerRegion();
 
       // set same number of mappers for each region
       if (context.getConfiguration().get(NUM_MAPPERS_PER_REGION) != null) {
         int nSplitsPerRegion = context.getConfiguration().getInt(NUM_MAPPERS_PER_REGION, 1);
         List<InputSplit> res = new ArrayList<>();
         for (int i = 0; i < splits.size(); i++) {
           List<InputSplit> tmp = createNInputSplitsUniform(splits.get(i), nSplitsPerRegion);
           res.addAll(tmp);
         }
         return res;
       }
 
       //The default value of ""hbase.mapreduce.input.autobalance"" is false.
-      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false) {
-        long maxAveRegionSize = context.getConfiguration().getInt(MAX_AVERAGE_REGION_SIZE, 8*1073741824);
+      if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false)) {
+        long maxAveRegionSize = context.getConfiguration()
+            .getLong(MAX_AVERAGE_REGION_SIZE, 8L*1073741824); //8GB
         return calculateAutoBalancedSplits(splits, maxAveRegionSize);
       }
 
       // return one mapper per region
       return splits;
     } finally {
       if (closeOnFinish) {
         closeTable();
       }
     }
   }",NotBuggy,"HBASE-19195 error-prone fixes for client, mr, and server
",Buggy,"The commit message indicates that the changes involve error-prone fixes, which implies addressing potential coding errors or anti-patterns. Here's the analysis of the changes:

1. **Boolean Comparison Simplification:**
   - The expression `if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false) != false)` is simplified to `if (context.getConfiguration().getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false))`.
   - This change removes a redundant comparison against `false`, which is a common improvement suggested by static analysis tools for cleaner and more readable code. This does not alter the logic but prevents unnecessary complexity.

2. **Correct Method for Retrieving Configuration Value:**
   - Modified from `context.getConfiguration().getInt(MAX_AVERAGE_REGION_SIZE, 8*1073741824)` to `context.getConfiguration().getLong(MAX_AVERAGE_REGION_SIZE, 8L*1073741824)`.
   - The change from `getInt` to `getLong` with the default value cast as a long (using `8L`) suggests a potential bug fix. If `MAX_AVERAGE_REGION_SIZE` was intended to be a long, using `getInt` could previously have truncated larger numbers, leading to incorrect behavior.

3. **Overall Changes:**
   - The diff aligns with activities often characterized by ""error-prone"" fixes, such as ensuring proper data type usage and eliminating common logical pitfalls with boolean expressions.
   - While the first change (boolean comparison simplification) is more of a code clean-up, the second change (using `getLong` instead of `getInt`) strategically addresses a potentially incorrect data handling bug if the value were ever intended to be a `long` and could exceed an `int` boundary.

Given these points, the changes not only improve the code's readability and maintainability but likely resolve a subtle bug related to how configuration values are retrieved and handled, particularly with respect to sizing calculations.

Thus, the suitable conclusion from this analysis is **Buggy**, indicating a bug fix."
hbase,5669.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,7 +1,6 @@
   public static void resetCounters() throws Exception {
     Class<?> cl = (new SplitLogCounters()).getClass();
-    Field[] flds = cl.getDeclaredFields();
-    for (Field fld : flds) {
+    for (Field fld : cl.getDeclaredFields()) {
       ((AtomicLong)fld.get(null)).set(0);
     }
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The change streamlines code by inlining `getDeclaredFields()` in the loop definition. This doesn't alter logic or fix errors. The message mentions addressing false positives from the FindBugs tool, indicating a likely intent to clean up static analysis warnings rather than actual bugs. This specific change alone doesn’t directly indicate a bug fix, as functionality remains unchanged. The answer is **NotBuggy**."
hbase,10177.json,9fac4877d3072c3589370c9d0446342ee2658ab6,"@@ -1,6 +1,5 @@
   public void start() {
     worker = new Thread(null, this, ""SplitLogWorker-"" + serverName);
     exitWorker = false;
     worker.start();
-    return;
   }",NotBuggy,"HBASE-5598 Analyse and fix the findbugs reporting by QA and add invalid bugs into findbugs-excludeFilter file

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1425351 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a cleanup effort related to FindBugs reports, specifically reducing false positives by eliminating or excluding non-issues from reports. The diff shows a trivial change: removing an unnecessary return statement at the end of a void method. This cleanup is in line with the commit message's goal to address and improve code analysis quality but does not involve fixing any functional bugs or defects in the application's logic or error handling.

Given the content of the diff and the commit message, the change does not fix a bug but rather improves code quality and adheres to best practices as related to static analysis warnings. Thus, the appropriate conclusion for this commit would be **NotBuggy**."
hbase,28329.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,8 +1,11 @@
-  public Get setTimeStamp(long timestamp) {
+  public Get setTimeStamp(long timestamp)
+  throws IOException {
     try {
       tr = new TimeRange(timestamp, timestamp+1);
     } catch(IOException e) {
-      // Will never happen
+      // This should never happen, unless integer overflow or something extremely wrong...
+      LOG.error(""TimeRange failed, likely caused by integer overflow. "", e);
+      throw e;
     }
     return this;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixing potential bugs in exception handlers. The diff shows a change in the `setTimeStamp` method, where an `IOException` is now declared to be thrown, and an error is logged if the exception is caught. Previously, the comment claimed the exception would ""never happen,"" but now it acknowledges potential integer overflow or other issues, logs the error, and throws the exception, allowing it to be handled appropriately.

- **Error Handling Update**: The method is updated to declare that it throws an `IOException`, and a log message is added for better diagnostics.
- **Logical Correction**: The comment is updated to reflect a realistic scenario where the exception might occur, addressing a ""potential bug.""

The changes align with the commit message regarding fixing potential bugs, particularly in exception handling. The inclusion of logging and not suppressing the exception signified a good practice to handle previously unanticipated errors.

Considering these changes align with fixing potential bugs in exception handlers, the conclusion is **Buggy**."
hbase,40756.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,25 +1,26 @@
   public int getNumberOfRunningProcess(){
     if (!isUnix()){
       return 0;
     }
 
     BufferedReader input = null;
     try {
       int count = 0;
       Process p = Runtime.getRuntime().exec(""ps -e"");
       input = new BufferedReader(new InputStreamReader(p.getInputStream()));
       while (input.readLine() != null) {
         count++;
       }
       return count - 1; //  -1 because there is a headline
     } catch (IOException e) {
       return -1;
     }  finally {
       if (input != null){
         try {
           input.close();
-        } catch (IOException ignored) {
+        } catch (IOException e) {
+          LOG.warn(""Not able to close the InputStream"", e);
         }
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixing potential bugs in exception handlers. The code changes improve exception handling: instead of ignoring IOException in the finally block, they log a warning with details. This change suggests handling exceptions more robustly, aligning with the commit message mentioning fixes to potential bugs in exception handlers.

Let's analyze the changes step by step:
1. **Original Code**:
    - Exceptions during input stream closing are caught but ignored.
    - This might lead to unlogged resources misuse or leaks.

2. **Modified Code**:
    - Catches IOException in the finally block, logs a warning with stack trace information.
    - This ensures that developers are informed if the input stream fails to close, which can help diagnose underlying issues.

Since the change aligns with improving exception handling—a potential bug in error handling—it confirms the commit's intent to fix such issues.

The conclusion is **Buggy**."
hbase,40081.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,10 +1,15 @@
   public static boolean isShowConfInServlet() {
     boolean isShowConf = false;
     try {
       if (Class.forName(""org.apache.hadoop.conf.ConfServlet"") != null) {
         isShowConf = true;
       }
-    } catch (Exception e) {
+    } catch (LinkageError e) {
+       // should we handle it more aggressively in addition to log the error?
+       LOG.warn(""Error thrown: "", e);
+    } catch (ClassNotFoundException ce) {
+      LOG.debug(""ClassNotFound: ConfServlet"");
+      // ignore
     }
     return isShowConf;
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that potential bugs in exception handlers were fixed. The diff details modifications in the exception handling logic:

1. **Changed Exception Types**: 
    - Originally, the code caught a general `Exception` for any issue arising from `Class.forName`. 
    - Now, it specifically catches a `LinkageError`, which can occur due to issues like missing dependencies, and logs a warning message. Catching this specific type suggests that anticipated runtime errors are being considered and logged appropriately.
    - It also adds a catch for `ClassNotFoundException`, which indicates that the class `ConfServlet` was not found. This catch logs the absence of the class at the debug level and provides a comment indicating this scenario is intentionally ignored.

By differentiating between types of exceptions and logging them accordingly, the code better handles unexpected scenarios, potentially avoiding bugs arising from unlogged runtime issues or inappropriate handling of specific exceptions. These amendments align with the commit's message of fixing potential bugs in exception handlers.

Thus, the changes indicate a bug fix. Therefore, the conclusion is **Buggy**."
hbase,10426.json,f1ad5cb93837e8d07d9d08da7c1a48caf74bbe9f,"@@ -1,30 +1,34 @@
   protected void configureForRegion(HRegion region) {
     super.configureForRegion(region);
     if (region != null) {
       prefixLength = 0;
 
       // read the prefix length from the table descriptor
       String prefixLengthString = region.getTableDesc().getValue(
           PREFIX_LENGTH_KEY);
       if (prefixLengthString == null) {
         //read the deprecated value
         prefixLengthString = region.getTableDesc().getValue(PREFIX_LENGTH_KEY_DEPRECATED);
         if (prefixLengthString == null) {
           LOG.error(PREFIX_LENGTH_KEY + "" not specified for table ""
               + region.getTableDesc().getTableName()
               + "". Using default RegionSplitPolicy"");
           return;
         }
       }
       try {
         prefixLength = Integer.parseInt(prefixLengthString);
       } catch (NumberFormatException nfe) {
-        // ignore
+        /* Differentiate NumberFormatException from an invalid value range reported below. */
+        LOG.error(""Number format exception when parsing "" + PREFIX_LENGTH_KEY + "" for table ""
+            + region.getTableDesc().getTableName() + "":""
+            + prefixLengthString + "". "" + nfe);
+        return;
       }
       if (prefixLength <= 0) {
         LOG.error(""Invalid value for "" + PREFIX_LENGTH_KEY + "" for table ""
             + region.getTableDesc().getTableName() + "":""
             + prefixLengthString + "". Using default RegionSplitPolicy"");
       }
     }
   }",NotBuggy,"HBASE-10452 Fix potential bugs in exception handlers (Ding Yuan)



git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1567979 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message suggests fixing potential bugs in exception handlers. The provided code diff modifies the exception handling in the `configureForRegion` method. Previously, when a `NumberFormatException` occurred due to issues in parsing the `prefixLengthString`, it was ignored. The updated code instead logs an error message detailing the exception, along with specific information about the table and the faulty value. This ensures that issues with parsing numeric values are tracked and logged, helping in diagnosing any potential bugs.

The change aligns with the commit message because it targets the improvement of exception handling, specifically making previously silent failures visible through logging. This adjustment is a typical response to rectify potential bugs related to improper handling of exceptions.

Therefore, this diff correctly reflects the intention to fix potential bugs in exception handlers, as described in the commit message.

**Buggy**"
ant,5703.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,3 +1,8 @@
     public String getJavacExecutable() {
+        if (forkedExecutable == null && isForkedJavac()) {
+            forkedExecutable = getSystemJavac();
+        } else if (forkedExecutable != null && !isForkedJavac()) {
+            forkedExecutable = null;
+        }
         return forkedExecutable;
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The changes in the diff modify the method `getJavacExecutable` to handle cases more gracefully regarding the `forkedExecutable` variable based on whether the Java compiler is forked. The logic checks:

1. If `forkedExecutable` is `null` and `isForkedJavac()` is true, it initializes `forkedExecutable` using `getSystemJavac()`.
2. If `forkedExecutable` is not `null` and `isForkedJavac()` is false, it resets `forkedExecutable` to `null`.

These conditions ensure that the state of `forkedExecutable` is consistent with the forking behavior of the Java compiler. This update addresses a problem with specific combinations of `fork` attribute and `build.compiler` settings as mentioned in the commit message.

The commit message states that there is a fix related to the fork attribute and build.compiler settings. This suggests that there was a problem in how these settings interacted, which now appears to be corrected by the introduced logic.

Therefore, the changes in the diff and the commit message suggest a bug fix related to the management of the `forkedExecutable`, ensuring compatibility with the fork attribute and `build.compiler` settings. The conclusion is **Buggy**."
ant,5699.json,a66a2b7f86fe7f4bab938d0cb5167b27c8fa4957,"@@ -1,15 +1,16 @@
     public void setFork(String f) {
         if (f.equalsIgnoreCase(""on"")
             || f.equalsIgnoreCase(""true"")
             || f.equalsIgnoreCase(""yes"")) {
             fork = ""true"";
             forkedExecutable = getSystemJavac();
         } else if (f.equalsIgnoreCase(""off"")
                    || f.equalsIgnoreCase(""false"")
                    || f.equalsIgnoreCase(""no"")) {
             fork = ""false"";
+            forkedExecutable = null;
         } else {
             fork = ""true"";
             forkedExecutable = f;
         }
     }",Buggy,"Add testcases for latest <javac> changes, fix some problem with some
rare combination of the fork attribute and build.compiler settings.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269879 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions fixing a problem with specific combinations of the fork attribute and build.compiler settings. The code change addresses precisely that: when the fork attribute is set to 'off', 'false', or 'no', the previously defined forkedExecutable is now explicitly set to null. This ensures there is no lingering state from earlier configurations, which could lead to unpredictable behavior. This adjustment is related to error or logical correction in the code, indicating a bug fix. Hence, the change aligns with the commit message.

The final conclusion is **Buggy**."
ant,9617.json,4e2a4f9c9507b19b248548ec6333698af3c1a4b8,"@@ -1,35 +1,38 @@
     public String substitute(String input, String argument, int options)
         throws BuildException {
         // translate \1 to $1 so that the Perl5Substitution will work
         StringBuffer subst = new StringBuffer();
         for (int i = 0; i < argument.length(); i++) {
             char c = argument.charAt(i);
-            if (c == '\\') {
+            if (c == '$') {
+                subst.append('\\');
+                subst.append('$');
+            } else if (c == '\\') {
                 if (++i < argument.length()) {
                     c = argument.charAt(i);
                     int value = Character.digit(c, 10);
                     if (value > -1) {
                         subst.append(""$"").append(value);
                     } else {
                         subst.append(c);
                     }
                 } else {
                     // XXX - should throw an exception instead?
                     subst.append('\\');
                 }
             } else {
                 subst.append(c);
             }
         }
         
 
         // Do the substitution
         Substitution s = 
             new Perl5Substitution(subst.toString(), 
                                   Perl5Substitution.INTERPOLATE_ALL);
         return Util.substitute(matcher,
                                getCompiledPattern(options),
                                s,
                                input,
                                getSubsOptions(options));
     }",Buggy,"fix for bug 20306 - regex handling of $ in replace string


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274633 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for handling `$` in regex replacement. The original code transformed `\1` to `$1` for regex substitution, but didn't properly handle `$`. The change introduces `$`-handling to prevent special meaning in replacements, following Java's regex rules.

Here's the reasoning:
1. **Problem:** The code incorrectly interpreted `$` directly within `argument`, considering it as part of Java's regex syntax, turning it into a special character when not intended.
2. **Solution:** This update checks `$` and escapes it with a backslash to avoid misinterpretation, addressing the issue specified in the commit message.
3. **Outcome:** The fix ensures `$` is treated as a literal character unless preceded by `\` that denotes a substitution pattern, aligning with typical regex replacement logic in Java.
4. **Handling Change:** The insertion of `\\` before `$` prevents `$` as a special character from erroneously being interpreted or leading to a syntax error, confirming alignment with the noted bug fix in the commit.

Given the problem description and the updated logic to address it, it indicates a bug fix concerning the regex replacement handling as mentioned in the commit message. Thus, this modification is concluded as **Buggy**."
ant,3632.json,94182a1dfa6d104e3b48d12a651dffd53f089e85,"@@ -1,21 +1,24 @@
     protected void scanDir(File srcDir, File dest, JspMangler mangler, String files[]) {
 
         long now = (new Date()).getTime();
 
         for (int i = 0; i < files.length; i++) {
             String filename = files[i];
             File srcFile = new File(srcDir, filename);
             File javaFile = mapToJavaFile(mangler, srcFile, srcDir, dest);
+            if(javaFile==null) {
+                continue;
+            }
 
             if (srcFile.lastModified() > now) {
                 log(""Warning: file modified in the future: "" + filename,
                         Project.MSG_WARN);
             }
             boolean shouldCompile = false;
             shouldCompile = isCompileNeeded(srcFile, javaFile);
             if (shouldCompile) {
                compileList.addElement(srcFile.getAbsolutePath());
                javaFiles.addElement(javaFile);
             }
         }
     }",Buggy,"this patch does not fix the failing of jspc test, it fixes two other bugs in bugzilla -NPE on inclusion of non jsp file, and webapp attr broken.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@273214 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states the patch addresses two bugs. The diff adds a null check for `javaFile`, continuing the loop if it's null. This NPE prevention aligns with the bug fix mentioned in the commit message about a ""NPE on inclusion of non jsp file."" The changes involve a logical correction to prevent a potential null pointer exception, ensuring stability during directory scanning and file processing.

Thus, this specific change is intended to fix a bug related to null pointer exceptions and aligns well with part of the commit message describing the intended fixes.

Therefore, the conclusion is **Buggy**."
ant,4122.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,28 +1,28 @@
     private Source getSource(InputStream is, Resource resource)
         throws ParserConfigurationException, SAXException {
         // todo: is this comment still relevant ??
         // FIXME: need to use a SAXSource as the source for the transform
         // so we can plug in our own entity resolver
         Source src = null;
         if (entityResolver != null) {
             if (getFactory().getFeature(SAXSource.FEATURE)) {
                 SAXParserFactory spFactory = SAXParserFactory.newInstance();
                 spFactory.setNamespaceAware(true);
                 XMLReader reader = spFactory.newSAXParser().getXMLReader();
                 reader.setEntityResolver(entityResolver);
                 src = new SAXSource(reader, new InputSource(is));
             } else {
                 throw new IllegalStateException(""xcatalog specified, but ""
                     + ""parser doesn't support SAX"");
             }
         } else {
             // WARN: Don't use the StreamSource(File) ctor. It won't work with
             // xalan prior to 2.2 because of systemid bugs.
             src = new StreamSource(is);
         }
         // The line below is a hack: the system id must an URI, but it is not
         // cleat to get the URI of an resource, so just set the name of the
         // resource as a system id
-        src.setSystemId(resource.getName());
+        src.setSystemId(resourceToURI(resource));
         return src;
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix related to the SystemId of a stylesheet, and references a Bugzilla issue. The code change replaces `resource.getName()` with `resourceToURI(resource)`. This suggests addressing the problem where SystemId needs to be a valid URI rather than a simple resource name. The comment in the diff supports this change, indicating the need for a URI-compliant SystemId.

This modification directly aligns with the commit message's indication of a bug fix, likely ensuring that the SystemId is correctly formed as a URI. Improving URI formation is likely the solution to the reported Bugzilla issue.

Therefore, the change aligns with the commit message indicating a bug fix. Hence, the conclusion is **Buggy**."
ant,4118.json,731eadf2ee7888aa3c30d70893409be7208b6746,"@@ -1,5 +1,6 @@
     public void setStylesheet(File stylesheet) throws Exception {
         FileResource fr = new FileResource();
         fr.setProject(project);
         fr.setFile(stylesheet);
+        setStylesheet(fr);
     }",Buggy,"Fix for SystemId of stylesheet
Bugzilla 39407.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@412369 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The given diff shows a change in the `setStylesheet` method, where a `FileResource` object is created and initialized with a `File` object representing the stylesheet. A single line is added: `setStylesheet(fr);`.

The addition of this line suggests that there's an intention to actually use the `FileResource` object after it has been configured. Before this change, the `FileResource` was created and set up, but not utilized. The added line calls `setStylesheet(fr)`, presumably to apply the stylesheet given as a `FileResource`. This could fix a bug where the stylesheet was not being correctly set or applied, linked to the note about SystemId, which often relates to identifying resources robustly as part of processing XML and stylesheets.

Considering the commit message and this change, it's reasonable to conclude that this is indeed fixing a bug related to how the stylesheet's SystemId (likely its identification or path resolution) is handled. 

Therefore, this commit reflects a bug fix. The verdict is **Buggy**."
ant,9255.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,4 +1,12 @@
     private void resetBufferInfo() {    
         Thread current = Thread.currentThread();
-        buffers.remove(current);
+        BufferInfo bufferInfo = (BufferInfo)buffers.get(current);
+        try {
+            bufferInfo.buffer.close();
+        }
+        catch (IOException e) {
+            // Shouldn't happen
+        }
+        bufferInfo.buffer = new ByteArrayOutputStream();
+        bufferInfo.skip = false;
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a specific output issue on Windows. The code changes show an enhancement in resource management and resetting logic for a thread's buffer, which directly relates to the output formatting issue. The changes involve:

1. Closing an existing stream (`buffer.close()`) to release system resources before creating a new `ByteArrayOutputStream`. This addresses potential issues such as stale data or resource leaks that could have caused unexpected output behavior.

2. Wrapping the `buffer.close()` in a try-catch block to handle `IOException`, which may occur during stream closing. Although the comment suggests this error is unlikely, including this exception handling ensures robustness.

These modifications focus on correcting the output behavior, aligning well with the problem described in the commit message. Based on these observations, the changes aim to fix a bug related to output inconsistencies.

The answer is **Buggy**."
ant,9260.json,f4f7024234069cab8a6853720a326d12065a2d8e,"@@ -1,3 +1,4 @@
     public void close() throws IOException {
         flush();
+        removeBuffer();
     }",Buggy,"Fixed problem which gave (on Windows) output such as:

[java] First line of real output
[java]
[java] Second line of real output
[java]


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271563 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The provided code diff indicates that a method `removeBuffer()` was added to the existing `close()` method. Since the commit message describes a problem specific to output on Windows, this change likely addresses an issue related to output management or clean-up, potentially resetting or clearing a buffer that could have caused unwanted additional output lines in certain environments. 

- **flush()**: This method call is related to ensuring all buffered output data is written out. It's often used before a close operation to ensure no data is lost.
- **removeBuffer()**: Adding this line suggests there's a need to clear or manage buffers more explicitly, preventing subsequent unexpected output. It aligns with the issue described in the commit message, where unwanted output lines (potentially buffered lines) appeared.

The addition of `removeBuffer()` to the `close()` operation appears to directly resolve the behavior described in the commit message, specifically targeting an issue with output on Windows. Clearing or resetting buffers in `close()` would address residual data causing additional output lines.

Therefore, this change fits the context of a bug fix as it directly addresses and corrects the described problem. The appropriate conclusion is **Buggy**."
ant,7847.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,10 +1,10 @@
     public void setClassname(String classname) {
         if (executableType == ExecutableType.MODULE) {
             javaCommand.setExecutable(createModuleClassPair(
                     parseModuleFromModuleClassPair(javaCommand.getExecutable()),
-                    classname));
+                    classname), false);
         } else {
             javaCommand.setExecutable(classname);
             executableType = ExecutableType.CLASS;
         }
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"The commit message refers to avoiding slash translations for module names, specifically targeting Windows test errors. 

Analysis of the Diff:
- The `setClassname` method logic is modified.
- A boolean argument `false` is introduced in the `createModuleClassPair` method call. This likely influences the handling of module names, possibly controlling slash translation or another aspect affecting module strings.

Correlation with Commit Message:
- The change introduces a boolean argument where it previously didn't exist. This parameter could potentially control behavior related to slash translations, aligning with the commit message's intent to adjust how module names are handled, especially relevant to platform-specific issues (like those encountered on Windows).

Conclusion:
- This likely modifies how module names are parsed or managed to address a specific issue (test errors) on Windows, matching the bug fix described in the commit message.

Thus, the changes align with the commit message indicating a fix for test errors on Windows, making the conclusion **Buggy**."
ant,7722.json,588ce1fbe3c07994b03283ddeb82662bb23a2785,"@@ -1,7 +1,3 @@
     public void setExecutable(String executable) {
-        if (executable == null || executable.length() == 0) {
-            return;
-        }
-        this.executable = executable.replace('/', File.separatorChar)
-            .replace('\\', File.separatorChar);
+        setExecutable(executable, true);
     }",Buggy,"avoid slash translations for module names

should fix Windows test errors introduced with #15
",Buggy,"Analyzing the provided diff and commit message:

1. **Commit Message**: The message indicates that changes are made to avoid slash translations for module names to fix Windows test errors introduced in a previous update (#15).

2. **Code Changes**:
   - The original code performs a check to ensure `executable` is not null or empty and then replaces '/' and '\' with the system-specific file separator character.
   - This has been replaced with a call to `setExecutable(executable, true);`. 

3. **Analysis**:
   - The replacement suggests a modification in how the executable string is processed. This could potentially address issues specific to path handling on Windows, which may be caused by unnecessary translation of slashes.
   - The diff hints at consolidating logic likely found in an overloaded `setExecutable` method, which seems to handle 'slash translation' logic differently based on parameters.
   - The removal of direct slash replacement suggests that translating slashes might have caused path resolution errors, particularly on Windows systems where path naming conventions differ from Unix-like systems (e.g., handling of drive letters or backslashes).
   - Given the commitment to address Windows test errors, this change aligns with the need to modify how paths are managed.

Given the reasoning above, the changes correct the path handling logic, directly addressing a regression causing test failures on Windows, consistent with fixing a bug introduced by a prior commit.

The conclusion, therefore, is **Buggy**."
ant,7516.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFilelist(FileList fl) throws BuildException {
+        if (fl.getProject() == null) {
+            fl.setProject(getProject());
+        }
         add(fl);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a NullPointerException (NPE) fix for a specific scenario when compiling with Eclipse's ECJ. The provided diff addresses this by checking if a `FileList` instance (`fl`) has a `null` project attribute and setting it using `fl.setProject(getProject())` before proceeding to add it. This change prevents the potential NPE by ensuring that the `FileList` has its necessary project context, aligning perfectly with the description of the problem and its solution in the commit message.

The modifications directly address an issue caused by missing project information on `FileSet`, which aligns with the provided explanation of the NPE occurrence in method `addExtDirs`. This is a clear indication of a bug fix, as it corrects error handling and provides missing context to avoid runtime exceptions.

Therefore, the code changes reflect a precise bug fix as described in the commit message. The conclusion is **Buggy**."
ant,7517.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addDirset(DirSet dset) throws BuildException {
+        if (dset.getProject() == null) {
+            dset.setProject(getProject());
+        }
         add(dset);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a null pointer exception (NPE) issue when compiling with a specific version of the Eclipse compiler (ecj 3.1.x), due to a DirSet being used without a Project attribute. The diff shows:

1. An if-statement checking if `dset.getProject()` returns `null`.
2. If `null`, `dset.setProject(getProject())` sets the current project context.

This change fixes the NPE by ensuring the `DirSet` has a Project set before proceeding, directly addressing the issue described in the commit message. The commit and the code change align well, confirming this is a bug fix.

Therefore, the conclusion is **Buggy**."
ant,7515.json,873b850f39a26e4911d612c157622751ac9582f9,"@@ -1,3 +1,6 @@
     public void addFileset(FileSet fs) throws BuildException {
+        if (fs.getProject() == null) {
+            fs.setProject(getProject());
+        }
         add(fs);
     }",Buggy,"fix for 
<javac> fails with NPE when compiling with eclipse ecj 3.1.x
Bugzilla 40839.
root cause of the problem was in org.eclipse.jdt.core.JDTCompiler
method addExtDirs. A FileSet was created without the Project attribute set,
then added to a Path.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@469050 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that an NPE (NullPointerException) occurs when compiling with a specific version of Eclipse. The problem is identified as a FileSet being added without its Project attribute set.

The diff clearly addresses this issue by checking if the FileSet's project attribute is null. If it is, the code sets the project attribute using `fs.setProject(getProject())`. This prevents the potential NullPointerException that would occur later when trying to use a FileSet without an associated project.

The change is directly correlated with fixing an issue related to null references, which is a common source of bugs. Thus, the change correctly implements a check-and-set step to avoid the NPE.

The connection between the commit message and the code changes indicates that the NPE is being addressed in a logical manner based on the described root cause. Therefore, the changes in the code diff effectively fix a bug.

**Buggy**"
ant,787.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,29 +1,22 @@
     public final int read() throws IOException {
         if (!getInitialized()) {
             initialize();
             setInitialized(true);
         }
 
         int ch = -1;
         if (unicodeBuf.length() == 0) {
             ch = in.read();
             if (ch != -1) {
                 char achar = (char) ch;
                 if (achar >= '\u0080') {
-                    unicodeBuf = new StringBuffer(""u0000"");
-                    String s = Integer.toHexString(ch);
-                    //replace the last 0s by the chars contained in s
-                    for (int i = 0; i < s.length(); i++) {
-                        unicodeBuf.setCharAt(unicodeBuf.length()
-                                             - s.length() + i,
-                                             s.charAt(i));
-                    }
+                    unicodeBuf = UnicodeUtil.EscapeUnicode(achar);
                     ch = '\\';
                 }
             }
         } else {
             ch = (int) unicodeBuf.charAt(0);
             unicodeBuf.deleteCharAt(0);
         }
         return ch;
     }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that it fixes bug 50515, relating to incorrect Unicode escapes in the propertyfile task. The code change replaces a manual method of converting characters to Unicode escape sequences with a call to `UnicodeUtil.EscapeUnicode(achar)`.

Let's analyze the changes:
1. **Existing Code**: The original code manually constructs a Unicode escape sequence for characters with values greater than `\u0080`. It constructs a buffer, fills it with ""u0000"", converts the character to a hex string, and then replaces the relevant positions with the hexadecimal characters. This is a prone-to-error manual approach to constructing Unicode escape sequences.

2. **Revised Code**: The change simplifies this by using `UnicodeUtil.EscapeUnicode(achar)` to handle the conversion. This method seems to replace the manual conversion and potentially addresses errors that might arise from incorrect manual Unicode escape construction.

The commit message clearly indicates that the bug involves incorrect Unicode escapes, and the code change directly targets the logic for Unicode escape creation. By replacing a potentially error-prone manual process with a utility method, the modification is aimed at correcting how Unicode escapes are generated.

Thus, the code change aligns well with the commit message and implies a bug fix, specifically targeting bug 50515 as described. Therefore, this change can be concluded as a fix for a bug, so the final assessment is **Buggy**."
ant,9748.json,bb2695af1464b52cacdd3d3672e428358c3142dd,"@@ -1,7 +1,3 @@
     private String escapeUnicode(char ch) {
-        StringBuffer buffy = new StringBuffer(""\\u"");
-        String hex = Integer.toHexString((int)ch);
-        buffy.append(""0000"".substring(4-hex.length()));
-        buffy.append(hex);
-        return buffy.toString();
-    }

+        return ""\\"" + UnicodeUtil.EscapeUnicode(ch);
+        }",Buggy,"fix for bug 50515, incorrect unicode escapes in propertyfile task

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1054711 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug related to incorrect Unicode escapes in a property file task. 

The diff shows changes in the `escapeUnicode` method:

- The original implementation manually constructs a Unicode escape sequence by converting the character to a hexadecimal string and appending necessary zeros for padding.
- The updated code replaces the manual construction with a call to `UnicodeUtil.EscapeUnicode(ch)`, which likely provides a more reliable or efficient way to create the Unicode escape.

This aligns with the commit message describing a fix for Unicode escape issues, as the change suggests an improvement in correctness or handling of Unicode characters.

Hence, the modifications reflect a bug fix as described in the commit message.

**Buggy**"
ant,4816.json,29fface4fb93fb33b33c86124a168c04779271c0,"@@ -1,24 +1,24 @@
     public boolean execute() throws BuildException {
         Rmic owner = getRmic();
         Commandline cmd = setupRmicCommand();
         Project project = owner.getProject();
         //rely on RMIC being on the path
-        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(SunRmic.RMIC_EXECUTABLE));
+        cmd.setExecutable(JavaEnvUtils.getJdkExecutable(getExecutableName()));
 
         //set up the args
         String[] args = cmd.getCommandline();
 
         try {
             Execute exe = new Execute(new LogStreamHandler(owner,
                     Project.MSG_INFO,
                     Project.MSG_WARN));
             exe.setAntRun(project);
             exe.setWorkingDirectory(project.getBaseDir());
             exe.setCommandline(args);
             exe.execute();
             return !exe.isFailure();
         } catch (IOException exception) {
-            throw new BuildException(""Error running "" + SunRmic.RMIC_EXECUTABLE
+            throw new BuildException(""Error running "" + getExecutableName()
                     + "" -maybe it is not on the path"", exception);
         }
     }",Buggy,"bug ID#38732 , rmic task doesn't work with -Xnew and JDK 6.0

Fixed by writing a new adapter, xnew, that extends the forking adapter and sets the -Xnew argument. Tests supplied, though the old test, the one that would fail on java1.6, is still there.

Also made the name matching code of rmic locale-independent.

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@434029 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message addresses a specific bug related to the `rmic` task not working with `-Xnew` in JDK 6.0, requiring a new adapter. The diff alters how the executable name is retrieved: it replaces hardcoded `SunRmic.RMIC_EXECUTABLE` with `getExecutableName()`. 

This change implies a dynamic way to fetch the executable name, likely accommodating variations, such as using `-Xnew`. This fits with the commit's claim to fix a bug related to JDK 6.0 functionality, although the actual adapter creation isn't shown here. The message also mentions locale-independent name matching, but this aspect is not present in the diff.

Overall, the modified code aligns with the message's bug fix description, focusing on supporting new behavior tied to JDK 6.0 execution. Given these points, the code change reflects a bug fix.

**Buggy**"
ant,10020.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,3 @@
     public boolean tryHardToDelete(File f) {
-        if (!f.delete()) {
-            if (ON_WINDOWS) {
-                System.gc();
-            }
-            try {
-                Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
-            } catch (InterruptedException ex) {
-                // Ignore Exception
-            }
-            return f.delete();
-        }
-        return true;
+        return tryHardToDelete(f, ON_WINDOWS);
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change modifies the tryHardToDelete method to delegate to a new method which seems to incorporate an improved approach for handling file deletions across different operating systems. The original code only attempted garbage collection on Windows OSes, utilizing System.gc() to help release file locks before retrying the deletion. The commit message suggests there is now an option to include this garbage collection in non-Windows environments as well, potentially addressing an NFS-related issue.

However, the specific change in the diff only indicates a change in method signature: it replaces the detailed deletion attempt logic with a call to what seems to be a newly introduced overloaded version of the method which can accept a parameter (likely to indicate OS type, though the actual logic of this is not visible in the diff).

The commit message aligns with a traditional bug fix by attempting to resolve a platform-specific file deletion issue which could indeed be connected to NFS problems. Since the provided diff appears to be part of a larger set of changes (indicating the inclusion of a conditional mechanism that generalizes the retry logic with garbage collection to all OSes as needed), and the aim is to address specific functionality problems seen in PR 45786, it is reasonable to consider this change as a bug fix.

The rationale is based on the fact that the diff signifies an intention to handle deletion failures more robustly across all systems, which addresses potential failure modes (known NFS problem) by allowing the retry mechanism to be customizable on non-Windows platforms too.

Thus, the appropriated conclusion based on the available information from the commit message and the changes is **Buggy**."
ant,10021.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
-    public boolean tryHardToDelete(File f) {
+    public boolean tryHardToDelete(File f, boolean runGC) {
         if (!f.delete()) {
-            if (ON_WINDOWS) {
+            if (runGC) {
                 System.gc();
             }
             try {
                 Thread.sleep(DELETE_RETRY_SLEEP_MILLIS);
             } catch (InterruptedException ex) {
                 // Ignore Exception
             }
             return f.delete();
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates adding a feature to run garbage collection (GC) before retrying to delete a file, applicable to non-Windows OSes. 

In the diff:
1. The method signature for `tryHardToDelete` is modified to include a new parameter, `runGC`, which determines if the GC should be run before retrying the deletion. This changes the behavior from the previous condition where GC was only run on Windows (`ON_WINDOWS`).
2. Inside the method, if the `runGC` parameter is true, `System.gc()` is called before retrying the delete operation. This allows GC to potentially release file handles on non-Windows systems as well.
3. The change is consistent with the intention described in the commit message: it adds a mechanism to reduce file deletion errors potentially caused by locked file handles, especially in NFS environments. Such issues are often observed in Java applications, which can fail to delete files until GC clears them.

While this change doesn't guarantee a bug fix, it attempts to address a specific file deletion issue on non-Windows systems, which is tied to the problem described in PR 45786 as per the commit message. Hence, it's a preventative measure aimed at mitigating a known problem.

Therefore, the conclusion is **Buggy** as it potentially resolves a reported issue."
ant,5445.json,ececc5c3e332b97f962b94a475408606433ee0e6,"@@ -1,14 +1,14 @@
     private boolean delete(File f) {
-        if (!FILE_UTILS.tryHardToDelete(f)) {
+        if (!FILE_UTILS.tryHardToDelete(f, performGc)) {
             if (deleteOnExit) {
                 int level = quiet ? Project.MSG_VERBOSE : Project.MSG_INFO;
                 log(""Failed to delete "" + f + "", calling deleteOnExit.""
                     + "" This attempts to delete the file when the Ant jvm""
                     + "" has exited and might not succeed."", level);
                 f.deleteOnExit();
                 return true;
             }
             return false;
         }
         return true;
     }",NotBuggy,"Add an option to <delete> to run the GC before retrying a failed build on non-Windows OSes as well.  Might fix the NFS problem described in PR 45786

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1177305 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit introduces a change in the `delete(File f)` method that involves an additional parameter, `performGc`, in the call to `FILE_UTILS.tryHardToDelete`. This parameter suggests a new feature rather than directly fixing a bug. The commit message indicates the option may ""fix"" a problem with NFS by allowing garbage collection to be attempted before retrying a file deletion, which is more about adding functionality than correcting a logical or error-handling flaw. Hence, the change appears to align with adding a new option or feature, potentially addressing a problem for specific cases, but it is not a straightforward bug fix in the traditional sense.

Additionally, the commit message mentions adding this option specifically for non-Windows operating systems to handle a particular scenario which might resolve a previously reported issue (PR 45786), yet it doesn't guarantee a resolution without testing.

Therefore, the implementation adds an enhancement that has the potential to mitigate an issue but is categorized more as an improvement rather than a definite bug fix.

Given this analysis, the conclusion is **NotBuggy**."
ant,7946.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,9 +1,11 @@
     public boolean isSelected(File basedir, String filename, File file) {
 
         // throw BuildException on error
         validate();
 
         if (file.isDirectory()) {
             return type.equals(FileType.DIR);
-        } else return type.equals(FileType.FILE);
+        } else {
+            return type.equals(FileType.FILE);
+        }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that this commit is about fixing checkstyle issues by reformatting code, not about fixing bugs. The code change involves adjusting formatting by explicitly using braces for the `else` statement, aligning with common code style guidelines for better readability and maintainability but not altering any functionality or fixing a bug.

No bug is being fixed in this change; rather, it's geared toward maintaining consistency and readability in the code according to style practices.

Therefore, the appropriate conclusion is **NotBuggy**."
ant,8194.json,f83f5c68c975f31a384cc24e94be4e5ebfa2da4d,"@@ -1,11 +1,10 @@
     public void verifySettings() {
         if (dateTime == null && millis < 0) {
             setError(""You must provide a datetime or the number of ""
-                + ""milliseconds."");
-        }
-        else if (millis < 0) {
+                    + ""milliseconds."");
+        } else if (millis < 0) {
             setError(""Date of "" + dateTime
-                + "" results in negative milliseconds""
-                + "" value relative to epoch (January 1, 1970, 00:00:00 GMT)."");
+                    + "" results in negative milliseconds value""
+                    + "" relative to epoch (January 1, 1970, 00:00:00 GMT)."");
         }
     }",NotBuggy,"First pass at fixing checkstyle issues by using IDEA to reformat code and identify problem spots


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274773 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that this is an attempt to fix checkstyle issues using formatting tools. The changes in the diff adjust the code layout to comply with style guidelines by reformatting the placement of braces and concatenation, improving readability. The logic and functionality of the method remain unchanged, and no adjustments related to logic or bug fixes are evident in this particular diff. It simply enhances code formatting, aligning with the commit message of addressing styling issues.

Since the changes are purely stylistic and do not touch on logic errors or adjustments that could be classified as bug fixes, the evaluation is **NotBuggy**."
ant,4305.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,87 +1,89 @@
         private URLConnection openConnection(final URL aSource) throws IOException {
 
             // set up the URL connection
             final URLConnection connection = aSource.openConnection();
             // modify the headers
             // NB: things like user authentication could go in here too.
             if (hasTimestamp) {
                 connection.setIfModifiedSince(timestamp);
             }
             // Set the user agent
             connection.addRequestProperty(""User-Agent"", this.userAgent);
 
             // prepare Java 1.1 style credentials
             if (uname != null || pword != null) {
                 final String up = uname + "":"" + pword;
                 String encoding;
                 // we do not use the sun impl for portability,
                 // and always use our own implementation for consistent
                 // testing
                 final Base64Converter encoder = new Base64Converter();
                 encoding = encoder.encode(up.getBytes());
                 connection.setRequestProperty(""Authorization"", ""Basic ""
                         + encoding);
             }
 
-            connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            if (tryGzipEncoding) {
+                connection.setRequestProperty(""Accept-Encoding"", GZIP_CONTENT_ENCODING);
+            }
 
             if (connection instanceof HttpURLConnection) {
                 ((HttpURLConnection) connection)
                         .setInstanceFollowRedirects(false);
                 ((HttpURLConnection) connection)
                         .setUseCaches(httpUseCaches);
             }
             // connect to the remote site (may take some time)
             try {
                 connection.connect();
             } catch (final NullPointerException e) {
                 //bad URLs can trigger NPEs in some JVMs
                 throw new BuildException(""Failed to parse "" + source.toString(), e);
             }
 
             // First check on a 301 / 302 (moved) response (HTTP only)
             if (connection instanceof HttpURLConnection) {
                 final HttpURLConnection httpConnection = (HttpURLConnection) connection;
                 final int responseCode = httpConnection.getResponseCode();
                 if (isMoved(responseCode)) {
                     final String newLocation = httpConnection.getHeaderField(""Location"");
                     final String message = aSource
                             + (responseCode == HttpURLConnection.HTTP_MOVED_PERM ? "" permanently""
                                     : """") + "" moved to "" + newLocation;
                     log(message, logLevel);
                     final URL newURL = new URL(aSource, newLocation);
                     if (!redirectionAllowed(aSource, newURL)) {
                         return null;
                     }
                     return openConnection(newURL);
                 }
                 // next test for a 304 result (HTTP only)
                 final long lastModified = httpConnection.getLastModified();
                 if (responseCode == HttpURLConnection.HTTP_NOT_MODIFIED
                         || (lastModified != 0 && hasTimestamp && timestamp >= lastModified)) {
                     // not modified so no file download. just return
                     // instead and trace out something so the user
                     // doesn't think that the download happened when it
                     // didn't
                     log(""Not modified - so not downloaded"", logLevel);
                     return null;
                 }
                 // test for 401 result (HTTP only)
                 if (responseCode == HttpURLConnection.HTTP_UNAUTHORIZED) {
                     final String message = ""HTTP Authorization failure"";
                     if (ignoreErrors) {
                         log(message, logLevel);
                         return null;
                     } else {
                         throw new BuildException(message);
                     }
                 }
             }
 
             //REVISIT: at this point even non HTTP connections may
             //support the if-modified-since behaviour -we just check
             //the date of the content and skip the write if it is not
             //newer. Some protocols (FTP) don't include dates, of
             //course.
             return connection;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The diff modifies the behavior of setting the ""Accept-Encoding"" request property for a URL connection. Previously, the code always included a request to accept GZIP encoding by setting this property to `GZIP_CONTENT_ENCODING`. The change introduces a conditional setting based on the new `tryGzipEncoding` flag. If `tryGzipEncoding` is true, then the property is set; otherwise, it is not.

The commit message states that the change ""only enables transparent gzip encoding when explicitly specified"", which aligns exactly with adding this conditional logic ensuring gzip encoding is only requested when `tryGzipEncoding` is true. This change prevents gzip encoding from being requested unless explicitly allowed, potentially resolving an issue where unintended or unexpected behavior might occur if servers responded with gzip encoding when it wasn't explicitly desired.

Given that the commit message references a specific bugzilla issue (57048) and the change in code logic directly relates to making the gzip encoding request conditional, this indicates the presence of a bug fix. The unexpected and possibly erroneous behavior (such as incorrect handling of gzip-encoded responses when not anticipated) may have been reported in the bugzilla issue.

Overall, this indicates a bug fix associated with the conditional logic for gzip encoding, matching the stated intent of the commit message. The determination is thus **Buggy**."
ant,4307.json,faedd2bc5b9fdcaa0650966bc7fef43c5c59cf9a,"@@ -1,51 +1,52 @@
         private boolean downloadFile()
                 throws FileNotFoundException, IOException {
             for (int i = 0; i < numberRetries; i++) {
                 // this three attempt trick is to get round quirks in different
                 // Java implementations. Some of them take a few goes to bind
                 // properly; we ignore the first couple of such failures.
                 try {
                     is = connection.getInputStream();
                     break;
                 } catch (final IOException ex) {
                     log(""Error opening connection "" + ex, logLevel);
                 }
             }
             if (is == null) {
                 log(""Can't get "" + source + "" to "" + dest, logLevel);
                 if (ignoreErrors) {
                     return false;
                 }
                 throw new BuildException(""Can't get "" + source + "" to "" + dest,
                         getLocation());
             }
 
-            if (GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
+            if (tryGzipEncoding
+                && GZIP_CONTENT_ENCODING.equals(connection.getContentEncoding())) {
                 is = new GZIPInputStream(is);
             }
 
             os = new FileOutputStream(dest);
             progress.beginDownload();
             boolean finished = false;
             try {
                 final byte[] buffer = new byte[BIG_BUFFER_SIZE];
                 int length;
                 while (!isInterrupted() && (length = is.read(buffer)) >= 0) {
                     os.write(buffer, 0, length);
                     progress.onTick();
                 }
                 finished = !isInterrupted();
             } finally {
                 FileUtils.close(os);
                 FileUtils.close(is);
 
                 // we have started to (over)write dest, but failed.
                 // Try to delete the garbage we'd otherwise leave
                 // behind.
                 if (!finished) {
                     dest.delete();
                 }
             }
             progress.endDownload();
             return true;
         }",Buggy,"only enable transparent gzip encoding when explicitly specified
fixes bugzilla issue 57048
",Buggy,"The commit message states that transparent gzip encoding should be enabled only when explicitly specified and mentions fixing a specific bug. The code change introduces a conditional check (`tryGzipEncoding`) before creating a `GZIPInputStream`. This ensures gzip decoding is applied only when explicitly allowed via the `tryGzipEncoding` flag. 

Originally, gzip decoding would be applied whenever the content encoding of the connection was gzip, regardless of any setting intended to control this behavior, likely causing unintended data handling. By introducing the `tryGzipEncoding` condition, the updated code aligns with the message's intent to control gzip encoding explicitly, potentially fixing an issue where content was being processed in a way not intended or expected by the user.

This adjustment directly addresses a behavioral bug, as indicated by the bugzilla issue reference in the commit message. Therefore, this change reflects a genuine bug fix regarding transparent gzip encoding application.

The review concludes with **Buggy**."
ant,658.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,45 +1,46 @@
     private final void tsort(String root, Hashtable targets,
                              Hashtable state, Stack visiting,
                              Vector ret)
         throws BuildException {
         state.put(root, VISITING);
         visiting.push(root);
 
         Target target = (Target)(targets.get(root));
 
         // Make sure we exist
         if (target == null) {
             StringBuffer sb = new StringBuffer(""Target `"");
             sb.append(root);
             sb.append(""' does not exist in this project. "");
             visiting.pop();
             if (!visiting.empty()) {
                 String parent = (String)visiting.peek();
                 sb.append(""It is used from target `"");
                 sb.append(parent);
                 sb.append(""'."");
             }
 
             throw new BuildException(new String(sb));
         }
 
-        for (Enumeration en=target.getDependencies(); en.hasMoreElements();) {
+        for (Enumeration en = target.getDependencies(); en.hasMoreElements();) {
             String cur = (String) en.nextElement();
-            String m=(String)state.get(cur);
+            String m = (String)state.get(cur);
             if (m == null) {
                 // Not been visited
                 tsort(cur, targets, state, visiting, ret);
             }
             else if (m == VISITING) {
                 // Currently visiting this node, so have a cycle
                 throw makeCircularException(cur, visiting);
             }
         }
 
         String p = (String) visiting.pop();
         if (root != p) {
-            throw new RuntimeException(""Unexpected internal error: expected to pop ""+root+"" but got ""+p);
+            throw new RuntimeException(""Unexpected internal error: expected to ""
+                + ""pop "" + root + "" but got "" + p);
         }
         state.put(root, VISITED);
         ret.addElement(target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests fixing errors related to the project format and Javadoc which were identified by checkstyle. The changes in the diff focus on formatting aspects, such as adding spaces around assignment operators and improving readability by splitting a long string concatenation expression across two lines. There's no indication that these changes fix logical errors, exceptions, or handling logic related to bug fixes in the program execution.

Checkstyle tools are primarily used for ensuring code adheres to certain coding standards and conventions, not typically for bug detection, unless the bug is a result of non-adherence to those standards (e.g., a logic error caused by misplacing braces or incorrect syntax).

Overall, the modifications are purely stylistic and related to code formatting standards and do not address any functional defects or bugs.

Thus, the examination concludes with **NotBuggy**."
ant,624.json,d0b4fb3912af11d2c3075c51c0715d581ea085f1,"@@ -1,7 +1,7 @@
-    public void addTarget(Target target) {
+    public void addTarget(Target target) throws BuildException {
         String name = target.getName();
         if (targets.get(name) != null) {
-            throw new BuildException(""Duplicate target: `""+name+""'"");
+            throw new BuildException(""Duplicate target: `"" + name + ""'"");
         }
         addOrReplaceTarget(name, target);
     }",NotBuggy,"Fix up errors in Project format and javadoc picked up with checkstyle


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271747 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The diff indicates two changes:

1. **Throws Declaration**: The method `addTarget` is updated to declare that it throws `BuildException`. This change would be important for both error handling and documentation consistency with what the method can produce during runtime.

2. **String Concatenation Format**: A minor format correction in the string concatenation within `BuildException`. This does not change functionality, though it enhances readability and style.

Regarding the commit message, it mentions fixing up errors related to project format and javadoc as per checkstyle recommendations. The changes align with such improvements:

- Adding the `throws BuildException` clause could be about aligning method signatures with possible runtime exceptions, ensuring documentation consistency, which may have been flagged by checkstyle if the method threw a checked exception but didn't declare it.

- The string concatenation formatting improves code readability, which checkstyle might consider a style issue, although this doesn't indicate a bug fix.

The primary change is adding the exception declaration, which is more about correctness and style fixing per checkstyle rules and does not indicate a core bug fix in functionality but rather making the method signature clear.

Thus, considering these observations, the emphasis on format and javadoc improvements suggests this is about style and proper documentation rather than functional bug fixes.

The conclusion for this commit based on the provided data is **NotBuggy**."
ant,1464.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,15 +1,25 @@
     private void processComment( final String line )
     {
         final String lineSeparator = System.getProperty( ""line.separator"" );
-        if( line.startsWith( ""======"" ) || line.startsWith( ""------"" ) )
+        if( line.startsWith( ""======"" ) )
+        {
+            //We have ended changelog for that particular file
+            //so we can save it
+            final int end = m_comment.length() - lineSeparator.length(); //was -1
+            m_comment = m_comment.substring( 0, end );
+            m_comment = ""<![CDATA["" + m_comment + ""]]>"";
+            saveEntry();
+            m_status = GET_FILE;
+        }
+        else if( line.startsWith( ""------"" ) )
         {
             final int end = m_comment.length() - lineSeparator.length(); //was -1
             m_comment = m_comment.substring( 0, end );
             m_comment = ""<![CDATA["" + m_comment + ""]]>"";
             m_status = GET_PREVIOUS_REV;
         }
         else
         {
             m_comment += line + lineSeparator;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"This code change updates the processComment method to handle log entries separated by ""======"" and ""------"" lines more correctly. Previously, the same block of code was executed for both separators, which might have led to incorrect processing when a single file's log was followed directly by another log of the same file without any separators in between.

In the revised code:
- The handling of lines starting with ""======"" (indicating the end of a changelog for a file) and ""------"" (indicating a separator between versions or logs) is separated.
- When a ""======"" line is encountered, the comment is treated as the end of the changelog for that file, and saveEntry() is called, allowing the status (m_status) to be set to GET_FILE after the log entry is properly saved.
- This change seems to ensure that the log of a single file correctly interprets and saves subsequent entries even when consecutive logs of the same file appear without other intervening logs.

The commit message describes the resolution of a specific bug where a log for a single file without intervening logs would only capture the first change. The code changes specifically address how comments for logs are processed and saved, aligning with the described bug's symptoms.

Therefore, analyzing the commit message and the code changes, it appears that this update directly addresses the described bug by correctly managing line separators and processing the log data appropriately for a file.

The conclusion is **Buggy** because the changes directly relate to a bug fix as described in the commit message."
ant,1463.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,24 +1,25 @@
     public void stdout( final String line )
     {
         switch( m_status )
         {
             case GET_FILE:
                 processFile( line );
                 break;
             case GET_REVISION:
                 processRevision( line );
                 //Was a fall through ....
-                //break;
+                break;
+
             case GET_DATE:
                 processDate( line );
                 break;
 
             case GET_COMMENT:
                 processComment( line );
                 break;
 
             case GET_PREVIOUS_REV:
                 processGetPreviousRevision( line );
                 break;
         }
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change adds a missing `break` statement to the `GET_REVISION` case in a switch block. Without this break, the code would ""fall through"" to the next case, unintentionally executing `processDate(line)`. This could lead to incorrect handling of log information and would match the bug described in the commit message, where only the first change is retrieved. Adding the `break` prevents this fall-through, ensuring each case executes in isolation. Therefore, based on the commit message and the code changes, these modifications are indicative of a bug fix related to correcting control flow in the code.

The answer is **Buggy**."
ant,1468.json,3d3b941aa7dd1e582d95c63df93ecd61423fa41d,"@@ -1,36 +1,13 @@
     private void processGetPreviousRevision( final String line )
     {
-        final String entryKey = m_date + m_author + m_comment;
-        if( line.startsWith( ""revision"" ) )
+        if( !line.startsWith( ""revision"" ) )
         {
-            m_previousRevision = line.substring( 9 );
-            m_status = GET_FILE;
+            throw new IllegalStateException( ""Unexpected line from CVS: "" + line );
+        }
+        m_previousRevision = line.substring( 9 );
 
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision, m_previousRevision );
-        }
-        else if( line.startsWith( ""======"" ) )
-        {
-            m_status = GET_FILE;
-            CVSEntry entry;
-            if( !m_entries.containsKey( entryKey ) )
-            {
-                entry = new CVSEntry( parseDate( m_date ), m_author, m_comment );
-                m_entries.put( entryKey, entry );
-            }
-            else
-            {
-                entry = (CVSEntry)m_entries.get( entryKey );
-            }
-            entry.addFile( m_file, m_revision );
-        }
+        saveEntry();
+
+        m_revision = m_previousRevision;
+        m_status = GET_COMMENT;
     }",Buggy,"Fix bug where a log of a single file without other logs in between would only retrieve first change


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272104 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the logging mechanism. Upon analyzing the code changes, the following observations can be made:

1. **Checking Revision Line**: Previously, the code logic processed when the line started with ""revision"". The fix inverts this logic to throw an exception when the line does not start with ""revision"". This change aims to ensure the correct handling of CVS lines, presumably to make sure processing only occurs when expected.

2. **Removed Redundant Code**: The previous implementation contained repeated code blocks for handling CVS entries, with an `if-else` structure ensuring entries are added with or without the previous revision. The new version abstracts this logic into a single method call `saveEntry();`, indicating a refactoring for clarity and potentially addressing the bug by refining how entries are stored.

3. **Correct Revision Handling**: The last part of the method sets `m_revision` to `m_previousRevision`, implying that after saving, the method wants to ensure the revision state is correctly updated.

The changes seem to address a bug where logs of files were incorrectly handled due to faulty logic in processing CVS log lines and storing entries. By refactoring the entry management logic and ensuring that revisions are correctly handled, the code aligns with the commit message.

Therefore, the changes indicate a bug fix in how single file logs are processed, and the declaration in the commit message matches the changes. Thus, the conclusion is **Buggy**."
ant,5442.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,116 +1,115 @@
     public void execute() throws BuildException {
         if (usedMatchingTask) {
             log(""DEPRECATED - Use of the implicit FileSet is deprecated.  ""
-                + ""Use a nested fileset element instead."");
+                + ""Use a nested fileset element instead."", quiet ? Project.MSG_VERBOSE : verbosity);
         }
 
         if (file == null && dir == null && filesets.size() == 0 && rcs == null) {
             throw new BuildException(""At least one of the file or dir ""
                                      + ""attributes, or a nested resource collection, ""
                                      + ""must be set."");
         }
 
         if (quiet && failonerror) {
             throw new BuildException(""quiet and failonerror cannot both be ""
                                      + ""set to true"", getLocation());
         }
 
         // delete the single file
         if (file != null) {
             if (file.exists()) {
                 if (file.isDirectory()) {
                     log(""Directory "" + file.getAbsolutePath()
                         + "" cannot be removed using the file attribute.  ""
-                        + ""Use dir instead."");
+                        + ""Use dir instead."", quiet ? Project.MSG_VERBOSE : verbosity);
                 } else {
                     log(""Deleting: "" + file.getAbsolutePath());
 
                     if (!delete(file)) {
                         handle(""Unable to delete file "" + file.getAbsolutePath());
                     }
                 }
             } else {
                 log(""Could not find file "" + file.getAbsolutePath()
-                    + "" to delete."",
-                    Project.MSG_VERBOSE);
+                    + "" to delete."", quiet ? Project.MSG_VERBOSE : verbosity);
             }
         }
 
         // delete the directory
         if (dir != null && dir.exists() && dir.isDirectory()
             && !usedMatchingTask) {
             /*
                If verbosity is MSG_VERBOSE, that mean we are doing
                regular logging (backwards as that sounds).  In that
                case, we want to print one message about deleting the
                top of the directory tree.  Otherwise, the removeDir
                method will handle messages for _all_ directories.
              */
             if (verbosity == Project.MSG_VERBOSE) {
                 log(""Deleting directory "" + dir.getAbsolutePath());
             }
             removeDir(dir);
         }
         Resources resourcesToDelete = new Resources();
         resourcesToDelete.setProject(getProject());
         Resources filesetDirs = new Resources();
         filesetDirs.setProject(getProject());
 
-        for (int i = 0; i < filesets.size(); i++) {
+        for (int i = 0, size = filesets.size(); i < size; i++) {
             FileSet fs = (FileSet) filesets.get(i);
             if (fs.getProject() == null) {
                 log(""Deleting fileset with no project specified;""
                     + "" assuming executing project"", Project.MSG_VERBOSE);
                 fs = (FileSet) fs.clone();
                 fs.setProject(getProject());
             }
             resourcesToDelete.add(fs);
             if (includeEmpty && fs.getDir().isDirectory()) {
               filesetDirs.add(new ReverseDirs(fs.getDir(),
                   fs.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         if (usedMatchingTask && dir != null && dir.isDirectory()) {
             //add the files from the default fileset:
             FileSet implicit = getImplicitFileSet();
             resourcesToDelete.add(implicit);
             if (includeEmpty) {
               filesetDirs.add(new ReverseDirs(dir,
                   implicit.getDirectoryScanner().getIncludedDirectories()));
             }
         }
         resourcesToDelete.add(filesetDirs);
         if (rcs != null) {
             // sort first to files, then dirs
             Restrict exists = new Restrict();
             exists.add(EXISTS);
             exists.add(rcs);
             Sort s = new Sort();
             s.add(REVERSE_FILESYSTEM);
             s.add(exists);
             resourcesToDelete.add(s);
         }
         try {
             if (resourcesToDelete.isFilesystemOnly()) {
                 for (Iterator iter = resourcesToDelete.iterator(); iter.hasNext();) {
                     FileResource r = (FileResource) iter.next();
                     // nonexistent resources could only occur if we already
                     // deleted something from a fileset:
                     if (!r.isExists()) {
                         continue;
                     }
                     if (!(r.isDirectory()) || r.getFile().list().length == 0) {
                         log(""Deleting "" + r, verbosity);
                         if (!delete(r.getFile()) && failonerror) {
                             handle(""Unable to delete ""
                                 + (r.isDirectory() ? ""directory "" : ""file "") + r);
                         }
                     }
                 }
             } else {
                  handle(getTaskName() + "" handles only filesystem resources"");
             }
         } catch (Exception e) {
             handle(e);
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message claims to fix a bug regarding logging checks. Let's analyze the diff to see if the changes align with this message.

1. **Quiet Logging Check Implementation**: The diff introduces the use of the `quiet` variable to determine the verbosity of logging messages:
   - The logs check the `quiet` flag, then decide between `Project.MSG_VERBOSE` or the current `verbosity` level.
   - This change seems designed to centralize verbosity level control, reducing noise in log outputs when `quiet` is true.

2. **Condition Logic Improvements**: The change to use the `quiet` variable for log messages only affects the verbosity. It aims to alter how much information is logged based on prior configurations (`quiet` level option). 
   
3. **Safety Validation for `quiet` and `failonerror`**: There's an added safeguard that throws a `BuildException` if both `quiet` and `failonerror` are true, ensuring these settings aren't simultaneously applied since they conflict in terms of expected behavior. 

4. **Loop Optimization**: The for-loop to iterate over `filesets` is slightly optimized by caching the size value which could improve performance, though this is more of a performance tweak than a bug fix.

Evaluating these changes, we can infer that the primary aim is to resolve potential issues arising from incorrect logging verbosity levels, aligning with the fix of a bug (`bug#32738`) related to message logging behavior.

Therefore, the modifications are consistent with the implementation described in the commit message, indicating they address a specific bug in logging behavior when enabling quiet mode. The presence of logical corrections like the validation of `quiet` and `failonerror` further underscores addressing a potential flaw in configuration behavior.

In conclusion, these changes include fixing an identified bug in the logging mechanism, aligning well with the commit message. Thus, we determine the commit as **Buggy**."
ant,5446.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,22 +1,22 @@
     protected void removeDir(File d) {
         String[] list = d.list();
         if (list == null) {
             list = new String[0];
         }
         for (int i = 0; i < list.length; i++) {
             String s = list[i];
             File f = new File(d, s);
             if (f.isDirectory()) {
                 removeDir(f);
             } else {
-                log(""Deleting "" + f.getAbsolutePath(), verbosity);
+                log(""Deleting "" + f.getAbsolutePath(), quiet ? Project.MSG_VERBOSE : verbosity);
                 if (!delete(f)) {
                     handle(""Unable to delete file "" + f.getAbsolutePath());
                 }
             }
         }
         log(""Deleting directory "" + d.getAbsolutePath(), verbosity);
         if (!delete(d)) {
             handle(""Unable to delete directory "" + dir.getAbsolutePath());
         }
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions a bug fix with the implementation of a quiet check during message logging. The code change reflects this by altering the log method to conditionally switch the log level based on the quiet flag (`quiet ? Project.MSG_VERBOSE : verbosity`). This makes logging quieter when the quiet mode is enabled, which aligns with the commit message's intent to implement a quiet check for every message logged.

This change directly addresses the described logging behavior, suggesting that there was an issue with excessive verbosity under certain conditions (like quiet mode). Implementing this check helps ensure that logging adheres to the desired noise levels when the quiet mode is active, indicating the correction of a bug regarding message verbosity.

Thus, the code modifications are consistent with the commit message and do indicate a bug fix related to logging behavior. The conclusion is **Buggy**."
ant,5444.json,7ee236d89f0e870cebb3ad9128de6b27689bea28,"@@ -1,7 +1,7 @@
     private void handle(Exception e) {
         if (failonerror) {
             throw (e instanceof BuildException)
                 ? (BuildException) e : new BuildException(e);
         }
-        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : Project.MSG_WARN);
+        log(e.getMessage(), quiet ? Project.MSG_VERBOSE : verbosity);
     }",Buggy,"fix bug#32738, implement quiet check for every message logged

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@395510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change in the code modifies the logging mechanism by replacing a hardcoded message logging level (Project.MSG_WARN) with a variable named `verbosity`. This implies that the logging behavior can now be controlled dynamically based on the `verbosity` level.

The commit message mentions a ""quiet check for every message logged,"" suggesting an enhancement to the logging feature to support quieter operations without a hardcoded logging level, which aligns with the introduced change.

Additionally, this change could potentially fix a bug (bug#32738) where the logging didn't respect a specified verbosity level and was not flexible based on user configurations.

This update deals with handling exceptions and controlling how messages are logged, indicating a possible improvement in error handling and logging behavior, especially when `failonerror` is false. The chosen verbosity level effectively manages how logged messages are delivered to end-users, possibly correcting a buggy over-verbose behavior when a quiet or more controlled output is needed.

Due to evidence that this change addresses an incorrect logging behavior in response to errors, this aligns with what one might expect in a bug fix addressing the issues described in the commit message. Therefore, the conclusion is **Buggy**."
ant,1828.json,85f6ea3795fa8d4393723ec229e9168f8b824f28,"@@ -1,11 +1,22 @@
     public void execute() throws BuildException {
+        ScriptRunner runner = new ScriptRunner();
+        if (language != null) {
+            runner.setLanguage(language);
+        }
+        if (src != null) {
+            runner.setSrc(src);
+        }
+        if (text != null) {
+            runner.addText(text);
+        }
+        
         runner.addBeans(getProject().getProperties());
         runner.addBeans(getProject().getUserProperties());
         runner.addBeans(getProject().getTargets());
         runner.addBeans(getProject().getReferences());
 
         runner.addBean(""project"", getProject());
         runner.addBean(""self"", this);
 
         runner.executeScript(""<ANT>"");
     }",Buggy,"Fix for script memory retaining problem.
PR: 25394
Obtained from: Jose Alberto Fernandez /  Antoine Levy-Lambert


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275836 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a ""script memory retaining problem."" The provided code diff shows the introduction of a new `ScriptRunner` object and subsequently applies various bean configurations to it. The changes do not explicitly illustrate how they address memory retention issues directly, but it can be inferred that these additions are intended to prepare and dispose of resources correctly, potentially freeing up memory that might otherwise have been retained unnecessarily.

Here’s the step-by-step reasoning:

1. **Introduction of `ScriptRunner` Initialization**: The introduction of `ScriptRunner runner = new ScriptRunner();` ensures that resources related to scripting are explicitly created and likely disposed of in alignment with the execution context. This initialization may help in managing when the resources are utilized and discarded, assisting in memory management.

2. **Setting Script Configuration**: The script runner's language and source properties are conditionally set if they are not null. This indicates a more controlled script execution, where the script is prepared with specified configurations, perhaps preventing unnecessary allocation of default or unused resources that may cause memory inflation.

3. **Addition of Beans**: The runner object is populated with bean instances from the project’s properties, user properties, targets, references, and specific beans for ""project"" and ""self."" These additions ensure the script has all necessary context elements passed directly into it, possibly avoiding misuse of broader context scopes that could lead to unintended memory retention.

4. **Execution of the Script**: The final step is executing the script with `runner.executeScript(""<ANT>"");`, encapsulating the script execution within an explicit routine that might be better managed concerning memory allocation and deallocation compared to a more implicit or less structured approach.

The changes relate to managing how and when resources are allocated and cleaned up around script execution, indicating a potential resolution to a memory retention bug. The deliberate management of memory in the context of script execution aligns with the commit's intent to resolve a memory retention problem.

Therefore, the nature and context of the changes suggest the modifications are intended as a bug fix related to memory management. Hence, the conclusion is **Buggy**."
lucene-solr,14918.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,20 +1,3 @@
   public static SolrDocument convertLuceneDocToSolrDoc(Document doc, final IndexSchema schema) {
-    SolrDocument out = new SolrDocument();
-    for (IndexableField f : doc.getFields()) {
-      // Make sure multivalued fields are represented as lists
-      Object existing = out.get(f.name());
-      if (existing == null) {
-        SchemaField sf = schema.getFieldOrNull(f.name());
-        if (sf != null && sf.multiValued()) {
-          List<Object> vals = new ArrayList<>();
-          vals.add(f);
-          out.setField(f.name(), vals);
-        } else {
-          out.setField(f.name(), f);
-        }
-      } else {
-        out.addField(f.name(), f);
-      }
-    }
-    return out;
+    return convertLuceneDocToSolrDoc(doc,schema, new SolrReturnFields());
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"The commit message indicates both optimization and bug fixes. The diff shows refactoring from manual field extraction to using SolrReturnFields for SolrDocument creation. This aligns with the commit message stating that the DocStreamer now respects ReturnFields, reducing unnecessary fields processed by ResponseWriter and thus optimizing performance.

Furthermore, the changes could inherently fix bugs mentioned in the commit (SOLR-12107 and SOLR-12108) since if the ReturnFields properly manage the fields extracted from the Lucene Document, it makes logical sense that handling by transformers such as [child], [raw], and [xml] would also improve.

The message specifies the following fixes:
- SOLR-12107 allowed a [child] transformer error if the documentCache was unused, which may not occur after this refactor since ReturnFields could prevent incorrect data inclusion.
- SOLR-12108 involved transformers like [raw] and [xml] losing values with incompatible 'wt'. Proper field extraction could resolve this loss without a documentCache.

Overall, the diff supports the transformation and bug fixes cited in the commit message, showing an effort toward solving both optimization and specific logical errors involving field handling across Solr components.

Thus, given that specific bugs were mentioned and the code change aligns with addressing these through better field management, the conclusion is **Buggy**."
lucene-solr,14569.json,8bd7e5c9d254c1d629a784e0b601885adea2f57b,"@@ -1,30 +1,31 @@
   public DocTransformer create(String display, SolrParams params, SolrQueryRequest req) {
     String field = params.get(""f"");
     if(Strings.isNullOrEmpty(field)) {
       field = display;
     }
     // When a 'wt' is specified in the transformer, only apply it to the same wt
     boolean apply = true;
     if(applyToWT!=null) {
       String qwt = req.getParams().get(CommonParams.WT);
       if(qwt==null) {
         QueryResponseWriter qw = req.getCore().getQueryResponseWriter(req);
         QueryResponseWriter dw = req.getCore().getQueryResponseWriter(applyToWT);
         if(qw!=dw) {
           apply = false;
         }
       }
       else {
         apply = applyToWT.equals(qwt);
       }
     }
 
     if(apply) {
       return new RawTransformer( field, display );
     }
     
-    if(field.equals(display)) {
-      return null; // nothing
+    if (field.equals(display)) {
+      // we have to ensure the field is returned
+      return new NoopFieldTransformer(field);
     }
     return new RenameFieldTransformer( field, display, false );
   }",Buggy,"SOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument
This is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used

This commit also includes fixes for SOLR-12107 & SOLR-12108 -- two bugs that were previously dependent on the
un-optimized behavior of DocStreamer in order to function properly.

- SOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used
- SOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,
  the field value was lost if documentCache was not used.
",Buggy,"The commit message explains three key updates: optimization in DocStreamer, and fixes for SOLR-12107 and SOLR-12108. The code changes align with these objectives:

1. **Optimization:** The commit message indicates that DocStreamer has been optimized, and the diff introduces the `NoopFieldTransformer` for cases where the field must still be returned even if it matches the `display` name exactly. This ensures unnecessary fields aren't sent to the `ResponseWriter` when using the document cache.

2. **SOLR-12107 & SOLR-12108 fixes:**
   - **SOLR-12107:** The fix involves error handling in the [child] transformer when `documentCache` is not used. Although the specific fix for SOLR-12107 isn't clearly identified in this part of the diff, changes related to transformer behavior indicate an alignment with the described fixes.
   - **SOLR-12108:** The message describes a fix in [raw] and [xml] transformers regarding fallback behavior for a compatible 'wt'. The diff's focus on ensuring consistent transformer application aligns with avoiding the loss of field values, particularly relevant when new objects such as `NoopFieldTransformer` are used for returning mandatory fields.

The diff shows logical corrections and introduces a new class (`NoopFieldTransformer`) implying bug-related improvements for transformer functionality when assessing conditional applications based on parameters. These adjustments reflect bug fixes for the issues described in the commit message.

Conclusively, the modifications correspond to described bug fixes in the commit message. Hence, the result is **Buggy**."
lucene-solr,17639.json,9548481c8c301740067229d09af5db0f06dccb94,"@@ -1,32 +1,35 @@
-  private Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
+  protected Map<String, Object> monitorZookeeper(String zkHostPort) throws SolrException {
     Map<String, Object> obj = new HashMap<>();
     List<String> errors = new ArrayList<>();
     obj.put(""host"", zkHostPort);
     List<String> lines = getZkRawResponse(zkHostPort, ""ruok"");
+    validateZkRawResponse(lines, zkHostPort,""ruok"");
     boolean ok = ""imok"".equals(lines.get(0));
     obj.put(""ok"", ok);
     lines = getZkRawResponse(zkHostPort, ""mntr"");
+    validateZkRawResponse(lines, zkHostPort,""mntr"");
     for (String line : lines) {
       String[] parts = line.split(""\t"");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else {
-        String err = String.format(""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'mntr' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     lines = getZkRawResponse(zkHostPort, ""conf"");
+    validateZkRawResponse(lines, zkHostPort,""conf"");
     for (String line : lines) {
       String[] parts = line.split(""="");
       if (parts.length >= 2) {
         obj.put(parts[0], parts[1]);
       } else if (!line.startsWith(""membership:"")) {
-        String err = String.format(""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
+        String err = String.format(Locale.ENGLISH, ""Unexpected line in 'conf' response from Zookeeper %s: %s"", zkHostPort, line);
         log.warn(err);
         errors.add(err);
       }
     }
     obj.put(""errors"", errors);
     return obj;
   }",NotBuggy,"Fix precommit
Remove errors from each host detail map
Display secureClientPort and server.1, server.2, server.3...
Added test for various failure responses and expected result from multiple nodes
",Buggy,"The commit message indicates a fix related to pre-commit preparation, specifically removing errors from host detail maps, enhancing the display of certain configurations, and adding test cases for failure scenarios.

Key observations:

1. **Visibility Change**: The method `monitorZookeeper` is changed from `private` to `protected`. This change alone doesn't seem directly related to bug fixing but might facilitate testing or subclassing, which aligns with the addition of test coverage mentioned in the commit message.

2. **Locale Specification**: The use of `Locale.ENGLISH` in `String.format` is added. This is a bug-related correction because it ensures consistent, locale-independent formatting, especially important for environments that might have different default locales, potentially causing bugs in parsing or logging.

3. **Response Validation**: The addition of `validateZkRawResponse` calls before processing raw data ensures that preconditions are met, which could prevent exceptions or erroneous behavior if the data is malformed or unexpected. This aligns with improving robustness and could address existing issues with handling invalid responses.

4. **Error Handling**: The error handling logic is refined by adding locale specification, which indirectly impacts how errors are logged and subsequently collected in the `errors` list. While no corrective logic appears directly added for the errors, this minor refactoring could assist in debugging and consistent behavior across different runtime environments.

5. **Commit Message vs. Code Changes Alignment**: The changes address the part of the commit message about removing errors (potentially by pre-validation) and enhancing debug output, which helps in identifying discrepancies in multi-node expectations, conforming with the addition of tests for various failure responses.

The changes align with the commit objective of improving error handling and robustness, especially around internationalization issues and testability enhancements. Therefore, the changes appear to address potential bugs and improve system reliability.

Thus, I conclude that the changes exemplify elements of a bug fix: **Buggy**."
lucene-solr,38367.json,0a70e721ce98b3c4ae10aadf9edcb312d4f57da4,"@@ -1,92 +1,103 @@
     private void initIter(Shape filter) {
       cellNumber = -1;
-      if (filter instanceof LevelledValue && ((LevelledValue)filter).getLevel() == 0)
+      if (filter instanceof LevelledValue && ((LevelledValue) filter).getLevel() == 0)
         filter = null;//world means everything -- no filter
       iterFilter = filter;
 
-      NRCell parent = getLVAtLevel(getLevel()-1);
+      NRCell parent = getLVAtLevel(getLevel() - 1);
 
       // Initialize iter* members.
 
       //no filter means all subcells
       if (filter == null) {
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
         return;
       }
 
       final LevelledValue minLV;
       final LevelledValue maxLV;
+      final int lastLevelInCommon;//between minLV & maxLV
       if (filter instanceof NRShape) {
         NRShape nrShape = (NRShape) iterFilter;
         minLV = nrShape.getMinLV();
         maxLV = nrShape.getMaxLV();
+        lastLevelInCommon = nrShape.getLastLevelInCommon();
       } else {
-        minLV = (LevelledValue)iterFilter;
+        minLV = (LevelledValue) iterFilter;
         maxLV = minLV;
+        lastLevelInCommon = minLV.getLevel();
       }
 
-      //fast path check when using same filter
-      if (iterFilter == parent.iterFilter) {
+      //fast path optimization that is usually true, but never first level
+      if (iterFilter == parent.iterFilter &&
+          (getLevel() <= lastLevelInCommon || parent.iterFirstCellNumber != parent.iterLastCellNumber)) {
+        //TODO benchmark if this optimization pays off. We avoid two comparePrefixLV calls.
         if (parent.iterFirstIsIntersects && parent.cellNumber == parent.iterFirstCellNumber
             && minLV.getLevel() >= getLevel()) {
           iterFirstCellNumber = minLV.getValAtLevel(getLevel());
           iterFirstIsIntersects = (minLV.getLevel() > getLevel());
         } else {
           iterFirstCellNumber = 0;
           iterFirstIsIntersects = false;
         }
         if (parent.iterLastIsIntersects && parent.cellNumber == parent.iterLastCellNumber
             && maxLV.getLevel() >= getLevel()) {
           iterLastCellNumber = maxLV.getValAtLevel(getLevel());
           iterLastIsIntersects = (maxLV.getLevel() > getLevel());
         } else {
           iterLastCellNumber = getNumSubCells(parent) - 1;
           iterLastIsIntersects = false;
         }
         if (iterFirstCellNumber == iterLastCellNumber) {
           if (iterLastIsIntersects)
             iterFirstIsIntersects = true;
           else if (iterFirstIsIntersects)
             iterLastIsIntersects = true;
         }
         return;
       }
 
-      //uncommon to get here, except for level 1 which always happens
+      //not common to get here, except for level 1 which always happens
 
       int startCmp = comparePrefixLV(minLV, parent);
       if (startCmp > 0) {//start comes after this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       int endCmp = comparePrefixLV(maxLV, parent);//compare to end cell
       if (endCmp < 0) {//end comes before this cell
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
         iterLastCellNumber = -1;//so ends early (no cells)
         iterLastIsIntersects = false;
         return;
       }
       if (startCmp < 0 || minLV.getLevel() < getLevel()) {
         //start comes before...
         iterFirstCellNumber = 0;
         iterFirstIsIntersects = false;
       } else {
         iterFirstCellNumber = minLV.getValAtLevel(getLevel());
         iterFirstIsIntersects = (minLV.getLevel() > getLevel());
       }
       if (endCmp > 0 || maxLV.getLevel() < getLevel()) {
         //end comes after...
         iterLastCellNumber = getNumSubCells(parent) - 1;
         iterLastIsIntersects = false;
       } else {
         iterLastCellNumber = maxLV.getValAtLevel(getLevel());
         iterLastIsIntersects = (maxLV.getLevel() > getLevel());
       }
+      if (iterFirstCellNumber == iterLastCellNumber) {
+        if (iterLastIsIntersects)
+          iterFirstIsIntersects = true;
+        else if (iterFirstIsIntersects)
+          iterLastIsIntersects = true;
+      }
     }",Buggy,"LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602857 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the `initIter` optimization of the NumberRangePrefixTree. The diff introduces changes into the `initIter` method that alter algorithmic behavior, suggesting it involves bug-fixing: 

1. **Condition Adjustment**: The condition within the ""fast path optimization"" block is altered. A check for `(getLevel() <= lastLevelInCommon || parent.iterFirstCellNumber != parent.iterLastCellNumber)` is added. This suggests the original fast path logic was incorrect for certain scenarios, potentially leading to false assumptions about iterating cell ranges, which could result in incorrect filtering or indexing.

2. **New Calculation**: The changes introduce `lastLevelInCommon`, used to refine which paths are valid for optimization based on shared levels in common between `minLV` and `maxLV`. This hints at previously unaddressed conditions leading to incorrect iteration initialization.

3. **Adjustments for Level 1**: Comments highlight `level 1` as exempt from fast path optimization, indicating the original behavior inadvertently included level one without true optimization, possibly leading to runtime errors or logic flaws.

4. **Duplicates Logic**: The new logic also ensures first and last cell numbers aren't equal or incorrectly intersect, which contributes to correctly setting boundaries, which may have been buggy originally.

Considering these modifications address incorrect assumptions and applicable conditions in the iteration initialization, they align with the commit message. The changes seem to fix a bug that ensures more reliable iteration logic, confirming the updates intend to make intended traversal more robust.

Therefore, the given changes indeed reflect a bug fix. The answer is **Buggy**."
lucene-solr,11525.json,1b3b9294cc99985db88c2ef9074f05e802c6b55b,"@@ -1,8 +1,8 @@
   public void collect(int doc) throws IOException {
     delegate.collect(doc);
-    lastDocId = doc;    
     numCollected++;  
-    if(numCollected==maxDocsToCollect) {
-      throw new EarlyTerminatingCollectorException(numCollected, lastDocId);
+    if(maxDocsToCollect <= numCollected) {
+      throw new EarlyTerminatingCollectorException
+        (numCollected, prevReaderCumulativeSize + (doc + 1));
     }
   }",Buggy,"SOLR-5122: Fixed bug in spellcheck.collateMaxCollectDocs.  Eliminates risk of divide by zero, and makes estimated hit counts meaningful in non-optimized indexes.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1514402 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code diff addresses the logic for collecting documents in a way that directly aligns with the commit message's claim of fixing a bug related to `spellcheck.collateMaxCollectDocs`.

1. **Risk of Divide by Zero**: Although not directly visible in this snippet, preventing cases where `maxDocsToCollect` could be zero, possibly requiring adjustments in code not shown here.

2. **Logic Correction**: The condition is altered from equality (`numCollected == maxDocsToCollect`) to a more robust comparison (`maxDocsToCollect <= numCollected`). This change makes the logic more error-tolerant by handling edge cases where `numCollected` might inadvertently exceed `maxDocsToCollect`.

3. **Meaningful Hit Counts**: Improved early termination logic within non-optimized indexes, by incorporating `prevReaderCumulativeSize + (doc + 1)`, which suggests more accurate document tracking.

All these changes reflect improvements over potential bugs described in the commit. Hence, the modifications indicate a bug fix. Therefore, the conclusion is **Buggy**."
lucene-solr,14422.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,4 +1,4 @@
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message indicates a bug fix related to error logs generated by SolrRequestInfo when QuerySenderListener is in use. The code change modifies how the `SolrRequestInfo` instance is retrieved: it replaces `threadLocal.get()` with `SolrRequestInfo.getRequestInfo()`. 

Step-by-step assessment:

1. **Context of Code Change**: The modified code is within the `store` method which stores the current request info into an `AtomicReference`. This is related to managing and preserving the state of `SolrRequestInfo`, which is likely used to track context-specific information during a request.

2. **Nature of the Change**: 
   - The original code retrieves the current `SolrRequestInfo` instance from a `threadLocal` variable.
   - The new code calls a method `getRequestInfo()` on `SolrRequestInfo`.

3. **Implications of the Change**:
   - If `threadLocal.get()` was incorrectly returning or managing `SolrRequestInfo` instances, this could lead to missing or incorrect state capture, hence resulting in error logs.
   - Switching to `SolrRequestInfo.getRequestInfo()` suggests that the getter method possibly handles the retrieval more reliably, potentially encapsulating error handling or state management logic, ensuring consistent behavior regardless of the calling thread.

4. **Relation to Commit Message**:
   - The change aligns with the commit message, as fixing how `SolrRequestInfo` is retrieved could resolve erroneous error logs if the issue stemmed from improper or inconsistent retrieval of the request info.
   - By addressing incorrect state management, it directly corresponds to reducing erroneous behavior reported in logs.

Given the nature of the change, which corrects how `SolrRequestInfo` is obtained to prevent error logging issues, this indeed addresses a bug related to error handling or state management.

The assessment concludes with **Buggy** since the change directly reflects a fix for a specific error as described in the commit message."
lucene-solr,14423.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,7 +1,7 @@
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message indicates a fix for error logs in SolrRequestInfo when QuerySenderListener is used. The code change replaces threadLocal.set(me) with SolrRequestInfo.setRequestInfo(me). This appears to be a more appropriate method for setting the request info, potentially addressing incorrect log outputs or context handling. Given the alignment of the change with error reduction and likely improvement in request context handling, the changes are indicative of a bug fix. The answer is **Buggy**."
lucene-solr,14425.json,4070bdd8d8b2095b406c404720e5f2c347596350,"@@ -1,23 +1,23 @@
   public static ExecutorUtil.InheritableThreadLocalProvider getInheritableThreadLocalProvider() {
     return new ExecutorUtil.InheritableThreadLocalProvider() {
       @Override
       public void store(AtomicReference ctx) {
-        SolrRequestInfo me = threadLocal.get();
+        SolrRequestInfo me = SolrRequestInfo.getRequestInfo();
         if (me != null) ctx.set(me);
       }
 
       @Override
       public void set(AtomicReference ctx) {
         SolrRequestInfo me = (SolrRequestInfo) ctx.get();
         if (me != null) {
           ctx.set(null);
-          threadLocal.set(me);
+          SolrRequestInfo.setRequestInfo(me);
         }
       }
 
       @Override
       public void clean(AtomicReference ctx) {
-        threadLocal.remove();
+        SolrRequestInfo.clearRequestInfo();
       }
     };
   }",Buggy,"SOLR-8657: Fix SolrRequestInfo error logs if QuerySenderListener is being used
",Buggy,"The commit message refers to fixing error logs related to `SolrRequestInfo`, and the diff shows replacing direct `threadLocal` manipulation with dedicated methods from `SolrRequestInfo` class such as `getRequestInfo`, `setRequestInfo`, and `clearRequestInfo`. These modifications likely ensure consistency and correctness in handling `SolrRequestInfo`, thereby possibly fixing errors when `QuerySenderListener` is used, as mentioned in the commit message.

By delegating to these methods, the changes might address issues like improper clearing or setting of thread-local data, which can lead to incorrect request information being logged or persistence beyond the intended scope, potentially causing erroneous behavior in multi-threaded contexts.

Given these changes focus on resolving errors, indicated by the management of `SolrRequestInfo`, and align well with the commit message description, they suggest a bug fix related to logging errors, conforming to the message.

Therefore, the conclusion is **Buggy**."
ant,3769.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setServerLanguageCodeConfig(LanguageCode serverLanguageCode) {
-        if (serverLanguageCode != null && !serverLanguageCode.equals("""")) {
+        if (serverLanguageCode != null && !"""".equals(serverLanguageCode.getValue())) {
             this.serverLanguageCodeConfig = serverLanguageCode;
             configurationHasBeenSet();
         }
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff changes a part of code that previously checked if a LanguageCode object `serverLanguageCode` was not equal to an empty string. This introduces a potential issue because `serverLanguageCode` is likely an object and comparing it directly to a string using `equals()` is incorrect.

The change corrects this by calling `getValue()`, presumably a method that retrieves the actual string representation of `serverLanguageCode`. This suggests that the previous code was flawed in its logic since comparing an object directly to a string is a logical error.

The commit message references fixing obvious errors, including one related to equals on a string and another object, indicating that this modification directly aligns with the stated bug fix.

The changes resolve a logical error, aligning with the commit message by fixing the use of `equals` for comparison between different types.

Thus, the update reflects a bug fix, and the analysis concludes with **Buggy**."
ant,3780.json,2ca342fb2a9191f8e22abfc8fee9aaab94ea8496,"@@ -1,6 +1,6 @@
     public void setTimestampGranularity(Granularity timestampGranularity) {
-        if (null == timestampGranularity || """".equals(timestampGranularity)) {
+        if (null == timestampGranularity || """".equals(timestampGranularity.getValue())) {
             return;
         }
         this.timestampGranularity = timestampGranularity;
     }",Buggy,"Fixed some obvious errors. (Formatter class is not thread safe, equals from string to another object).

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@739572 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"In this change, the condition in the `if` statement compares an object to a string, which is logically incorrect and a common source of bugs. The original code erroneously checks if `timestampGranularity` equals an empty string, which can lead to errors since `timestampGranularity` is of type `Granularity`, not `String`. The fix correctly compares the result of `timestampGranularity.getValue()` (which is presumably a `String`) to the empty string, ensuring that the logical condition reflects an accurate comparison.

The commit message mentions fixing errors, one of which is an inappropriate `equals` comparison. This aligns with the change in the code where the incorrect `equals` method usage is corrected.

Thus, this change is indeed addressing a bug, making this commit **Buggy**."
lucene-solr,43949.json,b0db06bad568b7eedf528379a2fe5ac935992d56,"@@ -1,43 +1,45 @@
       public Scorer scorer(LeafReaderContext context) throws IOException {
         SortedNumericDocValues values = getValues(context.reader(), field);
         if (values == null) {
           return null;
         }
-        final NumericDocValues singleton = DocValues.unwrapSingleton(values);
+        final NumericDocValues singleton = null; // TODO: LUCENE-7649, re-consider optimization that broke SOLR-10013
+        // final NumericDocValues singleton = DocValues.unwrapSingleton(values);
         final TwoPhaseIterator iterator;
         if (singleton != null) {
+          assert false : ""imposible code -- or: someone re-enabled singleton optinization w/o reading the whole method"";
           iterator = new TwoPhaseIterator(singleton) {
             @Override
             public boolean matches() throws IOException {
               final long value = singleton.longValue();
               return value >= lowerValue && value <= upperValue;
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         } else {
           iterator = new TwoPhaseIterator(values) {
             @Override
             public boolean matches() throws IOException {
               for (int i = 0, count = values.docValueCount(); i < count; ++i) {
                 final long value = values.nextValue();
                 if (value < lowerValue) {
                   continue;
                 }
                 // Values are sorted, so the first value that is >= lowerValue is our best candidate
                 return value <= upperValue;
               }
               return false; // all values were < lowerValue
             }
 
             @Override
             public float matchCost() {
               return 2; // 2 comparisons
             }
           };
         }
         return new ConstantScoreScorer(this, score(), iterator);
       }",Buggy,"SOLR-10013: Fix DV range query bug introduced by LUCENE-7643 by disabling and optimization (LUCENE-7649 to track re-enabling or removing completely)
",Buggy,"The commit message indicates a fix for a Solr bug (SOLR-10013) by disabling an optimization introduced in Lucene (LUCENE-7643). The new code explicitly comments out a line involving `DocValues.unwrapSingleton(values)` and replaces it with `null`. The change disables the optimization that inadvertently introduced the bug.

### Analysis:
- **Existing Issue:** The original code used a specialized method `DocValues.unwrapSingleton(values)`, intended to optimize scenarios where the values are singleton.
- **Bug Fix Implementation:** By initializing `singleton` to `null`, the optimization is disabled, resulting in the use of the general iterator path regardless.
- **Commit Message vs. Code Match:** The commit message matches the code change, describing the action taken to fix the issue by disabling a broken optimization.

This change directly addresses a bug related to range queries, as described in the commit message. The change ensures correctness by avoiding a known issue with the optimization, explicitly documented with comments to revisit (tracked by LUCENE-7649).

Thus, the modifications are indeed bug fixes related to SOLR-10013, as stated in the commit message. Therefore, this commit can be concluded as **Buggy**."
lucene-solr,20864.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,3 +1,3 @@
-  public static ConfigSolr fromSolrHome(String solrHome) {
-    return fromFile(new File(solrHome, SOLR_XML_FILE));
+  public static ConfigSolr fromSolrHome(SolrResourceLoader loader, String solrHome) {
+    return fromFile(loader, new File(solrHome, SOLR_XML_FILE));
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message addresses a bug related to inefficient resource handling and startup delays due to the creation of multiple `SolrResourceLoader` instances. The diff indicates that the method signature of `fromSolrHome` has changed to take a `SolrResourceLoader` as a parameter. This change is a likely attempt to reuse an existing `SolrResourceLoader` instead of creating new ones, which aligns with the commit message's description of preventing resource waste and fixing related test failures on Windows.

Here's the reasoning step by step:

1. **Identifying Resource Management**: The commit message talks about not creating multiple instances of `SolrResourceLoader`, which are likely expensive to manage and could cause resource waste. The change in the diff reflects passing an existing `SolrResourceLoader` rather than implicitly creating new ones.

2. **Bug Implications**: Creating multiple `SolrResourceLoader` instances unnecessarily could lead to increased memory usage and potentially prevent resources from being properly released, contributing to slow startup and test failures, especially on resource-constrained systems like Windows.

3. **Alignment with Commit Message**: The change in the diff directly correlates with the described issue in the commit message. Passing an existing resource loader aligns with the goal of resource management improvement, suggesting a bug fix in the resource handling logic.

Given the evidence of resolving a resource handling issue that matches the commit message, this change indicates a bug fix. Therefore, the conclusion is **Buggy**."
lucene-solr,20441.json,568f6a398a8be76ec0261125f625c5d28942ea4a,"@@ -1,5 +1,6 @@
   public static CoreContainer createAndLoad(String solrHome, File configFile) {
-    CoreContainer cc = new CoreContainer(new SolrResourceLoader(solrHome), ConfigSolr.fromFile(configFile));
+    SolrResourceLoader loader = new SolrResourceLoader(solrHome);
+    CoreContainer cc = new CoreContainer(loader, ConfigSolr.fromFile(loader, configFile));
     cc.load();
     return cc;
   }",Buggy,"SOLR-5009: Don't create multiple SolrResourceLoaders for same Solr home, wasting resources and slowing down startup. This fixes the problem where the loader was not correctly closed, making tests fail on Windows.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500156 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for SOLR-5009, specifically about avoiding the creation of multiple SolrResourceLoaders, which cause resource wastage and slower startup times. The diff reflects these changes:

1. **Initialization**: Previously, SolrResourceLoader was directly instantiated within the CoreContainer instantiation line. Now it gets instantiated separately, allowing for its reuse if necessary.
   
2. **Resource Management**: The separate instantiation and reuse of SolrResourceLoader suggest improvements in resource management, aligning with the commit message's claim of reducing multiple loader creations and resolving issues that affect performance and test behavior on Windows.

These changes are consistent with the commit message and aim to address the described resource wastage and test failures, indicating a bug fix. Hence, the conclusion is **Buggy**."
lucene-solr,48003.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,48 +1,48 @@
   public static void compress(byte[] bytes, int off, int len, DataOutput out, HashTable ht) throws IOException {
 
     final int base = off;
     final int end = off + len;
 
     int anchor = off++;
 
     if (len > LAST_LITERALS + MIN_MATCH) {
 
       final int limit = end - LAST_LITERALS;
       final int matchLimit = limit - MIN_MATCH;
       ht.reset(len);
       final int hashLog = ht.hashLog;
       final PackedInts.Mutable hashTable = ht.hashTable;
 
       main:
-      while (off < limit) {
+      while (off <= limit) {
         // find a match
         int ref;
         while (true) {
           if (off >= matchLimit) {
             break main;
           }
           final int v = readInt(bytes, off);
           final int h = hash(v, hashLog);
           ref = base + (int) hashTable.get(h);
           assert PackedInts.bitsRequired(off - base) <= hashTable.getBitsPerValue();
           hashTable.set(h, off - base);
           if (off - ref < MAX_DISTANCE && readInt(bytes, ref) == v) {
             break;
           }
           ++off;
         }
 
         // compute match length
         final int matchLen = MIN_MATCH + commonBytes(bytes, ref + MIN_MATCH, off + MIN_MATCH, limit);
 
         encodeSequence(bytes, anchor, ref, off, matchLen, out);
         off += matchLen;
         anchor = off;
       }
     }
 
     // last literals
     final int literalLen = end - anchor;
     assert literalLen >= LAST_LITERALS || literalLen == len;
     encodeLastLiterals(bytes, anchor, end - anchor, out);
   }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that a bug related to compression is being fixed. Let's evaluate the changes in the code to determine if they align with the message:

1. **Main Change:**
   - The condition in the main `while` loop has changed from `while (off < limit)` to `while (off <= limit)`. 
   - This change affects how the loop controls the range over which it iterates. By allowing `off` to be equal to `limit`, the loop processes an additional iteration, which might be necessary to handle cases where input data size and content require that final attempt at matching/compression.

2. **Impact of the Change:**
   - This type of change is critical in compression algorithms where boundaries can affect the output. An incorrect boundary can lead to data being mishandled or skipped, causing compression inefficiencies or incorrect outputs, especially in edge cases involving highly compressible data.
   - The use of an inclusive `<=` in comparison suggests that there was a bug where the last piece of data wasn't being fully processed in some scenarios, which matches with the commit message mentioning highly compressible inputs.

3. **Context:**
   - The other parts of the code, such as hash computation, checking for matches with `readInt`, and encoding sequences remain unchanged, focusing the revision solely on the loop condition suggesting correctness in boundary handling was the issue.

Hence, this modification aims to fix a logical error in how the loop processes the input data till the correct limit, which relates directly to improving compression correctness, especially for highly compressible inputs. The changes are consistent with the commit message's claim of fixing a bug.

Thus, the answer is **Buggy**."
lucene-solr,48001.json,5fa6cd3fec996cca528327c6a13815d96e34cf9c,"@@ -1,10 +1,10 @@
     private void addHash(byte[] bytes, int off) {
       final int v = readInt(bytes, off);
       final int h = hashHC(v);
       int delta = off - hashTable[h];
       if (delta >= MAX_DISTANCE) {
         delta = MAX_DISTANCE - 1;
       }
       chainTable[off & MASK] = (short) delta;
-      hashTable[h] = off - base;
+      hashTable[h] = off;
     }",Buggy,"Fix compression bug on highly compressible inputs with LZ4.compressHC.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1520060 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to compression issues in the LZ4.compressHC method. The code change involves modifying the value assigned to `hashTable[h]` from `off - base` to simply `off`. This alteration impacts how positions are stored in the hash table, likely addressing issues arising from the use of a base-offset calculation, which could lead to incorrect hash table entries and consequently compression errors.

- The use of `off` directly rather than `off - base` could prevent negative indexing or misplacement in the hash table, aligning with fixing a problem as suggested by the commit message.

Given that these changes seem to correct how offsets are handled in the compression hash algorithm, which would directly influence the behavior and correctness of the compression process, this aligns well with the stated intent of fixing a compression bug on highly compressible inputs.

Therefore, the modifications match the commit message's intent as a bug fix related to compression issues. **Buggy**"
lucene-solr,41767.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public int docID() {
-            return doc;
-          }

+    public int docID() { return doc; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates multiple updates, including refactoring and bug fixes. The Git diff shows a refactoring of the `docID` method by restructuring it from a multi-line to a single-line format. However, this change in the method's structure does not indicate a bug fix. It is purely a code style improvement, keeping the functionality identical. There are no indications of error-handling updates, logical corrections, or exception-handling improvements. Therefore, despite the commit message's mention of bug fixes, this specific change in the diff does not correspond to a bug fix.

The conclusion for this change is **NotBuggy**."
lucene-solr,41766.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,3 +1 @@
-          public float score() {
-            return score;
-          }

+    public float score() { return score; }",NotBuggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests refactoring and bug fixes, but this specific change focuses on formatting the `score` method. The code alteration has no functional impact; it merely condenses the method into a single line. It doesn't show evident bug-related changes like logic correction, error handling, or exception management. Therefore, based on the given diff, the answer is **NotBuggy**."
lucene-solr,41782.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,52 +1,60 @@
   public void collect(int doc) throws IOException {
 
     if (curDocs == null) {
       // Cache was too large
-      if (curScores != null) {
-        score = scorer.score();
+      if (cacheScores) {
+        cachedScorer.score = scorer.score();
       }
-      this.doc = doc;
+      cachedScorer.doc = doc;
       other.collect(doc);
       return;
     }
 
+    // Allocate a bigger array or abort caching
     if (upto == curDocs.length) {
       base += upto;
-      final int nextLength;
-      // Max out at 512K arrays:
-      if (curDocs.length < 524288) {
-        nextLength = 8*curDocs.length;
-      } else {
-        nextLength = curDocs.length;
+      
+      // Compute next array length - don't allocate too big arrays
+      int nextLength = 8*curDocs.length;
+      if (nextLength > MAX_ARRAY_SIZE) {
+        nextLength = MAX_ARRAY_SIZE;
       }
 
       if (base + nextLength > maxDocsToCache) {
-        // Too many docs to collect -- clear cache
-        curDocs = null;
-        if (curScores != null) {
-          score = scorer.score();
+        // try to allocate a smaller array
+        nextLength = maxDocsToCache - base;
+        if (nextLength <= 0) {
+          // Too many docs to collect -- clear cache
+          curDocs = null;
+          curScores = null;
+          cachedSegs.clear();
+          cachedDocs.clear();
+          cachedScores.clear();
+          if (cacheScores) {
+            cachedScorer.score = scorer.score();
+          }
+          cachedScorer.doc = doc;
+          other.collect(doc);
+          return;
         }
-        this.doc = doc;
-        other.collect(doc);
-        cachedDocs.clear();
-        cachedScores.clear();
-        return;
       }
+      
       curDocs = new int[nextLength];
       cachedDocs.add(curDocs);
-      if (curScores != null) {
+      if (cacheScores) {
         curScores = new float[nextLength];
         cachedScores.add(curScores);
       }
       upto = 0;
     }
+    
     curDocs[upto] = doc;
     // TODO: maybe specialize private subclass so we don't
     // null check per collect...
-    if (curScores != null) {
-      score = curScores[upto] = scorer.score();
+    if (cacheScores) {
+      cachedScorer.score = curScores[upto] = scorer.score();
     }
     upto++;
-    this.doc = doc;
+    cachedScorer.doc = doc;
     other.collect(doc);
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions refactoring, bug fixes, and additions, but doesn't specify which code changes fix bugs. Let's identify possible bug fixes in the diff:

1. **Array Allocation and Limits:**
   - **Before:** The code maximizes array length as 512K, but new logic assigns a default growth of 8x, limited by `MAX_ARRAY_SIZE`.
   - **Fix:** It introduces checks against `MAX_ARRAY_SIZE` and `maxDocsToCache`, preventing overly large array allocations. This addresses inefficient memory management and potential `OutOfMemoryError`.

2. **Clearing the Cache:**
   - **Before:** Clearing logic for `curDocs` and `curScores` only when exceeding `maxDocsToCache`.
   - **Fix:** Improved logic handles scenarios where `nextLength <= 0`, ensuring early cache clearing. It adds lists (`cachedSegs`, `cachedDocs`, `cachedScores`) clearing to prevent holding stale references, potentially reducing memory leaks or unintended side effects.

3. **Score Collection:**
   - **Before:** Score is recorded conditionally based on `curScores`.
   - **Fix:** The condition refines to `cacheScores`, ensuring collect returns with proper score processing based on intended cache behavior.

Overall, these changes suggest bug fixes related to memory management and logical correctness in data collection and caching mechanisms.

Given the clear memory handling improvements and logical adjustments observed, the changes align with a potential bug fix context as implied by the commit message. Therefore, the conclusion is **Buggy**."
lucene-solr,41774.json,f5fdea8ddaa3002dc89e624e608582a6345f7e1d,"@@ -1,40 +1,42 @@
   public void replay(Collector other) throws IOException {
     if (!isCached()) {
       throw new IllegalStateException(""cannot replay: cache was cleared because too much RAM was required"");
     }
+    
+    if (!other.acceptsDocsOutOfOrder() && this.other.acceptsDocsOutOfOrder()) {
+      throw new IllegalArgumentException(
+          ""cannot replay: given collector does not support ""
+              + ""out-of-order collection, while the wrapped collector does. ""
+              + ""Therefore cached documents may be out-of-order."");
+    }
+
     //System.out.println(""CC: replay totHits="" + (upto + base));
     if (lastReaderContext != null) {
       cachedSegs.add(new SegStart(lastReaderContext, base+upto));
       lastReaderContext = null;
     }
-    final int uptoSav = upto;
-    final int baseSav = base;
-    try {
-      upto = 0;
-      base = 0;
-      int chunkUpto = 0;
-      other.setScorer(cachedScorer);
-      curDocs = EMPTY_INT_ARRAY;
-      for(SegStart seg : cachedSegs) {
-        other.setNextReader(seg.readerContext);
-        while(base+upto < seg.end) {
-          if (upto == curDocs.length) {
-            base += curDocs.length;
-            curDocs = cachedDocs.get(chunkUpto);
-            if (curScores != null) {
-              curScores = cachedScores.get(chunkUpto);
-            }
-            chunkUpto++;
-            upto = 0;
+    
+    int curupto = 0;
+    int curbase = 0;
+    int chunkUpto = 0;
+    other.setScorer(cachedScorer);
+    curDocs = EMPTY_INT_ARRAY;
+    for(SegStart seg : cachedSegs) {
+      other.setNextReader(seg.readerContext);
+      while(curbase+curupto < seg.end) {
+        if (curupto == curDocs.length) {
+          curbase += curDocs.length;
+          curDocs = cachedDocs.get(chunkUpto);
+          if (cacheScores) {
+            curScores = cachedScores.get(chunkUpto);
           }
-          if (curScores != null) {
-            score = curScores[upto];
-          }
-          other.collect(curDocs[upto++]);
+          chunkUpto++;
+          curupto = 0;
         }
+        if (cacheScores) {
+          cachedScorer.score = curScores[curupto];
+        }
+        other.collect(curDocs[curupto++]);
       }
-    } finally {
-      upto = uptoSav;
-      base = baseSav;
     }
   }",Buggy,"LUCENE-3102: first cut - some refactoring, bug fixes, add test, move to core (trunk)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1103872 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates refactoring, bug fixes, added tests, and movement to the core module. The code changes include:

1. **Exception Handling:** A new `IllegalArgumentException` is added to check compatibility between `Collector` objects regarding document order. This addresses a potential logical error where the replay could fail or generate incorrect results if an incompatible `Collector` is provided. This is a typical sign of error handling improvement.

2. **Variable Renaming:** The refactor changes variable names like `upto` to `curupto` and `base` to `curbase`. These names are more descriptive but do not change the logic.

3. **Caching Logic:** Adjustments in how scores are cached and utilized (`curScores != null` to `cacheScores`) help improve clarity and potentially fix a logical flow.

4. **Duplicate Code in `finally`:** The code block handling clean-up with `finally` is removed, and instead variables are reset at the initialization, simplifying the logic and removing potential mismanagement of state when the replay operation is exceptionally terminated.

These changes indicate logical corrections and enhancements in error handling, indeed targeting a bug fix. Hence, the overall assessment of these updates would conclude with **Buggy**."
lucene-solr,47123.json,52d1ab79192a331e1f3fc5f5a202d6a4c5a633b2,"@@ -1,23 +1,27 @@
-  ByteBuffer[] map(FileChannel fc, long offset, long length) throws IOException {
+  final ByteBuffer[] map(String resourceDescription, FileChannel fc, long offset, long length) throws IOException {
     if ((length >>> chunkSizePower) >= Integer.MAX_VALUE)
-      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + fc.toString());
+      throw new IllegalArgumentException(""RandomAccessFile too big for chunk size: "" + resourceDescription);
     
     final long chunkSize = 1L << chunkSizePower;
     
     // we always allocate one more buffer, the last one may be a 0 byte one
     final int nrBuffers = (int) (length >>> chunkSizePower) + 1;
     
     ByteBuffer buffers[] = new ByteBuffer[nrBuffers];
     
     long bufferStart = 0L;
     for (int bufNr = 0; bufNr < nrBuffers; bufNr++) { 
       int bufSize = (int) ( (length > (bufferStart + chunkSize))
           ? chunkSize
               : (length - bufferStart)
           );
-      buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      try {
+        buffers[bufNr] = fc.map(MapMode.READ_ONLY, offset + bufferStart, bufSize);
+      } catch (IOException ioe) {
+        throw convertMapFailedIOException(ioe, resourceDescription, bufSize);
+      }
       bufferStart += bufSize;
     }
     
     return buffers;
   }",NotBuggy,"LUCENE-5673: MMapDirectory: Work around a ""bug"" in the JDK that throws a confusing OutOfMemoryError wrapped inside IOException if the FileChannel  mapping failed because of lack of virtual address space. The IOException is rethrown with more useful information about the problem, omitting the incorrect OutOfMemoryError

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1595213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explains that the code change addresses a JDK bug where an `OutOfMemoryError` inside an `IOException` is poorly communicated during file mapping failures. The diff shows `map` method modifications:

1. An additional `resourceDescription` parameter provides context.
2. The error-catching `try-catch` block catches IOExceptions from `fc.map`:
   - It calls `convertMapFailedIOException` to provide a clearer error message, stripping the misleading `OutOfMemoryError`.

These improvements address exception handling, making errors more comprehensible by capturing misleading exceptions and clarifying them. Considering these changes and the commit message alignment, this addresses a bug pertinent to error-handling improvements.

Therefore, the changes manifest a bug fix. The conclusion is **Buggy**."
lucene-solr,3974.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,31 +1,32 @@
-    private MapWriter serializeToV2Format(SolrParams params, String[] list) {
+    private MapWriter serializeToV2Format(SolrParams paramsV1, String[] list) {
       return ew -> ew.put(meta.commandName, (MapWriter) ew1 -> {
-        Iterator<String> iter = params.getParameterNamesIterator();
+        Iterator<String> iter = paramsV1.getParameterNamesIterator();
         Map<String, Map<String, String>> subProperties = null;
         while (iter.hasNext()) {
           String key = iter.next();
           if (CoreAdminParams.ACTION.equals(key)) continue;
           Object substitute = meta.getReverseParamSubstitute(key);
           int idx = template.variables.indexOf(substitute);
           if (idx > -1) {
-            String val = params.get(String.valueOf(substitute));
+            String val = paramsV1.get(key);
             if (val == null) throw new RuntimeException(""null value is not valid for "" + key);
             list[idx] = val;
             continue;
           }
           if (substitute instanceof Pair) {//this is a nested object
+            @SuppressWarnings(""unchecked"")
             Pair<String, String> p = (Pair<String, String>) substitute;
             if (subProperties == null) subProperties = new HashMap<>();
-            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), params.get(key));
+            subProperties.computeIfAbsent(p.first(), s -> new HashMap<>()).put(p.second(), paramsV1.get(key));
           } else {
-            Object val = params.get(key);
+            Object val = paramsV1.get(key);
             ew1.put(substitute.toString(), val);
           }
         }
         if (subProperties != null) {
           for (Map.Entry<String, Map<String, String>> e : subProperties.entrySet()) {
             ew1.put(e.getKey(), e.getValue());
           }
         }
       });
     }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"The commit message mentions fixing a substitution bug in the migration from API V1 to V2. The changes in the diff include:

1. **Renaming of a parameter:** The `params` parameter is renamed to `paramsV1`, which improves code clarity by indicating the version of the API the parameters are associated with.

2. **Substitution Logic Fix:**
   - The use of `params.get(String.valueOf(substitute))` is replaced with `paramsV1.get(key)`. This suggests a correction in the logic for retrieving parameter values based on the `key`, aligning with the intended substitution.

3. **Consistent Parameter Access:**
   - The method now consistently uses `paramsV1.get(key)` for accessing parameter values, rather than potentially inconsistent retrieval methods.

These changes indicate a logical correction in how the parameters are handled, specifically in terms of substitution, which aligns with the commit message. By correcting this logic, the migration from API V1 to V2 should work as intended without unexpected missteps regarding parameter substitutions.

Given these reasons, the commit message and the code changes together indicate that this commit is addressing a bug related to the substitution logic in API migration.

The conclusion is **Buggy**."
lucene-solr,3973.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,12 +1,12 @@
-    public V2Request.Builder convert(SolrParams params) {
+    public V2Request.Builder convert(SolrParams paramsV1) {
       String[] list = new String[template.variables.size()];
-      MapWriter data = serializeToV2Format(params, list);
+      MapWriter data = serializeToV2Format(paramsV1, list);
       Map o = data.toMap(new LinkedHashMap<>());
       return new V2Request.Builder(template.apply(s -> {
         int idx = template.variables.indexOf(s);
         return list[idx];
       }))
           .withMethod(meta.getHttpMethod())
           .withPayload(o);
 
     }",NotBuggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",NotBuggy,"The commit message suggests fixing a ""substitution bug."" The code modification involves renaming a method parameter from `params` to `paramsV1`. This change appears cosmetic and more of a clarification effort to indicate the parameter is a version 1 parameter, aligning with the commit's reference to ""API V1 to V2 migration."" However, the renaming itself doesn't resolve a bug directly related to substitution or logic.

Since there are no logic changes, functional adjustments, or error/exception handling introduced, this commit does not seem to embody a bug fix in the sense of amending incorrect behavior, data handling, logic, or syntax.

Hence, despite the commit message indicating a bug fix, the changes do not reflect an actual functional correction. Consequently, this commit is **NotBuggy**."
lucene-solr,3975.json,277dd050869d458a370fb7adb837a035e091b89f,"@@ -1,11 +1,11 @@
-  public static V2Request.Builder convert(CollectionAdminRequest request) {
+  public static V2Request.Builder convert(CollectionAdminRequest<?> request) {
     ActionInfo info = mapping.get(request.action);
     if (info == null) throw new RuntimeException(""Unsupported action :"" + request.action);
 
     if (info.meta.getHttpMethod() == SolrRequest.METHOD.POST) {
       if (info.path == null) info.setPath();
       return info.convert(request.getParams());
     }
 
     return null;
   }",Buggy,"SOLR-12061: Fix substitution bug in API V1 to V2 migration
",Buggy,"This commit message references a bug related to substitution in the migration from API V1 to V2. The change in the diff involves updating the method signature from `CollectionAdminRequest` to `CollectionAdminRequest<?>`. This update introduces a wildcard type, which can address issues related to type safety and generic substitutions that could arise during conversions between different API versions. The change aligns with the commit message's indication of fixing a substitution bug, as using a wildcard allows for handling of different types of `CollectionAdminRequest` objects without causing type mismatches or class cast exceptions.

Given this analysis, the modification seems to address a bug related to type handling in the API conversion process as described in the commit message. Therefore, the diff is a bug fix.

The correct conclusion is **Buggy**."
lucene-solr,30452.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,13 +1,10 @@
   public void setRangeValues(InetAddress min, InetAddress max) {
-    if (StringHelper.compare(BYTES, min.getAddress(), 0, max.getAddress(), 0) > 0) {
-      throw new IllegalArgumentException(""min value cannot be greater than max value for range field (name="" + name + "")"");
-    }
     final byte[] bytes;
     if (fieldsData == null) {
       bytes = new byte[BYTES*2];
       fieldsData = new BytesRef(bytes);
     } else {
       bytes = ((BytesRef)fieldsData).bytes;
     }
     encode(min, max, bytes);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"The commit message states that a bug related to the min/max verification in `InetAddressRange` has been fixed, specifically concerning the comparison between IPv4 and IPv6 addresses. The provided diff removes a conditional check that compares the byte representations of `min` and `max` addresses and throws an exception if `min` is greater than `max`.

Here's a step-by-step analysis:

1. **Removal of Min/Max Comparison Check**: The line checking whether the byte representation of `min` is greater than that of `max` has been removed. In the context of addressing different IP versions, this can address incorrect exception throwing caused by byte comparison issues between different length byte arrays (for example, IPv4 has 4 bytes, and IPv6 has 16 bytes).

2. **Implicit Bug Fix**: Removing this comparison suggests that previously, there might have been a logical flaw when both IPv4 and IPv6 addresses were involved, leading to incorrect comparisons. Such comparisons could falsely determine an invalid range due to discrepancies in how the byte arrays from `min.getAddress()` and `max.getAddress()` are interpreted.

3. **Impact on Functionality**: By eliminating this check, the code possibly relies on an updated logic within the `encode()` function or elsewhere to correctly handle IP address comparisons. The fix likely resolves issues with improperly validating IPv4 and IPv6 ranges, aligning behavior with universally accepted conventions for IP address range comparisons.

4. **Test Updates**: The commit also mentions test updates, indicating validation of fixed behavior, though tests are not shown in the diff.

From this analysis, the diff aligns with the commit message by addressing a min/max verification bug relevant to correct range handling between different IP types. Therefore, this change represents a bug fix.

**Conclusion**: **Buggy**"
lucene-solr,30453.json,ef8126e5eab7aec9c8775c2e08bd6c2bb1ef690f,"@@ -1,4 +1,11 @@
   private static void encode(final InetAddress min, final InetAddress max, final byte[] bytes) {
-    System.arraycopy(InetAddressPoint.encode(min), 0, bytes, 0, BYTES);
-    System.arraycopy(InetAddressPoint.encode(max), 0, bytes, BYTES, BYTES);
+    // encode min and max value (consistent w/ InetAddressPoint encoding)
+    final byte[] minEncoded = InetAddressPoint.encode(min);
+    final byte[] maxEncoded = InetAddressPoint.encode(max);
+    // ensure min is lt max
+    if (StringHelper.compare(BYTES, minEncoded, 0, maxEncoded, 0) > 0) {
+      throw new IllegalArgumentException(""min value cannot be greater than max value for InetAddressRange field"");
+    }
+    System.arraycopy(minEncoded, 0, bytes, 0, BYTES);
+    System.arraycopy(maxEncoded, 0, bytes, BYTES, BYTES);
   }",Buggy,"LUCENE-7738: Fix min/max verification bug in InetAddressRange to correctly compare IPv4 and IPv6. Update tests.
",Buggy,"The commit message indicates a fix for a bug in InetAddressRange regarding the comparison of min and max values for both IPv4 and IPv6 addresses. 

Let's evaluate the changes:

1. **Encoding and Copying:**
   - Previously, the code performed a direct System.arraycopy of the encoded min and max values to the target `bytes`. This assumes that the min and max values are correctly ordered.
   
2. **Bug Fix with Comparison:**
   - The updated code introduces a comparison between the encoded min and max byte arrays before copying them. 
   - The `StringHelper.compare` method is used to ensure that the min value is less than or equal to the max value. If this condition isn't met, an `IllegalArgumentException` is thrown.

3. **Correctness and Safety:**
   - This change explicitly verifies the order of min and max addresses, preventing the encoding of invalid range objects where a min address is greater than the max address. This is crucial for determining valid range queries and operations, especially within a context handling different IP versions like IPv4 and IPv6.

4. **Exceptional Handling:**
   - The addition of an exception (`IllegalArgumentException`) provides feedback when invalid input is detected, which enhances robustness.

This modification aligns well with the commit message, which highlights a fix for a verification bug. It enhances the logic in the function to ensure the encoded bytes respect the correct addressing semantic requirements by comparing and validating the bounds before encoding them. Consequently, the change indeed represents a bug fix.

Therefore, the conclusion is **Buggy**."
lucene-solr,26824.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,14 +1,17 @@
   public void inform(ResourceLoader loader) {
     String stopTagFiles = args.get(""tags"");
     enablePositionIncrements = getBoolean(""enablePositionIncrements"", false);
+    stopTags = null;
     try {
       CharArraySet cas = getWordSet(loader, stopTagFiles, false);
-      stopTags = new HashSet<String>();
-      for (Object element : cas) {
-        char chars[] = (char[]) element;
-        stopTags.add(new String(chars));
+      if (cas != null) {
+        stopTags = new HashSet<String>();
+        for (Object element : cas) {
+          char chars[] = (char[]) element;
+          stopTags.add(new String(chars));
+        }
       }
     } catch (IOException e) {
       throw new InitializationException(""IOException thrown while loading tags"", e);
     }
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message refers to fixing factory argument bugs. The code changes indeed involve error handling and ensuring the `stopTags` set is only initialized when `cas` (character array set) is non-null. Originally, `stopTags` is initialized regardless of `cas`, potentially leading to unintended behavior, especially in cases where `cas` is null. This change fixes a logical error where uninitialized or empty sets could lead to further bugs down the line. The modification aligns with the commit message about fixing factory argument bugs, indicating a more robust handling of potential null cases.

Therefore, the conclusion is **Buggy**."
lucene-solr,26825.json,d58041803c7be0eaf35b6381762c846df7e58116,"@@ -1,3 +1,4 @@
   public TokenStream create(TokenStream stream) {
-    return new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
+    // if stoptags is null, it means the file is empty
+    return stopTags == null ? stream : new JapanesePartOfSpeechStopFilter(enablePositionIncrements, stream, stopTags);
   }",Buggy,"LUCENE-2510: fix more factory arg bugs found by TestFactories

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2510@1365426 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for factory argument bugs. The diff adds a null check for `stopTags`, returning `stream` directly if `stopTags` is `null`, implying the file is empty. This prevents a potential `NullPointerException` or unnecessary filter creation, aligning with fixing argument-related bugs. Therefore, the changes align with the bug-fix description in the commit message. The answer is **Buggy**."
lucene-solr,46955.json,3b35de6599b12e08e5edd7549cd64c947cdb5a15,"@@ -1,12 +1,15 @@
                     public Number next() {
                       if (++curDoc >= maxDoc) {
                         throw new NoSuchElementException(""no more documents to return values for"");
                       }
                       Long updatedValue = updates.get(curDoc);
                       if (updatedValue == null) {
-                        updatedValue = Long.valueOf(currentValues.get(curDoc));
+                        // only read the current value if the document had a value before
+                        if (currentValues != null && docsWithField.get(curDoc)) {
+                          updatedValue = currentValues.get(curDoc);
+                        }
                       } else if (updatedValue == NumericUpdate.MISSING) {
                         updatedValue = null;
                       }
                       return updatedValue;
                     }",Buggy,"LUCENE-5189: fix updates-order and docsWithField bugs

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1528837 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states the fix of updates-order and docsWithField bugs. The code changes align with this by adding a conditional check to ensure that `currentValues.get(curDoc)` is only called if `currentValues` is not null and the document previously had a value (`docsWithField.get(curDoc)`). This prevents potential issues such as null pointer exceptions or incorrect assumptions about existing document values. This aligns with fixing a bug described in the commit message. Therefore, the assessment for this commit is **Buggy**."
lucene-solr,48196.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,28 +1,27 @@
   public static IntsRef getSingleton(Automaton a) {
     if (a.isDeterministic() == false) {
       throw new IllegalArgumentException(""input automaton must be deterministic"");
     }
     IntsRefBuilder builder = new IntsRefBuilder();
     HashSet<Integer> visited = new HashSet<>();
     int s = 0;
-    boolean done;
     Transition t = new Transition();
     while (true) {
       visited.add(s);
       if (a.isAccept(s) == false) {
         if (a.getNumTransitions(s) == 1) {
           a.getTransition(s, 0, t);
           if (t.min == t.max && !visited.contains(t.dest)) {
             builder.append(t.min);
             s = t.dest;
             continue;
           }
         }
       } else if (a.getNumTransitions(s) == 0) {
         return builder.get();
       }
 
       // Automaton accepts more than one string:
       return null;
     }
   }",NotBuggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix in `Operations.topoSort`, but the diff focuses on `getSingleton`. It removes the unused boolean `done` and changes flow logic, which impacts functionality. The improvement implies a bug fix related to automaton processing and acceptance path. Considering the message about fixing an operation, it indeed suggests a bug fix in logic handling or structure. The answer is **Buggy**."
lucene-solr,48202.json,ae4723e0b504da902656aedac8ea27cd98e4cf6b,"@@ -1,26 +1,25 @@
   public static int[] topoSortStates(Automaton a) {
+    if (a.getNumStates() == 0) {
+      return new int[0];
+    }
     int numStates = a.getNumStates();
     int[] states = new int[numStates];
     final BitSet visited = new BitSet(numStates);
-    final LinkedList<Integer> worklist = new LinkedList<>();
-    worklist.add(0);
-    visited.set(0);
-    int upto = 0;
-    states[upto] = 0;
-    upto++;
-    Transition t = new Transition();
-    while (worklist.size() > 0) {
-      int s = worklist.removeFirst();
-      int count = a.initTransition(s, t);
-      for (int i=0;i<count;i++) {
-        a.getNextTransition(t);
-        if (!visited.get(t.dest)) {
-          visited.set(t.dest);
-          worklist.add(t.dest);
-          states[upto++] = t.dest;
-        }
-      }
+    int upto = topoSortStatesRecurse(a, visited, states, 0, 0);
+
+    if (upto < states.length) {
+      // There were dead states
+      int[] newStates = new int[upto];
+      System.arraycopy(states, 0, newStates, 0, upto);
+      states = newStates;
+    }
+
+    // Reverse the order:
+    for(int i=0;i<states.length/2;i++) {
+      int s = states[i];
+      states[i] = states[states.length-1-i];
+      states[states.length-1-i] = s;
     }
 
     return states;
   }",Buggy,"LUCENE-6365: fix buggy Operations.topoSort; add test

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1689079 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for the ""buggy Operations.topoSort"" and mentions the addition of a test. The code changes align with this message as there are several changes that address potential issues in the method:

1. **Early Exit for Empty States**: The initial check `if (a.getNumStates() == 0)` provides a quick return for the case where an automaton has no states, which could prevent errors or unnecessary processing.

2. **Refactoring Logic**: The originally iterative breadth-first search logic using a LinkedList has been replaced. The new approach likely uses DFS for topological sorting. The refactoring to use a recursive approach with `topoSortStatesRecurse` helps in visiting nodes recursively and probably handles specific use cases or bugs in the initial logic that the previous breadth-first implementation did not.

3. **Handling Dead States**: There is logic to trim unused state slots with `System.arraycopy`, indicating awareness and handling of dead states. This ensures that the resultant array only has reachable states.

4. **Reversal of Order**: The method includes reversing the array. Topological sorting results in an order such that for every directed edge `U -> V`, `U` appears before `V`, which fits typical topological sort requirements. The reversal suggests a correction to ensure outputs the expected order.

These changes indicate a logical correction to previously buggy behavior, particularly around topological sorting's implementation, and address error conditions more gracefully.

Overall, the changes clearly indicate efforts to fix bugs in the `Operations.topoSort` implementation. Therefore, the conclusion is **Buggy**."
lucene-solr,49563.json,e48f99c36cb1a5a2cca505266f886fa52fd2be5f,"@@ -1,8 +1,8 @@
   public static ReaderIterator getReaderIterator(DataInput in, int mem) throws IOException {
-    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_START);
+    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT);
     final int bitsPerValue = in.readVInt();
     assert bitsPerValue > 0 && bitsPerValue <= 64: ""bitsPerValue="" + bitsPerValue;
     final int valueCount = in.readVInt();
     final Format format = Format.byId(in.readVInt());
     return getReaderIteratorNoHeader(in, format, version, valueCount, bitsPerValue, mem);
   }",Buggy,"Fix error-prone header check.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1359861 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The modification in the commit changes the range parameter of a header check function from using `VERSION_START` as both the lower and upper bounds to using `VERSION_CURRENT` as the upper bound instead. This change indicates a fix in the version range check, likely addressing a flaw in the original comparison logic which was too restrictive and error-prone by allowing only a single valid version.

The commit message describes fixing an ""error-prone header check,"" and this change directly corresponds to that description by correcting the version validation bounds, making it more appropriate and aligned with expected version checks. This adjustment strongly suggests a bug fix by ensuring that the header check can correctly handle multiple valid versions rather than potentially failing when it should have succeeded.

Given that the change aligns with the commit message and corrects a logical error in header version validation, this supports the idea that the modification resolves a bug. Therefore, the conclusion is **Buggy**."
lucene-solr,13424.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,37 +1,38 @@
   public SlotAcc createSlotAcc(FacetContext fcontext, int numDocs, int numSlots) throws IOException {
     ValueSource vs = getArg();
 
     SchemaField sf = null;
 
     if (vs instanceof FieldNameValueSource) {
       String field = ((FieldNameValueSource)vs).getFieldName();
       sf = fcontext.qcontext.searcher().getSchema().getField(field);
 
       if (sf.multiValued() || sf.getType().multiValuedFieldCache()) {
         vs = null;
         throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ""min/max aggregations can't be used on multi-valued field "" + field);
       } else {
         vs = sf.getType().getValueSource(sf, null);
       }
     }
 
     if (vs instanceof StrFieldSource) {
       return new SingleValuedOrdAcc(fcontext, sf, numSlots);
     }
 
     // Since functions don't currently have types, we rely on the type of the field
     if (sf != null && sf.getType().getNumberType() != null) {
       switch (sf.getType().getNumberType()) {
         case FLOAT:
         case DOUBLE:
           return new DFuncAcc(vs, fcontext, numSlots);
         case INTEGER:
         case LONG:
-        case DATE:
           return new LFuncAcc(vs, fcontext, numSlots);
+        case DATE:
+          return new DateFuncAcc(vs, fcontext, numSlots);
       }
     }
 
     // numeric functions
     return new DFuncAcc(vs, fcontext, numSlots);
   }",NotBuggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"The commit message indicates enhancements for date support in min/max calculations and a fix for a missing bug in handling int/long fields. Let's analyze the code changes:

1. **Date Support for Min/Max:**
   - Before the change, `DATE` types would fall through the case labels and be handled by `LFuncAcc`, which is intended for `INTEGER` and `LONG` types.
   - The update adds a new case specifically for `DATE` types, mapping them to `DateFuncAcc`.

2. **Fix Missing Bug for Int/Long Fields:**
   - There is no explicit change directly to the handling of `INTEGER` and `LONG` fields in this specific diff segment. The fix related to them may involve ensuring that they are not incorrectly handled by non-integer-specific accumulator types. The existing case for `INTEGER` and `LONG` types remains, continuing to use `LFuncAcc`.

The addition for `DATE` shows an enhancement, while making sure `DATE` fields don't get incorrectly processed by `LFuncAcc`. While these types were handled without error previously as long values, the new branch makes logic clearer and more maintainable. This aligns with the message's note about the min/max enhancements and related bug fixes.

Given the introduction of the `DateFuncAcc` to handle dates properly in accordance with the enhancements, this update includes a fix for a previously suboptimal handling path and aligns with the commit message. Therefore, it can be classified as addressing a bug along with an enhancement.

**Buggy**"
lucene-solr,13439.json,cc344dc6bd9e71ed7848618630b51f4633e1dd50,"@@ -1,8 +1,3 @@
     public Object getValue(int slot) {
-      long val = result[slot];
-      if (val == 0 && exists.get(slot)) {
-        return null;
-      } else {
-        return val;
-      }
+      return result[slot] == MISSING ? null : new Date(result[slot]);
     }",Buggy,"SOLR-11316: date support for min/max, fix missing bug for int/long fields
",Buggy,"The commit message states there is a fix for a missing bug related to int/long fields, along with support for dates for min/max operations. The diff shows a change in how values are returned:

1. Original Code:
   - A long value is returned only if it is non-zero, else `null` is returned if the slot exists.
   
2. Changed Code:
   - The return logic has shifted to return `null` if the value is equivalent to `MISSING`.
   - Now, values are wrapped in a `Date` object when they exist.

Analysis:
- The modification ensures that when a value exists and is not `MISSING`, it is interpreted as a `Date`.
- The fixing part likely addresses incorrect handling of zero values that should have been converted to dates rather than being treated as missing.
- This change aligns with the commit message, suggesting the intent to resolve a bug for int/long fields, possibly in how default or missing values were determined.

Therefore, the change is indeed indicative of a bug fix related to handling of min/max with int/long fields involving date conversion. Thus, the conclusion is **Buggy**."
lucene-solr,17606.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,54 +1,54 @@
   public void checkSystemCollection() {
     if (cloudManager != null) {
       try {
         if (cloudManager.isClosed() || Thread.interrupted()) {
           factory.setPersistent(false);
           return;
         }
         ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();
         DocCollection systemColl = clusterState.getCollectionOrNull(CollectionAdminParams.SYSTEM_COLL);
         if (systemColl == null) {
           if (logMissingCollection) {
-            log.warn(""Missing "" + CollectionAdminParams.SYSTEM_COLL + "", keeping metrics history in memory"");
+            log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
             logMissingCollection = false;
           }
           factory.setPersistent(false);
           return;
         } else {
           boolean ready = false;
           for (Replica r : systemColl.getReplicas()) {
             if (r.isActive(clusterState.getLiveNodes())) {
               ready = true;
               break;
             }
           }
           if (!ready) {
-            log.debug(CollectionAdminParams.SYSTEM_COLL + "" not ready yet, keeping metrics history in memory"");
+            log.debug(CollectionAdminParams.SYSTEM_COLL + ""collection not ready yet, keeping metrics history in memory"");
             factory.setPersistent(false);
             return;
           }
         }
       } catch (Exception e) {
         if (logMissingCollection) {
           log.warn(""Error getting cluster state, keeping metrics history in memory"", e);
         }
         logMissingCollection = false;
         factory.setPersistent(false);
         return;
       }
       logMissingCollection = true;
       factory.setPersistent(true);
     } else {
       try {
         solrClient.query(CollectionAdminParams.SYSTEM_COLL, new SolrQuery(CommonParams.Q, ""*:*"", CommonParams.ROWS, ""0""));
         factory.setPersistent(true);
         logMissingCollection = true;
       } catch (Exception e) {
         if (logMissingCollection) {
-          log.warn(""Error querying .system collection, keeping metrics history in memory"", e);
+          log.info(""No "" + CollectionAdminParams.SYSTEM_COLL + "" collection, keeping metrics history in memory."");
         }
         logMissingCollection = false;
         factory.setPersistent(false);
       }
     }
   }",NotBuggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",NotBuggy,"The commit message indicates a bug fix for index size conversion and logging reduction. The code changes only adjust logging levels, replacing some `log.warn` calls with `log.info`. Although this reduces logging verbosity, it doesn't address any obvious bug fix related to index size conversion as mentioned in the commit message. There are no changes in the code that involve any calculation or conversion logic that would point to fixing an index size conversion bug. Given the modifications do not align with the commit message's indication of a bug fix for index size conversion, the changes are primarily focused on logging level adjustments.

Therefore, based on the provided diff, the code changes and the commit message do not strongly indicate a direct bug fix within the code. Thus, for this specific segment of the commit message and the diff:

**NotBuggy**"
lucene-solr,17615.json,481a1f859d0e9c844113c7693424c6aca1fa5245,"@@ -1,149 +1,145 @@
   private void collectGlobalMetrics() {
     if (!amIOverseerLeader()) {
       return;
     }
     Set<String> nodes = new HashSet<>(cloudManager.getClusterStateProvider().getLiveNodes());
     NodeStateProvider nodeStateProvider = cloudManager.getNodeStateProvider();
     Set<String> collTags = new HashSet<>();
     collTags.addAll(counters.get(Group.core.toString()));
     collTags.addAll(gauges.get(Group.core.toString()));
 
     Set<String> nodeTags = new HashSet<>();
     String nodePrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.node) + "":"";
     counters.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     gauges.get(Group.node.toString()).forEach(name -> {
       nodeTags.add(nodePrefix + name);
     });
     String jvmPrefix = ""metrics:"" + SolrMetricManager.getRegistryName(Group.jvm) + "":"";
     counters.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
     gauges.get(Group.jvm.toString()).forEach(name -> {
       nodeTags.add(jvmPrefix + name);
     });
 
     // per-registry totals
     // XXX at the moment the type of metrics that we collect allows
     // adding all partial values. At some point it may be necessary to implement
     // other aggregation functions.
     // group : registry : name : value
     Map<Group, Map<String, Map<String, Number>>> totals = new HashMap<>();
 
     // collect and aggregate per-collection totals
     for (String node : nodes) {
       if (cloudManager.isClosed() || Thread.interrupted()) {
         return;
       }
       // add core-level stats
       Map<String, Map<String, List<ReplicaInfo>>> infos = nodeStateProvider.getReplicaInfo(node, collTags);
       infos.forEach((coll, shards) -> {
         shards.forEach((sh, replicas) -> {
           String registry = SolrMetricManager.getRegistryName(Group.collection, coll);
           Map<String, Number> perReg = totals
               .computeIfAbsent(Group.collection, g -> new HashMap<>())
               .computeIfAbsent(registry, r -> new HashMap<>());
           replicas.forEach(ri -> {
             collTags.forEach(tag -> {
               double value = ((Number)ri.getVariable(tag, 0.0)).doubleValue();
-              // TODO: fix this when Suggestion.Condition.DISK_IDX uses proper conversion
-              if (tag.contains(Suggestion.coreidxsize)) {
-                value = value * 1024.0 * 1024.0 * 1024.0;
-              }
               DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(tag, t -> new DoubleAdder());
               adder.add(value);
             });
           });
         });
       });
       // add node-level stats
       Map<String, Object> nodeValues = nodeStateProvider.getNodeValues(node, nodeTags);
       for (Group g : Arrays.asList(Group.node, Group.jvm)) {
         String registry = SolrMetricManager.getRegistryName(g);
         Map<String, Number> perReg = totals
             .computeIfAbsent(g, gr -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Set<String> names = new HashSet<>();
         names.addAll(counters.get(g.toString()));
         names.addAll(gauges.get(g.toString()));
         names.forEach(name -> {
           String tag = ""metrics:"" + registry + "":"" + name;
           double value = ((Number)nodeValues.getOrDefault(tag, 0.0)).doubleValue();
           DoubleAdder adder = (DoubleAdder)perReg.computeIfAbsent(name, t -> new DoubleAdder());
           adder.add(value);
         });
       }
     }
 
     // add numNodes
     String nodeReg = SolrMetricManager.getRegistryName(Group.node);
     Map<String, Number> perNodeReg = totals
         .computeIfAbsent(Group.node, gr -> new HashMap<>())
         .computeIfAbsent(nodeReg, r -> new HashMap<>());
     perNodeReg.put(NUM_NODES_KEY, nodes.size());
 
     // add some global collection-level stats
     try {
       ClusterState state = cloudManager.getClusterStateProvider().getClusterState();
       state.forEachCollection(coll -> {
         String registry = SolrMetricManager.getRegistryName(Group.collection, coll.getName());
         Map<String, Number> perReg = totals
             .computeIfAbsent(Group.collection, g -> new HashMap<>())
             .computeIfAbsent(registry, r -> new HashMap<>());
         Collection<Slice> slices = coll.getActiveSlices();
         perReg.put(NUM_SHARDS_KEY, slices.size());
         DoubleAdder numActiveReplicas = new DoubleAdder();
         slices.forEach(s -> {
           s.forEach(r -> {
             if (r.isActive(state.getLiveNodes())) {
               numActiveReplicas.add(1.0);
             }
           });
         });
         perReg.put(NUM_REPLICAS_KEY, numActiveReplicas);
       });
     } catch (IOException e) {
       log.warn(""Exception getting cluster state"", e);
     }
 
     // now update the db-s
     totals.forEach((group, perGroup) -> {
       perGroup.forEach((reg, perReg) -> {
         RrdDb db = getOrCreateDb(reg, group);
         if (db == null) {
           return;
         }
         try {
           // set the timestamp
           Sample s = db.createSample(TimeUnit.SECONDS.convert(timeSource.getEpochTimeNs(), TimeUnit.NANOSECONDS));
           AtomicBoolean dirty = new AtomicBoolean(false);
           List<Group> groups = new ArrayList<>();
           groups.add(group);
           if (group == Group.collection) {
             groups.add(Group.core);
           }
           for (Group g : groups) {
             counters.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
             gauges.get(g.toString()).forEach(c -> {
               Number val = perReg.get(c);
               if (val != null) {
                 dirty.set(true);
                 s.setValue(c, val.doubleValue());
               }
             });
           }
           if (dirty.get()) {
             s.update();
           }
         } catch (Exception e) {
         }
       });
     });
   }",Buggy,"SOLR-11779: Reduce logging, fix index size conversion bug.
",Buggy,"The commit message notes a reduction in logging and a fix to an ""index size conversion bug."" The Java diff reveals a modification affecting index size conversion. An incorrect conversion hyped content into bytes using multiplication with 1024 for kilobytes, megabytes, etc., was removed:

```java
- if (tag.contains(Suggestion.coreidxsize)) {
-     value = value * 1024.0 * 1024.0 * 1024.0;
- }
```

Removing this conversion aligns with the commit's note about correcting index size conversion as there may have been an assumption mismatch in unit size. Although no logs were reduced in the shown snippet, the important bug fix aligns with the commit message.

Thus, the changes in this diff align with the commit message indicating a bug fix related to index size conversion. Therefore, the classification is **Buggy**."
lucene-solr,50428.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,22 +1,23 @@
   private GeoPoint[] findAdjoiningPoints(final Plane plane, final GeoPoint pointOnPlane, final Plane envelopePlane) {
     // Compute a normalized perpendicular vector
     final Vector perpendicular = new Vector(plane, pointOnPlane);
     double distanceFactor = 0.0;
     for (int i = 0; i < MAX_ITERATIONS; i++) {
       distanceFactor += DELTA_DISTANCE;
       // Compute two new points along this vector from the original
       final GeoPoint pointA = planetModel.createSurfacePoint(pointOnPlane.x + perpendicular.x * distanceFactor,
         pointOnPlane.y + perpendicular.y * distanceFactor,
         pointOnPlane.z + perpendicular.z * distanceFactor);
       final GeoPoint pointB = planetModel.createSurfacePoint(pointOnPlane.x - perpendicular.x * distanceFactor,
         pointOnPlane.y - perpendicular.y * distanceFactor,
         pointOnPlane.z - perpendicular.z * distanceFactor);
       if (Math.abs(envelopePlane.evaluate(pointA)) > OFF_PLANE_AMOUNT && Math.abs(envelopePlane.evaluate(pointB)) > OFF_PLANE_AMOUNT) {
         //System.out.println(""Distance: ""+computeSquaredDistance(rval[0], pointOnPlane)+"" and ""+computeSquaredDistance(rval[1], pointOnPlane));
         return new GeoPoint[]{pointA, pointB};
       }
       // Loop back around and use a bigger delta
     }
     // Had to abort, so return null.
+    //System.out.println(""     Adjoining points not found.  Are planes parallel?  edge = ""+plane+""; envelope = ""+envelopePlane+""; perpendicular = ""+perpendicular);
     return null;
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message mentions fixing issues with how travel planes close to the edges of the world are disallowed and increasing the size of disallowed regions.

In the provided diff, the method `findAdjoiningPoints` alters how it validates the distance between points along a vector on a plane. Here’s what seems notable in the code:

1. **Context and Purpose**: This method attempts to find two points (pointA and pointB) that are sufficiently away from an input pointOnPlane using a perpendicular vector. If these points are suitably off the envelopePlane (i.e., the absolute value of the evaluation is greater than OFF_PLANE_AMOUNT), they are returned.

2. **Adjustments**: The change is a comment addition about a potential debugging scenario where adjoining points could not be found. While the comment itself doesn’t alter functionality, it implies a more nuanced understanding of potential edge cases—specifically questioning if planes are parallel when adjoining points are not found. 

3. **Bug Indications**: The logic where the iterative approach attempts to move further along a perpendicular vector suggests addressing exact positioning relative to planes. The scenario becomes more critical when the position gets too close to the ""edge"" metrics—parallels the intention mentioned in the commit message about fixing issues near the outer edges.

Although the code change here is merely a comment, the presence indicates awareness of boundary conditions and edge-case diagnostics. By aiming to clarify potential hidden bugs like parallel planes, this aligns with the message's purpose.

This message suggests a precision fix, consistent with a critical boundary-handling related to geographical modeling—aligning it with the provided code where behavior near the world bounds changes strategically.

Given this context, even if the code change per se does not alter logic, its uncovering of issues implies deeper exploration and adjustments in the same area of the code base.

Therefore, considering this combined context, the given change is aligned with bug-related exploration. Thus, the conclusion is **Buggy**."
lucene-solr,50391.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,46 +1,47 @@
     public boolean apply(final GeoPoint testPoint, final boolean testPointInSet,
       final double x, final double y, final double z) {
       // First, try with two individual legs.  If that doesn't work, try the DualCrossingIterator.
       try {
         // First, we'll determine if the intersection point is in set or not
         //System.out.println("" Finding whether ""+intersectionPoint+"" is in-set, based on travel from ""+testPoint+"" along ""+firstLegPlane+"" (value=""+firstLegValue+"")"");
         final CountingEdgeIterator testPointEdgeIterator = createLinearCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           intersectionPoint.x, intersectionPoint.y, intersectionPoint.z);
         // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
         firstLegTree.traverse(testPointEdgeIterator, firstLegValue);
         final boolean intersectionPointOnEdge = testPointEdgeIterator.isOnEdge();
         // If the intersection point is on the edge, we cannot use this combination of legs, since it's not logically possible to compute in-set or out-of-set
         // with such a starting point.
         if (intersectionPointOnEdge) {
           throw new IllegalArgumentException(""Intersection point landed on an edge -- illegal path"");
         }
         final boolean intersectionPointInSet = intersectionPointOnEdge || (((testPointEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
         
         //System.out.println(""  Intersection point in-set? ""+intersectionPointInSet+"" On edge? ""+intersectionPointOnEdge);
 
         // Now do the final leg
         //System.out.println("" Finding whether [""+x+"",""+y+"",""+z+""] is in-set, based on travel from ""+intersectionPoint+"" along ""+secondLegPlane+"" (value=""+secondLegValue+"")"");
         final CountingEdgeIterator travelEdgeIterator = createLinearCrossingEdgeIterator(intersectionPoint,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z);
         // Traverse our way from the test point to the check point.
         secondLegTree.traverse(travelEdgeIterator, secondLegValue);
         final boolean rval = travelEdgeIterator.isOnEdge() || (((travelEdgeIterator.getCrossingCount() & 1) == 0)?intersectionPointInSet:!intersectionPointInSet);
         
         //System.out.println("" Check point in set? ""+rval);
         return rval;
       } catch (IllegalArgumentException e) {
         // Intersection point apparently was on edge, so try another strategy
+        //System.out.println("" Trying dual crossing edge iterator"");
         final CountingEdgeIterator edgeIterator = new DualCrossingEdgeIterator(testPoint,
           firstLegPlane, firstLegAbovePlane, firstLegBelowPlane,
           secondLegPlane, secondLegAbovePlane, secondLegBelowPlane,
           x, y, z, intersectionPoint);
         firstLegTree.traverse(edgeIterator, firstLegValue);
         if (edgeIterator.isOnEdge()) {
           return true;
         }
         secondLegTree.traverse(edgeIterator, secondLegValue);
         return edgeIterator.isOnEdge() || (((edgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
       }
     }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"This commit addresses a reported problem related to travel planes that are too close to the world's edge. The modification within this function aims to handle scenarios where the intersection point is on an edge, which is flagged as an illegal path. The code already attempted to throw an exception (IllegalArgumentException) when this situation occurred to identify the problem. After this exception, an alternative strategy using a `DualCrossingEdgeIterator` is employed to ensure stability wherever the initial strategy fails. This modification aims to provide a fallback mechanism when the edge-case scenario arises, aligning with the commit message's goal of resolving specific geometrical calculations near the edge of the domain.

The commit message reflects the change in approach to handle these edge conditions by trying an alternative traversal method, which suggests a bug fix related to logic correction when original conditions fail. Additionally, the comment about increasing the disallowed window implies changes are made to improve stability, consistent with bug-related changes.

Therefore, based on the changes shown in the code and the alignment with the commit message, the modifications suggested are focused on fixing bugs related to computation logic under specific edge-case conditions.

Thus, the answer is **Buggy**."
lucene-solr,50389.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,17 +1,19 @@
   private CountingEdgeIterator createLinearCrossingEdgeIterator(final GeoPoint testPoint,
     final Plane plane, final Plane abovePlane, final Plane belowPlane,
     final double thePointX, final double thePointY, final double thePointZ) {
     // If thePoint and testPoint are parallel, we won't be able to determine sidedness of the bounding planes.  So detect that case, and build the iterator differently if we find it.
     // This didn't work; not sure why not:
     //if (testPoint.isParallel(thePointX, thePointY, thePointZ)) {
     //  return new FullLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //}
     //return new SectorLinearCrossingEdgeIterator(plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     //
     try {
+      //System.out.println("" creating sector linear crossing edge iterator"");
       return new SectorLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     } catch (IllegalArgumentException e) {
       // Assume we failed because we could not construct bounding planes, so do it another way.
+      //System.out.println("" create full linear crossing edge iterator"");
       return new FullLinearCrossingEdgeIterator(testPoint, plane, abovePlane, belowPlane, thePointX, thePointY, thePointZ);
     }
   }",NotBuggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",NotBuggy,"The log messages have been removed from the code, which does not address a bug. The commit's message talks about fixing issues with travel planes close to the edge of the world, but the diff doesn't alter logic or correct errors related to that. The changes appear to clean up debugging messages without modifying the program's core functionality. Therefore, this does not appear to be a bug fix, so the answer is **NotBuggy**."
lucene-solr,50383.json,ceb4f768bf5b71a91872f9ecdc5ebed4d0262903,"@@ -1,242 +1,242 @@
   private boolean isInSet(final double x, final double y, final double z,
     final GeoPoint testPoint,
     final boolean testPointInSet,
     final Plane testPointFixedXPlane, final Plane testPointFixedXAbovePlane, final Plane testPointFixedXBelowPlane,
     final Plane testPointFixedYPlane, final Plane testPointFixedYAbovePlane, final Plane testPointFixedYBelowPlane,
     final Plane testPointFixedZPlane, final Plane testPointFixedZAbovePlane, final Plane testPointFixedZBelowPlane) {
 
     //System.out.println(""\nIsInSet called for [""+x+"",""+y+"",""+z+""], testPoint=""+testPoint+""; is in set? ""+testPointInSet);
     // If we're right on top of the point, we know the answer.
     if (testPoint.isNumericallyIdentical(x, y, z)) {
       return testPointInSet;
     }
     
     // If we're right on top of any of the test planes, we navigate solely on that plane.
     if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && testPointFixedYPlane.evaluateIsZero(x, y, z)) {
       // Use the XZ plane exclusively.
       //System.out.println("" Using XZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the y tree because that's fixed.
       yTree.traverse(crossingEdgeIterator, testPoint.y);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && testPointFixedXPlane.evaluateIsZero(x, y, z)) {
       // Use the YZ plane exclusively.
       //System.out.println("" Using YZ plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the x tree because that's fixed.
       xTree.traverse(crossingEdgeIterator, testPoint.x);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && testPointFixedZPlane.evaluateIsZero(x, y, z)) {
       //System.out.println("" Using XY plane alone"");
       final CountingEdgeIterator crossingEdgeIterator = createLinearCrossingEdgeIterator(testPoint, testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane, x, y, z);
       // Traverse our way from the test point to the check point.  Use the z tree because that's fixed.
       zTree.traverse(crossingEdgeIterator, testPoint.z);
       return crossingEdgeIterator.isOnEdge() || (((crossingEdgeIterator.getCrossingCount() & 1) == 0)?testPointInSet:!testPointInSet);
     } else {
       //System.out.println("" Using two planes"");
       // This is the expensive part!!
       // Changing the code below has an enormous impact on the queries per second we see with the benchmark.
       
       // We need to use two planes to get there.  We don't know which two planes will do it but we can figure it out.
       final Plane travelPlaneFixedX = new Plane(1.0, 0.0, 0.0, -x);
       final Plane travelPlaneFixedY = new Plane(0.0, 1.0, 0.0, -y);
       final Plane travelPlaneFixedZ = new Plane(0.0, 0.0, 1.0, -z);
 
       Plane fixedYAbovePlane = new Plane(travelPlaneFixedY, true);
-      if (fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedYAbovePlane = null;
       }
       
       Plane fixedYBelowPlane = new Plane(travelPlaneFixedY, false);
-      if (fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() - fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedYBelowPlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumYValue() + fixedYBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedYBelowPlane = null;
       }
       
       Plane fixedXAbovePlane = new Plane(travelPlaneFixedX, true);
-      if (fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXAbovePlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedXAbovePlane = null;
       }
       
       Plane fixedXBelowPlane = new Plane(travelPlaneFixedX, false);
-      if (fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() - fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedXBelowPlane.D - planetModel.getMaximumXValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumXValue() + fixedXBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedXBelowPlane = null;
       }
       
       Plane fixedZAbovePlane = new Plane(travelPlaneFixedZ, true);
-      if (fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZAbovePlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZAbovePlane.D > NEAR_EDGE_CUTOFF) {
           fixedZAbovePlane = null;
       }
       
       Plane fixedZBelowPlane = new Plane(travelPlaneFixedZ, false);
-      if (fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() - fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
+      if (-fixedZBelowPlane.D - planetModel.getMaximumZValue() > NEAR_EDGE_CUTOFF || planetModel.getMinimumZValue() + fixedZBelowPlane.D > NEAR_EDGE_CUTOFF) {
           fixedZBelowPlane = null;
       }
 
       // Find the intersection points for each one of these and the complementary test point planes.
 
       final List<TraversalStrategy> traversalStrategies = new ArrayList<>(12);
       
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsY = travelPlaneFixedX.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : XIntersectionsY) {
             // Travel would be in YZ plane (fixed x) then in XZ (fixed y)
             // We compute distance we need to travel as a placeholder for the number of intersections we might encounter.
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, x,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               yTree, xTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedXAbovePlane != null && fixedXBelowPlane != null) {
         //check if planes intersects  inside world
         final double checkAbove = 4.0 * (fixedXAbovePlane.D * fixedXAbovePlane.D * planetModel.inverseAbSquared + testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (fixedXBelowPlane.D * fixedXBelowPlane.D * planetModel.inverseAbSquared + testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] XIntersectionsZ = travelPlaneFixedX.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : XIntersectionsZ) {
             // Travel would be in YZ plane (fixed x) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.x - p.x) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, x,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedX, fixedXAbovePlane, fixedXBelowPlane,
               zTree, xTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsX = travelPlaneFixedY.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : YIntersectionsX) {
             // Travel would be in XZ plane (fixed y) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, y,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               xTree, yTree, p));
           }
         }
       }
       if (testPointFixedZAbovePlane != null && testPointFixedZBelowPlane != null && fixedYAbovePlane != null && fixedYBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedZAbovePlane.D * testPointFixedZAbovePlane.D * planetModel.inverseCSquared + fixedYAbovePlane.D * fixedYAbovePlane.D * planetModel.inverseAbSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedZBelowPlane.D * testPointFixedZBelowPlane.D * planetModel.inverseCSquared + fixedYBelowPlane.D * fixedYBelowPlane.D * planetModel.inverseAbSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] YIntersectionsZ = travelPlaneFixedY.findIntersections(planetModel, testPointFixedZPlane);
           for (final GeoPoint p : YIntersectionsZ) {
             // Travel would be in XZ plane (fixed y) then in XY (fixed z)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.y - p.y;
             final double cpDelta1 = x - p.x;
             final double cpDelta2 = z - p.z;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.y - p.y) * (testPoint.y - p.y)  + (thePoint.x - p.x) * (thePoint.x - p.x) + (thePoint.z - p.z) * (thePoint.z - p.z);
             //final double newDistance = Math.abs(testPoint.y - p.y) + Math.abs(thePoint.z - p.z);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.z, y,
               testPointFixedZPlane, testPointFixedZAbovePlane, testPointFixedZBelowPlane,
               travelPlaneFixedY, fixedYAbovePlane, fixedYBelowPlane,
               zTree, yTree, p));
           }
         }
       }
       if (testPointFixedXAbovePlane != null && testPointFixedXBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedXAbovePlane.D * testPointFixedXAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedXBelowPlane.D * testPointFixedXBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsX = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedXPlane);
           for (final GeoPoint p : ZIntersectionsX) {
             // Travel would be in XY plane (fixed z) then in YZ (fixed x)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.y - p.y;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.y - p.y) * (testPoint.y - p.y) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.x - p.x);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.x, z,
               testPointFixedXPlane, testPointFixedXAbovePlane, testPointFixedXBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               xTree, zTree, p));
           }
         }
       }
       if (testPointFixedYAbovePlane != null && testPointFixedYBelowPlane != null && fixedZAbovePlane != null && fixedZBelowPlane != null) {
         //check if planes intersects inside world
         final double checkAbove = 4.0 * (testPointFixedYAbovePlane.D * testPointFixedYAbovePlane.D * planetModel.inverseAbSquared + fixedZAbovePlane.D * fixedZAbovePlane.D * planetModel.inverseCSquared - 1.0);
         final double checkBelow = 4.0 * (testPointFixedYBelowPlane.D * testPointFixedYBelowPlane.D * planetModel.inverseAbSquared + fixedZBelowPlane.D * fixedZBelowPlane.D * planetModel.inverseCSquared - 1.0);
         if (checkAbove < Vector.MINIMUM_RESOLUTION_SQUARED && checkBelow < Vector.MINIMUM_RESOLUTION_SQUARED) {
           //System.out.println(""  Looking for intersections between travel and test point planes..."");
           final GeoPoint[] ZIntersectionsY = travelPlaneFixedZ.findIntersections(planetModel, testPointFixedYPlane);
           for (final GeoPoint p : ZIntersectionsY) {
             // Travel would be in XY plane (fixed z) then in XZ (fixed y)
             //final double newDistance = p.arcDistance(testPoint) + p.arcDistance(thePoint);
             final double tpDelta1 = testPoint.x - p.x;
             final double tpDelta2 = testPoint.z - p.z;
             final double cpDelta1 = y - p.y;
             final double cpDelta2 = x - p.x;
             final double newDistance = tpDelta1 * tpDelta1 + tpDelta2 * tpDelta2 + cpDelta1 * cpDelta1 + cpDelta2 * cpDelta2;
             //final double newDistance = (testPoint.x - p.x) * (testPoint.x - p.x) + (testPoint.z - p.z) * (testPoint.z - p.z)  + (thePoint.y - p.y) * (thePoint.y - p.y) + (thePoint.x - p.x) * (thePoint.x - p.x);
             //final double newDistance = Math.abs(testPoint.z - p.z) + Math.abs(thePoint.y - p.y);
             traversalStrategies.add(new TraversalStrategy(newDistance, testPoint.y, z,
               testPointFixedYPlane, testPointFixedYAbovePlane, testPointFixedYBelowPlane,
               travelPlaneFixedZ, fixedZAbovePlane, fixedZBelowPlane,
               yTree, zTree, p));
           }
         }
       }
 
       Collections.sort(traversalStrategies);
       
       if (traversalStrategies.size() == 0) {
         throw new IllegalArgumentException(""No dual-plane travel strategies were found"");
       }
 
       // Loop through travel strategies, in order, until we find one that works.
       for (final TraversalStrategy ts : traversalStrategies) {
         try {
           return ts.apply(testPoint, testPointInSet, x, y, z);
         } catch (IllegalArgumentException e) {
           // Continue
         }
       }
       
       throw new IllegalArgumentException(""Exhausted all traversal strategies"");
     }
   }",Buggy,"LUCENE-8337: Fix problems with how travel planes too close to edge of world are disallowed, and increase the size of the disallowed window by an order of magnitude.
",Buggy,"The commit message mentions fixing problems related to how travel planes too close to the edge of the world are disallowed, along with increasing the size of the disallowed window. The code changes align with this message by adjusting the conditions under which certain planes are considered too close to the world's edge.

### Detailed Analysis:

1. **Logical Fixes:**
   - The original conditions checked `fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF` and similarly for other planes, which could have led to incorrect results regarding how close the planes are to the edge.
   - The updated conditions use `-fixedYAbovePlane.D - planetModel.getMaximumYValue() > NEAR_EDGE_CUTOFF`, by adjusting the logic to check against negative distances, likely reflecting the correct intended logic for determining ""closeness to the edge.""

2. **Concept of Disallowed Planes:**
   - The update involves calculations indicating when planes become invalid (i.e., `plane = null`) due to being too close to the planetary boundary, which is directly in line with what the commit message suggests to fix the issues with planes being too close to the world's edge.

3. **Increase in Disallowed Window:**
   - The commit message also mentions increasing the disallowed window by an order of magnitude. This change isn't explicitly shown in the diff, but the logical corrections likely contribute to redefining the window size in terms of conditions that determine edge proximity.

Overall, the code modification reflects an adjustment to fix a bug concerning planes' proximity checks, aligning with the commit message. This indicates a correction of faulty logic, potentially leading to incorrect plane categorization due to earlier incorrect distance evaluation.

Hence, the changes correspond to a bug fix, leading to the conclusion of **Buggy**."
lucene-solr,28702.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,132 +1,134 @@
   public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermNonLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
         ste.termExists = subCode == 0;
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       final int code = suffixesReader.readVInt();
       suffix = code >>> 1;
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: "" + ((code&1)==1 ? ""sub-block"" : ""term"") + "" "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       ste.termExists = (code & 1) == 0;
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
+      // Must save ord before we skip over a sub-block in case we push, below:
+      long prevTermOrd = termOrd;
       if (ste.termExists) {
         state.termBlockOrd++;
         termOrd++;
         subCode = 0;
       } else {
         subCode = suffixesReader.readVLong();
         termOrd += suffixesReader.readVLong();
         lastSubFP = fp - subCode;
       }
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
               //termExists = true;
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
           if (!exactOnly && !ste.termExists) {
             // We are on a sub-block, and caller wants
             // us to position to the next term after
             // the target, so we must recurse into the
             // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
+            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, prevTermOrd);
             ste.currentFrame.loadBlock();
             while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
+              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, prevTermOrd);
               ste.currentFrame.loadBlock();
             }
           }
                 
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",Buggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions fixing an ""ord bug,"" adding a test case, and removing dead code. Let's analyze the provided diff to assess whether the changes correspond with that description:

1. **Logical Correction in `prevTermOrd`:**
   - The diff introduces a variable `prevTermOrd` to capture the `termOrd` before it potentially changes as the method iterates over terms or sub-blocks. Previously, `termOrd` could increment incorrectly when sub-blocks were encountered, and it seems that `prevTermOrd` ensures the correct ordinal value is used when sub-frames are pushed onto the stack.

2. **Revised Usage of `prevTermOrd`:**
   - In two places within the code, the function `pushFrame` is called with `prevTermOrd` instead of `ste.currentFrame.termOrd`. This suggests that the bug was related to how term ordinals were managed across deeper recursive searches or sub-frame transitions, leading to incorrect behavior or state.

Given these insights, the changes appear to address a logic error where term ordinals were not tracked or updated correctly affecting the search and seek status within a Lucene term index. These corrections align with the commit message's mention of an ""ord bug,"" and the presence of refactoring in the diff suggests that this was a non-trivial issue that needed correction.

Therefore, given the nature of the changes and their alignment with the commit message, the changes likely indicate a bug fix. Thus, the evaluation is **Buggy**."
lucene-solr,42320.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,123 +1,110 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + brToString(target) + "" term="" + brToString(term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //   BytesRef suffixBytesRef = new BytesRef();
       //   suffixBytesRef.bytes = suffixBytes;
       //   suffixBytesRef.offset = suffixesReader.getPosition();
       //   suffixBytesRef.length = suffix;
       //   System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates three main actions: fixing an ""ord bug,"" adding a test case, and removing dead code. The provided diff does show the removal of a section of code that seems related to handling sub-blocks when the exact term is not found:

```diff
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
```

This section likely represented logic that was no longer necessary or incorrectly executed, which would classify it as dead code. Removing it might also have fixed a bug related to the ""ord"" functionality by preventing undesirable behavior when scanning for terms.

The diff does not directly show the addition of a test case, but the commit message indicates that one was added, likely in a different part of the codebase not shown here.

Overall, the changes in the diff align with the commit message's claim of fixing a bug by removing now-deprecated or malfunctioning logic, supporting the assertion of a problem in the ordinal term seeking process.

Therefore, given the removal of potentially incorrect or obsolete logic described as part of the ""ord bug,"" this diff is consistent with fixing a bug as stated in the commit message. Hence, the commit should be categorized as **Buggy**."
lucene-solr,28701.json,ec788948a64955acc0415281f353d4d7b2f797cc,"@@ -1,124 +1,111 @@
   public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
 
     // if (DEBUG) System.out.println(""    scanToTermLeaf: block fp="" + fp + "" prefix="" + prefix + "" nextEnt="" + nextEnt + "" (of "" + entCount + "") target="" + OrdsSegmentTermsEnum.brToString(target) + "" term="" + OrdsSegmentTermsEnum.brToString(ste.term));
 
     assert nextEnt != -1;
 
     ste.termExists = true;
     subCode = 0;
 
     if (nextEnt == entCount) {
       if (exactOnly) {
         fillTerm();
       }
       return SeekStatus.END;
     }
 
     assert prefixMatches(target);
 
     // Loop over each entry (term or sub-block) in this block:
     //nextTerm: while(nextEnt < entCount) {
     nextTerm: while (true) {
       nextEnt++;
       termOrd++;
 
       suffix = suffixesReader.readVInt();
 
       // if (DEBUG) {
       //    BytesRef suffixBytesRef = new BytesRef();
       //    suffixBytesRef.bytes = suffixBytes;
       //    suffixBytesRef.offset = suffixesReader.getPosition();
       //    suffixBytesRef.length = suffix;
       //    System.out.println(""      cycle: term "" + (nextEnt-1) + "" (of "" + entCount + "") suffix="" + OrdsSegmentTermsEnum.brToString(suffixBytesRef));
       // }
 
       final int termLen = prefix + suffix;
       startBytePos = suffixesReader.getPosition();
       suffixesReader.skipBytes(suffix);
 
       final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
       int targetPos = target.offset + prefix;
 
       // Loop over bytes in the suffix, comparing to
       // the target
       int bytePos = startBytePos;
       while(true) {
         final int cmp;
         final boolean stop;
         if (targetPos < targetLimit) {
           cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
           stop = false;
         } else {
           assert targetPos == targetLimit;
           cmp = termLen - target.length;
           stop = true;
         }
 
         if (cmp < 0) {
           // Current entry is still before the target;
           // keep scanning
 
           if (nextEnt == entCount) {
             if (exactOnly) {
               fillTerm();
             }
             // We are done scanning this block
             break nextTerm;
           } else {
             continue nextTerm;
           }
         } else if (cmp > 0) {
 
           // Done!  Current entry is after target --
           // return NOT_FOUND:
           fillTerm();
 
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen, ste.currentFrame.termOrd);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length, ste.currentFrame.termOrd);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
           //if (DEBUG) System.out.println(""        not found"");
           return SeekStatus.NOT_FOUND;
         } else if (stop) {
           // Exact match!
 
           // This cannot be a sub-block because we
           // would have followed the index to this
           // sub-block from the start:
 
           assert ste.termExists;
           fillTerm();
           //if (DEBUG) System.out.println(""        found!"");
           return SeekStatus.FOUND;
         }
       }
     }
 
     // It is possible (and OK) that terms index pointed us
     // at this block, but, we scanned the entire block and
     // did not find the term to position to.  This happens
     // when the target is after the last term in the block
     // (but, before the next term in the index).  EG
     // target could be foozzz, and terms index pointed us
     // to the foo* block, but the last term in this block
     // was fooz (and, eg, first term in the next block will
     // bee fop).
     //if (DEBUG) System.out.println(""      block end"");
     if (exactOnly) {
       fillTerm();
     }
 
     // TODO: not consistent that in the
     // not-exact case we don't next() into the next
     // frame here
     return SeekStatus.END;
   }",NotBuggy,"LUCENE-5819: fix ord bug; add test case; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1612217 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message claims a fix for an ""ord bug"" and includes test case additions and removed dead code. The code diff supports this as it removes a block of dead/commented-out code that appears to deal with sub-frames and term positioning logic. Removing dead code, while not a direct bug fix, complements bug-related changes by removing potentially confusing or outdated logic—improving clarity. Additional confirmation through test cases is mentioned but not shown.

Considering the provided diff alone primarily shows dead code removal with potential logic changes to avoid unnecessary recursion—a common bug symptom—the bug claim holds. Thus, the commit is most likely part of comprehensive bug resolution. 

The assessment with available data concludes **Buggy**."
lucene-solr,20313.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,11 +1,6 @@
-  private IndexSchema initSchema(SolrConfig config, IndexSchema schema) {
+  private void initSchema(SolrConfig config, IndexSchema schema) {
     if (schema == null) {
       schema = IndexSchemaFactory.buildIndexSchema(IndexSchema.DEFAULT_SCHEMA_FILE, config);
     }
-    final SimilarityFactory similarityFactory = schema.getSimilarityFactory();
-    if (similarityFactory instanceof SolrCoreAware) {
-      // Similarity needs SolrCore before inform() is called on all registered SolrCoreAware listeners below
-      ((SolrCoreAware) similarityFactory).inform(this);
-    }
-    return schema;
+    setLatestSchema(schema);
   }",Buggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug in the initialization of `SimilarityFactory` that affected `SolrCoreAware` factories. In the given diff, the method `initSchema` has been changed to return `void` and simplifies its logic by removing the `SimilarityFactory` initialization code that checks for `SolrCoreAware` and calls `inform`. Instead, it now only calls `setLatestSchema(schema)`.

This change seems to focus on correcting an error in the initialization process that improperly handled the `SolrCoreAware` interface's requirements. Removing this code suggests the setup and possibly the timing related to `inform` needed alteration to allow other parts of the initialization process to function correctly when using a managed schema, which aligns with the commit message's focus on a specific initialization bug.

Thus, the change looks like it addresses the initialization bug described in the commit message related to `SimilarityFactory` and its interaction with `SolrCoreAware`. Accordingly, this is a bug fix.

Therefore, the assessment of this change is **Buggy**."
lucene-solr,12746.json,766b017b1a356f57a5eb6e73bd70e67f34534013,"@@ -1,4 +1,6 @@
   public Similarity getSimilarity() {
-    assert core != null : ""inform must be called first"";
+    if (null == core) {
+      throw new IllegalStateException(""SchemaSimilarityFactory can not be used until SolrCoreAware.inform has been called"");
+    }
     return similarity;
   }",NotBuggy,"SOLR-8280: Fixed bug in SimilarityFactory initialization that prevented SolrCoreAware factories from functioning properly with managed schema features

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1715215 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in the initialization of `SimilarityFactory` that affects `SolrCoreAware` factories. The code change replaces an assertion with an exception, which is a common practice to ensure that the application can handle an error condition gracefully. Assertions are typically stripped out in production environments, so converting it to an `IllegalStateException` makes the code more robust, especially in cases where Solr might be used in a live environment.

This change ensures that the `inform` method is called before `getSimilarity` is used, aligning with the commit message's reference to fixing a bug in the initialization process related to `SolrCoreAware` factories and managed schema features.

The changes in the diff closely align with the commit message description as they address the handling of a critical state condition that could likely lead to errors if not properly handled.

Therefore, the answer is **Buggy**."
lucene-solr,43373.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,18 +1,16 @@
   private static final boolean isIntersectingPolygon(final Node start, final double x0, final double y0,
                                                      final double x1, final double y1) {
     Node node = start;
     Node nextNode;
     do {
       nextNode = node.next;
-      if(node.getX() != x0 && node.getY() != y0 && nextNode.getX() != x0
-          && nextNode.getY() != y0 && node.getX() != x1 && node.getY() != y1
-          && nextNode.getX() != x1 && nextNode.getY() != y1) {
+      if(isVertexEquals(node, x0, y0) == false && isVertexEquals(node, x1, y1) == false) {
         if (linesIntersect(node.getX(), node.getY(), nextNode.getX(), nextNode.getY(), x0, y0, x1, y1)) {
           return true;
         }
       }
       node = nextNode;
     } while (node != start);
 
     return false;
   }",Buggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message states that a bug related to skipped polygon edges during intersection checks is fixed. The code updates from multiple vertex comparisons to using a helper method, `isVertexEquals`, likely enhancing clarity and ensuring correct logic. This change ensures precision in intersection evaluations by focusing on vertex equivalency checks rather than broad condition checks. Therefore, the presented changes indeed address a likely bug in logical handling, aligning with the message. The answer is **Buggy**."
lucene-solr,43381.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
   private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+    return isVertexEquals(a, b.getX(), b.getY());
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message mentions a bug fix related to skipped polygon edges during intersection checks. The diff modifies the isVertexEquals method, changing its logic to use a new method signature. The method now delegates vertex equality testing to another form presumably defined in the class: `isVertexEquals(a, b.getX(), b.getY())`. This change suggests the possibility of adjusting how vertex equality is calculated, likely tied to ensuring polygon edges are accurately accounted for in intersection checks. Given that the commit explicitly states a bug fix for polygon edge intersection handling and the change seems to alter vertex comparison logic, it aligns with the commit message. Thus, the changes suggest a correction in the logic for checking polygon edges.

The conclusion is **Buggy**."
lucene-solr,43382.json,9952af099ae65f051056fc8ff55c8e8f4cfb3b93,"@@ -1,3 +1,3 @@
-  private static final boolean isVertexEquals(final Node a, final Node b) {
-    return a.getX() == b.getX() && a.getY() == b.getY();
+  private static final boolean isVertexEquals(final Node a, final double x, final  double y) {
+    return a.getX() == x && a.getY() == y;
   }",NotBuggy,"LUCENE-8559: Fix bug where polygon edges were skipped when checking for intersections
",Buggy,"The commit message indicates fixing a bug related to skipping polygon edges during intersection checks. The code change modifies the `isVertexEquals` function signature and logic to compare a vertex against coordinates directly, rather than another `Node`. This seems aligned with the need to check edges more robustly, likely affecting intersection logic accuracy. Although the diff itself is minor, it alters a utility that could be central to fixing the described bug with polygon edge checks.

Thus, the code change aligns with the commit message's stated bug fix for intersection checks. The answer is **Buggy**."
lucene-solr,38533.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,15 +1,16 @@
-  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int heatLen,
+  private static void intersectInterval(double heatMin, double heatMax, double heatCellLen, int numCells,
                                         double cellMin, double cellMax,
                                         int[] out) {
+    assert heatMin < heatMax && cellMin < cellMax;
     //precondition: we know there's an intersection
     if (heatMin >= cellMin) {
       out[0] = 0;
     } else {
       out[0] = (int) Math.round((cellMin - heatMin) / heatCellLen);
     }
     if (heatMax <= cellMax) {
-      out[1] = heatLen - 1;
+      out[1] = numCells - 1;
     } else {
       out[1] = (int) Math.round((cellMax - heatMin) / heatCellLen) - 1;
     }
   }",NotBuggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"The commit message refers to a bug fix in the spatial HeatmapFacetCounter related to handling the dateline and large non-point shapes. The changes in the diff reflect a correction in the logic by renaming a parameter from `heatLen` to `numCells`, which seems more intuitive and consistent with the purpose of determining the number of heatmap cells. By changing the parameter from `heatLen` to `numCells`, which is used in setting `out[1]`, the correction avoids a logical inconsistency or potential miscalculation when the heatmap boundaries are determined. This modification appears to be directly related to fixing the described bug.

Additionally, there is the addition of an assertion `assert heatMin < heatMax && cellMin < cellMax;`, which ensures that the input parameters meet the expected conditions before proceeding with the computation. This change aids in preventing incorrect assumptions about the input and potential errors during execution, serving as a safeguard against logical errors.

Given that the modifications include such logical corrections and safety checks, and the commit message explicitly mentions a bug fix in this part of the code, the changes are indeed relevant to resolving the mentioned issue.

Thus, the answer is **Buggy**."
lucene-solr,38532.json,b33d7176aa3624df2de1708b17919f20d034872f,"@@ -1,152 +1,153 @@
   public static Heatmap calcFacets(PrefixTreeStrategy strategy, IndexReaderContext context, Bits topAcceptDocs,
                                    Shape inputShape, final int facetLevel, int maxCells) throws IOException {
     if (maxCells > (MAX_ROWS_OR_COLUMNS * MAX_ROWS_OR_COLUMNS)) {
       throw new IllegalArgumentException(""maxCells ("" + maxCells + "") should be <= "" + MAX_ROWS_OR_COLUMNS);
     }
     if (inputShape == null) {
       inputShape = strategy.getSpatialContext().getWorldBounds();
     }
     final Rectangle inputRect = inputShape.getBoundingBox();
     //First get the rect of the cell at the bottom-left at depth facetLevel
     final SpatialPrefixTree grid = strategy.getGrid();
     final SpatialContext ctx = grid.getSpatialContext();
     final Point cornerPt = ctx.makePoint(inputRect.getMinX(), inputRect.getMinY());
     final CellIterator cellIterator = grid.getTreeCellIterator(cornerPt, facetLevel);
     Cell cornerCell = null;
     while (cellIterator.hasNext()) {
       cornerCell = cellIterator.next();
     }
     assert cornerCell != null && cornerCell.getLevel() == facetLevel : ""Cell not at target level: "" + cornerCell;
     final Rectangle cornerRect = (Rectangle) cornerCell.getShape();
     assert cornerRect.hasArea();
     //Now calculate the number of columns and rows necessary to cover the inputRect
     double heatMinX = cornerRect.getMinX();//note: we might change this below...
     final double cellWidth = cornerRect.getWidth();
     final Rectangle worldRect = ctx.getWorldBounds();
     final int columns = calcRowsOrCols(cellWidth, heatMinX, inputRect.getWidth(), inputRect.getMinX(), worldRect.getWidth());
     final double heatMinY = cornerRect.getMinY();
     final double cellHeight = cornerRect.getHeight();
     final int rows = calcRowsOrCols(cellHeight, heatMinY, inputRect.getHeight(), inputRect.getMinY(), worldRect.getHeight());
     assert rows > 0 && columns > 0;
     if (columns > MAX_ROWS_OR_COLUMNS || rows > MAX_ROWS_OR_COLUMNS || columns * rows > maxCells) {
       throw new IllegalArgumentException(
           ""Too many cells ("" + columns + "" x "" + rows + "") for level "" + facetLevel + "" shape "" + inputRect);
     }
 
     //Create resulting heatmap bounding rectangle & Heatmap object.
     final double halfCellWidth = cellWidth / 2.0;
     // if X world-wraps, use world bounds' range
     if (columns * cellWidth + halfCellWidth > worldRect.getWidth()) {
       heatMinX = worldRect.getMinX();
     }
     double heatMaxX = heatMinX + columns * cellWidth;
     if (Math.abs(heatMaxX - worldRect.getMaxX()) < halfCellWidth) {//numeric conditioning issue
       heatMaxX = worldRect.getMaxX();
     } else if (heatMaxX > worldRect.getMaxX()) {//wraps dateline (won't happen if !geo)
       heatMaxX = heatMaxX - worldRect.getMaxX() +  worldRect.getMinX();
     }
     final double halfCellHeight = cellHeight / 2.0;
     double heatMaxY = heatMinY + rows * cellHeight;
     if (Math.abs(heatMaxY - worldRect.getMaxY()) < halfCellHeight) {//numeric conditioning issue
       heatMaxY = worldRect.getMaxY();
     }
 
     final Heatmap heatmap = new Heatmap(columns, rows, ctx.makeRectangle(heatMinX, heatMaxX, heatMinY, heatMaxY));
 
     //All ancestor cell counts (of facetLevel) will be captured during facet visiting and applied later. If the data is
     // just points then there won't be any ancestors.
     //Facet count of ancestors covering all of the heatmap:
     int[] allCellsAncestorCount = new int[1]; // single-element array so it can be accumulated in the inner class
     //All other ancestors:
     Map<Rectangle,Integer> ancestors = new HashMap<>();
 
     //Now lets count some facets!
     PrefixTreeFacetCounter.compute(strategy, context, topAcceptDocs, inputShape, facetLevel,
         new PrefixTreeFacetCounter.FacetVisitor() {
       @Override
       public void visit(Cell cell, int count) {
         final double heatMinX = heatmap.region.getMinX();
         final Rectangle rect = (Rectangle) cell.getShape();
         if (cell.getLevel() == facetLevel) {//heatmap level; count it directly
           //convert to col & row
           int column;
           if (rect.getMinX() >= heatMinX) {
             column = (int) Math.round((rect.getMinX() - heatMinX) / cellWidth);
           } else { // due to dateline wrap
             column = (int) Math.round((rect.getMinX() + 360 - heatMinX) / cellWidth);
           }
           int row = (int) Math.round((rect.getMinY() - heatMinY) / cellHeight);
           //note: unfortunately, it's possible for us to visit adjacent cells to the heatmap (if the SpatialPrefixTree
           // allows adjacent cells to overlap on the seam), so we need to skip them
           if (column < 0 || column >= heatmap.columns || row < 0 || row >= heatmap.rows) {
             return;
           }
           // increment
           heatmap.counts[column * heatmap.rows + row] += count;
 
         } else if (rect.relate(heatmap.region) == SpatialRelation.CONTAINS) {//containing ancestor
           allCellsAncestorCount[0] += count;
 
         } else { // ancestor
           // note: not particularly efficient (possible put twice, and Integer wrapper); oh well
           Integer existingCount = ancestors.put(rect, count);
           if (existingCount != null) {
             ancestors.put(rect, count + existingCount);
           }
         }
       }
     });
 
     //Update the heatmap counts with ancestor counts
 
     // Apply allCellsAncestorCount
     if (allCellsAncestorCount[0] > 0) {
       for (int i = 0; i < heatmap.counts.length; i++) {
         heatmap.counts[i] += allCellsAncestorCount[0];
       }
     }
 
     // Apply ancestors
     //  note: This approach isn't optimized for a ton of ancestor cells. We'll potentially increment the same cells
     //    multiple times in separate passes if any ancestors overlap. IF this poses a problem, we could optimize it
     //    with additional complication by keeping track of intervals in a sorted tree structure (possible TreeMap/Set)
     //    and iterate them cleverly such that we just make one pass at this stage.
 
     int[] pair = new int[2];//output of intersectInterval
     for (Map.Entry<Rectangle, Integer> entry : ancestors.entrySet()) {
-      Rectangle rect = entry.getKey();
+      Rectangle rect = entry.getKey(); // from a cell (thus doesn't cross DL)
       final int count = entry.getValue();
+
       //note: we approach this in a way that eliminates int overflow/underflow (think huge cell, tiny heatmap)
       intersectInterval(heatMinY, heatMaxY, cellHeight, rows, rect.getMinY(), rect.getMaxY(), pair);
       final int startRow = pair[0];
       final int endRow = pair[1];
 
       if (!heatmap.region.getCrossesDateLine()) {
         intersectInterval(heatMinX, heatMaxX, cellWidth, columns, rect.getMinX(), rect.getMaxX(), pair);
         final int startCol = pair[0];
         final int endCol = pair[1];
         incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
 
       } else {
+        // note: the cell rect might intersect 2 disjoint parts of the heatmap, so we do the left & right separately
+        final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth);
+        final int rightColumns = heatmap.columns - leftColumns;
         //left half of dateline:
-        if (rect.getMaxX() >= heatMinX) {
-          final int leftColumns = (int) Math.round((180 - heatMinX) / cellWidth) + 1;
+        if (rect.getMaxX() > heatMinX) {
           intersectInterval(heatMinX, 180, cellWidth, leftColumns, rect.getMinX(), rect.getMaxX(), pair);
           final int startCol = pair[0];
           final int endCol = pair[1];
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
         //right half of dateline
-        if (rect.getMinY() <= heatMaxX) {
-          final int rightColumns = (int) Math.round(heatMaxX / cellWidth) + 1;
-          intersectInterval(0, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
-          final int startCol = pair[0];
-          final int endCol = pair[1];
+        if (rect.getMinX() < heatMaxX) {
+          intersectInterval(-180, heatMaxX, cellWidth, rightColumns, rect.getMinX(), rect.getMaxX(), pair);
+          final int startCol = pair[0] + leftColumns;
+          final int endCol = pair[1] + leftColumns;
           incrementRange(heatmap, startCol, endCol, startRow, endRow, count);
         }
       }
-
     }
 
     return heatmap;
   }",Buggy,"LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
",Buggy,"The commit message states that a bug in `HeatmapFacetCounter` related to handling the dateline and large non-point shapes is being fixed. Here's a step-by-step analysis of the diff to determine if the changes align with this claim:

1. **Understanding the Context**: The function `calcFacets` computes spatial facets for a heatmap using a spatial prefix tree strategy. It involves calculating the number of columns and rows required to cover a given `inputShape`, taking into consideration the world bounds and how shapes might wrap around datelines.

2. **Changes in Handling Dateline**:
   - **Left Half of Dateline**:
     - The condition `if (rect.getMaxX() > heatMinX)` replaces `if (rect.getMaxX() >= heatMinX)`. This change ensures that conditions are correctly interpreted for the part of the process that calculates intersections and updates heatmap counts for cells on the left side of the dateline.
     - It eliminates the `+ 1` addition when calculating `leftColumns`, suggesting an adjustment in how overlap is determined across the dateline.
   - **Right Half of Dateline**:
     - The condition `if (rect.getMinX() < heatMaxX)` replaces `if (rect.getMinY() <= heatMaxX)`, likely addressing a logical bug where the wrong coordinate (`getMinY` instead of `getMinX`) was used to determine overlap—this directly relates to correctly handling non-point shapes across the dateline.
     - Similarly, the calculation of `rightColumns` depends on corrected logic without the redundant `+ 1` operation, improving how overlap for the right side of the dateline is calculated.
     - The `intersectInterval` call includes an updated range parameter from `[0, heatMaxX]` to `[-180, heatMaxX]`. This change considers global bounds more accurately, accounting for dateline wrapping.

3. **Logical and Performance Fixes**:
   - The adjustments primarily focus on calculating and handling spatial region overlaps with respect to the world wrapping around the dateline. These corrections involve better integer value management and interval calculations, especially for large shapes that might cross the dateline.
   - The management of `leftColumns` and `rightColumns` neatly segments the heatmap into areas left and right of the dateline. It corrects inaccuracies that might have previously resulted in incorrect facet counts at these critical edges.
   - Clarifications and additions in conditional logic help handle edge cases where the dateline might cause discontinuities in the interpretation of coordinates.

4. **Conclusion**: The changes made in this diff address errors that likely occurred in the spatial computations when dealing with the dateline and large shapes. The focus on conditions and calculations directly related to edge cases implies a routine fix for how coordinates and overlaps are accounted concerning the dateline and their corresponding regions on the heatmap.

Therefore, since the given changes align well with the bug-fix claim for the given spatial functionality, the changes can indeed be concluded to address a **Buggy** situation."
lucene-solr,36488.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,42 +1,34 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     DoubleRange[] ranges = (DoubleRange[]) this.ranges;
 
     LongRange[] longRanges = new LongRange[ranges.length];
     for(int i=0;i<ranges.length;i++) {
       DoubleRange range = ranges[i];
       longRanges[i] =  new LongRange(range.label,
                                      NumericUtils.doubleToSortableLong(range.minIncl), true,
                                      NumericUtils.doubleToSortableLong(range.maxIncl), true);
     }
 
     LongRangeCounter counter = new LongRangeCounter(longRanges);
 
-    // Compute min & max over all ranges:
-    double minIncl = Double.POSITIVE_INFINITY;
-    double maxIncl = Double.NEGATIVE_INFINITY;
-    for(DoubleRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
         } else {
           missingCount++;
         }
         doc++;
       }
     }
 
     missingCount += counter.fillCounts(counts);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a test bug fix and removal of dead code. In the provided diff, calculations for `minIncl` and `maxIncl` over `DoubleRange` objects are removed, constituting dead code removal. Since this part of the code was not used elsewhere in the method (or affecting the method's outcome), its removal aligns with the message.

However, the commit message also mentions fixing a ""test bug,"" but the diff provided only shows the removal of dead code, with no indication of changes directly related to a test bug, such as assertions, conditions, or test-case-specific logic changes.

Given the changes shown, the diff aligns with removing dead code but does not clearly demonstrate a fix for a test bug in this segment. Thus, while the removal of the unused code is clear, without specific context or additional diffs showing changes to test cases or logic to support the ""test bug fix"" claim, we cannot definitively identify a bug fix related to tests.

Considering the provided information, the conclusion is **NotBuggy** regarding a direct bug fix; it mostly aligns with dead code removal."
lucene-solr,36489.json,75b0b5312c6f7722b88088fbc590259e9cd31567,"@@ -1,39 +1,31 @@
   private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
 
     LongRange[] ranges = (LongRange[]) this.ranges;
 
-    // Compute min & max over all ranges:
-    long minIncl = Long.MAX_VALUE;
-    long maxIncl = Long.MIN_VALUE;
-    for(LongRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
     LongRangeCounter counter = new LongRangeCounter(ranges);
 
     int missingCount = 0;
     for (MatchingDocs hits : matchingDocs) {
       FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
       final int length = hits.bits.length();
       int doc = 0;
       totCount += hits.totalHits;
       while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
         // Skip missing docs:
         if (fv.exists(doc)) {
           counter.add(fv.longVal(doc));
         } else {
           missingCount++;
         }
 
         doc++;
       }
     }
     
     int x = counter.fillCounts(counts);
 
     missingCount += x;
 
     //System.out.println(""totCount "" + totCount + "" missingCount "" + counter.missingCount);
     totCount -= missingCount;
   }",NotBuggy,"fix test bug; remove dead code

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1555715 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message notes the removal of dead code and fixing a test bug. The change removes calculations for `minIncl` and `maxIncl` over all ranges, which are not used later, indicating this was dead code. This aligns with ""remove dead code"" in the commit message. There is no direct indication of a test bug fix within the provided diff, as this relates more to code cleanup than resolving a specific bug.

Because this change is about refactoring by removing unused code rather than correcting a specific fault or logical error in the execution of the method, it does not seem to address a bug in the usual sense (even concerning the ""test bug,"" which doesn't appear directly related to this snippet). Therefore, the classification based on this particular diff is **NotBuggy**."
lucene-solr,46441.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,45 +1,51 @@
   private DocMap[] buildDocMaps(List<CodecReader> readers, Sort indexSort) throws IOException {
 
     int numReaders = readers.size();
 
     if (indexSort == null) {
       // no index sort ... we only must map around deletions, and rebase to the merged segment's docID space
 
       int totalDocs = 0;
       DocMap[] docMaps = new DocMap[numReaders];
 
       // Remap docIDs around deletions:
       for (int i = 0; i < numReaders; i++) {
         LeafReader reader = readers.get(i);
         Bits liveDocs = reader.getLiveDocs();
 
         final PackedLongValues delDocMap;
         if (liveDocs != null) {
           delDocMap = removeDeletes(reader.maxDoc(), liveDocs);
         } else {
           delDocMap = null;
         }
 
         final int docBase = totalDocs;
         docMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             if (liveDocs == null) {
               return docBase + docID;
             } else if (liveDocs.get(docID)) {
               return docBase + (int) delDocMap.get(docID);
             } else {
               return -1;
             }
           }
         };
         totalDocs += reader.numDocs();
       }
 
       return docMaps;
 
     } else {
       // do a merge sort of the incoming leaves:
-      return MultiSorter.sort(indexSort, readers);
+      long t0 = System.nanoTime();
+      DocMap[] result = MultiSorter.sort(indexSort, readers);
+      long t1 = System.nanoTime();
+      if (infoStream.isEnabled(""SM"")) {
+        infoStream.message(""SM"", String.format(Locale.ROOT, ""%.2f msec to build merge sorted DocMaps"", (t1-t0)/1000000.0));
+      }
+      return result;
     }
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",NotBuggy,"The commit message indicates fixes related to sorting and a test bug. The code changes focus on adding logging around the sorting process in the buildDocMaps method, using the infoStream to record the time taken for sorting operations. This provides more transparency into the performance characteristics of the merge sorting process but doesn't inherently address a bug in the functional logic itself.

The specific changes made include:
1. Adding timing logic around the `MultiSorter.sort` method call using `System.nanoTime()` to capture the duration of the sorting process.
2. Adding a conditional log statement controlled by `infoStream.isEnabled(""SM"")` to output the time taken to perform the sort if enabled.

No part of the code directly modifies functionality that would fix a logical error or incorrect behavior within the application. There is no modification to exception handling, logical corrections, or error fixing visible in these changes.

Therefore, these changes align with the commit message by adding more informational logging around a specific functional area but do not directly address bug-fixing. The given changes are related to better logging and performance transparency rather than a bug fix.

Based on the code edit and the provided commit message description, the conclusion is **NotBuggy**."
lucene-solr,46444.json,e283271aaf6da3033156f36b421d3241b5499d4e,"@@ -1,61 +1,65 @@
   private List<CodecReader> maybeSortReaders(List<CodecReader> originalReaders, SegmentInfo segmentInfo) throws IOException {
 
     // Default to identity:
     for(int i=0;i<originalReaders.size();i++) {
       leafDocMaps[i] = new DocMap() {
           @Override
           public int get(int docID) {
             return docID;
           }
         };
     }
 
     Sort indexSort = segmentInfo.getIndexSort();
     if (indexSort == null) {
       return originalReaders;
     }
 
     // If an incoming reader is not sorted, because it was flushed by IW, we sort it here:
     final Sorter sorter = new Sorter(indexSort);
     List<CodecReader> readers = new ArrayList<>(originalReaders.size());
 
     for (CodecReader leaf : originalReaders) {
       Sort segmentSort = leaf.getIndexSort();
 
       if (segmentSort == null) {
         // TODO: fix IW to also sort when flushing?  It's somewhat tricky because of stored fields and term vectors, which write ""live""
         // to their index files on each indexed document:
 
         // This segment was written by flush, so documents are not yet sorted, so we sort them now:
+        long t0 = System.nanoTime();
         Sorter.DocMap sortDocMap = sorter.sort(leaf);
+        long t1 = System.nanoTime();
+        double msec = (t1-t0)/1000000.0;
+        
         if (sortDocMap != null) {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted; wrapping for sort "" + indexSort + "" now"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted; wrapping for sort %s now (%.2f msec to sort)"", leaf, indexSort, msec));
           }
           leaf = SlowCodecReaderWrapper.wrap(SortingLeafReader.wrap(new MergeReaderWrapper(leaf), sortDocMap));
           leafDocMaps[readers.size()] = new DocMap() {
               @Override
               public int get(int docID) {
                 return sortDocMap.oldToNew(docID);
               }
             };
         } else {
           if (infoStream.isEnabled(""SM"")) {
-            infoStream.message(""SM"", ""segment "" + leaf + "" is not sorted, but is already accidentally in sort "" + indexSort + "" order"");
+            infoStream.message(""SM"", String.format(Locale.ROOT, ""segment %s is not sorted, but is already accidentally in sort %s order (%.2f msec to sort)"", leaf, indexSort, msec));
           }
         }
 
       } else {
         if (segmentSort.equals(indexSort) == false) {
           throw new IllegalArgumentException(""index sort mismatch: merged segment has sort="" + indexSort + "" but to-be-merged segment has sort="" + segmentSort);
         }
         if (infoStream.isEnabled(""SM"")) {
           infoStream.message(""SM"", ""segment "" + leaf + "" already sorted"");
         }
       }
 
       readers.add(leaf);
     }
 
     return readers;
   }",NotBuggy,"LUCENE-6766: more IW.infoStream logging around sorting; fix test bug
",NotBuggy,"In this diff, additional logging around the sorting process in the method `maybeSortReaders` is added. The logging enhancements include:

1. **Time Measurement**: The time taken to sort the documents is now measured and logged. This involves logging the time before sorting (`t0`), after sorting (`t1`), and calculating the duration in milliseconds (`msec`).

2. **Enhanced Log Messages**: The log messages are updated to include the time taken for sorting. This provides more insightful information for debugging and performance measurement.

Regarding the commit message, it mentions ""fix test bug"" and improved logging around sorting. Let's examine the relevance:

- The enhanced logging directly relates to the commit message's ""more `IW.infoStream` logging around sorting"".
- The ""fix test bug"" part of the commit message, however, is not directly visible in the presented code diff. It could refer to adjustments in tests or bug fixes elsewhere in the codebase that are not shown here. Without access to other sections of the code modified or corresponding test files, it's challenging to confirm the presence of a test bug fix strictly from this diff.

While the logging changes help with debugging and monitoring, they do not directly indicate an intrinsic bug fix in logic or behavior according to Java code modifications presented here. Without explicit bug changes related to functional or logical errors being addressed in this diff, it is considered **NotBuggy** with respect to the code modifications shown."
lucene-solr,44598.json,a2a9f2a6f9cba3b27b248102d88431b8b234530e,"@@ -1,32 +1,34 @@
     public SeekStatus seekCeil(BytesRef text) {
 
       // TODO: we could instead keep the BytesRefHash
       // intact so this is a hash lookup
 
       // binary search:
       int lo = 0;
       int hi = numTerms - 1;
       while (hi >= lo) {
         int mid = (lo + hi) >>> 1;
         int textStart = postingsArray.textStarts[sortedTermIDs[mid]];
         terms.bytePool.setBytesRef(scratch, textStart);
         int cmp = scratch.compareTo(text);
         if (cmp < 0) {
           lo = mid + 1;
         } else if (cmp > 0) {
           hi = mid - 1;
         } else {
           // found:
           ord = mid;
           return SeekStatus.FOUND;
         }
       }
 
       // not found:
       ord = lo + 1;
-      if (ord == numTerms) {
+      if (ord >= numTerms) {
         return SeekStatus.END;
       } else {
+        int textStart = postingsArray.textStarts[sortedTermIDs[ord]];
+        terms.bytePool.setBytesRef(scratch, textStart);
         return SeekStatus.NOT_FOUND;
       }
     }",Buggy,"test seekCeil in pull postings API; fix trunk-only bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1611305 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the change is related to testing `seekCeil` in a postings API and fixing a ""trunk-only bug"".

The code diff reveals two key refinements in the `seekCeil` method:

1. **Change of Condition**: 
   - Original: `if (ord == numTerms)`
   - Updated: `if (ord >= numTerms)`
   
   This indicates a fix for a potential off-by-one error that could occur when the calculated `ord` value is out of bounds. If `ord` equaled `numTerms`, it would be out of range since valid indices are `0` to `numTerms - 1`. The updated condition covers this by using `>=` to ensure that any value of `ord` that is not within the valid range results in the method returning `SeekStatus.END`.

2. **Additional Logic When Not Found**:
   - New lines were added to handle the scenario where `ord < numTerms`. Now, when a term is not found, the code resets the `scratch` buffer with the term data corresponding to the `ord` position, which ensures that the returned status `SeekStatus.NOT_FOUND` associates correctly with the next greater or equal term.

These changes improve the robustness of the method, addressing potential boundary cases related to index operations and the logic of scanning through the terms. This fits within a typical bug-fix pattern, particularly how edge cases with binary search algorithms might lead to incorrect index handling or skipped logic paths that don't account for all potential outcomes (such as missing the next closest term in the ordering).

The change aligns with the commit message describing a bug fix related to the `seekCeil` logic. Thus, the analysis of both the code refactor and commit message leads to the conclusion that this is indeed a bug fix.

**Buggy**"
jna,165.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,14 +1,15 @@
         private static long getVisualID(GraphicsConfiguration config) {
             // Use reflection to call
             // X11GraphicsConfig.getVisual
             try {
                 Object o = config.getClass()
                     .getMethod(""getVisual"", (Class[])null)
                     .invoke(config, (Object[])null);
                 return ((Number)o).longValue();
             }
             catch (Exception e) {
+                // FIXME properly handle this error
                 e.printStackTrace();
                 return -1;
             }
         }",NotBuggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",NotBuggy,"The commit message states that the changes fix memory faults specifically on 32-bit Windows systems. However, the diff shows a modification only in the error handling process of the method `getVisualID`, where a comment `// FIXME properly handle this error` has been added. The existing code captures exceptions and logs the stack trace but returns -1 to signify failure.

The critical points:
1. The current implementation with `e.printStackTrace()` and returning -1 looks more like a placeholder error handling strategy rather than a precise fix for memory faults.
2. Adding the `FIXME` comment suggests an acknowledgment of inadequate error handling, but it doesn’t provide a practical fix.
3. The diff does not exhibit changes directly related to memory management or 32-bit Windows compatibility issues.

Based on this analysis, the commit does not resolve the memory fault issue directly as described in the message, nor does it contain changes that indicate an actual bug fix in this context. The primary outcome of the change is the inclusion of a comment suggesting that better error handling should be implemented in the future. Consequently, the diff does not align with the commit message in addressing the purported memory faults on 32-bit Windows.

Therefore, the conclusion is **NotBuggy**."
jna,146.json,535518713bc51e6eacefeaeaccc7033ade905a9a,"@@ -1,17 +1,18 @@
-				public boolean callback(final HWND hwnd, final Pointer arg1) {
-					try {
-						final boolean visible = !onlyVisibleWindows
-								|| User32.INSTANCE.IsWindowVisible(hwnd);
-						if (visible) {
-							final String title = getWindowTitle(hwnd);
-							final String filePath = getProcessFilePath(hwnd);
-							final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
-							result.add(new DesktopWindow(hwnd, title, filePath,
-									locAndSize));
-						}
-					} catch (final Exception e) {
-						e.printStackTrace();
-					}
-
-					return true;
-				}

+                public boolean callback(final HWND hwnd, final Pointer arg1) {
+                    try {
+                        final boolean visible = !onlyVisibleWindows
+                            || User32.INSTANCE.IsWindowVisible(hwnd);
+                        if (visible) {
+                            final String title = getWindowTitle(hwnd);
+                            final String filePath = getProcessFilePath(hwnd);
+                            final Rectangle locAndSize = getWindowLocationAndSize(hwnd);
+                            result.add(new DesktopWindow(hwnd, title, filePath,
+                                                         locAndSize));
+                        }
+                    } catch (final Exception e) {
+                        // FIXME properly handle whatever error is raised
+                        e.printStackTrace();
+                    }
+                    
+                    return true;
+                }",Buggy,"fix mappings to properly work on 32-bit windows (fixes memory faults)
",Buggy,"The commit message indicates a fix for memory faults on 32-bit Windows. The code diff itself primarily shows a reformatting of the existing code with changes in the indentation. The critical functional change is the addition of a comment `// FIXME properly handle whatever error is raised` in the catch block, replacing the previous practice of printing the stack trace with proper error handling. 

However, the visible functional difference directly addressing the commit message is not shown. While the addition of this comment suggests an acknowledgment of potential issues in error handling that could lead to improper mappings or faults, it does not explicitly showcase a resolution of any memory fault-related issues unique to 32-bit Windows.

Nonetheless, since this change relates to improving error handling, potentially related to memory fault issues, and aligns with the commit message's intent to fix operational problems, it can be cautiously interpreted as part of the bug-fix effort mentioned. These kinds of improvements can often prevent faults by ensuring exceptions are correctly dealt with, even if it isn't a complete bug fix on its own.

Considering this analysis, if the fix for memory faults was truly implemented within this code snippet or elsewhere in the commit, it’s a supportive change related to that effort. However, with the information provided, there's no clear visible correction of the memory issue itself.

Given this analysis, the conclusion is **Buggy**, as the commit's intent aligns with bug resolution despite potentially lacking direct fixes visible in this particular diff snippet."
jna,3926.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,86 +1,84 @@
     public Object invoke(Class returnType, Object[] inArgs, Map options) {
         // Clone the argument array to obtain a scratch space for modified
         // types/values
         Object[] args = { };
         if (inArgs != null) {
             if (inArgs.length > MAX_NARGS) {
                 throw new UnsupportedOperationException(""Maximum argument count is "" + MAX_NARGS);
             }
             args = new Object[inArgs.length];
             System.arraycopy(inArgs, 0, args, 0, args.length);
         }
 
         TypeMapper mapper = 
             (TypeMapper)options.get(Library.OPTION_TYPE_MAPPER);
         Method invokingMethod = (Method)options.get(OPTION_INVOKING_METHOD);
         for (int i=0; i < args.length; i++) {
             args[i] = convertArgument(args, i, invokingMethod, mapper);
         }
         
         Class nativeType = returnType;
         FromNativeConverter resultConverter = null;
         if (NativeMapped.class.isAssignableFrom(returnType)) {
             NativeMappedConverter tc = new NativeMappedConverter(returnType);
             resultConverter = tc;
             nativeType = tc.nativeType();
         }
         else if (mapper != null) {
             resultConverter = mapper.getFromNativeConverter(returnType);
             if (resultConverter != null) {
                 nativeType = resultConverter.nativeType();
             }
         }
 
         Object result = invoke(args, nativeType);
 
         // Convert the result to a custom value/type if appropriate
         if (resultConverter != null) {
             FromNativeContext context;
             
             if (invokingMethod != null) {
                 context = new MethodResultContext(returnType, this, inArgs, invokingMethod);
             } else {
                 context = new FunctionResultContext(returnType, this, inArgs);
             }
             result = resultConverter.fromNative(result, context);
         }
 
         // Sync all memory which might have been modified by the native call
         if (inArgs != null) {
             for (int i=0; i < inArgs.length; i++) {
                 Object arg = inArgs[i];
                 if (arg == null)
                     continue;
                 if (arg instanceof Structure) {
                     if (!(arg instanceof Structure.ByValue)) {
                         ((Structure)arg).read();
                     }
                 }
                 else if (args[i] instanceof StringArray) {
-                    // Read back arrays of String, just in case they
-                    // were modified
                     ((StringArray)args[i]).read();
                 }
                 else if (args[i] instanceof PointerArray) {
                     PointerArray array = (PointerArray)args[i];
                     array.read();
                     if (Structure.ByReference[].class.isAssignableFrom(arg.getClass())) {
                         Class type = arg.getClass().getComponentType();
                         Structure[] ss = (Structure[])arg;
                         for (int si=0;si < ss.length;si++) {
                             Pointer p = array.getPointer(Pointer.SIZE * si);
                             ss[si] = Structure.updateStructureByReference(type, ss[si], p);
                         }
                     }
                 }
                 else if (Structure[].class.isAssignableFrom(arg.getClass())) {
                     Structure[] ss = (Structure[])arg;
                     for (int si=0;si < ss.length;si++) {
                         ss[si].read();
                     }
                 }
             }
         }
                         
         return result;
     }",NotBuggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",NotBuggy,"The commit message indicates a bug fix related to ""stdcall function mapping when using struct by value."" After analyzing the given diff, it's important to look for changes that relate to handling structures by value in function calls or mapping.

Here's the breakdown of the changes:

1. **Array Synchronization:** The diff shows the removal of a comment related to reading back string arrays, indicating that this is already part of the proper process for ensuring any updated native data is handled. This removal doesn't directly relate to the main bug fix target (struct by value), but it suggests refinement in handling memory reads after invoking native methods.
  
2. **Structure Handling:** The logic makes distinctions between `Structure` instances and differentiates between `Structure.ByValue` and other structures, which suggests that the handling of structures by value is consistent with the commit's focus. However, the diff does not show any modification to the handling or reading behavior of `Structure.ByValue`, directly relating to a bug fix there.

Judging by the provided changes alone, there's no direct modification made that adjusts handling specifically for structs passed by value, such as ensuring correct data treatment when structs are used in native interfaces.

Given the constraints of this segment alone and the commit message, it's not directly visible how the changes address a bug fix for structs by value. It looks more like general maintenance and clarification (removing the redundant comment).

Thus, considering only this segment of the diff, the changes don't directly indicate they are fixing a bug related to ""struct by value"" handling according to its commit message. Hence, I would conclude these changes do not directly demonstrate a bug fix evident from this diff alone.

**NotBuggy**"
jna,3554.json,e5958a2a159cd87a8168c801f569f5bcc7511c25,"@@ -1,23 +1,30 @@
-    private int getNativeSize(Class type, Object value) {
-        if (Structure.class.isAssignableFrom(type)) {
-            if (ByReference.class.isAssignableFrom(type)) {
-                return Pointer.SIZE;
-            }
-            else {
-                if (value == null)
-                    value = newInstance(type);
-                Structure s = (Structure)value;
-                return s.size();
-            }
-        }
+    private static int getNativeSize(Class type, Object value) {
         if (type.isArray()) {
             int len = Array.getLength(value);
             if (len > 0) {
                 Object o = Array.get(value, 0);
                 return len * getNativeSize(type.getComponentType(), o);
             }
             // Don't process zero-length arrays
-            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + this);
+            throw new IllegalArgumentException(""Arrays of length zero not allowed in structure: "" + type);
         }
-        return getNativeSize(type);
+        // May provide this in future; problematic on read, since we can't
+        // auto-create a java.nio.Buffer w/o knowing its size
+        if (Buffer.class.isAssignableFrom(type)) {
+            throw new IllegalArgumentException(""the type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
+        if (Structure.class.isAssignableFrom(type)
+            && !Structure.ByReference.class.isAssignableFrom(type)) {
+            if (value == null)
+                value = newInstance(type);
+            return ((Structure)value).size();
+        }
+        try {
+            return Native.getNativeSize(type);
+        }
+        catch(IllegalArgumentException e) {
+            throw new IllegalArgumentException(""The type \"""" + type.getName() 
+                                               + ""\"" is not supported as a structure field"");
+        }
     }",Buggy,"Fix bug in stdcall function mapping when using struct by value
Defer size_t/off_t definition, for now

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@436 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"This commit message indicates a bug fix related to the handling of `stdcall` function mapping when using structures by value.

Upon inspecting the diff:

1. **Refactoring and Adjustments**:
   - The `getNativeSize` method has changed from an instance method to a static one (`private int` to `private static int`). This does not directly correlate to a bug fix but could relate to a broader context, such as thread safety or utility usage.
   
2. **Logic for Handling Structure Types**:
   - Originally, the method first checked if a type was a subclass of `Structure`. Now, it checks `type.isArray()` first.
   - The new logic explicitly distinguishes between `Structure.ByReference` and structures passed by value (`!Structure.ByReference.class.isAssignableFrom(type)`). This aligns with the commit message regarding fixing a bug related to struct handling.

3. **Error Handling**:
   - More checks are added to throw `IllegalArgumentException` for unsupported types like `Buffer`. This change also refines error messages, offering clearer feedback about unsupported types.

4. **Use of `Native.getNativeSize(type)`**:
   - A call to `Native.getNativeSize(type)` is wrapped in a try-catch block that throws a custom `IllegalArgumentException` with a descriptive message in case of failure. This improves handling scenarios where `Native` might not support certain types.

5. **Array Handling**:
   - It enhances the exception message for illegal zero-length array structures with clearer information about the type causing the issue.

Overall, the changes involve significant restructuring of conditional logic, enhanced error handling, and more precise type checks. These alterations are directly relevant to the claim in the commit message about fixing a bug related to handling structs by value, addressing potential errors and improving robustness.

Thus, this commit indeed represents a bug fix. **Buggy**"
jna,3652.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,9 +1,10 @@
     Object getField(StructField structField) {
         try {
             return structField.field.get(this);
         }
         catch (Exception e) {
             throw new Error(""Exception reading field '""
-                            + structField.name + ""' in "" + getClass(), e);
+                            + structField.name + ""' in "" + getClass() 
+                            + "": "" + e);
         }
     }",NotBuggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",NotBuggy,"This diff modifies the error message to include the exception message when a field reading operation fails. The change focuses on improving the error-reporting mechanism by providing more context in the exception message.

Although the original commit message suggests a bug fix related to ""nested struct array read/write"", there is no direct evidence in the code change that addresses the specific reading or writing of nested arrays within a struct. The change in the code is strictly about improving the error message's detail, which is more of an enhancement to error reporting rather than a functional bug fix related to nested structures.

Therefore, while the commit message describes a bug fix, the code change itself does not explicitly reflect a modification that resolves a bug concerning nested struct arrays. It merely provides more context for debugging purposes.

The conclusion is **NotBuggy** because the changes in the code do not directly resolve the bug described in the commit message, but rather improve the error logging."
jna,4096.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,137 +1,147 @@
     void writeField(StructField structField) {
         // Get the offset of the field
         int offset = structField.offset;
 
         // Get the value from the field
         Object value = getField(structField);
         
         // Determine the type of the field
         Class nativeType = structField.type;
         ToNativeConverter converter = structField.writeConverter;
         if (converter != null) {
             value = converter.toNative(value, 
                     new StructureWriteContext(this, structField.field));
             // Assume any null values are pointers
             nativeType = value != null ? value.getClass() : Pointer.class;
         }
 
         // Java strings get converted to C strings, where a Pointer is used
         if (String.class == nativeType
             || WString.class == nativeType) {
 
             // Allocate a new string in memory
             boolean wide = nativeType == WString.class;
             if (value != null) {
                 NativeString nativeString = new NativeString(value.toString(), wide);
                 // Keep track of allocated C strings to avoid 
                 // premature garbage collection of the memory.
                 nativeStrings.put(structField.name, nativeString);
                 value = nativeString.getPointer();
             }
             else {
                 value = null;
             }
         }
 
         // Set the value at the offset according to its type
         if (nativeType == boolean.class || nativeType == Boolean.class) {
             memory.setInt(offset, Boolean.TRUE.equals(value) ? -1 : 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             memory.setByte(offset, ((Byte)value).byteValue());
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             memory.setShort(offset, ((Short)value).shortValue());
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             memory.setChar(offset, ((Character)value).charValue());
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             memory.setInt(offset, ((Integer)value).intValue());
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             memory.setLong(offset, ((Long)value).longValue());
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             memory.setFloat(offset, ((Float)value).floatValue());
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             memory.setDouble(offset, ((Double)value).doubleValue());
         }
         else if (nativeType == Pointer.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == String.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType == WString.class) {
             memory.setPointer(offset, (Pointer)value);
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             if (cls == byte.class) {
                 byte[] buf = (byte[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == short.class) {
                 short[] buf = (short[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == char.class) {
                 char[] buf = (char[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == int.class) {
                 int[] buf = (int[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == long.class) {
                 long[] buf = (long[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == float.class) {
                 float[] buf = (float[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (cls == double.class) {
                 double[] buf = (double[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 Pointer[] buf = (Pointer[])value;
                 memory.write(offset, buf, 0, buf.length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sbuf = (Structure[])value;
-                Pointer[] buf = new Pointer[sbuf.length];
-                for (int i=0;i < sbuf.length;i++) {
-                    buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] buf = new Pointer[sbuf.length];
+                    for (int i=0;i < sbuf.length;i++) {
+                        buf[i] = sbuf[i] == null ? null : sbuf[i].getPointer();
+                    }
+                    memory.write(offset, buf, 0, buf.length);
                 }
-                memory.write(offset, buf, 0, buf.length);
+                else {
+                    for (int i=0;i < sbuf.length;i++) {
+                        if (sbuf[i] == null) {
+                            sbuf[i] = newInstance(cls);
+                            sbuf[i].useMemory(memory, offset + i * sbuf[i].size());
+                        }
+                        sbuf[i].write();
+                    }
+                }
             }
             else {
                 throw new IllegalArgumentException(""Inline array of ""
                                                    + cls + "" not supported"");
             }
         }
         else if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)value;
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 memory.setPointer(offset, s == null ? null : s.getPointer());
             }
             else {
                 s.useMemory(memory, offset);
                 s.write();
             }
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             memory.setPointer(offset, CallbackReference.getFunctionPointer((Callback)value));
         }
         else {
         	String msg = ""Structure field \"""" + structField.name
         	    + ""\"" was declared as "" + nativeType 
         	    + "", which is not supported within a Structure"";
             throw new IllegalArgumentException(msg);
         }
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The provided code diff involves changes to the method `writeField` which is responsible for writing different field types to memory. The commit message mentions a bug fix related to nested struct array read/write. Let's examine the changes:

1. **Initial Condition Check Change**:
   - The original code checks if a `Structure` is ""assignable from"" `cls` and `ByReference` together as a condition for handling arrays of `Structure`.
   
2. **Refactored Condition and Loop**:
   - The refactored code separates the conditions:
     - If `Structure.class.isAssignableFrom(cls)`, the method proceeds to check for `ByReference`.
     - If `ByReference.class.isAssignableFrom(cls)` holds true, a `Pointer` array is used to store the results and write them to memory as before.
     - If `ByReference` is not applicable, it implies that the structures should be handled inline.
       - Inline handling involves writing each non-null structure in the array to the specified memory offset, using `useMemory(memory, offset + i * sbuf[i].size())` and calling `sbuf[i].write()` afterwards.
       - When encountering a `null` element, a new instance is created and initialized in memory.

3. **Bug Fix Implication**:
   - The change addresses the issue of handling `Structure` arrays that do not implement `ByReference`. Previously, such scenarios were likely not managed correctly, causing potential errors or memory mismanagement.
   - The structured handling for arrays of `Structure` objects improves the logic, ensuring proper memory operations are performed irrespective of `ByReference`.

Overall, the changes in this diff align well with the commit message regarding fixing a bug in nested struct array read/write operations. The new logic supports the correct handling of `Structure` arrays, particularly when they are intended to be used inline, addressing a previously unsupported case.

Thus, based on the examination, this modification does indeed represent a bug fix as described in the commit message. The conclusion is **Buggy**."
jna,4092.json,4068c6da1a057942ee7eb167c0e5436b0f225761,"@@ -1,136 +1,146 @@
     Object readField(StructField structField) {
         
         // Get the offset of the field
         int offset = structField.offset;
 
         // Determine the type of the field
         Class nativeType = structField.type;
         FromNativeConverter readConverter = structField.readConverter;
         if (readConverter != null) {
             nativeType = readConverter.nativeType();
         }
 
         // Get the value at the offset according to its type
         Object result = null;
         if (Structure.class.isAssignableFrom(nativeType)) {
             Structure s = (Structure)getField(structField);
             if (ByReference.class.isAssignableFrom(nativeType)) {
                 s = updateStructureByReference(nativeType, s, memory.getPointer(offset));
             }
             else {
                 s.useMemory(memory, offset);
                 s.read();
             }
             result = s;
         }
         else if (nativeType == boolean.class || nativeType == Boolean.class) {
             result = Boolean.valueOf(memory.getInt(offset) != 0);
         }
         else if (nativeType == byte.class || nativeType == Byte.class) {
             result = new Byte(memory.getByte(offset));
         }
         else if (nativeType == short.class || nativeType == Short.class) {
             result = new Short(memory.getShort(offset));
         }
         else if (nativeType == char.class || nativeType == Character.class) {
             result = new Character(memory.getChar(offset));
         }
         else if (nativeType == int.class || nativeType == Integer.class) {
             result = new Integer(memory.getInt(offset));
         }
         else if (nativeType == long.class || nativeType == Long.class) {
             result = new Long(memory.getLong(offset));
         }
         else if (nativeType == float.class || nativeType == Float.class) {
             result=new Float(memory.getFloat(offset));
         }
         else if (nativeType == double.class || nativeType == Double.class) {
             result = new Double(memory.getDouble(offset));
         }
         else if (nativeType == Pointer.class) {
             result = memory.getPointer(offset);
         }
         else if (nativeType == String.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? p.getString(0) : null;
         }
         else if (nativeType == WString.class) {
             Pointer p = memory.getPointer(offset);
             result = p != null ? new WString(p.getString(0, true)) : null;
         }
         else if (Callback.class.isAssignableFrom(nativeType)) {
             // Overwrite the Java memory if the native pointer is a different
             // function pointer.
             Pointer fp = memory.getPointer(offset);
             if (fp == null) {
                 result = null;
             }
             else {
                 Callback cb = (Callback)getField(structField);
                 Pointer oldfp = CallbackReference.getFunctionPointer(cb);
                 if (!fp.equals(oldfp)) {
                     cb = CallbackReference.getCallback(nativeType, fp);
                 }
                 result = cb;
             }
         }
         else if (nativeType.isArray()) {
             Class cls = nativeType.getComponentType();
             int length = 0;
             Object o = getField(structField);
             if (o == null) {
                 throw new IllegalStateException(""Array field in Structure not initialized"");
             }
             length = Array.getLength(o);
             result = o;
 
             if (cls == byte.class) {
                 memory.read(offset, (byte[])result, 0, length);
             }
             else if (cls == short.class) {
                 memory.read(offset, (short[])result, 0, length);
             }
             else if (cls == char.class) {
                 memory.read(offset, (char[])result, 0, length);
             }
             else if (cls == int.class) {
                 memory.read(offset, (int[])result, 0, length);
             }
             else if (cls == long.class) {
                 memory.read(offset, (long[])result, 0, length);
             }
             else if (cls == float.class) {
                 memory.read(offset, (float[])result, 0, length);
             }
             else if (cls == double.class) {
                 memory.read(offset, (double[])result, 0, length);
             }
             else if (Pointer.class.isAssignableFrom(cls)) {
                 memory.read(offset, (Pointer[])result, 0, length);
             }
-            else if (Structure.class.isAssignableFrom(cls)
-                     && ByReference.class.isAssignableFrom(cls)) {
+            else if (Structure.class.isAssignableFrom(cls)) {
                 Structure[] sarray = (Structure[])result;
-                Pointer[] parray = memory.getPointerArray(offset, sarray.length);
-                for (int i=0;i < sarray.length;i++) {
-                    sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                if (ByReference.class.isAssignableFrom(cls)) {
+                    Pointer[] parray = memory.getPointerArray(offset, sarray.length);
+                    for (int i=0;i < sarray.length;i++) {
+                        sarray[i] = updateStructureByReference(cls, sarray[i], parray[i]);
+                    }
+                }
+                else {
+                    for (int i=0;i < sarray.length;i++) {
+                        if (sarray[i] == null) {
+                            sarray[i] = newInstance(cls);
+                            sarray[i].useMemory(memory, offset + i * sarray[i].size());
+                        }
+                        sarray[i].read();
+                    }
                 }
             }
             else {
                 throw new IllegalArgumentException(""Array of ""
                                                    + cls + "" not supported"");
             }
         }
         else {
             throw new IllegalArgumentException(""Unsupported field type \""""
                                                + nativeType + ""\"""");
         }
 
         if (readConverter != null) {
             result = readConverter.fromNative(result, structField.context);
         }
 
         // Update the value on the field
         setField(structField, result);
         return result;
     }",Buggy,"Fix bug in nested struct array read/write

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@445 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message describes a bug fix related to reading and writing nested struct arrays. The code changes reflect this by adjusting how `Structure` arrays are read within the `readField` method. Specifically, the diff enhances the existing logic for handling `Structure` array elements when those elements are not of type `ByReference`.

### Analysis:
1. **Original Code**: The original code only handled `Structure` arrays if they were `ByReference`. It assumed all elements in the `Structure` array were references and used `updateStructureByReference`.

2. **Updated Code**: The change adds a condition to check if the `Structure` array elements are not `ByReference`. If they are not, it initializes each element if necessary, assigns memory correctly, and then calls `read()` on them individually. This is important for nested structs that don't use references but still need to be read from memory.

3. **Error-Handling/Constraint Addition**: 
   - More comprehensive handling is added for the case where `Structure` elements aren't `ByReference`.
   - There's also a new initialization step for potentially null array elements, ensuring that memory is correctly allocated and utilized.

4. **Logical Adjustments**: The addition of new logic within the `else` branch after checking `ByReference` demonstrates a critical adjustment to support a wider range of use cases in struct arrays.

The changes in the code address potential null pointer issues and memory misalignment problems when dealing with non-reference `Structure` arrays, indicating that a logical bug has been fixed in how these arrays are read and written to memory.

Given these points, the changes align well with the commit message stating a ""bug fix"" and indicate updates specifically to fix ""nested struct array read/write"" issues as mentioned.

Thus, the commit pertains to bug fixes, and I conclude with **Buggy**."
jna,2043.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,3 +1,3 @@
     public static boolean FAILED(int hr) {
-        return (hr != S_OK);
+        return hr < 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"The diff changes the logic in the `FAILED` method from checking if `hr` (HRESULT) is not equal to `S_OK` to checking if `hr` is less than zero. In Windows programming, HRESULT values use their sign to indicate success or failure: negative values represent failure. The original logic erroneously relied on a single success code (`S_OK`), ignoring other possible success codes. By changing the logic to `hr < 0`, the code correctly identifies failure across all HRESULTs, aligning with standard definitions provided by MSDN. This change addresses a logical error in determining failure conditions, which can lead to incorrect program behavior.

Given the commit message and this correction, the code change clearly indicates a bug fix rather than an adjustment or refactor. Therefore, the response is **Buggy**."
jna,2041.json,fe79ff84115369d6cc0e1d6fa240d2c431094af7,"@@ -1,6 +1,3 @@
     public static boolean SUCCEEDED(int hr) {
-        if (hr == S_OK)
-            return true;
-        else
-            return false;
+        return hr >= 0;
     }",Buggy,"Fix SUCCEEDED and FAILED instead of trying to fix the bug where it's visible. See MSDN for FAILED and SUCCEEDED definitions

Changes
",Buggy,"The commit message refers to fixing the SUCCEEDED and FAILED methods. The changes to the SUCCEEDED method modify the return condition logic. Previously, SUCCEEDED only returned true for a specific value (S_OK). 

The revised logic returns true for any non-negative value, aligning with the standard definition where in HRESULT systems, a non-negative value (including S_OK and all other non-negative success codes) indicates success. This change corrects the method to handle success codes beyond just S_OK.

No changes to the FAILED method are shown, but the SUCCEEDED adjustment is a bug fix because it resolves a logical flaw where only a single success code was recognized. Thus, the change enhances the function's adherence to industry-standard HRESULT handling.

Since the commit message and the changes indicate a logical correction to existing methods dealing with HRESULT values, this aligns with a bug fix.

The answer is **Buggy**."
jna,4111.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,21 +1,21 @@
     private void validateField(String name, Class type) {
         if (typeMapper != null) {
             ToNativeConverter toNative = typeMapper.getToNativeConverter(type);
             if (toNative != null) {
                 validateField(name, toNative.nativeType());
                 return;
             }
         }
         if (type.isArray()) {
             validateField(name, type.getComponentType());
         }
         else {
             try {
                 getNativeSize(type);
             }
             catch(IllegalArgumentException e) {
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + name + ""' ("" + type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
         }
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The change in the code involves enhancing the exception handling by including additional information in the exception message when an `IllegalArgumentException` is caught. In particular, the updated message now appends the original exception's message (`e.getMessage()`) to provide more details about what went wrong.

This modification is significant for debugging purposes. By making sure that all relevant information about the error is included in the message, the code change makes it easier to identify and rectify the underlying issue. The commit message suggests that this change ""fixes a broken test,"" implying that the previous lack of detailed exception information hindered testing and debugging.

Given that the modifications aim to improve the clarity and completeness of error reporting in a scenario where validation checks fail, this change aligns well with the description of fixing a problem related to how test failures were handled. Therefore, it can be interpreted as a bug fix in the sense that it resolves an issue related to insufficient error information during field validation.

Based on the analysis, the conclusion is **Buggy**."
jna,4113.json,64466deb5eb01a0d9c3e27ceb2a89d4e6d546908,"@@ -1,147 +1,147 @@
     private LayoutInfo deriveLayout(boolean force, boolean avoidFFIType) {
         int calculatedSize = 0;
         List fields = getFields(force);
         if (fields == null) {
             return null;
         }
 
         LayoutInfo info = new LayoutInfo();
         info.alignType = this.alignType;
         info.typeMapper = this.typeMapper;
 
         boolean firstField = true;
         for (Iterator i=fields.iterator();i.hasNext();firstField=false) {
             Field field = (Field)i.next();
             int modifiers = field.getModifiers();
 
             Class type = field.getType();
             if (type.isArray()) {
                 info.variable = true;
             }
             StructField structField = new StructField();
             structField.isVolatile = Modifier.isVolatile(modifiers);
             structField.isReadOnly = Modifier.isFinal(modifiers);
             if (structField.isReadOnly) {
                 if (!Platform.RO_FIELDS) {
                     throw new IllegalArgumentException(""This VM does not support read-only fields (field '""
                                                        + field.getName() + ""' within "" + getClass() + "")"");
                 }
                 // In J2SE VMs, this allows overriding the value of final
                 // fields
                 field.setAccessible(true);
             }
             structField.field = field;
             structField.name = field.getName();
             structField.type = type;
 
             // Check for illegal field types
             if (Callback.class.isAssignableFrom(type) && !type.isInterface()) {
                 throw new IllegalArgumentException(""Structure Callback field '""
                                                    + field.getName()
                                                    + ""' must be an interface"");
             }
             if (type.isArray()
                 && Structure.class.equals(type.getComponentType())) {
                 String msg = ""Nested Structure arrays must use a ""
                     + ""derived Structure type so that the size of ""
                     + ""the elements can be determined"";
                 throw new IllegalArgumentException(msg);
             }
 
             int fieldAlignment = 1;
             if (!Modifier.isPublic(field.getModifiers())) {
                 continue;
             }
 
             Object value = getFieldValue(structField.field);
             if (value == null && type.isArray()) {
                 if (force) {
                     throw new IllegalStateException(""Array fields must be initialized"");
                 }
                 // can't calculate size yet, defer until later
                 return null;
             }
             Class nativeType = type;
             if (NativeMapped.class.isAssignableFrom(type)) {
                 NativeMappedConverter tc = NativeMappedConverter.getInstance(type);
                 nativeType = tc.nativeType();
                 structField.writeConverter = tc;
                 structField.readConverter = tc;
                 structField.context = new StructureReadContext(this, field);
             }
             else if (typeMapper != null) {
                 ToNativeConverter writeConverter = typeMapper.getToNativeConverter(type);
                 FromNativeConverter readConverter = typeMapper.getFromNativeConverter(type);
                 if (writeConverter != null && readConverter != null) {
                     value = writeConverter.toNative(value,
                                                     new StructureWriteContext(this, structField.field));
                     nativeType = value != null ? value.getClass() : Pointer.class;
                     structField.writeConverter = writeConverter;
                     structField.readConverter = readConverter;
                     structField.context = new StructureReadContext(this, field);
                 }
                 else if (writeConverter != null || readConverter != null) {
                     String msg = ""Structures require bidirectional type conversion for "" + type;
                     throw new IllegalArgumentException(msg);
                 }
             }
 
             if (value == null) {
                 value = initializeField(structField.field, type);
             }
 
             try {
                 structField.size = getNativeSize(nativeType, value);
                 fieldAlignment = getNativeAlignment(nativeType, value, firstField);
             }
             catch(IllegalArgumentException e) {
                 // Might simply not yet have a type mapper set yet
                 if (!force && typeMapper == null) {
                     return null;
                 }
-                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + "")"";
+                String msg = ""Invalid Structure field in "" + getClass() + "", field name '"" + structField.name + ""' ("" + structField.type + ""): "" + e.getMessage();
                 throw new IllegalArgumentException(msg, e);
             }
 
             // Align fields as appropriate
             if (fieldAlignment == 0) {
                 throw new Error(""Field alignment is zero for field '"" + structField.name + ""' within "" + getClass());
             }
             info.alignment = Math.max(info.alignment, fieldAlignment);
             if ((calculatedSize % fieldAlignment) != 0) {
                 calculatedSize += fieldAlignment - (calculatedSize % fieldAlignment);
             }
             if (this instanceof Union) {
                 structField.offset = 0;
                 calculatedSize = Math.max(calculatedSize, structField.size);
             }
             else {
                 structField.offset = calculatedSize;
                 calculatedSize += structField.size;
             }
 
             // Save the field in our list
             info.fields.put(structField.name, structField);
 
             if (info.typeInfoField == null
                 || info.typeInfoField.size < structField.size
                 || (info.typeInfoField.size == structField.size
                     && Structure.class.isAssignableFrom(structField.type))) {
                 info.typeInfoField = structField;
             }
         }
 
         if (calculatedSize > 0) {
             int size = addPadding(calculatedSize, info.alignment);
             // Update native FFI type information, if needed
             if (this instanceof ByValue && !avoidFFIType) {
                 getTypeInfo();
             }
             info.size = size;
             return info;
         }
 
         throw new IllegalArgumentException(""Structure "" + getClass()
                                            + "" has unknown or zero size (ensure ""
                                            + ""all fields are public)"");
     }",NotBuggy,"Ensure exception message includes all faulty field information (fixes broken test)
",Buggy,"The commit message states that the update ensures exception messages include all faulty field information, fixing a broken test. Let's examine the code changes to see if this aligns with a bug fix in exception handling:

1. **Context of Change**: The code modification occurs within a `try-catch` block, where an `IllegalArgumentException` might be thrown during size and alignment calculations for fields in a structure.

2. **Original Issue**: The original code constructed an exception message without including specifics about the original exception (`e`). Thus, if an exception occurred, some detailed information regarding the error's origin or cause would be missed, potentially making the error message less informative and complicating debugging.

3. **Change Made**:
   - The modification appends `e.getMessage()` to the exception message. This change enriches the exception detail by including the message from the underlying `IllegalArgumentException` encountered within the `try` block.

4. **Analysis**:
   - This change improves the clarity and usefulness of error messages, aligning with the commit's stated goal to ensure all faulty field information is included in exception messages.
   - The enhanced exception message aids in debugging by providing more comprehensive insight into what specifically went wrong during the layout derivation process.

5. **Relation to Commit Message**: The update matches the commit's description of fixing a bug related to exception message completeness, thus directly addressing the issue noted in the broken test.

Given this analysis, the code change aligns well with improving exception-handling clarity and specificity, hence indicating a bug fix focusing on error message comprehensiveness and debugging facilitation.

The conclusion is **Buggy**."
spring-framework,25502.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,9 +1,7 @@
 	public static void sortBySpecificityAndQuality(List<MediaType> mediaTypes) {
 		Assert.notNull(mediaTypes, ""'mediaTypes' must not be null"");
 		if (mediaTypes.size() > 1) {
-			Comparator<?>[] comparators = new Comparator[2];
-			comparators[0] = MediaType.SPECIFICITY_COMPARATOR;
-			comparators[1] = MediaType.QUALITY_VALUE_COMPARATOR;
-			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(comparators));
+			Collections.sort(mediaTypes, new CompoundComparator<MediaType>(
+					MediaType.SPECIFICITY_COMPARATOR, MediaType.QUALITY_VALUE_COMPARATOR));
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message indicates refactoring and polishing without specific fixes for bugs or errors. The provided code diff simplifies the creation of a `CompoundComparator` by using a varargs constructor rather than an array. This change mainly improves code readability and maintenance without altering functionality or fixing a bug. It aligns well with the commit message, which describes refactoring efforts. Hence, there is no indication of a bug fix in this change.

**NotBuggy**"
spring-framework,15028.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof NullSafeComparator)) {
 			return false;
 		}
-		NullSafeComparator other = (NullSafeComparator) obj;
+		NullSafeComparator<T> other = (NullSafeComparator<T>) obj;
 		return (this.nonNullComparator.equals(other.nonNullComparator) && this.nullsLow == other.nullsLow);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The changes in the provided diff involve adding generic typing to the `NullSafeComparator` instance in the `equals` method. This change doesn't affect the runtime behavior of the code but rather improves type safety at compile-time. It aligns with the commit message which mentions adding generic typing where appropriate. There are no specific bug fixes, error-handling updates, logical corrections, or exception-handling improvements described in this change. 

Thus, considering the nature of the change and how it relates to improving the code structure and type safety rather than addressing a defect, the conclusion is **NotBuggy**."
spring-framework,15035.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof InvertibleComparator)) {
 			return false;
 		}
-		InvertibleComparator other = (InvertibleComparator) obj;
+		InvertibleComparator<T> other = (InvertibleComparator<T>) obj;
 		return (this.comparator.equals(other.comparator) && this.ascending == other.ascending);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message mentions refactoring, adding generic typing, and suppressing warnings. The change in the code aligns with part of these tasks as it adds generic typing to the `InvertibleComparator` in the `equals` method. The update ensures the `obj` is correctly cast to an `InvertibleComparator` with the same generic type `T`, improving type safety and matching one of the stated objectives in the commit message.

However, there is no indication of a bug being fixed here. This change is primarily aimed at improving type safety and suppressing generics warnings. There is no correction of logic, error handling, or other usual signs of a bug fix.

Therefore, the changes in this commit are **NotBuggy**."
spring-framework,15051.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,10 +1,10 @@
 	public boolean equals(Object obj) {
 		if (this == obj) {
 			return true;
 		}
 		if (!(obj instanceof CompoundComparator)) {
 			return false;
 		}
-		CompoundComparator other = (CompoundComparator) obj;
+		CompoundComparator<T> other = (CompoundComparator<T>) obj;
 		return this.comparators.equals(other.comparators);
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message describes various refactoring and polishing tasks, including adding generic typing where appropriate. The code change involves adding generic typing to the `CompoundComparator` object in the `equals` method. This change improves type safety and ensures that the method handles the `CompoundComparator` generically, but does not specifically address any bug or incorrect behavior in the existing logic. Therefore, this change aligns with the commit message related to refactoring and polishing but does not indicate any bug fix.

Concluding with **NotBuggy** because the change is about code quality improvement, not a bug fix."
spring-framework,15045.json,98218687070fc47eb6f91e81d3d714bf96068cc5,"@@ -1,5 +1,5 @@
 	public void invertOrder() {
-		for (InvertibleComparator comparator : this.comparators) {
+		for (InvertibleComparator<T> comparator : this.comparators) {
 			comparator.invertOrder();
 		}
 	}",NotBuggy,"Refactor and polish various Comparator impls

 - Refactor CompoundComparator constructor to use varargs
 - Refactor MediaType to consume new varargs constructor
 - Add notNull assertions where appropriate
 - Add generic typing where appropriate
 - Suppress generics warnings elsewhere
 - Fix whitespace errors
",NotBuggy,"The commit message describes changes to Comparator implementations, including refactoring, generic typing, and fixing whitespace. The diff adds a generic type parameter `<T>` to the `InvertibleComparator` within a loop, which aligns with the goal of adding generic typing as stated in the commit message. This does not correspond to a bug fix, as it mainly improves type safety and code clarity without altering the existing functionality or logic.

Given the nature of the change and how it matches the description of refining code structure, not solving incorrect behavior, the analysis concludes:

**NotBuggy**"
spring-framework,16701.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,8 @@
 	public ClientResponse build() {
+
 		ClientHttpResponse httpResponse =
 				new BuiltClientHttpResponse(this.statusCode, this.headers, this.cookies, this.body);
 
-		// When building ClientResponse manually, the ClientRequest.logPrefix() has to be passed,
-		// e.g. via ClientResponse.Builder, but this (builder) is not used currently.
-		return new DefaultClientResponse(httpResponse, this.strategies, """", """", () -> this.request);
+		return new DefaultClientResponse(
+				httpResponse, this.strategies, """", """", () -> this.request);
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",NotBuggy,"The commit message refers to introducing a new `mutate()` method as an alternative to `from()`, addressing an issue with the latter ignoring the body if it's set twice. The given diff, however, does not show any changes related to adding a `mutate()` method or deprecating the `from()` method. Instead, it only shows minor reformatting of code.

Because the changes in the diff don't reflect the commit message's described bug fix (related to `from()` and introduction of `mutate()`), and there's no logical/functional correction or new functionality that aligns with the mentioned bug issue, the answer is **NotBuggy**."
spring-framework,16453.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,8 +1,7 @@
 	public static ExchangeFilterFunction limitResponseSize(long maxByteCount) {
 		return (request, next) ->
-				next.exchange(request).map(response -> {
-					Flux<DataBuffer> body = response.body(BodyExtractors.toDataBuffers());
-					body = DataBufferUtils.takeUntilByteCount(body, maxByteCount);
-					return ClientResponse.from(response).body(body).build();
-				});
+				next.exchange(request).map(response ->
+						response.mutate()
+								.body(body -> DataBufferUtils.takeUntilByteCount(body, maxByteCount))
+								.build());
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message reflects the need to replace the `from()` method in `ClientResponse` due to a flaw that ignores the body, which cannot be resolved due to how applications currently set it. The change in the code replaces the `from()` method with `mutate()`, which seems to address the identified flaw and potential inefficiencies. `mutate()` allows for more efficient handling of the response and aligns with methodologies on the server side, addressing the flaw where setting the body multiple times could result in data being drained incorrectly.

Here's the breakdown:
- The previous implementation used `ClientResponse.from(response)` which had a known issue with handling multiple body modifications correctly.
- The updated implementation utilizes `response.mutate()`, which suggests a better design that avoids the body draining issue, allowing more efficient and correct processing within the filter chain.
- The commit message and code changes are consistent; both indicate a design flaw resolution by adopting a more efficient and reliable method (`mutate()`).

The change implies a bug fix because it corrects a known flaw in how response bodies were handled. Therefore, the conclusion is **Buggy**."
spring-framework,16698.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,11 +1,10 @@
 	public ClientResponse.Builder body(String body) {
 		Assert.notNull(body, ""Body must not be null"");
 		releaseBody();
-		DataBufferFactory dataBufferFactory = new DefaultDataBufferFactory();
 		this.body = Flux.just(body).
 				map(s -> {
 					byte[] bytes = body.getBytes(StandardCharsets.UTF_8);
-					return dataBufferFactory.wrap(bytes);
+					return new DefaultDataBufferFactory().wrap(bytes);
 				});
 		return this;
 	}",NotBuggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",NotBuggy,"The commit message discusses changes related to adding a `mutate()` method and deprecating an existing `from()` method due to specific limitations. However, the diff provided addresses neither the addition of `mutate()` nor the deprecation of `from()`. Instead, it modifies the `body` method within a similar context but does not address the issues specifically outlined in the commit message.

In the provided diff, the code change consolidates the creation of a `DefaultDataBufferFactory` within the `map` function. This does not relate directly to fixing a bug described in the commit message, nor does it relate to the addition or deprecation of methods.

Given the absence of a direct relation between the diff and the purported intention of the commit message regarding a bug in `from()`, and because this change does not seem to fix a bug but rather streamline object instantiation, I conclude the diff alone is **NotBuggy**."
spring-framework,16585.json,67a06f5edcc8697af0941e238ef29bdb2a73245d,"@@ -1,3 +1,3 @@
 	static Builder from(ClientResponse other) {
-		return new DefaultClientResponseBuilder(other);
+		return new DefaultClientResponseBuilder(other, false);
 	}",Buggy,"Add mutate() to ClientResponse and deprecate from()

from() has the flaw of ignoring the body and it can't be fixed because
applications are guaranteed to be setting it already and if set twice
the builder drains the first body.

mutate() is a better fit in any case for what needs to be done in a
filter chain. It can be done more efficiently and is consistent with
similar options on the server side.

See gh-24680
",Buggy,"The commit message explains that `from()` is being deprecated due to a flaw where it ignores the body and could cause issues if the body is set multiple times. The code change in the diff reflects this by modifying the constructor call to `DefaultClientResponseBuilder` to include an additional parameter `false`, which likely affects how the body is handled or some aspect of builder behavior in the context of this flawed method.

While the code itself doesn't involve adding the `mutate()` method or mark `from()` as deprecated in this snippet, it appears aligned with the commit message's intent to address the flaw described. The focus is on avoiding the issue tied to body handling. Thus, it seems like a corrective action related to a bug (ignoring or mishandling the body), even though the actual change to address that is not fully observable in this snippet.

Considering the context and intention behind the changes as narrated in the commit message, the modifications align with an attempt to rectify behavior issues that could be deemed ""bug-like"" in nature due to the problems described.

Therefore, the conclusion is **Buggy**."
sonarqube,9310.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,14 +1,13 @@
   private java.util.Optional<CeTask> submit(CeTaskSubmit submission, EnumSet<SubmitOption> submitOptions) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     try (DbSession dbSession = dbClient.openSession(false)) {
       if (submitOptions.contains(UNIQUE_QUEUE_PER_COMPONENT)
         && submission.getComponentUuid() != null
         && dbClient.ceQueueDao().countByStatusAndComponentUuid(dbSession, PENDING, submission.getComponentUuid()) > 0) {
         return java.util.Optional.empty();
       }
       CeQueueDto dto = addToQueueInDb(dbSession, submission);
       CeTask task = loadTask(dbSession, dto);
       dbSession.commit();
       return java.util.Optional.of(task);
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",NotBuggy,"The commit message indicates quality improvements by removing unused code. The diff shows the removal of a checkState invocation, which would raise an exception if new task submission was paused. This line ensures that the system doesn't accept new tasks when submission is paused, which is a functional rather than an unused piece of code.

However, without further context, it’s unclear if removing this line fixes a bug. Since the commit message suggests improving code quality by removing unused code and doesn't indicate any impact on functionality or fixing errors, this change appears to align with refactoring rather than a bug fix.

Therefore, the assessment based on the provided diff and commit message is **NotBuggy**."
sonarqube,9312.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,16 +1,15 @@
   public List<CeTask> massSubmit(Collection<CeTaskSubmit> submissions, SubmitOption... options) {
-    checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");
     if (submissions.isEmpty()) {
       return Collections.emptyList();
     }
 
     try (DbSession dbSession = dbClient.openSession(true)) {
       List<CeQueueDto> ceQueueDtos = submissions.stream()
         .filter(filterBySubmitOptions(options, submissions, dbSession))
         .map(submission -> addToQueueInDb(dbSession, submission))
         .collect(Collectors.toList());
       List<CeTask> tasks = loadTasks(dbSession, ceQueueDtos);
       dbSession.commit();
       return tasks;
     }
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",NotBuggy,"The commit message states that the purpose is to fix quality flaws by removing unused code. In the diff, the line `checkState(!submitPaused.get(), ""Compute Engine does not currently accept new tasks"");` is removed, which is not unused code per se, but a functional line that checks whether submissions are currently paused.

Removing a check like this may not directly fix a bug but can influence the system's behavior or state management. However, following the logic provided in the commit message, this line is likely considered redundant or unnecessary in the context of the current application design, thus marked for removal.

In this case, the commit does not specify a fix for a malfunction. It appears to be a refactoring for code quality purposes, not addressing an apparent bug.

The correct conclusion here is **NotBuggy**."
sonarqube,8268.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // do nothing
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The provided commit contains a diff where a comment ""// do nothing"" is added to an empty method called `stop()`. The commit message ""Fix some quality flaws"" is quite generic and does not indicate a specific bug fix.

Analyzing the change:
- Adding a comment to an empty method to indicate it intentionally does nothing is a code quality improvement, often for clarity or documentation purposes. 
- This kind of change does not involve error correction, exception handling, or adjustment of logic indicating a functional bug fix.

Since there is no indication that this change addresses a software bug, it appears to be purely an improvement in code quality by making the developer's intent clear.

Conclusively, the change aligns with the commit message's focus on improving code quality, but it does not represent a bug fix. Hence, the conclusion is **NotBuggy**."
sonarqube,1454.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,3 +1,3 @@
-  public String metadata() {
-    return metadata;
+  public String internalKey() {
+    return internalKey;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message states it fixes ""quality flaws,"" which usually refers to code quality improvements and refactors rather than bug fixes. The diff showcases name changes in method and variable occurrences from `metadata` to `internalKey`, but there's no change in functionality. Such a modification is more likely related to improving code clarity or aligning naming conventions and does not necessarily align with fixing an existing bug.

Given the lack of functional change or logic corrections indicative of a bug fix, the answer based on the provided information is **NotBuggy**."
sonarqube,1485.json,3bfcfa0de67e7f00d7cd0dc74649fef7e5772298,"@@ -1,4 +1,4 @@
-  public NewRule setMetadata(@Nullable String metadata) {
-    this.metadata = metadata;
+  public NewRule setInternalKey(@Nullable String s) {
+    this.internalKey = s;
     return this;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message mentions fixing quality flaws, which often refers to code style or readability improvements rather than bug fixes. The change shown in the diff involves a method name and parameter updates, renaming `setMetadata` to `setInternalKey` and `metadata` to `internalKey`. This suggests a refactor for clarity or improved naming conventions, rather than rectifying a functional defect or error in the code.

No logical correction, exception handling, or error-handling update is apparent in the provided diff. The refactoring does not indicate the presence of a bug fix but rather an effort to enhance code quality.

Therefore, the answer is **NotBuggy**."
sonarqube,2297.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,14 +1,15 @@
   boolean isDescendant(DbSession dbSession, QualityProfileDto childProfile, @Nullable QualityProfileDto parentProfile) {
     QualityProfileDto currentParent = parentProfile;
     while (currentParent != null) {
       if (childProfile.getName().equals(currentParent.getName())) {
         return true;
       }
-      if (currentParent.getParentKee() != null) {
-        currentParent = db.qualityProfileDao().getByKey(dbSession, currentParent.getParentKee());
+      String parentKey = currentParent.getParentKee();
+      if (parentKey != null) {
+        currentParent = db.qualityProfileDao().getByKey(dbSession, parentKey);
       } else {
         currentParent = null;
       }
     }
     return false;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"This commit message states that the changes address ""some quality flaws,"" but it doesn't specify fixing a bug. Looking at the code changes:

1. **Code Clarity**: The extracted `String parentKey` makes the code slightly clearer but doesn't change the function of the code. It simply makes the variable explicitly available for checks, improving readability.

2. **Functionality**: The logic remains the same; no conditions or computations have been modified. The modification only refactors `if (currentParent.getParentKee() != null)` to use an intermediate variable `parentKey`.

Since this revision predominantly enhances code clarity without altering logic or error-handling (i.e., no bug has been fixed), it aligns with improvements for code quality rather than correcting a bug. Thus, the changes described do not suggest a direct bug fix.

Therefore, the conclusion is **NotBuggy**."
sonarqube,6627.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,30 +1,34 @@
   private ActiveRuleDto doUpdate(ActiveRuleChange change, RuleActivatorContext context, DbSession dbSession) {
     ActiveRuleDto activeRule;
     ActiveRuleDao dao = db.activeRuleDao();
     activeRule = context.activeRule();
-    activeRule.setSeverity(change.getSeverity());
-    if (change.getInheritance() != null) {
-      activeRule.setInheritance(change.getInheritance().name());
+    String severity = change.getSeverity();
+    if (severity != null) {
+      activeRule.setSeverity(severity);
+    }
+    ActiveRule.Inheritance inheritance = change.getInheritance();
+    if (inheritance != null) {
+      activeRule.setInheritance(inheritance.name());
     }
     dao.update(dbSession, activeRule);
 
     for (Map.Entry<String, String> param : change.getParameters().entrySet()) {
       ActiveRuleParamDto activeRuleParamDto = context.activeRuleParamsAsMap().get(param.getKey());
       if (activeRuleParamDto == null) {
         // did not exist
         if (param.getValue() != null) {
           activeRuleParamDto = ActiveRuleParamDto.createFor(context.ruleParamsByKeys().get(param.getKey()));
           activeRuleParamDto.setValue(param.getValue());
           dao.addParam(dbSession, activeRule, activeRuleParamDto);
         }
       } else {
         if (param.getValue() != null) {
           activeRuleParamDto.setValue(param.getValue());
           dao.updateParam(dbSession, activeRule, activeRuleParamDto);
         } else {
           dao.deleteParam(dbSession, activeRule, activeRuleParamDto);
         }
       }
     }
     return activeRule;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates fixing quality flaws, which typically involve refactoring for better readability, maintainability, or efficiency without altering functionality. Let's analyze the code changes step by step:

1. **Severity Handling**: 
   - Original: `activeRule.setSeverity(change.getSeverity());`
   - Modified: Added a null check before setting severity - `String severity = change.getSeverity(); if (severity != null) { activeRule.setSeverity(severity); }`

   This change ensures that the `setSeverity` method is only called if `severity` is not null. It improves robustness by preventing potential `null` assignment.

2. **Inheritance Handling**:
   - Original: `if (change.getInheritance() != null) { activeRule.setInheritance(change.getInheritance().name()); }`
   - Modified: The check remains, but the inheritance variable is stored first and checked for `null` - `ActiveRule.Inheritance inheritance = change.getInheritance(); if (inheritance != null) { activeRule.setInheritance(inheritance.name()); }`

   This change does not modify functionality but makes the code slightly clearer by introducing a local variable.

3. **Parameter Handling**: No changes were made here in terms of logic or structure.

There are no changes here specifically indicating a bug fix, such as logic corrections, exception handling improvements, or error checks apart from the added null check for severity which could prevent a potential null pointer issue but seems more like a preventive measure for quality than a direct bug fix.

The commit message and changes are consistent in terms of focusing on quality improvements without explicit bug fixes.

Thus, the correct assessment is **NotBuggy**."
sonarqube,2295.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,27 +1,27 @@
   void setParent(DbSession dbSession, String profileKey, @Nullable String parentKey) {
     QualityProfileDto profile = db.qualityProfileDao().getNonNullByKey(dbSession, profileKey);
     if (parentKey == null) {
       // unset if parent is defined, else nothing to do
       removeParent(dbSession, profile);
 
-    } else if (profile.getParentKee() == null || !profile.getParentKee().equals(parentKey)) {
+    } else if (profile.getParentKee() == null || !parentKey.equals(profile.getParentKee())) {
       QualityProfileDto parentProfile = db.qualityProfileDao().getNonNullByKey(dbSession, parentKey);
       if (isDescendant(dbSession, profile, parentProfile)) {
         throw new BadRequestException(String.format(""Descendant profile '%s' can not be selected as parent of '%s'"", parentKey, profileKey));
       }
       removeParent(dbSession, profile);
 
       // set new parent
       profile.setParentKee(parentKey);
       db.qualityProfileDao().update(dbSession, profile);
       for (ActiveRuleDto parentActiveRule : db.activeRuleDao().findByProfileKey(dbSession, parentKey)) {
         try {
           RuleActivation activation = new RuleActivation(parentActiveRule.getKey().ruleKey());
           activate(dbSession, activation, profileKey);
         } catch (BadRequestException e) {
           // for example because rule status is REMOVED
           // TODO return errors
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",Buggy,"The commit message mentions fixing ""quality flaws,"" implying code improvements, but not necessarily bug fixes. Analyzing the changes:

1. The change in the condition from `!profile.getParentKee().equals(parentKey)` to `!parentKey.equals(profile.getParentKee())` corrects potential null pointer issues if `profile.getParentKee()` is `null`. This prevents the application from potentially throwing a `NullPointerException` if the null check for `profile.getParentKee()` wasn't accounted for appropriately.

2. The change doesn't introduce new functionality or fix an obvious logical error directly affecting application's primary use cases apart from enhancing code reliability by preventing possible runtime exceptions.

Although the change improves code quality and stability, preventing a `NullPointerException` from occurring could be considered a bug fix from a robustness perspective, as the code was improved to handle nulls more safely. 

Considering the potential null-safety issue and the need for defensive programming, I conclude with **Buggy**."
sonarqube,6621.json,074f5c655822a466819c899ed9c90aef4ba1d2b4,"@@ -1,57 +1,58 @@
   private List<ActiveRuleChange> doActivate(DbSession dbSession, RuleActivation activation, RuleActivatorContext context) {
     context.verifyForActivation();
     List<ActiveRuleChange> changes = Lists.newArrayList();
     ActiveRuleChange change;
     boolean stopPropagation = false;
 
-    if (context.activeRule() == null) {
+    ActiveRuleDto activeRule = context.activeRule();
+    if (activeRule == null) {
       if (activation.isReset()) {
         // ignore reset when rule is not activated
         return changes;
       }
       // new activation
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.ACTIVATED, context.activeRuleKey());
       applySeverityAndParamToChange(activation, context, change);
       if (activation.isCascade() || context.isSameAsParent(change)) {
         change.setInheritance(ActiveRule.Inheritance.INHERITED);
       }
     } else {
       // already activated
-      if (activation.isCascade() && context.activeRule().doesOverride()) {
+      if (activation.isCascade() && activeRule.doesOverride()) {
         // propagating to descendants, but child profile already overrides rule -> stop propagation
         return changes;
       }
       change = ActiveRuleChange.createFor(ActiveRuleChange.Type.UPDATED, context.activeRuleKey());
-      if (activation.isCascade() && context.activeRule().getInheritance() == null) {
+      if (activation.isCascade() && activeRule.getInheritance() == null) {
         // activate on child, then on parent -> mark child as overriding parent
         change.setInheritance(ActiveRule.Inheritance.OVERRIDES);
         change.setSeverity(context.currentSeverity());
         change.setParameters(context.activeRuleParamsAsStringMap());
         stopPropagation = true;
       } else {
         applySeverityAndParamToChange(activation, context, change);
         if (!activation.isCascade() && context.parentActiveRule() != null) {
           // override rule which is already declared on parents
           change.setInheritance(context.isSameAsParent(change) ? ActiveRule.Inheritance.INHERITED : ActiveRule.Inheritance.OVERRIDES);
         }
       }
       if (context.isSame(change)) {
         change = null;
       }
     }
 
     if (change != null) {
       changes.add(change);
       persist(change, context, dbSession);
     }
 
     if (!stopPropagation) {
       changes.addAll(cascadeActivation(dbSession, activation, context.profile().getKey()));
     }
 
     if (!changes.isEmpty()) {
       updateProfileDate(dbSession, context);
       previewCache.reportGlobalModification(dbSession);
     }
     return changes;
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message indicates fixing ""some quality flaws,"" suggesting code quality improvements rather than bug fixes. Let's assess the diff:

1. **Variable Optimization**:
   - The code replaces calls to `context.activeRule()` within the method body with a local `ActiveRuleDto activeRule` variable, enhancing readability and possibly efficiency. This change is more about refactoring than directly addressing bugs.

2. **Code Logic**:
   - The logic of the code remains unchanged. The conditions and flow maintain their initial structure while improving some operational clarity by untangling repeated method calls.

3. **Error Handling and Logical Corrections**:
   - There are no apparent error-handling improvements or logical corrections in the diff indicating the rectification of faulty behavior.

In conclusion, the modifications aim to improve code readability, maintainability, and possibly performance through local variable use. They do not correct specific logic or error-handling flaws that imply a bug fix. Consequently, the answer is **NotBuggy**."
sonarqube,11064.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,8 +1,8 @@
   private static String groupSearchToSql(@Nullable String query) {
     if (query == null) {
       return null;
     }
 
     String upperCasedNameQuery = StringUtils.upperCase(query, Locale.ENGLISH);
-    return DaoDatabaseUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
+    return DaoUtils.buildLikeValue(upperCasedNameQuery, WildcardPosition.BEFORE_AND_AFTER);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",NotBuggy,"The commit message focuses on removing unused code for quality improvements, but the diff shows a method call change from `DaoDatabaseUtils.buildLikeValue` to `DaoUtils.buildLikeValue`. This alteration suggests a possible consolidation or refactoring but not explicitly a bug fix. The commit message does not directly identify a bug that this change fixes.

1. **Commit Message**: The message suggests quality improvements by removing unused code, which typically addresses maintainability and clarity rather than specific bug fixes. There's no direct reference to correcting incorrect behavior.
2. **Code Diff Analysis**:
   - The method call is switched to potentially another utility class (`DaoUtils` instead of `DaoDatabaseUtils`), implying that the function's behavior might be the same and results in a cleaner codebase.
   - This change doesn't clearly indicate error correction, logic fixes, or exception-handling adjustments intrinsic to a bug fix.

Since there is no direct indication of a bug-related correction either in the message or the specific code change presented (besides the possibility of maintaining consistency or refactoring for clarity), this suggests the commit is about code quality improvement.

Based on the analysis, the determination is **NotBuggy**."
sonarqube,12391.json,e582be977c992d38fff928388bb1f1ae928fd146,"@@ -1,13 +1,13 @@
   private Optional<CeQueueDto> tryToPeek(DbSession session, EligibleTaskDto eligible, String workerUuid) {
     long now = system2.now();
     int touchedRows = mapper(session).updateIf(eligible.getUuid(),
       new UpdateIf.NewProperties(IN_PROGRESS, workerUuid, eligible.getExecutionCount() + 1, now, now),
       new UpdateIf.OldProperties(PENDING, eligible.getExecutionCount()));
     if (touchedRows != 1) {
       return Optional.empty();
     }
 
     CeQueueDto result = mapper(session).selectByUuid(eligible.getUuid());
     session.commit();
-    return Optional.of(result);
+    return Optional.ofNullable(result);
   }",NotBuggy,"Fix Quality flaws

mainly by removing unused code
",Buggy,"The change from `Optional.of(result)` to `Optional.ofNullable(result)` handles potential null values gracefully, which prevents a `NullPointerException` in case `result` is null. The commit message, however, mentions removing unused code to fix quality flaws, suggesting the focus is on code cleanup rather than bug fixes. Nevertheless, the code change directly modifies how null values are treated, which suggests an improvement related to error or exception handling. While the commit message might not explicitly mention this specific fix, the modification itself indicates a bug related to potential null handling.

Therefore, the change demonstrates a bug fix in exception handling, making the conclusion **Buggy**."
sonarqube,14783.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,18 +1,18 @@
-  private static Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
-    if (!measureOptional.isPresent()) {
-      return Optional.absent();
-    }
-    Measure measure = measureOptional.get();
-    switch (measure.getValueType()) {
-      case DOUBLE:
-        return Optional.of(measure.getDoubleValue());
-      case LONG:
-        return Optional.of((double) measure.getLongValue());
-      case INT:
-        return Optional.of((double) measure.getIntValue());
-      case NO_VALUE:
+    private Optional<Double> getDoubleValue(Optional<Measure> measureOptional) {
+      if (!measureOptional.isPresent()) {
         return Optional.absent();
-      default:
-        throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
-    }
-  }

+      }
+      Measure measure = measureOptional.get();
+      switch (measure.getValueType()) {
+        case DOUBLE:
+          return Optional.of(measure.getDoubleValue());
+        case LONG:
+          return Optional.of((double) measure.getLongValue());
+        case INT:
+          return Optional.of((double) measure.getIntValue());
+        case NO_VALUE:
+          return Optional.absent();
+        default:
+          throw new IllegalArgumentException(String.format(""Measure of type '%s' are not supported"", measure.getValueType().name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The changes mainly involve indentation, minor code restructuring, or access modifier alterations. They don't pertain to error-handling updates, logical corrections, or other bug fix indicators. The answer is **NotBuggy**."
sonarqube,14622.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,8 +1,8 @@
-  private static String convertType(Constants.ComponentLinkType reportType) {
-    String type = typesConverter.get(reportType);
-    if (type != null) {
-      return type;
-    } else {
-      throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
-    }
-  }

+    private String convertType(Constants.ComponentLinkType reportType) {
+      String type = typesConverter.get(reportType);
+      if (type != null) {
+        return type;
+      } else {
+        throw new IllegalArgumentException(String.format(""Unsupported type %s"", reportType.name()));
+      }
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message indicates an attempt to fix ""quality flaws,"" yet the diff only changes the method's static modifier from static to an instance method. This change does not directly imply a bug fix but is more about design preference or code organization. The method logic remains the same, meaning no errors are addressed. Without further context or indication of an actual bug, the modifications do not suggest fixing an issue but adjusting the code style.

Based on the information given, the conclusion is **NotBuggy**."
sonarqube,14062.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,6 +1,6 @@
-  private static String computeRevision(@Nullable Changeset latestChange) {
-    if (latestChange == null) {
-      return null;
-    }
-    return latestChange.getRevision();
-  }

+    private String computeRevision(@Nullable Changeset latestChange) {
+      if (latestChange == null) {
+        return null;
+      }
+      return latestChange.getRevision();
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message indicates a focus on addressing quality flaws, but not specifically bugs. The change here is minimal, altering the method from `static` to an instance method by removing the `static` keyword and adjusting indentation. This does not impact logic or functionality beyond possibly changing the context in which the method is called. There is no indication of a bug correction, such as error handling, logical errors, or exception management changes directly tied to debugging. Thus, the change primarily concerns code structure rather than fixing bugs.

Based on this reasoning, the conclusion is **NotBuggy**."
sonarqube,13389.json,1c73879c6bd7bb325e7e95acfadbb8431adc39f3,"@@ -1,16 +1,16 @@
-  private static DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
-    DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
-    if (sourceRange.hasStartLine()) {
-      targetRange.setStartLine(sourceRange.getStartLine());
-    }
-    if (sourceRange.hasStartOffset()) {
-      targetRange.setStartOffset(sourceRange.getStartOffset());
-    }
-    if (sourceRange.hasEndLine()) {
-      targetRange.setEndLine(sourceRange.getEndLine());
-    }
-    if (sourceRange.hasEndOffset()) {
-      targetRange.setEndOffset(sourceRange.getEndOffset());
-    }
-    return targetRange;
-  }

+    private DbCommons.TextRange.Builder convertTextRange(BatchReport.TextRange sourceRange) {
+      DbCommons.TextRange.Builder targetRange = DbCommons.TextRange.newBuilder();
+      if (sourceRange.hasStartLine()) {
+        targetRange.setStartLine(sourceRange.getStartLine());
+      }
+      if (sourceRange.hasStartOffset()) {
+        targetRange.setStartOffset(sourceRange.getStartOffset());
+      }
+      if (sourceRange.hasEndLine()) {
+        targetRange.setEndLine(sourceRange.getEndLine());
+      }
+      if (sourceRange.hasEndOffset()) {
+        targetRange.setEndOffset(sourceRange.getEndOffset());
+      }
+      return targetRange;
+    }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message indicates ""Fix quality flaws,"" which typically refers to code quality improvements rather than bug fixes. The diff shows a change in the visibility of the `convertTextRange` method from `private static` to just `private`. This alteration affects the method's accessibility, but it doesn't address logic, error handling, or bug-related issues in the implementation of the method itself.

The method's logic remains the same; it still converts a `BatchReport.TextRange` to a `DbCommons.TextRange.Builder` based on available fields. There are no changes to its logic or to any exception handling that would indicate a bug fix. 

Given that the nature of the change is more related to code structure and visibility (potentially for maintainability or encapsulation reasons) rather than fixing any misbehavior or bug in the logic, the commit message and the change align with quality improvements, not bug fixes.

Therefore, the conclusion is **NotBuggy**."
sonarqube,1136.json,371ab065edbef2e92b502878eec9a73d310af54b,"@@ -1,3 +1,3 @@
     public Reader getReader() {
-      return new StringReader(source_code);
+      return new StringReader(sourceCode);
     }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message mentions fixing ""quality flaws,"" while the provided code diff shows a change in variable naming from `source_code` to `sourceCode`. This update adheres to typical Java naming conventions (from snake_case to camelCase), often considered a code quality improvement, but it does not suggest any functional change or bug fix. There is no evident error correction, exception handling enhancements, or logical fixes present in the code change.

Given that the change is purely related to naming conventions and does not address functionality, this commit does not substantiate a bug fix. 

Thus, the conclusion is **NotBuggy**."
sonarqube,18140.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,7 +1,7 @@
   public String toString() {
     return Objects.toStringHelper(DebtRemediationFunction.class)
       .add(""type"", type)
-      .add(""coefficient"", factor)
+      .add(""coefficient"", coefficient)
       .add(""offset"", offset)
       .toString();
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"The change in the diff corrects the field name in a `toString()` method from `factor` to `coefficient`. This likely reflects an issue where the wrong object's field was being used, which could lead to incorrect string representations and possibly misleading logs or debugging information. This type of correction aligns with improving code quality and ensuring that functionality such as logging or debugging outputs the correct information about the object's state.

However, based on the commit message ""Fix quality flaws,"" it is ambiguous whether the change was intended to address a functionality issue (i.e., a bug) or merely to enhance code quality by correcting the name for clarity. The message does not specifically mention fixing a bug related to functionality or behavior, but instead refers to addressing ""quality flaws,"" which can often imply refactoring or cleanup tasks that don't necessarily correlate to bug fixes.

Considering these aspects, this change does improve the correctness of the output of the `toString()` method, which is important, but there's insufficient evidence from the commit message to definitively conclude that this change addresses a functional bug in the execution of the program.

Concluding step-by-step:
1. The diff corrects the field in the `toString()` method.
2. The change leads to the correct representation of object's state.
3. The commit message refers to fixing ""quality flaws"" without mentioning a specific bug.
4. Insufficient evidence to determine the change addresses a functional bug.

Given the primarily corrective nature of this change related to code quality and accurate state representation without direct indication of a functional error or mishap in the process, the conclusion would be:

**NotBuggy**"
sonarqube,18137.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,21 +1,21 @@
   private void validate() {
     switch (type) {
       case LINEAR:
-        if (this.factor == null || this.offset != null) {
+        if (this.coefficient == null || this.offset != null) {
           throw new IllegalArgumentException(String.format(""Only coefficient must be set on %s"", this));
         }
         break;
       case LINEAR_OFFSET:
-        if (this.factor == null || this.offset == null) {
+        if (this.coefficient == null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Both coefficient and offset are required on %s"", this));
         }
         break;
       case CONSTANT_ISSUE:
-        if (this.factor != null || this.offset == null) {
+        if (this.coefficient != null || this.offset == null) {
           throw new IllegalArgumentException(String.format(""Only offset must be set on %s"", this));
         }
         break;
       default:
         throw new IllegalArgumentException(String.format(""Unknown type on %s"", this));
     }
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message refers to fixing quality flaws, and the change involves renaming a variable from `factor` to `coefficient`. This adjustment appears to be for naming consistency and does not affect functionality or fix a logic error. Since there are no changes indicating a correction of functionality (such as fixing logic or handling exceptions), it does not seem to address any particular bug.

Thus, the change is more of a refactor for improved readability and maintainability rather than a bug fix.

The appropriate conclusion is **NotBuggy**."
sonarqube,18133.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,3 +1,3 @@
   public String coefficient() {
-    return factor;
+    return coefficient;
   }",NotBuggy,"Fix quality flaws
",Buggy,"The commit message ""Fix quality flaws"" is generic and doesn't provide specific information on bug fixes. Looking at the code change, it switches a method's return value from a variable `factor` to `coefficient`. This could indicate a bug fix if `factor` was incorrect and `coefficient` is the right variable intended to be returned, especially if the method's purpose aligns more with the name `coefficient`. However, without additional context on what `factor` and `coefficient` represent, it's challenging to definitively classify this change as a bug fix. If the swap corrects functionality based on method naming or expected behavior, it would be a bug fix. 

Thus, based on the assumption that the change corrects the method's functionality to align with its name or intended output, I conclude this is likely a bug fix. Hence, the answer is **Buggy**."
sonarqube,18138.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,16 +1,14 @@
   public boolean equals(Object o) {
+    if (!(o instanceof DefaultDebtRemediationFunction)) {
+      return false;
+    }
     if (this == o) {
       return true;
     }
-    if (o == null || getClass() != o.getClass()) {
-      return false;
-    }
-    DefaultDebtRemediationFunction that = (DefaultDebtRemediationFunction) o;
-    if (factor != null ? !factor.equals(that.factor) : that.factor != null) {
-      return false;
-    }
-    if (offset != null ? !offset.equals(that.offset) : that.offset != null) {
-      return false;
-    }
-    return type == that.type;
+    DefaultDebtRemediationFunction other = (DefaultDebtRemediationFunction) o;
+    return new EqualsBuilder()
+      .append(coefficient, other.coefficient())
+      .append(offset, other.offset())
+      .append(type, other.type())
+      .isEquals();
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message states ""Fix quality flaws,"" implying a focus on code quality improvements rather than bug fixes. Analyzing the code changes:

1. **equals Method Improvement**: 
   - The new implementation uses the EqualsBuilder utility from Apache Commons Lang, which is known for improving code readability and maintainability. 
   - The modifications help simplify the logic used to compare object fields, while ensuring the equals method structure adheres to common practices.

2. **Logic Handling**:
   - The earlier code checks both the type of the other object and explicitly compares fields, potentially leading to verbose and error-prone code.
   - The new implementation performs an `instanceof` check directly and compares fields using `EqualsBuilder`, focusing on readability and reducing manual checks.

In conclusion, while the changes improve the quality and readability of the code, they fall under code refactor rather than addressing a specific bug. The original code might have been working correctly but was less maintainable. The enhancements target code style and maintainability rather than fixing issues. Therefore, conclude with **NotBuggy**."
sonarqube,18139.json,4a108310e196bcff760bc81de326346b1ee4ac4c,"@@ -1,6 +1,6 @@
   public int hashCode() {
     int result = type.hashCode();
-    result = 31 * result + (factor != null ? factor.hashCode() : 0);
+    result = 31 * result + (coefficient != null ? coefficient.hashCode() : 0);
     result = 31 * result + (offset != null ? offset.hashCode() : 0);
     return result;
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message indicates it is intended to fix ""quality flaws,"" which often refers to improvements in code readability, maintainability, or design rather than addressing functional or logical bugs. 

Analyzing the diff:

1. **Code Modification:**
   - The modification changes `factor` to `coefficient` in the calculation of the `hashCode`.
   - This seems to reflect a renaming in the code for consistency or clarity, rather than a functional change. 

2. **Assessment:**
   - The change does not reflect a bug fix in terms of correcting an erroneous behavior or logical error. 
   - The modification seems purely semantic, possibly aligning variable names with updated terminology or improving readability.

Given the nature of this change and the vague commit message, the modification appears to be more about improving code quality through better naming rather than fixing a bug. 

Thus, the result is **NotBuggy**."
sonarqube,10480.json,d20f21e4cd409de904ef4a0ae5926b61706b402e,"@@ -1,4 +1,4 @@
-  public QualityProfileDto setParent(String parent) {
+  public QualityProfileDto setParent(@Nullable String parent) {
     this.parent = parent;
     return this;
   }",NotBuggy,"Fix quality flaws
",NotBuggy,"The commit message states ""Fix quality flaws,"" and the change involves adding a `@Nullable` annotation to a method parameter. This indicates improved code quality by clarifying that `null` is an acceptable value for the `parent` parameter. This change aids in better nullability checks and clearer API usage but does not fix a runtime bug or logical error. It rather enhances code quality by specifying the intended use of the method parameter.

Given that the changes focus on improving code clarity and do not address a functional bug, the conclusion is **NotBuggy**."
sonarqube,19951.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,12 +1,12 @@
   private void autodetection() {
-    for (ScmProvider provider : providerPerKey.values()) {
-      if (provider.supports(projectReactor.getRoot().getBaseDir())) {
+    for (ScmProvider installedProvider : providerPerKey.values()) {
+      if (installedProvider.supports(projectReactor.getRoot().getBaseDir())) {
         if (this.provider == null) {
-          this.provider = provider;
+          this.provider = installedProvider;
         } else {
-          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + provider.key()
+          throw new IllegalStateException(""SCM provider autodetection failed. Both "" + this.provider.key() + "" and "" + installedProvider.key()
             + "" claim to support this project. Please use "" + CoreProperties.SCM_PROVIDER_KEY + "" to define SCM of your project."");
         }
       }
     }
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The commit message, ""Fix some quality flaws,"" is vague, but based on the changes in the diff, there are no apparent bug fixes being performed. Instead, the code modification focuses on naming variables more descriptively, enhancing code readability and maintainability. The variable `provider` is renamed to `installedProvider`, which improves clarity but does not address logic, error handling, or exception cases.

The diff suggests that there is no direct correction of program logic or behavior errors. The changes are purely cosmetic and aimed at improving code quality.

Therefore, given the available information and context, the changes pertain to code quality improvements, not fixing bugs. The conclusion is **NotBuggy**."
sonarqube,19956.json,d102a8a9916bac0ab600cd5e90ba5359766f8d6d,"@@ -1,3 +1,3 @@
   public void stop() {
-
+    // Nothing to do
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"This commit modifies a method by adding a comment that says ""Nothing to do"" to what appears to be an empty method body. The change seems to be addressing a clarity or quality issue by documenting the intentional emptiness of the method. This is more likely a code quality improvement rather than a bug fix. The commit message also refers to fixing ""quality flaws,"" supporting the view that this change is about improving code readability or maintainability rather than fixing a bug.

The modification does not change the logic or behavior, nor does it address an error-handling mechanism, logical error, or exception handling, which are typical indicators of a bug fix.

**NotBuggy**"
sonarqube,4891.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,28 +1,30 @@
   public void handle(Request request, Response response) throws Exception {
     int pageSize = request.mandatoryParamAsInt(Param.PAGE_SIZE);
     int page = request.mandatoryParamAsInt(Param.PAGE);
     String queryString = request.param(Param.TEXT_QUERY);
     String selected = request.mandatoryParam(Param.SELECTED);
 
     try (DbSession dbSession = dbClient.openSession(false)) {
       GroupId group = support.findGroup(dbSession, request);
       userSession.checkPermission(OrganizationPermission.ADMINISTER, group.getOrganizationUuid());
 
       UserMembershipQuery query = UserMembershipQuery.builder()
         .groupId(group.getId())
         .organizationUuid(group.getOrganizationUuid())
         .memberSearch(queryString)
         .membership(getMembership(selected))
         .pageIndex(page)
         .pageSize(pageSize)
         .build();
       int total = dbClient.groupMembershipDao().countMembers(dbSession, query);
       Paging paging = forPageIndex(page).withPageSize(pageSize).andTotal(total);
       List<UserMembershipDto> users = dbClient.groupMembershipDao().selectMembers(dbSession, query, paging.offset(), paging.pageSize());
 
-      JsonWriter json = response.newJsonWriter().beginObject();
-      writeMembers(json, users);
-      writePaging(json, paging);
-      json.endObject().close();
+      try (JsonWriter json = response.newJsonWriter()) {
+        json.beginObject();
+        writeMembers(json, users);
+        writePaging(json, paging);
+        json.endObject();
+      }
     }
   }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The commit message mentions fixing ""Quality flaws,"" but it doesn't specify that a bug was addressed. Let's analyze the diff:

1. **Resource Management Improvement**: 
   - The original code uses `JsonWriter` with `beginObject`, `writeMembers`, `writePaging`, `endObject`, and `close` method calls sequentially. 
   - The updated code uses a try-with-resources statement for `JsonWriter`, ensuring it is properly closed after use, even if an exception occurs. 

This change focuses on enhancing resource management and follows best practices, which are typically associated with improving code quality and maintainability. While this change prevents resource leaks and aligns with improved quality practices, it does not explicitly fix a bug related to functionality or logic errors.

Given the assessment, the change is a quality improvement rather than a direct bug fix. Hence, the conclusion is **NotBuggy**."
sonarqube,2559.json,289be70bf6056b5f3c66a797ecab019891e731d6,"@@ -1,7 +1,9 @@
   static void writeResponse(BulkChangeResult result, Response response) {
-    JsonWriter json = response.newJsonWriter().beginObject();
-    json.prop(""succeeded"", result.countSucceeded());
-    json.prop(""failed"", result.countFailed());
-    WebServiceEngine.writeErrors(json, result.getErrors());
-    json.endObject().close();
+    try (JsonWriter json = response.newJsonWriter()) {
+      json.beginObject();
+      json.prop(""succeeded"", result.countSucceeded());
+      json.prop(""failed"", result.countFailed());
+      WebServiceEngine.writeErrors(json, result.getErrors());
+      json.endObject();
+    }
   }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The diff improves resource management in the `writeResponse` method by using a try-with-resources block for `JsonWriter`. This ensures `JsonWriter` is properly closed, even if an exception is thrown, which resolves potential resource leak issues. However, the commit message ""Fix Quality flaws"" is broad and doesn't clearly emphasize a specific bug. It focuses on enhancing code quality through better resource management.

While this improvement can prevent issues (like resource leaks), it doesn't necessarily indicate a direct bug fix in existing functionality or logic present in the program. Given the generalized commit message and the nature of the improvement, the changes are more aligned with quality improvements rather than fixing a previously existing bug.

Conclusion: **NotBuggy**"
sonarqube,18845.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,3 +1,3 @@
-  public void resolveOrder() throws CyclicDependenciesException {
+  public void resolveOrder() {
     resolveOrder(toString());
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The given change involves removing the `throws CyclicDependenciesException` declaration from the `resolveOrder` method. The commit message suggests fixing ""quality flaws,"" but let's analyze whether this change indicates a bug fix:

1. **Understanding the Change**: The original method signature indicates that `resolveOrder` might throw a `CyclicDependenciesException`. Removing the `throws` clause suggests either because:
   - The exception is no longer expected to be thrown.
   - The handling of such an exception was moved internally, thus the method callers aren't expected to handle it anymore.

2. **Implication of the Change**:
   - **Error-handling Changes**: If the exception is genuinely not thrown anymore due to logical corrections elsewhere, it's an improvement rather than strictly a bug fix.
   - **Robustness**: By handling such exceptions internally rather than passing them up to the caller, the method might become more predictable and robust.

3. **Bug Fix Analysis**:
   - Removing the exception declaration might suggest an improvement in the way errors are managed but it is not clear from the diff whether this addresses a bug or simply an improvement in code quality (maintainability/readability).
   - Without information on changes external to this diff, such as changes ensuring exceptions are internally managed, it's unclear if a logical issue affecting functionality in production was fixed.

Based solely on the code diff provided and its message, we conclude: **NotBuggy**. The change is more aligned with improving code quality or readability rather than explicitly fixing a bug."
sonarqube,18846.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,19 +1,19 @@
-  private int resolveOrder(String path) throws CyclicDependenciesException {
+  private int resolveOrder(String path) {
     seen = true;
     try {
       int highOrder = -1;
       for (Node dep : dependencies) {
         if (dep.seen) {
           throw new CyclicDependenciesException(path + "" -> "" + dep.toString());
         }
         highOrder = Math.max(highOrder, dep.resolveOrder(path + "" -> "" + dep.toString()));
 
       }
 
       order = highOrder + 1;
       return order;
 
     } finally {
       seen = false;
     }
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The update removes the `throws CyclicDependenciesException` declaration from the `resolveOrder` method signature, suggesting that the exception must still be handled, given it hasn’t been removed from the code block itself. Although the commit message states ""Fix some quality flaws,"" the diff does not directly indicate a bug fix as the primary focus. This change doesn't alter how the exception is generated or handled; it only cleans up the method signature. There's no indication of altered logic or error correction specific to bugs. Thus, the diff aligns more with code quality improvement rather than a bug fix.

Therefore, the changes do not straightforwardly reflect a bug fix based on the provided information.

The conclusion is **NotBuggy**."
sonarqube,8517.json,38e53c0572db4e267a704666cb0d1343992df5e1,"@@ -1,35 +1,33 @@
   public static Collection<String> listResources(ClassLoader classLoader, String rootPath, Predicate<String> predicate) {
     try {
       Collection<String> paths = Lists.newArrayList();
       rootPath = StringUtils.removeStart(rootPath, ""/"");
 
       URL root = classLoader.getResource(rootPath);
       if (root != null) {
-        if (!""jar"".equals(root.getProtocol())) {
-          throw new IllegalStateException(""Unsupported protocol: "" + root.getProtocol());
-        }
+        checkJarFile(root);
 
         // Path of the root directory
         // Examples :
         // org/sonar/sqale/index.txt  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale/  -> rootDirectory is org/sonar/sqale
         // org/sonar/sqale  -> rootDirectory is org/sonar/sqale
         String rootDirectory = rootPath;
         if (StringUtils.substringAfterLast(rootPath, ""/"").indexOf('.') >= 0) {
           rootDirectory = StringUtils.substringBeforeLast(rootPath, ""/"");
         }
         String jarPath = root.getPath().substring(5, root.getPath().indexOf(""!"")); //strip out only the JAR file
         JarFile jar = new JarFile(URLDecoder.decode(jarPath, CharEncoding.UTF_8));
         Enumeration<JarEntry> entries = jar.entries();
         while (entries.hasMoreElements()) {
           String name = entries.nextElement().getName();
           if (name.startsWith(rootDirectory) && predicate.apply(name)) {
             paths.add(name);
           }
         }
       }
       return paths;
     } catch (Exception e) {
       throw Throwables.propagate(e);
     }
   }",NotBuggy,"Fix some quality flaws
",NotBuggy,"The changes in the provided diff primarily involve refactoring existing code for improved quality and potentially better organization. Specifically, the check for the protocol ""jar"" is extracted into a separate method, `checkJarFile(root)`, simplifying the `listResources` method. This does not directly address any logical errors or bugs, nor does it change the functionality of the code. Instead, it suggests a cleaner, possibly more maintainable approach by isolating one of the checks into its own method.

The commit message mentions fixing ""some quality flaws,"" which aligns with the refactoring seen in the diff but does not point to any specific error handling, logical corrections, or exceptional cases being improved—typical indicators of bug fixes.

Therefore, based on the content and the scope of changes in the diff, the conclusion is **NotBuggy**."
sonarqube,7268.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,14 +1,14 @@
   private UserDto synchronize(String userLogin, UserDetails details, HttpServletRequest request) {
     String name = details.getName();
     UserIdentity.Builder userIdentityBuilder = UserIdentity.builder()
       .setLogin(userLogin)
-      .setName(isNullOrEmpty(name) ? userLogin : name)
+      .setName(isEmpty(name) ? userLogin : name)
       .setEmail(trimToNull(details.getEmail()))
       .setProviderLogin(userLogin);
     if (externalGroupsProvider != null) {
       ExternalGroupsProvider.Context context = new ExternalGroupsProvider.Context(userLogin, request);
       Collection<String> groups = externalGroupsProvider.doGetGroups(context);
       userIdentityBuilder.setGroups(new HashSet<>(groups));
     }
     return userIdentityAuthenticator.authenticate(userIdentityBuilder.build(), new ExternalIdentityProvider());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",NotBuggy,"The given diff replaces `isNullOrEmpty(name)` with `isEmpty(name)`. This suggests a change in how the `name` variable is checked, assuming both methods are available utility methods. 

### Analyzing the commit message:
- The message primarily talks about ""Quality flaws"" focused on missing tests for Protobuf and incorrect imports. It does not directly mention any logic changes related to the `synchronize` method.

### Analyzing the code change:
- If `isNullOrEmpty` checks for both null and empty strings, and `isEmpty` checks only for emptiness, then this change may introduce a bug by ignoring null cases unless other code ensures `name` is non-null.
- If `name` is guaranteed non-null (via other logic not shown), using `isEmpty` wouldn't change functionality but might indicate refactoring.
- The commit message talks about missing tests and import issues but does not specify fixing this method's logic.

The code change doesn't seem to introduce improvement in logic or fix a clear bug related to functionality. The change might be addressing a quality flaw if the null case is handled elsewhere or by convention, though this is not confirmed by the commit message or diff.

Given this analysis, and since this change doesn't clearly align with explicit bug fixes related to logic errors, I conclude this change itself doesn't indicate a bug fix. Therefore, the appropriate conclusion based on the provided information is **NotBuggy**."
sonarqube,7206.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,12 +1,12 @@
   private static Optional<String> getTokenFromCookie(HttpServletRequest request) {
     Optional<Cookie> jwtCookie = findCookie(JWT_COOKIE, request);
     if (!jwtCookie.isPresent()) {
       return Optional.empty();
     }
     Cookie cookie = jwtCookie.get();
     String token = cookie.getValue();
-    if (isNullOrEmpty(token)) {
+    if (isEmpty(token)) {
       return Optional.empty();
     }
     return Optional.of(token);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",NotBuggy,"The commit message refers to quality improvements rather than specific bug fixes. The code change replaces `isNullOrEmpty(token)` with `isEmpty(token)`. This suggests an evolution in logic, as `isEmpty` likely assumes non-null input, possibly improving performance or clarity but not clearly resolving an apparent fault. Without indications of previously mishandled `null` tokens (a bug), this change aligns with a quality improvement for code consistency or style. Thus, the analysis points to a refactor or quality upgrades sans explicit bug rectification.

Verdict: **NotBuggy**."
sonarqube,3674.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,7 +1,7 @@
   private List<Setting> loadDefaultSettings(Set<String> keys) {
     return propertyDefinitions.getAll().stream()
       .filter(definition -> keys.contains(definition.key()))
-      .filter(defaultProperty -> !isNullOrEmpty(defaultProperty.defaultValue()))
+      .filter(defaultProperty -> !isEmpty(defaultProperty.defaultValue()))
       .map(Setting::createForDefinition)
       .collect(Collectors.toList());
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",NotBuggy,"The commit message speaks to quality issues and doesn't specify fixes for bugs. The diff shows a method change from `isNullOrEmpty` to `isEmpty`. This could improve code clarity or consistency but is unlikely related to a bug fix since it wouldn't change behavior if `null` is already not a possibility. The commit message suggests improvements in test coverage and imports unrelated to this diff. The change seems more like a code quality improvement, aligning with the commit description. Therefore, the answer is **NotBuggy**."
sonarqube,4743.json,fd78d281d57d7b1ffa49d27cd03de39b6a961592,"@@ -1,8 +1,8 @@
   private UserDto authenticate(HttpServletRequest request) {
     String login = request.getParameter(""login"");
     String password = request.getParameter(""password"");
-    if (isNullOrEmpty(login) || isNullOrEmpty(password)) {
+    if (isEmpty(login) || isEmpty(password)) {
       throw new UnauthorizedException();
     }
     return credentialsAuthenticator.authenticate(login, password, request);
   }",NotBuggy,"Fix Quality flaws

- missing tests for Protobuf
- remove incorrect imports of org.elasticsearch.common.Strings
",NotBuggy,"The commit message states it aims to fix quality flaws related to missing tests for Protobuf and incorrect imports of org.elasticsearch.common.Strings. The code change involves replacing `isNullOrEmpty` with `isEmpty` in an authentication check. This update does not directly relate to the commit message's points on testing or import cleanup.

Analyzing the code change:
- The replacement suggests a difference in how `isEmpty` and `isNullOrEmpty` behave. If `isEmpty` checks non-emptiness and does not consider nulls (assuming inputs are non-null), it could be part of improving code quality or efficiency. However, without more context on these methods, it's challenging to relate directly to bug fixing.

Given the analysis:
- No clear bug fix can be inferred because the commit focuses on quality improvements (testing and imports).
- The specific code change improves or optimizes quality, potentially but not explicitly bug-related.

Thus, this change focuses on quality improvement without direct evidence of fixing a bug. Therefore, the assessment is **NotBuggy**."
sonarqube,20010.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,9 +1,9 @@
   private static void cleanTempFolders(Path path) throws IOException {
-    if (Files.exists(path)) {
+    if (path.toFile().exists()) {
       try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, new CleanFilter())) {
         for (Path p : stream) {
           deleteQuietly(p.toFile());
         }
       }
     }
   }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The commit message indicates a fix for quality flaws. However, the change modifies the method of checking file existence—switching from `Files.exists(path)` to `path.toFile().exists()`. While this could potentially be for performance or code style reasons, it doesn't directly address a bug or improve code quality except possibly in very context-specific scenarios where one method is preferable due to filesystem nuances.

There is no evidence here of fixing an error-handling issue, a logical error, or an exception-handling improvement. The change could even introduce a subtle difference in behavior depending on the underlying file system, as `Files.exists()` operates with the NIO.2 API, which provides better support for symbolic links and more accurate file-type recognition than `toFile().exists()`.

Thus, without further information, this appears more to be a stylistic adjustment rather than a fix for a specific defect. 

The conclusion is **NotBuggy**."
sonarqube,20011.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,24 +1,24 @@
     public boolean accept(Path path) throws IOException {
-      if (!Files.isDirectory(path)) {
+      if (!path.toFile().exists()) {
         return false;
       }
 
       if (!path.getFileName().toString().startsWith(TMP_NAME_PREFIX)) {
         return false;
       }
 
       long threshold = System.currentTimeMillis() - CLEAN_MAX_AGE;
 
       // we could also check the timestamp in the name, instead
       BasicFileAttributes attrs;
 
       try {
         attrs = Files.readAttributes(path, BasicFileAttributes.class);
       } catch (IOException ioe) {
         LOG.error(String.format(""Couldn't read file attributes for %s : "", path), ioe);
         return false;
       }
 
       long creationTime = attrs.creationTime().toMillis();
       return creationTime < threshold;
     }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The changes in this diff involve modifying the condition for checking if a file should be accepted. Originally, the method used `Files.isDirectory(path)` to determine if the path is a directory. This was replaced with `!path.toFile().exists()`. Here's the step-by-step analysis:

1. **Original Condition**: 
   - `if (!Files.isDirectory(path))`: This checks if the path is **not** a directory. Only directories were accepted in subsequent checks.
   
2. **Modified Condition**:
   - `if (!path.toFile().exists())`: This checks if the path **does not exist**. No longer concerned with directories specifically, it accepts paths that exist, regardless of whether they're files or directories.

3. **Potential Motivations for Change**:
   - The change might have been intended to include paths that are files, not just directories. The broader condition (`toFile().exists()`) is likely meant to address certain ""quality"" or usability concerns, allowing for cases when paths are not directories but are relevant to whatever processing follows.

4. **Assessing ""Quality Flaws"" Fix**:
   - The changes in logic adjust what kinds of paths qualify for further processing. This could be considered a fix, but not in terms of a ""bug"" related to malfunction or error. Rather, it fits a reclassification of what should logically pass as an acceptable path.

5. **Conclusion**:
   - Although the change could correct behavior from a usability perspective (perhaps aligning with new functional requirements or clarified specifications), it's not a correction of traditional bugs like exceptions or functional mistakes in prior logic.

Given the reasoning above, the purpose of the changes does not indicate a bug fix in the typical sense of resolving an erroneous or crashing behavior. It's more of an evolutionary change or a functional enhancement. The commit message ""Fix Quality flaws"" appropriately reflects this motivation, which focuses more on non-defective improvements.

Therefore, the analysis concludes with **NotBuggy**."
sonarqube,8900.json,5e0b742ac8299c7ddcd2bf3dfe682482c4d8eda4,"@@ -1,7 +1,7 @@
-  public static void main(String[] args) throws Exception {
+  public static void main(String[] args) {
     ProcessEntryPoint entryPoint = ProcessEntryPoint.createForArguments(args);
     Props props = entryPoint.getProps();
     new WebServerProcessLogging().configure(props);
     WebServer server = new WebServer(props);
     entryPoint.launch(server);
   }",NotBuggy,"Fix Quality flaws
",NotBuggy,"The change in the diff involves removing the `throws Exception` declaration from the `main` method signature. This modification relates more to improving code quality, potentially by handling exceptions explicitly within the method, rather than propagating them up the stack. This kind of change typically indicates an effort to improve code robustness and maintainability by ensuring any necessary exception handling is managed within the method.

The commit message ""Fix Quality flaws"" aligns with the observed change because removing the broad `throws Exception` clause forces better error handling practices, which is indeed a quality improvement.

However, this change does not indicate a bug fix; it reflects a shift toward better coding standards or conventions. Since there is no indication of a logical error correction, error-handling improvement, or exception handling directly influencing the application's functionality in a bug-related context, this commit does not constitute a bug fix.

Thus, the correct conclusion is **NotBuggy**."
sonarqube,4469.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,15 +1,15 @@
   public void handle(Request wsRequest, Response wsResponse) throws Exception {
     try (DbSession dbSession = dbClient.openSession(false)) {
       WsTemplateRef templateRef = WsTemplateRef.fromRequest(wsRequest);
       PermissionTemplateDto template = support.findTemplate(dbSession, templateRef);
       checkGlobalAdmin(userSession, template.getOrganizationUuid());
 
       PermissionQuery query = buildPermissionQuery(wsRequest);
-      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getId());
+      int total = dbClient.permissionTemplateDao().countGroupNamesByQueryAndTemplate(dbSession, query, template.getOrganizationUuid(), template.getId());
       Paging paging = Paging.forPageIndex(wsRequest.mandatoryParamAsInt(PAGE)).withPageSize(wsRequest.mandatoryParamAsInt(PAGE_SIZE)).andTotal(total);
       List<GroupDto> groups = findGroups(dbSession, query, template);
       List<PermissionTemplateGroupDto> groupPermissions = findGroupPermissions(dbSession, groups, template);
       WsPermissions.WsGroupsResponse groupsResponse = buildResponse(groups, groupPermissions, paging);
       writeProtobuf(groupsResponse, wsRequest, wsResponse);
     }
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message states that it fixes an error that occurs when filtering by name and a group name appears in multiple organizations. The code change adds `template.getOrganizationUuid()` as an extra parameter in the `countGroupNamesByQueryAndTemplate` method call. This addition likely empowers the DAO to filter results by organization, preventing miscounts of group names across organizations. Thus, it directly addresses the described issue in the commit message. The changes appear to fix a bug as outlined in the commit message, where group names were not correctly delineated by organization, leading to errors in total calculations. The answer is **Buggy**."
sonarqube,12672.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).selectGroupNamesByQueryAndTemplate(query, templateId, new RowBounds(query.getPageOffset(), query.getPageSize()));
+  public List<String> selectGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).selectGroupNamesByQueryAndTemplate(organizationUuid, templateId, query, new RowBounds(query.getPageOffset(), query.getPageSize()));
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error occurring when filtering by name in a context where a group name might exist in multiple organizations. The diff shows an additional parameter (`String organizationUuid`) being added to the `selectGroupNamesByQueryAndTemplate` method. This change allows the method to consider the organization context when querying group names, which effectively resolves the issue of potential ambiguity or error when the same group name exists in different organizations.

The modification aligns with the commit message by addressing the described error scenario through enhanced filtering criteria. This indicates a logical change to fix an error rather than just restructuring or refactoring code.

Therefore, the changes reflect a bug fix as described by the commit message. The answer is **Buggy**."
sonarqube,4472.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,8 +1,8 @@
   private List<GroupDto> findGroups(DbSession dbSession, PermissionQuery dbQuery, PermissionTemplateDto template) {
-    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getId());
+    List<String> orderedNames = dbClient.permissionTemplateDao().selectGroupNamesByQueryAndTemplate(dbSession, dbQuery, template.getOrganizationUuid(), template.getId());
     List<GroupDto> groups = dbClient.groupDao().selectByNames(dbSession, template.getOrganizationUuid(), orderedNames);
     if (orderedNames.contains(DefaultGroups.ANYONE)) {
       groups.add(0, new GroupDto().setId(0L).setName(DefaultGroups.ANYONE));
     }
     return Ordering.explicit(orderedNames).onResultOf(GroupDto::getName).immutableSortedCopy(groups);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a bug fix related to filtering by name when a group name exists in multiple organizations. The code change adds `template.getOrganizationUuid()` to a method call, ensuring the group names are filtered within the correct organization context. This change aligns with the commit message and addresses the specified bug scenario by preventing errors caused by ambiguities in group names across different organizations. The update also likely enhances the correctness of the group name selection process in the database.

Based on the analysis, this change directly addresses an error condition described in the commit message, aligning well with a bug fix.

**Buggy**"
sonarqube,12673.json,0584dc8d5d5d82a2035763c34afba0e12cf6e8d0,"@@ -1,3 +1,3 @@
-  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, long templateId) {
-    return mapper(session).countGroupNamesByQueryAndTemplate(query, templateId);
+  public int countGroupNamesByQueryAndTemplate(DbSession session, PermissionQuery query, String organizationUuid, long templateId) {
+    return mapper(session).countGroupNamesByQueryAndTemplate(organizationUuid, query, templateId);
   }",Buggy,"SONAR-8682 fix error when filtering on name

and a given group name exists in multiple organizations
",Buggy,"The commit message indicates a fix for an error related to filtering on name when a group name exists in multiple organizations. The code change reflects this: it adds an `organizationUuid` parameter to the method `countGroupNamesByQueryAndTemplate`. This change suggests that the previous implementation could not distinguish between groups with the same name across different organizations, likely causing incorrect counts or data retrieval. By incorporating `organizationUuid`, the method can now specifically target group names within a single organization, resolving such issues.

Given this analysis, the code alteration aligns with addressing the potential error described in the commit message. Therefore, the conclusion is **Buggy**."
voldemort,9608.json,4a463214895e56db53db78064bde3ebd10b6e949,"@@ -1,8 +1,8 @@
-    public void recordOpTimeUs(SocketDestination dest, long opTimeUs) {
+    public void recordOpTimeNs(SocketDestination dest, long opTimeNs) {
         if (dest != null) {
-            getOrCreateNodeStats(dest).recordOpTimeUs(null, opTimeUs);
-            recordOpTimeUs(null, opTimeUs);
+            getOrCreateNodeStats(dest).recordOpTimeNs(null, opTimeNs);
+            recordOpTimeNs(null, opTimeNs);
         } else {
-            this.opTimeRequestCounter.addRequest(opTimeUs * Time.NS_PER_US);
+            this.opTimeRequestCounter.addRequest(opTimeNs);
         }
     }",Buggy,"Fix time convertion (Nano to micro) bug

Make the method accept nanoseconds to avoid unnecessary
convertion.
",Buggy,"The commit message indicates a bug fix related to time conversion. The change in the code refactors a method to accept time in nanoseconds rather than microseconds, eliminating unnecessary conversion steps. The method name change from `recordOpTimeUs` to `recordOpTimeNs` reflects this adjustment.

1. **Method Signature Change**: The method now takes `opTimeNs` as a parameter, suggesting it operates in nanoseconds directly instead of converting from microseconds.
   
2. **Internal Calls**: Calls to `recordOpTimeUs` are updated to `recordOpTimeNs`, maintaining consistency with the new time unit.

3. **Conversion Elimination**: The line `this.opTimeRequestCounter.addRequest(opTimeUs * Time.NS_PER_US)` is now simply `this.opTimeRequestCounter.addRequest(opTimeNs)`, indicating that the conversion from microseconds to nanoseconds is no longer necessary, streamlining operations.

The commit message and the corresponding code changes clearly align, correcting a time conversion mistake, thus fixing a bug related to time unit handling.

In conclusion, the changes made in the commit clearly indicate a bug fix for time conversion, supporting the commit message accurately. Therefore, the analysis suggests that this change is **Buggy**."
voldemort,8659.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,15 +1,15 @@
         void streamingPut(ByteArray key, Versioned<byte[]> value) {
-            if(overwrite == false) {
-                dstStreamingClient.streamingPut(key, value);
-            } else {
+            if(overwrite) {
                 VectorClock denseClock = VectorClockUtils.makeClockWithCurrentTime(dstServerIds);
                 Versioned<byte[]> updatedValue = new Versioned<byte[]>(value.getValue(), denseClock);
                 dstStreamingClient.streamingPut(key, updatedValue);
+            } else {
+                dstStreamingClient.streamingPut(key, value);
             }
 
             entriesForkLifted++;
             if(entriesForkLifted % progressOps == 0) {
                 logger.info(workName + "" fork lifted "" + entriesForkLifted
                             + "" entries successfully"");
             }
         }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",NotBuggy,"The commit message states: ""Incorporate code review comments"", ""Fix error message"", and ""Change the order of if/else"". The code changes swap the branches of an `if/else` statement, but do not introduce new logic or fix previously incorrect behavior. Instead, the revised code now checks `if(overwrite)` as a condition to execute the logic to make a new `VectorClock`. This inversion is more idiomatic and likely fulfills a style or readability concern from a code review rather than addressing a bug.

There is no evidence of a fixed error message within this diff, so the code review comments mentioned might have also touched upon code style, readability, and maintainability rather than bug fixes specifically.

Given this analysis, the primary focus of the changes seems to revolve around improving code quality rather than correcting a flaw in behavior.

Concluding: **NotBuggy**."
voldemort,8669.json,da027c470c327f2e0759acae2b3b46dfa0b114b2,"@@ -1,82 +1,82 @@
     public static void main(String[] args) throws Exception {
         OptionParser parser = null;
         OptionSet options = null;
         try {
             parser = getParser();
             options = parser.parse(args);
         } catch(Exception oe) {
             logger.error(""Exception processing command line options"", oe);
             parser.printHelpOn(System.out);
             return;
         }
 
         /* validate options */
         if(options.has(""help"")) {
             parser.printHelpOn(System.out);
             return;
         }
 
         if(!options.has(""src-url"") || !options.has(""dst-url"")) {
             logger.error(""Both 'src-url' and 'dst-url' options are mandatory"");
             parser.printHelpOn(System.out);
             return;
         }
 
         String srcBootstrapUrl = (String) options.valueOf(""src-url"");
         String dstBootstrapUrl = (String) options.valueOf(""dst-url"");
         int maxPutsPerSecond = DEFAULT_MAX_PUTS_PER_SEC;
         if(options.has(""max-puts-per-second""))
             maxPutsPerSecond = (Integer) options.valueOf(""max-puts-per-second"");
         List<String> storesList = null;
         if(options.has(""stores"")) {
             storesList = new ArrayList<String>((List<String>) options.valuesOf(""stores""));
         }
         List<Integer> partitions = null;
         if(options.has(""partitions"")) {
             partitions = (List<Integer>) options.valuesOf(""partitions"");
         }
 
         int partitionParallelism = DEFAULT_PARTITION_PARALLELISM;
         if(options.has(""parallelism"")) {
             partitionParallelism = (Integer) options.valueOf(""parallelism"");
         }
         int progressOps = DEFAULT_PROGRESS_PERIOD_OPS;
         if(options.has(""progress-period-ops"")) {
             progressOps = (Integer) options.valueOf(""progress-period-ops"");
         }
 
         ForkLiftTaskMode mode;
         mode = ForkLiftTaskMode.primary_resolution;
         if(options.has(""mode"")) {
             mode = Utils.getEnumFromString(ForkLiftTaskMode.class, (String) options.valueOf(""mode""));
             if(mode == null)
                 mode = ForkLiftTaskMode.primary_resolution;
 
         }
 
         Boolean overwrite = false;
-        if(options.has(""overwrite"")) {
-            if(options.hasArgument(""overwrite"")) {
-                overwrite = (Boolean) options.valueOf(""overwrite"");
+        if(options.has(OVERWRITE_OPTION)) {
+            if(options.hasArgument(OVERWRITE_OPTION)) {
+                overwrite = (Boolean) options.valueOf(OVERWRITE_OPTION);
             } else {
                 overwrite = true;
             }
         }
 
         if(overwrite) {
             logger.warn(OVERWRITE_WARNING_MESSAGE);
         }
 
         ClusterForkLiftTool forkLiftTool = new ClusterForkLiftTool(srcBootstrapUrl,
                                                                    dstBootstrapUrl,
                                                                    overwrite,
                                                                    maxPutsPerSecond,
                                                                    partitionParallelism,
                                                                    progressOps,
                                                                    storesList,
                                                                    partitions,
                                                                    mode);
         forkLiftTool.run();
         // TODO cleanly shut down the hanging threadpool
         System.exit(0);
     }",NotBuggy,"Incorporate code review comments

Fix error message
Change the order of if/else
",NotBuggy,"The commit message identifies two key changes: fixing an error message and changing the order of an if/else statement. 

1. **Fix Error Message**: 
   The diff does not specifically show any modification to the error message itself, meaning the textual content logged or printed upon encountering an error remains unchanged in the visible diff. Therefore, this aspect of the commit might not be captured here but could suggest improvements in exception handling.

2. **Change the Order of if/else**: 
   The code changes involve replacing string literals with a constant (`OVERWRITE_OPTION`) for the ""overwrite"" option within two `if` blocks. This change does not seem to alter the logical flow of the if/else statement, nor does it affect the order of conditions. Since the change uses a constant instead of the actual string, the improvement likely focuses on code maintainability (reducing the risk of errors from typos) rather than logic.

No significant bug-fixing signs like bug-specific logical corrections, exception handling (beyond improving messages), or other changes typically associated with addressing bugs. Given these observations, the transition to constants does improve code quality but doesn't indicate a direct bug fix concerning logic or error resolution.

Thus, this commit primarily refines the code while maintaining its original functionality rather than explicitly fixing bugs. Therefore, the correct categorization of these changes is **NotBuggy**."
voldemort,9827.json,8fe6ef89e6f219a246413f918167b573f69e070c,"@@ -1,18 +1,20 @@
     public boolean delete(ByteArray key, Version version) throws VoldemortException {
         boolean deleteSuccessful = false;
         StoreUtils.assertValidKey(key);
         String keyString = new String(key.get());
         String initialValue = this.metadataMap.get(keyString);
         if(initialValue != null) {
             String removedValue = this.metadataMap.remove(keyString);
             if(removedValue != null) {
                 deleteSuccessful = (initialValue.equals(removedValue));
             }
         }
         if(deleteSuccessful) {
             this.flushData();
             // Reset the vector clock and persist it.
+            // FIXME this also needs to be done per entry, as opposed to
+            // versioning the file.
             writeVersion(new VectorClock());
         }
         return deleteSuccessful;
     }",NotBuggy,"Workaround SystemStore bug and fix quota store inserts
",NotBuggy,"The commit message indicates a workaround for a SystemStore bug and fixes related to quota store inserts. However, the provided diff does not show any direct changes to handle such issues. Instead, it adds a FIXME comment suggesting a needed change in the versioning logic, mentioning per-entry handling rather than file-wide versioning. This comment implies recognition of a potential problem but does not implement the fix.

The comment doesn't directly align with the commit message's explicit mention of a ""SystemStore bug"" workaround or anything dealing with quota store inserts. Therefore, from the diff alone, there are no visible code changes implementing a fix for the mentioned bugs.

Given that the comment suggests that a bug is identified but without actual code change to address it in this diff, the solution to the problem isn't implemented here, leading to the classification of the answer as **NotBuggy** based on this specific snippet."
voldemort,9441.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,35 +1,36 @@
     public void run() {
         Node proxyNode = metadata.getCluster().getNodeById(destinationNode);
         long startNs = System.nanoTime();
         try {
             // TODO there are no retries now if the node we want to write to is
             // unavailable
             redirectingStore.checkNodeAvailable(proxyNode);
             Store<ByteArray, byte[], byte[]> socketStore = redirectingStore.getRedirectingSocketStore(redirectingStore.getName(),
                                                                                                       destinationNode);
 
             socketStore.put(key, value, transforms);
             redirectingStore.recordSuccess(proxyNode, startNs);
+            redirectingStore.reportProxyPutSuccess();
             if(logger.isTraceEnabled()) {
                 logger.trace(""Proxy write for store "" + redirectingStore.getName() + "" key ""
-                             + ByteUtils.toBinaryString(key.get()) + "" to destinationNode:""
+                             + ByteUtils.toHexString(key.get()) + "" to destinationNode:""
                              + destinationNode);
             }
         } catch(UnreachableStoreException e) {
             redirectingStore.recordException(proxyNode, startNs, e);
             logFailedProxyPutIfNeeded(e);
         } catch(ObsoleteVersionException ove) {
             /*
              * Proxy puts can get an OVE if somehow there are two stealers for
              * the same proxy node and the other stealer's proxy put already got
              * tothe proxy node.. This will not result from online put winning,
              * since we don't issue proxy puts if the proxy node is still a
              * replica
              */
             logFailedProxyPutIfNeeded(ove);
         } catch(Exception e) {
             // Just log the key.. Not sure having values in the log is a good
             // idea.
             logFailedProxyPutIfNeeded(e);
         }
     }",NotBuggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message highlights multiple changes, but analyzing whether a bug fix is implemented requires focusing on the section mentioning the bug: ""Bug fix in proxy put stats"". The diff includes an addition to record proxy put successes with `redirectingStore.reportProxyPutSuccess();`. This line adds functionality to log successful proxy puts, improving accurate statistics for proxy operations.

The diff also includes a minor change in a log statement, specifically transforming the binary string output to a hex string (`ByteUtils.toBinaryString(key.get())` to `ByteUtils.toHexString(key.get())`). However, this change aligns more with enhancing logging clarity rather than directly addressing a bug.

Overall, the new line that reports a successful proxy put aligns with the claim of fixing a bug in proxy put statistics by ensuring successful operations are appropriately recorded. This added line ensures the tracking of successful operations, which can be considered an improvement in correctness related to stats recording. Thus, it aligns with part of the commit message focused on bug fixing.

Given these observations, the changes do imply a bug fix in the specific area of proxy put statistics. Therefore, the conclusion is **Buggy**."
voldemort,7092.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,122 +1,150 @@
     public void rebalanceStateChange(Cluster cluster,
                                      List<RebalancePartitionsInfo> rebalancePartitionsInfo,
                                      boolean swapRO,
                                      boolean changeClusterMetadata,
                                      boolean changeRebalanceState,
                                      boolean rollback) {
         Cluster currentCluster = metadataStore.getCluster();
 
         logger.info(""Server doing rebalance state change with options [ cluster metadata change - ""
                     + changeClusterMetadata + "" ], [ changing rebalancing state - ""
                     + changeRebalanceState + "" ], [ changing swapping RO - "" + swapRO
                     + "" ], [ rollback - "" + rollback + "" ]"");
 
         // Variables to track what has completed
         List<RebalancePartitionsInfo> completedRebalancePartitionsInfo = Lists.newArrayList();
         List<String> swappedStoreNames = Lists.newArrayList();
         boolean completedClusterChange = false;
         boolean completedRebalanceSourceClusterChange = false;
         Cluster previousRebalancingSourceCluster = null;
 
         try {
-            // CHANGE CLUSTER METADATA
-            if(changeClusterMetadata) {
-                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
-                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
-                completedClusterChange = true;
-            }
 
-            // SWAP RO DATA FOR ALL STORES
-            if(swapRO) {
-                swapROStores(swappedStoreNames, false);
-            }
-
+            /*
+             * Do the rebalancing state changes. It is important that this
+             * happens before the actual cluster metadata is changed. Here's
+             * what could happen otherwise. When a batch completes with
+             * {current_cluster c2, rebalancing_source_cluster c1} and the next
+             * rebalancing state changes it to {current_cluster c3,
+             * rebalancing_source_cluster c2} is set for the next batch, then
+             * there could be a window during which the state is
+             * {current_cluster c3, rebalancing_source_cluster c1}. On the other
+             * hand, when we update the rebalancing source cluster first, there
+             * is a window where the state is {current_cluster c2,
+             * rebalancing_source_cluster c2}, which still fine, because of the
+             * following. Successful completion of a batch means the cluster is
+             * finalized, so its okay to stop proxying based on {current_cluster
+             * c2, rebalancing_source_cluster c1}. And since the cluster
+             * metadata has not yet been updated to c3, the writes will happen
+             * based on c2.
+             * 
+             * Even if some clients have already seen the {current_cluster c3,
+             * rebalancing_source_cluster c2} state from other servers, the
+             * operation will be rejected with InvalidMetadataException since
+             * this server itself is not aware of C3
+             */
             // CHANGE REBALANCING STATE
             if(changeRebalanceState) {
                 try {
                     previousRebalancingSourceCluster = metadataStore.getRebalancingSourceCluster();
                     if(!rollback) {
                         // Save up the current cluster for Redirecting store
+                        logger.info(""Setting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to "" + currentCluster);
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, currentCluster);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.addRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     } else {
                         // Reset the rebalancing source cluster back to null
+                        logger.info(""Resetting rebalancing source cluster xml from ""
+                                    + previousRebalancingSourceCluster + ""to null"");
                         changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML, null);
                         completedRebalanceSourceClusterChange = true;
 
                         for(RebalancePartitionsInfo info: rebalancePartitionsInfo) {
                             metadataStore.deleteRebalancingState(info);
                             completedRebalancePartitionsInfo.add(info);
                         }
                     }
                 } catch(Exception e) {
                     throw new VoldemortException(e);
                 }
             }
+
+            // CHANGE CLUSTER METADATA
+            if(changeClusterMetadata) {
+                logger.info(""Switching metadata from "" + currentCluster + "" to "" + cluster);
+                changeCluster(MetadataStore.CLUSTER_KEY, cluster);
+                completedClusterChange = true;
+            }
+
+            // SWAP RO DATA FOR ALL STORES
+            if(swapRO) {
+                swapROStores(swappedStoreNames, false);
+            }
         } catch(VoldemortException e) {
 
             logger.error(""Got exception while changing state, now rolling back changes"", e);
 
             // ROLLBACK CLUSTER CHANGE
             if(completedClusterChange) {
                 try {
                     logger.info(""Rolling back cluster.xml to "" + currentCluster);
                     changeCluster(MetadataStore.CLUSTER_KEY, currentCluster);
                 } catch(Exception exception) {
                     logger.error(""Error while rolling back cluster metadata to "" + currentCluster,
                                  exception);
                 }
             }
 
             // SWAP RO DATA FOR ALL COMPLETED STORES
             if(swappedStoreNames.size() > 0) {
                 try {
                     swapROStores(swappedStoreNames, true);
                 } catch(Exception exception) {
                     logger.error(""Error while swapping back to old state "", exception);
                 }
             }
 
             // CHANGE BACK ALL REBALANCING STATES FOR COMPLETED ONES
             if(completedRebalancePartitionsInfo.size() > 0) {
                 if(!rollback) {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.deleteRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while deleting back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 } else {
                     for(RebalancePartitionsInfo info: completedRebalancePartitionsInfo) {
                         try {
                             metadataStore.addRebalancingState(info);
                         } catch(Exception exception) {
                             logger.error(""Error while adding back rebalance info during error rollback ""
                                                  + info,
                                          exception);
                         }
                     }
                 }
 
             }
 
             // Revert changes to REBALANCING_SOURCE_CLUSTER_XML
             if(completedRebalanceSourceClusterChange) {
                 logger.info(""Reverting the REBALANCING_SOURCE_CLUSTER_XML back to ""
                             + previousRebalancingSourceCluster);
                 changeCluster(MetadataStore.REBALANCING_SOURCE_CLUSTER_XML,
                               previousRebalancingSourceCluster);
             }
 
             throw e;
         }
 
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message indicates a bug fix in proxy put stats and several other changes. This specific diff involves reordering operations within the `rebalanceStateChange` method, emphasizing correctness. Reordering updates ensures that the rebalancing state changes occur before changing the cluster metadata to prevent temporary inconsistencies that could lead to errors, as the comment explains.

The change does not explicitly mention proxy puts, but the reordering directly addresses a potential bug where state changes might not reflect correctly if the operations were performed in the previous sequence. Thus, the changes align with the commit message stating a bug fix with respect to the rebalancing procedure.

Therefore, given that the reordering appears to resolve a potential error condition, these changes suggest a bug fix. Thus, the conclusion is **Buggy**."
voldemort,9466.json,e7ecec1bd3dd879221a56714fc774a2001d843e8,"@@ -1,4 +1,3 @@
     protected void recordSuccess(Node node, long startNs) {
-        proxyPutStats.reportProxyPutCompletion();
         failureDetector.recordSuccess(node, (System.nanoTime() - startNs) / Time.NS_PER_MS);
     }",Buggy,"1.Enabling proxy puts by dafault
2. Bug fix in proxy put stats
3. Changing order of state change updates for correctness
4. Setting proxy put tests to do one batch rebalancing
",Buggy,"The commit message indicates a bug fix related to proxy put statistics. The code change removes a call to `proxyPutStats.reportProxyPutCompletion()` from the `recordSuccess` method, which might have been incorrectly reporting the completion of proxy puts in situations where it was not actually appropriate or desired. If the completion was recorded incorrectly, this could lead to inaccuracies in tracking proxy put success rates, thereby representing an error in the application's logic regarding proxy put operations. Removing this call seems to correct such behavior by aligning the stats reporting more accurately with the actual events being tracked.

Given this context, the change can indeed be considered a bug fix. The change aligns with the commit message's mention of a ""Bug fix in proxy put stats,"" as it addresses the potential error in proxy put statistics reporting.

Therefore, I conclude that the changes are **Buggy**."
voldemort,8840.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getCheckedInResourcesCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.queue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.queue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getCheckedInResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",NotBuggy,"The commit message mentions cleaning up methods and fixing an error in a test case. The diff removes unnecessary initializations and simplifies the logic by correctly handling the case when a key doesn't exist. Exception handling remains for invalid keys, which indicates an improvement in logic but not a direct bug fix. The answer is **NotBuggy**."
voldemort,8883.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,14 +1,12 @@
     public int getRegisteredResourceRequestCount(K key) {
-        int rc = 0;
-        if(!requestQueueMap.containsKey(key)) {
-            return rc;
+        if(requestQueueMap.containsKey(key)) {
+            try {
+                Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
+                // FYI: .size() is not constant time in the next call. ;)
+                return requestQueue.size();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Queue<AsyncResourceRequest<V>> requestQueue = getRequestQueueForExistingKey(key);
-            // FYI: .size() is not constant time in the next call. ;)
-            rc = requestQueue.size();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getRegisteredResourceRequestCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",NotBuggy,"The commit message mentions cleaning up methods and fixing an error in a test case. Let's analyze the code changes for relevance to these tasks.

1. **Method Clean-Up:**
   - The code removes the redundant `int rc` variable initially set to `0` and returned at the end. Instead, it directly returns `0` if the `key` is absent in `requestQueueMap`.
   - It simplifies the logic by handling the `containsKey` check upfront, thus avoiding the need for a try-catch block unless the `key` is present.
   - It improves readability by consolidating the check and operation into a simpler structure.

2. **Handling of IllegalArgumentException:**
   - The behavior upon encountering an `IllegalArgumentException` remains the same since it logs the exception and proceeds, now emphasizing the significance of the previous check for existence in `requestQueueMap`.

3. **Early Return for Missing Key:**
   - By returning `0` immediately when `key` is absent, the function now has improved clarity of its functioning.

The changes predominantly address code clarity and improve efficiency in scenario handling, aligning with the description of cleaning up methods. However, the commit message mentions a ""fix error in test case,"" which isn't directly reflected in the provided diff as this change seems focused on method improvement in clean-up.

Given the context of ""fixing an error in a test case"" is absent from the diff, and the apparent focus is method refinements rather than explicit bug fixes related to incorrect logic or errors (aside from logic redundancy), the conclusion is that this change, while aligned with the message, does not directly indicate a bug fix based solely on the content provided.

Overall, since the focus is on code cleanup without explicit bug behavior correction observed, the conclusion is **NotBuggy**."
voldemort,8838.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getTotalResourceCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.size.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getTotalResourceCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.size.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getTotalResourceCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",NotBuggy,"The commit message mentions ""clean up get stats methods"" and ""fix error in test case."" The changes primarily focus on the `getTotalResourceCount` method.

Here's the analysis:

1. **Simplification of Logic**: The method was refactored to simplify the logic:
   - Previously, it initialized `rc` to 0, checked if `resourcePoolMap` does not contain the key, and returned `rc`. This has been condensed to an if-block that simplifies checking if `resourcePoolMap` contains the key.
   
2. **Improved Exception Handling**: 
   - The try-catch block was inside the if-else structure. Now, it's directly within the if conditionally returning the size or debugging the exception. This change means the method directly returns the size when the key exists without using a temporary variable.
   - The code handling an invalid key scenario remains to log a debug message, but it now returns 0 directly afterward, which effectively mirrors the original behavior.

While the updates improve code simplicity and structure, they don't seem targeted explicitly at fixing a bug. Instead, the logic simplification and exception handling improvements address maintainability and code clarity aspects.

Given these observations, if the ""fix error in test case"" mentioned in the commit message pertains to this change, it might be indirectly implied by improving how method failures are reported or handled. However, the diff alone does not provide explicit evidence of a direct bug fix.

Therefore, based on the provided context, the changes appear to be more aligned with code clean-up rather than fixing a specific bug. Thus, the verdict would be **NotBuggy** unless further information regarding the test case issue is made apparent."
voldemort,8842.json,e0707033c5a5c63fd044a59e7ee4055453334a01,"@@ -1,13 +1,11 @@
     public int getBlockingGetsCount(K key) {
-        int rc = 0;
-        if(!resourcePoolMap.containsKey(key)) {
-            return rc;
+        if(resourcePoolMap.containsKey(key)) {
+            try {
+                Pool<V> resourcePool = getResourcePoolForExistingKey(key);
+                return resourcePool.blockingGets.get();
+            } catch(IllegalArgumentException iae) {
+                logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
+            }
         }
-        try {
-            Pool<V> resourcePool = getResourcePoolForExistingKey(key);
-            rc = resourcePool.blockingGets.get();
-        } catch(IllegalArgumentException iae) {
-            logger.debug(""getBlockingGetsCount called on invalid key: "", iae);
-        }
-        return rc;
+        return 0;
     }",NotBuggy,"Clean up get stats methods in (Queued)KeyedResourcePool. Fix error in test case.
",NotBuggy,"The commit message indicates a cleanup and a fix of an error in a test case. The code modification here focuses on the `getBlockingGetsCount` method, which has been refactored for clarity. 

### Analysis of Code Changes:

1. **Initial Return Value Removal**: 
   - The initial value `rc` is removed, and instead, a default return value of `0` is used at the end if the key is not contained in the `resourcePoolMap`.
   
2. **Logic Clarification**: 
   - The conditional logic is inverted. Previously, it checked for the absence of the key, returning `0` immediately, and handled the presence of the key in a try-catch block next.
   - Now, it directly checks for the presence of the key and attempts to get the blocking gets count inside a try-catch block, streamlining the method's flow.

3. **Handling of `IllegalArgumentException`**:
   - The same try-catch structure remains, logging a debug message if `getResourcePoolForExistingKey` throws an `IllegalArgumentException`.

4. **Elimination of Redundant Variable**:
   - The local variable `rc` has been eliminated in favor of direct return statements. 

The mentioned ""error in test case"" is not directly observable here, but the refactor improves method clarity and removes potentially unreachable code paths (since `rc` was not really needed after the catch block, owing to direct returns). 

### Conclusion:

Although these changes clean up the method, they do not explicitly signal a resolution of a functional bug or error from a test case based on the diff alone. The modifications suggest improvements and simplification of logic rather than a fix to a previously faulty behavior. 

Therefore, based on the code provided and its alignment with the commit message, these changes are **NotBuggy** in terms of performing a bug fix."
voldemort,532.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,14 +1,18 @@
     public RebalancePlan getPlan(Cluster finalCluster,
                                  List<StoreDefinition> finalStoreDefs,
                                  int batchSize) {
         RebalanceUtils.validateClusterStores(finalCluster, finalStoreDefs);
         RebalanceUtils.validateCurrentFinalCluster(currentCluster, finalCluster);
 
+        // TODO: (currentCluster vs interimCluster) Add more validation before
+        // constructing plan? Given that currentCluster was polled from prod
+        // cluster, should confirm that it is an ""interim cluster"" i.e., has
+        // same (superset?) of nodes as are in finalCluster.
         String outputDir = null;
         return new RebalancePlan(currentCluster,
                                  currentStoreDefs,
                                  finalCluster,
                                  finalStoreDefs,
                                  batchSize,
                                  outputDir);
     }",NotBuggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",NotBuggy,"The commit message suggests bug fixes and the addition of TODOs. However, the code diff for `RebalancePlan.getPlan` only introduces a TODO comment related to the validation of `currentCluster` vs `interimCluster`. The comment doesn't change the code logic or fix a bug; it merely highlights a potential area for future improvement or concerns. Without any direct modification to the code logic that addresses an existing bug, this particular change is more of a reminder or note for developers.

From this specific diff, there are no visible bug fixes implemented. It's important to note that any actual bug fixes mentioned in the commit message do not appear in this snippet. Therefore, based solely on this information, we conclude the change is **NotBuggy**."
voldemort,887.json,81f0a160898ca88338ebf4834f7f1fd07f66c63b,"@@ -1,3 +1,3 @@
         public String waitForCompletion(int nodeId, int requestId) {
-            return waitForCompletion(nodeId, requestId, Long.MAX_VALUE, TimeUnit.SECONDS, null);
+            return waitForCompletion(nodeId, requestId, 0, TimeUnit.SECONDS, null);
         }",Buggy,"Fix some bugs I introduced and added more TODOs

Fixed overflow introduced in AdminClient.waitForCompletion by passing in
Long.MAX_VALUE for duration.

Verify cluster store definition in StoreRoutingPlan. This requires
working around existing problems with how system stores are handled (the
store definition is hard-coded for two zones). Left some TODOs about
testing and fixing all of this.

Added TODOs about currentCluster vs interimCluster. Need to tweak
interface to RebalanceController and RebalancePlan to be consistent with
recommended usage (i.e., deploying interimCluster before starting
rebalance).

Minor tweaks to tests based on above changes.
",Buggy,"The commit message discusses fixing a bug related to an overflow issue in `AdminClient.waitForCompletion` by adjusting the duration parameter to `Long.MAX_VALUE`. However, the provided diff shows the opposite change. Instead of using `Long.MAX_VALUE`, it changes the value to `0`, which suggests immediate timeout instead of waiting indefinitely. 

This modification might address an issue different from what the commit message suggests, possibly related to ensuring the function doesn't hang indefinitely. Without further context or additional diffs related to this function, it's difficult to conclude definitively if this resolves overflow or leads to a functional bug fix.

The change looks like an adjustment rather than returning to the intended fix of an overflow issue mentioned in the commit message. However, given the broader context of handling a problem with duration and the immediate effect the change can have regarding application behavior. It's essential to align changes with addressing meaningful issues even when the exact detail is a bit different than presumed in the initial commit message description.

Since the message indicates a fix around overflow and understanding the code impact as bug-related since it tries to correct wrong behavior or improve processing issues, let's conclude with a resolution of addressing potential issues:

**Buggy**"
voldemort,54.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,39 +1,42 @@
     public void reduce(BytesWritable key,
                        Iterator<BytesWritable> values,
                        OutputCollector<Text, Text> output,
                        Reporter reporter) throws IOException {
         BytesWritable writable = values.next();
         byte[] valueBytes = writable.get();
 
         if(this.nodeId == -1)
             this.nodeId = ByteUtils.readInt(valueBytes, 0);
         if(this.chunkId == -1)
             this.chunkId = ReadOnlyUtils.chunk(key.get(), this.numChunks);
 
         // Write key and position
         this.indexFileStream.write(key.get(), 0, key.getSize());
-        this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
         this.indexFileStream.writeInt(this.position);
-        this.checkSumDigestIndex.update(this.position);
+        if(this.checkSumDigestIndex != null) {
+            this.checkSumDigestIndex.update(key.get(), 0, key.getSize());
+            this.checkSumDigestIndex.update(this.position);
+        }
 
         // Write length and value
         int valueLength = writable.getSize() - 4;
         this.valueFileStream.writeInt(valueLength);
-        this.checkSumDigestValue.update(valueLength);
         this.valueFileStream.write(valueBytes, 4, valueLength);
-        this.checkSumDigestValue.update(valueBytes, 4, valueLength);
-
+        if(this.checkSumDigestValue != null) {
+            this.checkSumDigestValue.update(valueLength);
+            this.checkSumDigestValue.update(valueBytes, 4, valueLength);
+        }
         this.position += 4 + valueLength;
         if(this.position < 0)
             throw new VoldemortException(""Chunk overflow exception: chunk "" + chunkId
                                          + "" has exceeded "" + Integer.MAX_VALUE + "" bytes."");
 
         // if we have multiple values for this md5 that is a collision, throw an
         // exception--either the data itself has duplicates, there are trillions
         // of keys, or someone is attempting something malicious
         if(values.hasNext())
             throw new VoldemortException(""Duplicate keys detected for md5 sum ""
                                          + ByteUtils.toHexString(ByteUtils.copy(key.get(),
                                                                                 0,
                                                                                 key.getSize())));
     }",Buggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"The commit message indicates a fix for a bug that causes a NullPointerException (NPE) when a checksum is None. Examining the provided diff:

1. The code previously assumed `checkSumDigestIndex` and `checkSumDigestValue` were never null when calling `update()`. This could lead to a NullPointerException if either was null.

2. The updated code introduces conditional checks to see if `checkSumDigestIndex` and `checkSumDigestValue` are not null before calling their `update()` methods. This change prevents the NullPointerException when these fields are null.

The modifications directly address the potential for a NullPointerException when checksums are not initialized, which aligns well with the commit message's description of fixing such an issue. The changes improve error handling by preventing method calls on potentially null objects.

Therefore, these changes indeed address a bug, specifically a logical error that could lead to runtime exceptions. The assessment is **Buggy**."
voldemort,10530.json,0b2ef083f53872f2686e10990e3586177c693bd6,"@@ -1,10 +1,12 @@
     public static String toString(CheckSumType type) {
         if(type == CheckSumType.ADLER32) {
             return ""adler32"";
         } else if(type == CheckSumType.CRC32) {
             return ""crc32"";
         } else if(type == CheckSumType.MD5) {
             return ""md5"";
+        } else if(type == CheckSumType.NONE) {
+            return ""none"";
         }
-        return null;
+        return ""none"";
     }",NotBuggy,"Fixed minor bug which returns NPE when Checksum is None
",Buggy,"The commit message states a fix for a minor bug where a `NullPointerException` (NPE) was returned when the `CheckSumType` was `None`. The code change adds a condition to handle the `CheckSumType.NONE` explicitly, returning the string ""none"" instead of falling through to the default return, which was `null`. This addresses the potential NPE because calling `toString()` on the `null` result would have triggered the exception. The modifications directly align with the commit message as they introduce necessary logic to handle the `CheckSumType.NONE`, thereby preventing an NPE.

The changes include:
- Adding an explicit branch for `CheckSumType.NONE` to return ""none"".
- Altering the default return value from `null` to ""none"" for added safety.

These updates clearly indicate a bug fix in the logic that prevents `NullPointerException`. Therefore, the conclusion is **Buggy**."
weka,27677.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,27 +1,26 @@
   private double instanceTransformationProbability(Instance first, 
 						   Instance second) {
     String debug = ""(KStar.instanceTransformationProbability) "";
     double transProb = 1.0;
     int numMissAttr = 0;
     for (int i = 0; i < m_NumAttributes; i++) {
       if (i == m_Train.classIndex()) {
 	continue; // ignore class attribute
       }
       if (first.isMissing(i)) { // test instance attribute value is missing
 	numMissAttr++;
 	continue;
       }
       transProb *= attrTransProb(first, second, i);
       // normilize for missing values
       if (numMissAttr != m_NumAttributes) {
-	// I don't know where this comes from!!!
 	transProb = Math.pow(transProb, (double)m_NumAttributes / 
 			     (m_NumAttributes - numMissAttr));
       }
       else { // weird case!
 	transProb = 0.0;
       }
     }
     // normilize for the train dataset
      return transProb / m_NumInstances;
   }",NotBuggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states a fix for initialization with the `-E` option during incremental training, but this diff is unrelated, focusing on probability normalization logic. No incremental training or `-E` focus exists here. The answer is **NotBuggy**."
weka,27694.json,29dadd8d55df37a7a89f355922eb92adc307f713,"@@ -1,3 +1,4 @@
   private void update_m_Attributes() {
     m_NumInstances = m_Train.numInstances();
+    m_InitFlag = ON;
   }",Buggy,"Fixed bug that prevented correct initialization for -E option during incremental
training.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@445 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message talks about a bug fix related to initialization with the `-E` option during incremental training. The code diff shows an update to the `update_m_Attributes` method, where a flag `m_InitFlag` is set to `ON`. This suggests that this flag is likely used to denote initialization status, which the commit message implies was previously unhandled or incorrect, potentially causing the bug where initialization wasn't happening properly. The change seems directly related to the initialization issue mentioned in the commit message, suggesting that this update fixes the bug.

The added line directly sets an initialization flag that could play a crucial role during initialization, specifically associated with the `-E` option. The description fits the context of ensuring the program properly starts its incremental training without missing necessary initial steps due to an uninitialized flag.

Therefore, the code change aligns with the commit message, indicating a **Buggy** fix."
weka,31023.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,13 +1,14 @@
   public void setSource(InputStream input) throws IOException {
     BufferedReader	reader;
     String		line;
     
     m_structure    = null;
     m_sourceFile   = null;
     m_File         = null;
 
     m_StreamBuffer = new StringBuffer();
     reader         = new BufferedReader(new InputStreamReader(input));
-    while ((line = reader.readLine()) != null)
+    while ((line = reader.readLine()) != null) {
       m_StreamBuffer.append(line + ""\n"");
+    }
   }",NotBuggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"This commit message indicates a bug fix related to CSVLoader concerning loading the source file twice into the StringBuffer. The Git diff shows that braces `{}` have been added to explicitly define the block of code for appending lines to `m_StreamBuffer`. The absence of these braces in the original code could lead to bugs if additional code follows the `while` statement without indentation, causing logic errors or unintended behavior.

While the specific fix for loading a source file twice isn't explicit in this diff, encapsulating the `m_StreamBuffer.append(line + ""\n"");` operation within a block prevents such issues when modifying code in the future. It ensures all intended lines are appended correctly, aligning with safe programming practices to avoid mismanagement of the append operation.

Considering the context provided by the commit message and given the fix improves robustness in handling the stream, this change suggests addressing the base problem contributing to the described bug. Therefore, the analysis concludes that the changes are related to fixing a bug as described in the commit message.

**Buggy**"
weka,30923.json,91f37b47e0458a307defac739c4967d86935770c,"@@ -1,18 +1,22 @@
   public void setSource(File file) throws IOException {
     m_structure = null;
+    
     setRetrieval(NONE);
 
-    if (file == null) {
+    if (file == null)
       throw new IOException(""Source file object is null!"");
-    }
 
     try {
-      setSource(new FileInputStream(file));
+      if (file.getName().endsWith(getFileExtension() + FILE_EXTENSION_COMPRESSED)) {
+	setSource(new GZIPInputStream(new FileInputStream(file)));
+      } else {
+	setSource(new FileInputStream(file));
+      }
     }
     catch (FileNotFoundException ex) {
       throw new IOException(""File not found"");
     }
-
+    
     m_sourceFile = file;
     m_File       = file.getAbsolutePath();
-  }

+    }",Buggy,"Moved some code from setSource in ArffLoader to AbstractFileLoader so that other Loaders can handle gzip compression. Fixed bug in CSVLoader that was causing the source file to be loaded twice into the StringBuffer


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4322 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions fixing a bug in `CSVLoader` related to loading a file twice into a `StringBuffer`. However, the provided diff shows changes in the `setSource` method, likely belonging to a class related to file handling (perhaps `ArffLoader` as per the commit), and mainly introduces gzip compression handling by checking the file extension. 

The conditional block checks for a gzip file and uses `GZIPInputStream` for such files. The code related to the bug fix in `CSVLoader` mentioned in the commit message does not appear in the provided diff snippet. Therefore, we don't see direct evidence of a bug fix concerning `CSVLoader` within the visible changes.

The provided changes improve functionality by adding support for gzip compression but do not directly showcase resolving a bug (particularly the `CSVLoader` one mentioned in the commit message). Therefore, based on this diff alone, the changes don't seem to address a bug.

Thus, without evidence of directly addressing a bug in this segment of code, the conclusion is **NotBuggy**."
weka,11692.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,53 +1,52 @@
 	public void drawGraph() {
 		// build the panel
 		try {
 		  remove(canvas3D);
-		  System.out.println(""remove ok"");	
 		
 		} catch (Exception e) {
 		}
 //		if (!m_canvasCreated) {
 		  canvas3D = new Canvas3D(SimpleUniverse.getPreferredConfiguration());
 //		}
 
 		int scrHeight = (new Double(this.getSize().getHeight())).intValue();
 		int scrWidth = (new Double(this.getSize().getWidth())).intValue();
 		
 		canvas3D.setSize(scrWidth - 120, scrHeight - 50);
 		
 //		if (!m_canvasCreated) {
 		add(canvas3D, java.awt.BorderLayout.CENTER);
 		m_canvasCreated = true;
 //		}
 		
 		freeResources();				
 		
 		// build the visualisation
                 m_scene = createSceneGraph();
 
                 // compile the scene
                 m_scene.compile();
 
 	        // build the universe
 		m_simpleU = new SimpleUniverse(canvas3D);
 
 
 		// add the behaviors to the ViewingPlatform
 		ViewingPlatform viewingPlatform = m_simpleU.getViewingPlatform();
 
 		viewingPlatform.setNominalViewingTransform();
 
 		// add orbit behavior to ViewingPlatform
 		orbit =
 			new OrbitBehavior(
 				canvas3D,
 				OrbitBehavior.REVERSE_ALL | OrbitBehavior.STOP_ZOOM);
 		BoundingSphere bounds =
 			new BoundingSphere(new Point3d(0.0, 0.0, 0.0), 100.0);
 		orbit.setSchedulingBounds(bounds);
 		viewingPlatform.setViewPlatformBehavior(orbit);
 		
 
 		m_simpleU.addBranchGraph(m_scene);
 
 	}",NotBuggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes fixing a refresh bug that affects the UI when switching between tabs, along with a dependency update. The removal of `System.out.println` and the addition of `remove(canvas3D);` followed by handling the `canvas3D` creation and adding it back to the UI layout demonstrates an adjustment to the UI logic. By removing and recreating the `canvas3D`, the code ensures a clean state when redrawing the graph, potentially fixing the refresh issue described in the commit message. 

The lack of specific details on the dependency update suggests it might be handled elsewhere or is not directly related to the shown code changes. However, the code changes clearly address the graph and panel handling logic.

Therefore, these changes address the UI logic problem, corroborating the commit message's claim of a bug fix. Hence, the changes can be concluded as **Buggy**."
weka,11730.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,23 +1,37 @@
 	private void panelChanged() {
 		int numTab = viewerTabbedPanel.getSelectedIndex();
 		visu1Rules = selectionPanel.getSelectedRules(1);
 /*		visu2Rules = selectionPanel.getSelectedRules(2);
 		visu3Rules = selectionPanel.getSelectedRules(3); */
 
 		switch (numTab) {
+		case 0:
+			visu1PanelHolder.removeAll();
+			visu1PanelHolder.revalidate();
+			visu1PanelHolder.repaint();
+			break;
 			case 1 :
+				visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
+				revalidate();
+				repaint();
 				String[] selectedCriteria =
 					selectionPanel.getSelectedCriteria();
 				visu1Panel3D.setData(visu1Rules, selectedCriteria);
 //				visu1Panel.setSelectedRules(visu2Rules);
 				break;
 			case 2 :
 				visu2Panel.setData(visu2Rules, criteres);
 				visu2Panel.setSelectedRules(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 			case 3 :
 				visu3Panel.setData(visu3Rules);
+				visu1PanelHolder.removeAll();
+				visu1PanelHolder.revalidate();
+				visu1PanelHolder.repaint();
 				break;
 		}
 
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The diff addresses a UI update issue when switching tabs involving visual panels. The changes include removing all components from `visu1PanelHolder`, revalidating, and repainting at key points during tab switching. This aligns with fixing a refresh bug as stated in the commit message. Each tab change now ensures a clean and refreshed panel state, which is essential for addressing visual update bugs where users expect immediate feedback. Thus, the changes relate directly to a refresh bug in the UI interaction described in the commit. The solution includes logical corrections for UI rendering sequences often encountered in display bugs.

The answer is **Buggy**."
weka,11722.json,38a69e67c3d1f3dcb2e0c53e54f54b37ff25a090,"@@ -1,114 +1,115 @@
 	private void initComponents() {
 	    setLayout(new java.awt.BorderLayout());
 		viewerTabbedPanel = new javax.swing.JTabbedPane();
 		selectionPanel = new selection.SelectionPanel();
 		visu1Panel3D = new Panel3D();
+		visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER);
 		/*visu1Panel = new RulesSelectionPanel(visu1Panel3D);
 		visu1Panel.addActionListener(this); */
 /*		visu2PanelLine = new PanelLine();
 		visu2Panel = new RulesSelectionPanel(visu2PanelLine);
 		visu2Panel.setSingleSelection();
 		visu2Panel.addActionListener(this);
 		visu2Panel.setColored();
 		visu3Panel = new visu3.PanelDDecker(); */
 		// viewerBar = new javax.swing.JMenuBar();
 /*		fileMenu = new javax.swing.JMenu();
 		openItem = new javax.swing.JMenuItem();
 		printItem = new javax.swing.JMenuItem();
 		saveItem = new javax.swing.JMenuItem();
 		quitItem = new javax.swing.JMenuItem();
 		helpMenu = new javax.swing.JMenu();
 		aboutItem = new javax.swing.JMenuItem();
 		contentsItem = new javax.swing.JMenuItem(); */
 
 		selectionPanel.addMultipleListSelectionListener(this);
 
 //		setTitle(""Association Rules Viewer"");
 /*		addWindowListener(new java.awt.event.WindowAdapter() {
 			public void windowClosing(java.awt.event.WindowEvent evt) {
 				exitForm(evt);
 			}
 		}); */
 
 		viewerTabbedPanel.addChangeListener(new ChangeListener() {
 			public void stateChanged(ChangeEvent e) {
 				panelChanged();
 			}
 
 		});
 
 		viewerTabbedPanel.addTab(""Selection"", selectionPanel);
-		viewerTabbedPanel.addTab(""3D Representation"", visu1Panel3D);
+		viewerTabbedPanel.addTab(""3D Representation"", visu1PanelHolder);
 		/*viewerTabbedPanel.addTab(""N Dimensional Line"", visu2Panel);
 		viewerTabbedPanel.addTab(""Double Decker Plot"", visu3Panel); */
 
 		viewerTabbedPanel.setEnabledAt(1, false);
 		// viewerTabbedPanel.setEnabledAt(2, false);
 //		viewerTabbedPanel.setEnabledAt(3, false);
 
 		add(viewerTabbedPanel, java.awt.BorderLayout.CENTER);
 
 /*		fileMenu.setText(""File"");
 		openItem.setText(""Open"");
 		openItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				openPerformed(evt);
 			}
 		});
 
 		fileMenu.add(openItem);
 
 		printItem.setText(""Print"");
 		printItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				printPerformed(evt);
 			}
 		});
 
 		fileMenu.add(printItem);
 
 		saveItem.setText(""Save as"");
 		saveItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				savePerformed(evt);
 			}
 		});
 
 		fileMenu.add(saveItem);
 
 		quitItem.setText(""Quit"");
 		quitItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				quitPerformed(evt);
 			}
 		});
 
 		fileMenu.add(quitItem);
 
 		viewerBar.add(fileMenu);
 
 		helpMenu.setText(""Help"");
 		aboutItem.setText(""About"");
 		aboutItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showAbout(evt);
 			}
 		});
 
 		helpMenu.add(aboutItem);
 
 		contentsItem.setText(""Contents"");
 		contentsItem.addActionListener(new java.awt.event.ActionListener() {
 			public void actionPerformed(java.awt.event.ActionEvent evt) {
 				showHelp(evt);
 			}
 		});
 
 		helpMenu.add(contentsItem);
 
 		viewerBar.add(helpMenu);
 
 		setJMenuBar(viewerBar);
 
 		pack(); */
 	}",Buggy,"Fixed a refresh bug that prevented the switching back to the rules selection tab from the 3D viewer tab. Now depends on the scatterPlot3D package for the Java3D libraries

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12305 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes fixing a refresh bug that impacted switching between tabs in an interface involving a 3D viewer. The code changes introduce `visu1PanelHolder` in place of `visu1Panel3D` on the tab for the 3D representation:

1. **Bug Context**: The original implementation directly added `visu1Panel3D` to the tab, which might have caused issues when switching between tabs, perhaps due to layout issues or state changes not being properly handled.
   
2. **Code Changes**: 
   - `visu1PanelHolder` is introduced, which now contains `visu1Panel3D` using `visu1PanelHolder.add(visu1Panel3D, BorderLayout.CENTER)`.
   - The tab for representing the 3D view is changed from directly showing `visu1Panel3D` to showing `visu1PanelHolder`. This could encapsulate the 3D panel properly, fixing refresh or state change issues when switching tabs.

3. **Relevance to the Commit Message**: The specific problem of switching back to the rules selection tab could be causing improper rendering or managing the state of the 3D panel. Introducing a holder for the 3D panel seems to align with addressing such a refresh issue, conforming with the bug description.

Given these code changes and their strong alignment with addressing a bug that affects UI tab functionality as stated in the commit message, the changes indeed seem intended to fix the described bug.

Therefore, the changes indicate a bug fix, and the appropriate classification is **Buggy**."
weka,28529.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,11 +1,12 @@
     public static void main(String[] args) {
         try {
+	    System.err.println(""okidoki"");
             BIFReader br = new BIFReader();
             br.processFile(args[0]);
 	    System.out.println(br.toString());
         
         }
         catch (Throwable t) {
             t.printStackTrace();
         }
     } // main",NotBuggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states a bug fix, but the actual code diff only adds a debug print statement: `System.err.println(""okidoki"");`. This does not address any evident bug in functionality, error handling, or logic. It's likely meant for debugging purposes, rather than fixing a bug. Hence, there is no alignment between the commit message and code changes with regard to fixing a bug.

The answer is **NotBuggy**."
weka,28685.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,10 +1,10 @@
     public Enumeration listOptions() {
         Vector newVector = new Vector(4);
 
-        newVector.addElement(new Option(""\tUse ADTree data structure\n"", ""D"", 0, ""-D""));
+        newVector.addElement(new Option(""\tDo not use ADTree data structure\n"", ""D"", 0, ""-D""));
         newVector.addElement(new Option(""\tBIF file to compare with\n"", ""B"", 1, ""-B <BIF file>""));
         newVector.addElement(new Option(""\tSearch algorithm\n"", ""Q"", 1, ""-Q weka.classifiers.bayes.net.search.SearchAlgorithm""));
         newVector.addElement(new Option(""\tEstimator algorithm\n"", ""E"", 1, ""-E weka.classifiers.bayes.net.estimate.SimpleEstimator""));
 
         return newVector.elements();
     } // listOptions",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The provided diff changes a description string for an option in the `listOptions` method. The initial option text suggests enabling the use of the ADTree data structure, while the revised version corrects this to mean not using the ADTree data structure. This change implies that the option's purpose was originally misrepresented, leading to incorrect assumptions or usage by users relying on the options description. 

Such a modification typically addresses a bug in the user interface or documentation of software options, ensuring that the representation matches the functionality or design intent. Given that the commit message also explicitly mentions a ""bug fix"" spotted by an individual, this aligns with the need to correct misleading information for the end-user.

Therefore, the change is aligned with fixing a bug, not in the program logic or execution but in usability and clarity. Thus, this update can be classified as a bug fix. The evaluation concludes with **Buggy**."
weka,28381.json,c34e6f5c40e4a29dfcedb4b3333188288fb79717,"@@ -1,40 +1,41 @@
 	public double kFoldCV(BayesNet bayesNet, int nNrOfFolds) throws Exception {
 		m_BayesNet = bayesNet;
 		double fAccuracy = 0.0;
 		double fWeight = 0.0;
 		Instances instances = bayesNet.m_Instances;
 		// estimate CPTs based on complete data set
 		bayesNet.estimateCPTs();
 		int nFoldStart = 0;
 		int nFoldEnd = instances.numInstances() / nNrOfFolds;
 		int iFold = 1;
 		while (nFoldStart < instances.numInstances()) {
 			// remove influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 			
 			// measure accuracy on fold iFold
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				fAccuracy += accuracyIncrease(instance);
+				instance.setWeight(-instance.weight());
 				fWeight += instance.weight();
 			}
 
 			// restore influence of fold iFold from the probability distribution
 			for (int iInstance = nFoldStart; iInstance < nFoldEnd; iInstance++) {
 				Instance instance = instances.instance(iInstance);
 				instance.setWeight(-instance.weight());
 				bayesNet.updateClassifier(instance);
 			}
 
 			// go to next fold
 			nFoldStart = nFoldEnd;
 			iFold++;
 			nFoldEnd = iFold * instances.numInstances() / nNrOfFolds;
 		}
 		return fAccuracy / fWeight;
 	} // kFoldCV",Buggy,"Bug fix spotted by Gladys Castillo Jordan


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2176 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code diff adds an additional line to flip the weight of an instance back to its original value after computing the accuracy increase. Prior to the change, after measuring accuracy, the weight was not restored properly, leading to potential improper weight calculations on subsequent operations. Fixing this ensures the weights are consistent across folds. This change is likely meant to prevent corruption of the data used for classification and validation within the k-fold cross-validation method. Thus, the provided modification aligns with a bug fix related to the weight update of instances during cross-validation, confirming the commit message.

The commit message acknowledges a bug fix, specifically noting identification of the issue by an individual. Given the impact incorrect instance weighting could have on classification metrics, this fix is crucial for ensuring accurate model evaluation.

Therefore, the answer is **Buggy**."
weka,25948.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,61 +1,57 @@
   private double findSplitNominalNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal;
     double[][] counts = new double[theInstances.attribute(index).numValues() 
 				  + 1][theInstances.numClasses()];
     double[] sumCounts = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
     int numMissing = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (inst.isMissing(index)) {
 	numMissing++;
 	counts[theInstances.attribute(index).numValues()]
 	  [(int)inst.classValue()] += inst.weight();
       } else {
 	counts[(int)inst.value(index)][(int)inst.classValue()] += inst
 	  .weight();
       }
     }
 
     // Compute sum of counts
-
     for (int i = 0; i < theInstances.attribute(index).numValues() + 1; i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	sumCounts[j] += counts[i][j];
       }
     }
     
     // Make split counts for each possible split and evaluate
-
     System.arraycopy(counts[theInstances.attribute(index).numValues()], 0,
 		     m_Distribution[2], 0, theInstances.numClasses());
     for (int i = 0; i < theInstances.attribute(index).numValues(); i++) {
       for (int j = 0; j < theInstances.numClasses(); j++) {
 	m_Distribution[0][j] = counts[i][j];
 	m_Distribution[1][j] = sumCounts[j] - counts[i][j];
       }
       currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
       if (Utils.sm(currVal, bestVal)) {
 	bestVal = currVal;
 	m_SplitPoint = (double)i;
 	for (int j = 0; j < 3; j++) {
 	  System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			   theInstances.numClasses());
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sumCounts, 0, bestDist[2], 0, 
 		       theInstances.numClasses());
     }
    
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message claims to fix division by zero bugs, but the given code diff does not show any changes directly related to handling division by zero issues. The code modification instead includes some cosmetic adjustments, such as the removal of unnecessary empty lines, which do not address any arithmetic operations or introduce any checks to prevent division by zero.

Additionally, examining the method findSplitNominalNominal, there is no indication of division operations where checks were added or adjusted to prevent divide-by-zero errors. Since the changes in the diff do not pertain to fixing division by zero, the commit message does not align with the actual changes in the provided code diff.

Therefore, considering the disconnect between the commit message and the content of the actual code modifications, I conclude that this change does not indicate a bug fix related to division by zero.

**NotBuggy**"
weka,25951.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,54 +1,50 @@
   private double findSplitNumericNominal(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sum = new double[theInstances.numClasses()];
     double[][] bestDist = new double[3][theInstances.numClasses()];
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][(int)inst.classValue()] += inst.weight();
       } else {
 	m_Distribution[2][(int)inst.classValue()] += inst.weight();
 	numMissing++;
       }
     }
     System.arraycopy(m_Distribution[1], 0, sum, 0, theInstances.numClasses());
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][(int)inst.classValue()] += inst.weight();
       m_Distribution[1][(int)inst.classValue()] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = ContingencyTables.entropyConditionedOnRows(m_Distribution);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     theInstances.numClasses());
 	  }
 	}
       }
     }
 
     // No missing values in training data.
-
     if (numMissing == 0) {
       System.arraycopy(sum, 0, bestDist[2], 0, theInstances.numClasses());
     }
  
     m_Distribution = bestDist;
     return bestVal;
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states that division by zero bugs have been fixed in the code related to numeric prediction. However, the provided code diff doesn't include any explicit changes such as checks to prevent division by zero, modifications to denominators, or use of safeguards like conditionals to check for zero values before division. It is possible that other parts of the code that are not visible in this diff handle these issues, or the specific improvements might not require changes to explicitly handle division directly in the displayed context.

- The diff primarily shows changes in whitespace, which doesn't affect logic for preventing division by zero.
- There are no apparent modifications in calculations that manage potential division or ensure input checks to handle zero scenarios.

Since the diff doesn't explicitly reflect changes pertinent to division by zero, which the commit message focus on, and lacks substantive changes supporting the claim, it's unclear if the message aligns well with the visible code or if untouched parts are handling potential fixes. However, given the limited scope and conditional statements suggesting changes likely are elsewhere or diff scope is limited:

**NotBuggy**"
weka,25940.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,63 +1,58 @@
   public void buildClassifier(Instances instances) throws Exception {
     
     double bestVal = Double.MAX_VALUE, currVal;
     double bestPoint = -Double.MAX_VALUE, sum;
     int bestAtt = -1, numClasses;
 
     if (instances.checkForStringAttributes()) {
       throw new Exception(""Can't handle string attributes!"");
     }
 
     double[][] bestDist = new double[3][instances.numClasses()];
 
     theInstances = new Instances(instances);
     theInstances.deleteWithMissingClass();
     if (theInstances.classAttribute().isNominal()) {
       numClasses = theInstances.numClasses();
     } else {
       numClasses = 1;
     }
 
     // For each attribute
-    
     for (int i = 0; i < theInstances.numAttributes(); i++) {
       if (i != theInstances.classIndex()) {
 	
 	// Reserve space for distribution.
-	
 	m_Distribution = new double[3][numClasses];
 
 	// Compute value of criterion for best split on attribute
-	
 	if (theInstances.attribute(i).isNominal()) {
 	  currVal = findSplitNominal(i);
 	} else {
 	  currVal = findSplitNumeric(i);
 	}
 	if (Utils.sm(currVal, bestVal)) {
 	  bestVal = currVal;
 	  bestAtt = i;
 	  bestPoint = m_SplitPoint;
 	  for (int j = 0; j < 3; j++) {
 	    System.arraycopy(m_Distribution[j], 0, bestDist[j], 0, 
 			     numClasses);
 	  }
 	}
       }
     }
     
     // Set attribute, split point and distribution.
-    
     m_AttIndex = bestAtt;
     m_SplitPoint = bestPoint;
     m_Distribution = bestDist;
     if (theInstances.classAttribute().isNominal()) {
       for (int i = 0; i < m_Distribution.length; i++) {
 	Utils.normalize(m_Distribution[i]);
       }
     }
     
     // Save memory
-    
     theInstances = new Instances(theInstances, 0);
   }",NotBuggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states that division by zero bugs were fixed for numeric prediction, but the actual code changes don't show any direct handling related to division or zero checks. The diff involves minor formatting adjustments and logic flow clarifications, focusing more on the setup and processing of attributes. Without any direct changes to prevent division by zero errors in the provided diff, the commit message and the diff are not aligned in terms of a bug fix for such an issue.

Thus, based on the provided diff, the answer is **NotBuggy**."
weka,25952.json,449c6a2d0acb111174652c53dff0960ed4f6e606,"@@ -1,64 +1,60 @@
   private double findSplitNumericNumeric(int index) throws Exception {
 
     double bestVal = Double.MAX_VALUE, currVal, currCutPoint;
     int numMissing = 0;
     double[] sumsSquares = new double[3], sumOfWeights = new double[3];
     double[][] bestDist = new double[3][1];
-    double meanNoMissing;
+    double totalSum = 0, totalSumOfWeights = 0;
 
     // Compute counts for all the values
-
     for (int i = 0; i < theInstances.numInstances(); i++) {
       Instance inst = theInstances.instance(i);
       if (!inst.isMissing(index)) {
 	m_Distribution[1][0] += inst.classValue() * inst.weight();
 	sumsSquares[1] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[1] += inst.weight();
       } else {
 	m_Distribution[2][0] += inst.classValue() * inst.weight();
 	sumsSquares[2] += inst.classValue() * inst.classValue() 
 	  * inst.weight();
 	sumOfWeights[2] += inst.weight();
 	numMissing++;
       }
+      totalSumOfWeights += inst.weight();
+      totalSum += inst.classValue() * inst.weight();
     }
-    meanNoMissing = m_Distribution[1][0] / sumOfWeights[1];
 
     // Sort instances
-
     theInstances.sort(index);
     
     // Make split counts for each possible split and evaluate
-
     for (int i = 0; i < theInstances.numInstances() - (numMissing + 1); i++) {
       Instance inst = theInstances.instance(i);
       Instance instPlusOne = theInstances.instance(i + 1);
       m_Distribution[0][0] += inst.classValue() * inst.weight();
       sumsSquares[0] += inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[0] += inst.weight();
       m_Distribution[1][0] -= inst.classValue() * inst.weight();
       sumsSquares[1] -= inst.classValue() * inst.classValue() * inst.weight();
       sumOfWeights[1] -= inst.weight();
       if (Utils.sm(inst.value(index), instPlusOne.value(index))) {
 	currCutPoint = (inst.value(index) + instPlusOne.value(index)) / 2.0;
 	currVal = variance(m_Distribution, sumsSquares, sumOfWeights);
 	if (Utils.sm(currVal, bestVal)) {
 	  m_SplitPoint = currCutPoint;
 	  bestVal = currVal;
 	  for (int j = 0; j < 3; j++) {
-	    bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    if (!Utils.eq(sumOfWeights[j], 0)) {
+	      bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j];
+	    } else {
+	      bestDist[j][0] = totalSum / totalSumOfWeights;
+	    }
 	  }
 	}
       }
     }
 
-    // No missing values in training data
-    
-    if (numMissing == 0) {
-      bestDist[2][0] = meanNoMissing;
-    }
-
     m_Distribution = bestDist;
     return bestVal;
   }",Buggy,"Fixed division by zero bugs for numeric prediction.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@26 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states fixing division by zero issues, which aligns with the changes in the diff. Here's a detailed breakdown:

1. **Handling Division by Zero**: 
   - Previously, `bestDist[j][0] = m_Distribution[j][0] / sumOfWeights[j]` was executed without checking if `sumOfWeights[j]` was zero, which could lead to a division by zero exception.
   - The update adds a condition `if (!Utils.eq(sumOfWeights[j], 0))` before division. When `sumOfWeights[j]` is zero, it assigns `bestDist[j][0]` a default value based on `totalSum / totalSumOfWeights`, preventing division by zero.

2. **Total Sum Initialization**:
   - The variables `totalSum` and `totalSumOfWeights` are introduced to hold the total class value and weight sums respectively. This acts as a fallback default when division by zero would occur.
   
3. **Removed Unconditional Calculation for `meanNoMissing`**:
   - The line `meanNoMissing = m_Distribution[1][0] / sumOfWeights[1];` was removed since division is integrated into the loop with a zero-check. This is a direct measure to prevent a division by zero if `sumOfWeights[1]` is zero.

4. **Removal of Conditional Block**:
   - The condition checking `if (numMissing == 0)` and setting `bestDist[2][0] = meanNoMissing;` is removed, considering `bestDist` now handles divisions with zero-weight checks.

These changes focus on preventing the division by zero, which aligns with the commit's message to fix such a bug. Given these observations, the diff indeed fixes an issue related to division by zero, covered by the commit message. Thus, the conclusion is **Buggy**."
weka,20096.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,38 +1,40 @@
   public void performRequest(String request) {
     if (request.compareTo(""Show chart"") == 0) {
       try {
-	// popup visualize panel
-	if (!m_framePoppedUp) {
-	  m_framePoppedUp = true;
+        // popup visualize panel
+        if (!m_framePoppedUp) {
+          m_framePoppedUp = true;
 
-	  final javax.swing.JFrame jf = 
-	    new javax.swing.JFrame(""Model Performance Chart"");
-	  jf.setSize(800,600);
-	  jf.getContentPane().setLayout(new BorderLayout());
-	  jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
-	  jf.addWindowListener(new java.awt.event.WindowAdapter() {
-	      public void windowClosing(java.awt.event.WindowEvent e) {
-		jf.dispose();
-		m_framePoppedUp = false;
-	      }
-	    });
-	  jf.setVisible(true);
-	  m_popupFrame = jf;
-	} else {
-	  m_popupFrame.toFront();
-	}
+          final javax.swing.JFrame jf = new javax.swing.JFrame(
+              ""Model Performance Chart"");
+          jf.setSize(800, 600);
+          jf.getContentPane().setLayout(new BorderLayout());
+          jf.getContentPane().add(m_visPanel, BorderLayout.CENTER);
+          jf.addWindowListener(new java.awt.event.WindowAdapter() {
+            @Override
+            public void windowClosing(java.awt.event.WindowEvent e) {
+              jf.dispose();
+              m_framePoppedUp = false;
+            }
+          });
+          jf.setVisible(true);
+          m_popupFrame = jf;
+        } else {
+          m_popupFrame.toFront();
+        }
       } catch (Exception ex) {
-	ex.printStackTrace();
-	m_framePoppedUp = false;
+        ex.printStackTrace();
+        m_framePoppedUp = false;
       }
     } else if (request.equals(""Clear all plots"")) {
-        m_visPanel.removeAllPlots();
-        m_visPanel.validate(); m_visPanel.repaint();
-        m_visPanel = null;
-        m_masterPlot = null;
-        m_offscreenPlotData = null;
+      m_visPanel.removeAllPlots();
+      m_visPanel.validate();
+      m_visPanel.repaint();
+      m_visPanel = null;
+      m_masterPlot = null;
+      m_offscreenPlotData = null;
     } else {
       throw new IllegalArgumentException(request
-					 + "" not supported (Model Performance Chart)"");
+          + "" not supported (Model Performance Chart)"");
     }
   }",NotBuggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix related to error plots and their behavior upon receiving a ""VisualizableErrorEvent."" It describes an issue where the master plot, once set, was not updated with new events unless manually cleared via the GUI.

Examining the code changes:

1. **Behavioral Conditions**: The code checks if a visualization panel (`m_framePoppedUp`) is open; if not, it creates and displays a new frame for visualization. If the frame is already open, it brings it to the front. No logic here directly corresponds to updating the master plot with new data, merely managing UI display.

2. **Error & Exception Handling**: In both the old and new code, exceptions are printed to the stack trace, and `m_framePoppedUp` is marked as false. No change in actual error handling mechanisms was made.

3. **Clearing Plots**: When a ""Clear all plots"" request is received, the code nullifies certain fields (`m_visPanel`, `m_masterPlot`, `m_offscreenPlotData`) after removing all plots and repaints the `m_visPanel`. This aligns with the manual clearing described in the message but does not translate into handling updates with incoming `VisualizableErrorEvents`. No logic ensures that the master plot automatically updates without a manual reset.

4. **Conclusion**: Although the commit message suggests addressing a bug related to updating plots when receiving error events, the code diff does not show changes directly implementing a feature that ensures the master plot gets updated with new events.

The modifications mostly involve stylistic preferences, code formatting, or handling how plots are displayed or cleared in the GUI context, but nothing has been introduced or changed to handle the described bug of automatic updating of plots upon receiving events.

Thus, the result is **NotBuggy** since the diff doesn't implement the indicated bug fix as described."
weka,20081.json,f39f7d042b70f510b9721bbff0535ef0a3d348a8,"@@ -1,174 +1,179 @@
   public synchronized void acceptDataSet(VisualizableErrorEvent e) {
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     if (!GraphicsEnvironment.isHeadless()) {
       if (m_visPanel == null) {
         m_visPanel = new VisualizePanel();
       }
-      if (m_masterPlot == null) {
-        m_masterPlot = e.getDataSet();
-      }
+
+      m_masterPlot = e.getDataSet();
+
       try {
         m_visPanel.setMasterPlot(m_masterPlot);
       } catch (Exception ex) {
-        System.err.println(""Problem setting up visualization (ModelPerformanceChart)"");
+        System.err
+            .println(""Problem setting up visualization (ModelPerformanceChart)"");
         ex.printStackTrace();
       }
       m_visPanel.validate();
       m_visPanel.repaint();
     } else {
       m_headlessEvents = new ArrayList<EventObject>();
       m_headlessEvents.add(e);
     }
-    
+
     if (m_imageListeners.size() > 0 && !m_processingHeadlessEvents) {
       // configure the renderer (if necessary)
       setupOffscreenRenderer();
-     
-      m_offscreenPlotData = new ArrayList<Instances>();      
+
+      m_offscreenPlotData = new ArrayList<Instances>();
       Instances predictedI = e.getDataSet().getPlotInstances();
       if (predictedI.classAttribute().isNominal()) {
-        
+
         // split the classes out into individual series.
         // add a new attribute to hold point sizes - correctly
-        // classified instances get default point size (2); 
+        // classified instances get default point size (2);
         // misclassified instances get point size (5).
         // WekaOffscreenChartRenderer can take advantage of this
         // information - other plugin renderers may or may not
         // be able to use it
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
         newInsts.setClassIndex(predictedI.classIndex());
-        
+
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = 2; // default shape size
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
-        
+
         // predicted class attribute is always actualClassIndex - 1
         Instances[] classes = new Instances[newInsts.numClasses()];
         for (int i = 0; i < newInsts.numClasses(); i++) {
           classes[i] = new Instances(newInsts, 0);
           classes[i].setRelationName(newInsts.classAttribute().value(i));
         }
         Instances errors = new Instances(newInsts, 0);
         int actualClass = newInsts.classIndex();
         for (int i = 0; i < newInsts.numInstances(); i++) {
           Instance current = newInsts.instance(i);
-          classes[(int)current.classValue()].add((Instance)current.copy());
-          
+          classes[(int) current.classValue()].add((Instance) current.copy());
+
           if (current.value(actualClass) != current.value(actualClass - 1)) {
-            Instance toAdd = (Instance)current.copy();
-            
+            Instance toAdd = (Instance) current.copy();
+
             // larger shape for an error
             toAdd.setValue(toAdd.numAttributes() - 1, 5);
-            
+
             // swap predicted and actual class value so
             // that the color plotted for the error series
             // is that of the predicted class
             double actualClassV = toAdd.value(actualClass);
             double predictedClassV = toAdd.value(actualClass - 1);
             toAdd.setValue(actualClass, predictedClassV);
             toAdd.setValue(actualClass - 1, actualClassV);
-              
-            errors.add(toAdd);            
+
+            errors.add(toAdd);
           }
         }
-        
+
         errors.setRelationName(""Errors"");
         m_offscreenPlotData.add(errors);
-        
+
         for (int i = 0; i < classes.length; i++) {
           m_offscreenPlotData.add(classes[i]);
         }
-  
+
       } else {
         // numeric class - have to make a new set of instances
         // with the point sizes added as an additional attribute
         FastVector atts = new FastVector();
         for (int i = 0; i < predictedI.numAttributes(); i++) {
           atts.add(predictedI.attribute(i).copy());
         }
         atts.add(new Attribute(""@@size@@""));
-        Instances newInsts = new Instances(predictedI.relationName(),
-            atts, predictedI.numInstances());
+        Instances newInsts = new Instances(predictedI.relationName(), atts,
+            predictedI.numInstances());
 
         int[] shapeSizes = e.getDataSet().getShapeSize();
 
         for (int i = 0; i < predictedI.numInstances(); i++) {
           double[] vals = new double[newInsts.numAttributes()];
           for (int j = 0; j < predictedI.numAttributes(); j++) {
             vals[j] = predictedI.instance(i).value(j);
           }
           vals[vals.length - 1] = shapeSizes[i];
           Instance ni = new DenseInstance(1.0, vals);
           newInsts.add(ni);
         }
         newInsts.setRelationName(predictedI.classAttribute().name());
         m_offscreenPlotData.add(newInsts);
       }
-      
+
       List<String> options = new ArrayList<String>();
-      
+
       String additional = ""-color="" + predictedI.classAttribute().name()
-        + "",-hasErrors"";
+          + "",-hasErrors"";
       if (m_additionalOptions != null && m_additionalOptions.length() > 0) {
         additional += "","" + m_additionalOptions;
         try {
           additional = m_env.substitute(additional);
-        } catch (Exception ex) { }
-      }            
+        } catch (Exception ex) {
+        }
+      }
       String[] optionsParts = additional.split("","");
       for (String p : optionsParts) {
         options.add(p.trim());
       }
-      
-//      if (predictedI.classAttribute().isNumeric()) {
+
+      // if (predictedI.classAttribute().isNumeric()) {
       options.add(""-shapeSize=@@size@@"");
-//      }
-      
+      // }
+
       String xAxis = m_xAxis;
       try {
         xAxis = m_env.substitute(xAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String yAxis = m_yAxis;
       try {
         yAxis = m_env.substitute(yAxis);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       String width = m_width;
       String height = m_height;
       int defWidth = 500;
       int defHeight = 400;
       try {
         width = m_env.substitute(width);
         height = m_env.substitute(height);
-        
+
         defWidth = Integer.parseInt(width);
         defHeight = Integer.parseInt(height);
-      } catch (Exception ex) { }
-      
+      } catch (Exception ex) {
+      }
+
       try {
-        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth, defHeight, 
-            m_offscreenPlotData, xAxis, yAxis, options);
+        BufferedImage osi = m_offscreenRenderer.renderXYScatterPlot(defWidth,
+            defHeight, m_offscreenPlotData, xAxis, yAxis, options);
 
         ImageEvent ie = new ImageEvent(this, osi);
         notifyImageListeners(ie);
       } catch (Exception e1) {
         e1.printStackTrace();
-      }      
+      }
     }
   }",Buggy,"Fixed a bug for error plots where once the master plot had been set it did not get changed for subsequent incoming VisualizableErrorEvents (unless manually clearing all plots via the GUI).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9452 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The provided git diff and commit message suggest a bug fix related to updating plots based on new `VisualizableErrorEvent`s. Initially, once the `m_masterPlot` was set, it was not updated with new events unless manually reset, causing a logic bug. Now, every time `acceptDataSet` is called, `m_masterPlot` is always set to `e.getDataSet()`, ensuring that new data is reflected without manual intervention. This change aligns with the commit message, indicating it addresses the described bug. Therefore, the conclusion is **Buggy**."
weka,18379.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,44 +1,52 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
        // KOML?
        if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
           KOML.write(expFile.getAbsolutePath(), m_Exp);
        }
        else
        // XML?
        if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
           XMLExperiment xml = new XMLExperiment(); 
           xml.write(expFile, m_Exp);
        }
        // binary
        else {
           FileOutputStream fo = new FileOutputStream(expFile);
           ObjectOutputStream oo = new ObjectOutputStream(
                                   new BufferedOutputStream(fo));
           oo.writeObject(m_Exp);
           oo.close();
        }
       
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates bug fixes related to GUI changes. The code changes focus on ensuring that the correct file extension is appended based on the file filter selected in a file chooser dialog. This change improves the logic by accurately appending the appropriate file extension determined by the current filter. The previous implementation appended extensions based on file name checks for multiple extensions at once rather than specifically tailoring them to the chosen filter.

By adjusting the logic to apply extensions precisely related to the selected filter, the code fixes potential bugs where incorrect file extensions might have been appended, leading to issues when saving files. These changes directly correlate with the GUI-related changes outlined in the commit message.

Therefore, this change does indeed align with fixing a bug related to GUI functionalities as described in the commit message. Thus, the conclusion is **Buggy**."
weka,8611.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,31 +1,29 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
     
     double [] probs = new double[instance.numClasses()];
     int attIndex;
     
     for (int j = 0; j < instance.numClasses(); j++) {
       probs[j] = 1;
       Enumeration enumAtts = instance.enumerateAttributes();
       attIndex = 0;
       while (enumAtts.hasMoreElements()) {
 	Attribute attribute = (Attribute) enumAtts.nextElement();
 	if (!instance.isMissing(attribute)) {
 	  if (attribute.isNominal()) {
 	    probs[j] *= m_Counts[j][attIndex][(int)instance.value(attribute)];
 	  } else {
 	    probs[j] *= normalDens(instance.value(attribute),
 				   m_Means[j][attIndex],
 				   m_Devs[j][attIndex]);}
 	}
 	attIndex++;
       }
-      System.out.println(probs[j] + "" "");
       probs[j] *= m_Priors[j];
     }
-    System.out.println();
 
     // Normalize probabilities
     Utils.normalize(probs);
 
     return probs;
   }",NotBuggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions bug fixes related to GUI additions and removal of superfluous output in NaiveBayesSimple. The diff shows the removal of `System.out.println()`, which eliminates unnecessary console output but doesn't impact the logical correctness of the program. It simply makes the code cleaner and avoids cluttering the output, which aligns with the ""eliminated some superfluous output"" portion of the commit message. However, this isn't indicative of a bug fix related to program behavior or correctness.

The commit also mentions bug fixes from Peter regarding the latest GUI additions, but this diff doesn't include any changes that suggest UI-related bug fixes. It's strictly about removing console print statements.

Therefore, while the changes do align with the commit message about output removal, they do not demonstrate a bug fix in terms of logic or functionality in the code provided. Thus, I conclude with **NotBuggy**."
weka,18418.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,47 +1,55 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       setExperiment(exp);
       System.err.println(""Opened experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions ""bug fixes"" and ""eliminated some superfluous output."" This diff specifically addresses a GUI-related issue by refining how file extensions are managed when opening experiment files.

Here's a breakdown of the changes:

1. Previous Logic: The code originally used a compound condition to verify if the selected file had one of the expected extensions (`.exp`, `.koml`, or `.xml`). If none of these matched, it would append the `.exp` extension by default.

2. Updated Logic: The updated code now separately checks which filter is active in the file chooser (`m_ExpFilter`, `m_KOMLFilter`, or `m_XMLFilter`). Based on the active filter, it appends the corresponding file extension if it is not present.

3. Error-Handling: The try-catch block remains unchanged, but it includes an error message dialog box for robustness, to alert the user if the experiment file cannot be opened. This is not a change, but it's relevant for bug-related insights because it clarifies how errors are handled.

4. Superfluous Output: The commit message mentions the removal of superfluous output from NaiveBayesSimple. While the provided diff does not cover this part, it's important to recognize that the message is a general overview that could pertain to multiple files.

The changes in the diff align with the commit message's indication of bug fixes, specifically related to GUI functionality (file handling in this case). The adjustments ensure that the correct file type is being attempted to open, which is indeed a common source of user-related bugs in GUI applications.

Thus, the modifications in this Java method do indicate a bug fix as they specifically address issues concerning file extension handling with respect to the selected filter. Therefore, the changes are consistent with addressing a bug, and the conclusion is **Buggy**."
weka,18378.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,50 +1,58 @@
   private void openExperiment() {
     
     int returnVal = m_FileChooser.showOpenDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-            || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-            || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-      expFile = new File(expFile.getParent(), expFile.getName()
-                         + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       Experiment exp; 
       
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          exp = (Experiment) KOML.read(expFile.getAbsolutePath());
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          exp = (Experiment) xml.read(expFile);
       }
       // binary
       else {
          FileInputStream fi = new FileInputStream(expFile);
          ObjectInputStream oi = new ObjectInputStream(
                                 new BufferedInputStream(fi));
          exp = (Experiment)oi.readObject();
          oi.close();
       }
       
       if (!setExperiment(exp)) {
 	if (m_modePanel != null) m_modePanel.switchToAdvanced(exp);
       }
       System.err.println(""Opened experiment:\n"" + exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't open experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Open Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
       // Pop up error dialog
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message refers to several bug fixes in GUI additions along with removing unnecessary output. Let's analyze the provided code diff for indicators of bug fixes related to GUI additions:

1. **File Extension Checking**: The original code added a file extension based on a compound condition mixing all possible formats (default, KOML, XML). The new code separates these checks into specific if-else blocks aligned with the selected file filter (m_ExpFilter, m_KOMLFilter, m_XMLFilter). This aligns the extension logic with user-selected filters, leading to more accurate behaviors for file extension processing during file selection operations.

2. **Logic and Clarity Improvements**: Handling these conditions separately ensures that only the relevant file extension is appended when missing, making it less error-prone and more aligned with expected user actions. This is likely a correction to earlier faulty behavior.

3. **Validation and Processing**: Beyond mere restructuring, the change ensures only the correct file extensions are added when saving files, preventing potential ""file not found"" or ""unsupported file type"" errors when opening files later. This counts as a bug fix because it corrects potentially incorrect file-handling scenarios leading to runtime errors or user confusion.

Given these analysis points, the change aligns with the commit message's claim of fixing bugs in the GUI area. Hence, this corresponds to **Buggy**."
weka,18419.json,41f296c55b87a1ead276fffbafb67535b858a70f,"@@ -1,43 +1,51 @@
   private void saveExperiment() {
 
     int returnVal = m_FileChooser.showSaveDialog(this);
     if (returnVal != JFileChooser.APPROVE_OPTION) {
       return;
     }
     File expFile = m_FileChooser.getSelectedFile();
-    if ( !(    (expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
-          || (KOML.isPresent() && expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
-          || (expFile.getName().toLowerCase().endsWith("".xml"")) ) )
-    {
-       expFile = new File(expFile.getParent(), expFile.getName()
-                          + Experiment.FILE_EXTENSION);
+    
+    // add extension if necessary
+    if (m_FileChooser.getFileFilter() == m_ExpFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(Experiment.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + Experiment.FILE_EXTENSION);
     }
+    else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION))
+        expFile = new File(expFile.getParent(), expFile.getName() + KOML.FILE_EXTENSION);
+    }
+    else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
+      if (!expFile.getName().toLowerCase().endsWith("".xml""))
+        expFile = new File(expFile.getParent(), expFile.getName() + "".xml"");
+    }
+    
     try {
       // KOML?
       if ( (KOML.isPresent()) && (expFile.getAbsolutePath().toLowerCase().endsWith(KOML.FILE_EXTENSION)) ) {
          KOML.write(expFile.getAbsolutePath(), m_Exp);
       }
       else
       // XML?
       if (expFile.getAbsolutePath().toLowerCase().endsWith("".xml"")) {
          XMLExperiment xml = new XMLExperiment(); 
          xml.write(expFile, m_Exp);
       }
       // binary
       else {
          FileOutputStream fo = new FileOutputStream(expFile);
          ObjectOutputStream oo = new ObjectOutputStream(
                                  new BufferedOutputStream(fo));
          oo.writeObject(m_Exp);
          oo.close();
       }
       System.err.println(""Saved experiment:\n"" + m_Exp);
     } catch (Exception ex) {
       ex.printStackTrace();
       JOptionPane.showMessageDialog(this, ""Couldn't save experiment file:\n""
 				    + expFile
 				    + ""\nReason:\n"" + ex.getMessage(),
 				    ""Save Experiment"",
 				    JOptionPane.ERROR_MESSAGE);
     }
   }",Buggy,"A few bug fixes from Peter regarding the latest GUI additions. Also eliminated some superfluous output from NaiveBayesSimple.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2259 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message suggests bug fixes in the GUI, while the diff shows an update adding specific conditions for handling file extensions based on the selected file filter. The previous logic checked for extensions universally, introducing potential errors when a file filter did not match its condition and appended incorrect extensions. The new logic correctly applies the file extension only when corresponding file filters are selected, reducing errors in experiment saving, particularly user error or oversight in selecting file types. Additionally, explicit file filter checks prevent unexpected extension mismatch errors, aligning well with bug fix objectives.

Let's break down the changes:
- Initially, the code appends batch extensions depending on file's name ending, causing probable erroneous extensions for the wrong filter type.
- The revised code applies targeted extension appending, improving extension management by separating checks for each file type filter (m_ExpFilter, m_KOMLFilter, and m_XMLFilter).

These changes improve the accuracy and predictability of the `saveExperiment` function's behavior regarding file names and types, indicating a bug-related fix.

Thus, the review concludes with **Buggy**, as it aligns with the commit message about addressing bugs."
weka,24330.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,15 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    // Tell the resultproducer to send results to us
+    m_ResultProducer.setResultListener(this);
+    findKeyIndex();
+    if (m_KeyIndex == -1) {
+      throw new Exception(""No key field called "" + m_KeyFieldName
+			  + "" produced by ""
+			  + m_ResultProducer.getClass().getName());
+    }
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to the connection of a sub-ResultProducer. The code change confirms this, as it includes a new line `m_ResultProducer.setResultListener(this);` that establishes a connection by setting the current instance as the result listener for `m_ResultProducer`. Additionally, the update includes checking for a valid key index and throwing an exception if it is not found, which would likely prevent downstream errors linked to misconfiguration or the lack of proper setup. These changes directly align with fixing the connection bug mentioned in the commit message. Therefore, the modifications are effectively addressing the stated bug.

**Buggy**"
weka,24356.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,5 +1,6 @@
   public void setResultProducer(ResultProducer newResultProducer) {
-    
+
     m_ResultProducer = newResultProducer;
+    m_ResultProducer.setResultListener(this);
     findKeyIndex();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix where a sub-ResultProducer was not correctly connected. The code modification aligns with this message. The diff shows the addition of a line that sets the current object as the result listener of the ResultProducer, which ensures it is properly connected. This is likely addressing the mentioned bug. The answer is **Buggy**."
weka,24326.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,71 +1,63 @@
   protected void doAverageResult(Object [] template) throws Exception {
 
     // Generate the key and ask whether the result is required
     String [] newKey = new String [template.length - 1];
     System.arraycopy(template, 0, newKey, 0, m_KeyIndex);
     System.arraycopy(template, m_KeyIndex + 1,
 		     newKey, m_KeyIndex,
 		     template.length - m_KeyIndex - 1);
-    /*
-    System.err.println(""new key: "" + DatabaseUtils.arrayToString(newKey));
-    */
     if (m_ResultListener.isResultRequired(this, newKey)) {
       Object [] resultTypes = m_ResultProducer.getResultTypes();
       Stats [] stats = new Stats [resultTypes.length];
       for (int i = 0; i < stats.length; i++) {
 	stats[i] = new Stats();
       }
       Object [] result = getResultTypes();
       int numMatches = 0;
       for (int i = 0; i < m_Keys.size(); i++) {
 	Object [] currentKey = (Object [])m_Keys.elementAt(i);
 	// Skip non-matching keys
 	if (!matchesTemplate(template, currentKey)) {
 	  continue;
 	}
 	// Add the results to the stats accumulator
 	Object [] currentResult = (Object [])m_Results.elementAt(i);
 	numMatches++;
-	/*
-	System.err.println(""Match: "" + DatabaseUtils.arrayToString(currentKey)
-			   + "" -- ""
-			   + DatabaseUtils.arrayToString(currentResult));
-	*/
 	for (int j = 0; j < resultTypes.length; j++) {
 	  if (resultTypes[j] instanceof Double) {
 	    if (currentResult[j] == null) {
 	      throw new Exception(""Null numeric result field found:\n""
 				  + DatabaseUtils.arrayToString(currentKey)
 				  + "" -- ""
 				  + DatabaseUtils
 				  .arrayToString(currentResult));
 	    }
 	    double currentVal = ((Double)currentResult[j]).doubleValue();
 	    stats[j].add(currentVal);
 	  }
 	}
       }
       if (numMatches != m_ExpectedResultsPerAverage) {
 	throw new Exception(""Expected "" + m_ExpectedResultsPerAverage
 			    + "" results matching key \""""
 			    + DatabaseUtils.arrayToString(template)
 			    + ""\"" but got ""
 			    + numMatches);
       }
       result[0] = new Double(numMatches);
       Object [] currentResult = (Object [])m_Results.elementAt(0);
       int k = 1;
       for (int j = 0; j < resultTypes.length; j++) {
 	if (resultTypes[j] instanceof Double) {
 	  stats[j].calculateDerived();
 	  result[k++] = new Double(stats[j].mean);
 	  if (getCalculateStdDevs()) {
 	    result[k++] = new Double(stats[j].stdDev);
 	  }
 	} else {
 	  result[k++] = currentResult[j];
 	}
       }
       m_ResultListener.acceptResult(this, newKey, result);      
     }
   }",NotBuggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states that a bug where a sub-ResultProducer sometimes wouldn't connect correctly was fixed. However, the code diff shows the removal of commented-out debug print statements, which have no effect on functionality. There is no evidence here of an actual bug fix or change in how the `sub-ResultProducer` connects. Since the primary change involves cleanup of debug comments, it does not align with the commit message indicating a bug fix.

Without changes addressing connection logic or any error-prone parts of the code that might have contributed to a connectivity issue, the code modifications shown do not conclusively support the commit message's claim regarding fixing a connection bug.

Therefore, the conclusion is that the commit does not involve a bug fix related to the stated connection issue with sub-ResultProducer, so the verdict is **NotBuggy**."
weka,24324.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,33 +1,31 @@
   public void doRun(int run) throws Exception {
-
+    
     if (m_Instances == null) {
       throw new Exception(""No Instances set"");
     }
-    // Tell the resultproducer to send results to us
-    m_ResultProducer.setResultListener(this);
     m_ResultProducer.setInstances(m_Instances);
-    
+
     // Clear the collected results
     m_Keys.removeAllElements();
     m_Results.removeAllElements();
     
     //    System.err.println(""Starting run "" + run);
     // Collect all the results
     // Should be smarter -- first doing a run collecting only the keys
     // from isResultRequired(), then determining if the average is required,
     // then getting the actual results.
     m_ResultProducer.doRun(run);
 
     // Average the results collected
     //System.err.println(""Number of results collected: "" + m_Keys.size());
 
     // Check that the keys only differ on the selected key field
     checkForMultipleDifferences();
 
     Object [] template = (Object [])((Object [])m_Keys.elementAt(0)).clone();
     template[m_KeyIndex] = null;
     // Check for duplicate keys
     checkForDuplicateKeys(template);
     // Calculate the average and submit it if necessary
     doAverageResult(template);
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message notes a bug regarding the incorrect connection of a sub-ResultProducer. In the diff, the call to `m_ResultProducer.setResultListener(this);` was removed. This line is critical for ensuring that results from `m_ResultProducer` are sent to the current object (acting as a listener). Removing this line without any replacement or adjustment seems contrary to the intent described in the message, which indicates a fix for better connection handling, not removal.

The current modification seems like it may cause the issue the commit message claims to solve, unless `setResultListener(this)` was incorrectly placed outside the context it should belong. However, based only on the provided diff, there's no alternative connection mechanism added or corrected.

In this review, the change seems to inadvertently contradict the commit's intent. However, without additional context or further confirmation, especially if replacement code isn't visible, assessing for a bug fix solely based on the demonstrated diff is challenging.

Because it doesn't directly show signs of a bug fix, and the removal of key connection logic appears suspicious without additional supportive modifications, this initial assessment would conclude as **NotBuggy**, given the risk of breaking existing functionality outweighs the purported fix claim solely based on this diff."
weka,24382.json,0613647d320672d37f6da68efd32bd14bb88435b,"@@ -1,7 +1,8 @@
   public void preProcess() throws Exception {
     
     if (m_ResultProducer == null) {
       throw new Exception(""No ResultProducer set"");
     }
+    m_ResultProducer.setResultListener(this);
     m_ResultProducer.preProcess();
   }",Buggy,"- Fixed bug where sometimes the sub-ResultProducer would not be correctly
connected.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@171 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix related to the sub-ResultProducer's connection. The diff adds a call to `m_ResultProducer.setResultListener(this);` before calling `m_ResultProducer.preProcess();`. 

1. **Understanding the Context**: The method `preProcess()` throws an exception if `m_ResultProducer` is null, which means `m_ResultProducer` is an important object that must be properly initialized and configured before `preProcess()` runs.

2. **Modification Analysis**: The new line `m_ResultProducer.setResultListener(this);` sets a result listener to the current object (`this`) on the `m_ResultProducer`. This modification suggests that the `ResultProducer` required a listener to be set for its operation, a step that might have been previously missed.

3. **Logical Correction**: This line would ensure that the `m_ResultProducer` is correctly connected to a result listener before its `preProcess()` method is executed. The absence of this step may have led to errors or incomplete operations, which aligns with the commit message stating that it fixes a bug regarding the connection of the sub-ResultProducer.

4. **Conclusion**: Setting the listener appears to be crucial for the correct operation of the sub-ResultProducer, and omitting it had likely caused a bug where results were not produced or handled as expected.

Given the circumstances and the stated intention in the commit message, the change clearly addresses a specific bug related to proper operation of a component of the system.

**Buggy**"
weka,19514.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,58 +1,66 @@
   public boolean eventGeneratable(String eventName) {
     if (eventName.compareTo(""trainingSet"") == 0) { 
       if (m_trainingProvider == null) {
 	return false;
       } else {
 	if (m_trainingProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_trainingProvider).
 	      eventGeneratable(""trainingSet"")) {
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""dataSet"") == 0) { 
       if (m_dataProvider == null) {
-	m_connectedFormat = null;
-	notifyDataFormatListeners();
+	if (m_instanceProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_dataProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_dataProvider).
 	      eventGeneratable(""dataSet"")) {
 	    m_connectedFormat = null;
 	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""instance"") == 0) { 
       if (m_instanceProvider == null) {
+	if (m_dataProvider == null) {
+	  m_connectedFormat = null;
+	  notifyDataFormatListeners();
+	}
 	return false;
       } else {
 	if (m_instanceProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_instanceProvider).
 	      eventGeneratable(""instance"")) {
+	    m_connectedFormat = null;
+	    notifyDataFormatListeners();
 	    return false;
 	  }
 	}
       }
     }
 
     if (eventName.compareTo(""testSet"") == 0) {
       if (m_testProvider == null) {
 	return false;
       } else {
 	if (m_testProvider instanceof EventConstraints) {
 	  if (!((EventConstraints)m_testProvider).
 	      eventGeneratable(""testSet"")) {
 	    return false;
 	  }
 	}
       }
     }
     return true;
   }",Buggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a format handling bug fix, and the changes in the diff match this context. Before changes, `m_connectedFormat` was reset, and listeners were notified only if `m_dataProvider` was null for ""dataSet"" events. The change expands the conditions, ensuring this reset and notification also occur when `m_instanceProvider` is null. The added logic for ""instance"" events mirrors the handling of ""dataSet"" events. This suggests the original code may have had incorrect behavior in certain scenarios, leaving `m_connectedFormat` improperly set. Thus, the commit addresses a bug in format handling by ensuring `m_connectedFormat` is reset as necessary for both ""dataSet"" and ""instance"" events in the absence of required providers. The answer is **Buggy**."
weka,19490.json,90f4455c8174476ac55591cfcab3ea403fa3db02,"@@ -1,17 +1,18 @@
   public void acceptInstance(InstanceEvent e) {
     if (e.getStatus() == InstanceEvent.FORMAT_AVAILABLE) {
       //      Instances dataSet = e.getInstance().dataset();
       m_connectedFormat = e.getStructure();
       
       //      System.err.println(""Assigning class column..."");
       assignClass(m_connectedFormat);
       notifyInstanceListeners(e);
 
       // tell any listening customizers (or other interested parties)
+      System.err.println(""Notifying customizer..."");
       notifyDataFormatListeners();
     } else {
       //      Instances dataSet = e.getInstance().dataset();
       //      assignClass(dataSet);
       notifyInstanceListeners(e);
     }
   }",NotBuggy,"Fixed bug in format handling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1966 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit mentions a bug fix in format handling. The diff shows minor code changes, including adding a logging statement: `System.err.println(""Notifying customizer..."");` This doesn't directly influence any logical or algorithmic behavior related to handling formats, but it's useful for debugging or informational purposes. The rest of the code remains unchanged, and there's no visible correction to any existing bug in the handling logic itself based on this snippet alone.

Since adding logging doesn't inherently fix a bug, and no other changes affect the handling, this specific diff doesn't clearly demonstrate a bug fix in the format handling process. Therefore, my assessment based on this snippet is **NotBuggy**."
weka,25728.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,35 +1,37 @@
     protected void performBoostingCV() throws Exception{			
 	
 	//completed iteration keeps track of the number of iterations that have been
 	//performed in every fold (some might stop earlier than others). 
 	//Best iteration is selected only from these.
 	int completedIterations = m_maxIterations;
 	
 	Instances allData = new Instances(m_train);
 	
 	allData.stratify(m_numFoldsBoosting);	      
 
 	double[] error = new double[m_maxIterations + 1];	
 	
+        SimpleLinearRegression[][] backup = m_regressions;
+
 	for (int i = 0; i < m_numFoldsBoosting; i++) {
 	    //split into training/test data in fold
 	    Instances train = allData.trainCV(m_numFoldsBoosting,i);
 	    Instances test = allData.testCV(m_numFoldsBoosting,i);
 
 	    //initialize LogitBoost
 	    m_numRegressions = 0;
-	    m_regressions = initRegressions();
+	    m_regressions = copyRegressions(backup);
 
 	    //run LogitBoost iterations
 	    int iterations = performBoosting(train,test,error,completedIterations);	    
 	    if (iterations < completedIterations) completedIterations = iterations;	    
 	}
 
 	//determine iteration with minimum error over the folds
 	int bestIteration = getBestIteration(error,completedIterations);
 
 	//rebuild model on all of the training data
 	m_numRegressions = 0;
-        m_regressions = initRegressions();
+        m_regressions = backup;
 	performBoosting(bestIteration);
     }",Buggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to cross-validation being performed locally at each node. The diff shows changes to how the regression model (`m_regressions`) is handled in the `performBoostingCV` method.

### Analysis of Code Changes:

1. **Backup of Regressions:**
   - A backup of `m_regressions` is created before the cross-validation loop: `SimpleLinearRegression[][] backup = m_regressions;`.
   - This suggests that the existing state of `m_regressions` is now preserved across cross-validation folds, which was not the case before.

2. **Initialization vs. Copy:**
   - **Before:** `m_regressions = initRegressions();`
     - This indicates `m_regressions` was re-initialized at the start of each fold.
   - **After:** `m_regressions = copyRegressions(backup);`
     - This change implies that instead of re-initializing from scratch, the code now copies the initial state stored in `backup`.

3. **Restoring Regressions Post-CV:**
   - **After the Loop:** `m_regressions = backup;`
     - This ensures the initial state of `m_regressions`, stored in `backup`, is restored after boosting and cross-validation are completed.
   - Previously, `m_regressions` would be reinitialized again post-CV.

### Relevance to Commit Message:

- The changes directly relate to managing state or configuration specific to nodes during cross-validation, aligning with the commit message which refers to fixing behavior where cross-validation is performed locally at each node.
- By using a backup and restoring it, this change seems to ensure consistent state management across folds and after the cross-validation process is complete, which likely resolves unintended behavior when models are trained node-by-node.

Given these observations, the changes in the diff do address a specific handling of state across cross-validation iterations, aligning well with the commit message's indication of a ""bug fix"" for cross-validation performed locally. Therefore, this is indeed a bug fix.

**Conclusion: Buggy**"
weka,25729.json,296933fb5c56e6b698f0f76c710c4ef61eaa9a3c,"@@ -1,22 +1,13 @@
     protected SimpleLinearRegression[][] copyRegressions(SimpleLinearRegression[][] a)	
         throws Exception {
     
         SimpleLinearRegression[][] result = initRegressions();
         for (int i = 0; i < a.length; i++) {
             for (int j = 0; j < a[i].length; j++) {
                 if (j != m_numericDataHeader.classIndex()) {
                     result[i][j].addModel(a[i][j]);
                 }
             }
         }
-
-        /*        SimpleLinearRegression[][] result = null;
-        try {
-            SerializedObject so = new SerializedObject(a);
-            result = (SimpleLinearRegression[][])so.getObject();
-        } catch (Exception ex) {
-            System.err.println(""Can't copy array of simple linear regression objects."");
-            System.err.println(ex);
-            }*/
         return result;
     }",NotBuggy,"Bug fix for the case where cross-validation is performed locally at each node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions a bug fix related to cross-validation performed locally at each node. The code changes in the Git diff indicate the removal of old commented-out code for serializing and deserializing an array of `SimpleLinearRegression` objects.

Here's a step-by-step analysis:

1. **Current Functionality**: The method `copyRegressions` initializes a new 2D array of `SimpleLinearRegression` objects and copies models from the input array `a` into this new array, skipping the column matching the class index.

2. **Removed Code**: The removed block was commented-out code that attempted to serialize and then deserialize the array `a` to copy it. This method was commented out, suggesting it was an old or deprecated way to achieve the copying.

3. **No Functional Change**: The active code itself did not change, only commented-out lines were removed. This removal does not affect the current Git working version of the method.

4. **Relevance to Bug Fix**: The commit message and code changes do not appear directly related because the existing code logic remains unchanged. The change is more about cleanliness and possibly preventing confusion regarding how regression objects should be copied.

Despite not impacting functionality, removing old and irrelevant commentary could indirectly relate to ensuring clarity and correctness during cross-validation operations. However, since the operational code remained, it does not present a direct bug fix within the logic but rather improves code maintainability.

Thus, based on the current working version of code logic, I would conclude that while the commit's intent is aligned with improving code quality, it's not directly fixing a logical bug in terms of the diff provided.

Therefore, the appropriate conclusion is **NotBuggy**."
weka,20414.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,15 +1,15 @@
   protected void newStructure() throws Exception {
     
     m_Loader.reset();
     
     // Set environment variables
     if (m_Loader instanceof EnvironmentHandler && m_env != null) {
       try {
         ((EnvironmentHandler)m_Loader).setEnvironment(m_env);
       }catch (Exception ex) {
       }
     }
     m_dataFormat = m_Loader.getStructure();
-//    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
-//    notifyStructureAvailable(m_dataFormat);
+    System.out.println(""[Loader] Notifying listeners of instance structure avail."");
+    notifyStructureAvailable(m_dataFormat);
   }",NotBuggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code involves uncommenting two lines. Originally, these lines were commented out and thus not executed, which prevented the system from notifying listeners about the availability of new data structures. By uncommenting these lines, the `notifyStructureAvailable(m_dataFormat);` method is now called as intended, ensuring that any changes in configuration or data structure are communicated to downstream processes.

This modification directly aligns with the commit message, which states a bug fix related to configuration changes not being sent to downstream steps. The uncommented lines resolve this issue, thereby fixing the bug.

Thus, the change is categorized as a **Buggy** because it fixes an existing issue in the system's behavior."
weka,20407.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,4 +1,11 @@
   public void setDB(boolean flag){
   
       m_dbSet = flag;
+      if (m_dbSet) {
+        try {
+          newStructure();
+        } catch (Exception e) {
+          e.printStackTrace();
+        }
+      }
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug was fixed where configuration changes were not being sent to downstream steps. In the provided code, a change was made to the `setDB` method. 

The original code simply sets the `m_dbSet` flag. The updated code now includes a condition that checks if `m_dbSet` is true, and if so, it attempts to create a new structure by calling `newStructure()`. If an exception occurs during this process, it gets caught and printed to the error output. This suggests that the previous version of the code failed to invoke necessary updates or changes when the database configuration was modified.

This change introduces a mechanism to ensure additional operations (through `newStructure()`) are invoked when the database configuration is set, which aligns with the commit message's description of fixing configuration change propagation. As this change involves handling a specific situation where an important update wasn't being processed, and includes exception handling, it addresses a functional bug.

**Buggy**"
weka,19057.json,d5169a182f63b31a737d9b5d57ecdb49b8fbd56a,"@@ -1,6 +1,41 @@
   private void setUpOther() {
     removeAll();
     add(m_LoaderEditor, BorderLayout.CENTER);
+    
+    JPanel buttonsP = new JPanel();
+    buttonsP.setLayout(new FlowLayout());
+    JButton ok,cancel;
+    buttonsP.add(ok = new JButton(""OK""));
+    buttonsP.add(cancel=new JButton(""Cancel""));
+    ok.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under an OK condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingOK();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    cancel.addActionListener(new ActionListener(){
+      public void actionPerformed(ActionEvent evt){
+        
+        // Tell the editor that we are closing under a CANCEL condition
+        // so that it can pass on the message to any customizer that
+        // might be in use
+        m_LoaderEditor.closingCancel();
+        
+        if (m_parentWindow != null) {
+          m_parentWindow.dispose();
+        }
+      }
+    });
+    
+    add(buttonsP, BorderLayout.SOUTH);
+    
     validate();
     repaint();
   }",Buggy,"Fixed a bug where configuration changes (new files selected or database configuration changed) was not getting sent to downstream steps

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states a fix was implemented for a bug concerning configuration changes not being sent downstream. However, the provided code diff does not directly address this issue. Instead, the update introduces a graphical user interface (GUI) component with ""OK"" and ""Cancel"" buttons, alongside event handlers for user interactions. These changes allow the GUI to handle dialog closures appropriately but do not directly address the issue of sending configuration changes downstream.

While these buttons and their handlers might be part of facilitating configuration changes, the direct problem of tracking and sending configurations to downstream steps isn't evident in this modification. The focus here is on GUI enhancements rather than bug fixes directly related to configuration data flow.

Therefore, given the content of this particular code change, the connection to the described bug fix in the commit message is not apparent. Thus, the conclusion is **NotBuggy**."
titan,4498.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,4 +1,4 @@
         public Builder set(String path, Object value) {
-            super.set(path, value);
+            writeConfiguration.set(path, value);
             return this;
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",NotBuggy,"The commit message discusses a change in TitanFactory.Builder to accept arbitrary keys by using WriteConfiguration instead of ModifiableConfiguration, removing validation constraints. The diff matches this description, as `set` now manipulates a `writeConfiguration` object using dumb strings instead of a method enforcing validation. The change supports Elasticsearch's flexible configuration requirements for reserved keys, as the commit message explains.

There's no indication of a bug fix; rather, the changes render the API more adaptable and extensible. Therefore, the answer is **NotBuggy**."
titan,353.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,16 +1,16 @@
     public Map<ConfigElement.PathIdentifier,Object> getAll() {
         Map<ConfigElement.PathIdentifier,Object> result = Maps.newHashMap();
 
         for (String key : config.getKeys("""")) {
             Preconditions.checkArgument(StringUtils.isNotBlank(key));
             try {
                 ConfigElement.PathIdentifier pid = ConfigElement.parse(getRootNamespace(),key);
                 Preconditions.checkArgument(pid.element.isOption() && !pid.lastIsUmbrella);
                 result.put(pid,get((ConfigOption)pid.element,pid.umbrellaElements));
             } catch (IllegalArgumentException e) {
-                log.info(""Ignored configuration entry for {} since it does not map to an option"",key,e);
+                log.debug(""Ignored configuration entry for {} since it does not map to an option"",key,e);
                 continue;
             }
         }
         return result;
     }",NotBuggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",NotBuggy,"The commit message discusses changing the TitanFactory.Builder to accept arbitrary keys by using a less restrictive configuration method. This change allows Elasticsearch configurations, which can include arbitrary options, to be integrated into Titan's configuration system without validation issues.

Analyzing the code change:
- The code modification downgrades the logging from `log.info` to `log.debug`. This prevents unnecessary information logs for configurations that do not map to a known configuration option, which are expected due to the new allowance for arbitrary keys with Elasticsearch.

This change aligns with the description in the commit message about accepting arbitrary keys without them being validated against a predefined set of options. While this is a functional modification, it is not directly indicative of a bug fix. Instead, it reflects an adjustment in logging severity to accommodate the new system design, which now accepts configurations that might not conventionally map directly to a defined option.

Since the primary objective is to allow arbitrary keys and manage their logging more appropriately rather than correcting a bug, the conclusion for this change is **NotBuggy**."
titan,4499.json,3b4dd304515746e895b4ea7cd1728c0bcfd5d460,"@@ -1,3 +1,5 @@
         public TitanGraph open() {
-            return TitanFactory.open(super.getConfiguration());
+            ModifiableConfiguration mc = new ModifiableConfiguration(GraphDatabaseConfiguration.ROOT_NS,
+                    writeConfiguration.copy(), BasicConfiguration.Restriction.NONE);
+            return TitanFactory.open(mc);
         }",Buggy,"Let TitanFactory.Builder accept arbitrary keys

Elasticsearch reserves a Titan config namespace under which the user
may provide arbitrary Elasticsearch config options.  These options
can't be validated, since it's fundamentally a bit of a perversion of
the config API: the keys are defined in and controlled by
Elasticsearch, but we're cramming them into a Titan configuration.

This commit tweaks TitanFactory.Builder to use a WriteConfiguration
(which is based on dumb strings) instead of a ModifiableConfiguration
(which is based on ConfigOption and includes validation).

A more elegant way to approach this might be to introduce an
abstraction to support Elasticsearch's reserved namespace: some kind
of boolean field that says, in effect, ""don't try to validate any keys
underneath me"".  That still wouldn't totally address the problem,
though; (User)ModifiableConfiguration still wouldn't be a suitable
basis for TitanFactory.Builder, since it requires that any
user-provided config key string map to a ConfigOption.

This commit also adds a test (that fails without the TitanFactory
change).

Fixes #1114
",NotBuggy,"The commit message states that the change allows TitanFactory.Builder to accept arbitrary keys by switching to a WriteConfiguration. The modification in the diff aligns with this intent by using a ModifiableConfiguration with unrestricted validation. This isn't a bug fix but an enhancement to handle more configuration flexibility for Elasticsearch. Therefore, the changes don't indicate a bug fix, as they're about expanding the configuration capability rather than correcting an error.

Thus, I conclude the analysis with **NotBuggy**."
spring-boot,5691.json,abe3d3852761430c9f5f1368d82509d0c19ac4d9,"@@ -1,12 +1,11 @@
 	private Stream<Wrapper> getLoadOnStartupWrappers(Container[] children) {
 		Map<Integer, List<Wrapper>> grouped = new TreeMap<>();
 		for (Container child : children) {
 			Wrapper wrapper = (Wrapper) child;
 			int order = wrapper.getLoadOnStartup();
 			if (order >= 0) {
-				grouped.computeIfAbsent(order, ArrayList::new);
-				grouped.get(order).add(wrapper);
+				grouped.computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper);
 			}
 		}
 		return grouped.values().stream().flatMap(List::stream);
 	}",Buggy,"Fix OoM error when starting Tomcat with max int load on startup

Fixes gh-17927
",Buggy,"The commit message indicates a bug fix related to an Out of Memory (OoM) error when starting Tomcat with a maximum integer load on startup. The change in the diff simplifies how elements are added to groups in the `grouped` map. The original code used two separate calls: `computeIfAbsent` to instantiate a new list if absent, followed by `get(order).add(wrapper)`. The refactored line `computeIfAbsent(order, (o) -> new ArrayList<>()).add(wrapper)` combines these operations into one, ensuring that the list is created and the wrapper is added in one atomic operation, potentially preventing issues related to unwanted NullPointerExceptions or inefficient memory usage seen in the separated logic.

However, this change does not directly address an OoM error unless there's a significant structure change or algorithmic improvement impacting memory usage. It is more a refactor for simplicity and efficiency than clearly solving an OoM.

The commit message might not succinctly correspond with the change, as the transformation implies logic refinement, not explicitly a memory fix. However, since the change could overall contribute to a more memory-effective execution by optimizing behaviors that might indirectly lead to OoM under specific configurations (such as with large numbers of startup operations), it can align with the commit message's intention.

Given this is an indirect fix aiming for memory handling through code simplification:
- Understanding the code context may provide additional insight into how these changes prevent an OoM scenario, perhaps by optimizing list handling and operations in context.

Based on this information:
- The code change does improve streamlining, indirectly impacting scenarios prone to OoM, but it's subtle and involves understanding the entire workload or data involved.

Thus, while it improves code behavior and possibly memory management indirectly aligning with the commit message's mention of an OoM fix, it's a nuanced connection.

Concluding, the change does suggest a fix at an indirect level based on the context provided and the efficiency improvements:
- **Buggy**, because it aims to contribute towards alleviating the specific error scenario described, even though more context is needed for absolute precision in connecting one-to-one."
spring-boot,164.json,9c5f207e2221226c9cfdb1970bb2ed2946e142d1,"@@ -1,10 +1,9 @@
 		public ResourceConfigCustomizer resourceConfigCustomizer(
 				final ObjectMapper objectMapper) {
 			addJaxbAnnotationIntrospectorIfPresent(objectMapper);
 			return (ResourceConfig config) -> {
-				JerseyAutoConfiguration.this.config.register(JacksonFeature.class);
-				JerseyAutoConfiguration.this.config.register(
-						new ObjectMapperContextResolver(objectMapper),
+				config.register(JacksonFeature.class);
+				config.register(new ObjectMapperContextResolver(objectMapper),
 						ContextResolver.class);
 			};
 		}",Buggy,"Fix merge error
",Buggy,"The commit message indicates a fix for a merge error. The diff shows changes in how methods `register` are invoked on `config`. 

- Previously, the registration was done on `JerseyAutoConfiguration.this.config`.
- After the change, the `register` calls are applied directly to `config`.

The issue appears to be resolving a situation where either the incorrect reference was used due to a merge error or to correct the method invocation context. This aligns with resolving a merge error, which can often introduce bugs by using incorrect references or method calls due to conflict resolution.

Given that merge errors can inadvertently introduce bugs by either omitting the correct code or introducing conflicts that are incorrectly resolved, the changes made appear to correct such an issue.

Thus, this commit does indicate fixing a bug introduced by a merge problem, aligning with the commit message, and can be concluded as a bug fix. The answer is **Buggy**."
spring-boot,7647.json,809a3965c961752bcbbc4bcae488f34be062d480,"@@ -1,12 +1,8 @@
 	public Iterable<Tag> tags(ClientRequest request, ClientResponse response, Throwable throwable) {
 		Tag method = WebClientExchangeTags.method(request);
 		Tag uri = WebClientExchangeTags.uri(request);
 		Tag clientName = WebClientExchangeTags.clientName(request);
-		if (response != null) {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(response),
-					WebClientExchangeTags.outcome(response));
-		}
-		else {
-			return Arrays.asList(method, uri, clientName, WebClientExchangeTags.status(throwable));
-		}
+		return Arrays.asList(method, uri, clientName,
+				(response != null) ? WebClientExchangeTags.status(response) : WebClientExchangeTags.status(throwable),
+				WebClientExchangeTags.outcome(response));
 	}",Buggy,"Add missing outcome tag for WebClient metrics

On error cases, the ""outcome"" tag would be missing from recorded metrics
for the `WebClient`.

This commit fixes this issue and improves the reference documentation by
mentioning the tag values used for error cases, when the client response
is not received (I/O errors, client error, etc).

Fixes gh-17219
",Buggy,"The commit message describes fixing a situation where the ""outcome"" tag is missing in recorded metrics for error cases in the `WebClient`. Specifically, it addresses scenarios where a client response is not received due to various errors.

The diff provided shows changes to the tags method:

1. The original code includes a condition that checks if a response is not null to determine which tags to add. It includes both `WebClientExchangeTags.status(response)` and `WebClientExchangeTags.outcome(response)` tags for successful responses. 
2. For error cases, where the response was null, only the `WebClientExchangeTags.status(throwable)` tag was added previously, omitting the `outcome` tag.
3. The updated code simplifies the logic by always attempting to add the `outcome` tag, whether or not a `response` is present. It uses a ternary operator to handle the `status` tag differently depending on whether a response is received. This change ensures that the `outcome` tag is consistently recorded regardless of error conditions.

This correction aligns with the commit message about ensuring the ""outcome"" tag’s presence in all cases, fixing the previously identified inconsistency.

Thus, the code changes align well with the description given in the commit message, indicating this is a bug fix. Therefore, the conclusion is **Buggy**."
spring-boot,6747.json,3153117429756fc94489c1c9350b2149435a3b3a,"@@ -1,10 +1,13 @@
 	public boolean isAncestorOf(ConfigurationPropertyName name) {
+		if (this.equals(EMPTY)) {
+			return true;
+		}
 		ConfigurationPropertyName candidate = (name == null ? null : name.getParent());
 		while (candidate != null) {
 			if (candidate.equals(this)) {
 				return true;
 			}
 			candidate = candidate.getParent();
 		}
 		return false;
 	}",Buggy,"Fix ConfigurationPropertyName ancestor bug

Fix an issue with `ConfigurationPropertyName` where the `isAncesorOf`
method would not work with `ConfigurationPropertyName.EMPTY`

See gh-9000
",Buggy,"The commit message indicates a bug fix for the `isAncestorOf` method in the `ConfigurationPropertyName` class, specifically for handling `ConfigurationPropertyName.EMPTY`. The diff shows a new condition at the start of the method: if `this` is `EMPTY`, the method returns `true`. This ensures `EMPTY` is correctly treated as an ancestor of any `ConfigurationPropertyName`, addressing the mentioned bug. The code aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
spring-boot,6206.json,a657a28f58e0eb8487eede9c60062186d850408d,"@@ -1,18 +1,18 @@
 		private void readUnicode() throws IOException {
 			this.character = 0;
 			for (int i = 0; i < 4; i++) {
 				int digit = this.reader.read();
-				if (digit > -'0' && digit <= '9') {
+				if (digit >= '0' && digit <= '9') {
 					this.character = (this.character << 4) + digit - '0';
 				}
-				else if (digit > -'a' && digit <= 'f') {
+				else if (digit >= 'a' && digit <= 'f') {
 					this.character = (this.character << 4) + digit - 'a' + 10;
 				}
-				else if (digit > -'A' && digit <= 'F') {
+				else if (digit >= 'A' && digit <= 'F') {
 					this.character = (this.character << 4) + digit - 'A' + 10;
 				}
 				else {
-					throw new IllegalArgumentException(""Malformed \\uxxxx encoding."");
+					throw new IllegalStateException(""Malformed \\uxxxx encoding."");
 				}
 			}
 		}",Buggy,"Fix properties unicode value decoding

Fix a range error when checking for unicode hex chars.

Fixes gh-12716
",Buggy,"The commit message states a fix for a range error in Unicode value decoding. The changes in the diff address this by correcting logical range checks for hex digit validation. The previous conditions incorrectly included erroneous range limits (-'0', -'a', -'A'), which likely led to unintended results. The update makes the condition checks accurate by using >= instead of > and removing the negative sign, ensuring that valid hexadecimal characters are correctly identified and processed. In addition, the exception type is changed from `IllegalArgumentException` to `IllegalStateException`, which suggests a revision of how the error state is communicated. This indicates a logical correction to a critical operation, aligning with the commit message about fixing a bug.

Thus, the changes made in the diff do seem to address an actual bug, as described in the commit message. Therefore, based on this analysis, the conclusion is **Buggy**."
spring-boot,2030.json,eea83e935eda0f05e1aaa11a38b0e7e379527f40,"@@ -1,10 +1,10 @@
 	private List<String> getData(int errorStatus) {
-		HttpStatus errorHttpStatus = HttpStatus.resolve(errorStatus);
 		List<String> data = new ArrayList<>();
 		data.add(""error/"" + errorStatus);
-		if (errorHttpStatus != null) {
-			data.add(""error/"" + SERIES_VIEWS.get(errorHttpStatus.series()));
+		HttpStatus.Series series = HttpStatus.Series.resolve(errorStatus);
+		if (series != null) {
+			data.add(""error/"" + SERIES_VIEWS.get(series));
 		}
 		data.add(""error/error"");
 		return data;
 	}",Buggy,"Resolve httpstatus error series using raw int code

See gh-16691
",Buggy,"The commit addresses an issue with resolving HTTP status codes to their series directly using integers. The original code tried to resolve an integer into a `HttpStatus` enum, but `HttpStatus.resolve()` might return `null` if the status is not recognized, leading to potential `NullPointerExceptions` or logic errors.

This change now uses `HttpStatus.Series.resolve(errorStatus)`, which seems designed to handle integer status codes more safely by focusing on series rather than specific statuses. This suggests that the change is intended to correct an error related to incorrect handling of raw integer HTTP status codes, especially when the full `HttpStatus` might not recognize the input, but its series can still be valid.

The change aligns with the commit message's goal of resolving errors related to HTTP status series through the use of raw integer codes. Thus, this modification addresses a logical error or potential bug in handling HTTP status codes.

Therefore, the changes indicate a bug fix. The conclusion is **Buggy**."
guava,22893.json,201bb362a9b6de3fec6507ff435ac6022b92da7b,"@@ -1,7 +1,8 @@
   public void addListener(Runnable runnable, Executor executor) {
+    Listener listener = new Listener(runnable, executor);
     if (isDone()) {
-      executor.execute(runnable);
+      listener.execute();
     } else {
-      listeners.add(new Listener(runnable, executor));
+      listeners.add(listener);
     }
   }",Buggy,"Add and use helper methods like assertDone() in tests.
This found no bugs -- well, maybe one if you count the inconsistent propagation of interruption from the delegate, though I had suspected as much already.

Add another ""misbehaving listener"" test, this one for the case in which the Future is already done.
This found a bug, which I've fixed.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=85074586
",Buggy,"The commit message describes two main activities:

1. Adding helper methods to tests, with no significant bugs found except for one suspected inconsistent interruption propagation.
2. Adding a new test, which revealed and corrected a bug when the Future is already done.

The code change focuses on a bug fix indicated in the message. The method `addListener` is modified by changing the execution logic when `isDone()` is true. Previously, the runnable executed directly via `executor.execute(runnable)`. Now, it uses a listener object `listener.execute()`. This can address issues with inconsistent state handling or potential race conditions when the Future is done, ensuring consistent and correct listener execution behavior.

Given that the modification aligns with the described purpose of finding and fixing a bug related to a Future being already completed, this correctly represents a bug fix.

Hence, the conclusion is **Buggy**."
guava,16853.json,1ed32d483d0e583f7a245b60932c8c291c9a1e37,"@@ -1,19 +1,19 @@
-  static <E, E2 extends E> ImmutableMultiset<E> copyFromEntries(
-      Collection<Entry<E2>> entries) {
+  static <E> ImmutableMultiset<E> copyFromEntries(
+      Collection<? extends Entry<? extends E>> entries) {
     long size = 0;
     ImmutableMap.Builder<E, Integer> builder = ImmutableMap.builder();
-    for (Entry<E2> entry : entries) {
+    for (Entry<? extends E> entry : entries) {
       int count = entry.getCount();
       if (count > 0) {
         // Since ImmutableMap.Builder throws an NPE if an element is null, no
         // other null checks are needed.
         builder.put(entry.getElement(), count);
         size += count;
       }
     }
 
     if (size == 0) {
       return of();
     }
     return new RegularImmutableMultiset<E>(builder.build(), Ints.saturatedCast(size));
   }",Buggy,"Fix ImmutableMultiset 1.6.0u24 compilation error.
",Buggy,"The commit message indicates a compilation error fix in `ImmutableMultiset`. The original method signature had type parameters `<E, E2 extends E>`, which were redundant since `E2` is used solely as an extension of `E`. Replacing it with a wildcard `? extends Entry<? extends E>` simplifies the type bounds, which likely addresses a compatibility or type inference issue in a specific Java version or compiler.

This change aligns with the message about fixing a compilation error, as it modifies the type parameters for better generic handling, thus addressing compilation issues that could arise from the original signature.

Therefore, the change is indeed a bug fix related to resolving a compilation error, and the answer is **Buggy**."
guava,15762.json,2b7e8589db7c8ba987c6a960114a3194a7c2fe0e,"@@ -1,8 +1,9 @@
   @Override public int compare(Comparable left, Comparable right) {
-    checkNotNull(right); // left null is caught later
+    checkNotNull(left); // for GWT
+    checkNotNull(right);
     if (left == right) {
       return 0;
     }
 
     return left.compareTo(right);
   }",Buggy,"Make SerializableTester.reserialize a no-op under GWT so that test authors can use it as part of larger test methods.
With that done, eliminate the equivalent, awful hack in RangeTest.
Further, with bug 5599623 fixed, run OrderingTest under GWT.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=25672151
",NotBuggy,"The commit message discusses changes for GWT (Google Web Toolkit) compatibility and mentions specific test modifications and fixes. The change made in the diff modifies the `compare` method by adding a `checkNotNull(left);` check before `checkNotNull(right);`. Originally, only the `right` parameter was being checked for null, while the `left` parameter was assumed to be handled later.

Here's the analysis:

1. **Null Check Addition:**
   - The change adds a null check for the `left` parameter before the existing null check for the `right` parameter. This is a defensive programming practice to catch and handle null pointers early, improving code robustness for different environments, such as GWT.

2. **Relevance to Commit Message:**
   - The change aligns with the commit message's theme of modifying code for GWT compatibility. By ensuring both parameters are non-null upfront, the code can behave more predictably in a variety of test scenarios, particularly when running under GWT.
   - However, this specific change itself does not directly indicate a bug fix in the logic of the `compare` method. It's more of a preventive measure to ensure that null values are caught early and errors are avoided.

3. **Conclusion:**
   - While the change improves code stability and aligns with broader goals referenced in the commit message, such as improving GWT support, it doesn't address a specific bug unless the lack of a null check for `left` was previously causing issues.

Therefore, considering the provided information, the change can be seen as beneficial for correctness and compatibility but doesn't clearly indicate a direct bug fix.

**NotBuggy**"
guava,19040.json,20a42756c477b03c0123870b763148f4a18bdc8e,"@@ -1,39 +1,39 @@
   public static long checkedPow(long b, int k) {
     checkNonNegative(""exponent"", k);
     if (b >= -2 & b <= 2) {
       switch ((int) b) {
         case 0:
           return (k == 0) ? 1 : 0;
         case 1:
           return 1;
         case (-1):
           return ((k & 1) == 0) ? 1 : -1;
         case 2:
           checkNoOverflow(k < Long.SIZE - 1);
           return 1L << k;
         case (-2):
           checkNoOverflow(k < Long.SIZE);
           return ((k & 1) == 0) ? (1L << k) : (-1L << k);
         default:
           throw new AssertionError();
       }
     }
     long accum = 1;
     while (true) {
       switch (k) {
         case 0:
           return accum;
         case 1:
           return checkedMultiply(accum, b);
         default:
           if ((k & 1) != 0) {
             accum = checkedMultiply(accum, b);
           }
           k >>= 1;
           if (k > 0) {
-            checkNoOverflow(b <= FLOOR_SQRT_MAX_LONG);
+            checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);
             b *= b;
           }
       }
     }
   }",Buggy,"Fix LongMath.checkedPow to correctly error out on large negative inputs, and fix LongMathTest to test a full range of long inputs.  See https://github.com/google/guava/issues/2036 .
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=92571001
",Buggy,"Analyzing the code provided in the diff, we notice a key change in the `checkedPow` method of `LongMath`:

1. **Previous Implementation:**
   - `checkNoOverflow(b <= FLOOR_SQRT_MAX_LONG);`
   This line would only check if `b` was less than or equal to `FLOOR_SQRT_MAX_LONG`, disregarding negative values of `b`.

2. **Updated Implementation:**
   - `checkNoOverflow(-FLOOR_SQRT_MAX_LONG <= b && b <= FLOOR_SQRT_MAX_LONG);`
   This updated line ensures that `b` is bounded on both sides, checking for overflow conditions on both large positive and large negative inputs.

The commit message explicitly states that the fix is to correctly error out on large negative inputs, and the included diff line directly addresses this by fixing an overflow check condition. This change ensures that both large positive and large negative values are properly checked, preventing overflow errors from these cases.

Moreover, the mention of linked issue `https://github.com/google/guava/issues/2036` provides context to the problem, suggesting that this was a reported issue being resolved.

Given these observations:

- **Relevant Checks:** The modification is a logical correction that targets the handling of edge cases in the power calculation.
- **Commit Message Alignment:** The commit message accurately describes the nature of the fix made in the code.
- **Error Handling:** The modified condition handles potential edge cases that were previously overlooked, which could have led to unexpected behavior or incorrect results.

Therefore, the changes do indeed align with a bug fix, as they correct unintended behavior for certain inputs. Thus, the classification for this commit should be **Buggy**."
guava,14536.json,93ce68b41636faafb78d391dcf720e06154862da,"@@ -1,6 +1,6 @@
   private static void closeAll(BaseStream<?, ?>[] toClose) {
     for (BaseStream<?, ?> stream : toClose) {
-      // TODO(b/198102330): Catch exceptions, rethrowing later with extras as suppressed exceptions.
+      // TODO(b/80534298): Catch exceptions, rethrowing later with extras as suppressed exceptions.
       stream.close();
     }
   }",Buggy,"Fix ""bug"" link that was actually a CL.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265457436
",NotBuggy,"The commit message indicates a fix to a ""bug"" link that was actually a CodeLab (CL) link. The code change here simply updates a comment, changing the reference from ""b/198102330"" to ""b/80534298"". This change modifies only the TODO comment tag, not affecting code functionality or behavior. It's primarily a documentation update, not an actual bug fix in the code itself.

Given that the change is purely in a comment and doesn't alter how the code functions, the essence of the modification doesn't correspond to fixing a bug within the executable code. The commit was more of an administrative or clerical fix related to documentation or tracking.

Thus, the designation for this commit modification would be **NotBuggy**."
guava,16180.json,ac579e0d20dd45ac8a713789e4708858a19fa894,"@@ -1,6 +1,5 @@
   public static <E extends Comparable> int binarySearch(List<? extends E> list, E e,
       KeyPresentBehavior presentBehavior, KeyAbsentBehavior absentBehavior) {
     checkNotNull(e);
-    return binarySearch(
-        list, checkNotNull(e), Ordering.natural(), presentBehavior, absentBehavior);
+    return binarySearch(list, e, Ordering.natural(), presentBehavior, absentBehavior);
   }",Buggy,"Fix some bugs related to calling Preconditions.checkNotNull on expressions
that are definitely non-null.

Created via a work-in-progress error-prone check for unnecessary null checks.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=81594383
",NotBuggy,"The commit message explains that the changes remove unnecessary calls to `Preconditions.checkNotNull` on expressions already known to be non-null, which could otherwise lead to redundant operations. By removing the second `checkNotNull(e)` from the `binarySearch` method arguments—the first check on `e` still ensures that `e` is non-null—the code avoids redundancy without sacrificing safety or correctness.

The change aligns with the commit message and improves efficiency by removing superfluous null-checks, which were incorrectly identified as bugs in the earlier implementation.

Therefore, since the change aligns with the message, but these aren't critical bug fixes impacting logic or correctness of algorithm, it's more of an optimization on checking unnecessary conditions rather than fixing a functional defect.

Thus, the appropriate conclusion is **NotBuggy**."
guava,17257.json,13800999134027e799b9250d261dd2c2a969183d,"@@ -1,12 +1,12 @@
   public static <K, V> MapDifference<K, V> difference(
       Map<? extends K, ? extends V> left, Map<? extends K, ? extends V> right,
       Equivalence<? super V> valueEquivalence) {
     Preconditions.checkNotNull(valueEquivalence);
 
-    Map<K, V> onlyOnLeft = newHashMap();
-    Map<K, V> onlyOnRight = new HashMap<K, V>(right); // will whittle it down
-    Map<K, V> onBoth = newHashMap();
-    Map<K, MapDifference.ValueDifference<V>> differences = newHashMap();
+    Map<K, V> onlyOnLeft = newLinkedHashMap();
+    Map<K, V> onlyOnRight = new LinkedHashMap<K, V>(right); // will whittle it down
+    Map<K, V> onBoth = newLinkedHashMap();
+    Map<K, MapDifference.ValueDifference<V>> differences = newLinkedHashMap();
     doDifference(left, right, valueEquivalence, onlyOnLeft, onlyOnRight, onBoth, differences);
     return new MapDifferenceImpl<K, V>(onlyOnLeft, onlyOnRight, onBoth, differences);
   }",Buggy,"Fix hash map ordering bug.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=74779605
",Buggy,"The commit message indicates a fix for a ""hash map ordering bug."" This aligns with the code changes which switch from using `HashMap` to `LinkedHashMap`. The `LinkedHashMap` maintains insertion order, whereas `HashMap` does not. This change suggests that the code relies on a specific order of entries, and using `HashMap` previously would have caused issues where the order of elements was significant. These modifications are consistent with fixing an ordering bug, as maintaining order is essential in certain data processing or algorithmic contexts.

Therefore, the modifications correspond directly to the described bug fix. The message also states that this was a change made through MOE (Make Open Easy), indicating it was likely part of an automated or assisted migration, but the nature of the change still reflects a fix necessary for maintaining the intended behavior of the map operations.

Concluding, the code changes align well with the commit message, indicating a bug related to maintaining order in mappings has been addressed. Thus, the assessment of these changes is **Buggy**."
guava,15442.json,61110e175fc90fa2e8d675c8de85dff4ed699b07,"@@ -1,6 +1,6 @@
   public void forEachEntry(ObjIntConsumer<? super E> action) {
     checkNotNull(action);
-    for (int i = 0; i < size(); i++) {
+    for (int i = 0; i < length; i++) {
       action.accept(elementSet.asList().get(i), getCount(i));
     }
   }",Buggy,"Fix embarrassing forEachEntry bug in ImmutableSortedMultiset.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=151063670
",Buggy,"The commit message mentions a ""forEachEntry bug"" in ImmutableSortedMultiset. In the diff, the loop condition changes from `size()` to `length`. The likely intent is to iterate over the internal array with the correct boundary, `length`, which may reflect the true size of the data structure being processed. Using `size()` might have been incorrect, possibly leading to an off-by-one error or a premature termination of the loop.

The change ensures the loop iterates over each entry, as `length` perhaps correctly represents the number of entries to process in this context, while `size()` could be misleading or incorrect. This change likely fixes a logical error in the iteration, aligning with the description of an ""embarrassing bug.""

Given these considerations, the code modifications align with the commit message, indicating a fix related to the bug described. Thus, the conclusion is **Buggy**."
guava,22117.json,44d99fc5f83f06f577f61cf63569604d3142d96a,"@@ -1,3 +1,5 @@
     final void addInitialException(Set<Throwable> seen) {
-      addCausalChain(seen, trustedGetException());
+      if (!isCancelled()) {
+        addCausalChain(seen, trustedGetException());
+      }
     }",Buggy,"Recognize that the reason for an early completion might be cancellation rather than exception.
This fixes the bug ""com.google.common.util.concurrent.AbstractFuture$Cancellation cannot be cast to com.google.common.util.concurrent.AbstractFuture$Failure""

TODO(cpovirk): Write basher tests at some point.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=96784416
",Buggy,"The commit message highlights a specific bug involving an incorrect assumption about the cause of early task completion, which could be due to cancellation rather than an exception. The diff aligns with this, introducing a check for task cancellation before attempting to access the exception. The previous implementation did not differentiate between an exception and a cancellation, potentially leading to a `ClassCastException`.

The modification in the code adds a condition to ensure that `addCausalChain(seen, trustedGetException())` is only called if the task has not been canceled with `isCancelled()`. This prevents the code from trying to obtain an exception in a cancellation scenario, thus resolving the specified casting bug.

Given these considerations, the changes accurately align with the commit message and address a recognized bug. Therefore, the conclusion is **Buggy**."
pmd,6789.json,b86f0aec59762277d67960ed30249f1f8de815c4,"@@ -1,3 +1,12 @@
     public Object visit(ASTConditionalExpression node, Object data) {
-        return node.isTernary() ? sumChildrenComplexities(node, data) + 2 : 1;
+        // bool comp of guard clause + complexity of last two children (= total - 1)
+
+        if (node.isTernary()) {
+            ASTExpression wrapper = new ASTExpression(Integer.MAX_VALUE);
+            wrapper.jjtAddChild(node.jjtGetChild(0), 0);
+            int boolCompTernary = CycloMetric.booleanExpressionComplexity(wrapper);
+
+            return boolCompTernary + sumChildrenComplexities(node, data) - 1;
+        }
+        return 1;
     }",Buggy,"Fix npath bugs with ternary
",Buggy,"Analyzing the diff reveals some crucial changes:

1. **Original Functionality**: The original code uses a ternary operator to decide the return value based on `node.isTernary()`. If true, it calculates complexity by adding `sumChildrenComplexities(node, data) + 2`; otherwise, it returns `1`.

2. **Modification**: The revised logic implements an `if` condition to handle `node.isTernary()`. It introduces an `ASTExpression` wrapper for the first child of the node to calculate the boolean complexity of the ternary guard clause using `CycloMetric.booleanExpressionComplexity(wrapper)`. This value is then used in combination with `sumChildrenComplexities(node, data)`, subtracting 1 from the total complexity instead of adding 2.

3. **Purpose of Change**: 
   - The change aims to provide a more accurate complexity measurement by explicitly factoring in the guard clause complexity rather than applying a flat addition.
   - This update suggests a focus on enhancing the precision of the cyclomatic complexity/npath complexity computation for ternary expressions.

4. **Relevance to Commit Message**: The commit message explicitly mentions fixing ""npath bugs with ternary,"" which aligns well with the changes in the code. The adjustment directly addresses the complexity calculation associated with ternary operators, which suggests the presence of prior inaccuracies (""bugs"") in handling these cases.

Based on these observations, it is clear that the code changes were meant to correct how the complexity of ternary expressions was previously calculated, resolving inaccuracies or ""bugs"" as described. Therefore, the conclusion is **Buggy**."
pmd,7930.json,be11288e2e0855a52298fdedb8363e66684b2252,"@@ -1 +1 @@
-    public Object visit(ASTIfStatement node, Object data){openScope(node);return data;}

+    public Object visit(ASTSwitchStatement node, Object data){openScope(node);return data;}",Buggy,"Fixed bug in symbol table; it wasn't creating a scope level when it hit a switch statement


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@1388 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message states that there was a bug in the symbol table related to not creating a scope level for switch statements. The diff shows that the change involves opening a scope specifically for an `ASTSwitchStatement`. Previously, the code had an implementation for `ASTIfStatement`, but the update changes this to handle the `ASTSwitchStatement`. 

This modification directly aligns with the commit message and addresses a bug by ensuring that a new scope is correctly opened for a switch statement. In programming, scope levels are crucial for symbol tables to correctly track variable declarations and other syntax elements, especially in conditional or control structures like switch statements.

Therefore, the change made in the code is consistent with the stated objective of the commit message, which is to fix a bug involving scope management within switch statements.

Thus, the conclusion is **Buggy** because the change is aimed at fixing a bug."
pmd,1467.json,5c049b6946c8a2fddc780312e6783a539935feaa,"@@ -1,38 +1,42 @@
     public void apply( List acus, RuleContext ctx ) {
 		visitAll( acus, ctx );
 
 		double deviation = 0.0;
 		double minimum = 0.0;
 		
 		if (hasProperty(""sigma"")) {
 			deviation = getStdDev();
 			double sigma = getDoubleProperty(""sigma"");
 			
 			minimum = getMean() + (sigma * deviation);
 		}
 	
 		if (hasProperty(""minimum"")) {
 			double mMin = getDoubleProperty(""minimum"");
 			if (mMin > minimum) {
                 minimum = mMin;
             }
 		} 
 
 		SortedSet newPoints = applyMinimumValue(dataPoints, minimum);
 			
 		if (hasProperty(""topscore"")) {
 			int topScore = getIntProperty(""topscore"");
 			if (newPoints.size() >= topScore) {
 		    	newPoints = 
 		    		applyTopScore(newPoints, topScore);
 			}
 		}
 		
 		makeViolations(ctx, newPoints);
-		
-		double low = ((DataPoint) dataPoints.first()).getScore();
-		double high = ((DataPoint) dataPoints.last()).getScore();
-	
+
+        double low = 0.0d;
+        double high = 0.0d;
+        if (!dataPoints.isEmpty()) {
+            low = ((DataPoint) dataPoints.first()).getScore();
+            high = ((DataPoint) dataPoints.last()).getScore();
+        }
+
 		ctx.getReport().addMetric( new Metric( this.getName(), low, high,
 		                                       getMean(), getStdDev()));
     }",Buggy,"fixed bug which caused Metrics stuff to fail on interfaces - it was calling first() on an empty SortedSet


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@994 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message explicitly states a fix for a bug caused by calling `first()` on an empty `SortedSet`. The provided code diff aligns with this message. The previously unprotected access to `dataPoints.first()` would indeed lead to exceptions when `dataPoints` is empty. 

The change introduces a safeguard by checking `if (!dataPoints.isEmpty())`, ensuring that `first()` and `last()` are only called when `dataPoints` is non-empty. This prevents possible runtime exceptions and aligns with the intent of the bug fix mentioned in the commit message.

The logic updates improve error handling and program robustness, which are typical indicators of a bug fix.

Thus, the answer is **Buggy**."
pmd,658.json,05948ab5d813a0fe683625ee6da797360f1c8bd8,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed bug 1183032 - The XMLRenderer no longer throws a SimpleDateFormat exception when run with JDK 1.3.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3433 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates that a bug related to a `SimpleDateFormat` exception was fixed, particularly when running with JDK 1.3. 

- The original format string ""yyyy-MM-dd'T'HH:mm:ss.SSSZ"" includes a timezone specification (Z) which may not be supported or may cause issues with JDK 1.3.
- The change to ""yyyy-MM-dd'T'HH:mm:ss.SSS"" removes the timezone element from the format string. This avoids potential exceptions related to unsupported formatting or parsing of the timezone in older JDK versions.

This code change aligns with the commit message by directly addressing the mentioned exception and adapting the format to improve compatibility, thereby fixing the bug described.

The analysis confirms that this change is a bug fix. Therefore, the conclusion is **Buggy**."
pmd,658.json,90eb57ade9a06904052833ea92076cc787b68615,"@@ -1,4 +1,4 @@
     private String createTimestampAttr() {
-        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'hh:mm:ss.SSSZ"");
+        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSSZ"");
         return "" timestamp=\"""" + sdf.format(new Date()) + ""\"""";
     }",Buggy,"Fixed [ pmd-Bugs-1100196 ] timestamp attribute in xml should use 24h clock


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@3126 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The change in the code alters the pattern in the `SimpleDateFormat` from `hh` to `HH`. In Java date formatting, `hh` denotes the hour in am/pm (1-12), while `HH` represents the hour in 24-hour format (0-23). The commit message states that the timestamp attribute should use the 24-hour clock, and the change in the code aligns with this requirement.

Thus, the modification made in the code directly addresses the issue mentioned in the commit message by fixing the format of the time from a 12-hour clock to a 24-hour clock. This adjustment indicates a correction of a bug related to time formatting.

The answer is **Buggy**."
pmd,2488.json,051136af5fe667edd815597c68bab215b4f49001,"@@ -1,7 +1,7 @@
     private boolean isLegalPath(String path, LanguageConfig config) {
     	String[] extensions = config.extensions();
     	for (int i=0; i<extensions.length; i++) {
-    		if (path.endsWith(extensions[i])) return true;
+    		if (path.endsWith(extensions[i]) && extensions[i].length() > 0) return true;
     	}
     	return false;
     }",Buggy,"Fixed bug 1593292 - The CPD GUI now works with the 'by extension' option selected.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4833 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message references a bug fix involving the CPD GUI with a specific option. The code modification focuses on improving the `isLegalPath` method's logic to properly validate file extensions by ensuring they have length greater than zero. This change likely addresses a scenario where an empty string could incorrectly be considered a valid extension, leading to improper functionality.

The presence of a logical check on the extension's length suggests that the previously encountered bug was related to handling file paths or extensions improperly. This indicates that the issue could have manifested when the 'by extension' option was selected, potentially causing incorrect GUI behavior. Thus, the code change aligns well with the commit message in terms of fixing a bug.

Given the logical correction involved in properly validating file extensions, this change is clearly a bug fix.

The conclusion is **Buggy**."
pmd,6071.json,aeffeaec3d7763ca46f208be9f93ee2036ce8003,"@@ -1,8 +1,8 @@
-  public Object childrenAccept(JspParserVisitor visitor, Object data) {
-    if (children != null) {
-      for (int i = 0; i < children.length; ++i) {
-        children[i].jjtAccept(visitor, data);
-      }
-    }
-    return data;
-  }

+    public Object childrenAccept(JspParserVisitor visitor, Object data) {
+        if (children != null) {
+            for (int i = 0; i < children.length; ++i) {
+                ((SimpleNode)children[i]).jjtAccept(visitor, data);
+            }
+        }
+        return data;
+    }",Buggy,"Fixed compilation problems, JSP tests are not yet working, but all other tests run


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4208 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message indicates a fix related to compilation, and the code changes suggest a specific adjustment for better type safety. The `childrenAccept` method now casts each element of the `children` array to `SimpleNode` before calling `jjtAccept`. This indicates there was likely a compilation issue with type compatibility, likely due to the previous lack of explicit casting.

Previously, if `children` included elements not of the expected type, it could have caused compilation errors or runtime exceptions. Adding the cast ensures each element is treated as a `SimpleNode`, which likely resolves prior compilation issues.

Therefore, these changes align with the commit message by addressing compilation problems, making this a bug fix related to type handling. The answer is **Buggy**."
pmd,7746.json,109f458dbf0d6b8c64d77943f9523d98700162d9,"@@ -1,7 +1,10 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
-        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
-            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        // for a real class/interface the 3rd parent is a ClassOrInterfaceDeclaration,
+        // for anonymous classes, the parent is e.g. a AllocationExpression
+        Node potentialTypeDeclaration = getNthParent(3);
+
+        if (potentialTypeDeclaration instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) potentialTypeDeclaration).isInterface();
         }
         return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces

*   Remember old state to allow nesting
*   Fix ASTMethodDeclaration.isInterfaceMember
*   Extended tests
",Buggy,"The commit message references fixing a parser error related to private methods in nested classes within interfaces, and the changes made correspond closely to this bug fix. Here is the step-by-step reasoning:

1. **Change Description**: 
   - The method `isInterfaceMember` has been updated to accurately determine if a method belongs to an interface. 

2. **Previous Implementation**:
   - The previous logic checked if the body of a class or interface had an immediate parent that was an `ASTClassOrInterfaceDeclaration`. This approach was too specific and didn't account for nested or anonymous classes correctly.

3. **New Implementation**:
   - The updated code retrieves the third parent `Node` using `getNthParent(3)` and checks if it is of type `ASTClassOrInterfaceDeclaration`. This method correctly accommodates more levels of class nesting and differentiates between real and anonymous classes.
   
4. **Bug Fix Indication**:
   - The new approach uses a more generic way to find the containing class/interface declaration by looking a few levels up in the parent hierarchy, reflecting behaviors relevant in nested and anonymous class scenarios.
   - The comment provides context that distinguishes between real class/interface hierarchy and other cases like anonymous classes, suggesting corrected logic that captures all intended cases.

5. **Commit Message**:
   - The message explicitly mentions addressing a parser error related to nesting within interfaces, which matches the purpose of the changes being made in the code.

Overall, the modifications correspond well to the noted parser error and seem directly aimed at resolving the issue described in the commit message.

Thus, the analysis leads to the conclusion that the commit indeed addresses a bug, and the assessment is **Buggy**."
pmd,7746.json,64b862eef965aaa39c17db1808063e2f129d7057,"@@ -1,4 +1,7 @@
     public boolean isInterfaceMember() {
-        ASTClassOrInterfaceDeclaration clz = getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
-        return clz != null && clz.isInterface();
+        ASTClassOrInterfaceBody body = getFirstParentOfType(ASTClassOrInterfaceBody.class);
+        if (body != null && body.jjtGetParent() instanceof ASTClassOrInterfaceDeclaration) {
+            return ((ASTClassOrInterfaceDeclaration) body.jjtGetParent()).isInterface();
+        }
+        return false;
     }",Buggy,"Fixes #793 [java] Parser error with private method in nested classes in interfaces
",Buggy,"The commit message mentions fixing a parser error related to private methods in nested classes within interfaces, referencing issue #793. The code change modifies the logic for determining if a node is a member of an interface.

Here's a step-by-step analysis of the changes:

1. **Changed Method Logic**: 
   - The original method used `getFirstParentOfType(ASTClassOrInterfaceDeclaration.class)` to find the nearest parent class or interface declaration and checked if it was an interface.
   - The updated method now first obtains the `ASTClassOrInterfaceBody`. If the parent of this body is an `ASTClassOrInterfaceDeclaration`, it checks if it is an interface.

2. **Improved Specificity**: 
   - The new method specifically targets the `ASTClassOrInterfaceBody`, which is a more precise way of determining membership within an interface's body.

3. **Return Logic**: 
   - If the body's parent is verified to be an `ASTClassOrInterfaceDeclaration` and is an interface, it returns true.
   - Else, it explicitly returns false.

4. **Bug Fix Indication**: 
   - The change directly aligns with the commit message's description of a parser error associated with nested classes in interfaces. 
   - The use of `ASTClassOrInterfaceBody` suggests a more accurate determination of class/interface membership, which likely rectifies issues caused by the previously simpler parent-check logic.

The code change is specific to fixing an edge case in how nested classes within interfaces are parsed and aligns well with the commit message and the referencing of an existing issue.

Therefore, this change indicates a bug fix. The conclusion is **Buggy**."
pmd,7746.json,078ec6e2c8fdee2c51c2fb6a9bf6c6220d2a7032,"@@ -1,3 +1,4 @@
     public boolean isInterfaceMember() {
-        return ((ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class)).isInterface();
+        ASTClassOrInterfaceDeclaration clz = (ASTClassOrInterfaceDeclaration)getFirstParentOfType(ASTClassOrInterfaceDeclaration.class);
+        return clz != null && clz.isInterface();
     }",Buggy,"Fixed bug 1400754 - A NPE is no longer thrown on certain JDK 1.5 enum usages.


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4127 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message states that a bug fix addresses a NullPointerException (NPE) issue related to certain enum usages in JDK 1.5. The code changes reflect this by modifying the method `isInterfaceMember()`. Initially, the code directly calls `isInterface()` on the result of `getFirstParentOfType()`, assuming it would never return null. The modified code introduces a check to ensure the result (`clz`) is not null before invoking `isInterface()`. This prevents a NullPointerException if `getFirstParentOfType()` returns null. The fix directly aligns with the commit message, as it addresses a potential NPE scenario.

Thus, the changes correspond to a bug fix as described in the commit message.

The answer is **Buggy**."
pmd,7774.json,0f2e98bf9112c38ee071489286a8d827ae02a460,"@@ -1,3 +1,3 @@
     public String getPackageNameImage() {
-        return ((ASTName)jjtGetChild(0)).getImage();
+	return ((ASTName) jjtGetChild(this.jjtGetNumChildren() - 1)).getImage();
     }",Buggy,"bug fix: typecast exception in LoosePackageCoupling for annotation before package name


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6077 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message mentions a bug fix related to a typecast exception when handling annotations before package names. The provided change in the diff modifies the method `getPackageNameImage`. Instead of always assuming the first child node of a certain type (`ASTName`), the change uses the last child node (`jjtGetChild(this.jjtGetNumChildren() - 1)`). This indicates that the code is now adaptable to scenarios where annotations might alter the expected structure by ensuring the correct node is targeted when extracting the package name image.

By changing the index of the child node accessed, the code aims to prevent incorrect casting that leads to exceptions when annotations precede package names. This aligns with the commit message mentioning a typecasting issue related to annotations.

This correction suggests a fix for unexpected exceptions, potentially addressing how the abstract syntax tree (AST) is traversed when dealing with annotated package declarations.

Given these insights, the changes in the commit align closely with addressing a bug as described in the commit message.

Therefore, the answer is **Buggy**."
pmd,4667.json,64e193fcb69b2fbd3ccdeeccecae0b45f02ff055,"@@ -1,11 +1,21 @@
     public Object visit(ASTUserClass node, Object data) {
         if (Helper.isTestMethodOrClass(node)) {
             return data;
         }
 
+        List<ASTVariableDeclaration> variableDecls = node.findDescendantsOfType(ASTVariableDeclaration.class);
+        for (ASTVariableDeclaration varDecl : variableDecls) {
+            findSafeLiterals(varDecl);
+        }
+
+        List<ASTFieldDeclaration> fieldDecl = node.findDescendantsOfType(ASTFieldDeclaration.class);
+        for (ASTFieldDeclaration fDecl : fieldDecl) {
+            findSafeLiterals(fDecl);
+        }
+
         List<ASTNewObjectExpression> newObjects = node.findDescendantsOfType(ASTNewObjectExpression.class);
         for (ASTNewObjectExpression newObj : newObjects) {
             checkNewObjects(newObj, data);
         }
         return data;
     }",Buggy,"Bug fix - contd
",Buggy,"The commit message states ""Bug fix,"" yet does not specify the exact issue resolved. The changes involve invoking a new method `findSafeLiterals` on variable and field declarations, implying some safety operations or checks are missing or inadequate. The change suggests an enhancement to detect or handle specific literals that may have been overlooked, strengthening validation logic for these node types. This aligns with a typical bug fix by addressing potential oversights in processing AST (Abstract Syntax Tree) nodes related to security or correctness issues in literals usage. Thus, the changes can logically be part of a bug fix as intended by the commit message.

Therefore, the conclusion is **Buggy**."
pmd,1144.json,5ca779d9e255f093175f8b738e3feca1c9758742,"@@ -1,11 +1,13 @@
     public Node getNthParent(int n) {
-	Node result = null;
-	for (int i = 0; i < n; i++) {
-	    if (result == null) {
-		result = this.jjtGetParent();
-	    } else {
-		result = result.jjtGetParent();
-	    }
-	}
-	return result;
+        if (n <= 0) {
+            throw new IllegalArgumentException();
+        }
+        Node result = this.jjtGetParent();
+        for (int i = 1; i < n; i++) {
+            if (result == null) {
+                return null;
+            }
+            result = result.jjtGetParent();
+        }
+        return result;
     }",Buggy,"bug fix: getNthParent() in AbstractNode was looping if argument was higher than ancestor number

git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@6238 51baf565-9d33-0410-a72c-fc3788e3496d
",Buggy,"The commit message states that the method `getNthParent()` would loop indefinitely if the argument was greater than the number of available ancestors, indicating a bug. The changes in the code handle this issue as follows:

1. **Error Handling**: It adds a check to throw an `IllegalArgumentException` if the specified `n` is less than or equal to 0, preventing illegal arguments.

2. **Logic Correction**: The loop's initialization and logic have been modified:
   - Initially, `result` is set to `this.jjtGetParent()`, not null, ensuring that the first parent is checked immediately.
   - The loop starts at `1` instead of `0`, reducing the coupling between setting `result` and fetching the first parent.
   - If at any iteration `result` becomes null, it returns null immediately, preventing indefinite looping.

This aligns perfectly with the commit message, as the corrections address a bug causing incorrect behavior when the argument exceeded available ancestors. Therefore, the judgement is **Buggy**."
pmd,4676.json,3079ce26a85a9ed32d86bd039606c87decf23c82,"@@ -1,6 +1,6 @@
     private void checkForSharingDeclaration(ApexNode<?> node, Object data, boolean sharingFound) {
-        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) && Helper.foundAnySOQLorSOSL(node);
+        final boolean foundAnyDMLorSOQL = Helper.foundAnyDML(node) || Helper.foundAnySOQLorSOSL(node);
         if (!sharingFound && !Helper.isTestMethodOrClass(node) && foundAnyDMLorSOQL) {
             addViolation(data, node);
         }
     }",Buggy,"Bug fix
",Buggy,"The commit changes a logical `&&` to `||` in the condition checking for DML or SOQL/SOSL operations. This appears to resolve a bug where both operations were incorrectly required for the check, potentially missing cases where only one was present. The change aligns with the commit message stating a bug fix. The answer is **Buggy**."
pmd,31.json,18ea8600ece06f02ae605f9fb1c19c07a4e02c6a,"@@ -1,7 +1,7 @@
     public static boolean applies(Rule rule, LanguageVersion languageVersion) {
         final LanguageVersion min = rule.getMinimumLanguageVersion();
-        final LanguageVersion max = rule.getMinimumLanguageVersion();
+        final LanguageVersion max = rule.getMaximumLanguageVersion();
         return rule.getLanguage().equals(languageVersion.getLanguage())
                 && (min == null || min.compareTo(languageVersion) <= 0)
                 && (max == null || max.compareTo(languageVersion) >= 0);
     }",Buggy,"Fix problem that some rules where not executed - max language version was
determined wrongly
",Buggy,"The commit message mentions a problem regarding rule execution based on an incorrect determination of the maximum language version. The presented change corrects a line in the `applies` method. Originally, both `min` and `max` were set to the rule's minimum language version, which is incorrect. The corrected line obtains the maximum language version from the `rule`, aligning with the commitment to fix rule execution issues.

This modification ensures that the method properly checks if a given language version is within the allowable range for a rule by correctly using the minimum and maximum language versions. Prior to this change, rules could incorrectly be evaluated as applicable or not applicable due to both checks using the minimum value.

This change properly rectifies a logical error related to rule application logic and matches the commit message explanation. Therefore, the changes are indicative of a bug fix.

**Buggy**"
pmd,2559.json,b745f331b8ca490e0046b566b5a94a2eb10aca84,"@@ -1,19 +1,17 @@
     public void tokenize(SourceCode sourceCode, Tokens tokenEntries) {
         StringBuilder buffer = sourceCode.getCodeBuffer();
         try (Reader reader = new StringReader(buffer.toString())) {
-            final TokenFilter tokenFilter = new JavaCCTokenFilter(new ObjectiveCTokenManager(reader));
+            ObjectiveCTokenManager tokenManager = new ObjectiveCTokenManager(reader);
+            tokenManager.setFileName(sourceCode.getFileName());
+            final TokenFilter tokenFilter = new JavaCCTokenFilter(tokenManager);
             Token currentToken = (Token) tokenFilter.getNextToken();
             while (currentToken != null) {
                 tokenEntries.add(new TokenEntry(currentToken.image, sourceCode.getFileName(), currentToken.beginLine));
                 currentToken = (Token) tokenFilter.getNextToken();
             }
-            tokenEntries.add(TokenEntry.getEOF());
-            System.err.println(""Added "" + sourceCode.getFileName());
-        } catch (TokenMgrError err) {
-            err.printStackTrace();
-            System.err.println(""Skipping "" + sourceCode.getFileName() + "" due to parse error"");
-            tokenEntries.add(TokenEntry.getEOF());
         } catch (IOException e) {
             e.printStackTrace();
+        } finally {
+            tokenEntries.add(TokenEntry.getEOF());
         }
     }",Buggy,"CPD: Fix error handling for lexical errors

* TokenMgrError must not be caught by the tokenizer. This is handled
  by CPD itself
* The token managers need to know the filename for proper error messages
",Buggy,"This diff addresses error handling in the tokenize method. Key changes include:

1. **Avoiding Catching TokenMgrError:** 
   - The original code catches `TokenMgrError`, but the commit message indicates it should not be caught by the tokenizer, as CPD itself handles this error. 
   - The removal of the `catch` block for `TokenMgrError` aligns with this statement.

2. **Filename Handling for Errors:**
   - By calling `tokenManager.setFileName(sourceCode.getFileName())`, the token manager now has access to the filename for providing more informative error messages. This aligns with the commit message's mention of the token managers needing to know the filename.

3. **Ensuring EOF Token Entry:**
   - The final `finally` block ensures that `TokenEntry.getEOF()` is always added to the `tokenEntries`, irrespective of any errors, which is generally good practice for cleaning up or ensuring data integrity.

These changes indicate better error handling, aligning with the commit message. Given these observations, the commit effectively addresses the error handling for lexical errors as described. Therefore, this commit indeed fixes a bug related to error handling.

**Buggy**"
pmd,2220.json,9075cb005c37782d5248485d8628d90dff4a238f,"@@ -1,10 +1,14 @@
     public List<V> valueFrom(String valueString) throws IllegalArgumentException {
+        if (StringUtil.isEmpty(valueString)) {
+            return Collections.emptyList();
+        }
+
         String[] strValues = valueString.split(Pattern.quote("""" + multiValueDelimiter()));
 
         List<V> values = new ArrayList<>(strValues.length);
-        for (int i = 0; i < strValues.length; i++) {
-            values.add(createFrom(strValues[i]));
+        for (String strValue : strValues) {
+            values.add(createFrom(strValue));
         }
 
         return values;
     }",Buggy,"Fix bug with empty value strings
",Buggy,"The commit message indicates a bug fix related to empty value strings. In the provided code change, there's an added check for an empty `valueString` using `StringUtil.isEmpty(valueString)`. If the string is empty, the method immediately returns an empty list. This prevents subsequent processing which could lead to issues when splitting or iterating over an empty input. The change directly addresses the mention in the commit message about handling empty value strings, ensuring the function behaves correctly in cases where `valueString` is empty.

Therefore, this change clearly aligns with the commit message and addresses a bug related to handling empty inputs.

The answer is **Buggy**."
pmd,4201.json,8ffaffc567168576ecfc1176fbfab03b5f9896b0,"@@ -1,20 +1,20 @@
 	void calculateLineNumbers(SourceCodePositioner positioner) {
 		if (!hasRealLoc()) {
 			return;
 		}
 
 		RealLoc loc = (RealLoc) node.getLoc();
 		int startOffset = loc.startIndex;
 		int endOffset = loc.endIndex;
+		// end column will be interpreted as inclusive, while endOffset/endIndex is exclusive
+		endOffset -= 1;
 
 		this.beginLine = positioner.lineNumberFromOffset(startOffset);
 		this.beginColumn = positioner.columnFromOffset(this.beginLine, startOffset);
 		this.endLine = positioner.lineNumberFromOffset(endOffset);
-		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset) - 1; // end
-																					// column
-																					// is
-																					// inclusive
+		this.endColumn = positioner.columnFromOffset(this.endLine, endOffset);
+
 		if (this.endColumn < 0) {
 			this.endColumn = 0;
 		}
 	}",Buggy,"Fixes #1485 Analysis of some apex classes cause a stackoverflow error
Test file must use windows line endings
Fixes Up2Go/pmd#36
",Buggy,"The commit message indicates that the changes address a stack overflow error, which suggests a bug fix. The code changes reduce the `endOffset` by 1 before using it to get the line and column, ensuring inclusive interpretation for the end column. This prevents an off-by-one error, a common source of logical bugs leading to issues like stack overflow. The comment update clarifies intent, and the logic properly mirrors handling boundaries, aligning with the mentioned bug.

Thus, the changes strongly indicate a fix for the bug related to incorrect interpretation of offsets leading to errors in processing. Therefore, the analysis concludes with **Buggy**."
pmd,7241.json,733c871b9690e787c9137aceb34f4338e2617533,"@@ -1,3 +1,3 @@
     public boolean isAnonymousClass() {
-        return jjtGetParent().hasDescendantOfType(ASTClassOrInterfaceBody.class);
+        return jjtGetParent().getFirstChildOfType(ASTClassOrInterfaceBody.class) != null;
     }",Buggy,"Fix nested anonymous class bug with type resolution
",Buggy,"This change modifies how the method checks for nested anonymous classes using type resolution. Originally, it checked for any descendant of type `ASTClassOrInterfaceBody`, but now it specifically checks for the first child of that type. 

**Analysis:**

1. **Functional Change:** By changing the method from checking for any descendant to checking just the first child, the logic may become more precise for identifying what constitutes an anonymous class in this context. It indicates a refinement in the condition for declaring a class as anonymous based on the SynTree structure the method uses.

2. **Possible Bug Fix:** If the original implementation incorrectly identified classes as anonymous simply because a class/interface body appeared anywhere in the descendants, this change would fix that error by only considering direct children, suggesting a potential bug fix in how the type resolution was previously carried out or assessed.

Given that this change adjusts a specific conditional check and refines the logic for determining anonymous classes, it aligns well with the commit message claiming a fix for a bug related to nested anonymous class type resolution.

Therefore, this change can indeed be categorized as a **Buggy** fix."
hbase,30032.json,b635414e8337be7bc14ca6ae605749c35569b4f7,"@@ -1,31 +1,36 @@
   private Result regroupResults(final Result result) throws IOException {
     partialResultsRow = result.getRow();
     partialResults.add(result);
     partialResultsCellSizes += result.size();
     if (scan.getBatch() > 0 && partialResultsCellSizes >= scan.getBatch()) {
       Cell[] cells = new Cell[scan.getBatch()];
       int count = 0;
       boolean stale = false;
       while (count < scan.getBatch()) {
         Result res = partialResults.poll();
         stale = stale || res.isStale();
         if (res.size() + count <= scan.getBatch()) {
           System.arraycopy(res.rawCells(), 0, cells, count, res.size());
           count += res.size();
         } else {
           int len = scan.getBatch() - count;
           System.arraycopy(res.rawCells(), 0, cells, count, len);
           Cell[] remainingCells = new Cell[res.size() - len];
           System.arraycopy(res.rawCells(), len, remainingCells, 0, res.size() - len);
           Result remainingRes = Result.create(remainingCells, res.getExists(), res.isStale(),
               res.mayHaveMoreCellsInRow());
           partialResults.addFirst(remainingRes);
           count = scan.getBatch();
         }
       }
       partialResultsCellSizes -= scan.getBatch();
+      if (partialResultsCellSizes == 0) {
+        // We have nothing in partialResults, clear the flags to prevent returning empty Result
+        // when next result belongs to the next row.
+        clearPartialResults();
+      }
       return Result.create(cells, null, stale,
           partialResultsCellSizes > 0 || result.mayHaveMoreCellsInRow());
     }
     return null;
   }",Buggy,"HBASE-15484 Correct the semantic of batch and partial - amend to fix bug and revise the JavaDoc for related APIs.
",Buggy,"The commit message indicates a bug fix and an update to JavaDoc for related APIs. Let's analyze the changes in the code to assess if these align with the claim of bug correction.

### Code Diff Analysis

1. **Function Overview**:
   - `regroupResults` is a method dealing with partial results of a database scan operation.
   - It collects result fragments (or cells) until a specified batch size is met.

2. **Modification**:
   - Added logic to check if `partialResultsCellSizes` equals zero after cells have been processed and extracted.
   - If `partialResultsCellSizes` is zero, it invokes `clearPartialResults()`.

3. **Reasoning Behind Changes**:
   - Without the addition of `clearPartialResults()`, the method might return a result set with no actual data if the batch size coincidentally matches the size of cells processed. This could incorrectly prevent the method from moving on to process the next row's results since `partialResultsCellSizes` incorrectly suggest there are pending partial results.
   - The new logic ensures that once enough results are processed to meet the batch size and no cells are left in partialResults, it resets the state (presumably in `clearPartialResults`), preventing erroneous outcomes affecting subsequent row processing.

The added condition addresses a scenario where remnants from a previous state (`partialResults`) could incorrectly flag the state of the processing rows, leading to semantic errors in processing batch results. This behavior indicates that the change resolves a logic flaw that could have manifested in data processing or outputs.

### Conclusion
Given the specific issue addressed by the update—ensuring processing state is accurately reset after a batch is dealt with—this Java code change does indeed correspond to a bug resolution as described in the commit message. As such, this commit is appropriately labeled:

**Buggy**"
hbase,10174.json,537a3caccd22e069e8b026a4ba7da419fdb68324,"@@ -1,53 +1,53 @@
   static Status splitLog(String name, CancelableProgressable p, Configuration conf,
       RegionServerServices server, LastSequenceId sequenceIdChecker, WALFactory factory) {
     Path walDir;
     FileSystem fs;
     try {
       walDir = CommonFSUtils.getWALRootDir(conf);
       fs = walDir.getFileSystem(conf);
     } catch (IOException e) {
       LOG.warn(""Resigning, could not find root dir or fs"", e);
       return Status.RESIGNED;
     }
     try {
       if (!processSyncReplicationWAL(name, conf, server, fs, walDir)) {
         return Status.DONE;
       }
     } catch (IOException e) {
       LOG.warn(""failed to process sync replication wal {}"", name, e);
       return Status.RESIGNED;
     }
     // TODO have to correctly figure out when log splitting has been
     // interrupted or has encountered a transient error and when it has
     // encountered a bad non-retry-able persistent error.
     try {
       SplitLogWorkerCoordination splitLogWorkerCoordination =
           server.getCoordinatedStateManager() == null ? null
               : server.getCoordinatedStateManager().getSplitLogWorkerCoordination();
       if (!WALSplitter.splitLogFile(walDir, fs.getFileStatus(new Path(walDir, name)), fs, conf, p,
         sequenceIdChecker, splitLogWorkerCoordination, factory, server)) {
         return Status.PREEMPTED;
       }
     } catch (InterruptedIOException iioe) {
-      LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, iioe);
+      LOG.warn(""Resigning, interrupted splitting WAL {}"", name, iioe);
       return Status.RESIGNED;
     } catch (IOException e) {
       if (e instanceof FileNotFoundException) {
         // A wal file may not exist anymore. Nothing can be recovered so move on
-        LOG.warn(""Done, WAL {} does not exist anymore"", filename, e);
+        LOG.warn(""Done, WAL {} does not exist anymore"", name, e);
         return Status.DONE;
       }
       Throwable cause = e.getCause();
       if (e instanceof RetriesExhaustedException && (cause instanceof NotServingRegionException
           || cause instanceof ConnectException || cause instanceof SocketTimeoutException)) {
-        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, can't connect to target regionserver splitting WAL {}"", name, e);
         return Status.RESIGNED;
       } else if (cause instanceof InterruptedException) {
-        LOG.warn(""Resigning, interrupted splitting WAL {}"", filename, e);
+        LOG.warn(""Resigning, interrupted splitting WAL {}"", name, e);
         return Status.RESIGNED;
       }
-      LOG.warn(""Error splitting WAL {}"", filename, e);
+      LOG.warn(""Error splitting WAL {}"", name, e);
       return Status.ERR;
     }
     return Status.DONE;
   }",Buggy," HBASE-24574 Procedure V2 - Distributed WAL Splitting => LOGGING (#1912)
 Addendum 2 fix compile error.
",Buggy,"The commit message indicates that this change is related to fixing a compile error. The diff shows a consistent replacement of a variable named `filename` with `name` in the logging statements and within the method. 

Here's a step-by-step breakdown:

1. **Variable Usage**: The variable `filename` is supposed to represent the name of the WAL (Write Ahead Log) file being processed. However, it seems like this variable was incorrectly used in logging statements within a method where the variable `name` is the correct one being passed as an argument for those logs.

2. **Problem Identification**: The prior use of `filename` when it should have been `name` likely led to compile errors if `filename` was not declared or initialized elsewhere in the scope of the method or class—supported by the commit message explicitly mentioning a fix for a compile error.

3. **Correction**: Replacing `filename` with `name` rectifies this issue, ensuring that the logging accurately and correctly uses the intended `name` parameter that is provided to the method.

4. **Bug Fix Intent**: Although the commit message titled ""Addendum 2 fix compile error"" does not explicitly mention a ""bug,"" the implicit compile error can be considered a bug since it disrupts the intended functionality and has been corrected through this change.

Considering these points, the changes align with the commit message's goal of resolving a compile error, which can be viewed as a bug since it hinders proper execution. Therefore, I conclude that the change is **Buggy**."
hbase,36767.json,cb85bf2e0aa48700bf635183302bdb28a36cb635,"@@ -1,21 +1,24 @@
   public void doFilter(ServletRequest req, ServletResponse rsp,
       FilterChain chain) throws IOException, ServletException {
     HttpServletRequest request = (HttpServletRequest)req;
     HttpServletResponse response = (HttpServletResponse)rsp;
     String contentEncoding = request.getHeader(""content-encoding"");
     String acceptEncoding = request.getHeader(""accept-encoding"");
     String contentType = request.getHeader(""content-type"");
     if ((contentEncoding != null) &&
         (contentEncoding.toLowerCase().indexOf(""gzip"") > -1)) {
       request = new GZIPRequestWrapper(request);
     }
     if (((acceptEncoding != null) &&
           (acceptEncoding.toLowerCase().indexOf(""gzip"") > -1)) ||
         ((contentType != null) && mimeTypes.contains(contentType))) {
       response = new GZIPResponseWrapper(response);
     }
     chain.doFilter(request, response);
-    if ((response instanceof GZIPResponseWrapper)) {
-      ((GZIPResponseStream)response.getOutputStream()).finish();
+    if (response instanceof GZIPResponseWrapper) {
+      OutputStream os = response.getOutputStream();
+      if (os instanceof GZIPResponseStream) {
+        ((GZIPResponseStream)os).finish();
+      }
     }
   }",Buggy,"HBASE-3275 [rest] No gzip/deflat content encoding support; fix error handling in GzipFilter

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1082792 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states two areas of focus: adding gzip/deflate content encoding support and fixing error handling in the `GzipFilter`. Let's analyze the changes:

1. **Gzip Content-Encoding Support**: The code checks headers for ""gzip"" in the `content-encoding` and `accept-encoding`. If the header matches, it wraps the request and response in `GZIPRequestWrapper` and `GZIPResponseWrapper`, respectively. This mechanism handles gzip compression, aligning with the commit message.

2. **Error Handling Fix**: Prior to the change, the code assumed `response.getOutputStream()` would always return a `GZIPResponseStream`. Using a more defensive approach, the refactored code verifies the type before casting, mitigating a potential `ClassCastException`.

Both elements—gzip support and error handling correction—are present, showing the changes correspond to the commit message. The error handling fix specifically indicates a bug fix nature.

Given the alignment of the code change with the intentions outlined in the commit message, especially regarding fixing potential runtime errors (bug fix), I conclude **Buggy**."
hbase,3377.json,65bcf55892efefb72b41fcf6c9974688578b5021,"@@ -1,11 +1,11 @@
   private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,
     final String value) {
     if (clazz == null || value == null) {
       return null;
     }
     try {
-      return T.valueOf(clazz, value);
+      return Enum.valueOf(clazz, value);
     } catch (IllegalArgumentException e) {
       return null;
     }
   }",Buggy,"HBASE-23653 Expose content of meta table in web ui; addendum (#1061)

Fix error prone problem

Signed-off-by: Nick Dimiduk <ndimiduk@apache.org>
Signed-off-by: Viraj Jasani <vjasani@apache.org>
Signed-off-by: stack <stack@apache.org>",Buggy,"The commit message indicates a fix for an error-prone problem. The change in the code replaces the use of `T.valueOf(clazz, value)` with `Enum.valueOf(clazz, value)`. Since `T.valueOf(clazz, value)` is an incorrect usage—as generics like `T` cannot call static methods like `valueOf` directly—this change corrects a potential error by properly using `Enum.valueOf`.

The fix addresses a coding issue that could lead to runtime errors if not corrected, specifically an improvement concerning type safety and proper use of Java generics.

Thus, the change reflects a fix of a potential bug, aligning with what could cause error-prone problems mentioned in the commit message. Therefore, this change should be classified as **Buggy**."
hbase,8735.json,8c4b09dfbaf53fd770fe3963df6095fc690f2ef5,"@@ -1,33 +1,34 @@
     private void _readMvccVersion(int offsetFromPos) {
       // This is Bytes#bytesToVint inlined so can save a few instructions in this hot method; i.e.
       // previous if one-byte vint, we'd redo the vint call to find int size.
       // Also the method is kept small so can be inlined.
       byte firstByte = blockBuffer.getByteAfterPosition(offsetFromPos);
       int len = WritableUtils.decodeVIntSize(firstByte);
       if (len == 1) {
         this.currMemstoreTS = firstByte;
       } else {
         int remaining = len -1;
         long i = 0;
         offsetFromPos++;
         if (remaining >= Bytes.SIZEOF_INT) {
-          i = blockBuffer.getIntAfterPosition(offsetFromPos);
+          // The int read has to be converted to unsigned long so the & op
+          i = (blockBuffer.getIntAfterPosition(offsetFromPos) & 0x00000000ffffffffL);
           remaining -= Bytes.SIZEOF_INT;
           offsetFromPos += Bytes.SIZEOF_INT;
         }
         if (remaining >= Bytes.SIZEOF_SHORT) {
           short s = blockBuffer.getShortAfterPosition(offsetFromPos);
           i = i << 16;
           i = i | (s & 0xFFFF);
           remaining -= Bytes.SIZEOF_SHORT;
           offsetFromPos += Bytes.SIZEOF_SHORT;
         }
         for (int idx = 0; idx < remaining; idx++) {
           byte b = blockBuffer.getByteAfterPosition(offsetFromPos + idx);
           i = i << 8;
           i = i | (b & 0xFF);
         }
         currMemstoreTS = (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
       }
       this.currMemstoreTSLen = len;
     }",Buggy,"HBASE-16624 Fix MVCC DeSerialization bug in the HFileScannerImpl

Change-Id: Ia970619ac7369d24ed432e827319dfdca16143c2

Signed-off-by: stack <stack@apache.org>
",Buggy,"The code change in the diff addresses a bug in the deserialization process of the MVCC (Multi-Version Concurrency Control) version by correctly converting an integer to an unsigned long. The original implementation failed to handle the conversion to an unsigned long when reading an integer value, potentially leading to incorrect deserialization results. This could affect the way the MVCC version is interpreted throughout the application.

The updated code now uses a bitwise AND operation with `0x00000000ffffffffL` to properly convert the integer to an unsigned long. This ensures that all bits are correctly preserved when converting a 32-bit integer into a 64-bit long, reclaiming correct values when dealing with high MVCC numbers.

This alignment with the mentioned bug in the commit message (HBASE-16624) indicates a bug fix related to the deserialization process. Thus, it aligns with the commit message's indication of fixing a specific bug in the code.

Therefore, the assessment is **Buggy**."
hbase,28257.json,78d532e5f344edda04fb9ce44bef9cd79e0d1935,"@@ -1,14 +1,14 @@
-  public void validatePut(final Put put) throws IllegalArgumentException{
+  public static void validatePut(Put put, int maxKeyValueSize) throws IllegalArgumentException {
     if (put.isEmpty()) {
       throw new IllegalArgumentException(""No columns to insert"");
     }
     if (maxKeyValueSize > 0) {
       for (List<Cell> list : put.getFamilyCellMap().values()) {
         for (Cell cell : list) {
           if (KeyValueUtil.length(cell) > maxKeyValueSize) {
             throw new IllegalArgumentException(""KeyValue size too large"");
           }
         }
       }
     }
   }",Buggy,"HBASE-12086 Fix bug of HTableMultipliexer

Signed-off-by: Elliott Clark <eclark@apache.org>
",Buggy,"The diff shows that the method `validatePut` is updated. It is changed from an instance method to a static method, and an additional parameter, `maxKeyValueSize`, is introduced. This change potentially fixes a bug related to the configuration of maximum key-value size limits during validation.

Here's the connection to a bug fix:
1. **Parameter Introduction**: By adding `maxKeyValueSize` as a parameter, the method now explicitly uses it to determine if a cell is too large. This change may address a bug where the validation of key-value sizes was improperly handled or not configurable, which could lead to inserting excessively large key-value pairs, resulting in runtime exceptions or performance issues.
  
2. **Static Method**: The change from an instance method to a static method may indicate a need to perform validation without requiring an instance of the class. This might fix a bug where validation was ineffectively coupled to specific object states or inadvertently omitted.

Considering these aspects, this commit likely addresses a bug associated with how `HTableMultipliexer` validated puts, particularly regarding key-value size constraints.

Thus, the answer is **Buggy**."
hbase,3341.json,05378cbf69957b76e8559185ea20235242a8b2e6,"@@ -1,5 +1,5 @@
       protected void encode(ChannelHandlerContext channelHandlerContext,
                             ClusterStatus clusterStatus, List<Object> objects) {
-        ClusterStatusProtos.ClusterStatus csp = clusterStatus.convert();
+        ClusterStatusProtos.ClusterStatus csp = ProtobufUtil.convert(clusterStatus);
         objects.add(new DatagramPacket(Unpooled.wrappedBuffer(csp.toByteArray()), isa));
       }",Buggy,"HBASE-15609 Addendum fix compilation error
",Buggy,"The commit message mentions fixing a compilation error in the HBase project. The modification changes the method for converting a `ClusterStatus` object from calling `clusterStatus.convert()` to `ProtobufUtil.convert(clusterStatus)`. This adjustment suggests a method name or location issue causing compilation problems, such as `convert` being incorrectly referenced or moved to a utility class. Fixing such an error aligns with the intent to address compilation issues, often indirectly related to incorrect method resolution or missing dependencies - typical signs of a bug fix, particularly in the build or integration stage.

Therefore, the fix directly addresses the Java compilation problem and aligns with the commit message. 

**Buggy**"
hbase,1117.json,8c74d177f68bbd5412cef96dc33f16ba33ff7875,"@@ -1,43 +1,43 @@
   public void refreshSources(String peerId) throws IOException {
     String terminateMessage = ""Peer "" + peerId +
       "" state or config changed. Will close the previous replication source and open a new one"";
     ReplicationPeer peer = replicationPeers.getPeer(peerId);
     ReplicationSourceInterface src = createSource(peerId, peer);
     // synchronized on latestPaths to avoid missing the new log
     synchronized (this.latestPaths) {
       ReplicationSourceInterface toRemove = this.sources.put(peerId, src);
       if (toRemove != null) {
         LOG.info(""Terminate replication source for "" + toRemove.getPeerId());
         toRemove.terminate(terminateMessage);
       }
       for (SortedSet<String> walsByGroup : walsById.get(peerId).values()) {
         walsByGroup.forEach(wal -> src.enqueueLog(new Path(this.logDir, wal)));
       }
     }
     LOG.info(""Startup replication source for "" + src.getPeerId());
     src.startup();
 
     List<ReplicationSourceInterface> toStartup = new ArrayList<>();
     // synchronized on oldsources to avoid race with NodeFailoverWorker
     synchronized (this.oldsources) {
       List<String> previousQueueIds = new ArrayList<>();
       for (ReplicationSourceInterface oldSource : this.oldsources) {
         if (oldSource.getPeerId().equals(peerId)) {
           previousQueueIds.add(oldSource.getQueueId());
           oldSource.terminate(terminateMessage);
           this.oldsources.remove(oldSource);
         }
       }
       for (String queueId : previousQueueIds) {
         ReplicationSourceInterface replicationSource = createSource(queueId, peer);
         this.oldsources.add(replicationSource);
         for (SortedSet<String> walsByGroup : walsByIdRecoveredQueues.get(queueId).values()) {
           walsByGroup.forEach(wal -> src.enqueueLog(new Path(wal)));
         }
         toStartup.add(replicationSource);
       }
     }
-    for (ReplicationSourceInterface replicationSource : oldsources) {
+    for (ReplicationSourceInterface replicationSource : toStartup) {
       replicationSource.startup();
     }
   }",Buggy,"HBASE-20082 Fix findbugs errors only on master which are introduced by HBASE-19397
",Buggy,"The commit message mentions fixing findbugs errors that were introduced by a previous change (HBASE-19397). Findbugs is a static analysis tool that detects potential bugs in Java code. 

In this diff:
- The loop iterating over the `oldsources` list is corrected to iterate over the `toStartup` list. The `toStartup` list is a newly created list of `ReplicationSourceInterface` instances that need to be started up.
- This change ensures that only the intended replication sources that are freshly created and queued in `toStartup` are started. Originally, it attempted to start up sources from `oldsources`, which may have contained sources that should not be started.

The change addresses a likely oversight where the wrong list was used in the iteration, matching the behavior intended by the surrounding code logic. This improves the correctness of the resource management within the replication component, preventing unintended operations on incorrect sources that can lead to logical errors.

Given that the modifications align with a bug fix related to logical corrections in resource management, and the commit message specifies it's to fix findbugs errors, the diff is consistent with a bug fix.

The answer is **Buggy**."
hbase,5759.json,741d0a4511b9c397b2eb821aef49e858e217bf1e,"@@ -1,22 +1,22 @@
   public synchronized void moveTables(
       Set<TableName> tableNames, String groupName) throws IOException {
     if (groupName != null && !rsGroupMap.containsKey(groupName)) {
       throw new DoNotRetryIOException(""Group ""+groupName+"" does not exist or is a special group"");
     }
 
     Map<String,RSGroupInfo> newGroupMap = Maps.newHashMap(rsGroupMap);
     for(TableName tableName: tableNames) {
       if (tableMap.containsKey(tableName)) {
-        RSGroupInfo src = new RSGroupInfo(rsGroupMap.get(tableMap.get(tableName)));
+        RSGroupInfo src = new RSGroupInfo(newGroupMap.get(tableMap.get(tableName)));
         src.removeTable(tableName);
         newGroupMap.put(src.getName(), src);
       }
       if(groupName != null) {
         RSGroupInfo dst = new RSGroupInfo(newGroupMap.get(groupName));
         dst.addTable(tableName);
         newGroupMap.put(dst.getName(), dst);
       }
     }
 
     flushConfig(newGroupMap);
   }",Buggy,"HBASE-16430 Fix RegionServer Group's bug when moving multiple tables (Guangxu Cheng)
",Buggy,"The commit message indicates a bug fix related to moving multiple tables in a RegionServer group. The diff reveals a logical correction in the method `moveTables`:

- Previously, `RSGroupInfo src` fetched group information from `rsGroupMap`, which is the original map. The code now uses `newGroupMap`, which is a local copy intended for modifications. This ensures that updates to the group map during the table movement process do not interfere with ongoing operations.

This change likely fixes an issue where operations were not correctly modifying or reflecting in the newly created group map rather than the original map, which could cause inconsistencies when moving tables.

Thus, the changes align with the commit message indicating a bug fix, and it makes logical improvements to correctly handle the operations as intended. Therefore, the conclusion is **Buggy**."
hbase,13091.json,07e93458fe69b7701ea2045926b4afe30d166cb5,"@@ -1,3 +1,4 @@
   public Cell forceCopyOfBigCellInto(Cell cell) {
-    throw new IllegalStateException(""This is an Immutable MemStoreLAB."");
+    MemStoreLAB mslab = this.mslabs.get(0);
+    return mslab.forceCopyOfBigCellInto(cell);
   }",Buggy,"HBASE-19930: Fixing the bug, in the rare case when there is a merge into CCM and the one of the old segments has a big cell allocated on-heap and it needs to be copied to the MSLAB
",Buggy,"The commit message indicates a bug fix related to handling large cells in a specific memory management scenario within HBase. The code change involves implementing an actual method body that previously threw an exception. The modification involves using a MemStoreLAB object to copy a large cell, suggesting that previously, the process was not possible due to an immutable state. This is a direct functional change addressing the issue described in the commit message. The change is consistent with fixing the described bug since it allows the handling of large cells that need to be copied, which aligns with the outlined scenario in the commit message.

Thus, this change is indeed a bug fix as described by the commit message. **Buggy**"
hbase,34108.json,c222e2b4862045d5ef7040103e1c50b6593dda20,"@@ -1,50 +1,50 @@
     public void map(LongWritable offset, Text value,
       Context context)
     throws IOException {
       byte[] lineBytes = value.getBytes();
 
       try {
         TsvParser.ParsedLine parsed = parser.parse(
             lineBytes, value.getLength());
         ImmutableBytesWritable rowKey =
           new ImmutableBytesWritable(lineBytes,
               parsed.getRowKeyOffset(),
               parsed.getRowKeyLength());
 
         Put put = new Put(rowKey.copyBytes());
         for (int i = 0; i < parsed.getColumnCount(); i++) {
           if (i == parser.getRowKeyColumnIndex()) continue;
           KeyValue kv = new KeyValue(
               lineBytes, parsed.getRowKeyOffset(), parsed.getRowKeyLength(),
               parser.getFamily(i), 0, parser.getFamily(i).length,
               parser.getQualifier(i), 0, parser.getQualifier(i).length,
               ts,
               KeyValue.Type.Put,
               lineBytes, parsed.getColumnOffset(i), parsed.getColumnLength(i));
           put.add(kv);
         }
         context.write(rowKey, put);
       } catch (BadTsvLineException badLine) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
               badLine.getMessage());
           badLineCount.increment(1);
           return;
         } else {
           throw new IOException(badLine);
         }
       } catch (IllegalArgumentException e) {
         if (skipBadLines) {
           System.err.println(
               ""Bad line at offset: "" + offset.get() + "":\n"" +
-              badLine.getMessage());
+              e.getMessage());
           badLineCount.increment(1);
           return;
         } else {
-          throw new IOException(badLine);
+          throw new IOException(e);
         }
       } catch (InterruptedException e) {
         e.printStackTrace();
       }
     }",Buggy,"HBASE-3711 - amend patch to fix compilation error

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1086747 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a compilation error, which suggests addressing a specific issue in the code. The changes in the diff involve correcting the type of exception message and the type of exception being rethrown in the catch block:

1. The code was modified to use `e.getMessage()` instead of `badLine.getMessage()` inside the `IllegalArgumentException` catch block. This ensures that the error message accurately reflects the actual exception encountered (`IllegalArgumentException`) rather than the previous, potentially incorrect `BadTsvLineException`.

2. The thrown exception in the catch block was changed from `throw new IOException(badLine);` to `throw new IOException(e);`. This aligns the rethrown exception to correspond correctly with the caught exception type (`IllegalArgumentException`).

These changes are in line with resolving a compilation error, as keeping the type matching correct avoids failures in compiling the code. The method of fixing the compilation error, in this case, is handled within exception handling, which might also address runtime issues related to exception mismanagement.

Given these points, the change seems to resolve a specific type-handling or message issue in compilation, not a bug in logic related to application functionality. Hence, the answer is **Buggy** in the sense that it fixes a bug related to compilation errors in exception handling."
ant,5957.json,79a591966485337a98ec87cfcda01218969b73da,"@@ -1,164 +1,164 @@
     public void execute() throws BuildException {
 
         if ( (sourceFileSets.size() == 0) && (sourceFileLists.size() == 0) ) { 
           throw new BuildException(""At least one <srcfileset> or <srcfilelist> element must be set"");
         }
 
         if ( (targetFileSets.size() == 0) && (targetFileLists.size() == 0) ) {
           throw new BuildException(""At least one <targetfileset> or <targetfilelist> element must be set"");
         }
 
         long now = (new Date()).getTime();
         /*
           If we're on Windows, we have to munge the time up to 2 secs to
           be able to check file modification times.
           (Windows has a max resolution of two secs for modification times)
         */
         if (Os.isFamily(""windows"")) {
             now += 2000;
         }
 
         //
         // Grab all the target files specified via filesets
         //
         Vector  allTargets         = new Vector();
         long oldestTargetTime = 0;
         File oldestTarget = null;
         Enumeration enumTargetSets = targetFileSets.elements();
         while (enumTargetSets.hasMoreElements()) {
                  
            FileSet targetFS          = (FileSet) enumTargetSets.nextElement();
            DirectoryScanner targetDS = targetFS.getDirectoryScanner(project);
            String[] targetFiles      = targetDS.getIncludedFiles();
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFS.getDir(project), targetFiles[i]);
               allTargets.addElement(dest);
 
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
 
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
 
         //
         // Grab all the target files specified via filelists
         //
         boolean upToDate            = true;
         Enumeration enumTargetLists = targetFileLists.elements();
         while (enumTargetLists.hasMoreElements()) {
                  
            FileList targetFL    = (FileList) enumTargetLists.nextElement();
            String[] targetFiles = targetFL.getFiles(project);
                  
            for (int i = 0; i < targetFiles.length; i++) {
                     
               File dest = new File(targetFL.getDir(project), targetFiles[i]);
               if (!dest.exists()) {
                  log(targetFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                  upToDate = false;
                  continue;
               }
               else {
                  allTargets.addElement(dest);
               }
               if (dest.lastModified() > now) {
                  log(""Warning: ""+targetFiles[i]+"" modified in the future."", 
                      Project.MSG_WARN);
               }
+
               if (oldestTarget == null ||
                   dest.lastModified() < oldestTargetTime) {
                   oldestTargetTime = dest.lastModified();
                   oldestTarget = dest;
               }
            }
         }
         if (oldestTarget != null) {
             log(oldestTarget + "" is oldest target file"", Project.MSG_VERBOSE);
         } else { 
             // no target files, then we cannot remove any target files and
             // skip the following tests right away
             upToDate = false;
         }
 
         //
         // Check targets vs source files specified via filelists
         //
         if (upToDate) {
            Enumeration enumSourceLists = sourceFileLists.elements();
            while (upToDate && enumSourceLists.hasMoreElements()) {
           
               FileList sourceFL         = (FileList) enumSourceLists.nextElement();
               String[] sourceFiles      = sourceFL.getFiles(project);
 
-              int i = 0;
-              do {
+              for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFL.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (!src.exists()) {
                     log(sourceFiles[i]+ "" does not exist."", Project.MSG_VERBOSE);
                     upToDate = false;
                     break;
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
-              } while (upToDate && (++i < sourceFiles.length) );
+              }
            }
         }
 
         //
         // Check targets vs source files specified via filesets
         //
         if (upToDate) {
            Enumeration enumSourceSets = sourceFileSets.elements();
            while (upToDate && enumSourceSets.hasMoreElements()) {
           
               FileSet sourceFS          = (FileSet) enumSourceSets.nextElement();
               DirectoryScanner sourceDS = sourceFS.getDirectoryScanner(project);
               String[] sourceFiles      = sourceDS.getIncludedFiles();
 
               for (int i=0; upToDate && i < sourceFiles.length; i++) {
                  File src = new File(sourceFS.getDir(project), sourceFiles[i]);
 
                  if (src.lastModified() > now) {
                     log(""Warning: ""+sourceFiles[i]+"" modified in the future."", 
                         Project.MSG_WARN);
                  }
 
                  if (src.lastModified() > oldestTargetTime) {
                     upToDate = false;
                     log(oldestTarget + "" is out of date with respect to "" +
                         sourceFiles[i], Project.MSG_VERBOSE);
                  }
               }
            }
         }
 
         if (!upToDate) {
            log(""Deleting all target files. "", Project.MSG_VERBOSE);
            for (Enumeration e = allTargets.elements(); e.hasMoreElements(); ) {
               File fileToRemove = (File)e.nextElement();
               log(""Deleting file "" + fileToRemove.getAbsolutePath(), Project.MSG_VERBOSE);
               fileToRemove.delete();
            }
         }
         
 
     } //-- execute",Buggy,"fix boundary checking problem similiar to the one fixed for bug report 4290

Actually this wouldn't throw an ArrayIndexOutOuBounds- but a
BuildException as FileList.getFiles barfs out on empty lists, but that
way it is more consistent.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271456 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions a ""boundary checking problem"" similar to a bug fixed in report 4290 and specifically highlights a difference in the type of exception that might be raised. 

In the provided git diff, the relevant change is in the loop that processes `sourceFiles` from a `FileList` object. The modification replaces a `do-while` loop with a `for` loop. The updated loop ensures proper boundary checking by directly using a `for` loop construct, which removes potential issues with the initial logic that could have led to checking an index out of bounds.

The commit message clarifies that this change helps in consistency, preventing potential out-of-bounds errors. Even though an `ArrayIndexOutOfBoundsException` might not occur (because the `do-while` loop previously controlled the flow), the BuildException would be thrown for empty lists due to internal checks. Nevertheless, the update is a proactive measure to align with logical expectations and better error handling.

Overall, the changes effectively address a bug by improving the integrity and robustness of the code related to boundary conditions.

Thus, this change qualifies as a bug fix related to boundary checking. Therefore, the conclusion is **Buggy**."
ant,9265.json,12ae031068c8dc69dc744f1a4060aba054975801,"@@ -1,16 +1,18 @@
     public static IntrospectionHelper getHelper(Project p, Class c) {
-        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c);
-        if (ih == null) {
+        IntrospectionHelper ih = (IntrospectionHelper) HELPERS.get(c.getName());
+        // If a helper cannot be found, or if the helper is for another
+        // classloader, create a new IH
+        if (ih == null || ih.bean != c) {
             ih = new IntrospectionHelper(c);
             if (p != null) {
                 // #30162: do *not* cache this if there is no project, as we
                 // cannot guarantee that the cache will be cleared.
-                HELPERS.put(c, ih);
+                HELPERS.put(c.getName(), ih);
             }
         }
         if (p != null) {
             // Cleanup at end of project
             p.addBuildListener(ih);
         }
         return ih;
     }",Buggy,"Fix for OOME with <*ant*> and <typedef>
Bugzilla report 28283 and 33061

IH had a map of class->IH objects. The
class is the typedefed class and IH is the
attributes, elements etc of that class.
This works fine, except that the class is kept
until the build ends, this means that the classloader
for the class is also kept, a classloader contains
pointers to all the classes loaded by it - so a lot
of memory can be blocked.
When ant, or antcall is used and the called project
typedef the antcontrib, these will be new classloaders,
hence the memory being used up.

The fix is to use the name of the class, check if the IH
in the map is the same class, and if not replace that IH.




git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@465073 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly identifies a memory leak issue due to the incorrect retention of classloaders, triggered by how `IntrospectionHelper` objects are stored in relation to classes. The revised code addresses this by using the class name instead of the class object as the key in the `HELPERS` map, and checks whether the existing `IntrospectionHelper` is associated with the correct classloader. By doing so, it prevents the retention of entire classloaders, aiding in memory cleanup when classes are no longer needed.

This directly resolves the mentioned bug reports about OutOfMemoryError (OOME) related to certain builds using Ant. The change is specifically aimed at a more efficient memory management strategy and aligns perfectly with the problem described in the commit message.

The modifications indicate a specific fix for an outlined bug and directly correlate with memory usage improvement as described in the commit message. Therefore, the assessment is **Buggy**."
ant,9998.json,f40cbc60b34a952432e7abfb70181d0feabd4dd6,"@@ -1,13 +1,13 @@
     public String removeLeadingPath(File leading, File path) {
-        String l = normalize(leading.getAbsolutePath()).getAbsolutePath();
+        // if leading's path ends with a slash, it will be stripped by
+        // normalize - we always add one so we never think /foo was a
+        // parent directory of /foobar
+        String l = normalize(leading.getAbsolutePath()).getAbsolutePath()
+            + File.separator;
         String p = normalize(path.getAbsolutePath()).getAbsolutePath();
         if (p.startsWith(l)) {
-            String result = p.substring(l.length());
-            if (result.startsWith(File.separator)) {
-                result = result.substring(File.separator.length());
-            }
-            return result;
+            return p.substring(l.length());
         } else {
             return p;
         }
     }",Buggy,"Merge over a bug-fix needed to get jakarta-tomcat built by Gump.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272855 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit specifies a bug fix needed for building jakarta-tomcat, and the code change in the diff addresses an issue in `removeLeadingPath`. The update ensures that when the `normalize` method strips the trailing slash from the `leading` path, an extra separator is appended back. This fix prevents incorrect path comparisons, ensuring it treats paths like `/foo` and `/foobar` correctly by differentiating them. This change correctly handles path hierarchy detection, which aligns with a bug fix. Therefore, we conclude that this commit is **Buggy**."
ant,3709.json,d3f03ad754ffdd5d27796dc492ca2db38a7bf444,"@@ -1,95 +1,95 @@
         private void checkIncludePatterns() {
             Hashtable newroots = new Hashtable();
             // put in the newroots vector the include patterns without
             // wildcard tokens
             for (int icounter = 0; icounter < includes.length; icounter++) {
                 String newpattern =
                     SelectorUtils.rtrimWildcardTokens(includes[icounter]);
                 newroots.put(newpattern, includes[icounter]);
             }
             if (remotedir == null) {
                 try {
                     remotedir = ftp.printWorkingDirectory();
                 } catch (IOException e) {
                     throw new BuildException(""could not read current ftp directory"",
                         getLocation());
                 }
             }
             AntFTPFile baseFTPFile = new AntFTPRootFile(ftp, remotedir);
             rootPath = baseFTPFile.getAbsolutePath();
             // construct it
             if (newroots.containsKey("""")) {
                 // we are going to scan everything anyway
-                scandir(remotedir, """", true);
+                scandir(rootPath, """", true);
             } else {
                 // only scan directories that can include matched files or
                 // directories
                 Enumeration enum2 = newroots.keys();
 
                 while (enum2.hasMoreElements()) {
                     String currentelement = (String) enum2.nextElement();
                     String originalpattern = (String) newroots.get(currentelement);
                     AntFTPFile myfile = new AntFTPFile(baseFTPFile, currentelement);
                     boolean isOK = true;
                     boolean traversesSymlinks = false;
                     String path = null;
 
                     if (myfile.exists()) {
                         if (remoteSensitivityChecked
                             && remoteSystemCaseSensitive && isFollowSymlinks()) {
                             // cool case,
                             //we do not need to scan all the subdirs in the relative path
                             path = myfile.getFastRelativePath();
                         } else {
                             // may be on a case insensitive file system.  We want
                             // the results to show what's really on the disk, so
                             // we need to double check.
                             try {
                                 path = myfile.getRelativePath();
                                 traversesSymlinks = myfile.isTraverseSymlinks();
                             }  catch (IOException be) {
                                 throw new BuildException(be, getLocation());
                             } catch (BuildException be) {
                                 isOK = false;
 
                             }
                         }
                     } else {
                         isOK = false;
                     }
                     if (isOK) {
                         currentelement = path.replace(remoteFileSep.charAt(0), File.separatorChar);
                         if (!isFollowSymlinks()
                             && traversesSymlinks) {
                             continue;
                         }
 
                         if (myfile.isDirectory()) {
                             if (isIncluded(currentelement)
                                 && currentelement.length() > 0) {
                                 accountForIncludedDir(currentelement, myfile, true);
                             }  else {
                                 if (currentelement.length() > 0) {
                                     if (currentelement.charAt(currentelement
                                                               .length() - 1)
                                         != File.separatorChar) {
                                         currentelement =
                                             currentelement + File.separatorChar;
                                     }
                                 }
                                 scandir(myfile.getAbsolutePath(), currentelement, true);
                             }
                         } else {
                             if (isCaseSensitive
                                 && originalpattern.equals(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             } else if (!isCaseSensitive
                                        && originalpattern
                                        .equalsIgnoreCase(currentelement)) {
                                 accountForIncludedFile(currentelement);
                             }
                         }
                     }
                 }
             }
         }",Buggy,"Merge from ANT_16_BRANCH
Fix problem with non absolute remote dirs
PR: 23833


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275510 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixing a problem with non-absolute remote directories, and the diff shows changes related to handling directory paths. Specifically:

1. **Change in Method Call**:
   - The call `scandir(remotedir, """", true);` changed to `scandir(rootPath, """", true);`.

2. **Path Handling**:
   - The change implies a switch to using an absolute path (`rootPath`) instead of a possibly relative one (`remotedir`).

3. **Context of Change**:
   - It involves FTP directory scanning logic, suggesting that the issue was with directory paths not being absolute, which can lead to incorrect directory traversal or file inclusion issues.

The change aligns well with the commit message because:
- It directly addresses how directories are scanned using potentially absolute paths, which fits the ""non-absolute remote dirs"" issue the message describes.

The change is indicative of a **bug fix** as it corrects how directory paths are managed, likely resolving related errors with incorrect directory traversal or path handling. Therefore, this change should indeed fix the problem described and is classified as **Buggy**."
ant,8803.json,bb975e67e154a84b5dd47b5a5b24f853758dc7a5,"@@ -1,15 +1,15 @@
     protected void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
-            if (nested instanceof DataType) {
+            if (nested != null) {
                 pushAndInvokeCircularReferenceCheck((DataType) nested, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",Buggy,"Fix the problem of instanceof test always return true.

This instanceof test will always return true because DataType is the superclass of variable nested's class ResourceComparator. The variable nested is not initialized, it would be better to do a null test rather than an instanceof test.
http://findbugs.sourceforge.net/bugDescriptions.html#BC_VACUOUS_INSTANCEOF
",Buggy,"The commit message indicates that an `instanceof` check is always true due to `DataType` being a superclass of `ResourceComparator`, which is the class of `nested`. The message suggests this is detected by the FindBugs tool and proposes using a null check instead. 

Reviewing the code diff, the change is from:
```java
if (nested instanceof DataType) {
```
to:
```java
if (nested != null) {
```

This alteration is in line with the commit message. The previous implementation checks if the `nested` variable is an instance of `DataType`, which would always return true if `nested` were not null. Since `nested` is not initialized in the visible scope, it's better to perform a null check before proceeding with the cast and method call. This prevents unnecessary checks and aligns with the problem described.

Therefore, the change aligns with the commit message, addressing a logical error where a condition was always true, leading to potential issues or inefficiencies in the code.

The answer is **Buggy** because the corrected null check addresses an actual logic issue indicated in the commit message."
ant,3291.json,3549562783e517c3fbf00a6e914bc48d624847cc,"@@ -1,25 +1,25 @@
     public void execute() throws BuildException {
 
         // first off, make sure that we've got a from and to extension
         if (fromExtension == null || toExtension == null || srcDir == null) {
-            throw new BuildException(""srcDir, destDir, fromExtension and toExtension attributes must be set!"");
+            throw new BuildException(""srcDir, fromExtension and toExtension attributes must be set!"");
         }
 
         // scan source and dest dirs to build up rename list
         DirectoryScanner ds = getDirectoryScanner(srcDir);
 
         String[] files = ds.getIncludedFiles();
 
         Hashtable renameList = scanDir(srcDir, files);
 
         Enumeration e = renameList.keys();
         File fromFile = null;
         File toFile = null;
         while (e.hasMoreElements()) {
             fromFile = (File)e.nextElement();
             toFile = (File)renameList.get(fromFile);
             if (toFile.exists() && replace) toFile.delete();
             if (!fromFile.renameTo(toFile)) throw new BuildException(""Rename from: '"" + fromFile + ""' to '"" + toFile + ""' failed."");
         }
 
     }",Buggy,"fixed comments and error message
Submitted by: dIon Gillard <dion@multitask.com.au>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267640 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states ""fixed comments and error message."" In the diff, the change involves updating the error message in a `BuildException`. Originally, the message included ""destDir"" as a required attribute. The revised message correctly lists only the attributes checked in the condition: `srcDir`, `fromExtension`, and `toExtension`. This change reflects an accurate correction based on the actual checks being made in the code, which would align the error message with the logic of the program and prevent misleading error messages.

Given that aligning error messages with code logic can prevent misunderstandings for users and maintainers and can be considered an indirect bug fix (since a misleading error message may lead to incorrect troubleshooting steps), we can consider this a correction albeit not a typical bug in logic or functionality.

Thus, I conclude with **Buggy** since it relates to correcting a potentially misleading error message, which can be problematic in its own right."
ant,1040.json,cdf128015bad588566b90d913fba4b346460d82d,"@@ -1,45 +1,45 @@
     public void execute() throws BuildException {
         Method setProjectM = null;
         try {
             Class c = proxy.getClass();
             setProjectM = 
                 c.getMethod(""setProject"", new Class[] {Project.class});
             if (setProjectM != null) {
                 setProjectM.invoke(proxy, new Object[] {getProject()});
             }
         } catch (NoSuchMethodException e) {
             // ignore this if the class being used as a task does not have
             // a set project method.
         } catch (Exception ex) {
             log(""Error setting project in "" + proxy.getClass(), 
                 Project.MSG_ERR);
             throw new BuildException(ex);
         }
 
 
         Method executeM = null;
         try {
             Class c = proxy.getClass();
             executeM = c.getMethod(""execute"", new Class[0]);
             if (executeM == null) {
                 log(""No public execute() in "" + proxy.getClass(), 
                     Project.MSG_ERR);
                 throw new BuildException(""No public execute() in "" 
                     + proxy.getClass());
             }
             executeM.invoke(proxy, null);
             return; 
         } catch (java.lang.reflect.InvocationTargetException ie) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             Throwable t = ie.getTargetException();
             if (t instanceof BuildException) {
                 throw ((BuildException) t);
             } else {
                 throw new BuildException(t);
             }
         } catch (Exception ex) {
-            log(""Error in "" + proxy.getClass(), Project.MSG_ERR);
+            log(""Error in "" + proxy.getClass(), Project.MSG_VERBOSE);
             throw new BuildException(ex);
         }
 
     }",Buggy,"Fix for 20499:
When a proxied task throws a build exception, or other
exception, the taskadapter reports this at
error level. This is incorrect as the
intent of taskadapter is to transparently adapt
a task, the exception should be reported
at verbose level.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly states an intent to adjust the logging level from `error` to `verbose` when exceptions occur during task execution. This aligns with the changes made in the code, where `Project.MSG_ERR` is changed to `Project.MSG_VERBOSE` in both catch blocks of the `execute` method.

The key points reflecting a bug fix include:
1. The original behavior logged exceptions at the error level (`Project.MSG_ERR`). According to the commit message and typical logging practices, this was incorrect for the intended transparency of the `taskadapter`.
2. The changes rectify the behavior, ensuring exceptions are logged at the verbose level (`Project.MSG_VERBOSE`), consistent with the commit message and the intended functionality.

Therefore, the changes in the diff successfully address the problem outlined in the commit message by changing the logging level, which constitutes a correct and supported behavior modification.

Thus, the assessment is **Buggy**, indicating a bug fix in the code."
ant,6876.json,320e41bc89d8ac60180cbea1c4110dea68cebc33,"@@ -1,3 +1,9 @@
-    public void setTag(String p) {
-	this.tag = p;
-    }

+    public void setTag(String p) { 
+        // Check if not real tag => set it to null 
+        if (p != null) { 
+            if (p.trim().equals("""")) 
+                p = null; 
+        } 
+
+        this.tag = p; 
+    }",Buggy,"Fixed problem when an empty tag was supplied.

Submitted by:	Jean-Noel Gadreau <jngadreau@activcard.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@267670 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for handling empty tags. The diff shows that the `setTag` method is updated to set the tag to `null` if an empty string (or a string with only whitespace) is provided. This resolves potential issues where an empty tag might not have been handled correctly, thus matching the intention in the commit message. Therefore, the changes correspond to a bug fix for the problem when an empty tag was supplied. The answer is **Buggy**."
ant,5580.json,c807cf91c6fa47ef199cce3ae19a9d0e74fb4b18,"@@ -1,28 +1,28 @@
     private int removeEmptyDirectories(File dir, boolean removeIfEmpty,
                                        Set preservedEmptyDirectories) {
         int removedCount = 0;
-        if (!preservedEmptyDirectories.contains(dir) && dir.isDirectory()) {
+        if (dir.isDirectory()) {
             File[] children = dir.listFiles();
             for (int i = 0; i < children.length; ++i) {
                 File file = children[i];
                 // Test here again to avoid method call for non-directories!
-                if (!preservedEmptyDirectories.contains(file)
-                    && file.isDirectory()) {
+                if (file.isDirectory()) {
                     removedCount +=
                         removeEmptyDirectories(file, true,
                                                preservedEmptyDirectories);
                 }
             }
             if (children.length > 0) {
                 // This directory may have become empty...
                 // We need to re-query its children list!
                 children = dir.listFiles();
             }
-            if (children.length < 1 && removeIfEmpty) {
+            if (children.length < 1 && removeIfEmpty
+                && !preservedEmptyDirectories.contains(dir)) {
                 log(""Removing empty directory: "" + dir, Project.MSG_DEBUG);
                 dir.delete();
                 ++removedCount;
             }
         }
         return removedCount;
     }",Buggy,"properly handle non-recursive excludes (breaks one other test because of the same slowscan bug that I'm going to fix later)

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@727978 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message suggests handling non-recursive excludes correctly. The key change in the code is the condition under which a directory is considered for removal. The original code checked if the directory is not in `preservedEmptyDirectories` early on, possibly skipping directories that should be checked deeply. The revised code defers this check to the last condition when deciding to delete, thus improving the logic to prevent premature removal.

The change specifically affects the logic of when a directory is deleted. Originally, if a directory was in `preservedEmptyDirectories`, it would not attempt to remove empty directories within it. In the updated version, the directory is only checked against `preservedEmptyDirectories` right before deletion, allowing the process to correctly account for all potential empty directories inside.

This updated logic aligns with the commit message regarding handling exclusions properly. This suggests the presence of a bug in how preservations were previously handled, and the fix now correctly processes non-recursive exclusions.

Thus, the changes in the diff correspond to fixing a logical error regarding directory deletions based on exclusions, and the commit message adequately describes the intent to handle these exclusions correctly, evidently pointing to a bug fix. Therefore, the conclusion is **Buggy**."
ant,244.json,99684cfd77a3851d098824e9eb871cd6c087934f,"@@ -1,79 +1,79 @@
     private byte[] createCentralFileHeader(ZipEntry ze, ByteBuffer name, long lfhOffset,
                                            boolean needsZip64Extra) throws IOException {
         byte[] extra = ze.getCentralDirectoryExtra();
 
         // file comment length
         String comm = ze.getComment();
         if (comm == null) {
             comm = """";
         }
 
         ByteBuffer commentB = getEntryEncoding(ze).encode(comm);
         final int nameLen = name.limit() - name.position();
         final int commentLen = commentB.limit() - commentB.position();
         int len= CFH_FILENAME_OFFSET + nameLen + extra.length + commentLen;
         byte[] buf = new byte[len];
 
         System.arraycopy(CFH_SIG,  0, buf, CFH_SIG_OFFSET, WORD);
 
         // version made by
         // CheckStyle:MagicNumber OFF
         putShort((ze.getPlatform() << 8) | (!hasUsedZip64 ? DATA_DESCRIPTOR_MIN_VERSION : ZIP64_MIN_VERSION),
                 buf, CFH_VERSION_MADE_BY_OFFSET);
 
         final int zipMethod = ze.getMethod();
         final boolean encodable = zipEncoding.canEncode(ze.getName());
         putShort(versionNeededToExtract(zipMethod, needsZip64Extra), buf, CFH_VERSION_NEEDED_OFFSET);
         getGeneralPurposeBits(zipMethod, !encodable && fallbackToUTF8).encode(buf, CFH_GPB_OFFSET);
 
         // compression method
         putShort(zipMethod, buf, CFH_METHOD_OFFSET);
 
 
         // last mod. time and date
         ZipUtil.toDosTime(calendarInstance, ze.getTime(), buf, CFH_TIME_OFFSET);
 
         // CRC
         // compressed length
         // uncompressed length
         putLong(ze.getCrc(), buf, CFH_CRC_OFFSET);
         if (ze.getCompressedSize() >= ZIP64_MAGIC
                 || ze.getSize() >= ZIP64_MAGIC) {
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_COMPRESSED_SIZE_OFFSET);
             ZipLong.ZIP64_MAGIC.putLong(buf, CFH_ORIGINAL_SIZE_OFFSET);
         } else {
             putLong(ze.getCompressedSize(), buf, CFH_COMPRESSED_SIZE_OFFSET);
             putLong(ze.getSize(), buf, CFH_ORIGINAL_SIZE_OFFSET);
         }
 
         putShort(nameLen, buf, CFH_FILENAME_LENGTH_OFFSET);
 
         // extra field length
         putShort(extra.length, buf, CFH_EXTRA_LENGTH_OFFSET);
 
         putShort(commentLen, buf, CFH_COMMENT_LENGTH_OFFSET);
 
         // disk number start
         System.arraycopy(ZERO, 0, buf, CFH_DISK_NUMBER_OFFSET, SHORT);
 
         // internal file attributes
         putShort(ze.getInternalAttributes(), buf, CFH_INTERNAL_ATTRIBUTES_OFFSET);
 
         // external file attributes
         putLong(ze.getExternalAttributes(), buf, CFH_EXTERNAL_ATTRIBUTES_OFFSET);
 
         // relative offset of LFH
         putLong(Math.min(lfhOffset, ZIP64_MAGIC), buf, CFH_LFH_OFFSET);
 
         // file name
         System.arraycopy(name.array(), name.arrayOffset(), buf, CFH_FILENAME_OFFSET, nameLen);
 
         int extraStart = CFH_FILENAME_OFFSET + nameLen;
         System.arraycopy(extra, 0, buf, extraStart, extra.length);
 
-        int commentStart = extraStart + commentLen;
+        int commentStart = extraStart + extra.length;
 
         // file comment
         System.arraycopy(commentB.array(), commentB.arrayOffset(), buf, commentStart, commentLen);
         return buf;
     }",Buggy,"yet another potential AIOBException in zip package

Bug found and fix provided by Earl Hood
",Buggy,"The commit message indicates a potential ArrayIndexOutOfBoundsException (AIOBException) was found and fixed in the zip package. The changes in the git diff provide evidence of this kind of error being potentially addressed.

### Analysis:

1. **Context**: The `createCentralFileHeader` method constructs a byte array representing the central file header for a zip entry. The byte array `buf` is created based on calculated lengths which consider the name, extra bytes, and comment associated with a zip entry.

2. **Bug Location**: The line `int commentStart = extraStart + commentLen;` was changed to `int commentStart = extraStart + extra.length;`.

3. **Nature of the Original Bug**: The potential bug lay in how the starting index for the file comment was calculated. Initially, `commentStart` was computed using `extraStart + commentLen`. This approach mistakenly offset the starting point of the comment segment by `commentLen` instead of using the actual length of the extra data. 

4. **Corrective Measure**: By changing the computation to `int commentStart = extraStart + extra.length;`, the corrected version accurately sets the start of the comment segment immediately after the extra field. This fixes any potential out-of-bounds exceptions by ensuring offsets are calculated based on the actual sections preceding them, thus aligning `commentStart` accurately.

5. **Relevance to Commit Message**: The issue addressed by the change directly aligns with resolving a likely ArrayIndexOutOfBoundsException, as indicated in the commit message. The message specifies a potential AIOBException, and the fix modifies the logic to prevent incorrect memory access when filling the `buf` array.

Based on the above analysis, the code change fixes a legitimate bug matching the bug description in the commit message. Therefore, the conclusion is **Buggy**."
ant,6044.json,e2a62c3179e855f7e102af8701aff81d6ba3c7e8,"@@ -1,56 +1,55 @@
         public int read(char[] cbuf, int off, int len)
             throws IOException {
 
             int amountRead = 0;
             while (pos < sourceFiles.size() || (needAddSeparator)) {
                 if (needAddSeparator) {
                     cbuf[off] = eolString.charAt(lastPos++);
                     if (lastPos >= eolString.length()) {
                         lastPos = 0;
                         needAddSeparator = false;
                         pos++;
                     }
                     len--;
                     off++;
                     amountRead++;
                     if (len == 0) {
                         return amountRead;
                     }
                     continue;
                 }
-
                 int nRead = getReader().read(cbuf, off, len);
                 if (nRead == -1 || nRead == 0) {
                     reader.close();
                     reader = null;
                     if (fixLastLine && isMissingEndOfLine()) {
                         needAddSeparator = true;
                         lastPos = 0;
                     } else {
                         pos++;
                     }
                 } else {
                     if (fixLastLine) {
                         for (int i = nRead;
                                  i > (nRead - lastChars.length);
                                  --i) {
                             if (i < 0) {
                                 break;
                             }
-                            addLastChar(cbuf[off + i]);
+                            addLastChar(cbuf[off + i - 1]);
                         }
                     }
                     len -= nRead;
                     off += nRead;
                     amountRead += nRead;
                     if (len == 0) {
                         return amountRead;
                     }
                 }
             }
             if (amountRead == 0) {
                 return -1;
             } else {
                 return amountRead;
             }
         }",Buggy,"Fix off-by-one error in concat with fixlastline=""yes""
PR: 25464
Obtained from: wang liang


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@275773 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that an ""off-by-one error"" has been fixed in the concat function with `fixLastLine` enabled. The given diff reflects this fix by modifying the index inside a for loop:

- **Original Code:** `addLastChar(cbuf[off + i]);`
- **Updated Code:** `addLastChar(cbuf[off + i - 1]);`

The correction shifts the index by one when calling `addLastChar`. This matches the typical pattern of fixing off-by-one errors which often occur when dealing with array indexes where the correct positioning is crucial. Without this fix, the function could have incorrectly processed the end of the buffer, potentially leading to out-of-bounds access or incorrect handling of the last character.

This adjustment aligns with the claim made in the commit message and is indeed indicative of a bug fix.

Thus, the verdict on whether the changes indicate a bug fix is **Buggy**."
ant,8293.json,feb03ce2d86cb52fbe8ce55b6b74dfe2536924b1,"@@ -1,5 +1,5 @@
-    protected Object clone() throws CloneNotSupportedException {
+    public Object clone() throws CloneNotSupportedException {
         Assertions that = (Assertions) super.clone();
         that.assertionList = (ArrayList) assertionList.clone();
         return that;
     }",Buggy,"added cloning support; bug #27218


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@276160 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code diff reflects a change from a protected to a public access modifier in the `clone` method, implementing Java's `Cloneable` pattern. This aligns with the commit message, which indicates adding cloning support as a means to address a reported bug (#27218). The change would fix issues if cloning was meant to be accessible outside the class. The commit message also implies fixing a bug, which is supported by the code change.

1. **Cloning Support**: The commit message mentions adding cloning support, which is consistent with making the `clone` method public.
2. **Bug Reference**: The message references a specific bug number, suggesting this change addresses a particular issue related to clone accessibility.

Considering these factors, the changes are likely involved in handling a bug regarding object cloning. Therefore, the conclusion is **Buggy**."
ant,4097.json,84d3e4a158b155664d401f0d8b57124504532b74,"@@ -1,3 +1,11 @@
     public void setGlib(String superGrammar) {
-        this.superGrammar = superGrammar;
+        String sg = null;
+        if (Os.isFamily(""dos"")) {
+            sg = superGrammar.replace('\\','/');
+        }
+        else
+        {
+            sg = superGrammar;
+        }
+        this.superGrammar = sg;
     }",Buggy,"All the tests in ANTLRTest.java involving a super-grammar file were failing on Windows.
This change fixes the problem.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@274716 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that tests involving a super-grammar file were failing on Windows, which is common due to differences in file path separators. The code change addresses this by replacing backslashes with forward slashes on DOS-based systems (including Windows). This aligns well with the commit message as it directly addresses the problem of file path normalization on Windows.

The use of `Os.isFamily(""dos"")` checks if the system is part of the DOS OS family, which includes Windows. If true, it modifies the path to use '/' instead of '\\', which is necessary because many language processing or file-handling libraries expect a uniform file separator character.

This change is directly related to fixing the test failures mentioned in the commit message, which were due to platform-specific path handling issues. Thus, the changes effectively resolve a typical platform-dependent bug.

Therefore, the conclusion is **Buggy**."
ant,8414.json,08413ada31e6a9e9dbed3073158d17123d5028e0,"@@ -1,4 +1,6 @@
     private boolean hasPatterns(PatternSet ps) {
-        return ps.getIncludePatterns(getProject()).length > 0
-            || ps.getExcludePatterns(getProject()).length > 0;
+        String[] includePatterns = ps.getIncludePatterns(getProject());
+        String[] excludePatterns = ps.getExcludePatterns(getProject());
+        return (includePatterns != null && includePatterns.length > 0)
+            || (includePatterns != null && excludePatterns.length > 0);
     }",Buggy,"Fix Bug 42397: NPE in <path><files refid>

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@540055 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff makes changes to the method `hasPatterns` that check for nullity before checking the length of the arrays returned by `ps.getIncludePatterns(getProject())` and `ps.getExcludePatterns(getProject())`. 

Before this change, the code could potentially throw a NullPointerException (NPE) if either `getIncludePatterns` or `getExcludePatterns` returned `null`. The updated code first checks if the arrays are not null before attempting to access their length, which prevents the occurrence of a NullPointerException.

The commit message specifically mentions fixing an NPE, and the changes directly address this by introducing these null checks. Therefore, the modifications align with the description in the commit message as they prevent a bug related to null values, which was causing the NPE. 

Thus, the answer is **Buggy**."
ant,7366.json,2a5857c384ef5a9e02b4264be44bf67f3a584d57,"@@ -1,48 +1,48 @@
     public PlanarImage executeDrawOperation() {
         log(""\tCreating Rectangle w="" + width + "" h="" + height + "" arcw=""
             + arcwidth + "" arch="" + archeight);
         BufferedImage bi = new BufferedImage(width, height, BufferedImage.TYPE_4BYTE_ABGR_PRE);
 
         Graphics2D graphics = (Graphics2D) bi.getGraphics();
 
         if (!stroke.equals(""transparent"")) {
             BasicStroke bStroke = new BasicStroke(stroke_width);
             graphics.setColor(ColorMapper.getColorByName(stroke));
             graphics.setStroke(bStroke);
 
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.drawRoundRect(0, 0, width, height, arcwidth, archeight);
             } else {
                 graphics.drawRect(0, 0, width, height);
             }
         }
 
         if (!fill.equals(""transparent"")) {
             graphics.setColor(ColorMapper.getColorByName(fill));
             if ((arcwidth != 0) || (archeight != 0)) {
                 graphics.fillRoundRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2),
                     arcwidth, archeight);
             } else {
                 graphics.fillRect(stroke_width, stroke_width,
                     width - (stroke_width * 2), height - (stroke_width * 2));
             }
         }
 
 
         final int size = instructions.size();
         for (int i = 0; i < size; i++) {
             ImageOperation instr = ((ImageOperation) instructions.elementAt(i));
             if (instr instanceof DrawOperation) {
                 PlanarImage img = ((DrawOperation) instr).executeDrawOperation();
                 graphics.drawImage(img.getAsBufferedImage(), null, 0, 0);
             } else if (instr instanceof TransformOperation) {
-                graphics = (Graphics2D) bi.getGraphics();
                 PlanarImage image
                     = ((TransformOperation) instr)
                     .executeTransformOperation(PlanarImage.wrapRenderedImage(bi));
                 bi = image.getAsBufferedImage();
+                graphics = (Graphics2D) bi.getGraphics();
             }
         }
         return PlanarImage.wrapRenderedImage(bi);
     }",Buggy,"port image type bug fixes from Java8 refactoring in master
",Buggy,"The commit message indicates that this change ports image type bug fixes from a Java 8 refactor. The provided diff only removes and then immediately reinstates a line of code inside a loop, which suggests that the order of operations related to the `graphics` object is altered. Previously, `graphics` would be reset before each iteration of the loop for `TransformOperation`, potentially leading to drawing on outdated graphics contexts. The change ensures that `graphics` is reassigned only after transformation, which likely aligns with bug fixes that are being ported. This adjustment is consistent with addressing issues from earlier implementations and fixing potential bugs due to the improper use of the `graphics` object after rendering transformations. Therefore, the answer is **Buggy**."
ant,9982.json,869b123c0437855adde003d242e4e084e8b35731,"@@ -1,30 +1,31 @@
     public File normalize(final String path) {
         Stack s = new Stack();
         String[] dissect = dissect(path);
         s.push(dissect[0]);
 
         StringTokenizer tok = new StringTokenizer(dissect[1], File.separator);
         while (tok.hasMoreTokens()) {
             String thisToken = tok.nextToken();
             if (""."".equals(thisToken)) {
                 continue;
             } else if ("".."".equals(thisToken)) {
                 if (s.size() < 2) {
-                    throw new BuildException(""Cannot resolve path "" + path);
+                    // Cannot resolve it, so skip it.
+                    return new File(path);
                 }
                 s.pop();
             } else { // plain component
                 s.push(thisToken);
             }
         }
         StringBuffer sb = new StringBuffer();
         for (int i = 0; i < s.size(); i++) {
             if (i > 1) {
                 // not before the filesystem root and not after it, since root
                 // already contains one
                 sb.append(File.separatorChar);
             }
             sb.append(s.elementAt(i));
         }
         return new File(sb.toString());
     }",Buggy,"#40281: ""Cannot resolve path"" error thrown gratuitously.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@432379 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates the addressal of an error where paths could not be resolved appropriately in the given code. Examining the code diff:

1. **Attempt Path Normalization**: The method `normalize` seeks to streamline a given file path.
2. **Stack Usage**: Uses a `Stack` to handle components of the path effectively.
3. **Handling ""."" and "".."" Components**:
   - The `"".""` token signifies current directory and is correctly ignored.
   - For `""..""`, indicative of navigating up one directory, a check ensures sufficient preceding path exists (`s.size() < 2`).
4. **Previous Behavior**: Previously, encountering "".."" without sufficient prior directories would throw a `BuildException` error citing an irresolvable path.
5. **New Behavior**: Rather than raising an exception, the updated code simply returns a new `File` object with the original unresolved `path`.

This modification implies the user’s files will skip further normalization when issues arise, preventing the unwanted ""cannot resolve path"" error. The diff aligns with handling the bug or unhandled scenario described by the commit message by catering to execution without exception interruptions.

Hence, given all these aspects, the changes do indeed align with the described bug fix. Thus, the conclusion for this commit is **Buggy**."
ant,7113.json,2fbb2e62ab0d3d323a75c6baf00a2e675c7fd6f5,"@@ -1,46 +1,55 @@
     protected void slowScan() {
         synchronized (slowScanLock) {
             if (haveSlowResults) {
                 return;
             }
             if (slowScanning) {
                 while (slowScanning) {
                     try {
                         slowScanLock.wait();
                     } catch (InterruptedException e) {
                     }
                 }
                 return;
             }
             slowScanning = true;
         }
         try {
             synchronized (this) {
 
+                // set in/excludes to reasonable defaults if needed:
+                boolean nullIncludes = (includes == null);
+                includes = nullIncludes ? new String[] {""**""} : includes;
+                boolean nullExcludes = (excludes == null);
+                excludes = nullExcludes ? new String[0] : excludes;
+
                 String[] excl = new String[dirsExcluded.size()];
                 dirsExcluded.copyInto(excl);
         
                 String[] notIncl = new String[dirsNotIncluded.size()];
                 dirsNotIncluded.copyInto(notIncl);
         
                 for (int i = 0; i < excl.length; i++) {
                     if (!couldHoldIncluded(excl[i])) {
                         scandir(new File(basedir, excl[i]),
                                 excl[i] + File.separator, false);
                     }
                 }
                 for (int i = 0; i < notIncl.length; i++) {
                     if (!couldHoldIncluded(notIncl[i])) {
                         scandir(new File(basedir, notIncl[i]),
                                 notIncl[i] + File.separator, false);
                     }
                 }
+                clearCaches();
+                includes = nullIncludes ? null : includes;
+                excludes = nullExcludes ? null : excludes;
             }
         } finally {
             synchronized (slowScanLock) {
                 haveSlowResults = true;
                 slowScanning = false;
                 slowScanLock.notifyAll();
             }
         }
     }",Buggy,"Not 100% sure this fixes the entire bug, but it eliminates NPEs for me.
PR: 34722


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278219 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message suggests that the purpose of the changes is to address occurrences of a NullPointerException (NPE). Let's analyze the code changes to verify this:

1. **Default Initialization**:
   - The code now sets reasonable default values for the `includes` and `excludes` arrays if they are `null`. Before the modification, if `includes` or `excludes` were `null`, this might have led to NPEs when trying to use them later in the method.
   - `includes` is set to a default value containing `""**""` if it is `null`, which presumably matches all paths.
   - `excludes` is set to an empty array if it is `null`, implying that nothing is excluded by default.

2. **Restoration After Usage**:
   - At the end of the method, `includes` and `excludes` are reset to `null` if they were initially `null`, preserving the original state of the object unless they were modified.

3. **Null Pointer Exception Prevention**:
   - By initializing `includes` and `excludes` to default non-null values, the changes indeed prevent potential NPEs that might occur if any operation on these arrays assumed they were initialized.

Overall, the alterations directly address and help eliminate NPEs by ensuring `includes` and `excludes` have non-null default values, which aligns with the commit message. Although the message expresses some uncertainty about resolving the entire bug, it effectively communicates the intent to fix the NPE issue.

Therefore, based on the modifications made and the problem of NPEs as highlighted in the commit message, the changes suggest a bug fix related to handling null values. The assessment is **Buggy**."
ant,9433.json,fef3ea39f8bd474add292bb6437df6cbd22e1ba7,"@@ -1,47 +1,49 @@
     private static void buildJrePackages() {
         jrePackages = new Vector<String>();
         switch(javaVersionNumber) {
             case VERSION_1_9:
             case VERSION_1_8:
             case VERSION_1_7:
+                jrePackages.addElement(""jdk"");
+                // fall through
             case VERSION_1_6:
             case VERSION_1_5:
                 //In Java1.5, the apache stuff moved.
                 jrePackages.addElement(""com.sun.org.apache"");
                 //fall through.
             case VERSION_1_4:
                 if (javaVersionNumber == VERSION_1_4) {
                     jrePackages.addElement(""org.apache.crimson"");
                     jrePackages.addElement(""org.apache.xalan"");
                     jrePackages.addElement(""org.apache.xml"");
                     jrePackages.addElement(""org.apache.xpath"");
                 }
                 jrePackages.addElement(""org.ietf.jgss"");
                 jrePackages.addElement(""org.w3c.dom"");
                 jrePackages.addElement(""org.xml.sax"");
                 // fall through
             case VERSION_1_3:
                 jrePackages.addElement(""org.omg"");
                 jrePackages.addElement(""com.sun.corba"");
                 jrePackages.addElement(""com.sun.jndi"");
                 jrePackages.addElement(""com.sun.media"");
                 jrePackages.addElement(""com.sun.naming"");
                 jrePackages.addElement(""com.sun.org.omg"");
                 jrePackages.addElement(""com.sun.rmi"");
                 jrePackages.addElement(""sunw.io"");
                 jrePackages.addElement(""sunw.util"");
                 // fall through
             case VERSION_1_2:
                 jrePackages.addElement(""com.sun.java"");
                 jrePackages.addElement(""com.sun.image"");
                 // are there any here that we forgot?
                 // fall through
             case VERSION_1_1:
             default:
                 //things like sun.reflection, sun.misc, sun.net
                 jrePackages.addElement(""sun"");
                 jrePackages.addElement(""java"");
                 jrePackages.addElement(""javax"");
                 break;
         }
     }",Buggy,"fix Bug 59556 - support ""jdk"" package for Java 7+

Patch based on Chris Hegarty (Oracle) work.
""jdk"" package has been introduced in JDK7 but is not known from Ant.
""jdk.net.Sockets"" has been chosen as test class because it is available in JDK7, JDK8 and JDK9.
",Buggy,"The commit message indicates the addition of support for the ""jdk"" package starting from Java 7+, which is not known to Ant according to the message. The change in the code reflects this by adding ""jdk"" to the `jrePackages` for Java versions 1.7 (Java 7) and above. This aligns with the commit message and addresses a compatibility issue for Ant when dealing with newer JDK versions starting from Java 7. 

The change does not fix a traditional runtime bug or an error in logic, but rather updates compatibility with newer Java versions, which aligns with a bug fix in terms of feature support and correctness in configuration. Given the context and the changes being directly tied to supporting a feature that was previously inadequate, this is closer to a ""bug"" fix concerning compatibility.

Considering that this change updates the software's capability to recognize newer features introduced in Java 7 and above, the correct conclusion would be **Buggy**."
lucene-solr,36661.json,d815a3608b2742dad7ed0c0b6233fdea1653c285,"@@ -1,33 +1,33 @@
   private final void count(List<MatchingDocs> matchingDocs) throws IOException {
     IntsRef scratch  = new IntsRef();
     for(MatchingDocs hits : matchingDocs) {
       OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
       FixedBitSet bits = hits.bits;
     
       final int length = hits.bits.length();
       int doc = 0;
       while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
         ords.get(doc, scratch);
         for(int i=0;i<scratch.length;i++) {
-          ++counts[scratch.ints[i]];
+          counts[scratch.ints[scratch.offset+i]]++;
         }
         ++doc;
       }
     }
 
     // nocommit we could do this lazily instead:
 
     // Rollup any necessary dims:
     for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
       String dim = ent.getKey();
       FacetsConfig.DimConfig ft = ent.getValue();
       if (ft.hierarchical && ft.multiValued == false) {
         int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
         // It can be -1 if this field was declared in the
         // config but never indexed:
         if (dimRootOrd > 0) {
           counts[dimRootOrd] += rollup(children[dimRootOrd]);
         }
       }
     }
   }",Buggy,"LUCENE-5339: migrate some more tests; fix 'ignores IntsRef.offset bug' in TaxoFacetCounts; add FacetTestCase.getFacetCounts

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1543506 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix involving the 'IntRef.offset bug' in TaxoFacetCounts. The code change indeed addresses this issue: 

- In the updated line, `counts[scratch.ints[i]]` is replaced with `counts[scratch.ints[scratch.offset+i]]`. This adjustment properly incorporates the `offset` field of the `IntsRef` object. 

- Such a change indicates that previously, when processing the array `scratch.ints`, the offset was ignored, potentially leading to incorrect indexing and counts misalignment, which could affect the facet counting functionality in certain scenarios.

Given this change and the corresponding commit message, it is clear that this is a legitimate bug fix addressing an ""offset"" handling problem in the code. Thus, the answer is **Buggy**."
lucene-solr,26469.json,4336de5e81d66e255abbface1bddf8c18e500a90,"@@ -1,16 +1,16 @@
   private void recordOffsetDiff(int inputLength, int outputLength) {
     if (inputLength == outputLength) {
       charCount += outputLength;
       return;
     }
     final int diff = inputLength - outputLength;
     final int cumuDiff = getLastCumulativeDiff();
     if (diff < 0) {
       for (int i = 1;  i <= -diff; ++i) {
         addOffCorrectMap(charCount + i, cumuDiff - i);
       }
     } else {
-      addOffCorrectMap(charCount + Math.min(1, outputLength), cumuDiff + diff);
+      addOffCorrectMap(charCount + outputLength, cumuDiff + diff);
     }
     charCount += outputLength;
   }",Buggy,"LUCENE-5547: fix bug in offset correction

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1580829 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a bug fix related to offset correction. The provided code diff modifies the logic under certain conditions. Specifically, the change updates how the offset difference is recorded when the input length is greater than the output length. The original logic used `Math.min(1, outputLength)`, which effectively means always adding `1` to `charCount`, regardless of `outputLength`. This seems incorrect when `outputLength > 1`.

The updated code now correctly adjusts `charCount` by adding it to the actual `outputLength`, which seems like a proper correction to ensure accurate recording of offsets. The scenario here indicates a fix to ensure correct mapping between input and output lengths, aligning with the stated bug fix in the commit message.

Therefore, the change correctly addresses a bug related to offset correction, as stated in the commit message. The assessment of the diff and the commit message leads to the conclusion: **Buggy**."
lucene-solr,19683.json,748cb61e824feefe8a6ef0fb2735af183c4afd5a,"@@ -1,9 +1,9 @@
   public IndexOutput createOutput(String name, IOContext context)
       throws IOException {
     if (SEGMENTS_GEN.equals(name)) {
-      return NULL_WRITER;
+      return new NullIndexOutput();
     }
     HdfsFileWriter writer = new HdfsFileWriter(getFileSystem(), new Path(
         hdfsDirPath, name));
     return new HdfsIndexOutput(writer);
   }",Buggy,"SOLR-4916: Fix bugs & usage of NullIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1502167 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bugs related to the use of `NullIndexOutput`. The code change reflects this by replacing the `NULL_WRITER` with `new NullIndexOutput()`. This ensures that each request for an index output returns a new and correctly instantiated `NullIndexOutput`, rather than relying on a potentially erroneous shared instance.

- **Error Handling Update**: The change from `NULL_WRITER` to `new NullIndexOutput()` suggests that the previous implementation may have caused unintended behavior when re-using the same instance across different operations, possibly leading to concurrency issues or incorrect handling of the output.

- **Code Correction**: This update aligns with ensuring that the usage of `NullIndexOutput` is consistent with its intended functionality of providing a no-op operation where required, indicating a logical correction from a potentially buggy static/shared instance approach to a safe, bug-free instantiation.

Therefore, the commit message about fixing bugs aligns well with this code modification, showing signs of a bug fix. The assessment concludes with **Buggy** as the changes indicate bug-related fixes."
lucene-solr,50349.json,b8feb92271c62b3bc1c24e2e3610ecd1cf646eab,"@@ -1,54 +1,60 @@
-  public double surfaceDistance(final GeoPoint p1, final GeoPoint p2) {
-    final double latA = p1.getLatitude();
-    final double lonA = p1.getLongitude();
-    final double latB = p2.getLatitude();
-    final double lonB = p2.getLongitude();
+  public double surfaceDistance(final GeoPoint pt1, final GeoPoint pt2) {
+    final double L = pt2.getLongitude() - pt1.getLongitude();
+    final double U1 = Math.atan((1.0-flattening) * Math.tan(pt1.getLatitude()));
+    final double U2 = Math.atan((1.0-flattening) * Math.tan(pt2.getLatitude()));
 
-    final double L = lonB - lonA;
-    final double oF = 1.0 - this.flattening;
-    final double U1 = Math.atan(oF * Math.tan(latA));
-    final double U2 = Math.atan(oF * Math.tan(latB));
-    final double sU1 = Math.sin(U1);
-    final double cU1 = Math.cos(U1);
-    final double sU2 = Math.sin(U2);
-    final double cU2 = Math.cos(U2);
+    final double sinU1 = Math.sin(U1);
+    final double cosU1 = Math.cos(U1);
+    final double sinU2 = Math.sin(U2);
+    final double cosU2 = Math.cos(U2);
 
-    double sigma, sinSigma, cosSigma;
-    double cos2Alpha, cos2SigmaM;
-    
+    final double dCosU1CosU2 = cosU1 * cosU2;
+    final double dCosU1SinU2 = cosU1 * sinU2;
+
+    final double dSinU1SinU2 = sinU1 * sinU2;
+    final double dSinU1CosU2 = sinU1 * cosU2;
+
+
     double lambda = L;
-    double iters = 100;
+    double lambdaP = Math.PI * 2.0;
+    int iterLimit = 0;
+    double cosSqAlpha;
+    double sinSigma;
+    double cos2SigmaM;
+    double cosSigma;
+    double sigma;
+    double sinAlpha;
+    double C;
+    double sinLambda, cosLambda;
 
     do {
-      final double sinLambda = Math.sin(lambda);
-      final double cosLambda = Math.cos(lambda);
-      sinSigma = Math.sqrt((cU2 * sinLambda) * (cU2 * sinLambda) + (cU1 * sU2 - sU1 * cU2 * cosLambda)
-          * (cU1 * sU2 - sU1 * cU2 * cosLambda));
-      if (Math.abs(sinSigma) < Vector.MINIMUM_RESOLUTION)
+      sinLambda = Math.sin(lambda);
+      cosLambda = Math.cos(lambda);
+      sinSigma = Math.sqrt((cosU2*sinLambda) * (cosU2*sinLambda) +
+                                    (dCosU1SinU2 - dSinU1CosU2 * cosLambda) * (dCosU1SinU2 - dSinU1CosU2 * cosLambda));
+
+      if (sinSigma==0.0) {
         return 0.0;
-
-      cosSigma = sU1 * sU2 + cU1 * cU2 * cosLambda;
+      }
+      cosSigma = dSinU1SinU2 + dCosU1CosU2 * cosLambda;
       sigma = Math.atan2(sinSigma, cosSigma);
-      final double sinAlpha = cU1 * cU2 * sinLambda / sinSigma;
-      cos2Alpha = 1.0 - sinAlpha * sinAlpha;
-      cos2SigmaM = cosSigma - 2.0 * sU1 * sU2 / cos2Alpha;
+      sinAlpha = dCosU1CosU2 * sinLambda / sinSigma;
+      cosSqAlpha = 1.0 - sinAlpha * sinAlpha;
+      cos2SigmaM = cosSigma - 2.0 * dSinU1SinU2 / cosSqAlpha;
 
-      final double c = this.flattening * 0.625 * cos2Alpha * (4.0 + this.flattening * (4.0 - 3.0 * cos2Alpha));
-      final double lambdaP = lambda;
-      lambda = L + (1.0 - c) * this.flattening * sinAlpha * (sigma + c * sinSigma * (cos2SigmaM + c * cosSigma *
-          (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)));
-      if (Math.abs(lambda - lambdaP) < Vector.MINIMUM_RESOLUTION)
-        break;
-    } while (--iters > 0);
+      if (Double.isNaN(cos2SigmaM))
+        cos2SigmaM = 0.0;  // equatorial line: cosSqAlpha=0
+      C = flattening / 16.0 * cosSqAlpha * (4.0 + flattening * (4.0 - 3.0 * cosSqAlpha));
+      lambdaP = lambda;
+      lambda = L + (1.0 - C) * flattening * sinAlpha *
+        (sigma + C * sinSigma * (cos2SigmaM + C * cosSigma * (-1.0 + 2.0 * cos2SigmaM *cos2SigmaM)));
+    } while (Math.abs(lambda-lambdaP) > Vector.MINIMUM_RESOLUTION && ++iterLimit < 40);
 
-    if (iters == 0)
-      return 0.0;
+    final double uSq = cosSqAlpha * this.squareRatio;
+    final double A = 1.0 + uSq / 16384.0 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
+    final double B = uSq / 1024.0 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
+    final double deltaSigma = B * sinSigma * (cos2SigmaM + B / 4.0 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM)-
+                                        B / 6.0 * cos2SigmaM * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
 
-    final double uSq = cos2Alpha * this.squareRatio;
-    final double A = 1.0 + uSq * 0.00006103515625 * (4096.0 + uSq * (-768.0 + uSq * (320.0 - 175.0 * uSq)));
-    final double B = uSq * 0.0009765625 * (256.0 + uSq * (-128.0 + uSq * (74.0 - 47.0 * uSq)));
-    final double deltaSigma = B * sinSigma * (cos2SigmaM + B * 0.25 * (cosSigma * (-1.0 + 2.0 * cos2SigmaM * cos2SigmaM) - B * 0.16666666666666666666667 * cos2SigmaM
-            * (-3.0 + 4.0 * sinSigma * sinSigma) * (-3.0 + 4.0 * cos2SigmaM * cos2SigmaM)));
-
-    return this.c * A * (sigma - deltaSigma);
+    return c * A * (sigma - deltaSigma);
   }",Buggy,"LUCENE-7139: fix bugs in geo3d's vincenty distance implementation
",Buggy,"The changes in this diff involve several modifications to the `surfaceDistance` method in the context of calculating distances using Vincenty's formulae. Let's analyze the changes in detail:

1. **Variable Renaming and Initialization**: The diff shows some renaming of variables (e.g., `p1`, `p2` changed to `pt1`, `pt2`) and reorganization of initial variable assignments.

2. **Mathematical Adjustments**: 
   - The computation of `dCosU1CosU2`, `dCosU1SinU2`, `dSinU1SinU2`, and `dSinU1CosU2` consolidates trigonometric operations, making the equations clearer and likely less error-prone.
   - Calculations of `sinSigma`, `cosSigma`, and `cos2SigmaM` are modified to ensure precision and correctness, particularly addressing scenarios where these values might lead to invalid arithmetic operations (such as dividing by zero or encountering NaN values).

3. **Loop Adjustments**: 
   - The termination condition for the while loop has been changed to include checks for NaN and ensures the iteration does not exceed 40 cycles. The previous version limited attempts by a fixed number which could lead to infinite loops if convergence didn't happen within that scope.
   
4. **Angle and Iteration Calculations**: 
   - There are updates to how angles and corresponding changes (`lambda`, `lambdaP`) are computed, focusing on precision and correct handling of edge cases.
   
5. **Additional Condition Handling**: 
   - There's improved handling for cases when `cos2SigmaM` is NaN, setting it to 0. This adjustment prevents subsequent calculations from propagating NaN errors further.

6. **Final Distance Calculation**: 
   - The formula to compute `deltaSigma` has been slightly adjusted to ensure accurate distance measurement without premature terms evaluation.

Considering these changes:

- Many of the adjustments directly address edge cases and incorrect calculations that could have resulted in erroneous distance outputs. 
- Handling of potential NaN values and other special conditions suggests improvements aimed at resolving bugs.
- The mention of fixing bugs in the commit message is consistent with error corrections reflected in code updates.

Therefore, the diff along with the commit message strongly indicates bug-related changes. The conclusion is **Buggy**."
lucene-solr,8422.json,dfdd1b7363afa9e56a47fec7b92960468edd7763,"@@ -1,37 +1,37 @@
   public List<ZkCoreNodeProps> getReplicaProps(String collection,
       String shardId, String thisNodeName, String coreName, String stateFilter) {
     CloudState cloudState = this.cloudState;
     if (cloudState == null) {
       return null;
     }
     Map<String,Slice> slices = cloudState.getSlices(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
           ""Could not find collection in zk: "" + collection + "" ""
               + cloudState.getCollections());
     }
     
     Slice replicas = slices.get(shardId);
     if (replicas == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST, ""Could not find shardId in zk: "" + shardId);
     }
     
     Map<String,ZkNodeProps> shardMap = replicas.getShards();
     List<ZkCoreNodeProps> nodes = new ArrayList<ZkCoreNodeProps>(shardMap.size());
     String filterNodeName = thisNodeName + ""_"" + coreName;
     for (Entry<String,ZkNodeProps> entry : shardMap.entrySet()) {
       ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
-      String coreNodeName = nodeProps.getNodeName() + ""_"" + coreName;
+      String coreNodeName = nodeProps.getNodeName() + ""_"" + nodeProps.getCoreName();
       if (cloudState.liveNodesContain(nodeProps.getNodeName()) && !coreNodeName.equals(filterNodeName)) {
         if (stateFilter == null || stateFilter.equals(nodeProps.getState())) {
           nodes.add(nodeProps);
         }
       }
     }
     if (nodes.size() == 0) {
       // no replicas - go local
       return null;
     }
 
     return nodes;
   }",Buggy,"SOLR-3108: Error in SolrCloud's replica lookup code when replica's are hosted in same Solr instance (fix only)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1242212 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix in SolrCloud's replica lookup code when replicas are hosted on the same Solr instance. The diff shows a change in forming `coreNodeName` by using `nodeProps.getCoreName()` instead of `coreName` directly. This change likely corrects how the core node name is generated, preventing incorrect filtering of replica nodes from the same instance. 

When replicas are hosted on the same Solr instance, it is crucial to distinguish them accurately based on their actual core name rather than a potentially incorrect or hardcoded value. This change is consistent with the commit message and corrects a logical error in identifying the proper set of replicas, aligning with the description of a bug fix.

Therefore, the change is indeed a bug fix, and the answer is **Buggy**."
lucene-solr,10932.json,56476fb8c70486c21324879f1bd1efc79df92ffb,"@@ -1,16 +1,24 @@
   public Map<String, Metric> getMetrics() {
     final Map<String, Metric> metrics = new HashMap<>();
 
     try {
       final ObjectName on = new ObjectName(""java.lang:type=OperatingSystem"");
       // verify that it exists
-      mBeanServer.getMBeanInfo(on);
+      MBeanInfo info = mBeanServer.getMBeanInfo(on);
+      // collect valid attributes
+      Set<String> attributes = new HashSet<>();
+      for (MBeanAttributeInfo ai : info.getAttributes()) {
+        attributes.add(ai.getName());
+      }
       for (String metric : METRICS) {
-        metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        // verify that an attribute exists before attempting to add it
+        if (attributes.contains(metric)) {
+          metrics.put(metric, new JmxAttributeGauge(mBeanServer, on, metric));
+        }
       }
     } catch (JMException ignored) {
       log.debug(""Unable to load OperatingSystem MBean"", ignored);
     }
 
     return metrics;
   }",Buggy,"SOLR-9805 Fix assertion error on Windows where SystemLoadAverage is reported as -1.
Don't expose non-existent attributes.
",Buggy,"The commit message describes a fix for an assertion error that occurs on Windows when SystemLoadAverage is reported as -1 and ensures non-existent attributes aren't exposed.

The code changes in the diff add additional checks to ensure only existing attributes are added to the metrics map:

1. **Retrieval of MBeanInfo**: 
   - The code now retrieves `MBeanInfo` to inspect available attributes of the OperatingSystem MBean.

2. **Attribute Validation**:
   - It collects attribute names into a set.
   - Before adding a metric to the map, it checks if the attribute for that metric exists in the set of available attributes.
   - This prevents attempts to access non-existent attributes, which could lead to errors (for instance, assertion errors mentioned in the commit message).

These changes directly address the issue described in the commit message by ensuring only valid attributes are exposed, thus eliminating the possibility of attempting to read a non-existent attribute like SystemLoadAverage when it isn't available (particularly on Windows).

Therefore, the modifications effectively fix the reported issue, which is aligned with the problem described in the commit message.

Conclusion: **Buggy**."
lucene-solr,28559.json,f55c8d1247fa9183f90bf222e827727ba0dee481,"@@ -1,11 +1,11 @@
     public DocsAndPositionsEnum reset(int[] postings, byte[] payloadBytes) {
       this.postings = postings;
       upto = 0;
       skipPositions = 0;
       startOffset = -1;
       endOffset = -1;
       docID = -1;
       payloadLength = 0;
-      payload.bytes = payloadBytes;
+      this.payloadBytes = payloadBytes;
       return this;
     }",Buggy,"fix bugs in DirectPF's lowFreq d-and-p-enum, set payload.bytes/offset/length in getPayload, also skip payload pointer correctly when scanning over deleted docs in nextDoc

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1364070 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions fixing bugs related to setting payload.bytes/offset/length in getPayload. The change here corrects the assignment from `payload.bytes = payloadBytes` to `this.payloadBytes = payloadBytes`. This indicates a fix where the wrong or non-existing field `payload.bytes` was being set instead of the actual `payloadBytes` field of the class. This change addresses a potential bug where the payload might not be set correctly during processing, which matches the commit message. Therefore, the classification is **Buggy**."
lucene-solr,2958.json,785bebbcbd8f77ccc6d75acf3fb3d42ee29770fc,"@@ -1,4 +1,4 @@
     public String mapSafeElement(String name) {
       String lowerName = name.toLowerCase(Locale.ROOT);
-      return lowerName.equals(""br"") ? null : lowerName;
+      return (lowerName.equals(""br"") || lowerName.equals(""body"")) ? null : lowerName;
     }",Buggy,"SOLR-8981 remove ""don't test with java-9"" commands; fix bug introduced by TIKA-995 -- doubling of body elements in HTML tags; add copyright info for Jackcess.
",Buggy,"The commit message indicates multiple changes, one of which is a bug fix for ""doubling of body elements in HTML tags"" introduced by TIKA-995. The code change modifies the mapSafeElement method to treat ""body"" tags similarly to ""br"" tags, returning null for them. This suggests the function likely prevents certain tags from being processed further, which aligns with the described bug of ""doubling of body elements."" By nullifying ""body"" tags, it plausibly resolves unintended duplication.

The fix's specificity to the mentioned bug and code logic strongly indicate this change addresses the functionality described in the commit message.

Steps:
1. Analyze the Logic: The function mapSafeElement handles HTML tag names by processing them in lowercase. The original logic returns null for ""br"" and passes other tags unchanged.
2. Identify Addition: The update includes ""body"" alongside ""br"", making mapSafeElement return null for ""body"" tags. This decision matches the bug description, aimed at fixing ""doubling of body elements.""
3. Correlate to Bug: Ensuring ""body"" returns null bypasses further processing, potentially resolving doubly handled body content in larger functionality.

Given this clear alignment between the code change and the bug described in the commit message, the conclusion is **Buggy**."
lucene-solr,21433.json,c0e5935900ef5f0c65b006f3071d2132143dda58,"@@ -1,24 +1,25 @@
   public void init(NamedList args) {
 
-    String patternParam = args.remove(PATTERN_PARAM).toString();
+    Object patternParam = args.remove(PATTERN_PARAM);
 
     if(patternParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + PATTERN_PARAM);
     }
+    
     try {
-      pattern = Pattern.compile(patternParam);      
+      pattern = Pattern.compile(patternParam.toString());      
     } catch (PatternSyntaxException e) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Invalid regex: "" + patternParam, e);
     }                                
 
-    String replacementParam = args.remove(REPLACEMENT_PARAM).toString();
+    Object replacementParam = args.remove(REPLACEMENT_PARAM);
     if(replacementParam == null) {
       throw new SolrException(ErrorCode.SERVER_ERROR, 
                               ""Missing required init parameter: "" + REPLACEMENT_PARAM);
     }
-    replacement = Matcher.quoteReplacement(replacementParam);
+    replacement = Matcher.quoteReplacement(replacementParam.toString());
 
     super.init(args);
   }",Buggy,"fix a small bug around these null checks

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1357983 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that it fixes a small bug concerning null checks. The code modifications support this:

1. The initial code immediately converts initialization parameters to Strings using `toString()`. This could provoke a NullPointerException if the parameter is missing, as `args.remove()` could return null.
2. The updated code separates the removal and type conversion. It checks for null, throwing a SolrException if a parameter isn't provided before using `toString()`. This prevents the potential NullPointerException.

These updates precisely target the risk of the bug described in the commit message by enhancing error handling around null values. Therefore, this commit is relevant to bug fixing.

The changes align with the commit message, as they address handling nulls more defensively, preventing server errors due to missing parameters. Given the nature of the fix, it certainly pertains to improving code robustness by appropriately managing exceptional cases.

The conclusion is **Buggy**."
lucene-solr,48864.json,cd5b023219a4e7cd060e88799581704d15820edf,"@@ -1,11 +1,11 @@
     void mergeAt(int i) {
       assert stackSize > i + 1;
       final int l = runBase(i+1);
       final int pivot = runBase(i);
       final int h = runEnd(i);
       merge(l, pivot, h, pivot - l, h - pivot);
-      for (int j = 1; j <= i+1; ++j) {
+      for (int j = i + 1; j > 0; --j) {
         setRunEnd(j, runEnd(j-1));
       }
       --stackSize;
     }",Buggy,"LUCENE-4839: Fix bug in SorterTemplate.timSort().


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1457315 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly states the intention to fix a bug in the `SorterTemplate.timSort()` method. 

Analyzing the diff:

1. Original Code:
   - The original loop iterates using `j` from 1 to `i+1` increasing, which indicates a forward iteration.

2. Modified Code:
   - The loop now iterates from `i+1` down to 1, decrementing `j`.

This change signifies an adjustment in the order of operations within the `mergeAt` method. In sorting algorithms, the order in which elements are processed is crucial. The alteration from a forward loop to a backward loop suggests a bug where the order of merging and updating the `runEnd` was incorrect. By updating it in reverse order, this change likely corrects a sequence problem or ensures that the merge operations are correctly updating the end boundaries of runs.

Therefore, this diff supports the intention in the commit message by addressing a specific bug related to how runs are merged or how run boundaries are updated during sorting.

In conclusion, this change aligns well with the commit message indicating a bug fix in the sort operation. Therefore, the answer is **Buggy**."
lucene-solr,38904.json,008bc74bebef96414f19118a267dbf982aba58b9,"@@ -1,30 +1,30 @@
     void add(int startOffset, int endOffset) {
       assert enumIdx == -1 : ""bad state"";
 
       // loop backwards since we expect a match at the end or close to it.  We expect O(1) not O(N).
       int pairIdx = numPairs - 1;
       for (; pairIdx >= 0; pairIdx--) {
         int iStartOffset = startOffsets[pairIdx];
         int iEndOffset = endOffsets[pairIdx];
         int cmp = Integer.compare(iStartOffset, startOffset);
         if (cmp == 0) {
           cmp = Integer.compare(iEndOffset, endOffset);
         }
         if (cmp == 0) {
           return; // we already have this offset-pair for this term
         } else if (cmp < 0) {
           break; //we will insert offsetPair to the right of pairIdx
         }
       }
       // pairIdx is now one position to the left of where we insert the new pair
       // shift right any pairs by one to make room
       final int shiftLen = numPairs - (pairIdx + 1);
       if (shiftLen > 0) {
-        System.arraycopy(startOffsets, pairIdx + 2, startOffsets, pairIdx + 3, shiftLen);
-        System.arraycopy(endOffsets, pairIdx + 2, endOffsets, pairIdx + 3, shiftLen);
+        System.arraycopy(startOffsets, pairIdx + 1, startOffsets, pairIdx + 2, shiftLen);
+        System.arraycopy(endOffsets, pairIdx + 1, endOffsets, pairIdx + 2, shiftLen);
       }
       // now we can place the offset pair
       startOffsets[pairIdx + 1] = startOffset;
       endOffsets[pairIdx + 1] = endOffset;
       numPairs++;
     }",Buggy,"LUCENE-8365: Fix ArrayIndexOutOfBoundsException in UnifiedHighlighter

This fixes a ""off by one"" error in the UnifiedHighlighter's code that
is only triggered when two nested SpanNearQueries contain the same term.

Signed-off-by: Simon Willnauer <simonw@apache.org>
Closes #408
",Buggy,"The commit message specifies a fix for an ArrayIndexOutOfBoundsException due to an ""off by one"" error, which aligns with the changes made in the code. Here's a step-by-step analysis:

1. **Initial State**: The code in question deals with inserting offset pairs into arrays named `startOffsets` and `endOffsets`.
  
2. **Original Issue**: The original code attempted to shift elements in these arrays starting from `pairIdx + 2` to `pairIdx + 3`. This shift potentially results in accessing an out-of-bounds index, as the index computation was incorrect for how elements should be shifted to make room for a new element.

3. **Fixed Code**: The modified code now shifts elements from `pairIdx + 1` to `pairIdx + 2`. This adjustment ensures the elements are moved correctly, avoiding the potential out-of-bounds access and correctly preparing to insert the new pair.

4. **Consistency with the Commit Message**: The correction addresses the ""off by one"" issue that could lead to an ArrayIndexOutOfBoundsException. This is exactly what the commit message describes as the problem being fixed.

Given this reasoning, it's clear that the changes are intended to fix a specific bug that aligns with the commit message's description. The appropriate conclusion is **Buggy**."
lucene-solr,2699.json,bee8d7ccb32bc23bd808f729493631b60a64bffb,"@@ -1,18 +1,9 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      boolean aIsNull = a.getValue() == null;
-      boolean bIsNull = b.getValue() == null;
-
-      if (aIsNull && bIsNull) return 0;
-
-      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
-        return aIsNull ? 1
-          : bIsNull ? -1
-          : a.getValue().compareTo(b.getValue());
+      if( direction == FacetSortDirection.ASCENDING ){
+        return a.getValue().compareTo(b.getValue());
       } else {
-        return aIsNull ? -1
-          : bIsNull ? 1
-          : b.getValue().compareTo(a.getValue());
+        return b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"Revert ""SOLR-9981: Performance improvements and bug fixes for the Analytics component""

This reverts commit a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3.
",Buggy,"This commit is a reversion of a previous change identified by the hash a5dce16. The revert removes logic handling null values in a comparator, which previously accommodated null entries by positioning them last for ascending sorts and first for descending. This is indicative of a correction back to original behavior due to potential issues that arose from the previous optimization attempt. Reverting changes often indicates addressing unforeseen bugs introduced by those changes.

Given that this reversion likely addresses issues or bugs introduced in the reverted commit, I conclude the changes indicate a bug correction. Therefore, the assessment of this reversion is **Buggy**."
lucene-solr,2699.json,a5dce163eb09dcc0eb7f7eb81d692bf3d19964a3,"@@ -1,9 +1,18 @@
   public Comparator<Expression> comparator(final FacetSortDirection direction) {
     return (a, b) -> {
-      if( direction == FacetSortDirection.ASCENDING ){
-        return a.getValue().compareTo(b.getValue());
+      boolean aIsNull = a.getValue() == null;
+      boolean bIsNull = b.getValue() == null;
+
+      if (aIsNull && bIsNull) return 0;
+
+      if( direction == FacetSortDirection.ASCENDING ){ // nulls are last for ASC sort
+        return aIsNull ? 1
+          : bIsNull ? -1
+          : a.getValue().compareTo(b.getValue());
       } else {
-        return b.getValue().compareTo(a.getValue());
+        return aIsNull ? -1
+          : bIsNull ? 1
+          : b.getValue().compareTo(a.getValue());
       }
     };
   }",Buggy,"SOLR-9981: Performance improvements and bug fixes for the Analytics component
",Buggy,"The code changes in the diff involve enhancements to the comparator used for sorting `Expression` objects. The changes introduce handling for `null` values:

1. **Null Handling:** The modified comparator checks if either of the `Expression` objects (`a` or `b`) have a `null` value. This is done by introducing two boolean flags, `aIsNull` and `bIsNull`.

2. **Sorting Logic Adjustment:** 
   - For ascending order (`FacetSortDirection.ASCENDING`), if `a` is `null`, it places `a` after `b` by returning `1`. Conversely, if `b` is `null`, it places `b` after `a` by returning `-1`. If neither is `null`, it compares their actual values.
   - For descending order (else case), it handles `null` values in the reverse order: `a` if `null` comes before `b` and vice versa. This is as per typical sorting conventions where `null` values are often placed at the end of lists for ascending sorts and at the beginning for descending sorts.

3. **Bug Fix and Performance Improvement:** The original code did not account for `null` values, which could have led to `NullPointerException`s during sorting. By incorporating null checks, the code is more robust and prevents runtime failures—a likely bug fix. Additionally, it's a performance improvement because it avoids unnecessary exception handling and ensures sorting completes without errors.

The commit message indicates bug fixes and performance improvements, which align with these changes, addressing possible exceptions (bugs) and ensuring efficient sorting operations. Therefore, the changes in this commit align with the bug fix as described.

The conclusion is **Buggy**."
lucene-solr,5640.json,6e4924cbfc828506550fd27b0350e3f12c572746,"@@ -1,65 +1,65 @@
   private ResultSetValueSelector[] constructValueSelectors(ResultSetMetaData metadata) throws SQLException{
     ResultSetValueSelector[] valueSelectors = new ResultSetValueSelector[metadata.getColumnCount()];
     
     for(int columnIdx = 0; columnIdx < metadata.getColumnCount(); ++columnIdx){
       
       final int columnNumber = columnIdx + 1; // cause it starts at 1        
-      final String columnName = metadata.getColumnName(columnNumber);
+      final String columnName = metadata.getColumnLabel(columnNumber);
       String className = metadata.getColumnClassName(columnNumber);
       String typeName = metadata.getColumnTypeName(columnNumber);
             
       if(directSupportedTypes.contains(className)){
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Object obj = resultSet.getObject(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj;
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Short.class.getName() == className){
+      else if(Short.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Short obj = resultSet.getShort(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Integer.class.getName() == className){
+      else if(Integer.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Integer obj = resultSet.getInt(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.longValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
-      else if(Float.class.getName() == className){
+      else if(Float.class.getName().equals(className)) {
         valueSelectors[columnIdx] = new ResultSetValueSelector() {
           public Object selectValue(ResultSet resultSet) throws SQLException {
             Float obj = resultSet.getFloat(columnNumber);
             if(resultSet.wasNull()){ return null; }
             return obj.doubleValue();
           }
           public String getColumnName() {
             return columnName;
           }
         };
       }
       else{
         throw new SQLException(String.format(Locale.ROOT, ""Unable to determine the valueSelector for column '%s' (col #%d) of java class '%s' and type '%s'"", columnName, columnNumber, className, typeName));
       }
     }
     
     return valueSelectors;
   }",Buggy,"Update dependencies and fix minor errors
",Buggy,"The commit message states that the update is to fix minor errors. The changes in the code include:

1. Changing `getColumnName(columnNumber)` to `getColumnLabel(columnNumber)`: This may address an issue where `getColumnName` returns the name from the database schema, whereas `getColumnLabel` would return the alias used in SQL queries. This change aligns with handling column aliases.

2. Replacing `==` with `equals` for `String` comparison: The original code used `==` to compare class names, which is unreliable for `String` objects due to reference comparison instead of content comparison. Using `equals` fixes potential logical errors by ensuring content-based comparison.

Both changes address issues (one potential misunderstanding of database metadata, the other actual errors with string comparison), aligning with the commit message's implication of fixing minor errors.

Thus, the changes indicate bug fixes in this context. The conclusion is **Buggy**."
lucene-solr,26464.json,96150badce8234cac00a23c2d5da55545e0be958,"@@ -1,16 +1,24 @@
-  private int readInputToBuffer() throws IOException {
-    final int len = input.read(tmpBuffer);
-    if (len == -1) {
-      inputFinished = true;
-      return 0;
+  private void readInputToBuffer() throws IOException {
+    while (true) {
+      // CharacterUtils.fill is supplementary char aware
+      final boolean hasRemainingChars = CharacterUtils.fill(tmpBuffer, input);
+
+      assert tmpBuffer.getOffset() == 0;
+      inputBuffer.append(tmpBuffer.getBuffer(), 0, tmpBuffer.getLength());
+
+      if (hasRemainingChars == false) {
+        inputFinished = true;
+        break;
+      }
+
+      final int lastCodePoint = Character.codePointBefore(tmpBuffer.getBuffer(), tmpBuffer.getLength());
+      if (normalizer.isInert(lastCodePoint)) {
+        // we require an inert char so that we can normalize content before and
+        // after this character independently
+        break;
+      }
     }
-    inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
-    // so we treat all surrogates as non-inert for simplicity
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
-      return len;
-    } else return len + readInputToBuffer();
   }",Buggy,"LUCENE-7956: Fixed potential stack overflow error in ICUNormalizer2CharFilter.
",Buggy,"The commit message indicates a fix for a potential stack overflow error in the `ICUNormalizer2CharFilter`. Stack overflow errors in recursive methods can be a concern, particularly with large or malformed input, because each recursive call adds a new frame to the call stack. The original code was recursive, as indicated by the self-invocation `readInputToBuffer()` in the `else` clause. Recursion continued until a non-inert character was found, potentially leading to many recursive calls if many inert characters were present or input was particularly lengthy.

The revised code replaces recursion with a `while` loop. This change avoids adding new frames to the stack for each iteration, thus eliminating the potential for a stack overflow error regardless of input size or content. The use of a loop, coupled with checks for buffer content and inert character detection, ensures the function operates iteratively instead of recursively.

Evaluating all of this, the code changes directly correspond to preventing a stack overflow, a common bug-related issue. Therefore, the bug in the commit message is accurately reflected and addressed in the code changes.

**Buggy**"
lucene-solr,26464.json,63d67496cfae2cbe4611b294c18d285e89c8e254,"@@ -1,14 +1,16 @@
   private int readInputToBuffer() throws IOException {
     final int len = input.read(tmpBuffer);
     if (len == -1) {
       inputFinished = true;
       return 0;
     }
     inputBuffer.append(tmpBuffer, 0, len);
 
     // if checkedInputBoundary was at the end of a buffer, we need to check that char again
     checkedInputBoundary = Math.max(checkedInputBoundary - 1, 0);
-    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isHighSurrogate(tmpBuffer[len-1])) {
+    // this loop depends on 'isInert' (changes under normalization) but looks only at characters.
+    // so we treat all surrogates as non-inert for simplicity
+    if (normalizer.isInert(tmpBuffer[len - 1]) && !Character.isSurrogate(tmpBuffer[len-1])) {
       return len;
     } else return len + readInputToBuffer();
   }",Buggy,"fix bug in buffering logic of this charfilter

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1586473 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a bug fix in the buffering logic of a character filter. Let's analyze the changes:

1. **Original Code**:
   - The condition checks if a character is ""inert"" (unchanged by normalization) and not a high surrogate.
   - High surrogates are only half of a surrogate pair, used in UTF-16 to represent characters outside the Basic Multilingual Plane.

2. **Modified Code**:
   - The condition was changed to check if the character is not any kind of surrogate (both high and low surrogates).

3. **Implication**:
   - The original logic only excluded high surrogates, potentially missing cases where a low surrogate should also be excluded.
   - The updated logic simplifies the condition to treat all surrogates as non-inert, ensuring complete surrogate pairs are checked and handled.

4. **Context and Impact**:
   - Treating all surrogates as non-inert helps ensure that surrogate pairs, which have specific handling requirements, don’t trigger incorrect assumptions about non-changes.
   - This likely prevents issues where incomplete surrogate handling could cause unintended behavior during normalization.

The logic improvement aligns with fixing a bug that affects buffering, especially when dealing with Unicode surrogate pairs. This change improves correctness in character processing, as indicated by the commit message.

Thus, given the commit message and the code change focusing on ensuring correct handling of surrogate pairs affecting buffering logic, the conclusion is **Buggy**."
lucene-solr,15099.json,07b37ff26becf214bae000dff08b9091d31327a8,"@@ -1,14 +1,12 @@
   private boolean predefinedPermissionAppliesToRequest(Permission predefinedPermission, AuthorizationContext context) {
-    if (context.getHandler() instanceof PermissionNameProvider) {
+    if (predefinedPermission.wellknownName == PermissionNameProvider.Name.ALL) {
+      return true; //'ALL' applies to everything!
+    } else if (! (context.getHandler() instanceof PermissionNameProvider)) {
+      return false; // We're not 'ALL', and the handler isn't associated with any other predefined permissions
+    } else {
       PermissionNameProvider handler = (PermissionNameProvider) context.getHandler();
       PermissionNameProvider.Name permissionName = handler.getPermissionName(context);
-      if (permissionName == null || !predefinedPermission.name.equals(permissionName.name)) {
-        return false;
-      }
-    } else {
-      //all is special. it can match any
-      if(predefinedPermission.wellknownName != PermissionNameProvider.Name.ALL) return false;
-    }
 
-    return true;
+      return permissionName != null && predefinedPermission.name.equals(permissionName.name);
+    }
   }",Buggy,"SOLR-13355: Obey 'ALL' for handlers with other predefined perms

Prior to this commit, RuleBasedAuthorizationPlugin would check for the
predefined 'ALL' permission only when the endpoint being hit wasn't
associated with another predefined-permission.

This resulted in some very unintuitive behavior. For example, the
permission {name:all, role:admin} would correctly prevent a
role:foo user from accessing /admin/info/properties, but would allow
write access to /admin/authorization because of the SECURITY_EDIT
predefined perm associated with that endpoint.

This commit fixes this bug so that the 'all' permission is always
consulted whether or not the endpoint is associated with other predefined
permissions.
",Buggy,"The commit message indicates that the update addresses a bug related to the handling of permissions in the RuleBasedAuthorizationPlugin. The change in the code modifies the logic to ensure that the 'ALL' permission is always checked, even when a handler has another predefined permission. Here's the analysis of the diff provided:

1. **Logic Change for Permission Check**: 
   - The original implementation checked if the handler was an instance of `PermissionNameProvider` and then compared permission names. However, it had special handling for the 'ALL' permission after those checks.
   - The updated implementation moves the 'ALL' permission check to the beginning of the method. This makes sure that if the permission is 'ALL', it returns `true` immediately as 'ALL' is meant to apply universally.
   - This aligns with the description in the commit message that suggests prior behavior was unintuitive, where 'ALL' permissions were sometimes not applied correctly.

2. **Reorganizing Conditional Logic**: 
   - In the refactored logic, if the handler is not an instance of `PermissionNameProvider`, it directly returns `false`, simplifying the conditions.
   - For handlers that are `PermissionNameProvider` instances, it checks whether the permission name matches, ensuring precise permission application.

3. **Commit Message**: 
   - The commit message effectively describes a previous flaw where 'ALL' permission wasn’t affecting endpoints with other predefined permissions as expected.
   - The update fixes this by always considering the 'ALL' permission, which addresses the described bug where unintuitive permission behavior was observed (like differing access to ‘/admin/info/properties’ and '/admin/authorization').
   
The changes in the diff align well with the commit message, explicitly addressing the bug related to the permission handling logic.

Therefore, I conclude that the commit indicates a **Buggy** fix."
lucene-solr,14965.json,0857bb60d3bb5bd6d8d851ce2e01edd9bde310ea,"@@ -1,20 +1,21 @@
     public void refresh(SolrQueryRequest req, SolrQueryResponse rsp, PayloadObj<String> payload) {
       String p = payload.get();
       if (p == null) {
         payload.addError(""Package null"");
         return;
       }
       PackageLoader.Package pkg = coreContainer.getPackageLoader().getPackage(p);
       if (pkg == null) {
         payload.addError(""No such package: "" + p);
         return;
       }
-
+      //first refresh my own
+      packageLoader.notifyListeners(p);
       for (String s : coreContainer.getPackageStoreAPI().shuffledNodes()) {
         Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(),
             coreContainer.getZkController().zkStateReader.getBaseUrlForNodeName(s).replace(""/solr"", ""/api"") + ""/cluster/package?wt=javabin&omitHeader=true&refreshPackage="" + p,
             Utils.JAVABINCONSUMER);
       }
 
 
     }",Buggy,"SOLR-13662, SOLR-13822: Fixing bug with refresh API, un-ignoring the test and new reference guide on Package Management
",Buggy,"The commit message indicates a bug fix for the refresh API. The code change involves adding functionality to the `refresh` method where it now notifies listeners through `packageLoader.notifyListeners(p)` to refresh their state about the package. This improvement suggests addressing a logic error where the package listeners might not have been refreshed earlier, which aligns with fixing a bug regarding package management consistency.

The added line notifies listeners about a package refresh, thus ensuring that the correct state is communicated across components, which is crucial for maintaining consistent application behavior, especially after a package update.

Given these points, the changes align with the commit message specifying a bug fix in the refresh API, and the reasoning suggests handling logic correctness, often typical in bug resolution.

Therefore, the diffs suggest a bug fix, leading to a conclusion of **Buggy**."
lucene-solr,19226.json,9e78be40c338005b75609a3b123778aea822bcf0,"@@ -1,7 +1,7 @@
   public void setNextReader(LeafReaderContext context) throws IOException {
-    if (globalDocValues instanceof MultiDocValues.MultiSortedDocValues) {
+    if (ordinalMap != null) {
       toGlobal = ordinalMap.getGlobalOrds(context.ord);
-      docValues = DocValues.getSorted(context.reader(), field);
     }
+    docValues = DocValues.getSorted(context.reader(), field);
     lastDocID = 0;
   }",Buggy,"SOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents
",Buggy,"The commit message indicates a bug fix related to setting and resetting string doc-values during document export. The diff shows a modification in a conditional structure within the `setNextReader` method. Originally, `docValues` was set only if `globalDocValues` was an instance of `MultiDocValues.MultiSortedDocValues`. The change makes `docValues` always set, regardless of the condition, while `toGlobal` is set if `ordinalMap` is not null.

Here's a breakdown of how these changes might relate to the described bug:

1. **Condition Update**: 
   - The conditional check changes from `globalDocValues instanceof MultiDocValues.MultiSortedDocValues` to `ordinalMap != null`. This suggests that the logic determining when to compute `toGlobal` has been modified. This could address some logical bug where `ordinalMap` is a more appropriate condition to use.

2. **Always Set `docValues`**:
   - Setting `docValues` wasn't reliably happening due to it being inside the conditional block originally. Now it occurs every time `setNextReader` is called. This change can be crucial for resetting or ensuring `docValues` is prepared for each new context, which aligns with the ""resetting string doc-values"".

3. **Bug Fix Indication**: 
   - The change would fix a bug related to unreliable `docValues` initialization. This would be consistent with the description in the commit message that pertains to setting and resetting issues.

Combined, these changes corroborate the claim made in the commit message about fixing a bug within the context of document value handling in SOLR related to string doc-values. Therefore, the answer is **Buggy**."
lucene-solr,38500.json,1d35bd0ea8beb635997c3cf131ded8ebe58d15a9,"@@ -1,9 +1,9 @@
   public void clearFieldsAfter(Calendar cal, int field) {
     int assertEra = -1;
     assert (assertEra = (((Calendar)cal.clone()).get(Calendar.ERA))) >= 0;//a trick to only get this if assert enabled
     //note: Calendar.ERA == 0;
-    for (int f = field+1; f <= Calendar.MILLISECOND; f++) {
+    for (int f = field + 1; f <= Calendar.MILLISECOND; f++) {
       cal.clear(f);
     }
-    assert ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
+    assert field + 1 == Calendar.ERA || ((Calendar)cal.clone()).get(Calendar.ERA) == assertEra : ""Calendar underflow"";
   }",Buggy,"LUCENE-7278: DRPT: fix bug in assert statement
",Buggy,"The commit message indicates that a bug in an assert statement is being fixed. The diff shows that the assertion logic has been updated to handle a specific case when `field + 1` equals `Calendar.ERA`. The original assertion only checked if the ERA remained the same before and after clearing fields beyond the specified one, but didn't account for the scenario when the field to start clearing from was just before the `ERA`. 

The revised assertion ensures calibration of the ERA whenever `field + 1` matches `Calendar.ERA`. This adjustment is crucial because clearing the fields could potentially affect the ERA field specifically under this circumstance, which the original assertion didn't handle.

Given that a logical check in an assert has been refined to address potential incorrect assertions (calendar underflow situation), it clearly indicates a bug fix.

Therefore, the answer is **Buggy**."
lucene-solr,13814.json,d7f397056bcb52564bf5cbffa96152aa74c0fa1f,"@@ -1,28 +1,29 @@
   public Query parse() throws SyntaxError {
     Query q = super.parse();
     if (!(q instanceof BooleanQuery)) {
       return q;
     }
     BooleanQuery obq = (BooleanQuery)q;
     Collection<Query> should = new ArrayList<Query>();
     Collection<BooleanClause> prohibOrReq = new ArrayList<BooleanClause>();
     BooleanQuery newq = new BooleanQuery();
 
     for (BooleanClause clause : obq.getClauses()) {
       if(clause.isProhibited() || clause.isRequired()) {
         prohibOrReq.add(clause);
       } else {
         BooleanQuery bq = new BooleanQuery();
         bq.add(clause);
         should.add(bq);
       }
     }
     if (should.size() > 0) {
       DisjunctionMaxQuery dmq = new DisjunctionMaxQuery(should, tie);
       newq.add(dmq, BooleanClause.Occur.SHOULD);
     }
     for(BooleanClause c : prohibOrReq) {
       newq.add(c);
     }
+    newq.setBoost(obq.getBoost());
     return newq;
   }",Buggy,"SOLR-4785: Fixed bug with missing boost on toplevel query

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1486898 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that a bug related to missing boosts on top-level queries has been fixed. The change in the code involves setting the boost for the new BooleanQuery `newq` to match that of the original BooleanQuery `obq`. The absence of this line previously meant the boost from the original query was not being carried over to the new query, which could lead to incorrect query scoring and behavior.

Here's the step-by-step reasoning:

1. **Identifying the Issue**: The bug pertains to the missing boost on a top-level query, as described in the commit message.
2. **Code Modification**: The change adds a single line `newq.setBoost(obq.getBoost());`, which sets the boost of the new BooleanQuery to that of the original BooleanQuery.
3. **Understanding the Fix**: By explicitly copying the boost from the original query `obq` to the new query `newq`, it ensures that any boost that was meant to apply to the top-level query is not lost in the translation, thereby fixing the boost-related bug.
4. **Logical Correction**: This modification is a logical correction to make sure the new query mirrors all attributes, including boosts, from the original query, which is essential for proper scoring during query evaluation.

The code change aligns perfectly with the intention stated in the commit message, fixing a specific bug regarding query boosts. Therefore, the assessment is **Buggy**."
lucene-solr,20938.json,88eb9719fa0df2ed0477ec2bb9f20c8ab1644f85,"@@ -1,15 +1,15 @@
   private static boolean matchPath(String path, String name){
     List<String> pathSplit = StrUtils.splitSmart(path, '/');
     List<String> nameSplit = StrUtils.splitSmart(name, '/');
     for (int i = 0; i < nameSplit.size(); i++) {
       String s = nameSplit.get(i);
       String ps = pathSplit.size()>i ?  pathSplit.get(i) :null;
       if(ps == null) return false;
       if(s.equals(ps)) continue;
       if(""*"".equals(ps) && nameSplit.size()==i+1) return true;
       if(""**"".equals(ps)) return true;
       return false;
     }
-    return true;
+    return false;
 
   }",Buggy,"SOLR-6365 bug fix matching wrong  name when it is a shorter prefix of path

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1649996 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change made in the method `matchPath` involves adjusting the return condition at the end of the function. Previously, the function would return `true` if the loop finished without finding a mismatch, indicating a match by default if every condition within the loop was met. The change modifies this final return statement to return `false` instead of `true` when no explicit match is found through the loop conditions.

The commit message mentions a bug fix regarding matching the wrong name when it is a shorter prefix of a path. The code change reflects this intention by concluding mismatches as `false` when the early termination conditions are not met, ensuring that only exact matches, or those allowed by wildcard conditions (""*"" or ""**""), are accounted as valid.

Given this relationship, and the clear correction of logical matching in the code, the change accurately aligns with the bug description in the commit message. Therefore, the code change represents a bug fix.

**Buggy**"
lucene-solr,46829.json,71c65184562499eba365d166fe3fabe0dbdc747b,"@@ -1,37 +1,37 @@
   public static final BinaryDocValues emptyBinary() {
     return new BinaryDocValues() {
       private int doc = -1;
       
       @Override
       public int advance(int target) {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public boolean advanceExact(int target) throws IOException {
         doc = target;
-        return true;
+        return false;
       }
       
       @Override
       public int docID() {
         return doc;
       }
       
       @Override
       public int nextDoc() {
         return doc = NO_MORE_DOCS;
       }
       
       @Override
       public long cost() {
         return 0;
       }
 
       @Override
       public BytesRef binaryValue() {
         assert false;
         return null;
       }
     };
   }",Buggy,"LUCENE-7462: Fix buggy advanceExact impl of empty binary doc values.
",Buggy,"The commit message indicates a fix for a bug in the `advanceExact` method of an empty binary doc values implementation. In the diff, the return value of the `advanceExact` method changes from `true` to `false`. This change corrects the method's logic, as returning `true` when advancing to an exact target document in an empty collection would be incorrect.

- **Original Code:** `advanceExact` returned `true`, suggesting a match was found when advancing to a specific target, which cannot be correct for an empty collection.
- **Fixed Code:** `advanceExact` now returns `false`, accurately representing that no document exists at the target index for this empty structure.

The update aligns with the expectation for an empty data structure where no document exists, correcting a logic error. Thus, this reflects a bug fix in the handling of edge cases for binary document values.

Therefore, the changes are consistent with the commit message and indeed address a bug. The assessment is **Buggy**."
lucene-solr,21965.json,876573650b72adbf1ca005fe0f33607140c23841,"@@ -1,28 +1,28 @@
   public static SchemaField getAndCheckVersionField(IndexSchema schema) 
     throws SolrException {
-    final String errPrefix = VERSION_FIELD + ""field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
+    final String errPrefix = VERSION_FIELD + "" field must exist in schema, using indexed=\""true\"" stored=\""true\"" and multiValued=\""false\"""";
     SchemaField sf = schema.getFieldOrNull(VERSION_FIELD);
 
     if (null == sf) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" does not exist)"");
     }
     if ( !sf.indexed() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not indexed"");
     }
     if ( !sf.stored() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is not stored"");
     }
     if ( sf.multiValued() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
          errPrefix + "" ("" + VERSION_FIELD + "" is multiValued"");
     }
     
     return sf;
   }",Buggy,"SOLR-5259: Fix typo in error message when _version_ field is missing

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1525620 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The change in the diff corrects a minor typo in an error message by adding a space between a concatenated string, ensuring proper message formatting. This is purely a cosmetic fix and does not address any functional or logical issues within the code. The commit message reflects this accurately by stating that it's a fix for a typo.

While correct error messages improve code readability and maintainability, this sort of change is not considered a bug fix in terms of altering code functionality or resolving any errors in the program’s logic.

Thus, the relevance between the commit message and the code modifications is perfectly aligned; however, since it is just changing an error message without altering any functional aspect of the code, it does not qualify as fixing a bug in the code itself.

The answer is **NotBuggy**."
lucene-solr,17330.json,be813bd0aefcf480f854a05c7880494da5e8c8bf,"@@ -1,21 +1,45 @@
   public void validateRouteValue(AddUpdateCommand cmd) throws SolrException {
 
     final Instant docTimestamp =
         parseRouteKey(cmd.getSolrInputDocument().getFieldValue(getRouteField()));
 
     // FUTURE: maybe in some cases the user would want to ignore/warn instead?
     if (docTimestamp.isAfter(Instant.now().plusMillis(getMaxFutureMs()))) {
       throw new SolrException(BAD_REQUEST,
           ""The document's time routed key of "" + docTimestamp + "" is too far in the future given "" +
               ROUTER_MAX_FUTURE + ""="" + getMaxFutureMs());
     }
 
     // Although this is also checked later, we need to check it here too to handle the case in Dimensional Routed
     // aliases where one can legally have zero collections for a newly encountered category and thus the loop later
     // can't catch this.
-    Instant startTime = parseRouteKey(start);
+
+    // SOLR-13760 - we need to fix the date math to a specific instant when the first document arrives.
+    // If we don't do this DRA's with a time dimension have variable start times across the other dimensions
+    // and logic gets much to complicated, and depends too much on queries to zookeeper. This keeps life simpler.
+    // I have to admit I'm not terribly fond of the mutation during a validate method however.
+    Instant startTime;
+    try {
+      startTime = Instant.parse(start);
+    } catch (DateTimeParseException e) {
+      startTime = DateMathParser.parseMath(new Date(), start).toInstant();
+      SolrCore core = cmd.getReq().getCore();
+      ZkStateReader zkStateReader = core.getCoreContainer().getZkController().zkStateReader;
+      Aliases aliases = zkStateReader.getAliases();
+      Map<String, String> props = new HashMap<>(aliases.getCollectionAliasProperties(aliasName));
+      start = DateTimeFormatter.ISO_INSTANT.format(startTime);
+      props.put(ROUTER_START, start);
+
+      // This could race, but it only occurs when the alias is first used and the values produced
+      // should all be identical and who wins won't matter (baring cases of Date Math involving seconds,
+      // which is pretty far fetched). Putting this in a separate thread to ensure that any failed
+      // races don't cause documents to get rejected.
+      core.runAsync(() -> zkStateReader.aliasesManager.applyModificationAndExportToZk(
+          (a) -> aliases.cloneWithCollectionAliasProperties(aliasName, props)));
+
+    }
     if (docTimestamp.isBefore(startTime)) {
       throw new SolrException(BAD_REQUEST, ""The document couldn't be routed because "" + docTimestamp +
           "" is before the start time for this alias "" +start+"")"");
     }
   }",Buggy,"SOLR-13760 - restore viability of date math in TRA start property (#879)

* SOLR-13760 - restore viability of date math in TRA start property by 
fixing the start date for time routed aliases
upon the receipt of the first document to avoid problems
with date math calculations required by DRA's
",Buggy,"The commit message indicates a bug fix related to the viability of date math calculations for time routed aliases (TRA) in Apache Solr. The aim is to ensure a consistent start date for the first incoming document to avoid complications with date math calculations.

1. **Logical Context**: The code change is within the `validateRouteValue` method, which is responsible for checking certain conditions against incoming documents related to their routing.

2. **Changes Made**:
   - The original logic simply parsed a predefined `start` value without additional handling.
   - The modified code introduces error handling for parsing `start` using `Instant.parse()`. If parsing fails due to a `DateTimeParseException`, a fallback is used: computing the start time dynamically with `DateMathParser`.
   - The computed start time is then formatted and saved back into the alias properties, ensuring that the start date is fixed for future operations.
   - Asynchronous update logic is added to handle the new start time mutation in ZooKeeper (Solr's configuration database), updating alias properties without blocking main operations.

3. **Commit Message vs. Code Changes**:
   - The message highlights bug resolution concerning date math calculations in TRA start property. The primary described change is meant to stabilize the influence of date math on the system state.
   - The code changes align with the description. They enhance start time processing for routes by making it variable based on document arrival yet consistent across further invocations. The handling of start time ensures that the variable start due to date math isn't a problem.

4. **Signs of Bug Fixing**:
   - The addition of parsing logic wrapped with exception handling shows an effort to correct issues arising from invalid date formats, a common source of runtime errors.
   - Storing and maintaining the computed `startTime` as an exact instant reduces erratic behavior from fluctuating start values.
   - The utilization of asynchronous behavior to update the start time ensures atomicity and consistency without race-condition risks which could have been previous issues.

Given the modifications aim at resolving inconsistency and errors related to date math, this change is evidently a bug fix for the problems described in the commit message.

Therefore, the provided change embodies a bug fix. The verdict is **Buggy**."
lucene-solr,11507.json,75e69c5198c02e6635eed274b03ea759ef1c4818,"@@ -1,19 +1,19 @@
     public void collect(int doc) throws IOException{
       int valuesDocID = leafOutcomeValue.docID();
       if (valuesDocID < doc) {
-        valuesDocID = leafOutcomeValue.advance(valuesDocID);
+        valuesDocID = leafOutcomeValue.advance(doc);
       }
       int outcome;
       if (valuesDocID == doc) {
         outcome = (int) leafOutcomeValue.longValue();
       } else {
         outcome = 0;
       }
 
       outcome = trainingParams.positiveLabel == outcome? 1 : 0;
       if (outcome == 1) {
         positiveDocsSet.set(context.docBase + doc);
       }
       docsSet.set(context.docBase+doc);
 
     }",Buggy,"SOLR-9549: Fix bug in advancing docValues
",Buggy,"In this diff, the code modifies the logic for advancing the document ID in the `collect` method. Originally, `leafOutcomeValue.advance` was called with `valuesDocID`, which keeps advancing the docValue to its current position or higher, potentially leading to improper alignment between the document being processed and the docValues iterator. The fix changes the code to call `leafOutcomeValue.advance(doc)` instead. This adjustment ensures the docValues iterator advances to the targeted document, which aligns with the process being executed.

The change corrects the logic to prevent scenarios where the iterator might not align with the document being processed, resulting in potential incorrect outcomes being calculated. The commit message explicitly indicates a bug fix for advancing docValues, and the code change straightforwardly aligns with this fix.

Therefore, this commit addresses a bug fix, and the conclusion is **Buggy**."
lucene-solr,6041.json,e4d4e582a0049de34990fcff3df5fb220f14ee4b,"@@ -1,5 +1,13 @@
   public void open() throws IOException {
-    Map<String, List<Tuple>> lets = streamContext.getLets();
-    List<Tuple> tuples = lets.get(name);
-    tupleIterator = tuples.iterator();
+    Map<String, Object> lets = streamContext.getLets();
+    Object o = lets.get(name);
+    List l = null;
+    if(o instanceof List) {
+        l = (List)o;
+      if(l.get(0) instanceof Tuple) {
+        tupleIterator = l.iterator();
+      } else {
+        throw new IOException(""Get was not passed a list of tuples:""+o.getClass());
+      }
+    }
   }",Buggy,"SOLR-10559: Fixed compilation error
",Buggy,"The commit message states that a compilation error is fixed. The code changes address a potential ClassCastException by checking if the retrieved object is indeed a List and contains Tuple instances. If the checks fail, an IOException is thrown. This change prevents runtime errors that occur after successful compilation, but it addresses type safety, which can indirectly affect compilation if incorrect type assumptions were leading to syntactic issues initially.

Given that the purpose aligns with fixing an issue preventing successful compilation (usually implying type mismatches or incorrect class usage), the changes are consistent with addressing that problem. The changes establish correct handling of the lets map and enhance type safety, ensuring the correct operation flow based on the runtime types, which might solve the compilation errors caused by type inconsistency in indirect contexts.

Thus, this enhancement aligns with the commit message and is addressing a potential bug related to handling specific runtime types, even though it is framed under compilation error correction. The conclusion is **Buggy**."
lucene-solr,48392.json,05d62a357711d1e4e850a5d2fb7336bf0a7acf24,"@@ -1,7 +1,9 @@
   public static double haversinSortKey(double lat1, double lon1, double lat2, double lon2) {
     double x1 = lat1 * TO_RADIANS;
     double x2 = lat2 * TO_RADIANS;
     double h1 = 1 - cos(x1 - x2);
     double h2 = 1 - cos((lon1 - lon2) * TO_RADIANS);
-    return h1 + cos(x1) * cos(x2) * h2;
+    double h = h1 + cos(x1) * cos(x2) * h2;
+    // clobber crazy precision so subsequent rounding does not create ties.
+    return Double.longBitsToDouble(Double.doubleToRawLongBits(h) & 0xFFFFFFFFFFFFFFF8L);
   }",Buggy,"LUCENE-7185: fix tie-breaker sort bug
",Buggy,"The commit message mentions fixing a ""tie-breaker sort bug,"" and the code change reflects that. Originally, the haversinSortKey method could produce very precise double values due to floating-point arithmetic, which might not always yield consistent tie-breaking behavior when used for sorting.

The modification introduces a way to clobber or reduce precision by manipulating the bits of the resulting double value. This is done using bitwise operations to mask the least significant bits, helping to ensure that ties are consistently broken by slightly reducing the precision.

The connection between the commit message and the actual code changes is clear. The method now explicitly addresses the precision issue that can lead to sorting ties, which aligns with the described bug fix.

Thus, this change indeed pertains to fixing a bug, as noted in the commit message.

The conclusion is **Buggy**."
lucene-solr,50327.json,97a5295f075d37b1a31c5e77e85f7a9934cae096,"@@ -1,5 +1,5 @@
   public double getLongitude() {
     if (Math.abs(x) < MINIMUM_RESOLUTION && Math.abs(y) < MINIMUM_RESOLUTION)
       return 0.0;
-    return Math.atan2(y,z);
+    return Math.atan2(y,x);
   }",Buggy,"LUCENE-6487: Geo3D with WGS84 patch from Karl: fix bug in GeoPoint.getLongitude with test
from https://reviews.apache.org/r/34744/diff/raw/

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene6487@1682357 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff shows a modification in the calculation of longitude in the `getLongitude` method. Originally, the code used `Math.atan2(y, z)`, which would incorrectly compute the angle given the typical Cartesian coordinates (x, y) for planar coordinates, especially in a spherical context such as WGS84. This change to `Math.atan2(y, x)` corrects the computation to use the proper axis values, which aligns with standard practices for computing angles in a Cartesian plane or sphere.

The commit message clearly mentions a bug fix related to `GeoPoint.getLongitude`, and the code change aligns with that description. Moreover, the change fixes a logical error in the calculation of the longitude.

Therefore, the code change is consistent with the commit message, and it indeed addresses a bug.

The answer is **Buggy**."
lucene-solr,22328.json,942750c33fc97c7f021c4831b61cb617f5cccf24,"@@ -1,69 +1,69 @@
     private long readWord(final int position) {
         if(position < 0) {
             throw new ArrayIndexOutOfBoundsException(position);
         }
 
         // First bit of the word
-        final long firstBitIndex = (position * wordLength);
+        final long firstBitIndex = ((long)position) * ((long)wordLength);
         final int firstByteIndex = (bytePadding + (int)(firstBitIndex / BITS_PER_BYTE));
         final int firstByteSkipBits = (int)(firstBitIndex % BITS_PER_BYTE);
 
         // Last bit of the word
         final long lastBitIndex = (firstBitIndex + wordLength - 1);
         final int lastByteIndex = (bytePadding + (int)(lastBitIndex / BITS_PER_BYTE));
         final int lastByteBitsToConsume;
 
         final int bitsAfterByteBoundary = (int)((lastBitIndex + 1) % BITS_PER_BYTE);
         // If the word terminates at the end of the last byte, consume the whole
         // last byte.
         if(bitsAfterByteBoundary == 0) {
             lastByteBitsToConsume = BITS_PER_BYTE;
         } else {
             // Otherwise, only consume what is necessary.
             lastByteBitsToConsume = bitsAfterByteBoundary;
         }
 
         if(lastByteIndex >= bytes.length) {
             throw new ArrayIndexOutOfBoundsException(""Word out of bounds of backing array."");
         }
 
         // Accumulator
         long value = 0;
 
         // --------------------------------------------------------------------
         // First byte
         final int bitsRemainingInFirstByte = (BITS_PER_BYTE - firstByteSkipBits);
         final int bitsToConsumeInFirstByte = Math.min(bitsRemainingInFirstByte, wordLength);
         long firstByte = (long)bytes[firstByteIndex];
 
         // Mask off the bits to skip in the first byte.
         final long firstByteMask = ((1L << bitsRemainingInFirstByte) - 1L);
         firstByte &= firstByteMask;
         // Right-align relevant bits of first byte.
         firstByte >>>= (bitsRemainingInFirstByte - bitsToConsumeInFirstByte);
 
         value |= firstByte;
 
         // If the first byte contains the whole word, short-circuit.
         if(firstByteIndex == lastByteIndex) {
             return value;
         }
 
         // --------------------------------------------------------------------
         // Middle bytes
         final int middleByteCount = (lastByteIndex - firstByteIndex - 1);
         for(int i=0; i<middleByteCount; i++) {
             final long middleByte = (bytes[firstByteIndex + i + 1] & BYTE_MASK);
             // Push middle byte onto accumulator.
             value <<= BITS_PER_BYTE;
             value |= middleByte;
         }
 
         // --------------------------------------------------------------------
         // Last byte
         long lastByte = (bytes[lastByteIndex] & BYTE_MASK);
         lastByte >>= (BITS_PER_BYTE - lastByteBitsToConsume);
         value <<= lastByteBitsToConsume;
         value |= lastByte;
         return value;
     }",Buggy,"SOLR-7954: Fixed an integer overflow bug in the HyperLogLog code used by the 'cardinality' option of stats.field to prevent ArrayIndexOutOfBoundsException in a distributed search when a large precision is selected and a large number of values exist in each shard

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1697969 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a fix for an integer overflow bug in the HyperLogLog code used by Solr's `cardinality` option of `stats.field`, particularly to prevent an `ArrayIndexOutOfBoundsException` during distributed searches with large precisions and numerous values.

Analyzing the code changes:
- The line `final long firstBitIndex = (position * wordLength);` has been altered to `final long firstBitIndex = ((long)position) * ((long)wordLength);`.

By explicitly casting `position` and `wordLength` to `long` before multiplication, the update prevents potential integer overflow that could occur if the product of `position` and `wordLength` exceeds the range of a 32-bit `int`. This change aligns with the intention stated in the commit message to address an integer overflow bug.

The cast ensures that the multiplication happens within the bounds of a `long` type, which has a wider range and can accommodate larger numbers, effectively preventing the issue where the index calculation might incorrectly lead to an out-of-bounds array access due to overflow.

Given that the change clearly addresses the issue described in the commit message by preventing an integer overflow and subsequent array index issues, this commit indeed fixes a bug.

The analysis concludes with **Buggy**."
lucene-solr,21543.json,3c436362810e80ce036d8785eb03abbda2d10014,"@@ -1,42 +1,42 @@
   private static List<TypeMapping> parseTypeMappings(NamedList args) {
     List<TypeMapping> typeMappings = new ArrayList<TypeMapping>();
     List<Object> typeMappingsParams = args.getAll(TYPE_MAPPING_PARAM);
     for (Object typeMappingObj : typeMappingsParams) {
       if (null == typeMappingObj) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param cannot be null"");
       }
       if ( ! (typeMappingObj instanceof NamedList) ) {
         throw new SolrException(SERVER_ERROR, ""'"" + TYPE_MAPPING_PARAM + ""' init param must be a <lst>"");
       }
       NamedList typeMappingNamedList = (NamedList)typeMappingObj;
 
       Object fieldTypeObj = typeMappingNamedList.remove(FIELD_TYPE_PARAM);
       if (null == fieldTypeObj) {
         throw new SolrException(SERVER_ERROR,
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       if ( ! (fieldTypeObj instanceof CharSequence)) {
         throw new SolrException(SERVER_ERROR, ""'"" + FIELD_TYPE_PARAM + ""' init param must be a <str>"");
       }
       if (null != typeMappingNamedList.get(FIELD_TYPE_PARAM)) {
         throw new SolrException(SERVER_ERROR,
-            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain a '"" + FIELD_TYPE_PARAM + ""' <str>"");
+            ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> may contain only one '"" + FIELD_TYPE_PARAM + ""' <str>"");
       }
       String fieldType = fieldTypeObj.toString();
 
       Collection<String> valueClasses
           = FieldMutatingUpdateProcessorFactory.oneOrMany(typeMappingNamedList, VALUE_CLASS_PARAM);
       if (valueClasses.isEmpty()) {
         throw new SolrException(SERVER_ERROR, 
             ""Each '"" + TYPE_MAPPING_PARAM + ""' <lst/> must contain at least one '"" + VALUE_CLASS_PARAM + ""' <str>"");
       }
       typeMappings.add(new TypeMapping(fieldType, valueClasses));
 
       if (0 != typeMappingNamedList.size()) {
         throw new SolrException(SERVER_ERROR, 
             ""Unexpected '"" + TYPE_MAPPING_PARAM + ""' init sub-param(s): '"" + typeMappingNamedList.toString() + ""'"");
       }
       args.remove(TYPE_MAPPING_PARAM);
     }
     return typeMappings;
   }",Buggy,"SOLR-4894: fix error message

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1503275 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff updates an error message for when multiple instances of `FIELD_TYPE_PARAM` exist in a configuration `NamedList`. The original error message implied that each list must contain a `FIELD_TYPE_PARAM`, aligning with an earlier check in the method; however, the logic indicates the intent is to enforce singularity, not presence, at this specific point. 

By changing ""must contain"" to ""may contain only one,"" the revised message now accurately reflects the actual cause of this error, enhancing clarity for users encountering the error. Even though the change is minor and pertains to an error message, it addresses a specific, likely misleading behavior during misconfiguration.

Conclusively, the commit message accurately reflects the content of this change, as it explicitly mentions fixing an error message. While this doesn't directly fix a 'bug' with regards to erroneous code execution or logic, it does fix a 'documentation' bug where the feedback to the user was incorrect.

Given this analysis: **Buggy**."
lucene-solr,39255.json,33d18a0c599a5bc294f9503a6b8fa3e326f589a7,"@@ -1,114 +1,116 @@
   private void  setInternalDependencyProperties() {
     log(""Loading module dependencies from: "" + moduleDependenciesPropertiesFile, verboseLevel);
     Properties moduleDependencies = new Properties();
     try (InputStream inputStream = new FileInputStream(moduleDependenciesPropertiesFile);
          Reader reader = new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {
       moduleDependencies.load(reader);
     } catch (FileNotFoundException e) {
       throw new BuildException(""Properties file does not exist: "" + moduleDependenciesPropertiesFile.getPath());
     } catch (IOException e) {
       throw new BuildException(""Exception reading properties file "" + moduleDependenciesPropertiesFile.getPath(), e);
     }
     Map<String,SortedSet<String>> testScopeDependencies = new HashMap<>();
     Map<String, String> testScopePropertyKeys = new HashMap<>();
     for (Map.Entry entry : moduleDependencies.entrySet()) {
       String newPropertyKey = (String)entry.getKey();
       StringBuilder newPropertyValue = new StringBuilder();
       String value = (String)entry.getValue();
       Matcher matcher = MODULE_DEPENDENCIES_COORDINATE_KEY_PATTERN.matcher(newPropertyKey);
       if ( ! matcher.matches()) {
         throw new BuildException(""Malformed module dependencies property key: '"" + newPropertyKey + ""'"");
       }
       String antProjectName = matcher.group(1);
       boolean isTest = null != matcher.group(2);
       String artifactName = antProjectToArtifactName(antProjectName);
       newPropertyKey = artifactName + (isTest ? "".internal.test"" : "".internal"") + "".dependencies""; // Add "".internal""
       if (isTest) {
         testScopePropertyKeys.put(artifactName, newPropertyKey);
       }
       if (null == value || value.isEmpty()) {
         allProperties.setProperty(newPropertyKey, """");
         Map<String,SortedSet<String>> scopedDependencies
             = isTest ? testScopeDependencies : internalCompileScopeDependencies;
         scopedDependencies.put(artifactName, new TreeSet<String>());
       } else {
         // Lucene analysis modules' build dirs do not include hyphens, but Solr contribs' build dirs do
         String origModuleDir = antProjectName.replace(""analyzers-"", ""analysis/"");
+        // Exclude the module's own build output, in addition to UNWANTED_INTERNAL_DEPENDENCIES
         Pattern unwantedInternalDependencies = Pattern.compile
-            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
+            (""(?:lucene/build/|solr/build/(?:contrib/)?)"" + origModuleDir + ""/"" // require dir separator 
+             + ""|"" + UNWANTED_INTERNAL_DEPENDENCIES);
         SortedSet<String> sortedDeps = new TreeSet<>();
         for (String dependency : value.split("","")) {
           matcher = SHARED_EXTERNAL_DEPENDENCIES_PATTERN.matcher(dependency);
           if (matcher.find()) {
             String otherArtifactName = matcher.group(1);
             boolean isTestScope = null != matcher.group(2) && matcher.group(2).length() > 0;
             otherArtifactName = otherArtifactName.replace('/', '-');
             otherArtifactName = otherArtifactName.replace(""lucene-analysis"", ""lucene-analyzers"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-solr-"", ""solr-"");
             otherArtifactName = otherArtifactName.replace(""solr-contrib-"", ""solr-"");
             if ( ! otherArtifactName.equals(artifactName)) {
               Map<String,Set<String>> sharedDeps
                   = isTest ? interModuleExternalTestScopeDependencies : interModuleExternalCompileScopeDependencies;
               Set<String> sharedSet = sharedDeps.get(artifactName);
               if (null == sharedSet) {
                 sharedSet = new HashSet<>();
                 sharedDeps.put(artifactName, sharedSet);
               }
               if (isTestScope) {
                 otherArtifactName += "":test"";
               }
               sharedSet.add(otherArtifactName);
             }
           }
           matcher = unwantedInternalDependencies.matcher(dependency);
           if (matcher.find()) {
             continue;  // skip external (/(test-)lib/), and non-jar and unwanted (self) internal deps
           }
           String artifactId = dependencyToArtifactId(newPropertyKey, dependency);
           String groupId = ""org.apache."" + artifactId.substring(0, artifactId.indexOf('-'));
           String coordinate = groupId + ':' + artifactId;
           sortedDeps.add(coordinate);
         }
         if (isTest) {  // Don't set test-scope properties until all compile-scope deps have been seen
           testScopeDependencies.put(artifactName, sortedDeps);
         } else {
           internalCompileScopeDependencies.put(artifactName, sortedDeps);
           for (String dependency : sortedDeps) {
             int splitPos = dependency.indexOf(':');
             String groupId = dependency.substring(0, splitPos);
             String artifactId = dependency.substring(splitPos + 1);
             appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, false, false, null, null);
           }
           if (newPropertyValue.length() > 0) {
             newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
           }
           allProperties.setProperty(newPropertyKey, newPropertyValue.toString());
         }
       }
     }
     // Now that all compile-scope dependencies have been seen, include only those test-scope
     // dependencies that are not also compile-scope dependencies.
     for (Map.Entry<String,SortedSet<String>> entry : testScopeDependencies.entrySet()) {
       String module = entry.getKey();
       SortedSet<String> testDeps = entry.getValue();
       SortedSet<String> compileDeps = internalCompileScopeDependencies.get(module);
       if (null == compileDeps) {
         throw new BuildException(""Can't find compile scope dependencies for module "" + module);
       }
       StringBuilder newPropertyValue = new StringBuilder();
       for (String dependency : testDeps) {
         // modules with separate compile-scope and test-scope POMs need their compile-scope deps
         // included in their test-scope deps.
         if (modulesWithSeparateCompileAndTestPOMs.contains(module) || ! compileDeps.contains(dependency)) {
           int splitPos = dependency.indexOf(':');
           String groupId = dependency.substring(0, splitPos);
           String artifactId = dependency.substring(splitPos + 1);
           appendDependencyXml(newPropertyValue, groupId, artifactId, ""    "", null, true, false, null, null);
         }
       }
       if (newPropertyValue.length() > 0) {
         newPropertyValue.setLength(newPropertyValue.length() - 1); // drop trailing newline
       }
       allProperties.setProperty(testScopePropertyKeys.get(module), newPropertyValue.toString());
     }
   }",Buggy,"LUCENE-6607: Fix spatial3d module's Maven config - include dependency interpolation sites, make packaging jar instead of pom, don't skip deploy phase, etc.; and fix GetMavenDependenciesTask to exclude a module's build artifacts only if the build dir fully matches, rather than a prefix (this bug caused lucene-spatial's test dependency on the lucene-spatial3d jar to be left out of the generated POM, because lucene/build/spatial3d matched the regex for lucene-spatial's build output dir: 'lucene/build/spatial', i.e. with no dir separator)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1690842 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message outlines a fix for a bug in the Maven configuration of the spatial3d module and improvements to the GetMavenDependenciesTask. Specifically, the issue was with incorrect pattern matching that affected the inclusion of dependencies.

Analyzing the diff:
- It modifies the regex pattern used to exclude a module's own build output from dependencies.
- Originally, it matched any directory beginning with 'lucene/build/\<module\>' (e.g., 'lucene/build/spatial'), causing unintended exclusions if the directory name matched partially (prefix without a directory separator).
- The updated pattern requires a directory separator after the module directory name, ensuring exact matches (e.g., 'lucene/build/spatial3d/').

This change addresses the described bug in the commit message regarding the exclusion of dependencies, aligning with the task of correcting dependency patterns. The adjustment prevents the unintended exclusion of artifacts, like the mentioned 'lucene-spatial3d' jar from 'lucene-spatial' test dependencies.

In the context of the commit message, this change is indeed a bug fix related to the Maven build configurations, specifically in how dependencies are determined and included.

Therefore, the conclusion is **Buggy**."
lucene-solr,46217.json,4cfc012bfa66d31b129495b258b5537663f37a10,"@@ -1,4 +1,7 @@
   public IndexWriterConfig setCodec(Codec codec) {
+    if (codec == null) {
+      throw new NullPointerException();
+    }
     this.codec = codec;
     return this;
   }",Buggy,"fix test bug (and fix IWC to fail immediately if you do this)

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4547@1439519 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a ""test bug"" fix and failure modification for ""IWC"". The code adds a null check in `setCodec`, throwing a `NullPointerException` if `codec` is null. This is a clear bug fix because it introduces immediate feedback for invalid input, preventing potential null pointer issues. The answer is **Buggy**."
lucene-solr,49999.json,a14972d7ae2357ab7150909332a04c9a1a94474c,"@@ -1,3 +1,3 @@
     int docId() {
-      return doc > 1 ? NumericDocValues.NO_MORE_DOCS : doc;
+      return doc > 0 ? NumericDocValues.NO_MORE_DOCS : doc;
     }",Buggy,"LUCENE-8055: MemoryIndex.MemoryDocValuesIterator returns 2 documents instead of 1

Fixes a bug if there is a DV field in the MemoryIndex the
`MemoryIndex.MemoryDocValuesIterator` will return 2 documents instead of 1.
",Buggy,"The provided git diff shows a change in the `docId()` method of the `MemoryIndex.MemoryDocValuesIterator`. Originally, the code returned `NumericDocValues.NO_MORE_DOCS` if `doc` was greater than 1. The commit changes the condition to check if `doc` is greater than 0 instead.

The commit message states that there was a bug where the `MemoryIndex.MemoryDocValuesIterator` was returning 2 documents instead of 1. This change aligns with the intention described in the commit message by fixing the logic that determines when to stop iterating documents. Instead of returning an extra document when `doc` was 1, it now correctly stops iteration by returning `NO_MORE_DOCS` when `doc` is any positive number, which suggests the iterator has finished processing the last document.

This change directly addresses the description in the commit message and resolves the stated problem, indicating a bug fix.

**Buggy**"
lucene-solr,5155.json,ebd130b7e2720a4a5cfc3c542461f61932caadf0,"@@ -1,44 +1,54 @@
   public Tuple read() throws IOException {
 
     if(finished) {
       Map<String,Object> m = new HashMap<>();
       m.put(""EOF"", true);
       return new Tuple(m);
     } else {
       finished = true;
       Map<String, Object> values = new HashMap<>();
 
-      String plot= stringParams.get(""type"");
-      StreamEvaluator xvalues = evaluatorParams.get(""x"");
-      StreamEvaluator yvalues = evaluatorParams.get(""y"");
+      // add all string based params
+      // these could come from the context, or they will just be treated as straight strings
+      for(Entry<String,String> param : stringParams.entrySet()){
+        if(streamContext.getLets().containsKey(param.getValue())){
+          values.put(param.getKey(), streamContext.getLets().get(param.getValue()));
+        }
+        else{
+          values.put(param.getKey(), param.getValue());
+        }
+      }
 
-      List<Number> y = (List<Number>)yvalues.evaluateOverContext();
-      List<Number> x = null;
+      // add all evaluators
+      for(Entry<String,StreamEvaluator> param : evaluatorParams.entrySet()){
+        values.put(param.getKey(), param.getValue().evaluateOverContext());
+      }
 
-      if(xvalues == null) {
+      List<Number> y = (List<Number>)values.get(""y"");
+      List<Number> x = (List<Number>)values.get(""x"");
+
+      if(x == null) {
         //x is null so add a sequence
         x = new ArrayList();
         for(int i=0; i<y.size(); i++) {
           x.add(i+1);
         }
-      } else {
-        x = (List<Number>) xvalues.evaluateOverContext();
       }
 
       List<List<Number>> xy = new ArrayList();
       for(int i=0; i<x.size(); i++) {
         List<Number> pair = new ArrayList();
         pair.add(x.get(i));
         pair.add(y.get(i));
         xy.add(pair);
       }
 
-      values.put(""plot"", plot);
+      values.put(""plot"", values.get(""type""));
       values.put(""data"", xy);
 
       Tuple tup = new Tuple(values);
       tup.fieldLabels = fieldLabels;
       tup.fieldNames = fieldNames;
       return tup;
     }
   }",Buggy,"SOLR-10802: Fix problem with variable assignment
",Buggy,"The commit message indicates a fix related to variable assignment, and the diff reflects this by modifying how the `values` map is populated.

1. **Refactoring of Variable Assignment**: 
   - The original code directly assigns certain evaluators and a string parameter to variables `xvalues`, `yvalues`, and `plot`. The new implementation refactors this approach by iterating over `stringParams` and `evaluatorParams` to dynamically populate the `values` map.
   - Parameters are retrieved from a context map (`streamContext.getLets()`), if available, allowing for more flexible and accurate variable resolution. If a parameter is not in the context, its string value is used.
   
2. **Use of Evaluated Values**:
   - Instead of storing the evaluator parameters in separate variables and evaluating them later, the refactor immediately evaluates and stores results in the `values` map.

3. **Bug Fix Indication**:
   - The mapping of `plot` to `values.get(""type"")` instead of directly from `stringParams.get(""type"")` indicates addressing a potential overwriting issue where variable resolution might not have been considering dynamic input sources.

4. **Fix in Variable Population**:
   - This more comprehensive assignment structure suggests a fix related to how values were previously computed or retrieved, making the process less error-prone and more consistent with the intended dynamic behavior of the application.

The changes focus on ensuring the correct retrieval and assignment of variables, which aligns with the commit message regarding a fix in variable assignment. Thus, the changes involve correction and clarification in handling the input variables dynamically and appear to address a problem with prior variable assignments that may have caused incorrect behavior.

Conclusively, the modifications are consistent with the commit message indicating a bug fix in variable assignment processes, leading to the conclusion: **Buggy**."
lucene-solr,7768.json,e81dd4e870d2a9b27e1f4366e92daa6dba054da8,"@@ -1,5 +1,6 @@
     public boolean isNodeAlive(String node) {
-      if (zkClientClusterStateProvider != null && zkClientClusterStateProvider.getLiveNodes().contains(node))
-        return true;
+      if (zkClientClusterStateProvider != null) {
+        return zkClientClusterStateProvider.getLiveNodes().contains(node);
+      }
       return true;
     }",Buggy,"SOLR-12977: fixed bug
",Buggy,"In this diff, we see a modification in the logic of the `isNodeAlive` method. Originally, the code returned `true` if the node was contained within the live nodes set obtained from `zkClientClusterStateProvider`. However, even if `zkClientClusterStateProvider` was null, the method would still return `true`. The change introduces a conditional that checks if `zkClientClusterStateProvider` is not null before evaluating whether the node is contained in the live nodes, ensuring that the return value directly reflects the status of the node. If `zkClientClusterStateProvider` is null, the method will now fall through to return `true`.

This modification suggests the correction of a potential bug where the method could mistakenly return `true` without properly checking the existence of `zkClientClusterStateProvider`, only implying the node's liveness was verified. The change enhances the logical flow by properly isolating the evaluation of both conditions, thereby aligning the logic to its intended purpose of reflecting the accurate node liveness situation.

Thus, this code change correlates with a bug fix as indicated in the commit message, and therefore, I conclude **Buggy**."
lucene-solr,13947.json,9cfba4a728e38a7e6c59c60a377420abc769be46,"@@ -1,39 +1,45 @@
   public static QParser getParser(String qstr, String parserName, boolean allowLocalParams, SolrQueryRequest req) throws SyntaxError {
     // SolrParams localParams = QueryParsing.getLocalParams(qstr, req.getParams());
     if (parserName == null) {
       parserName = QParserPlugin.DEFAULT_QTYPE;//""lucene""
     }
     String stringIncludingLocalParams = qstr;
     ModifiableSolrParams localParams = null;
     SolrParams globalParams = req.getParams();
     boolean valFollowedParams = true;
     int localParamsEnd = -1;
 
     if (allowLocalParams && qstr != null && qstr.startsWith(QueryParsing.LOCALPARAM_START)) {
       localParams = new ModifiableSolrParams();
       localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localParams, globalParams);
 
       String val = localParams.get(QueryParsing.V);
       if (val != null) {
         // val was directly specified in localParams via v=<something> or v=$arg
         valFollowedParams = false;
         //TODO if remainder of query string after '}' is non-empty, then what? Throw error? Fall back to lucene QParser?
       } else {
         // use the remainder of the string as the value
         valFollowedParams = true;
         val = qstr.substring(localParamsEnd);
         localParams.set(QueryParsing.V, val);
       }
 
       parserName = localParams.get(QueryParsing.TYPE,parserName);
-      qstr = localParams.get(""v"");
+      qstr = localParams.get(QueryParsing.V);
     }
 
     QParserPlugin qplug = req.getCore().getQueryPlugin(parserName);
+    if (qplug == null) {
+      // there should a way to include parameter for which parsing failed
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
+          ""invalid query parser '"" + parserName + (stringIncludingLocalParams == null?
+              ""'"": ""' for query '"" + stringIncludingLocalParams + ""'""));
+    }
     QParser parser =  qplug.createParser(qstr, localParams, req.getParams(), req);
 
     parser.stringIncludingLocalParams = stringIncludingLocalParams;
     parser.valFollowedParams = valFollowedParams;
     parser.localParamsEnd = localParamsEnd;
     return parser;
   }",Buggy,"SOLR-13187: Fix NPE when invalid qParser is specified

* When non-existent qParser is specified return 400 error code
* SOLR-13197: Fix NPE when createQParser is called in StatsField
",Buggy,"The commit message indicates a bug fix for a NullPointerException (NPE) when an invalid query parser (qParser) is specified. Let's analyze the diff to verify this:

1. **Null Check for `qplug`:** 
   - The core change involves adding a null check for `qplug` after attempting to retrieve a query parser plugin using `req.getCore().getQueryPlugin(parserName)`.
   - If `qplug` is null (meaning the specified parser name does not exist), a `SolrException` is thrown with `SolrException.ErrorCode.BAD_REQUEST` and an informative error message indicating the invalid parser name and the affected query.

2. **Prevention of NPE:**
   - Previously, if an invalid parser name was provided, attempting to call `createParser` on `qplug` would have resulted in a NullPointerException because `qplug` would be null. The added null check prevents this by throwing a more user-friendly and specific error before such an operation can occur.

3. **Relevance to Commit Message:**
   - The changes align perfectly with the commit message. The NPE fix when an invalid qParser is specified is clearly implemented by adding the null check and throwing a `SolrException` instead.

The logic correction effectively prevents a potential runtime exception (NPE) by handling the scenario where the parser does not exist, replacing it with a controlled error response (400 error code), which confirms it is fixing a bug as described in the commit message.

Therefore, the changes in the commit confirm a bug fix, and the conclusion is **Buggy**."
lucene-solr,47147.json,2d422a995af8132e3d39d5ef576216a3105c457a,"@@ -1,9 +1,14 @@
   public void close() throws IOException {
     try (final OutputStream o = os) {
       // We want to make sure that os.flush() was running before close:
       // BufferedOutputStream may ignore IOExceptions while flushing on close().
-      // TODO: this is no longer an issue in Java 8:
-      // http://hg.openjdk.java.net/jdk8/tl/jdk/rev/759aa847dcaf
-      o.flush();
+      // We keep this also in Java 8, although it claims to be fixed there,
+      // because there are more bugs around this! See:
+      // # https://bugs.openjdk.java.net/browse/JDK-7015589
+      // # https://bugs.openjdk.java.net/browse/JDK-8054565
+      if (!flushedOnClose) {
+        flushedOnClose = true; // set this BEFORE calling flush!
+        o.flush();
+      }
     }
   }",Buggy,"LUCENE-6152: Fix double close bug in OutputStreamIndexOutput

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1648724 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a ""double close bug"" in the `OutputStreamIndexOutput`. Analyzing the code, the change introduces a `flushedOnClose` flag to track if the stream has already been flushed during close. This prevents potential repeated flush calls, which aligns with fixing a ""double close"" scenario by ensuring resources are not redundantly or improperly managed.

The addition of the `flushedOnClose` flag implies that previously, there could have been scenarios where multiple flush attempts occurred, possibly leading to issues if `o.flush()` had side effects or if the underlying stream was not tolerant to repeated operations before closure.

Further commentary in the code highlights awareness of bugs in the JDK related to flushing and encourages retaining the flush operation across Java versions, signifying deliberate handling and avoidance of known issues.

Based on the safe handling of resources and addressing a potential double-operation issue, this change aligns with the commit's intention to fix a bug.

Thus, the answer is **Buggy**."
lucene-solr,5763.json,8dddd88d3054596b7afb536429b42792145fdffe,"@@ -1,51 +1,49 @@
   protected void constructStreams() throws IOException {
 
     try {
 
       ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();
       ClusterState clusterState = zkStateReader.getClusterState();
 
       //System.out.println(""Connected to zk an got cluster state."");
 
       Collection<Slice> slices = clusterState.getActiveSlices(this.collection);
 
       if(slices == null) {
-
-        String colLower = this.collection.toLowerCase(Locale.getDefault());
         //Try case insensitive match
         for(String col : clusterState.getCollections()) {
-          if(col.toLowerCase(Locale.getDefault()).equals(colLower)) {
+          if(col.equalsIgnoreCase(this.collection)) {
             slices = clusterState.getActiveSlices(col);
             break;
           }
         }
 
         if(slices == null) {
           throw new Exception(""Collection not found:"" + this.collection);
         }
       }
 
       params.put(""distrib"",""false""); // We are the aggregator.
 
       for(Slice slice : slices) {
         Collection<Replica> replicas = slice.getReplicas();
         List<Replica> shuffler = new ArrayList();
         for(Replica replica : replicas) {
           shuffler.add(replica);
         }
 
         Collections.shuffle(shuffler, new Random());
         Replica rep = shuffler.get(0);
         ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);
         String url = zkProps.getCoreUrl();
         SolrStream solrStream = new SolrStream(url, params);
         if(streamContext != null) {
           solrStream.setStreamContext(streamContext);
         }
         solrStream.setFieldMappings(this.fieldMappings);
         solrStreams.add(solrStream);
       }
     } catch (Exception e) {
       throw new IOException(e);
     }
   }",Buggy,"Fix more locale bugs in SQL handler ans streams
",Buggy,"The commit message mentions fixing locale bugs, which aligns with the code changes in the diff. Here's the analysis:

1. **Modification of Locale Handling:**
   - The original code uses `this.collection.toLowerCase(Locale.getDefault())` and compares it with `col.toLowerCase(Locale.getDefault())` to perform a case-insensitive match when checking collections. 
   - This involves explicit calls to `toLowerCase(Locale.getDefault())`, which could cause problems if the default locale is sensitive to case transformation, potentially leading to incorrect matches.

2. **Refactoring to `equalsIgnoreCase`:**
   - The updated code uses `col.equalsIgnoreCase(this.collection)` for the collection match.
   - This change directly addresses potential locale issues, as `equalsIgnoreCase` performs a case-insensitive comparison that is independent of locale considerations, thus avoiding locale-dependent bugs.

3. **Bug Fix Indicators:**
   - The specific change from manual case transformation with locale reliance to a built-in method designed for case-insensitivity aligns with fixing bugs that arise due to locale differences.
   - This improves the robustness and correctness of the code, as `equalsIgnoreCase` handles string comparison more safely across different locales.

Given that this change explicitly targets and resolves locale-related bugs by eliminating locale-dependent transformations, the modifications fit the commit message description accurately and indicate a bug fix. Therefore, the conclusion is **Buggy**."
lucene-solr,21252.json,ad453aeeda705e120fb53ba5e8c607cb0a13d85d,"@@ -1,10 +1,10 @@
   private void ensureLog() {
     if (tlog == null) {
-      String newLogName = String.format(Locale.ENGLISH, ""%s.%019d"", TLOG_NAME, id);
+      String newLogName = String.format(Locale.ENGLISH, LOG_FILENAME_PATTERN, TLOG_NAME, id);
       try {
         tlog = new TransactionLog(new File(tlogDir, newLogName), globalStrings);
       } catch (IOException e) {
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, ""Can't open new tlog!"", e);
       }
     }
   }",Buggy,"SOLR-3206: fixed the test, will follow up with a root problem fix too.

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1297927 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that this change is a temporary fix to a test rather than addressing the root cause of a problem. In the diff, a format string is changed from a hardcoded pattern to a pattern variable (LOG_FILENAME_PATTERN), which likely aims to ensure the formatting is consistent with other parts of the application or makes it configurable. 

While the modification might improve the consistency or correctness of log naming, it does not appear to directly address a specific bug but rather adjusts the code in a test helper method. The message indicates that a root problem fix will follow, suggesting that this change alone does not solve the underlying issue.

Given these observations, the diff does not indicate a direct bug fix but a preliminary adjustment before addressing the core problem. Therefore, the conclusion is **NotBuggy**."
lucene-solr,23008.json,da832d4f3aa2e87cf1947ef4373977778ea6d2e0,"@@ -1,30 +1,30 @@
       public Map.Entry<String, String> next() {
         if (!hasNext()) {
           throw new NoSuchElementException();
         }
 
         if (headerValue == null || !headerValue.hasMoreElements()) {
           headerName = headerNameIt.nextElement();
           headerValue = request.getHeaders(headerName);
         }
 
         String key = headerName;
         String val = headerValue.nextElement();
 
-        return new Map.Entry<>() {
+        return new Map.Entry<String, String>() {
           @Override
           public String getKey() {
             return key;
           }
 
           @Override
           public String getValue() {
             return val;
           }
 
           @Override
           public String setValue(String value) {
             throw new UnsupportedOperationException();
           }
         };
       }",Buggy,"SOLR-13434: Fixes problem on Java 8 build
",Buggy,"The diff explicitly adds type parameters `<String, String>` to the `Map.Entry` return type, which is already inferred in Java 8. This change resolves a potential compatibility issue for Java 8 compiler compliance, where diamond operators with anonymous inner classes can cause compilation errors or warnings. The change fixes a Java 8 compatibility problem, aligning with the commit message.

Since it addresses a build problem specific to Java 8 (not a runtime bug affecting logic), it relates more to build compatibility rather than fixing a buggy behavior in a running application. Nonetheless, it resolves an issue pertinent to compiling/building the application, thus the change is correctly aligned with the message about fixing a Java 8 build issue.

Given the context, the resolution of a build-related issue could be considered a fix, albeit not in the operational logic sense. Therefore, for this specific analysis, we could consider it marginally **Buggy** because it resolves a technical issue that could impede proper functioning in certain build environments."
lucene-solr,22004.json,9374fcc6e91c841772fd6d26a7599abcd01dba38,"@@ -1,8 +1,8 @@
   public Query rewrite(IndexReader reader) throws IOException {
     Query rewritten = in.rewrite(reader);
     if (rewritten != in) {
-      return new DeleteByQueryWrapper(in, schema);
+      return new DeleteByQueryWrapper(rewritten, schema);
     } else {
       return this;
     }
   }",Buggy,"LUCENE-5666: fix rewrite bug

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666@1594418 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff addresses a bug in the rewrite method. Initially, the method always wrapped the original query object `in` with `DeleteByQueryWrapper`, regardless of whether `in` was rewritten. If the query was rewritten, this behavior could lead to incorrect results or performance issues since the wrapper should use the rewritten query version instead. The fix updates the behavior to wrap the `rewritten` query only if it differs from `in`. This change directly relates to correcting logic, which aligns with the commit message indicating a ""rewrite bug"" fix.

Therefore, the commit is clearly addressing a bug, and the assessment concludes with **Buggy**."
lucene-solr,12659.json,30de6c512ae51391159b28e0250a9482f12690c7,"@@ -1,23 +1,24 @@
   public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
     if (SortingMergePolicy.isSorted(context.reader(), sort)) {
       // segment is sorted, can early-terminate
       return new FilterLeafCollector(super.getLeafCollector(context)) {
+        private int numCollected;
 
         @Override
         public void collect(int doc) throws IOException {
           super.collect(doc);
           if (++numCollected >= numDocsToCollect) {
             throw new CollectionTerminatedException();
           }
         }
 
         @Override
         public boolean acceptsDocsOutOfOrder() {
           return false;
         }
 
       };
     } else {
       return super.getLeafCollector(context);
     }
   }",Buggy,"LUCENE-5623: fix bug in earlyterminatingcollector, fix test to be reproducible and more evil

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1588953 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff adds a private variable `numCollected` to track the number of documents collected in the `getLeafCollector` method. This ensures that once the number of collected documents (`numCollected`) reaches the `numDocsToCollect` threshold, the collection is terminated early by throwing a `CollectionTerminatedException`.

The commit message indicates a bug fix was needed in the `earlyterminatingcollector`. The change aligns with the message, suggesting the bug was related to not tracking the number of documents correctly, which could have led to improper termination of the collection process.

Furthermore, since the method `acceptsDocsOutOfOrder` is overridden to return `false`, it clarifies the expected order of documents, which suggests correctness in document handling when the segment is sorted.

Overall, these changes address logical corrections to the collection process and improve the reliability of the early termination feature within the `LeafCollector`.

Therefore, the changes in the diff correspond with a bug fix in the `earlyterminatingcollector`. The appropriate conclusion is **Buggy**."
jna,352.json,af1ef9330f4ef7d4f3618d5c4edc8e49b2b4862a,"@@ -1,5 +1,5 @@
-    public static final String getPrivateProfileString(final String appName, final String keyName, final String defaultValue, final String fileName) {
+    public static final String getPrivateProfileString(final String lpAppName, final String lpKeyName, final String lpDefault, final String lpFileName) {
         final char buffer[] = new char[1024];
         Kernel32.INSTANCE.GetPrivateProfileString(appName, keyName, defaultValue, buffer, new DWORD(buffer.length), fileName);
         return Native.toString(buffer);
     }",Buggy,"fix javadoc errors/warnings
",NotBuggy,"The commit message mentions fixing Javadoc errors or warnings, which usually involve documentation rather than changing functionality. The code change updates parameter names in a method signature, likely aligning them with Javadoc comments without affecting logic. There's no indication of bug fixing, just a refactor for clarity or consistency with comments. The answer is **NotBuggy**."
jna,3664.json,fce337b823bcc6130313cd71740b053abf681368,"@@ -1,7 +1,7 @@
     public void read() {
         boolean returnWide = original instanceof WString[];
         for (int si=0;si < original.length;si++) {
             String s = getPointer(si * Pointer.SIZE).getString(0, wide);
-            original[si] = returnWide ? new WString(s) : s; 
+            original[si] = returnWide ? new WString(s) : (Object)s; 
         }
     }",Buggy,"Fix 1.4 compile error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@294 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message indicates that the change fixes a compilation error specific to Java 1.4. The change involves casting a string to an object. This is likely due to stricter type-checking in Java 1.4 where implicit upcasting wasn't inferred. This update would address such an error because the array `original` may be typed as `Object[]`, so casting is necessary. Given the context and Java version, the answer is **Buggy** due to resolving a compilation issue."
jna,3945.json,563931750bb0f5af89da6d55030bdfd4c8854906,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return callFlags + options.hashCode() + peer.hashCode();
+        return callFlags + options.hashCode() + super.hashCode();
     }",Buggy,"fix compiler error

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@1143 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",NotBuggy,"The commit message indicates that the change fixes a compiler error, which often involves resolving issues like syntax errors, type mismatches, or similar problems that prevent successful compilation.

Analyzing the provided diff:

- The `hashCode` method originally returned a combination of `callFlags`, `options.hashCode()`, and `peer.hashCode()`.
- The updated version changes `peer.hashCode()` to `super.hashCode()`, indicating that there was likely an issue with accessing or calling `peer.hashCode()`, possibly due to `peer` not being initialized, not existing in the class, or having a different type that lacked a `hashCode` method.

These changes suggest that the original code may not have compiled due to the attempt to call `hashCode()` on `peer`. By replacing it with `super.hashCode()`, the change corrects this issue.

Given that the modification resolves an error preventing compilation, it aligns with the provided commit message claiming to fix a compiler error. The conclusion for this change is **NotBuggy**, as it doesn't address an application logic error but rather fixes a coding mistake preventing compilation."
jna,4035.json,40f00b1dcba4c24b37452d05cb3118bdbc1480f3,"@@ -1,29 +1,31 @@
     public void setValue(long value) {
         long truncated = value;
         switch (size) {
         case 1:
             truncated = (byte) value;
             this.value = new Byte((byte) value);
             break;
         case 2:
             truncated = (short) value;
             this.value = new Short((short) value);
             break;
         case 4:
             truncated = (int) value;
             this.value = new Integer((int) value);
             break;
         case 8:
             this.value = new Long(value);
             break;
         default:
             throw new IllegalArgumentException(""Unsupported size: "" + size);
         }
-        long mask = (-1L >> size * 8) << (size * 8);
-        if ((value < 0 && truncated != value)
-            || (value >= 0 && (mask & value) != 0)) {
-            throw new IllegalArgumentException(""Argument (0x""
-                + Long.toHexString(value) + "") exceeds native capacity (""
-                + size + "" bytes)"");
+        if (size < 8) {
+            long mask = ~((1L << (size*8)) - 1);
+            if ((value < 0 && truncated != value)
+                    || (value >= 0 && (mask & value) != 0)) {
+                throw new IllegalArgumentException(""Argument value 0x""
+                        + Long.toHexString(value) + "" exceeds native capacity (""
+                        + size + "" bytes) mask=0x"" + Long.toHexString(mask));
+            }
         }
     }",Buggy,"Fix bug checking IntegerType limits

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@391 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",Buggy,"The commit message states a bug fix related to checking IntegerType limits. Here's the breakdown of the changes and how they relate to the commit message:

1. **Original Code Issue**: The original code creates a `mask` to check if a given `value` exceeds the capacity as defined by `size`. However, the creation of the mask (`(-1L >> size * 8) << (size * 8)`) is incorrect and does not correctly represent the boundary of `size` bytes. This could lead to incorrect evaluations of when an `IllegalArgumentException` should be thrown, thus failing its purpose to ensure the value fits within the intended size limit.

2. **Modified Code Improvement**:
   - The revised code correctly calculates `mask` as `~((1L << (size*8)) - 1)`, which accurately represents bits above the intended size boundary that should not contain any significant data.
   - The condition to apply this check `if (size < 8)` ensures that the mask logic is only applied to sizes less than 8 bytes, i.e., 1, 2, and 4-byte integers, accommodating the no-truncation needed for a full 8-byte `long`.

3. **Commit Message Relevance**: The update directly aligns with the commit message claiming a fix for checking limits. It resolves the incorrect mask logic that previously allowed values larger than the designated size to pass through without exception.

These changes correct logic that could lead to integer value overflow exceeding specified limits, effectively fixing the described bug.

Thus, the code modifications reflect a bug fix as described in the commit message. The conclusion is **Buggy**."
spring-framework,28571.json,582864802e49f0cc47575be00194e4194d7b1dc5,"@@ -1,8 +1,11 @@
 	public ModelMap getModel() {
 		if (useDefaultModel()) {
 			return this.defaultModel;
 		}
 		else {
-			return (this.redirectModel != null) ? this.redirectModel : new ModelMap();
+			if (this.redirectModel == null) {
+				this.redirectModel = new ModelMap();
+			}
+			return this.redirectModel;
 		}
 	}",Buggy,"Ensure RedirectModel is initialized

This commit fixes an old bug in ModelAndViewContainer where getModel
returns a new ModelMap instance that isn't saved and re-used.

Issue: SPR-14045
",Buggy,"The code change in the diff directly addresses the issue described in the commit message. Originally, `getModel` would return a new `ModelMap` each time `redirectModel` was `null`, which means the `ModelMap` was not being saved for reuse. With the modification, `redirectModel` is initialized once and stored, ensuring that subsequent calls to `getModel` reuse the same instance. This resolves the bug of not saving and reusing the `ModelMap`, which aligns perfectly with the commit message's explanation. Therefore, this is indeed a bug fix as described.

**Buggy**"
spring-framework,5833.json,ee95f171c8a934080421e802fa6868843af1c45f,"@@ -1,4 +1,4 @@
 	public PropertySources getAppliedPropertySources() throws IllegalStateException {
-		Assert.state(this.appliedPropertySources != null, ""PropertySources have not get been applied"");
+		Assert.state(this.appliedPropertySources != null, ""PropertySources have not yet been applied"");
 		return this.appliedPropertySources;
 	}",Buggy,"Update PropertySourcesPlaceholderConfigurer.java

Fix the error message when PropertySources are not applied",NotBuggy,"The code diff shows a simple change in an error message from ""PropertySources have not get been applied"" to ""PropertySources have not yet been applied."" This is a correction of a typographical error in the error message rather than a functional change in the logic or error-handling behavior of the program. 

The change does not alter the program's operation or error-checking logic; it only improves the clarity and correctness of the error message shown to the end user or developer. There is no underlying bug in the functionality or logic being corrected—just an improvement in the conveyed message.

Therefore, the changes in this commit do not indicate a bug fix in terms of error-handling logic, exception improvements, or logic corrections — only a correction of the message content itself.

Thus, the conclusion is **NotBuggy**."
spring-framework,20130.json,7e799295e55f5e5984a1040a016169ea0883bc2d,"@@ -1,10 +1,10 @@
 	public HttpInputMessage beforeBodyRead(HttpInputMessage request, MethodParameter parameter,
-			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) {
+			Type targetType, Class<? extends HttpMessageConverter<?>> converterType) throws IOException {
 
 		for (RequestBodyAdvice advice : getMatchingAdvice(parameter, RequestBodyAdvice.class)) {
 			if (advice.supports(parameter, targetType, converterType)) {
 				request = advice.beforeBodyRead(request, parameter, targetType, converterType);
 			}
 		}
 		return request;
 	}",Buggy,"Fix compile error
",Buggy,"The diff shows that the method signature for `beforeBodyRead` was updated to include an `IOException` that it can throw. This is indicative of fixing a compile-time error where the method likely needed to declare an `IOException` to match an interface or superclass method definition or to handle an exception that can be thrown within its body. Including this throws declaration resolves that discrepancy, allowing the code to compile correctly. 

Given that compile errors are indeed errors, even if they are not runtime bugs, this type of change aligns with fixing a bug related to incorrect code compilation.

Therefore, the change aligns with the commit message and indicates a fix for a bug (specifically a compile-time bug). The answer is **Buggy**."
spring-framework,24540.json,f084b632864685d8e65d0cdfa719a813824fc59f,"@@ -1,31 +1,36 @@
 	private static String decodeHeaderFieldParam(String input) {
 		Assert.notNull(input, ""Input String should not be null"");
 		int firstQuoteIndex = input.indexOf('\'');
 		int secondQuoteIndex = input.indexOf('\'', firstQuoteIndex + 1);
 		// US_ASCII
 		if (firstQuoteIndex == -1 || secondQuoteIndex == -1) {
 			return input;
 		}
 		Charset charset = Charset.forName(input.substring(0, firstQuoteIndex));
 		Assert.isTrue(UTF_8.equals(charset) || ISO_8859_1.equals(charset),
 				""Charset should be UTF-8 or ISO-8859-1"");
 		byte[] value = input.substring(secondQuoteIndex + 1, input.length()).getBytes(charset);
 		ByteArrayOutputStream bos = new ByteArrayOutputStream();
 		int index = 0;
 		while (index < value.length) {
 			byte b = value[index];
 			if (isRFC5987AttrChar(b)) {
 				bos.write((char) b);
 				index++;
 			}
-			else if (b == '%') {
-				char[] array = { (char)value[index + 1], (char)value[index + 2]};
-				bos.write(Integer.parseInt(String.valueOf(array), 16));
+			else if (b == '%' && index < value.length - 2) {
+				char[] array = new char[]{(char) value[index + 1], (char) value[index + 2]};
+				try {
+					bos.write(Integer.parseInt(String.valueOf(array), 16));
+				}
+				catch (NumberFormatException ex) {
+					throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT, ex);
+				}
 				index+=3;
 			}
 			else {
-				throw new IllegalArgumentException(""Invalid header field parameter format (as defined in RFC 5987)"");
+				throw new IllegalArgumentException(INVALID_HEADER_FIELD_PARAMETER_FORMAT);
 			}
 		}
 		return new String(bos.toByteArray(), charset);
 	}",Buggy,"Fix ""array index out of bounds"" problem reported by LGTM.com
",Buggy,"The commit message clearly states that it fixes an ""array index out of bounds"" issue. Upon analysis of the code diff:

1. The condition `else if (b == '%')` was vulnerable to an ""array index out of bounds"" exception if `index + 1` or `index + 2` exceeded `value.length`. The updated condition `else if (b == '%' && index < value.length - 2)` ensures there are at least two more characters available to read, preventing the bounds error.

2. The try-catch block for `NumberFormatException` is introduced to handle invalid hex sequences in the percent-encoded string. While this addresses potential parsing errors rather than specifically an array index issue, it strengthens error handling.

These changes are directly aligned with the commit message about fixing an ""array index out of bounds"" issue, and they improve error handling related to the decoding logic.

Thus, based on the analysis, the changes do indicate a bug fix. The conclusion is **Buggy**."
sonarqube,18348.json,9b57d0b379935e053ef050b0379c65fc32810bc1,"@@ -1,3 +1,3 @@
   public boolean isEnabled() {
-    return getRule().isEnabled();
+    return getRule()!=null && getRule().isEnabled();
   }",Buggy,"Fix bug on deprecated profiles
",Buggy,"The commit message indicates a bug fix related to ""deprecated profiles,"" but the code change addresses a potential `NullPointerException` by adding a `null` check before calling `getRule().isEnabled()`. This ensures that if `getRule()` returns `null`, the method won't attempt to call `isEnabled()` and will return `false`. This change indeed addresses a logical error and enhances the robustness of the code by preventing a runtime exception.

Given the addition of a `null` check to handle an error condition, this change aligns with a bug fix. Therefore, based on the provided diff and the analysis:

**Buggy**"
sonarqube,4866.json,56963334491068e70db0695c7fc36d9c40bfca9a,"@@ -1,6 +1,7 @@
   private void checkNoOtherMetricWithTargetKey(DbSession dbSession, MetricDto metricInDb, MetricDto template) {
-    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, template.getKey());
+    String targetKey = template.getKey();
+    MetricDto metricWithTargetKey = dbClient.metricDao().selectNullableByKey(dbSession, targetKey);
     if (isMetricFoundInDb(metricWithTargetKey) && !metricInDb.getId().equals(metricWithTargetKey.getId())) {
-      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, ""A me metric exists with the key: "" + metricInDb.getKey());
+      throw new ServerException(HttpURLConnection.HTTP_CONFLICT, String.format(""The key '%s' is already used by an existing metric."", targetKey));
     }
   }",Buggy,"SONAR-6572 WS metrics/update fix error message when updating with an existing key
",Buggy,"The commit message suggests this update is about fixing an error message when updating with an existing key. The code change indeed addresses this by improving the error message for clarity. Initially, the message was vague and slightly contradictory (""A me metric exists with the key""), while the new message explicitly indicates that a key is already in use by another metric and uses the correct key in the message.

The change aligns with the commit message focus. Although the modification primarily improves error message clarity, it directly addresses a potential user-facing issue during key updates. The fact it was mentioned as a fix indicates it was something misleading or confusing, which can be considered a type of bug from the user's perspective (not necessarily a functional bug but a usability issue).

Therefore, the change aligns with the described fix to the error message in the commit message and addresses an issue that could confuse users, supporting the conclusion of **Buggy**."
sonarqube,4786.json,7e1ea5d43a0fd5231baddfc22bc4c76209622968,"@@ -1,14 +1,12 @@
   public void define(WebService.NewController controller) {
     WebService.NewAction action = controller.createAction(""uninstall"")
       .setPost(true)
       .setDescription(""Uninstalls the plugin specified by its key."" +
         ""<br/>"" +
-        ""Plugin information is retrieved from Update Center."" +
-        ""<br/>"" +
-        ""Requires user to be authenticated with Administer System permissions"")
+        ""Requires user to be authenticated with Administer System permissions."")
       .setHandler(this);
 
     action.createParam(PARAM_KEY)
       .setDescription(""The key identifying the plugin to uninstall"")
       .setRequired(true);
   }",Buggy,"SONAR-6380 fix error in action description
",NotBuggy,"The commit message refers to a fix regarding an ""error in action description"". The code diff corresponds to some adjustments in the description text of an action. The change removes redundant information about the plugin information retrieval process and corrects punctuation in the sentence. 

In terms of functionality, the code change improves the textual description associated with the action but does not impact code behavior, logic, or execution. It does not involve error handling, logical corrections, or exception-handling improvements, so it does not fix a ""bug"" in the sense of incorrect functionality in the code logic.

Given this analysis, the change addresses a documentation error, not a functional bug. Thus, the answer is **NotBuggy**."
sonarqube,15877.json,51f8fa984e4a2c11ededba97e6b2c0a2fe29bb02,"@@ -1,8 +1,8 @@
   public String toString() {
     return Objects.toStringHelper(this)
       .add(""key"", qpKey)
       .add(""name"", qpName)
       .add(""language"", languageKey)
-      .add(""rulesUpdatedAt"", rulesUpdatedAt)
+      .add(""rulesUpdatedAt"", rulesUpdatedAt.getTime())
       .toString();
   }",Buggy,"fix some coverage flaws
",NotBuggy,"The commit message indicates fixing some coverage flaws, but the code change modifies how `rulesUpdatedAt` is added to the `toString` method from directly using the `rulesUpdatedAt` object to instead showing its time in milliseconds via `getTime()`. This typically enhances the detail and clarity in the string representation. While this could address a flaw in test coverage if the goal is to ensure clarity or consistency for testing outputs, it does not suggest a correction of functional, logical, or exception-handling errors consistent with a bug fix. 

Given that no apparent runtime or logical bug is evident in the change, and it seems related to improving or clarifying string outputs (possibly for better logging or debugging), the changes align more with improving or extending test coverage rather than directly fixing a bug.

Therefore, the answer is **NotBuggy**."
sonarqube,20277.json,5e5fc1731d0e99ab4a457fb9eeee121f342d1f33,"@@ -1,7 +1,7 @@
   static String[] getListFromProperty(Map<String, String> properties, String key) {
     String propValue = properties.get(key);
     if (propValue != null) {
-      return DefaultConfiguration.parseAsCsv(ProjectDefinition.SOURCES_PROPERTY, propValue);
+      return DefaultConfiguration.parseAsCsv(key, propValue);
     }
     return new String[0];
   }",Buggy,"SONAR-10122 fix misleading error message for multivalue properties

when parsing fails in ProjectReactor
it concerns only sonar.sources, sonar.tests and sonar.modules
",Buggy,"The commit message indicates a fix for a misleading error message related to multivalue properties parsing in ProjectReactor. The code change modifies the method call `parseAsCsv` to use the key directly instead of a constant `SOURCES_PROPERTY`. 

### Analysis:
- **Logical Correction**: Initially, the error message might be misleading because it uses a constant key (`ProjectDefinition.SOURCES_PROPERTY`) instead of the actual key. Changing this to use the `key` parameter directly ensures that any error messages generated reflect the actual property key being processed, improving clarity for specific cases like `sonar.sources`, `sonar.tests`, and `sonar.modules`.
- **Relevance to Commit Message**: The change aligns with the commit message. The code modification directly addresses the problem mentioned by ensuring that error messages or parsing logic reflect the correct property key, thereby fixing the described bug related to misleading error messages.

The change importantly improves error handling by ensuring clarity in the property name reported, which is critical for debugging and user comprehension in tools like ProjectReactor.

Consequently, this update directly pertains to and addresses the bug described in the commit message. Therefore, I will conclude with **Buggy**."
sonarqube,1092.json,e1ecfa7d65c33df5ee054e57e2b5d696da038154,"@@ -1,12 +1,10 @@
   public TokenQueue chunk(Reader reader) {
-    CodeReaderConfiguration codeReaderConfiguration = new CodeReaderConfiguration();
-    codeReaderConfiguration.setBufferCapacity(BUFFER_CAPACITY);
-    CodeReader code = new CodeReader(reader, codeReaderConfiguration);
+    CodeReader code = new CodeReader(reader);
     TokenQueue queue = new TokenQueue();
     try {
       channelDispatcher.consume(code, queue);
       return queue;
     } catch (Exception e) {
       throw new DuplicationsException(""Unable to lex source code at line : "" + code.getLinePosition() + "" and column : "" + code.getColumnPosition(), e);
     }
   }",Buggy,"SONAR-2923 and SONAR-2632: Fixed some side-effect compilations error in other modules
",NotBuggy,"The commit message refers to fixing side-effect compilation errors. The code change involves removing the configuration of buffer capacity in the `CodeReader`. By eliminating this explicit configuration, the function likely defaults to a standard buffer size, which might have been causing compile-time issues in the dependent modules or inconsistent behavior.

Given that the change is within the context of handling readers and exceptions, it suggests that there were side effects due to buffer configuration that could lead to compilation or runtime errors in modules that use this method. Although the fix seems to address these issues potentially indirectly caused by buffer misconfigurations, it's not explicitly a bug fix related to logic or handling errors directly visible in the exception.

In essence, this adjustment might be part of broader efforts to address compiler or configuration issues rather than fixing a logical or runtime bug directly seen here.

Therefore, the relevance of the code modifications with the commit message does not strongly indicate a typical bug fix as inferred from logic errors, exception handling changes, or incorrect results/output. It seems more about preventing configuration errors propagated to other modules.

The conclusion is **NotBuggy**."
voldemort,8473.json,a8fa1f7a2beb35aa81fc82167eb061901506d6ac,"@@ -1,21 +1,17 @@
     public static long copyLarge(Reader input, Writer output, long limit) throws IOException {
         char[] buffer = new char[DEFAULT_BUFFER_SIZE];
         long count = 0;
         int n = 0;
         long remaining = limit;
         while(remaining > 0) {
-            if(remaining > DEFAULT_BUFFER_SIZE) {
-                n = input.read(buffer);
-            } else {
-                char[] remainingbuffer = new char[(int) remaining];
-                n = input.read(remainingbuffer);
-            }
+            n = (remaining > DEFAULT_BUFFER_SIZE) ? input.read(buffer)
+                                                 : input.read(buffer, 0, (int) remaining);
             if(n == -1) {
                 break;
             }
             output.write(buffer, 0, n);
             count += n;
             remaining -= n;
         }
         return count;
     }",Buggy,"fix bug in last read
",Buggy,"The commit message ""fix bug in last read"" suggests a bug fix related to reading data. Analyzing the code changes:

1. **Old Logic**: 
   - If the remaining bytes to read were more than `DEFAULT_BUFFER_SIZE`, it would read into `buffer`.
   - Otherwise, it created a new `char[] remainingbuffer` of size `remaining` and read into it. This could be inefficient due to array allocation in each loop iteration and potential misalignment with the buffer used for writing, causing potential bugs when reading exactly the remaining limit.

2. **New Logic**:
   - It condenses the logic into a single `read` operation on `buffer`, using `input.read(buffer, 0, (int) remaining)` when less data remains than the buffer size. This avoids unnecessary allocation of a new array and ensures all reads go into the same buffer.

3. **Bug Fix**:
   - The new logic prevents buffer allocation for each iteration when `remaining` is less than `DEFAULT_BUFFER_SIZE`. 
   - This change fixes potential issues with reading the last chunks of data and improves performance by avoiding excessive memory allocation.

Given these points, the commit does align with the stated message of fixing a bug in the reading logic. Therefore, the conclusion is **Buggy**."
voldemort,10824.json,6980fd513d4c6545813e2294f5ac10afaa3bcbb6,"@@ -1,26 +1,29 @@
     public HintedHandoffStrategy updateHintedHandoffStrategy(StoreDefinition storeDef,
                                                              Cluster cluster) {
         if(HintedHandoffStrategyType.CONSISTENT_STRATEGY.toDisplay()
                                                         .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                            .toDisplay()) == 0) {
             Integer hintPrefListSize = storeDef.getHintPrefListSize();
+
+            // Default value for hint pref list size = replication factor
             if(null == hintPrefListSize) {
-                if(cluster.getNumberOfNodes() > 6)
-                    hintPrefListSize = cluster.getNumberOfNodes() / 2;
+                if(cluster.getNumberOfNodes() == storeDef.getReplicationFactor())
+                    hintPrefListSize = storeDef.getReplicationFactor() - 1;
                 else
-                    hintPrefListSize = cluster.getNumberOfNodes();
+                    hintPrefListSize = storeDef.getReplicationFactor();
             }
+
             return new ConsistentHandoffStrategy(cluster,
                                                  hintPrefListSize,
                                                  enableZoneRouting,
                                                  clientZoneId);
         } else if(HintedHandoffStrategyType.TO_ALL_STRATEGY.toDisplay()
                                                            .compareTo(storeDef.getHintedHandoffStrategyType()
                                                                               .toDisplay()) == 0) {
             return new HandoffToAllStrategy(cluster, enableZoneRouting, clientZoneId);
         } else {
             throw new VoldemortException(""HintedHandoffStrategyType:""
                                          + storeDef.getHintedHandoffStrategyType()
                                          + "" not handled by "" + this.getClass());
         }
     }",Buggy,"Fixed another bug in Consistent strategy + Updated stores.xml so as to get EndToEnd Test running
",Buggy,"The commit message indicates a bug fix in the Consistent strategy. Let's analyze the changes:

1. **Logic Update in HintedHandoffStrategy:**
   - The if condition dealing with `hintPrefListSize` has changed from:
     ```java
     if(cluster.getNumberOfNodes() > 6)
         hintPrefListSize = cluster.getNumberOfNodes() / 2;
     else
         hintPrefListSize = cluster.getNumberOfNodes();
     ```
     To:
     ```java
     if(cluster.getNumberOfNodes() == storeDef.getReplicationFactor())
         hintPrefListSize = storeDef.getReplicationFactor() - 1;
     else
         hintPrefListSize = storeDef.getReplicationFactor();
     ```

2. **Reasoning:**
   - The original code had a conditional logic based on the number of nodes, which set `hintPrefListSize` to half the nodes or the total node count.
   - The updated code compares the node count with the replication factor, setting `hintPrefListSize` relative to the replication factor.
   - This suggests a logical flaw was present where the preference list size was calculated incorrectly, impacting how ""hints"" get distributed in a consistent strategy. This correction better aligns the hinted handoff logic with expected behavior.

3. **Default Value Adjustment:**
   - The comment `// Default value for hint pref list size = replication factor` and the logic changes indicate ensuring the preference list aligns more closely with the replication factor, suggesting this default value logic was revisited to fix possible incorrect assignments.

These changes address what appears to be a bug in the logic of hinted handoff strategy, specifically for the ""Consistent"" strategy related to how many node hints are stored based on cluster and replication information. Based on these observations, the changes fix a bug in the consistent strategy logic.

Thus, the analysis confirms that the commit successfully addresses a bug in the system as claimed in the commit message.

The answer is **Buggy**."
voldemort,9755.json,97bfa6751eba8afb13bfc801afa47d2b6bd9af1d,"@@ -1,3 +1,3 @@
     public double get99thWaitTimeMs() {
-        return this.histogramWaitMs.getQuantile(0.99);
+        return (double) (this.histogramWaitNs.getQuantile(0.99)) / Time.NS_PER_MS;
     }",Buggy,"Fixed bug and verified monitoring feature of q99th wait time
",Buggy,"The commit message indicates a bug fix related to the 99th percentile wait time and its monitoring feature. The code change involves modifying the method get99thWaitTimeMs(), which originally retrieved a quantile directly from a histogram of wait times measured in milliseconds (`histogramWaitMs`). The updated code changes this to use a different histogram, `histogramWaitNs`, which appears to record wait times in nanoseconds. The result is then explicitly converted to milliseconds by dividing by `Time.NS_PER_MS`.

This change strongly suggests a bug fix for the following reasons:

1. **Correct Data Source**: The original implementation may have mistakenly used a histogram for milliseconds instead of nanoseconds, possibly leading to inaccurate wait time calculations. The new code corrects this by switching to a potentially more precise source.

2. **Unit Conversion**: The need to divide by `Time.NS_PER_MS` indicates that the units were being misrepresented in the original method, where wait times might have been expected to be in milliseconds but were provided in nanoseconds.

3. **Functional Correction**: Both aspects above signify a functional correction rather than a non-functional performance improvement or code refactoring.

Given the alignment between the code changes and the stated intent of fixing a bug in the commit message, the conclusion is **Buggy**."
voldemort,9005.json,c07b777533cdbdfe4042258dbf44f2d9d51e3bc0,"@@ -1,286 +1,283 @@
     public void run() {
         Object message = messageEvent.getMessage();
         if(message instanceof CoordinatorStoreClientRequest) {
             CoordinatorStoreClientRequest storeClientRequestObject = (CoordinatorStoreClientRequest) message;
             this.requestObject = storeClientRequestObject.getRequestObject();
             this.storeClient = storeClientRequestObject.getStoreClient();
 
             // This shouldn't ideally happen.
             if(this.requestObject != null) {
 
                 switch(requestObject.getOperationType()) {
                     case VoldemortOpCode.GET_METADATA_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET Metadata request received."");
                         }
 
                         try {
 
                             String queryStoreName = ByteUtils.getString(this.requestObject.getKey()
                                                                                           .get(),
                                                                         ""UTF-8"");
                             StoreDefinition storeDef = StoreDefinitionUtils.getStoreDefinitionWithName(this.coordinatorMetadata.getStoresDefs(),
                                                                                                        queryStoreName);
                             String serializerInfoXml = RestUtils.constructSerializerInfoXml(storeDef);
                             GetMetadataResponseSender metadataResponseSender = new GetMetadataResponseSender(messageEvent,
                                                                                                              serializerInfoXml.getBytes());
 
                             metadataResponseSender.sendResponse(this.coordinatorPerfStats,
                                                                 true,
                                                                 this.requestObject.getRequestOriginTimeInMs());
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET Metadata successful !"");
                             }
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET METADATA request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET request received."");
                         }
 
                         try {
                             boolean keyExists = false;
                             List<Versioned<byte[]>> versionedValues = this.storeClient.getWithCustomTimeout(this.requestObject);
                             if(versionedValues == null || versionedValues.size() == 0) {
                                 if(this.requestObject.getValue() != null) {
                                     if(versionedValues == null) {
                                         versionedValues = new ArrayList<Versioned<byte[]>>();
                                     }
                                     versionedValues.add(this.requestObject.getValue());
                                     keyExists = true;
 
                                 }
                             } else {
                                 keyExists = true;
                             }
 
                             if(keyExists) {
                                 GetResponseSender responseConstructor = new GetResponseSender(messageEvent,
                                                                                               requestObject.getKey(),
                                                                                               versionedValues,
                                                                                               this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET successful !"");
                                 }
 
                             } else {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key does not exist"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.GET_ALL_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""GET ALL request received."");
                         }
 
                         try {
                             Map<ByteArray, List<Versioned<byte[]>>> versionedResponses = this.storeClient.getAllWithCustomTimeout(this.requestObject);
                             if(versionedResponses == null
                                || versionedResponses.values().size() == 0) {
                                 logger.error(""Error when doing getall. Keys do not exist."");
 
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Error when doing getall. Keys do not exist."");
                             } else {
                                 GetAllResponseSender responseConstructor = new GetAllResponseSender(messageEvent,
                                                                                                     versionedResponses,
                                                                                                     this.storeClient.getStoreName());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""GET ALL successful !"");
                                 }
 
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET ALL request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     // TODO: Implement this in the next pass
                     case VoldemortOpCode.GET_VERSION_OP_CODE:
 
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming get version request"");
                         }
 
                         try {
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""GET versions request successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""GET VERSION request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                getVersionErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            getVersionErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     case VoldemortOpCode.PUT_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""PUT request received."");
                         }
 
                         try {
                             VectorClock successfulPutVC = null;
 
                             if(this.requestObject.getValue() != null) {
                                 successfulPutVC = ((VectorClock) this.storeClient.putVersionedWithCustomTimeout(this.requestObject)).clone();
                             } else {
                                 successfulPutVC = ((VectorClock) this.storeClient.putWithCustomTimeout(this.requestObject)).clone();
                             }
 
                             PutResponseSender responseConstructor = new PutResponseSender(messageEvent,
                                                                                           successfulPutVC,
                                                                                           this.storeClient.getStoreName(),
                                                                                           this.requestObject.getKey());
                             responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                              true,
                                                              this.requestObject.getRequestOriginTimeInMs());
 
                             if(logger.isDebugEnabled()) {
                                 logger.debug(""PUT successful !"");
                             }
 
                         } catch(Exception e) {
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""PUT request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                putErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            putErrorHandler.handleExceptions(messageEvent, e);
                         }
 
                         break;
 
                     case VoldemortOpCode.DELETE_OP_CODE:
                         if(logger.isDebugEnabled()) {
                             logger.debug(""Incoming delete request"");
                         }
 
                         try {
                             boolean isDeleted = this.storeClient.deleteWithCustomTimeout(this.requestObject);
                             if(isDeleted) {
                                 DeleteResponseSender responseConstructor = new DeleteResponseSender(messageEvent,
                                                                                                     this.storeClient.getStoreName(),
                                                                                                     this.requestObject.getKey());
                                 responseConstructor.sendResponse(this.coordinatorPerfStats,
                                                                  true,
                                                                  this.requestObject.getRequestOriginTimeInMs());
 
                                 if(logger.isDebugEnabled()) {
                                     logger.debug(""DELETE request successful !"");
                                 }
 
                             } else {
                                 logger.error(""Requested Key with the specified version does not exist"");
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     NOT_FOUND,
                                                                     ""Requested Key with the specified version does not exist"");
                             }
 
                         } catch(Exception e) {
 
                             /*
                              * We might get InsufficientOperationalNodes
                              * exception due to a timeout, thus creating
                              * confusion in the root cause. Hence explicitly
                              * check for timeout.
                              */
                             if(System.currentTimeMillis() >= (this.requestObject.getRequestOriginTimeInMs() + this.requestObject.getRoutingTimeoutInMs())) {
                                 RestErrorHandler.writeErrorResponse(this.messageEvent,
                                                                     REQUEST_TIMEOUT,
                                                                     ""DELETE request timed out: ""
                                                                             + e.getMessage());
+                            } else {
+                                deleteErrorHandler.handleExceptions(messageEvent, e);
                             }
-
-                            deleteErrorHandler.handleExceptions(messageEvent, e);
                         }
                         break;
 
                     default:
                         System.err.println(""Illegal operation."");
                         return;
-
                 }
 
             }
-
         }
-
     }",Buggy,"fix duplicate error message bug and add large value size test for coordinator
",Buggy,"The commit message states that it fixes a ""duplicate error message bug."" Upon examining the diff, the previous code called `getErrorHandler.handleExceptions(messageEvent, e);` (or its equivalent for different types of operations) immediately after the timeout error message was sent. This caused duplicate error handling, especially if the timeout error was sent first. 

The changes introduce an `else` statement: if there is no timeout, it calls `getErrorHandler.handleExceptions(messageEvent, e);`. This aligns with the commit message: it fixes the bug by preventing duplicate error messages by ensuring only one error handling occurs in the case of an exception.

Thus, the change accurately corresponds to fixing a bug related to error messages as described in the commit message. Moreover, no evidence directly relates to performance improvements in the given diff, so the focus is clearly on error handling, aligning with the ""fix duplicate error message bug"" part.

Given these observations, this change indicates a bug fix, and my conclusion is **Buggy**."
voldemort,7694.json,d1b041d2e3ef3148bfcfae07d6e2609a5aa0ac45,"@@ -1,33 +1,33 @@
     public VAdminProto.UpdateMetadataPairResponse handleUpdateMetadataPair(VAdminProto.UpdateMetadataPairRequest request) {
         VAdminProto.UpdateMetadataPairResponse.Builder response = VAdminProto.UpdateMetadataPairResponse.newBuilder();
         try {
             ByteArray clusterKey = ProtoUtils.decodeBytes(request.getClusterKey());
             ByteArray storesKey = ProtoUtils.decodeBytes(request.getStoresKey());
             String clusterKeyString = ByteUtils.getString(clusterKey.get(), ""UTF-8"");
             String storesKeyString = ByteUtils.getString(storesKey.get(), ""UTF-8"");
 
             if(MetadataStore.METADATA_KEYS.contains(clusterKeyString)
                && MetadataStore.METADATA_KEYS.contains(storesKeyString)) {
 
                 Versioned<byte[]> clusterVersionedValue = ProtoUtils.decodeVersioned(request.getClusterValue());
                 Versioned<byte[]> storesVersionedValue = ProtoUtils.decodeVersioned(request.getStoresValue());
 
                 metadataStore.writeLock.lock();
                 try {
                     logger.info(""Updating metadata for keys '"" + clusterKeyString + ""'"" + "" and '""
                                 + storesKeyString + ""'"");
                     metadataStore.put(clusterKey, clusterVersionedValue, null);
-                    metadataStore.put(storesKey, storesVersionedValue, null);
+                    metadataStore.updateStoreDefinitions(storesVersionedValue);
                     logger.info(""Successfully updated metadata for keys '"" + clusterKeyString + ""'""
                                 + "" and '"" + storesKeyString + ""'"");
                 } finally {
                     metadataStore.writeLock.unlock();
                 }
             }
         } catch(VoldemortException e) {
             response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
             logger.error(""handleUpdateMetadataPair failed for request("" + request.toString() + "")"",
                          e);
         }
         return response.build();
     }",Buggy,"Bug fix in RebalanceRebootstrapConsistencyTest and AdminServiceRequestHandler. Adding wrapper for updateRemoteStoreDefList in AdminClient
",Buggy,"The code change involves modifying the way metadata for store definitions is updated in the `handleUpdateMetadataPair` method. Previously, the method used `metadataStore.put(storesKey, storesVersionedValue, null)` to update store definitions, and it was changed to `metadataStore.updateStoreDefinitions(storesVersionedValue)`.

Here's the breakdown of the change:

1. **Functionality Change:** The call to `metadataStore.put` was replaced with a call to `metadataStore.updateStoreDefinitions`. This indicates a change in how the metadata store is being updated, suggesting that there was a specific requirement or error addressed by using a more specific method instead of a generic `put`.

2. **Intended Behavior:** The new method `updateStoreDefinitions` seems to suggest a more tailored approach to updating store definitions, which could imply that `put` was not handling certain complexities or cases properly related to store definitions.

3. **Bug Fix Implication:** If `put` method did not correctly handle aspects or integrity constraints of store definitions, switching to a specialized method likely addresses those issues, ensuring consistency and correctness in store metadata updates.

4. **Commit Message Correlation:** The commit message states that there is a bug fix in `AdminServiceRequestHandler` (which matches the name of the class in the diff) and mentions an addition of a wrapper for update functions. Although the wrapper addition isn't shown in this snippet, the specialization of a method for updating indicates an improvement inline with fixing an error.

Given these factors, the code change aligns with the commit message indicating a bug fix by using a more appropriate function for the update task. Therefore, this is a bug fix, marked as **Buggy**."
voldemort,7656.json,1f057d7e3c68dc4d67387a5485ac6793d3feb8b7,"@@ -1,31 +1,33 @@
     public VAdminProto.RebalanceStateChangeResponse handleRebalanceStateChange(VAdminProto.RebalanceStateChangeRequest request) {
-
         VAdminProto.RebalanceStateChangeResponse.Builder response = VAdminProto.RebalanceStateChangeResponse.newBuilder();
 
-        try {
-            // Retrieve all values first
-            List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
-            for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
-                rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+        synchronized(rebalancer) {
+            try {
+                // Retrieve all values first
+                List<RebalancePartitionsInfo> rebalancePartitionsInfo = Lists.newArrayList();
+                for(RebalancePartitionInfoMap map: request.getRebalancePartitionInfoListList()) {
+                    rebalancePartitionsInfo.add(ProtoUtils.decodeRebalancePartitionInfoMap(map));
+                }
+
+                Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
+
+                boolean swapRO = request.getSwapRo();
+                boolean changeClusterMetadata = request.getChangeClusterMetadata();
+                boolean changeRebalanceState = request.getChangeRebalanceState();
+                boolean rollback = request.getRollback();
+
+                rebalancer.rebalanceStateChange(cluster,
+                                                rebalancePartitionsInfo,
+                                                swapRO,
+                                                changeClusterMetadata,
+                                                changeRebalanceState,
+                                                rollback);
+            } catch(VoldemortException e) {
+                response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
+                logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
+                             + "")"", e);
             }
-
-            Cluster cluster = new ClusterMapper().readCluster(new StringReader(request.getClusterString()));
-
-            boolean swapRO = request.getSwapRo();
-            boolean changeClusterMetadata = request.getChangeClusterMetadata();
-            boolean changeRebalanceState = request.getChangeRebalanceState();
-            boolean rollback = request.getRollback();
-
-            rebalancer.rebalanceStateChange(cluster,
-                                            rebalancePartitionsInfo,
-                                            swapRO,
-                                            changeClusterMetadata,
-                                            changeRebalanceState,
-                                            rollback);
-        } catch(VoldemortException e) {
-            response.setError(ProtoUtils.encodeError(errorCodeMapper, e));
-            logger.error(""handleRebalanceStateChange failed for request("" + request.toString()
-                         + "")"", e);
         }
+
         return response.build();
     }",Buggy,"fix two concurrency bugs during the termination of donor-based rebalancing
",Buggy,"The diff primarily focuses on changes within the `handleRebalanceStateChange` method to address concurrency issues during rebalancing. Here's a step-by-step analysis:

1. **Synchronization Addition**: The block of code handling the rebalance logic is now wrapped with `synchronized(rebalancer)`. This change suggests that the rebalancer might have been accessed concurrently, leading to potential race conditions or inconsistent states. Synchronizing on the `rebalancer` object ensures that these actions are thread-safe, effectively correcting concurrency-related bugs.

2. **Error Handling**: Error handling logic remains the same, catching `VoldemortException` and logging it. This ensures errors are documented, aiding in debugging efforts without any effect on the concurrency bug fix itself.

3. **Code Structure**: The structure remains logically the same, with added synchronization indicating a targeted approach to resolving concurrency issues rather than altering the core logic.

The commit message aligns with these changes. The synchronization directly addresses the concurrency bugs mentioned, particularly during state transitions in donor-based rebalancing.

Given these observations, the changes reflect corrections made to concurrency issues, fitting the context of a bug fix. Thus, the assessment is **Buggy**."
voldemort,6967.json,9f2e07c6e18984df4d2428dd51accd37534e9a68,"@@ -1,11 +1,12 @@
     public void stop() {
         logger.info(""Stopping "" + getType().getDisplayName());
         synchronized(this) {
             if(!isStarted()) {
                 logger.info(""The service is already stopped, ignoring duplicate attempt."");
+                return;
             }
 
             stopInner();
             isStarted.set(false);
         }
     }",Buggy,"Fixed the re-introduction of BindExceptions upon start up of some recently added tests:

All of the following tests directly used the method ServerTestUtils.startVoldemortServer
  test/unit/voldemort/client/ClientRegistryTest.java
  test/unit/voldemort/client/AdminFetchTest.java
  test/unit/voldemort/client/EndToEndRebootstrapTest.java
  test/unit/voldemort/store/system/AsyncMetadataVersionManagerTest.java
  test/unit/voldemort/store/system/SystemStoreTest.java

This method of starting Voldemort servers is susceptible to
BindException errors. Unless something very fancy is being done, the
method ServerTestUtils.startVoldemortCluster should be used to start a
cluster within a test.

test/unit/voldemort/client/ClientRegistryTest.java extended from
TestCase. This is a ""Junit3"" idiom that we should not use in new
tests. The Junit4 approach to tests relies solely on annotation.
",NotBuggy,"The commit message mentions addressing `BindException` issues sometimes encountered during test startup by changing how servers are initiated. However, the provided diff shows a modification to the `stop` method, adding a return statement when `isStarted` is false to skip further execution, preventing redundant operations and logs when stopping an already stopped service.

While the change itself improves the stop method's handling to avoid unnecessary operations, it does not directly align with the primary focus of the commit message, which was to fix `BindExceptions`. The diff does not modify test server startup procedures or address port binding directly, which is central to the commit message's claim. However, it is possible that preventing redundant stopping might indirectly relate to avoiding exceptions or errors related to server lifecycle management, though the connection isn't explicit.

Due to the potential for indirectly enhancing error handling and the context provided by the commit message, it might support broader stability improvements. However, based solely on the code diff presented, since there's no direct connection to fixing `BindException` or changing server startup logic as described in the commit message, the actual impact on the specific bug mentioned concerning `BindException` is unclear.

Therefore, although it shows an improvement in error handling related to stopping services, without concrete evidence of addressing the specific bug mentioned regarding startup `BindException` issues, this specific change doesn't fully align with the core bug fix described in the commit message.

Thus, while there is a potential for a broader context fix, based solely on the diff provided:
The answer is **NotBuggy**."
voldemort,8213.json,46a5b4d7e916e3fe0a9a77659599d74709f18c2d,"@@ -1,55 +1,56 @@
     private void write(DataOutputStream output, Object object, Object type) throws IOException {
         try {
             if(type instanceof Map) {
                 if(object != null && !(object instanceof Map))
                     throw new SerializationException(""Expected Map, but got "" + object.getClass()
                                                      + "": "" + object);
                 writeMap(output, (Map<String, Object>) object, (Map<String, Object>) type);
             } else if(type instanceof List) {
                 if(object != null && !(object instanceof List))
                     throw new SerializationException(""Expected List but got "" + object.getClass()
                                                      + "": "" + object);
                 writeList(output, (List<Object>) object, (List<Object>) type);
             } else if(type instanceof JsonTypes) {
                 JsonTypes jsonType = (JsonTypes) type;
                 switch(jsonType) {
                     case STRING:
                         writeString(output, (String) object);
                         break;
                     case INT8:
                         writeInt8(output, (Byte) object);
                         break;
                     case INT16:
                         writeInt16(output, coerceToShort(object));
                         break;
                     case INT32:
                         writeInt32(output, coerceToInteger(object));
                         break;
                     case INT64:
                         writeInt64(output, coerceToLong(object));
                         break;
                     case FLOAT32:
                         writeFloat32(output, coerceToFloat(object));
                         break;
                     case FLOAT64:
                         writeFloat64(output, coerceToDouble(object));
                         break;
                     case DATE:
                         writeDate(output, coerceToDate(object));
                         break;
                     case BYTES:
                         writeBytes(output, (byte[]) object);
                         break;
                     case BOOLEAN:
                         writeBoolean(output, (Boolean) object);
+                        break;
                     default:
                         throw new SerializationException(""Unknown type: "" + type);
                 }
             }
         } catch(ClassCastException e) {
             // simpler than doing every test
             throw new SerializationException(""Expected type "" + type
                                              + "" but got object of incompatible type ""
                                              + object.getClass().getName() + ""."");
         }
     }",Buggy,"Fix serialization bug with writing boolean values--missing break in case statement.
",Buggy,"This commit message accurately describes the change made in the diff. The modification involves adding a missing `break` statement to the `BOOLEAN` case in a `switch` statement inside the `write` method. 

Here’s the analysis:

1. **Commit Message:** The commit message states there was a serialization bug related to writing boolean values and a missing `break` in the case statement. 
   
2. **Code Changes:**
   - Before the change, the `case BOOLEAN:` did not have a `break` statement.
   - This could have caused the `default` case to get executed after the `BOOLEAN` case in case a `JsonTypes.BOOLEAN` was handled. This would throw an unnecessary exception due to the absence of a break, confusing logic flow and causing incorrect serialization behavior.

3. **Impact of Change:**
   - Adding the `break;` after `writeBoolean(output, (Boolean) object);` ensures that once the boolean value is processed, the switch statement exits without continuing to the `default` case.
   - This fixes the bug by preventing unintentional fall-through, which eliminates erroneous behavior and aligns serialization processing with the intended logic.

The commit message directly relates to the change, correctly identifying a bug fix in the form of a control flow error fix. Since this modification is correcting a logic flow oversight (missing break in a switch-case construction leading to potential unexpected behavior), the conclusion is **Buggy**."
voldemort,9151.json,ff128a8ad5de83bc55b8cec80d7775e61816e4e6,"@@ -1,16 +1,21 @@
     public List<Versioned<T>> resolveConflicts(List<Versioned<T>> items) {
+        List<ClockEntry> maxClock = null;
         if(items.size() <= 1) {
             return items;
         } else {
             Versioned<T> max = items.get(0);
             long maxTime = ((VectorClock) items.get(0).getVersion()).getTimestamp();
+            maxClock = ((VectorClock) items.get(0).getVersion()).getEntries();
             for(Versioned<T> versioned: items) {
                 VectorClock clock = (VectorClock) versioned.getVersion();
                 if(clock.getTimestamp() > maxTime) {
                     max = versioned;
                     maxTime = ((VectorClock) versioned.getVersion()).getTimestamp();
                 }
+                maxClock = VectorClock.maxClockList(maxClock, clock.getEntries());
             }
-            return Collections.singletonList(max);
+            Versioned<T> maxTimeClockVersioned = new Versioned<T>(max.getValue(),
+                                                                  new VectorClock(maxClock, maxTime));
+            return Collections.singletonList(maxTimeClockVersioned);
         }
     }",Buggy,"TimeBasedInconsistency resolover bug fix
",Buggy,"This commit message indicates a bug fix in a ""TimeBasedInconsistency resolver."" Analyzing the code changes:

1. **Initial Condition Handling**: 
   - A new `maxClock` is initialized as `null`. This will be used to track the highest `ClockEntry`.

2. **Max Time and Clock Gathering**:
   - Previously, the code only kept track of and returned the version with the maximum timestamp.
   - The updated code not only finds the max timestamp (`maxTime`) but also determines the max `ClockEntry` list using a hypothetical method `VectorClock.maxClockList(maxClock, clock.getEntries())`.

3. **Result Construction**:
   - A new `Versioned<T>` object is created using both the maximum timestamp and the resolved clock entries, and then returned.

The changes suggest a correction in logic where simply having the max timestamp was not enough. The correct resolution now includes merging or selecting the maximal clock entries along with the timestamp, which aligns with resolving potential ""time-based inconsistencies.""

Given these points, the modifications are directly related to fixing a bug in how conflicts were previously resolved using time-based logic. Thus, the assessment of this commit is **Buggy**."
voldemort,11028.json,c3773f409dffb3761aaeba6811d3ed6836263987,"@@ -1,15 +1,15 @@
     public static void validateUserStoreNamesOnNode(AdminClient adminClient,
                                                  Integer nodeId,
                                                  List<String> storeNames) {
         List<StoreDefinition> storeDefList = adminClient.metadataMgmtOps.getRemoteStoreDefList(nodeId)
                                                                         .getValue();
-        List<String> storeNameList = Lists.newArrayList();
+        Map<String, Boolean> existingStoreNames = new HashMap<String, Boolean>();
         for(StoreDefinition storeDef: storeDefList) {
-            storeNameList.add(storeDef.getName());
+          existingStoreNames.put(storeDef.getName(), true);
         }
         for(String storeName: storeNames) {
-            if(!storeNameList.contains(storeName)) {
+            if(!Boolean.TRUE.equals(existingStoreNames.get(storeName))) {
                 Utils.croak(""Store "" + storeName + "" does not exist!"");
             }
         }
     }",Buggy,"Fix a vadmin error
",Buggy,"The commit message refers to fixing a ""vadmin error,"" and the code modifications support this claim. Here's the breakdown:

1. **Original Code**: The original code uses a `List<String>` to track store names fetched from `storeDefList`. It checks for existence by using `List.contains()`, which can be inefficient due to linear search time complexity O(n).

2. **Modified Code**: The revised code switches to a `Map<String, Boolean>` named `existingStoreNames`. It populates the map with store names as keys with a value of `true`, and checks for the existence of a store name with `Boolean.TRUE.equals(existingStoreNames.get(storeName))`, which has a time complexity of O(1) due to hashmap lookup.

3. **Reason for Change**: This alteration improves performance and addresses potential issues with store existence checks. A hashmap is more efficient for frequent existence checks compared to a list.

4. **Error Fixing**: By changing the approach to use a hashmap, the likelihood of errors related to inefficient store lookup is reduced, especially relevant in cases where performance is critical or where store definitions are vast.

These points indicate a genuine bug fix related to the validation process, likely improving reliability and performance to correct or prevent errors. Thus, the evaluation concludes with **Buggy**."
voldemort,755.json,0548406c86c8ed3af48c0a59586f8dd03d3aefdd,"@@ -1,11 +1,10 @@
             public void nodeUnavailable(Node node) {
                 if(logger.isInfoEnabled())
-                    logger.info(""Node "" + node
-                                + "" has been marked as unavailable, destroying socket pool"");
+                    logger.info(node + "" has been marked as unavailable, destroying socket pool"");
 
                 // Kill the socket pool for this node...
                 SocketDestination destination = new SocketDestination(node.getHost(),
                                                                       node.getSocketPort(),
                                                                       config.getRequestFormatType());
                 socketPool.close(destination);
             }",Buggy,"Fixed inconsistent error message.
",NotBuggy,"The commit message indicates a fix for an ""inconsistent error message."" The code change modifies the logging statement by removing some redundant text: ""Node "" + added before node when forming the log message. This change could indeed make the log more consistent with other log messages by using `node.toString()` directly, assuming that `node.toString()` implicitly adds its own identifier or prefix.

However, there is no evidence from the provided information that this change addresses a bug in the functionality or behavior of the application beyond consistency in logging format. Therefore, the adjustment is primarily cosmetic, aimed at style or message uniformity, unless there is background context about previous inconsistent error messages causing issues in the log parsing that aren't detailed here.

Thus, based on the information provided, the changes do not directly indicate a bug fix regarding functionality or error handling beyond message consistency.

The conclusion is **NotBuggy**."
voldemort,151.json,a36d1fe39e88e79875bef39bc0ce10419d8cbcf2,"@@ -1,8 +1,8 @@
-    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml) {
+    private AdminClient createAdminClient(String url, boolean fetchAllStoresXml, int connectionTimeoutSec, int socketTimeoutSec) {
         ClientConfig config = new ClientConfig().setBootstrapUrls(url)
-                .setConnectionTimeout(15,TimeUnit.SECONDS)
+                .setConnectionTimeout(connectionTimeoutSec ,TimeUnit.SECONDS)
                 .setFetchAllStoresXmlInBootstrap(fetchAllStoresXml);
 
-        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(60);
+        AdminClientConfig adminConfig = new AdminClientConfig().setAdminSocketTimeoutSec(socketTimeoutSec);
         return new AdminClient(adminConfig, config);
     }",Buggy,"Made admin connection/socket timeout configurable in BnP.

Also changed the default socket timeout to 180 seconds.

This fixes the following problem: when a node is unreachable and
completely shut down, requests to it will time out, which takes
60 seconds. When BnP notices this, it will reach one of the live
nodes in the cluster and ask it to deal with the failure. The
live node will try to talk to the dead node, which will also
take 60 seconds to time out. By the time the live node decides
that the dead node is unreachable, and responds to the BnP job,
the BnP job will have already timed out. Then, the BnP job will
think that the HandleFailedFetchRequest could not complete
successfully (even though it did in fact complete successfully)
and BnP HA will be aborted.

The solution is that the BnP job's socket timeout must be greater
than the server's default connection timeout.

This was not an issue before when we had insanely long time outs,
but those time outs have been reduced considerably in commit
34debd34c5896b6c2a01b1012e89dd1a3a0a0242. This is likely when we
regressed on the handling of this failure mode.
",Buggy,"The commit introduces the following changes:

1. **Configurable Timeouts**: The method `createAdminClient` now allows for configurable `connectionTimeoutSec` and `socketTimeoutSec` by replacing hard-coded timeout values with method parameters.

2. **Bug Fix Rationale**: The commit message details a problem with timeouts related to node availability and BnP job completion. The issue occurs when both the BnP and the live node use a 60-second timeout to determine if a node is unreachable, causing the BnP job to time out before receiving a response from the live node. The solution is to make socket and connection timeouts configurable, allowing the BnP job to have a longer timeout than the server's default connection timeout to prevent premature job termination.

3. **Bug Fix Correlation**: The code modification directly addresses the problem described in the commit message. By making the timeouts configurable, users can ensure that the BnP job's timeout exceeds the server's timeout, effectively resolving the issue where the BnP job times out before successfully completing the HandleFailedFetchRequest.

Given the clear explanation of the problem and how the code change remedies it, this commit aligns with a bug fix as described in the message. Therefore, the conclusion is **Buggy**."
voldemort,132.json,da4a1bf5015df49a9e8ba867a90d4dab7f0ba194,"@@ -1,29 +1,29 @@
-    public static void main(String[] args) {
+    public static void main(String[] args) throws Exception {
         // Validate arguments
         if (args.length < 1) {
             logger.error(""Please provide a job config file name as the argument to this script."");
             System.exit(1);
         }
         String fileName = args[0];
 
         // Load config
         logger.info(""Extracting config properties out of: "" + fileName);
         Props props = null;
         try {
             props = new Props(null, fileName);
         } catch (IOException e) {
             logger.error(""Exception while reading config file!"", e);
             System.exit(1);
         }
 
         // Run job
         VoldemortBuildAndPushJob job = new VoldemortBuildAndPushJob(""shell-job"", props);
         try {
             job.run();
         } catch (Exception e) {
             logger.error(""Exception while running BnP job!"", e);
             System.exit(1);
         }
         logger.info(""BnP job finished successfully (:"");
         System.exit(0);
     }",Buggy,"Fixed a compilation error introduced by last commit. (#468)

",Buggy,"The commit message indicates a fix for a compilation error introduced by the previous commit. The code diff shows the addition of `throws Exception` to the `main` method signature. Without the `throws` declaration, the code likely failed to compile due to unhandled checked exceptions, such as `IOException` and `Exception`, thrown within the `try-catch` blocks and method calls. By explicitly declaring `throws Exception`, the method now correctly handles the compilation requirements for checked exceptions.

This change aligns with the commit message as it directly addresses what would have been a compilation issue in the form of unhandled exceptions. Since resolving a compilation error can be considered a bug fix, I conclude that this is a **Buggy** change."
voldemort,9849.json,41cf6a7204e1b9821833bf9f6022f61abd10f007,"@@ -1,17 +1,17 @@
     private VectorClock readVersion(String key) {
         try {
             File versionFile = new File(getVersionDirectory(), key);
             if(!versionFile.exists()) {
                 // bootstrap file save default clock as version.
-                VectorClock clock = new VectorClock();
+                VectorClock clock = new VectorClock(0);
                 writeVersion(key, clock);
                 return clock;
             } else {
                 // read the version file and return version.
                 String hexCode = FileUtils.readFileToString(versionFile, ""UTF-8"");
                 return new VectorClock(Hex.decodeHex(hexCode.toCharArray()));
             }
         } catch(Exception e) {
             throw new VoldemortException(""Failed to read Version for Key:"" + key, e);
         }
     }",Buggy,"fixed a bug that throws exceptions for some tests
",Buggy,"The commit message indicates a change that addresses an exception issue during testing. The code change modifies the instantiation of a `VectorClock`. Initially, a default constructor was used, which is replaced by a constructor with an argument (`new VectorClock(0)`). This modification likely fixes an issue where the default constructor could lead to improper initialization, potentially resulting in exceptions when the object is used later.

This change aligns with the commit message, suggesting that previously, some tests threw exceptions due to inadequate initialization of the `VectorClock`. By specifying the `0` value, it ensures the `VectorClock` is correctly initialized, hence resolving the exception problem.

These factors together indicate that the code update is indeed a bug fix related to exception handling during tests. Therefore, the conclusion is **Buggy**."
cassandra,15770.json,3740f815c21254bd625ad1cbe8d47aa657727a83,"@@ -1,26 +1,29 @@
     public boolean maybeWaitForArchiving(String name)
     {
         Future<?> f = archivePending.remove(name);
         if (f == null)
             return true; // archiving disabled
 
         try
         {
             f.get();
         }
         catch (InterruptedException e)
         {
             throw new AssertionError(e);
         }
         catch (ExecutionException e)
         {
-            if (e.getCause() instanceof IOException)
+            if (e.getCause() instanceof RuntimeException)
             {
-                logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name);
-                return false;
+                if (e.getCause().getCause() instanceof IOException)
+                {
+                    logger.error(""Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now."", name, e.getCause().getCause());
+                    return false;
+                }
             }
             throw new RuntimeException(e);
         }
 
         return true;
     }",Buggy,"Fix bugs in commit log archiving startup behavior

patch by Ariel Weisberg; reviewed by Branimir Lambov for CASSANDRA-10593
",Buggy,"The code changes indicate a fix in error handling logic in the method `maybeWaitForArchiving`. Here's a step-by-step breakdown:

1. **Original Behavior**: The method originally checked if the cause of an `ExecutionException` was specifically an `IOException`. If so, it logged an error message and returned `false`.

2. **Modified Behavior**: The updated code now first checks if the cause of the `ExecutionException` is an instance of `RuntimeException`. If it is, it then checks if the underlying cause is an `IOException`. It logs an error message with more detailed exception information and then returns `false`.

3. **Error-Handling Improvement**: This change adds an additional layer of exception checking, likely because some `IOException`s were being wrapped in `RuntimeException`s. This more precise handling of exceptions ensures that the correct log message is produced and the correct action is taken if the archiving fails due to an `IOException`.

4. **Logging Improvement**: The error logging now includes the underlying exception cause, providing more context for debugging purposes.

These updates seem to address an issue where `IOException`s were not being correctly detected because they were wrapped inside `RuntimeException`s. This aligns with the commit message mentioning a bug fix related to commit log archiving startup behavior.

Given these observations, the change is indeed indicative of a bug fix. The answer is **Buggy**."
cassandra,5689.json,02030dd658e6ca0cdb8987fd72e0454066e1b6d6,"@@ -1,4 +1,7 @@
     public boolean contains(T position)
     {
-        return Range.contains(left, right, position) || left.equals(position);
+        // Range.contains doesnt work correctly if left == right because for
+        // Range that means a wrapping range that select the whole ring. So we
+        // must explicitely handle this case
+        return left.equals(position) || (!left.equals(right) && Range.contains(left, right, position));
     }",Buggy,"Fix LCS bug with sstables containing only 1 row

patch by slebresne; reviewed by jbellis for CASSANDRA-4411
",Buggy,"The commit message indicates a fix for a bug in handling cases where sstables contain only one row. The diff provides a modification to the `contains` method by adding a conditional check to handle cases where `left` is equal to `right`. The comment explains that when `left` equals `right`, the `Range.contains` method wrongly assumes the range selects the whole ring, so it is necessary to handle this case explicitly.

The added code `!left.equals(right)` ensures that a range is only checked when `left` is not equal to `right`, avoiding the incorrect assumption of a wrapping range. This change directly addresses a bug in range logic, which aligns with the commit message's description.

Thus, the change here is correctly identified as a bug fix. The answer is **Buggy**."
cassandra,13729.json,967a2cfe179548835d5e8c1640889420ce0d40ce,"@@ -1,13 +1,13 @@
     private void updateCrc()
     {
-        if (crcPosition == buffer.position() | crcUpdateDisabled)
+        if (crcPosition == buffer.position() || crcUpdateDisabled)
             return;
 
         assert crcPosition >= 0 && crcPosition < buffer.position();
 
         ByteBuffer unprocessed = buffer.duplicate();
         unprocessed.position(crcPosition)
                    .limit(buffer.position());
 
         crc.update(unprocessed);
     }",Buggy,"8630: fixed coverity defects
",Buggy,"The diff corrects a logical operator from `|` (bitwise OR) to `||` (logical OR) in an `if` condition. Using `|` meant the condition could result in unintended behavior, evaluating both sides regardless of the first condition's result—likely a bug if the intent was short-circuit logic with `||`. This fix prevents unnecessary evaluations and potential side effects when `crcUpdateDisabled` is true. The fix is straightforward, addressing a potential logic bug. Given the clear indication of correcting improper logic, the conclusion is **Buggy**."
cassandra,14898.json,50c1987265f8ddaf5032a7ace07b2462aba4f09b,"@@ -1,38 +1,38 @@
     static Set<List<String>> getCompactionBuckets(List<String> files, long min)
     {
-    	Map<List<String>, Long> buckets = new NonBlockingHashMap<List<String>, Long>();
+    	Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
     	for(String fname : files)
     	{
     		File f = new File(fname);
     		long size = f.length();
 
     		boolean bFound = false;
             // look for a bucket containing similar-sized files:
             // group in the same bucket if it's w/in 50% of the average for this bucket,
             // or this file and the bucket are all considered ""small"" (less than `min`)
             for (List<String> bucket : buckets.keySet())
     		{
                 long averageSize = buckets.get(bucket);
                 if ((size > averageSize/2 && size < 3*averageSize/2)
                     || ( size < min && averageSize < min))
     			{
                     // remove and re-add because adding changes the hash
                     buckets.remove(bucket);
     				averageSize = (averageSize + size) / 2 ;
                     bucket.add(fname);
                     buckets.put(bucket, averageSize);
     				bFound = true;
     				break;
     			}
     		}
             // no similar bucket found; put it in a new one
     		if(!bFound)
     		{
                 ArrayList<String> bucket = new ArrayList<String>();
                 bucket.add(fname);
                 buckets.put(bucket, size);
     		}
     	}
 
         return buckets.keySet();
     }",Buggy,"fix getCompactionBuckets -- something is broken in NonBlockingHashMap when removing and re-adding the same collection mid-iteration; it ends up with multiple references to that collection.  going back to ConcurrentHashMap fixes the problem.  (any additional overhead from CHM is negligible here.)  patch by jbellis; reviewed by Eric Evans for #57

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/trunk@764004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a specific issue encountered with `NonBlockingHashMap` when removing and re-adding the same collection during iteration. The code change switches from using a `NonBlockingHashMap` to a `ConcurrentHashMap`. 

Here's the analysis:

1. **Commit Message Context**: 
   - The message highlights that there's a bug in `getCompactionBuckets` when using `NonBlockingHashMap`.
   - It mentions that removing and re-adding collections during iteration results in multiple references to the same collection.
   - The proposed solution is to revert to `ConcurrentHashMap`, which avoids the problem with `NonBlockingHashMap`.

2. **Code Change**:
   - The core change is the replacement of `NonBlockingHashMap` with `ConcurrentHashMap`.
   - `ConcurrentHashMap` is known for providing more reliable iteration, as it does not throw `ConcurrentModificationException` when the map is modified during iteration and does not result in duplicate entries as described in the message.
   - This change is likely addressing the bug detailed in the message, as `ConcurrentHashMap` would prevent the described issue with accidental duplicates because of its thread-safe operations and consistent state during iterations.

3. **Bug Fix Analysis**:
   - The code change directly addresses the bug described in the commit message.
   - By using `ConcurrentHashMap`, the specific problem of multiple references and incorrect behavior during iteration in `NonBlockingHashMap` is mitigated.
   - This change solves the duplicate collection reference issue caused by removal and re-adding mid-iteration.

Based on the alignment of the code modification with the commit message description of a bug and its resolution, it can be concluded that this change indeed addresses a bug.

**Buggy**"
cassandra,6678.json,7b083a4e083442cb6e6bf34735753c6103d88654,"@@ -1,8 +1,6 @@
     public static void rescheduleFailedTasks()
     {
-        for (SnapshotDeletingTask task : failedTasks)
-        {
-            failedTasks.remove(task);
+        Runnable task;
+        while ( null != (task = failedTasks.poll()))
             ScheduledExecutors.nonPeriodicTasks.submit(task);
-        }
     }",Buggy,"Fix coverity errors with SSTableDeletingTask and SnapshotDeletingtask

Patch by jmckenzie; reviewed by stefania for CASSANDRA-10222
",Buggy,"The commit message suggests fixing errors identified by the Coverity static analysis tool related to `SSTableDeletingTask` and `SnapshotDeletingTask`. However, the code diff provided only shows changes concerning `SnapshotDeletingTask`, and it provides a fix for managing the `failedTasks` collection.

### Code Analysis

1. **Original Implementation**:
   - The original implementation uses an enhanced for-loop to iterate over the `failedTasks` collection.
   - Inside the loop, it removes each task immediately after processing: `failedTasks.remove(task)`.
   - This is usually problematic because you cannot modify a collection while iterating over it using an enhanced for-loop, as it leads to a `ConcurrentModificationException`.

2. **Modified Implementation**:
   - The modified implementation replaces the for-loop and uses a `while` loop, which iteratively polls tasks from the `failedTasks` collection using `poll()`.
   - The `poll()` method retrieves and removes the head of this queue, or returns `null` if this queue is empty, which prevents concurrent modification issues.

### Conclusion

The changes in the code fix a potential `ConcurrentModificationException` issue by altering the way tasks are retrieved and submitted. This adjustment aligns with the commit message's indication of addressing Coverity errors, which likely flagged the concurrent modification issue.

Therefore, the change is a bug fix related to safe handling of task collection modifications, thus the conclusion is **Buggy**."
cassandra,9236.json,2a656e6bd5e1f5ac8161b65a211dd8bed74b7def,"@@ -1,10 +1,10 @@
     private MerkleTree getMerkleTree(Token t)
     {
         for (Range<Token> range : merkleTrees.keySet())
         {
             if (range.contains(t))
                 return merkleTrees.get(range);
         }
 
-        return null;
+        throw new AssertionError(""Expected tree for token "" + t);
     }",Buggy,"Fixed coverity defects
Reviewed by Marcus Olsson for CASSANDRA-5220
",Buggy,"The modification in the code changes the return behavior of the `getMerkleTree` method. Initially, if no matching `Range<Token>` was found, the method would return `null`. The updated code now throws an `AssertionError` when the expected tree for a token is not found.

This change can indeed be related to fixing a defect:

1. **Error Handling Improvement**: By throwing an `AssertionError`, the code now explicitly indicates that not finding a tree for the given token is unexpected and likely represents a logical oversight or error condition in the calling code. Such clarity is often an improvement over returning `null`, which can lead to `NullPointerExceptions` if the caller does not handle the null case properly.

2. **Coverity Defects Context**: Coverity is a static analysis tool that identifies potential defects in code, including paths that might lead to null dereferences, dead code, or other logical issues. Changing the behavior from returning `null` to throwing an error can resolve defects where null dereferences were possible because it forces handle the case where a tree isn't found.

Given that this change enhances the robustness and clarity of the method in handling error conditions and aligns with the commit message of fixing defects, it indicates an improvement over previous logic that could lead to bugs.

Thus, the change should be classified as **Buggy**."
cassandra,20480.json,6991556e431a51575744248a4c484270c4f918c9,"@@ -1,4 +1,4 @@
     public static void truncate()
     {
-        ALL.forEach(table -> getSchemaCFS(table).truncateBlocking());
+        ALL.reverse().forEach(table -> getSchemaCFS(table).truncateBlocking());
     }",Buggy,"Fix startup problems due to schema tables not completely flushed

patch by Stefania Alborghetti; reviewed by Aleksey Yeschenko for CASSANDRA-12213.
",Buggy,"The change modifies the order of operations in the `truncate` method by applying `reverse()` to the list `ALL` before executing `truncateBlocking()` on each schema table. This suggests addressing a specific sequence or dependency issue during the flush operation of schema tables, as the startup problem referenced in the commit message is due to tables not being completely flushed. By reversing the order, the change likely ensures a more correct and complete truncation process, potentially solving issues related to lingering or dependent data preventing a clean startup.

This adjustment directly correlates with the problem described in the commit message, suggesting that the original order of truncation led to issues not flushing all necessary tables appropriately, which could result in startup errors.

Thus, the code change addresses a logical correction to resolve a bug related to the schema table flushing process, as described in the commit message.

The final assessment is **Buggy**."
cassandra,17609.json,0f977c597a6b70984de96bcc49474acdb12ad2ea,"@@ -1,24 +1,24 @@
     public int compare(Composite c1, Composite c2)
     {
         // This method assumes that simple composites never have an EOC != NONE. This assumption
         // stands in particular on the fact that a Composites.EMPTY never has a non-NONE EOC. If
         // this ever change, we'll need to update this.
 
         if (isByteOrderComparable)
         {
             // toByteBuffer is always cheap for simple types, and we keep virtual method calls to a minimum:
             // hasRemaining will always be inlined, as will most of the call-stack for BBU.compareUnsigned
             ByteBuffer b1 = c1.toByteBuffer();
             ByteBuffer b2 = c2.toByteBuffer();
             if (!b1.hasRemaining() || !b2.hasRemaining())
                 return b1.hasRemaining() ? 1 : (b2.hasRemaining() ? -1 : 0);
             return ByteBufferUtil.compareUnsigned(b1, b2);
         }
 
         boolean c1isEmpty = c1.isEmpty();
         boolean c2isEmpty = c2.isEmpty();
         if (c1isEmpty || c2isEmpty)
-            return c1isEmpty ? 1 : (c2isEmpty ? -1 : 0);
+            return !c1isEmpty ? 1 : (!c2isEmpty ? -1 : 0);
 
         return type.compare(c1.get(0), c2.get(0));
     }",Buggy,"Fix CellName comparison bugs

Patch by tjake; reviewed by bes for CASSANDRA-7227
",Buggy,"The commit message states that it fixes comparison bugs in CellName. The code diff shows a change in the logic used to compare empty composites:

- **Original Code:**
  - `return c1isEmpty ? 1 : (c2isEmpty ? -1 : 0);`

- **Updated Code:**
  - `return !c1isEmpty ? 1 : (!c2isEmpty ? -1 : 0);`

This modification corrects the logic to check if a composite is not empty and return the appropriate comparison result. In the context of composite comparison, the buggy behavior would have resulted in incorrect ordering when dealing with empty composites. The change ensures the logic properly reflects the intended comparison operation.

Given the corrected logic is directly altering comparison operations, this update is indeed a bug fix. Thus, the analysis aligns well with the commit message's claim of addressing comparison bugs. 

Therefore, the answer is **Buggy**."
cassandra,16419.json,653dcc63e0f6b0c5b3c3592beb1e0dd3bc1ee0cf,"@@ -1,4 +1,4 @@
     public Long compose(ByteBuffer bytes)
     {
-        return ByteBufferUtil.toLong(bytes);
+        return CounterContext.instance().total(bytes);
     }",Buggy,"fix merge problem with CHANGES.txt, use right call in AbstractCommutativeType.compose()

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@1082155 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a correction in the method AbstractCommutativeType.compose(). The original lines use ByteBufferUtil.toLong to convert bytes to a Long, but it's changed to CounterContext.instance().total. This change suggests that the original method did not perform the intended logic, possibly misinterpreting the byte data. The change to use CounterContext's method implies a need to correctly handle or interpret the data using context-specific logic present in CounterContext. Given the commit message also mentions fixing a merge problem and the nature of the change, it indeed rectifies a logical error in the code concerning data interpretation or handling.

Therefore, this change is aligned with fixing a bug and improving the correctness of the code. Hence, the evaluation is **Buggy**."
jgit,9755.json,d2600693bd5fb8bda20bae41467132668caa1e14,"@@ -1,34 +1,40 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
-		while (tokenCount < 3 && nextSpace < lineEnd) {
+		while (tokenCount < 3 && nextSpace <= lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin,
 						nextSpace - tokenBegin - 1, UTF_8);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
-				String commitToken = new String(buf, tokenBegin,
-						nextSpace - tokenBegin - 1, UTF_8);
+				String commitToken;
+				if (nextSpace > lineEnd + 1) {
+					commitToken = new String(buf, tokenBegin,
+							lineEnd - tokenBegin + 1, UTF_8);
+				} else {
+					commitToken = new String(buf, tokenBegin,
+							nextSpace - tokenBegin - 1, UTF_8);
+				}
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit,
 						RawParseUtils.decode(buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
 		if (tokenCount == 2)
 			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix off-by-one error in RebaseTodoFile when reading a todo file

Commit messages of length 1 were not read. 'lineEnd' is the offset
of the last character in the line before the terminating LF or CR-LF,
and 'nextSpace' is actually the offset of the character _after_ the
next space. With a one-character commit message, nextSpace == lineEnd.

The code also assumes the commit message to be optional, but actually
failed in that case because it read beyond the line ending. Fix that,
too.

Add a test case for reading a todo file.

Bug: 546245
Change-Id: I368d63615930ea2398a6230e756442fd88870654
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"The commit message clearly indicates that the changes are intended to fix an off-by-one error and the handling of an optional commit message when reading a todo file. Let's analyze the code modifications in the diff to determine if they indeed address these issues:

1. **Earlier Condition Error:** 
   - Original Code: `while (tokenCount < 3 && nextSpace < lineEnd)` suggests that `nextSpace` must be strictly less than `lineEnd`. This is problematic if `nextSpace` is exactly `lineEnd`, especially for short commit messages of length 1.
   - Changed Code: `while (tokenCount < 3 && nextSpace <= lineEnd)` allows the loop to process cases where `nextSpace` equals `lineEnd`, fixing the off-by-one error.

2. **Handling a One-Character Commit Message:**
   - The commit message description highlights cases where `nextSpace == lineEnd` for one-character commit messages.
   - The conditional `if (nextSpace > lineEnd + 1)` ensures that the code correctly handles single-character commit messages (or those ending at `lineEnd`). It sets `commitToken` by either reading till `lineEnd` or treating everything until `nextSpace`.

3. **Optional Commit Message Handling:**
   - Under certain conditions, the code ensures that if there are exactly two tokens without space, it still creates a `RebaseTodoLine`, as evidenced by `if (tokenCount == 2)` returning a line with an empty message rather than failing. This aligns with the message that correctly reading a file even when a commit message is absent.

Overall, these changes address the issues mentioned in the commit message: fixing the off-by-one error and handling lines lacking a commit message appropriately. Therefore, the changes do indicate a bug fix, and I conclude with **Buggy**."
jgit,9755.json,4feace2b9ecb90fe591c4317403f6d8e6309287e,"@@ -1,32 +1,34 @@
 	private static RebaseTodoLine parseLine(byte[] buf, int tokenBegin,
 			int lineEnd) {
 		RebaseTodoLine.Action action = null;
 		AbbreviatedObjectId commit = null;
 
 		int nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 		int tokenCount = 0;
 		while (tokenCount < 3 && nextSpace < lineEnd) {
 			switch (tokenCount) {
 			case 0:
 				String actionToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				action = RebaseTodoLine.Action.parse(actionToken);
 				if (action == null)
 					return null; // parsing failed
 				break;
 			case 1:
 				nextSpace = RawParseUtils.next(buf, tokenBegin, ' ');
 				String commitToken = new String(buf, tokenBegin, nextSpace
 						- tokenBegin - 1);
 				tokenBegin = nextSpace;
 				commit = AbbreviatedObjectId.fromString(commitToken);
 				break;
 			case 2:
 				return new RebaseTodoLine(action, commit, RawParseUtils.decode(
 						buf, tokenBegin, 1 + lineEnd));
 			}
 			tokenCount++;
 		}
+		if (tokenCount == 2)
+			return new RebaseTodoLine(action, commit, """"); //$NON-NLS-1$
 		return null;
 	}",Buggy,"Fix parsing Rebase todo lines when commit message is missing

Bug: 422253
Change-Id: I9739b16c91d2df31a481360a712d3479a4eeee2e
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message claims to fix an issue with parsing Rebase todo lines when the commit message is missing. 

Analyzing the code changes:
1. The method `parseLine` is responsible for interpreting lines in the ""Rebase Todo"" file, breaking down the line into an action, a commit ID, and a commit message.
2. Originally, if fewer than 3 tokens were found, the method returned `null`, indicating a parsing failure.
3. The change adds a condition `if (tokenCount == 2)` that will create and return a new `RebaseTodoLine` object if the action and commit ID were parsed but the commit message was missing. In this case, it assigns an empty string to the commit message.

This modification directly addresses parsing lines when the commit message is missing, ensuring valid `RebaseTodoLine` creation from such inputs instead of failing. Thus, the changes described in the commit message align with this fix.

The fix shows improvements in handling incomplete input data (missing commit message) gracefully. This indicates a bug fix related to parsing logic when specific input components are absent. 

Thus, the assessment of these changes relative to the commit message is **Buggy**."
jgit,168.json,846ef78a02edceb99940d7aa92dcd2462a85c602,"@@ -1,24 +1,24 @@
 	private RefTree rebuild(RefDatabase refdb) throws IOException {
 		RefTree tree = RefTree.newEmptyTree();
 		List<org.eclipse.jgit.internal.storage.reftree.Command> cmds
 			= new ArrayList<>();
 
 		Ref head = refdb.exactRef(HEAD);
 		if (head != null) {
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					head));
 		}
 
 		for (Ref r : refdb.getRefs(RefDatabase.ALL).values()) {
-			if (r.getName().equals(txnCommitted)
+			if (r.getName().equals(txnCommitted) || r.getName().equals(HEAD)
 					|| r.getName().startsWith(txnNamespace)) {
 				continue;
 			}
 			cmds.add(new org.eclipse.jgit.internal.storage.reftree.Command(
 					null,
 					db.peel(r)));
 		}
 		tree.apply(cmds);
 		return tree;
 	}",Buggy,"Fix RebuildRefTree trying to add HEAD twice to RefTree

14dfa70520 fixed the problem that HEAD wasn't added to the reftree when
rebuilding the reftree in an empty repository where HEAD isn't yet
resolvable. Since non-resolvable refs are filtered out by
RefDatabase.getRefs(ALL) we have to add HEAD to the reftree explicitly
in this special case.

This fix resulted in another bug: rebuilding the reftree in a repository
which has a resolvable HEAD failed with a DirCacheNameConflictException
in RefTree.apply(). If HEAD is resolvable RefDatabase.getRefs(ALL) does
not filter out HEAD. This results in two identical CREATE commands for
HEAD which RefTree.apply() refuses to execute.

Fix this by no longer creating a duplicate CREATE command for HEAD.

See: I46cbc2611b9ae683ef7319dc46af277925dfaee5
Change-Id: I58dd6bcdef88820aa7de29761d43e2edfa18fcbe
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message states that it fixes an issue where rebuilding the reftree tries to add the HEAD reference twice, causing a `DirCacheNameConflictException`. The logic in the code is updated to avoid adding a duplicate CREATE command for HEAD when it is already resolvable.

Let's analyze the changes step by step:

1. The code introduces a condition in the loop iterating through `refdb.getRefs(RefDatabase.ALL).values()` to skip adding a command for HEAD if it's already resolved. The new condition `r.getName().equals(HEAD)` is added to the existing filter criteria.

2. If the HEAD reference is resolved (i.e., `refdb.exactRef(HEAD)` returns a non-null value), a corresponding command is added to the command list `cmds` before the loop.

3. The aim of this logic is to ensure that the command for creating the HEAD reference is not added twice to the command list, which would lead to errors when the commands are applied. 

Given these changes and how they reflect the commit message description, the modifications clearly address a bug. The update in logic prevents the erroneous addition of duplicate commands for HEAD, which aligns with preventing the specific exception mentioned. These are indeed changes to fix a legitimate error.

Thus, the conclusion for this analysis is **Buggy**."
jgit,8367.json,ec97912762754ee88f1af5ed80e993c545778242,"@@ -1,26 +1,26 @@
 	public int match(final RawCharSequence rcs) {
 		final int needleLen = needle.length;
 		final byte first = needle[0];
 
 		final byte[] text = rcs.buffer;
 		int matchPos = rcs.startPtr;
 		final int maxPos = rcs.endPtr - needleLen;
 
-		OUTER: for (; matchPos < maxPos; matchPos++) {
+		OUTER: for (; matchPos <= maxPos; matchPos++) {
 			if (neq(first, text[matchPos])) {
-				while (++matchPos < maxPos && neq(first, text[matchPos])) {
+				while (++matchPos <= maxPos && neq(first, text[matchPos])) {
 					/* skip */
 				}
-				if (matchPos == maxPos)
+				if (matchPos > maxPos)
 					return -1;
 			}
 
-			int si = ++matchPos;
+			int si = matchPos + 1;
 			for (int j = 1; j < needleLen; j++, si++) {
 				if (neq(needle[j], text[si]))
 					continue OUTER;
 			}
-			return matchPos - 1;
+			return matchPos;
 		}
 		return -1;
 	}",Buggy,"Fix multiple bugs in RawSubStringPattern used by MessageRevFilter

* Match at end of input was not handled correctly.
* When more than one character matched but not all, the next character
  was not considered as a match start (e.g. pattern ""abab"" didn't match
  input ""abaabab"").

Bug: 409144
Change-Id: Ia44682c618bfbb927f5567c194227421d222a160
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates fixes for multiple bugs related to matching logic in `RawSubStringPattern`. Analyzing the diff:

1. **End of Input Handling**: 
   - Originally, the loop iterated while `matchPos < maxPos`, meaning it didn't correctly handle matches at the extreme end. The change to `matchPos <= maxPos` ensures the last possible starting position for a match is considered.

2. **Match Position Adjustments**: 
   - The position `si = ++matchPos;` has been changed to `si = matchPos + 1;`. Additionally, the return statement was corrected from `return matchPos - 1;` to `return matchPos;`. This corrects the start index returned when a match is found, aligning with the intended logic that `matchPos` correctly points to the start of the match.

3. **Character Match Loop Update**:
   - The loop checking for non-equal first characters was adjusted from `while (++matchPos < maxPos` to `while (++matchPos <= maxPos`, ensuring all possible positions, including the last position, are considered.

Each of these changes addresses errors described in the commit message:
- The loop adjustments handle matches at the very end, fixing the bug about not handling matches at the end of input.
- The adjustments ensure correct behavior when potential matches partially align, allowing reevaluation on subsequent starts, fixing the described multi-character issue.

Given the identified corrections aligning with the commit message, the answer is **Buggy**."
jgit,6829.json,5e44bfa3ad462e1220426492c53606c6a643a970,"@@ -1,6 +1,10 @@
 	private boolean isNoNewlineAtEndOfFile(FileHeader fh) {
-		HunkHeader lastHunk = fh.getHunks().get(fh.getHunks().size() - 1);
+		List<? extends HunkHeader> hunks = fh.getHunks();
+		if (hunks == null || hunks.isEmpty()) {
+			return false;
+		}
+		HunkHeader lastHunk = hunks.get(hunks.size() - 1);
 		RawText lhrt = new RawText(lastHunk.getBuffer());
-		return lhrt.getString(lhrt.size() - 1).equals(
-				""\""); //$NON-NLS-1$
+		return lhrt.getString(lhrt.size() - 1)
+				.equals(""\""); //$NON-NLS-1$
 	}",Buggy,"Fix ApplyCommand which doesn't work if patch adds empty file

Bug: 548219
Change-Id: Ibb32132a38e54508a24489322da58ddfd80a1d9a
Signed-off-by: Anton Khodos <khodosanton@gmail.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message describes fixing how ApplyCommand handles patches adding an empty file, and the code change addresses this scenario. Originally, isNoNewlineAtEndOfFile assumed non-empty hunk lists, failing if empty. The update adds a check for null or empty hunk lists, defaulting the return to false, aligning the behavior with the commit message. This indicates a correction in the logic that would impact the handling of certain scenarios, specifically when dealing with patches involving empty files. Thus, the change indicates a bug fix. The answer is **Buggy**."
jgit,9865.json,3e2ec7e5e81a489a326d872aa0670119ff2c1152,"@@ -1,8 +1,8 @@
 	public int compareTo(LfsPointer o) {
 		int x = getOid().compareTo(o.getOid());
 		if (x != 0) {
 			return x;
 		}
 
-		return (int) (getSize() - o.getSize());
+		return Long.compare(getSize(), o.getSize());
 	}",Buggy,"Fix BadComparable error flagged by error prone

Running recent error prone version complaining on that code:

LfsPointer.java:171: error: [BadComparable] Possible sign flip from
narrowing conversion
		return (int) (getSize() - o.getSize());
		       ^
    (see https://errorprone.info/bugpattern/BadComparable)
  Did you mean 'return Long.compare(getSize(), o.getSize());'?

Bug: 562756
Change-Id: I0522f1025319a9290c448a064fbafdb4b16d1d59
Signed-off-by: David Ostrovsky <david@ostrovsky.org>
",Buggy,"The commit message indicates a fix for a potential `BadComparable` issue identified by Error Prone. The original implementation attempted to perform a subtraction operation on `long` values and then cast the result to an `int`, which could lead to a sign flip due to narrowing conversion, as detailed in the linked Error Prone documentation. This type of error could lead to incorrect behavior when comparing large numbers, as the result could overflow an `int`.

The change replaces this with `Long.compare(getSize(), o.getSize())`, which correctly returns a comparison value for two `long` values without the risk of overflow or sign flip. This change directly addresses the issue mentioned in the commit message by preventing incorrect results in comparisons caused by data loss in casting.

Given this analysis, the changes align well with the commit message and resolve a bug related to the comparison logic. Therefore, the changes are categorized as a bug fix.

**Buggy**"
jgit,7605.json,56ee8117802a672e80011ee07ea6b253a5b7d0a9,"@@ -1,13 +1,17 @@
 	public boolean isPathSuffix(final byte[] p, final int pLen) {
 		final AbstractTreeIterator t = currentHead;
 		final byte[] c = t.path;
 		final int cLen = t.pathLen;
-		int ci;
 
-		for (ci = 1; ci < cLen && ci < pLen; ci++) {
-			if (c[cLen-ci] != p[pLen-ci])
+		for (int i = 1; i <= pLen; i++) {
+			// Pattern longer than current path
+			if (i > cLen)
+				return false;
+			// Current path doesn't match pattern
+			if (c[cLen - i] != p[pLen - i])
 				return false;
 		}
 
+		// Whole pattern tested -> matches
 		return true;
 	}",Buggy,"Fix bugs in TreeWalk#isPathSuffix used by PathSuffixFilter

* It didn't check the first character in the pattern due to a off-by-one
  error. Spotted by James Roper.
* It returned true even when pattern was longer than current path, e.g.
  it returned that "".txt"" is suffix of ""txt"".

Bug: 411999
Change-Id: I9fbcd68a11fb57cc49956b70c387a47271a0424f
Signed-off-by: Robin Stocker <robin@nibor.org>
",Buggy,"The commit message explains two specific bug fixes in the `TreeWalk#isPathSuffix` method. There are two primary issues addressed:

1. **Off-by-One Error**: The loop originally started at index 1 when it should have included index 0. Moreover, the loop condition did not properly ensure that the first characters were being checked against each other, effectively skipping the first character of the pattern.

2. **Pattern Length Check**: The previous implementation allowed the method to return `true` even when the pattern was longer than the current path, which was incorrect as `.txt` should not be considered a suffix of `txt`.

Now let's analyze the changes:

- The loop now starts with `int i = 1` instead of an uninitialized `ci` and runs from 1 up to `pLen` inclusive (`i <= pLen`). By using `cLen - i` and `pLen - i`, it correctly checks each corresponding character from the end of the `p` and `c` arrays against each other, which should have been checked in both cases.

- An additional condition `if (i > cLen) return false;` has been added to ensure that if the pattern is longer than the current path (`c`), the method returns false immediately, addressing the second described issue.

- The for loop logic and additional comments outline the correct logical flow and boundaries to determine if the path has the given suffix and the condition to evaluate if the whole pattern matches accordingly.

Considering these modifications, the changes directly address described logical errors and align well with the commit message stating the nature of the bug fixes. The improvements in logic indicate clear bug-related changes.

Hence, the changes reflect a bug fix, leading to a conclusion of **Buggy**."
jgit,7675.json,5b55498b16267102b021a47b3a7c0bdbfae63e71,"@@ -1,5 +1,3 @@
 	protected byte[] idSubmodule(final Entry e) {
-		if (repository == null)
-			return idSubmodule(getDirectory(), e);
-		return super.idSubmodule(e);
+		return idSubmodule(getDirectory(), e);
 	}",Buggy,"Fix FileTreeIterator.idSubmodule(Entry)

FileTreeIterator was calling by mistake
WorkingTreeIterator.idSubmodule(Entry). Instead it should always compute
idSubmodule on its own.

Change-Id: Id1b988aded06939b1d7edd2671e34bf756896c0e
",Buggy,"The commit message and the diff align well to indicate a bug fix. Here's the breakdown:

1. **Commit Message Analysis**: 
   - The commit message explicitly states that there was a mistake in calling the method `WorkingTreeIterator.idSubmodule(Entry)` within `FileTreeIterator` and that the correct behavior should involve computing `idSubmodule` internally in `FileTreeIterator`.

2. **Code Diff Analysis**:
   - The original code checks whether `repository` is null. If it is not, it calls `super.idSubmodule(e)`, which is likely the erroneous call to `WorkingTreeIterator.idSubmodule(Entry)`.
   - The new code removes this conditional statement and consistently calls `idSubmodule(getDirectory(), e)`, which fulfills the requirement outlined in the commit message: FileTreeIterator should compute it on its own.

3. **Bug Fix Determination**:
   - The change corrects the logic by ensuring that the `idSubmodule` is always computed in a consistent manner, addressing the incorrect invocation of the parent class's method.
   - This logical correction ensures that the method behavior aligns with the intended design, as referenced in the commit message.

Consequently, the changes made in this commit reflect a clear bug fix according to the stated issue in the commit message.

The answer is **Buggy**."
jgit,4217.json,e60b9e1879f8774e1afe07be4224605045f49eec,"@@ -1,8 +1,9 @@
 	private long getEffectiveRacyThreshold() {
 		long timestampResolution = fileStoreAttributeCache
 				.getFsTimestampResolution().toNanos();
 		long minRacyInterval = fileStoreAttributeCache.getMinimalRacyInterval()
 				.toNanos();
-		// add a 30% safety margin
-		return Math.max(timestampResolution, minRacyInterval) * 13 / 10;
+		long max = Math.max(timestampResolution, minRacyInterval);
+		// safety margin: factor 2.5 below 100ms otherwise 1.25
+		return max < 100_000_000L ? max * 5 / 2 : max * 5 / 4;
 	}",Buggy,"FileSnapshot: fix bug with timestamp thresholding

Increase the safety factor to 2.5x for extra safety if max of measured
timestamp resolution and measured minimal racy threshold is < 100ms, use
1.25 otherwise since for large filesystem resolution values the
influence of finite resolution of the system clock should be negligible.

Before, not yet using the newly introduced minRacyThreshold measurement,
the threshold was 1.1x FS resolution, and we could issue the
following sequence of events,

  start
  create-file
  read-file (currentTime)
  end

which had the following timestamps:

  create-file 1564589081998
  start 1564589082002
  read 1564589082003
  end 1564589082004

In this case, the difference between create-file and read is 5ms,
which exceeded the 4ms FS resolution, even though the events together
took just 2ms of runtime.

Reproduce with:
  bazel test --runs_per_test=100 \
    //org.eclipse.jgit.test:org_eclipse_jgit_internal_storage_file_FileSnapshotTest

The file system timestamp resolution is 4ms in this case.

This code assumes that the kernel and the JVM use the same clock that
is synchronized with the file system clock. This seems plausible,
given the resolution of System.currentTimeMillis() and the latency for
a gettimeofday system call (typically ~1us), but it would be good to
justify this with specifications.

Also cover a source of flakiness: if the test runs under extreme load,
then we could have

  start
  create-file
  <long delay>
  read
  end

which would register as an unmodified file. Avoid this by skipping the
test if end-start is too big.

[msohn]:
- downported from master to stable-5.1
- skip test if resolution is below 10ms
- adjust safety factor to 1.25 for resolutions above 100ms

Change-Id: I87d2cf035e01c44b7ba8364c410a860aa8e312ef
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>
",Buggy,"The commit message and code changes are about addressing a bug related to timestamp thresholding in file snapshots. Here's a step-by-step analysis:

1. **Commit Message Analysis**:
    - The message explicitly mentions a bug fix concerning timestamp thresholding.
    - It describes increasing the safety margins based on measured thresholds and timestamp resolutions.
    - It discusses a specific scenario where the previous threshold calculation could incorrectly consider a file as not modified because of timestamp resolution issues.
    - The message also talks about test adjustments to avoid flakiness under extreme load.

2. **Code Diff Analysis**:
    - The code change is in the method `getEffectiveRacyThreshold`.
    - Previously, the effective racy threshold was a static calculation using a 30% safety margin: `max * 13 / 10`.
    - The new change introduces a conditional safety margin: 
        - If the maximum of timestamp resolution and minimal racy interval is less than 100ms, it uses a factor of 2.5 (`max * 5 / 2`).
        - For values 100ms or more, it uses a factor of 1.25 (`max * 5 / 4`).
    - This change increases the threshold, addressing potential miscalculations identified in the commit message.

3. **Relevance and Indication of Bug Fix**:
    - The code change aligns well with the commit message. The updated safety margins address the reported bug of false negatives in file modification detection due to insufficient thresholds.
    - The message and code indicate an understanding of the underlying problem with specific examples and reasoning for the calculations.
    - The changes to accommodate different conditions and prevent flakiness demonstrate a robust bug fix methodology.

Given these observations, the changes are indicative of a bug fix concerning file modification timestamp handling. Therefore, the conclusion is **Buggy**."
jgit,5424.json,35b01dac4c81542b195169e3b7365c12a165232c,"@@ -1,45 +1,41 @@
 	public void fillTo(final int highMark) throws MissingObjectException,
 			IncorrectObjectTypeException, IOException {
 		if (walker == null || size > highMark)
 			return;
 
-		Generator p = walker.pending;
-		RevCommit c = p.next();
+		RevCommit c = walker.next();
 		if (c == null) {
-			walker.pending = EndGenerator.INSTANCE;
 			walker = null;
 			return;
 		}
 		enter(size, (E) c);
 		add((E) c);
-		p = walker.pending;
 
 		while (size <= highMark) {
 			int index = size;
 			Block s = contents;
 			while (index >> s.shift >= BLOCK_SIZE) {
 				s = new Block(s.shift + BLOCK_SHIFT);
 				s.contents[0] = contents;
 				contents = s;
 			}
 			while (s.shift > 0) {
 				final int i = index >> s.shift;
 				index -= i << s.shift;
 				if (s.contents[i] == null)
 					s.contents[i] = new Block(s.shift - BLOCK_SHIFT);
 				s = (Block) s.contents[i];
 			}
 
 			final Object[] dst = s.contents;
 			while (size <= highMark && index < BLOCK_SIZE) {
-				c = p.next();
+				c = walker.next();
 				if (c == null) {
-					walker.pending = EndGenerator.INSTANCE;
 					walker = null;
 					return;
 				}
 				enter(size++, (E) c);
 				dst[index++] = c;
 			}
 		}
 	}",Buggy,"Fix RevCommitList to work with subclasses of RevWalk

Bug: 321502
Change-Id: Ic4bc49a0da90234271aea7c0a4e344a1c3620cfc
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message suggests a fix to enable `RevCommitList` to work with subclasses of `RevWalk`. The diff shows a change in how `RevCommits` are fetched. Originally, `RevCommits` were fetched using a `Generator` (`p`), but the updated version calls `walker.next()` directly, both for the initial commit retrieval and inside the loop. The removal of `walker.pending = EndGenerator.INSTANCE;` also indicates a change in handling the terminal state of the walker, which likely addresses a bug related to the subclass behavior, ensuring that the progression through commits is handled appropriately without relying on the `pending` state.

- The issue appears to be that the previous implementation relied on the internal state (`pending`) that might not work correctly with subclasses.
- Shifting to `walker.next()` directly should better accommodate subclasses, since any overridden `next()` logic in subclasses will be naturally included.
- This change indicates fixing a bug, as it aligns with the ability to handle varied implementations within the scope of `RevWalk`, which the original approach might have incorrectly handled.

Therefore, the changes in the diff do reflect the commit message and indicate a bug fix related to this handling.

The conclusion is **Buggy**."
jgit,5825.json,b07db609089749ed49a7f0b1fb3841a8f74110c2,"@@ -1,13 +1,16 @@
 	static int count(String s, char c, boolean ignoreFirstLast) {
 		int start = 0;
 		int count = 0;
-		while (true) {
+		int length = s.length();
+		while (start < length) {
 			start = s.indexOf(c, start);
-			if (start == -1)
+			if (start == -1) {
 				break;
-			if (!ignoreFirstLast || (start != 0 && start != s.length()))
+			}
+			if (!ignoreFirstLast || (start != 0 && start != length - 1)) {
 				count++;
+			}
 			start++;
 		}
 		return count;
 	}",Buggy,"Fix off-by-one error in Strings.count()

Change-Id: I0667b1624827d1cf0cc1b81f86c7bb44eafd68a7
Signed-off-by: Thomas Wolf <thomas.wolf@paranor.ch>",Buggy,"The commit message mentions a fix for an off-by-one error in `Strings.count()`. The changes in the diff support this:

1. The loop condition `while (true)` is refined to `while (start < length)`. This ensures the loop iterates correctly and doesn't run indefinitely, addressing potential off-by-one error issues.
   
2. The condition `if (!ignoreFirstLast || (start != 0 && start != s.length()))` is modified to `if (!ignoreFirstLast || (start != 0 && start != length - 1))`. This corrects the logic to prevent counting the last character when `ignoreFirstLast` is true, resolving an off-by-one error by properly referencing the last index as `length - 1`.

The changes relate directly to correcting an error in the counting logic, aligning perfectly with the stated purpose of the commit message. Thus, this is indeed a bug fix. Therefore, the answer is **Buggy**."
jgit,6619.json,51a5cc7f1a1033664ee2fb760ed217b665b12b34,"@@ -1,6 +1,6 @@
 	private static int findForwardLine(IntList lines, int idx, int ptr) {
 		final int end = lines.size() - 2;
-		while (idx < end && lines.get(idx + 2) <= ptr)
+		while (idx < end && lines.get(idx + 2) < ptr)
 			idx++;
 		return idx;
 	}",Buggy,"Fix diff when first text is the start of the other

The problem occurred when the first text ends in the middle
of the last line of the other text and the first text has no
end of line.

Bug: 344975
Change-Id: I1f0dd9f8062f2148a7c1341c9122202e082ad19d
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
",Buggy,"The commit statement indicates a bug fix for a specific edge case involving text diffing when one text ends in the middle of the last line of the other. 

In the code diff:
- The while condition in `findForwardLine` changes from `lines.get(idx + 2) <= ptr` to `lines.get(idx + 2) < ptr`.

This change suggests that the previous logic might cause an off-by-one error, leading to incorrect index selection when the `ptr` is equal to `lines.get(idx + 2)`. By changing the condition to strictly less than (`<`), we ensure the correct line is selected, which directly addresses the issue described in the commit message.

Additionally, the linked bug number (344975) supports the presence of a specific bug that needed fixing.

The reasoning aligns with the commit message's description of the problem, and the code change corrects this specific comparison logic, indicating it is a bug fix.

Thus, the analysis concludes with **Buggy**."
jgit,6447.json,e0e52cb0110a908959f7df13c5e66bf911bad74f,"@@ -1,6 +1,6 @@
 	private boolean result(Candidate n) throws IOException {
 		n.beginResult(revPool);
 		outCandidate = n;
 		outRegion = n.regionList;
-		return true;
+		return outRegion != null;
 	}",Buggy,"Fix NPE in BlameGenerator.getSourceStart()

Bug: 499543
Change-Id: I99f6ebb1c3ceea20e8ca093acbe824c9f0362d45
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) in the `BlameGenerator.getSourceStart()` method. The provided code change corresponds to this by modifying the `result` method. The change ensures that the method only returns `true` if `outRegion` is not `null`. This adjustment likely prevents the NPE by checking the nullability of `outRegion`, which could previously have been `null` upon returning `true`, thus leading to a potential NPE in subsequent operations. This is a classic example of a bug fix addressing an NPE.

The analysis of the diff and the reasoning regarding the potential NPE aligns with the commit message, showing that the change indeed serves as a bug fix.

Therefore, the conclusion is **Buggy**."
jgit,6994.json,0505657d6a7dd4575a64ddfb5c0928870fe5843b,"@@ -1,9 +1,9 @@
 	public CommitCommand setAll(boolean all) {
 		checkCallable();
-		if (!only.isEmpty())
+		if (all && !only.isEmpty())
 			throw new JGitInternalException(MessageFormat.format(
 					JGitText.get().illegalCombinationOfArguments, ""--all"", //$NON-NLS-1$
 					""--only"")); //$NON-NLS-1$
 		this.all = all;
 		return this;
 	}",Buggy,"commit command: allow to specify path(s) argument(s)

This fixes the command below:

jgit commit a -m ""added file a""

which currently fails with:

org.eclipse.jgit.api.errors.JGitInternalException: The combination of
arguments --all and --only is not allowed

Bug: 484973
Change-Id: I37a4ccd68101a66520ef99110f7aa0cbdcc8beba
Signed-off-by: Andrey Loskutov <loskutov@gmx.de>
",Buggy,"The change in the diff modifies the condition for throwing a JGitInternalException in the setAll method. Previously, an exception would be thrown if the ""only"" list was not empty. Now, the condition is more precise: it throws an exception only if ""all"" is true and the ""only"" list is not empty. This alteration correctly aligns with the commit message, which aims to allow command options that specify paths without causing a conflict between --all and --only arguments. Since the diff adjusts logic to prevent a specific error scenario (illustrated by the exception message), this reflects a bug fix.

Therefore, based on this analysis, the changes indicate a bug fix. The conclusion is **Buggy**."
jgit,7416.json,e56d50a94bfe60663bd589da61e07b9461f2c5f7,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
+		// Add the previous message without header (i.e first line)
+		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
+		sb.append(""\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
 			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
-		// Add the previous message without header (i.e first line)
-		sb.append(""\n"");
-		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Interactive rebase: Fix order of commit messages on squash

Bug: 431214
Change-Id: I295bfdc5751545b046d7fe7efc3f8b39ab4f5415
Signed-off-by: Stefan Lay <stefan.lay@sap.com>
Signed-off-by: Robin Rosennberg <robin.rosenberg@dewire.com>
",Buggy,"The commit message describes fixing the order of commit messages during a rebase when squashing commits. The code change moves the appending of the `currSquashMessage` to an earlier point in the method. This change reflects the purpose of the commit: ensuring that the order in which messages are composed aligns correctly during squash operations. The previous placement of the `currSquashMessage` appending made it occur after new messages were added, potentially disrupting the intended order. By moving the appending line above the new message additions, the change addresses the stated bug of incorrect order during commit squashing.

Therefore, the changes in the diff align well with the commit message, which indicates fixing the order of messages in a specific context (interactive rebase and squash). The change in code logic supports this by correcting the order of operations, likely resolving the issue described.

The conclusion is **Buggy**."
jgit,7416.json,8339a07e8314d6a40e15252bcc736a46c0aca0ea,"@@ -1,22 +1,22 @@
 	private static String composeSquashMessage(boolean isSquash,
 			RevCommit commitToPick, String currSquashMessage, int count) {
 		StringBuilder sb = new StringBuilder();
 		String ordinal = getOrdinal(count);
 		sb.setLength(0);
 		sb.append(""# This is a combination of "").append(count)
 				.append("" commits.\n"");
 		if (isSquash) {
 			sb.append(""# This is the "").append(count).append(ordinal)
 					.append("" commit message:\n"");
 			sb.append(commitToPick.getFullMessage());
 		} else {
 			sb.append(""# The "").append(count).append(ordinal)
 					.append("" commit message will be skipped:\n# "");
-			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r]+)"",
+			sb.append(commitToPick.getFullMessage().replaceAll(""([\n\r])"",
 					""$1# ""));
 		}
 		// Add the previous message without header (i.e first line)
 		sb.append(""\n"");
 		sb.append(currSquashMessage.substring(currSquashMessage.indexOf(""\n"") + 1));
 		return sb.toString();
 	}",Buggy,"Fix FIXUP error for blank lines in interactive rebase

Empty lines of discarded commit messages were added to the commit
message because they were not commented out properly.

Bug: 422246
Change-Id: I263e8a6b30a3392d8b4f09c0695505068a0a485d
Signed-off-by: Stefan Lay <stefan.lay@sap.com>",Buggy,"The commit message states a fix for an issue where empty lines in discarded commit messages weren't properly commented out during an interactive rebase. In the code diff, the replaceAll method's regex pattern changes from ""([\n\r]+)"" to ""([\n\r])"". The original pattern added a comment character (#) only once at the start of consecutive newline sequences. The revised pattern ensures each new line and carriage return character is followed by #, consistently commenting out lines. This aligns with the described bug fix of addressing the incorrect handling of empty lines in commit messages.

The change directly addresses the problem described in the commit message. Thus, this change is aimed at fixing a bug related to how empty lines in commit messages are processed during the rebase. 

The conclusion is **Buggy**."
jgit,9377.json,325cb35ccd6108eaf9e6c3ec6343ca6988f10a21,"@@ -1,6 +1,8 @@
 	private ObjectId idFor(int objType, byte[] raw) {
 		if (skipList != null) {
-			return new ObjectInserter.Formatter().idFor(objType, raw);
+			try (ObjectInserter.Formatter fmt = new ObjectInserter.Formatter()) {
+				return fmt.idFor(objType, raw);
+			}
 		}
 		return null;
 	}",Buggy,"[infer] Fix resource leak in ObjectChecker

Bug: 509385
Change-Id: I6b6ff5b721d959eb0708003a40c8f97d6826ac46
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",Buggy,"The commit message specifies a resource leak fix, which aligns with the code change. The modification uses Java's try-with-resources statement to ensure that the `ObjectInserter.Formatter` is properly closed after use. Previously, the `ObjectInserter.Formatter` was not enclosed in a try-with-resources block, which could lead to resource leaks if the formatter held onto system resources like file handles or streams without being explicitly closed.

This change addresses potential issues with resource management, thus confirming the commit's claim to fix a resource leak.

The code change matches the intention described in the commit message, indicating a bug fix. The conclusion is **Buggy**."
weka,20283.json,8215c432375cb2ff1bbda2825426fa0535b77834,"@@ -1,25 +1,13 @@
-  public EventSetDescriptor [] getEventSetDescriptors() {
+  public EventSetDescriptor[] getEventSetDescriptors() {
     try {
-      EventSetDescriptor [] esds = 
-      { new EventSetDescriptor(DataSource.class, 
-                               ""dataSet"", 
-                               DataSourceListener.class, 
-                               ""acceptDataSet""),
-        new EventSetDescriptor(DataSource.class, 
-                               ""instance"", 
-                               InstanceListener.class, 
-                               ""acceptInstance""),
-        new EventSetDescriptor(TrainingSetProducer.class, 
-                               ""trainingSet"", 
-                               TrainingSetListener.class, 
-                               ""acceptTrainingSet""),
-        new EventSetDescriptor(TestSetProducer.class, 
-                               ""testSet"", 
-                               TestSetListener.class, 
-                               ""acceptTestSet"")  };
+      EventSetDescriptor[] esds = {
+        new EventSetDescriptor(DataSource.class, ""dataSet"",
+          DataSourceListener.class, ""acceptDataSet""),
+        new EventSetDescriptor(DataSource.class, ""instance"",
+          InstanceListener.class, ""acceptInstance""), };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to event types produced by an `Appender`, including changes in how those event types are specified. The code diff shows a change in the `getEventSetDescriptors` method, which alters the array of `EventSetDescriptor` objects returned by the method. The modified code removes `EventSetDescriptor` entries related to `TrainingSetProducer` and `TestSetProducer`, potentially implying that these events should no longer be generated in the context where `getEventSetDescriptors` is used.

The reduction in event types aligns with the commit message's indication of fixing a bug related to the specification of event types. This suggests that there was a bug leading to the incorrect inclusion of those event types, and removing them fixes that issue. The try-catch block remains the same, implying no changes in exception handling this time.

Although the change also includes a minor formatting update by removing unnecessary spaces and improving readability, its more significant impact is the modification of the set of event descriptors, aligning with the commit message. This is directly related to the bug fix mentioned.

Therefore, the changes indicate a fix in event specification as described in the commit message. Concluding based on this analysis, the answer is **Buggy**."
weka,26539.json,88c70a2f184a0b7b2d27a48ebc04083d0d800049,"@@ -1,3 +1,6 @@
   public static List<String> getAllMetricNames() {
-    return Evaluation.getAllEvaluationMetricNames();
+    List<String> metrics = getBuiltInMetricNames();
+    metrics.addAll(getPluginMetricNames());
+
+    return metrics;
   }",Buggy,"Fixed a bug in the getAllMetricNames() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10919 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that there is a bug fix in the `getAllMetricNames()` method. Analyzing the code changes:

1. The original method directly returned the output of `Evaluation.getAllEvaluationMetricNames()`.

2. The new version of the method separates the metric names retrieval into two parts: `getBuiltInMetricNames()` and `getPluginMetricNames()`. It combines these two lists into a single list called `metrics`.

3. The combination of built-in and plugin metric names likely addresses the bug wherein the method previously might not have been returning all expected metric names, especially those added by plugins.

The change reflects a logical correction to ensure that all relevant metric names are accounted for by aggregating them from multiple sources. This aligns with addressing a bug as described in the commit message.

Thus, the nature of the change and its alignment with the commit message indicate that it is a bug fix. The conclusion is **Buggy**."
weka,20282.json,289721c0795b80e42b2664ffaa47dfab08f7ceac,"@@ -1,6 +1,7 @@
   private String statusMessagePrefix() {
     return getCustomName() + ""$"" + hashCode() + ""|""
-    + ((m_Filter instanceof OptionHandler) 
+    + ((m_Filter instanceof OptionHandler &&
+        Utils.joinOptions(((OptionHandler)m_Filter).getOptions()).length() > 0) 
         ? Utils.joinOptions(((OptionHandler)m_Filter).getOptions()) + ""|""
             : """");
   }",Buggy,"Fixed a minor status logging bug.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4797 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a ""minor status logging bug"" fix. The diff shows the addition of a condition to check if the string returned by `Utils.joinOptions()` is non-empty before appending it to the status message. 

Here's a step-by-step analysis:

1. **Original Code Functionality:**
   - Appends options from `m_Filter` if it implements `OptionHandler`, but does not check whether the options are non-empty.

2. **Modified Code Functionality:**
   - Adds a condition to check that the options string has a length greater than 0 before appending it to the status message.
   - Ensures that a separator might not be added unnecessarily if there are no actual options, improving the correctness of the status message.

3. **Commit Message Consistency:**
   - The commit message describes fixing a minor logging bug. This code change directly addresses this by ensuring the logging output is correct even when the options are empty.

This update improves the logic of appending to the status message, correcting potentially erroneous output in logs when `Utils.joinOptions(...)` returns an empty string. Thus, the change aligns well with the commit message, indicating a bug fix in the logging mechanism.

The changes are consistent with a bug fix, so the answer is **Buggy**."
weka,19858.json,06022e54131102f3817c490551c2babc818ef2e2,"@@ -1,47 +1,49 @@
   protected void newFlow() {
     m_newFlowBut.setEnabled(false);
 
     String user = m_viewer.getUser();
     String password = m_viewer.getPassword();
     String uRL = m_viewer.getURL();
     String query = m_viewer.getQuery();
 
     if (query == null) {
       query = """";
     }
 
     try {
       DatabaseLoader dbl = new DatabaseLoader();
       dbl.setUser(user);
       dbl.setPassword(password);
       dbl.setUrl(uRL);
       dbl.setQuery(query);
 
       BeanContextSupport bc = new BeanContextSupport();
       bc.setDesignTime(true);
 
       Loader loaderComp = new Loader();
       bc.add(loaderComp);
       loaderComp.setLoader(dbl);
 
       KnowledgeFlowApp singleton = KnowledgeFlowApp.getSingleton();
       m_mainPerspective.addTab(""DBSource"");
-      /*
-       * BeanInstance beanI = new
-       * BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
-       * .getNumTabs() - 1), loaderComp, 50, 50, m_mainPerspective.getNumTabs()
-       * - 1);
-       */
-      Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
-        .getNumTabs() - 1);
-      Vector<BeanConnection> connections = BeanConnection
-        .getConnections(m_mainPerspective.getNumTabs() - 1);
-      singleton.integrateFlow(beans, connections, true, false);
+
+      // The process of creating a BeanInstance integrates will result
+      // in it integrating itself into the flow in the specified tab
+      new BeanInstance(m_mainPerspective.getBeanLayout(m_mainPerspective
+        .getNumTabs() - 1), loaderComp, 50, 50,
+        m_mainPerspective.getNumTabs()
+        - 1);
+
+      // Vector<Object> beans = BeanInstance.getBeanInstances(m_mainPerspective
+      // .getNumTabs() - 1);
+      // Vector<BeanConnection> connections = BeanConnection
+      // .getConnections(m_mainPerspective.getNumTabs() - 1);
+      // singleton.integrateFlow(beans, connections, true, false);
       singleton.setActivePerspective(0); // switch back to the main perspective
 
       m_newFlowBut.setEnabled(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a bug where the code that creates the DatabaseLoader component when the new flow button is pressed got accidently commented out.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11286 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix regarding accidental comment-out of code responsible for creating the `DatabaseLoader` component when a new flow is initiated. The diff clearly reflects the changes mentioned in the commit message:

1. The code block responsible for creating a `BeanInstance` was previously commented out, which resulted in the intended functionality being disabled.
   
2. The diff shows that the code for creating the `BeanInstance` is uncommented, thus restoring the functionality.

3. The additional comment clarifies that creating a `BeanInstance` integrates it into the flow, implying that the previous manual integration via `integrateFlow()` is no longer necessary and is commented out.

This change directly resolves the issue described in the commit message by reinstating the functionality related to new flow creation, which had been accidentally disabled.

Thus, the diff aligns with the commit message and indicates a bug fix. Therefore, the conclusion is **Buggy**."
weka,5643.json,a6289b3f0cf2a2301bce6c1a9f33e7f522996b43,"@@ -1,24 +1,28 @@
   protected Instance mergeInstances(Instance source, Instance dest) {
 
     Instances outputFormat = outputFormatPeek();
     double[] vals = new double[outputFormat.numAttributes()];
     for(int i = 0; i < vals.length; i++) {
       if ((i != outputFormat.classIndex()) && (m_SelectedCols.isInRange(i))) {
         if (source != null) {
           vals[i] = source.value(i);
         } else {
           vals[i] = Utils.missingValue();
         }
       } else {
         vals[i] = dest.value(i);
       }
     }
     Instance inst = null;
     if (dest instanceof SparseInstance) {
       inst = new SparseInstance(dest.weight(), vals);
     } else {
       inst = new DenseInstance(dest.weight(), vals);
     }
-    inst.setDataset(dest.dataset());
+    // inst.setDataset(dest.dataset());
+    // push() sets the dataset to the output format, however, if
+    // a preview transformation is being done then push() does not
+    // get called, so set the output format correctly here.
+    inst.setDataset(outputFormat);
     return inst;
   }",Buggy,"Fixed a bug that affected the structure (dataset) assigned to a transformed instance when doing a preview transformation

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13412 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix related to the structure (dataset) of a transformed instance during a preview transformation. The code changes reflect this by modifying how the dataset is set on the newly created instance `inst`. 

Originally, `inst.setDataset(dest.dataset())` assigns the dataset of the destination instance to `inst`. The change comments out this line and replaces it with `inst.setDataset(outputFormat)`. The code comments indicate that when a preview transformation occurs, `push()` is not called, which normally sets the dataset to the output format. The new code compensates for this by directly setting the output format as the dataset, ensuring `inst` has the correct structure during a preview transformation.

The adjustments in the code address a structural issue during preview transformations, aligning well with the bug description in the commit message. Thus, the code changes are consistent with the claimed bug fix.

Conclusively, the changes in the commit can be classified as **Buggy**."
weka,31430.json,ac9fe6291a2243cdf039b445dd9190343c4c6d51,"@@ -1,18 +1,13 @@
   public final Matrix transpose() {
 
     int nr = m_Elements.length, nc = m_Elements[0].length;
-    Matrix b;
-    try {
-      b = (Matrix)clone();
-    } catch (CloneNotSupportedException ex) {
-      b = new Matrix(nr, nc);
-    }
+    Matrix b = new Matrix(nc, nr);
 
     for(int i = 0;i < nc; i++) {
       for(int j = 0; j < nr; j++) {
 	b.m_Elements[i][j] = m_Elements[j][i];
       }
     }
 
     return b;
   }",Buggy,"Fixed the matrix transpose problem introduced with last checkin.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@773 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a matrix transpose problem. The changes in the diff show that the clone method, previously used to create the transposed matrix, is replaced with a new Matrix instantiation with swapped dimensions (nc and nr). The old approach might have incorrectly initialized the matrix dimensions, leading to transpose errors due to incorrect sizes. 

Additionally, reliance on cloning could lead to complications, especially if the clone method is not working as intended, resulting in issues with matrix transposition. By directly creating a Matrix with the correct dimensions, the changes ensure that the transpose operation functions correctly, eliminating dependency on potentially flawed cloning logic.

Thus, these changes match the commit message, addressing the matrix transpose problem, indicating a clear **Buggy** fix."
weka,30779.json,a24ded419dad69fad9e5bdfca523043e633de4a1,"@@ -1,32 +1,34 @@
     public Instance nextElement(Instances dataset) {
       Instance	result;
       
       result = null;
       
       if (isIncremental()) {
 	// is there still an instance in the buffer?
 	if (m_IncrementalBuffer != null) {
 	  result              = m_IncrementalBuffer;
 	  m_IncrementalBuffer = null;
 	}
 	else {
 	  try {
 	    result = m_Loader.getNextInstance(dataset);
 	  }
 	  catch (Exception e) {
 	    e.printStackTrace();
 	    result = null;
 	  }
 	}
       }
       else {
 	if (m_BatchCounter < m_BatchBuffer.numInstances()) {
 	  result = m_BatchBuffer.instance(m_BatchCounter);
 	  m_BatchCounter++;
 	}
       }
 
-      result.setDataset(dataset);
+      if (result != null) {
+        result.setDataset(dataset);
+      }
       
       return result;
     }",Buggy,"Fixed a bug where a null pointer could get dereferenced in the nextElement() method of DataSource.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6417 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions fixing a bug in the nextElement() method where a null pointer could be dereferenced.

In the original code, `result.setDataset(dataset);` is called without checking if `result` is `null`. If `result` happens to be `null`, this line would lead to a `NullPointerException`. 

The change wraps this call with a null check: `if (result != null) { result.setDataset(dataset); }`. This ensures that `setDataset` is only called if `result` is not `null`, which prevents the potential `NullPointerException` mentioned in the commit message.

This change directly addresses the bug described in the commit message by adding a null check before dereferencing `result`.

Therefore, the changes in the code are indeed related to a bug fix, as described in the commit message. The conclusion is **Buggy**."
weka,14864.json,8ed966e4e1e65cf3ca1691aac3767c74ad03ae50,"@@ -1,214 +1,211 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
-    }
-    else {
+    } else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
-    }
-    else {
+    } else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
-    lookup.put(hashC, """");
+    lookup.put(hashC, new Double(best_merit));
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
-	  done = 2;
-	  sd = SELECTION_FORWARD;
-	}
-      else {
+        done = 2;
+        sd = SELECTION_FORWARD;
+      } else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
-	  }
-	  else {
+	  } else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
-
+          
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
 	      size++;
-	    }
-	    else {
+	    } else {
 	      temp_group.clear(i);
 	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
+	    
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
-
-	      if (m_debug) {
-		System.out.print(""Group: "");
-		printGroup(tt, m_numAttribs);
-		System.out.println(""Merit: "" + merit);
-	      }
-
-	      // is this better than the best?
-	      if (sd == SELECTION_FORWARD) {
-		z = ((merit - best_merit) > 0.00001);
-	      }
-	      else {
-		if (merit == best_merit) {
-		  z = (size < best_size);
-		} else {
-		  z = (merit >  best_merit);
-		} 
-	      }
-
-	      if (z) {
-		added = true;
-		stale = 0;
-		best_merit = merit;
-		//		best_size = (size + best_size);
-		best_size = size;
-		best_group = (BitSet)(temp_group.clone());
-	      }
-
+	      
+	      // insert this one in the hashtable
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
-	      // insert this one in the list and in the hash table
-	      Object [] add = new Object[1];
-	      add[0] = tt.clone();
-	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
-	      lookup.put(hashC, """");
-	      insertCount++;
+    	      lookup.put(hashC, new Double(merit));
+    	      insertCount++;
 	    } else {
-	      cacheHits++;
+	      merit = ((Double)lookup.get(hashC)).doubleValue();
+	      cacheHits++;  
+	    }
+	    
+	    // insert this one in the list
+	    Object[] add = new Object[1];
+	    add[0] = tt.clone();
+	    bfList.addToList(add, merit);
+	    
+	    if (m_debug) {
+	      System.out.print(""Group: "");
+	      printGroup(tt, m_numAttribs);
+	      System.out.println(""Merit: "" + merit);
+	    }
+
+	    // is this better than the best?
+	    if (sd == SELECTION_FORWARD) {
+	      z = ((merit - best_merit) > 0.00001);
+	    } else {
+	      if (merit == best_merit) {
+		z = (size < best_size);
+	      } else {
+		z = (merit >  best_merit);
+	      } 
+	    }
+
+	    if (z) {
+	      added = true;
+	      stale = 0;
+	      best_merit = merit;
+	      //		best_size = (size + best_size);
+	      best_size = size;
+	      best_group = (BitSet)(temp_group.clone());
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
 	      size--;
-	    }
-	    else {
+	    } else {
 	      temp_group.set(i);
 	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Added Martin Guetlein's bug fix for merit caching


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4162 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the changes are adding a bug fix related to ""merit caching."" Let's analyze the code changes to understand if this aligns with the message.

1. **Hashtable Usage:** 
   - The updated code changes the handling of the `lookup` hashtable by storing `Double` merit values instead of empty strings. Initially, the code was using `lookup.put(hashC, """");`, but now it uses `lookup.put(hashC, new Double(merit));`.
   - Additionally, when a subset is found in the cache, the merit is retrieved from the map, which wasn't happening before. The change includes `merit = ((Double)lookup.get(hashC)).doubleValue();` on cache hits.

2. **Insertion Logic:**
   - The logic to insert into the list (`bfList`) and the hash table has been modified. The sequence was also restructured slightly for clarity and optimization.

3. **Debug Information:**
   - There is an update to log debug information about the group being processed, which was previously not output for certain conditions.
   
4. **Evaluation and Best Criteria:**
   - The criteria for deciding if the current subset of attributes is better than the existing best one remains largely unchanged, but insertion and evaluation logic is slightly restructured, potentially impacting decision paths if caching altered behavior.

The commit effectively addresses merit caching as part of the search mechanism in the `search` method, storing correct values in the cache for retrieval on subsequent evaluations. Such a change could indeed fix bugs related to performance and accuracy in previous logic where merits were not used properly from the cache. These adjustments align with fixing an implied fault in caching behavior impacting functionality.

Thus, the changes align with the commit message, indicating it is indeed aimed at rectifying a pre-existing flaw related to merit caching.

**Conclusion: Buggy**"
weka,14864.json,89300e74cf24e09a676e8b2c132e4381f57905ac,"@@ -1,208 +1,216 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     m_totalEvals = 0;
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable(m_cacheSize * m_numAttribs);
     int insertCount = 0;
     int cacheHits = 0;
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     m_startRange.setUpper(m_numAttribs-1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {
       if (m_searchDirection == SELECTION_BACKWARD) {
 	setStartSet(""1-last"");
 	m_starting = new int[m_numAttribs];
 
 	// init initial subset to all attributes
 	for (i = 0, j = 0; i < m_numAttribs; i++) {
 	  if (i != m_classIndex) {
 	    best_group.set(i);
 	    m_starting[j++] = i;
 	  }
 	}
 
 	best_size = m_numAttribs - 1;
 	m_totalEvals++;
       }
     }
 
     // evaluate the initial subset
     best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     Object [] best = new Object[1];
     best[0] = best_group.clone();
     bfList.addToList(best, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     String hashC = tt.toString();
     lookup.put(hashC, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == SELECTION_BIDIRECTIONAL) {
 	// bi-directional search
 	  done = 2;
 	  sd = SELECTION_FORWARD;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getData()[0]);
       temp_group = (BitSet)temp_group.clone();
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == SELECTION_FORWARD) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.set(i);
+	      size++;
 	    }
 	    else {
 	      temp_group.clear(i);
+	      size--;
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 	    hashC = tt.toString();
 	    if (lookup.containsKey(hashC) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == SELECTION_FORWARD) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
-		z = ((merit >= best_merit) && ((size) < best_size));
+		if (merit == best_merit) {
+		  z = (size < best_size);
+		} else {
+		  z = (merit >  best_merit);
+		} 
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		//		best_size = (size + best_size);
 		best_size = size;
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      if (insertCount > m_cacheSize * m_numAttribs) {
 		lookup = new Hashtable(m_cacheSize * m_numAttribs);
 		insertCount = 0;
 	      }
 	      // insert this one in the list and in the hash table
 	      Object [] add = new Object[1];
 	      add[0] = tt.clone();
 	      bfList.addToList(add, merit);
 	      hashC = tt.toString();
 	      lookup.put(hashC, """");
 	      insertCount++;
 	    } else {
 	      cacheHits++;
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == SELECTION_FORWARD) {
 	      temp_group.clear(i);
+	      size--;
 	    }
 	    else {
 	      temp_group.set(i);
+	      size++;
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = SELECTION_BACKWARD;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed bug in backward mode


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2250 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix in the ""backward mode"" of some functionality. The diff adjusts logic affecting this mode, crucially in the block that compares merit values during backward selection:

Originally:
```java
z = ((merit >= best_merit) && ((size) < best_size));
```

Updated to:
```java
if (merit == best_merit) {
  z = (size < best_size);
} else {
  z = (merit > best_merit);
}
```

This change refines conditions for updating 'best', ensuring more precise selection when merits are equal. It also properly adjusts `size` after adding/removing elements in selected attributes.

The changes align with the commit message, indicating logical corrections in handling backward selection, suggesting a bug fix. Therefore, the classification is **Buggy**."
weka,14864.json,316eaac24eb968e7441b31b4e54c4ba10926a75a,"@@ -1,191 +1,189 @@
   public int[] search (int[] startSet, ASEvaluation ASEval, Instances data)
     throws Exception
   {
     if (!(ASEval instanceof SubsetEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a "" 
 			   + ""Subset evaluator!"");
     }
 
     if (startSet != null) {
       m_starting = startSet;
     }
 
     if (ASEval instanceof UnsupervisedSubsetEvaluator) {
       m_hasClass = false;
     }
     else {
       m_hasClass = true;
       m_classIndex = data.classIndex();
     }
 
     SubsetEvaluator ASEvaluator = (SubsetEvaluator)ASEval;
     m_numAttribs = data.numAttributes();
     int i, j;
     int best_size = 0;
     int size = 0;
     int done;
     int sd = m_searchDirection;
     int evals = 0;
     BitSet best_group, temp_group;
     int stale;
     double best_merit;
     boolean ok = true;
     double merit;
     boolean z;
     boolean added;
     Link2 tl;
     Hashtable lookup = new Hashtable((int)(200.0*m_numAttribs*1.5));
     LinkedList2 bfList = new LinkedList2(m_maxStale);
     best_merit = -Double.MAX_VALUE;
     stale = 0;
     best_group = new BitSet(m_numAttribs);
 
     // If a starting subset has been supplied, then initialise the bitset
     if (m_starting != null) {
       for (i = 0; i < m_starting.length; i++) {
 	if ((m_starting[i]) != m_classIndex) {
 	  best_group.set(m_starting[i]);
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_starting.length;
       m_totalEvals++;
     }
     else {if (m_searchDirection == -1) {
       m_starting = new int[m_numAttribs];
 
       // init initial subset to all attributes
       for (i = 0, j = 0; i < m_numAttribs; i++) {
 	if (i != m_classIndex) {
 	  best_group.set(i);
 	  m_starting[j++] = i;
 	}
       }
 
-      // evaluate the initial set
-      best_merit = ASEvaluator.evaluateSubset(best_group);
       best_size = m_numAttribs - 1;
       m_totalEvals++;
     }
     }
 
+    // evaluate the initial subset
+    best_merit = ASEvaluator.evaluateSubset(best_group);
     // add the initial group to the list and the hash table
     bfList.addToList(best_group, best_merit);
     BitSet tt = (BitSet)best_group.clone();
     lookup.put(tt, """");
 
     while (stale < m_maxStale) {
       added = false;
 
       if (m_searchDirection == 0) // bi-directional search
 	{
 	  done = 2;
 	  sd = 1;
 	}
       else {
 	done = 1;
       }
 
       // finished search?
       if (bfList.size() == 0) {
 	stale = m_maxStale;
 	break;
       }
 
       // copy the attribute set at the head of the list
       tl = bfList.getLinkAt(0);
       temp_group = (BitSet)(tl.getGroup().clone());
       // remove the head of the list
       bfList.removeLinkAt(0);
       // count the number of bits set (attributes)
       int kk;
 
       for (kk = 0, size = 0; kk < m_numAttribs; kk++) {
 	if (temp_group.get(kk)) {
 	  size++;
 	}
       }
 
       do {
 	for (i = 0; i < m_numAttribs; i++) {
 	  if (sd == 1) {
 	    z = ((i != m_classIndex) && (!temp_group.get(i)));
 	  }
 	  else {
 	    z = ((i != m_classIndex) && (temp_group.get(i)));
 	  }
 
 	  if (z) {
 	    // set the bit (attribute to add/delete)
 	    if (sd == 1) {
 	      temp_group.set(i);
 	    }
 	    else {
 	      temp_group.clear(i);
 	    }
 
 	    /* if this subset has been seen before, then it is already 
 	       in the list (or has been fully expanded) */
 	    tt = (BitSet)temp_group.clone();
 
 	    if (lookup.containsKey(tt) == false) {
 	      merit = ASEvaluator.evaluateSubset(temp_group);
 	      m_totalEvals++;
 
 	      if (m_debug) {
 		System.out.print(""Group: "");
 		printGroup(tt, m_numAttribs);
 		System.out.println(""Merit: "" + merit);
 	      }
 
 	      // is this better than the best?
 	      if (sd == 1) {
 		z = ((merit - best_merit) > 0.00001);
 	      }
 	      else {
 		z = ((merit >= best_merit) && ((size + sd) < best_size));
 	      }
 
 	      if (z) {
 		added = true;
 		stale = 0;
 		best_merit = merit;
 		best_size = (size + best_size);
 		best_group = (BitSet)(temp_group.clone());
 	      }
 
 	      // insert this one in the list and in the hash table
 	      bfList.addToList(tt, merit);
 	      lookup.put(tt, """");
 	    }
 
 	    // unset this addition(deletion)
 	    if (sd == 1) {
 	      temp_group.clear(i);
 	    }
 	    else {
 	      temp_group.set(i);
 	    }
 	  }
 	}
 
 	if (done == 2) {
 	  sd = -1;
 	}
 
 	done--;
       } while (done > 0);
 
       /* if we haven't added a new attribute subset then full expansion 
 	 of this node hasen't resulted in anything better */
       if (!added) {
 	stale++;
       }
     }
 
     m_bestMerit = best_merit;
     return  attributeList(best_group);
   }",Buggy,"Fixed small bug in evaluation of an empty subset.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@169 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a small bug is fixed related to evaluating an empty subset. Let's analyze the changes:

1. **Removal of Duplicate Evaluation**: 
   - **Removed Evaluation**: The lines removing initial evaluation in two branches (depending on whether a starting subset is provided) are commented out:
     ```java
     - if (m_starting != null) {
     -    ...
     -    best_merit = ASEvaluator.evaluateSubset(best_group);
     - }
     - else {
     -    if (m_searchDirection == -1) {
     -        ...
     -        best_merit = ASEvaluator.evaluateSubset(best_group);
     -    }
     - }
     ```
   - **Moved Evaluation**: One line is added after these branches:
     ```java
     +    // evaluate the initial subset
     +    best_merit = ASEvaluator.evaluateSubset(best_group);
     ```
   - This change consolidates the initial evaluation into a single step, which applies regardless of how the starting subset is determined.

2. **Impact of the Change**: 
   - The change ensures that the initial subset evaluation always occurs once, regardless of the starting conditions. Previously, if these branches did not execute (e.g., a starting subset is null and search direction isn't -1), evaluation wouldn't occur.
   - The potential bug is likely related to an initial condition where the subset evaluation might have been missed, particularly in cases where both conditions inside the `else` structure are false, leaving `best_merit` undefined for subsequent logic.

The commit's message aligns with the code change as the modification corrects an oversight in the logic regarding initial subset evaluation, which can be particularly crucial when dealing with an empty or null starting subset. This change effectively ensures the evaluated starting point is always defined which is pertinent to bug fixing.

Based on this analysis, the commit does indeed appear to be addressing a bug as described concerning the evaluation logic. Thus, it should be marked as **Buggy**."
weka,20981.json,6c77ccd674985be37160a0788865e371bf1cb5e1,"@@ -1,49 +1,53 @@
   private void saveLayout(int tabIndex, boolean showDialog) {
     // m_loadB.setEnabled(false);
     // m_saveB.setEnabled(false);
     java.awt.Color bckC = getBackground();
 
     File sFile = m_mainKFPerspective.getFlowFile(tabIndex);
     int returnVal = JFileChooser.APPROVE_OPTION;
+    boolean shownDialog = false;
 
     if (showDialog || sFile.getName().equals(""-NONE-"")) {
       returnVal = m_FileChooser.showSaveDialog(this);
+      shownDialog = true;
     }
 
     if (returnVal == JFileChooser.APPROVE_OPTION) {
       // temporarily remove this panel as a property changle listener from
       // each bean
 
       Vector beans = BeanInstance.getBeanInstances(tabIndex);
       detachFromLayout(beans);
 
-      // determine filename
-      sFile = m_FileChooser.getSelectedFile();
+      // determine filename (if necessary)
+      if (shownDialog) {
+        sFile = m_FileChooser.getSelectedFile();
+      }
 
       // add extension if necessary
       if (m_FileChooser.getFileFilter() == m_KfFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION)) {
           sFile = new File(sFile.getParent(), sFile.getName() + FILE_EXTENSION);
         }
       } else if (m_FileChooser.getFileFilter() == m_KOMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(KOML.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + KOML.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XStreamFilter) {
         if (!sFile.getName().toLowerCase()
             .endsWith(XStream.FILE_EXTENSION + ""kf"")) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + XStream.FILE_EXTENSION + ""kf"");
         }
       } else if (m_FileChooser.getFileFilter() == m_XMLFilter) {
         if (!sFile.getName().toLowerCase().endsWith(FILE_EXTENSION_XML)) {
           sFile = new File(sFile.getParent(), sFile.getName()
               + FILE_EXTENSION_XML);
         }
       }
 
       saveLayout(sFile, m_mainKFPerspective.getCurrentTabIndex(), false);
       m_mainKFPerspective.setFlowFile(tabIndex, sFile);
     }
   }",Buggy,"Fixed a bug where a tab would get saved with the filename associated with another tab

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9860 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes a bug fix related to a tab's filename being incorrectly set. The code changes confirm this fix by introducing the `shownDialog` boolean. This ensures the filename is only updated if the Save Dialog was shown (`shownDialog` is true), thereby preventing an incorrect file association when the dialog is not displayed and avoiding overwriting the wrong tab's file reference.

- The original code always updated the filename from the file chooser, leading to potential overwrites.
- The change ensures the filename is only updated when necessary, matching the commit's description.

This appropriately addresses the issue described, indicating a bug fix. The conclusion is **Buggy**."
weka,37320.json,24265153c621e05ff04b8fd1e06a4dfd6a63950a,"@@ -1,12 +1,13 @@
   public void addStepOutputListener(StepOutputListener listener,
     String outputConnectionName) {
     List<StepOutputListener> listenersForConnectionType =
       m_outputListeners.get(outputConnectionName);
     if (listenersForConnectionType == null) {
       listenersForConnectionType = new ArrayList<StepOutputListener>();
+      m_outputListeners.put(outputConnectionName, listenersForConnectionType);
     }
 
     if (!listenersForConnectionType.contains(listener)) {
       listenersForConnectionType.add(listener);
     }
   }",Buggy,"Fixed a bug in the step output listener framework

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12498 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code modifies the method `addStepOutputListener`. It ensures that a new list of `StepOutputListener` objects is properly associated with the `outputConnectionName` in the `m_outputListeners` map when no list is present initially. Previously, the new list was created but not added to the map, meaning listeners wouldn't actually be registered under their `outputConnectionName` if no previous listeners existed for that name. This omission would prevent the intended functionality of registering listeners, which is indeed a bug.

The commit message, ""Fixed a bug in the step output listener framework,"" matches the change in the code, which corrects this mishandling of listener registration.

Thus, the code change aligns with the commit message and represents a bug fix. The conclusion is **Buggy**."
weka,21662.json,ac1cf6e094d4d41b6cff6e286f9abb0cdd58e95b,"@@ -1,9 +1,9 @@
   public void addObject(String name, Object o) {
     String nameCopy = name;
     int i = 0;
-    while (m_Results.containsKey(nameCopy)) {
+    while (m_Objs.containsKey(nameCopy)) {
       nameCopy = name + ""_"" + i++;
     }
 
     m_Objs.put(nameCopy, o);
   }",Buggy,"Fixed bug, introduced in the last change, that affected accessing of objects stored against results

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13734 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change modifies the loop condition from checking `m_Results.containsKey(nameCopy)` to `m_Objs.containsKey(nameCopy)`. The commit message mentions a bug affecting object access linked to results, and this change ensures names for objects don't duplicate keys in `m_Objs`, which is crucial for correct storage and retrieval. The previous logic checking `m_Results` may have unintentionally failed to prevent key conflicts in `m_Objs`, leading to potential overwrites or errors when accessing objects. This update correctly fixes that oversight, aligning with the commit message that describes fixing an error in accessing stored objects.

Thus, this commit indicates a bug fix. The answer is **Buggy**."
weka,32460.json,c3cf13ff2162159996a765cb4fcfb11c86359d6e,"@@ -1,23 +1,23 @@
   private static void initStemmers() {
     Vector<String> classnames;
     int i;
 
     if (m_Stemmers != null) {
       return;
     }
 
     m_Stemmers = new Vector<String>();
 
     if (!m_Present) {
       return;
     }
 
     classnames = GenericObjectEditor.getClassnames(SNOWBALL_PROGRAM);
     // try dynamic discovery if not in props file
     if (classnames.size() == 0) {
       classnames = ClassDiscovery.find(SNOWBALL_PROGRAM, PACKAGE_EXT);
-      for (i = 0; i < classnames.size(); i++) {
-        m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
-      }
+    }
+    for (i = 0; i < classnames.size(); i++) {
+      m_Stemmers.add(getStemmerName(classnames.get(i).toString()));
     }
   }",Buggy,"Fixed a bug that prevented snowball classes from being found when static entries in GenericObjectEditor.props are being used (instead of dynamic class discovery)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The diff shows a change in the initStemmers method logic. Previously, if no classes were found in GenericObjectEditor's static entries, dynamic class discovery was used, and resulting classes' names were added to m_Stemmers. The loop used to add names was inside the if block checking if classnames are zero-length, which is illogical because class discovery must have found some class names to loop through. The fix moves the loop outside this block to ensure stemmer names are added correctly, regardless of how classnames are populated.

The commit message indicates a bug related to static entries vs. dynamic discovery in class finding. The code change aligns with this by addressing incorrect conditional block placement, which hindered the addition of discovered class names.

Therefore, based on the commit message and the changes, this commit fixes a bug, thus the answer is **Buggy**."
weka,24188.json,639ea63bec73dc387b27664947c01effdfc97c4c,"@@ -1,153 +1,153 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             k;
     int             size;
     String          line;
     int             indexBase;
     int             indexSecond;
     StringBuffer    head;
     StringBuffer    body;
     StringBuffer    foot;
     int[]           startMeans;
     int[]           startSigs;
     int             maxLength;
 
     result     = new StringBuffer();
     head       = new StringBuffer();
     body       = new StringBuffer();
     foot       = new StringBuffer();
     cells      = toArray();
     startMeans = new int[getColCount()];
     startSigs  = new int[getColCount() - 1];
     maxLength  = 0;
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n, true, true);
       for (i = 1; i < cells.length - 1; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // index of base column in array
     indexBase = 1;
     if (getShowStdDev())
       indexBase++;
 
     // index of second column in array
     indexSecond = indexBase + 1;
     if (getShowStdDev())
       indexSecond++;
 
     // output data (without ""(v/ /*)"")
     j = 0;
     k = 0;
     for (i = 1; i < cells.length - 1; i++) {
       line = """";
       
       for (n = 0; n < cells[0].length; n++) {
         // record starts
         if (i == 1) {
           if (isMean(n)) {
             startMeans[j] = line.length();
             j++;
           }
 
           if (isSignificance(n)) {
             startSigs[k] = line.length();
             k++;
           }
         }
         
         if (n == 0) {
           line += padString(cells[i][n], getRowNameWidth());
-          line += padString(""("" + Utils.doubleToString(getCount(i-1), 0) + "")"", 
+          line += padString(""("" + Utils.doubleToString(getCount(getDisplayRow(i-1)), 0) + "")"", 
                         getCountWidth(), true);
         }
         else {
           // additional space before means
           if (isMean(n))
             line += ""  "";
 
           // print cell
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))              
                 line += ""("" + cells[i][n] + "")"";
               else
                 line += "" "" + cells[i][n] + "" "";
             }
             else
               line += "" "" + cells[i][n];
           }
           else {
             line += "" "" + cells[i][n];
           }
         }
 
         // add separator after base column
         if (n == indexBase)
           line += "" |"";
       }
 
       // record overall length
       if (i == 1)
         maxLength = line.length();
       
       body.append(line + ""\n"");
     }
 
     // column names
     line = padString(cells[0][0], startMeans[0]);
     i    = -1;
     for (n = 1; n < cells[0].length; n++) {
       if (isMean(n)) {
         i++;
 
         if (i == 0)
           line = padString(line, startMeans[i] - getCountWidth());
         else if (i == 1)
           line = padString(line, startMeans[i] - "" |"".length());
         else if (i > 1)
           line = padString(line, startMeans[i]);
         
         if (i == 1)
           line += "" |"";
         
         line += "" "" + cells[0][n];
       }
     }
     line = padString(line, maxLength);
     head.append(line + ""\n"");
     head.append(line.replaceAll(""."", ""-"") + ""\n"");
     body.append(line.replaceAll(""."", ""-"") + ""\n"");
 
     // output wins/losses/ties
     if (getColCount() > 1) {
       line = padString(cells[cells.length - 1][0], startMeans[1]-2, true) + "" |"";
       i    = 0;
       for (n = 1; n < cells[cells.length - 1].length; n++) {
         if (isSignificance(n)) {
           line = padString(
                   line, startSigs[i] + 1 - cells[cells.length - 1][n].length());
           line += "" "" + cells[cells.length - 1][n];
           i++;
         }
       }
       line = padString(line, maxLength);
     }
     else {
       line = padString(cells[cells.length - 1][0], line.length() - 2) + "" |"";
     }
     foot.append(line + ""\n"");
     
     // assemble output
     result.append(head.toString());
     result.append(body.toString());
     result.append(foot.toString());
 
     return result.toString();
   }",Buggy,"Fixed bug in ResultMatrixPlainText, which meant that displayed counts could be out of line with corresponding dataset and results. Changed TTesters to not throw an exception when missing values are encountered.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2816 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that it fixes a bug in `ResultMatrixPlainText` concerning displayed counts that could be misaligned with the corresponding dataset and results. The relevant change in the diff is:

```java
- line += padString(""("" + Utils.doubleToString(getCount(i-1), 0) + "")"", 
+ line += padString(""("" + Utils.doubleToString(getCount(getDisplayRow(i-1)), 0) + "")"", 
                         getCountWidth(), true);
```

The change modifies the call to `getCount` by introducing `getDisplayRow(i-1)`. This suggests that previously, the count was incorrectly calculated or displayed due to a direct use of `i-1`, which did not account for how rows were intended to be displayed. Using `getDisplayRow(i-1)` likely corrects alignment or indexing issues, ensuring that the counts reflect the correct dataset rows.

Given that this change directly addresses the count misalignment issue mentioned in the commit message, it indeed represents a bug fix. The commit message also notes another change related to `TTesters`, which is not shown in this diff, but the part of the change shown aligns with the bug fix description.

Thus, the changes indicate a bug fix as described in the commit message. Therefore, the assessment is **Buggy**."
weka,17500.json,d109992d4145875b41832b4a599e8cc7b7e5d4a2,"@@ -1,71 +1,74 @@
   protected void setNumeric() {
     m_isNumeric = true;
     /*      m_maxC = mxC;
 	    m_minC = mnC; */
 
     double min=Double.POSITIVE_INFINITY;
     double max=Double.NEGATIVE_INFINITY;
     double value;
 
     for (int i=0;i<m_Instances.numInstances();i++) {
       if (!m_Instances.instance(i).isMissing(m_cIndex)) {
 	value = m_Instances.instance(i).value(m_cIndex);
 	if (value < min) {
 	  min = value;
 	}
 	if (value > max) {
 	  max = value;
 	}
       }
     }
      
+    // handle case where all values are missing
+    if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
     m_minC = min; m_maxC = max;
 
     int whole = (int)Math.abs(m_maxC);
     double decimal = Math.abs(m_maxC) - whole;
     int nondecimal;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
     m_precisionC = (decimal > 0) 
       ? (int)Math.abs(((Math.log(Math.abs(m_maxC)) / 
 				      Math.log(10))))+2
       : 1;
     if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
       m_precisionC = 1;
     }
 
     String maxStringC = Utils.doubleToString(m_maxC,
 					     nondecimal+1+m_precisionC
 					     ,m_precisionC);
     if (m_labelMetrics != null) {
       m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
     }
 
     whole = (int)Math.abs(m_minC);
     decimal = Math.abs(m_minC) - whole;
     nondecimal = (whole > 0) 
       ? (int)(Math.log(whole) / Math.log(10))
       : 1;
     
      m_precisionC = (decimal > 0) 
        ? (int)Math.abs(((Math.log(Math.abs(m_minC)) / 
 				      Math.log(10))))+2
       : 1;
      if (m_precisionC > VisualizeUtils.MAX_PRECISION) {
        m_precisionC = 1;
      }
     
      maxStringC = Utils.doubleToString(m_minC,
 				       nondecimal+1+m_precisionC
 				       ,m_precisionC);
      if (m_labelMetrics != null) {
        if (m_labelMetrics.stringWidth(maxStringC) > m_HorizontalPad) {
 	 m_HorizontalPad = m_labelMetrics.stringWidth(maxStringC);
        }
      }
 
     setOn(true);
     this.repaint();
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1219 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug related to missing numeric values has been fixed. The code change introduces a check for cases where all the numeric values are missing (i.e., when the variable `min` is still `Double.POSITIVE_INFINITY` after looping through instances). In such cases, both `min` and `max` are set to `0.0`, which is likely a sensible default or neutral value. This ensures that `m_minC` and `m_maxC` are not left with inaccurate or undefined values, which would otherwise cause errors or incorrect behavior when hypotheses based on numeric values are applied.

This change directly addresses the potential error of having incorrect uninitialized values when all instances have missing values, aligning well with the bug fix described in the commit message. Therefore, this is a correction to the way the code handles a specific edge condition which indicates a bug fix.

The resolution of this problem related to managing missing numeric values aligns with the commit message.

The final assessment is **Buggy**."
weka,31181.json,feacc664ba9b84371e98b74802bc9f6a36edcff4,"@@ -1,19 +1,19 @@
   public String[] getOptions() {
 
     Vector<String> result = new Vector<String>();
 
     result.add(""-R"");
     result.add(getAttributeIndices());
 
     if (getInvertSelection()) {
       result.add(""-V"");
     }
 
-    result.add(""F"");
+    result.add(""-F"");
     result.add("""" + getFilterSpec());
 
-    result.add(""D"");
+    result.add(""-D"");
     result.add("""" + getDistanceSpec());
 
     return result.toArray(new String[result.size()]);
   }",Buggy,"Small bug fix in getOptions().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11186 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a ""Small bug fix in getOptions()"". The diff reveals changes in option formatting, adding hyphens before flags ""-F"" and ""-D"". Without hyphens, the options might be misinterpreted, causing incorrect behavior. Correcting option formatting aligns with the message of bug fixing. The conclusion is **Buggy**."
weka,36114.json,f20281f569296add23cdf098a2ef5d7c1c9101a3,"@@ -1,31 +1,31 @@
   public static boolean hasInterface(Class intf, Class cls) {
     Class[]       intfs;
     int           i;
     boolean       result;
     Class         currentclass;
     
     result       = false;
     currentclass = cls;
     do {
       // check all the interfaces, this class implements
       intfs = currentclass.getInterfaces();
       for (i = 0; i < intfs.length; i++) {
         if (intfs[i].equals(intf)) {
           result = true;
           break;
         }
       }
 
       // get parent class
       if (!result) {
         currentclass = currentclass.getSuperclass();
         
-        // topmost class reached?
-        if (currentclass.equals(Object.class))
+        // topmost class reached or no superclass?
+        if ( (currentclass == null) || (currentclass.equals(Object.class)) )
           break;
       }
     } 
     while (!result);
       
     return result;
   }",Buggy,"fixed bug with interfaces (return null as superclass)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2976 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code changes in this diff appear to correct a bug involving interface detection within a class hierarchy. The specific adjustment made was to check if `currentclass` becomes `null`, signifying the class has no superclass. Previously, the code only compared `currentclass` to `Object.class`, which possibly missed situations where a class directly implements interfaces but has no superclass other than `null`. This fix accurately prevents null-pointer exceptions when there is no superclass, thus more reliably determining interface implementation across potentially non-standard class hierarchies.

The commit message supports this reasoning by stating it ""fixed bug with interfaces (return null as superclass),"" indicating an improvement in handling class items with no superclass, consistent with the code changes.

Given this information, the modification aligns with the commit message and targets a logical correction around class hierarchy handling, which represents a bug fix.

The conclusion is **Buggy**."
weka,36431.json,a91eb3b6273ee2b25218e3fd6a1520a0f11b6e6e,"@@ -1,17 +1,18 @@
   public void stepInit() throws WekaException {
     m_isReset = true;
+    m_streamingData = null;
 
     // see if the specified downstream steps are connected
     m_validTrueStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfTrueStep)) != null;
     m_validFalseStep =
       getStepManager().getOutgoingConnectedStepWithName(
         environmentSubstitute(m_customNameOfFalseStep)) != null;
 
     m_incomingStructure = null;
 
     if (m_expressionString == null || m_expressionString.length() == 0) {
       throw new WekaException(""No expression defined!"");
     }
   }",Buggy,"Fixed a small bug in FlowByExpression - object holding streaming data output did not get set to null in reset() method

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13301 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that there was a bug in which an object holding streaming data was not set to null in the `reset()` method. The code change explicitly initializes `m_streamingData` to null in the `stepInit()` method. This change aligns with the intent stated in the commit message: to correctly reset the object's state by nullifying the streaming data to prevent unintended data carryover.

Setting `m_streamingData` to null as part of the initialization process ensures that any previous data held by `m_streamingData` is cleared when `stepInit()` is called, effectively fixing the bug mentioned.

The modification is a direct fix for the described issue, ensuring consistent and expected behavior upon resetting. Therefore, these changes are indicative of a bug fix.

The conclusion is **Buggy**."
weka,29966.json,a5e2d1ef0f62ebac46d095c840ea7ff4a9145b48,"@@ -1,172 +1,192 @@
   public static void main(String[] args) {
     try {
       if (args.length == 0 || args[0].equalsIgnoreCase(""-h"") ||
           args[0].equalsIgnoreCase(""-help"")) {
-        System.err.println(""Usage:\n\tweka.Run [-no-scan | -no-load] <scheme name [scheme options]>"");
+        System.err.println(""Usage:\n\tweka.Run [-no-scan] [-no-load] <scheme name [scheme options]>"");
         System.exit(1);
       }
+      boolean noScan = false;
+      boolean noLoad = false;
       if (args[0].equals(""-list-packages"")) {
         weka.core.WekaPackageManager.loadPackages(true);
         System.exit(0);
-      } else if (!args[0].equals(""-no-load"")) {
+      } else if (args[0].equals(""-no-load"")) {
+        noLoad = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-scan"")) {
+            noScan = true;
+          }
+        }
+      } else if (args[0].equals(""-no-scan"")) {
+        noScan = true;
+        if (args.length > 1) {
+          if (args[1].equals(""-no-load"")) {
+            noLoad = true;
+          }
+        }
+      }
+      
+      if (!noLoad) {
         weka.core.WekaPackageManager.loadPackages(false);
       }
       
+      int schemeIndex = 0;
+      if (noLoad && noScan) {
+        schemeIndex = 2;
+      } else if (noLoad || noScan) {
+        schemeIndex = 1;
+      }
+      
       String schemeToRun = null;
       String[] options = null;
-      if (args[0].equals(""-no-scan"") || args[0].equals(""-no-load"")) {
-        if (args.length < 2) {
-          System.err.println(""No scheme name given."");
-          System.exit(1);
-        }
-        schemeToRun = args[1];
-        options = new String[args.length - 2];
-        if (options.length > 0) {
-          System.arraycopy(args, 2, options, 0, options.length);
-        }
-      } else {
-        // scan packages for matches
-        schemeToRun = args[0];
-        options = new String[args.length - 1];
-        if (options.length > 0) {
-          System.arraycopy(args, 1, options, 0, options.length);
-        }
-
-        ArrayList<String> matches = weka.core.ClassDiscovery.find(args[0]);
+      
+      if (schemeIndex >= args.length) {
+        System.err.println(""No scheme name given."");
+        System.exit(1);
+      }
+      schemeToRun = args[schemeIndex];
+      options = new String[args.length - schemeIndex - 1];
+      if (options.length > 0) {
+        System.arraycopy(args, schemeIndex + 1, options, 0, options.length);
+      }
+      
+           
+      if (!noScan) {     
+        ArrayList<String> matches = weka.core.ClassDiscovery.find(schemeToRun);
         ArrayList<String> prunedMatches = new ArrayList<String>();
         // prune list for anything that isn't a runnable scheme      
         for (int i = 0; i < matches.size(); i++) {
           try {
             Object scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
                 matches.get(i));          
             if (scheme instanceof weka.classifiers.Classifier ||
                 scheme instanceof weka.clusterers.Clusterer ||
                 scheme instanceof weka.associations.Associator ||
                 scheme instanceof weka.attributeSelection.ASEvaluation ||
                 scheme instanceof weka.filters.Filter) {
               prunedMatches.add(matches.get(i));
             }
           } catch (Exception ex) {
             // ignore any classes that we can't instantiate due to no no-arg constructor
           }
         }
 
         if (prunedMatches.size() == 0) {
-          System.err.println(""Can't find scheme "" + args[0] + "", or it is not runnable."");
+          System.err.println(""Can't find scheme "" + schemeToRun + "", or it is not runnable."");
           System.exit(1);
         } else if (prunedMatches.size() > 1) {
           java.io.BufferedReader br = 
             new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
           boolean done = false;
           while (!done) {
             System.out.println(""Select a scheme to run, or <return> to exit:"");
             for (int i = 0; i < prunedMatches.size(); i++) {
               System.out.println(""\t"" + (i+1) + "") "" + prunedMatches.get(i));
             }
             System.out.print(""\nEnter a number > "");
             String choice = null;
             int schemeNumber = 0;
             try {
               choice = br.readLine();
               if (choice.equals("""")) {
                 System.exit(0);
               } else {
                 schemeNumber = Integer.parseInt(choice);
                 schemeNumber--;
                 if (schemeNumber >= 0 && schemeNumber < prunedMatches.size()) {
                   schemeToRun = prunedMatches.get(schemeNumber);
                   done = true;
                 }
               }
             } catch (java.io.IOException ex) {
               // ignore
             }
           }
         } else {
           schemeToRun = prunedMatches.get(0);
         }
       }
 
       Object scheme = null;
       try {
         scheme = java.beans.Beans.instantiate((new Run()).getClass().getClassLoader(),
             schemeToRun);
       } catch (Exception ex) {
         System.err.println(schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       // now see which interfaces/classes this scheme implements/extends
       ArrayList<SchemeType> types = new ArrayList<SchemeType>();      
       if (scheme instanceof weka.classifiers.Classifier) {
         types.add(SchemeType.CLASSIFIER);
       }
       if (scheme instanceof weka.clusterers.Clusterer) {
         types.add(SchemeType.CLUSTERER);
       }
       if (scheme instanceof weka.associations.Associator) {
         types.add(SchemeType.ASSOCIATOR);
       }
       if (scheme instanceof weka.attributeSelection.ASEvaluation) {
         types.add(SchemeType.ATTRIBUTE_SELECTION);
       }
       if (scheme instanceof weka.filters.Filter) {
         types.add(SchemeType.FILTER);
       }
       
       SchemeType selectedType = null;
       if (types.size() == 0) {
         System.err.println("""" + schemeToRun + "" is not runnable!"");
         System.exit(1);
       }
       if (types.size() == 1) {
         selectedType = types.get(0);
       } else {
         java.io.BufferedReader br = 
           new java.io.BufferedReader(new java.io.InputStreamReader(System.in));
         boolean done = false;
         while (!done) {
           System.out.println("""" + schemeToRun + "" can be executed as any of the following:"");
           for (int i = 0; i < types.size(); i++) {
             System.out.println(""\t"" + (i+1) + "") "" + types.get(i));
           }
           System.out.print(""\nEnter a number > "");
           String choice = null;
           int typeNumber = 0;
           try {
             choice = br.readLine();
             if (choice.equals("""")) {
               System.exit(0);
             } else {
               typeNumber = Integer.parseInt(choice);
               typeNumber--;
               if (typeNumber >= 0 && typeNumber < types.size()) {
                 selectedType = types.get(typeNumber);
                 done = true;
               }
             }
           } catch (java.io.IOException ex) {
             // ignore
           }
         }
       }
             
       if (selectedType == SchemeType.CLASSIFIER) {
         weka.classifiers.AbstractClassifier.runClassifier((weka.classifiers.Classifier)scheme, options);
       } else if (selectedType == SchemeType.CLUSTERER) {
         weka.clusterers.AbstractClusterer.runClusterer((weka.clusterers.Clusterer)scheme, options);
       } else if (selectedType == SchemeType.ATTRIBUTE_SELECTION) {
         weka.attributeSelection.ASEvaluation.runEvaluator((weka.attributeSelection.ASEvaluation)scheme, options);
       } else if (selectedType == SchemeType.ASSOCIATOR) {
         weka.associations.AbstractAssociator.runAssociator((weka.associations.Associator)scheme, options);
       } else if (selectedType == SchemeType.FILTER) {
         weka.filters.Filter.runFilter((weka.filters.Filter)scheme, options);
       }
     } 
     catch (Exception e) {
       if (    ((e.getMessage() != null) && (e.getMessage().indexOf(""General options"") == -1))
 	   || (e.getMessage() == null) )
 	e.printStackTrace();
       else
 	System.err.println(e.getMessage());
     }
   }",Buggy,"Fixed a bug that prevented the -no-scan and -no-load options to be used at the same time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6676 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the fix resolves an issue that prevented the use of `-no-scan` and `-no-load` options simultaneously. The code changes in the diff are consistent with this purpose and involve:

1. A modification to the usage message to reflect the ability to use both `-no-scan` and `-no-load` options together: 

```diff
- weka.Run [-no-scan | -no-load]
+ weka.Run [-no-scan] [-no-load]
```

2. Introduction of boolean variables `noScan` and `noLoad` to manage the state of these options:

```java
boolean noScan = false;
boolean noLoad = false;
```

3. Conditional checks now allow both options to be independently set based on the arguments, resolving the issue where previously only one of them could be set:

```java
if (args[0].equals(""-no-load"")) {
    noLoad = true;
    if (args.length > 1) {
        if (args[1].equals(""-no-scan"")) {
            noScan = true;
        }
    }
} else if (args[0].equals(""-no-scan"")) {
    noScan = true;
    if (args.length > 1) {
        if (args[1].equals(""-no-load"")) {
            noLoad = true;
        }
    }
}
```

4. Adjusted the logic to handle the arguments array accordingly to retrieve the scheme and options based on the presence or absence of `-no-scan` and `-no-load`.

5. Updated the logic to scan for packages only if `noScan` is `false`.

The changes effectively solve the problem indicated by the commit message: enabling the simultaneous use of both `-no-scan` and `-no-load` options.

Thus, the result of this investigation is **Buggy**, as the changes indicate a bug fix."
weka,6813.json,017098ae02f2d6bdf3fb5d25b0d3ccedd8c46b7e,"@@ -1,28 +1,35 @@
   protected static Vector instanceToVector(Instance toProcess, int classIndex) {
     if (toProcess instanceof SparseInstance) {
       int classModifier = classIndex >= 0 ? 1 : 0;
+      if (classModifier > 0) {
+        double classValue = toProcess.classValue();
+        if (classValue == 0) {
+          classModifier = 0; // class is sparse
+        }
+      }
       SparseInstance toProcessSparse = ((SparseInstance) toProcess);
       int[] indices = new int[toProcessSparse.numValues() - classModifier];
       double[] values = new double[toProcessSparse.numValues() - classModifier];
       int index = 0;
       for (int i = 0; i < toProcessSparse.numValues(); i++) {
         if (toProcessSparse.index(i) != classIndex) {
           indices[index] = toProcessSparse.index(i);
           values[index++] = toProcessSparse.valueSparse(i);
         }
       }
-      return Vectors.sparse(toProcess.numValues(), indices, values);
+      return Vectors.sparse(
+        toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0), indices, values);
     } else {
       if (classIndex < 0) {
         return Vectors.dense(toProcess.toDoubleArray());
       }
       double[] independent = new double[toProcess.numAttributes() - 1];
       int index = 0;
       for (int i = 0; i < toProcess.numAttributes(); i++) {
         if (i != classIndex) {
           independent[index++] = toProcess.value(i);
         }
       }
       return Vectors.dense(independent);
     }
   }",Buggy,"Fixed a bug in sparse instance to sparse Vector conversion.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14868 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the method converting a sparse instance to a sparse Vector. Here's how the changes in the diff align with the message and indicate a bug fix:

1. **Addition of Class Modifier Logic**:
   - The change introduces logic to adjust the `classModifier` based on whether the class value is zero, indicating that the class attribute is also sparse. This suggests that the previous implementation did not correctly handle cases where the class attribute of a sparse instance was zero, leading to potential errors or incorrect data handling.

2. **Adjustment of Vector Creation**:
   - The vector creation call is changed from `toProcess.numValues()` to `toProcess.numAttributes() - (classIndex >= 0 ? 1 : 0)`. This ensures that the full attribute count minus the class index (if applicable) is considered, which seems to fix an error where the wrong size was previously provided to the sparse vector creation method, especially in cases where the class index is excluded.

Both changes address specific issues that could lead to incorrect conversion from a sparse instance to a vector. These are indicative of bug-related changes:

- The handling of zero class values for sparse instances was previously incorrect, potentially leading to logical errors or incorrect vector conversion.
- The corrected size parameter for the sparse vector reflects a fix in how vectors are constructed from instances, ensuring accuracy in the resulting sparse vector representation.

Therefore, the changes in the code directly aim to fix incorrect behavior in the conversion process, aligning well with the commit message that refers explicitly to fixing a bug in the conversion mechanism. Thus, my assessment is **Buggy**."
weka,1604.json,a3134adec9386c387ea691ca59e46a35f35f858d,"@@ -1,175 +1,179 @@
   public void buildClassifier(Instances data) throws Exception {
     // can classifier handle the data?
     getCapabilities().testWithFail(data);
 
     m_errorsFromR = new ArrayList<String>();
 
     if (m_modelHash == null) {
       m_modelHash = """" + hashCode();
     }
 
     data = new Instances(data);
     data.deleteWithMissingClass();
 
     if (data.numInstances() == 0 || data.numAttributes() == 1) {
       if (data.numInstances() == 0) {
         System.err
           .println(""No instances with non-missing class - using ZeroR model"");
       } else {
         System.err.println(""Only the class attribute is present in ""
           + ""the data - using ZeroR model"");
       }
       m_zeroR = new ZeroR();
       m_zeroR.buildClassifier(data);
       return;
     }
 
     // remove useless attributes
     m_removeUseless = new RemoveUseless();
     m_removeUseless.setInputFormat(data);
     data = Filter.useFilter(data, m_removeUseless);
 
     if (!m_dontReplaceMissingValues) {
       m_missingFilter = new ReplaceMissingValues();
       m_missingFilter.setInputFormat(data);
       data = Filter.useFilter(data, m_missingFilter);
     }
 
     data = handleZeroFrequencyNominalValues(data);
 
     m_serializedModel = null;
 
     if (!m_initialized) {
       init();
 
       if (!m_mlrAvailable) {
         throw new Exception(
           ""MLR is not available for some reason - can't continue!"");
       }
     } else {
       // unload and then reload MLR to try and clear any errors/inconsistent
       // state
       reloadMLR();
     }
 
     m_baseLearnerLibraryAvailable = false;
     loadBaseLearnerLibrary();
     if (!m_baseLearnerLibraryAvailable) {
       throw new Exception(""Library ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getIDStr() + "" for learner ""
         + MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable()
         + "" is not available for some reason - can't continue!"");
     }
 
     RSession eng = null;
     eng = RSession.acquireSession(this);
     eng.setLog(this, m_logger);
     eng.clearConsoleBuffer(this);
 
+    // Change to a temporary directory where we have write access
+    // in case an mlr scheme tries to write a local file
+    eng.parseAndEval(this, ""setwd(tempdir())"");
+
     // Set seed for random number generator in R in a data-dependent manner
     eng.parseAndEval(this,
       ""set.seed("" + data.getRandomNumberGenerator(getSeed()).nextInt() + "")"");
 
     // clean up any previous model
     // suffix model identifier with hashcode of this object
     eng.parseAndEval(this, ""remove(weka_r_model"" + m_modelHash + "")"");
 
     // transfer training data into a data frame in R
     Object[] result = RUtils.instancesToDataFrame(eng, this, data, ""mlr_data"");
     m_cleanedAttNames = (String[])result[0];
     m_cleanedAttValues = (String[][])result[1];
 
     try {
       String mlrIdentifier =
         MLRClassifier.TAGS_LEARNER[m_rLearner].getReadable();
 
       if (data.classAttribute().isNumeric()
         && mlrIdentifier.startsWith(""classif"")) {
         throw new Exception(""Training instances has a numeric class but ""
           + ""selected R learner is a classifier!"");
       } else if (data.classAttribute().isNominal()
         && mlrIdentifier.startsWith(""regr"")) {
         throw new Exception(""Training instances has a nominal class but ""
           + ""selected R learner is a regressor!"");
       }
 
       m_errorsFromR.clear();
       // make classification/regression task
       String taskString = null;
       if (data.classAttribute().isNominal()) {
         taskString = ""task <- makeClassifTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       } else {
         taskString = ""task <- makeRegrTask(data = mlr_data, target = \""""
           + m_cleanedAttNames[data.classIndex()] + ""\"")"";
       }
 
       if (m_Debug) {
         System.err.println(""Prediction task: "" + taskString);
       }
 
       eng.parseAndEval(this, taskString);
       // eng.parseAndEval(this, ""print(task)"");
       checkForErrors();
 
       m_schemeProducesProbs = schemeProducesProbabilities(mlrIdentifier, eng);
 
       String probs =
         (data.classAttribute().isNominal() && m_schemeProducesProbs)
           ? "", predict.type = \""prob\"""" : """";
       String learnString = null;
       if (m_schemeOptions != null && m_schemeOptions.length() > 0) {
         learnString = ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs
           + "", "" + m_schemeOptions + "")"";
       } else {
         learnString =
           ""l <- makeLearner(\"""" + mlrIdentifier + ""\"""" + probs + "")"";
       }
 
       if (m_Debug) {
         System.err.println(""Make a learner object: "" + learnString);
       }
 
       eng.parseAndEval(this, learnString);
       checkForErrors();
 
       // eng.parseAndEval(this, ""print(l)"");
 
       // train model
       eng.parseAndEval(this,
         ""weka_r_model"" + m_modelHash + "" <- train(l, task)"");
 
       checkForErrors();
 
       // get the model for serialization
       REXP serializedRModel = eng.parseAndEval(this,
         ""serialize(weka_r_model"" + m_modelHash + "", NULL)"");
 
       checkForErrors();
 
       m_modelText = new StringBuffer();
 
       // get the textual representation
       eng.parseAndEval(this,
         ""print(getLearnerModel(weka_r_model"" + m_modelHash + ""))"");
       m_modelText.append(eng.getConsoleBuffer(this));
 
       // now try and serialize the model
       XStream xs = new XStream();
       String xml = xs.toXML(serializedRModel);
       if (xml != null && xml.length() > 0) {
         m_serializedModel = new StringBuffer();
         m_serializedModel.append(xml);
       }
     } catch (Exception ex) {
       ex.printStackTrace();
       // remove R training data frame after completion
       eng.parseAndEval(this, ""remove(mlr_data)"");
       RSession.releaseSession(this);
 
       throw new Exception(ex.getMessage());
     }
 
     eng.parseAndEval(this, ""remove(mlr_data)"");
     RSession.releaseSession(this);
   }",Buggy,"Fixed problem for learning algorithms such as xgboost that create files. Now MLRClassifier creates and uses a temporary directory.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code change improves handling when learning algorithms like xgboost need to create files. Previously, if an algorithm attempted to write to disk, it might have led to issues depending on the directory's write permissions. By explicitly changing to a temporary directory (`setwd(tempdir())`) before executing learning tasks, this update ensures there is always a directory with write access where temporary files can be safely created and used. This is a bug fix because it resolves a potential failure point in running learning algorithms due to file writing errors in non-writable locations.

Given this analysis, the change aligns with fixing a problem mentioned in the commit message. The answer is **Buggy**."
weka,23659.json,04f417158535105b2ada4d2cb15709a24d2924e2,"@@ -1,125 +1,125 @@
   public String toStringMatrix() {
     StringBuffer    result;
     String[][]      cells;
     int             i;
     int             j;
     int             n;
     int             size;
 
     result  = new StringBuffer();
     cells   = toArray();
 
     result.append(  ""\\begin{table}[thb]\n\\caption{\\label{labelname}""
                   + ""Table Caption}\n"");
     if (!getShowStdDev())
       result.append(""\\footnotesize\n"");
     else
       result.append(""\\scriptsize\n"");
 
     // output the column alignment characters
     // one for the dataset name and one for the comparison column
     if (!getShowStdDev()) {
       result.append(  ""{\\centering \\begin{tabular}{""
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     );
     } else {
       // dataset, mean, std dev
       result.append(  ""{\\centering \\begin{tabular}{"" 
                     + ""l""                     // dataset
                     + """"                      // separator
                     + ""r""                     // mean
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""c""                     // +/-
                     + ""@{\\hspace{0cm}}""      // separator
                     + ""r""                     // stddev
                     );
     }
 
     for (j = 1; j < getColCount(); j++) {
       if (getColHidden(j))
         continue;
       if (!getShowStdDev())
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
       else 
         result.append(  ""r""                   // mean
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""c""                   // +/-
                       + ""@{\\hspace{0cm}}""    // separator
                       + ""r""                   // stddev
                       + ""@{\\hspace{0.1cm}}""  // separator
                       + ""c""                   // significance
                       );
     }
     result.append(""}\n\\\\\n\\hline\n"");
     if (!getShowStdDev())
-      result.append(""Dataset & "" + getColName(0));
+      result.append(""Dataset & "" + cells[0][1]);
     else
-      result.append(""Dataset & \\multicolumn{3}{c}{"" + getColName(0) + ""}"");
+      result.append(""Dataset & \\multicolumn{3}{c}{"" + cells[0][1] + ""}"");
 
     // now do the column names (numbers)
-    for (j = 1; j < getColCount(); j++) {
-      if (getColHidden(j))
+    for (j = 2; j < cells[0].length; j++) {
+      if (!isMean(j))
         continue;
       if (!getShowStdDev())
-        result.append(""& "" + getColName(j) + "" & "");
+        result.append(""& "" + cells[0][j] + "" & "");
       else
-        result.append(""& \\multicolumn{4}{c}{"" + getColName(j) + ""} "");
+        result.append(""& \\multicolumn{4}{c}{"" + cells[0][j] + ""} "");
     }
     result.append(""\\\\\n\\hline\n"");
 
     // change ""_"" to ""-"" in names
     for (i = 1; i < cells.length; i++)
       cells[i][0] = cells[i][0].replace('_', '-');
 
     // pad numbers
     for (n = 1; n < cells[0].length; n++) {
       size = getColSize(cells, n);
       for (i = 1; i < cells.length; i++)
         cells[i][n] = padString(cells[i][n], size, true);
     }
 
     // output data (w/o wins/ties/losses)
     for (i = 1; i < cells.length - 1; i++) {
       for (n = 0; n < cells[0].length; n++) {
         if (n == 0) {
           result.append(padString(cells[i][n], getRowNameWidth()));
         }
         else {
           if (getShowStdDev()) {
             if (isMean(n - 1)) {
               if (!cells[i][n].trim().equals(""""))
                 result.append("" & $\\pm$ & "");
               else
                 result.append("" &       & "");
             }
             else
               result.append("" & "");
           }
           else {
             result.append("" & "");
           }
           result.append(cells[i][n]);
         }
       }
       
       result.append(""\\\\\n"");
     }
 
     result.append(""\\hline\n\\multicolumn{"" + cells[0].length + ""}{c}{$\\circ$, $\\bullet$""
 		  +"" statistically significant improvement or degradation}""
 		  +""\\\\\n\\end{tabular} "");
     if (!getShowStdDev())     
       result.append(""\\footnotesize "");
     else
       result.append(""\\scriptsize "");
     
     result.append(""\\par}\n\\end{table}""
 		  +""\n"");
      
     return result.toString();
   }",Buggy,"fixed bug with column names (weren't in the correct order)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2393 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message reports a bug concerning the order of column names, and modifications in the diff suggest a fix for it. The change replaces `getColName` calls with direct access to column names using `cells`, indicating that column names are likely managed differently now, possibly fixing the bug in their order.

Let's examine the key changes:
1. **Column Header Correction**: The method initially used `getColName(0)` to retrieve the first column name, but this was replaced with `cells[0][1]`. This suggests that the previous approach was not retrieving column names in the intended order and the patch corrects this by accessing them directly from the `cells` array, ensuring they are correctly matched with the data.

2. **Column Iteration Fix**: The diff changes how columns are iterated: `for (j = 1; ...)` becomes `for (j = 2; ...)`, and the condition `if (getColHidden(j))` changes to `if (!isMean(j))`. These adjustments likely correct the logic to ensure only relevant columns are processed and displayed.

3. **Direct Column Name Parsing**: Throughout the modified lines, `getColName(j)` calls are replaced with `cells[0][j]`. This change likely aligns with the intent to ensure that column names and data are accessed from the same structure, maintaining order consistency.

These updates reflect a logical correction of column ordering in the header and table content representation, directly addressing the commit message's bug claim.

Therefore, this change is consistent with a bug fix related to column name handling and presentation. The result is **Buggy**."
weka,36869.json,1a86d86af7ad235af95da7469c4252a5adf77d13,"@@ -1,27 +1,29 @@
   protected void checkPendingStreaming() throws WekaException {
     try {
       m_streamingFilter.batchFinished();
       Instances structureCopy =
         m_streamingFilter.getOutputFormat().stringFreeStructure();
       while (m_streamingFilter.numPendingOutput() > 0) {
         getStepManager().throughputUpdateStart();
         Instance filteredI = m_streamingFilter.output();
         if (m_stringAttsPresent) {
           for (int i = 0; i < filteredI.numAttributes(); i++) {
-            String val = filteredI.stringValue(i);
-            structureCopy.attribute(i).setStringValue(val);
-            filteredI.setValue(i, 0);
+            if (filteredI.attribute(i).isString() && ! filteredI.isMissing(i)) {
+              String val = filteredI.stringValue(i);
+              structureCopy.attribute(i).setStringValue(val);
+              filteredI.setValue(i, 0);
+            }
           }
           filteredI.setDataset(structureCopy);
         }
         m_incrementalData
           .setPayloadElement(StepManager.CON_INSTANCE, filteredI);
         if (!isStopRequested()) {
           getStepManager().outputData(m_incrementalData);
         }
         getStepManager().throughputUpdateEnd();
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",Buggy,"Fixed a small bug in processing of string attributes

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13321 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in processing string attributes, and the code changes support this. The modification adds a condition to check if an attribute is of string type and not missing before attempting to manipulate its value. This corrects an issue where non-string or missing attributes might have been incorrectly processed, leading to erroneous behavior or exceptions.

Here's a step-by-step analysis of the changes:
1. **New Condition Introduced**: `if (filteredI.attribute(i).isString() && ! filteredI.isMissing(i))` - This condition ensures that only non-missing string attributes undergo value manipulation. This prevents potential runtime errors or logical issues when manipulating non-string or missing values.
   
2. **Error Prevention**: The added check safeguards against inadvertently setting values for attributes that are not string types or are missing, which would have been inappropriate or even erroneous previously.

3. **String Attribute Handling**: The logic for handling string attributes, specifically setting string values and resetting them to `0` (which seems to denote a default state), is now correctly conditioned on relevant attributes being string type and non-missing.

Given the commit message context and the nature of the code modifications, it's clear that these changes address a bug. Therefore, the conclusion is **Buggy**."
weka,29259.json,a01e554f4dee170f439264d96f61f10c13bb316c,"@@ -1,58 +1,58 @@
   public double[] pairwiseCoupling(double[][] n, double[][] r) {
 
     // Initialize p and u array
     double[] p = new double[r.length];
     for (int i =0; i < p.length; i++) {
       p[i] = 1.0 / (double)p.length;
     }
     double[][] u = new double[r.length][r.length];
     for (int i = 0; i < r.length; i++) {
       for (int j = i + 1; j < r.length; j++) {
 	u[i][j] = 0.5;
       }
     }
 
     // firstSum doesn't change
     double[] firstSum = new double[p.length];
     for (int i = 0; i < p.length; i++) {
       for (int j = i + 1; j < p.length; j++) {
 	firstSum[i] += n[i][j] * r[i][j];
 	firstSum[j] += n[i][j] * (1 - r[i][j]);
       }
     }
 
     // Iterate until convergence
     boolean changed;
     do {
       changed = false;
       double[] secondSum = new double[p.length];
       for (int i = 0; i < p.length; i++) {
 	for (int j = i + 1; j < p.length; j++) {
 	  secondSum[i] += n[i][j] * u[i][j];
 	  secondSum[j] += n[i][j] * (1 - u[i][j]);
 	}
       }
       for (int i = 0; i < p.length; i++) {
-	if (firstSum[i] == 0) {
+	if ((firstSum[i] == 0) || (secondSum[i] == 0)) {
 	  if (p[i] > 0) {
 	    changed = true;
 	  }
 	  p[i] = 0;
 	} else {
 	  double factor = firstSum[i] / secondSum[i];
 	  double pOld = p[i];
 	  p[i] *= factor;
 	  if (Math.abs(pOld - p[i]) > 1.0e-3) {
 	    changed = true;
 	  }
 	}
       }
       Utils.normalize(p);
       for (int i = 0; i < r.length; i++) {
 	for (int j = i + 1; j < r.length; j++) {
 	  u[i][j] = p[i] / (p[i] + p[j]);
 	}
       }
     } while (changed);
     return p;
   }",Buggy,"Fixed division by zero problem in pairwise coupling


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1430 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a ""division by zero problem"" fix in the context of pairwise coupling, which aligns well with the changes in the code. The diff introduces a check for both `firstSum[i] == 0` and `secondSum[i] == 0`. This check prevents division by zero in the subsequent calculation `firstSum[i] / secondSum[i]`. If `secondSum[i]` is zero, the code now sets `p[i]` to `0`, avoiding the problematic division. This additional condition ensures that the code won't attempt to divide by zero, thereby preventing a potential runtime error.

The code change directly addresses a possible issue that would manifest as a runtime ""division by zero"" error, which is indeed a type of bug. Therefore, based on the alignment of the commit message with the functional fix in the code, this commit should be classified as **Buggy**."
weka,5830.json,4e28ba24b34efd31ec6954aa8312a90cd500ce2a,"@@ -1,12 +1,12 @@
   public void stepInit() throws WekaException {
     if ((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) &&
-      m_fileName == null || isEmpty(m_fileName.toString())) {
+      (m_fileName == null || isEmpty(m_fileName.toString()))) {
       throw new WekaException(""No forecaster specified!"");
     }
 
     m_isReset = true;
     m_isStreaming = false;
     m_overlayData = null;
     m_bufferedPrimeData = null;
     m_streamingData = new Data(StepManager.CON_INSTANCE);
   }",Buggy,"Fixed a bug that prevented initialization to complete successfully when the model being used is serialized into the metadata for the step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13349 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes fixing a bug related to initialization when a model is serialized into metadata. The code diff addresses a logical error in the condition check for initializing a forecaster.

Original logic:
```java
if ((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) &&
  m_fileName == null || isEmpty(m_fileName.toString())) {
```

Updated logic:
```java
if ((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) &&
  (m_fileName == null || isEmpty(m_fileName.toString()))) {
```

### Explanation:

1. **Logical Correction**: The key change in the diff is the addition of parentheses. In the original code, the `&&` operator has higher precedence than the `||` operator, leading to the problematic logical grouping `((m_encodedForecaster == null || m_encodedForecaster.equals(""-NONE-"")) && m_fileName == null) || isEmpty(m_fileName.toString())`. This may have resulted in incorrect evaluation of conditions, causing unwanted exceptions or the bypassing of necessary initialization routines.

2. **Correctly Grouping Conditions**: The fix correctly groups conditions to check if either `m_encodedForecaster` is undefined/none, and `m_fileName` is also undefined/empty, before throwing an exception. This ensures that the initialization logic triggers correctly when needed.

### Conclusion:
This change is a fix for a logical error that caused incorrect conditions for initialization warnings. The update aligns with the commit message's statement regarding an initialization bug. Therefore, the nature of this change is indicative of a bug fix. 

**Buggy**"
weka,25919.json,92304dbdd06bfcf597e16639d7f8b1656d4269d9,"@@ -1,3 +1,4 @@
   public void setComputeAttributeImportance(boolean computeAttributeImportance) {
     m_computeAttributeImportance = computeAttributeImportance;
+    ((RandomTree)m_Classifier).setComputeImpurityDecreases(computeAttributeImportance);
   }",Buggy,"Fixed another small bug in option setting

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13190 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code suggests that when setting the `computeAttributeImportance` option, it now additionally sets the `computeImpurityDecreases` option on a `RandomTree` classifier. This update likely resolves an issue where the `computeAttributeImportance` setting was not being correctly propagated to the classifier, leading to inconsistencies in its operation. This aligns with the commit message describing a fix in option setting.

Given the context and nature of the change, this appears indicative of a bug fix. Therefore, the analysis concludes with **Buggy**."
weka,10363.json,e68621d5b0b175c4b155cb383a8c007a83d10875,"@@ -1,8 +1,8 @@
   public String globalInfo() {
-    return ""Iteratively fits a regression model by attempting to minimize absolute error, based on""
+    return ""Iteratively fits a regression model by attempting to minimize absolute error, using""
             + ""a base learner that minimizes weighted squared error.\n\n""
             + ""Weights are bounded from below by 1.0 / Utils.SMALL.\n\n""
             + ""Resamples data based on weights if base learner is not a WeightedInstancesHandler.\n\n""
       +""For more information see:\n\n""
       + getTechnicalInformation().toString();
   }",Buggy,"Updated Javadoc in IterativeAbsoluteErrorRegression. Fixed bug in globalInfo().

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12364 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message claims a bug fix in the `globalInfo()` method. The change in the diff replaces the phrase ""based on"" with ""using"" in the Javadoc comment string returned by `globalInfo()`. This adjustment refines the wording but doesn't resolve any functional issues or fix a bug in the code. There are no corrections in logic, error handling, or functional behavior. As such, the change doesn't align with the traditional understanding of a ""bug fix.""

Considering the above points, the diff seems to be more of a documentation update rather than a bug fix in the code's behavior:

- **Javadoc Update**: The change affects only the comment string without altering any logic or behavior of the method.
- **No Functional Impact**: The method's functionality—how it operates or the data it processes—is unchanged.

Therefore, despite the commit message indicating a bug fix, the code changes themselves do not reflect a resolution of a software defect in the traditional sense. The conclusion based on the diff is **NotBuggy**."
weka,12909.json,1e5d54bb479e458b94b035c9d201dae6a4aec513,"@@ -1,45 +1,48 @@
   public void updateDefaultList() {
     
     ((ModelList.SortedListModel) m_ModelList.getModel()).clear();
     
     String ensemblePackageString = getPackageName();
     
     int index = m_DefaultFilesComboBox.getSelectedIndex();
     
     Vector classifiers = null;
     
     LibrarySerialization serialization;
     try {
       
       serialization = new LibrarySerialization();
       
       String defaultFileString = ensemblePackageString
       + m_DefaultFileNames[index].trim() + "".model.xml"";
       
       //System.out.println(defaultFileString);
-      
-      InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+    
+      ClassLoader thisLoader = getClass().getClassLoader();
+      // InputStream is = ClassLoader.getSystemResourceAsStream(defaultFileString);
+      InputStream is = thisLoader.getResourceAsStream(defaultFileString);
       
       if (is == null) {
 	File f = new File(defaultFileString);
 	if (f.exists()) {
 	  System.out.println(""file existed: "" + f.getPath());
 	} else {
 	  System.out.println(""file didn't exist: "" + f.getPath());
 	}
 	
       }
       
-      classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      // classifiers = (Vector) serialization.read(ClassLoader.getSystemResourceAsStream(defaultFileString));
+      classifiers = (Vector) serialization.read(thisLoader.getResourceAsStream(defaultFileString));
       
       for (Iterator it = classifiers.iterator(); it.hasNext();) {
 	EnsembleLibraryModel model = m_ListModelsPanel.getLibrary().createModel((Classifier) it.next());
 	model.testOptions();
 	((ModelList.SortedListModel) m_ModelList.getModel()).add(model);
       }
       
     } catch (Exception e) {
       // TODO Auto-generated catch block
       e.printStackTrace();
     }
   }",Buggy,"Fixed a bug that was preventing a property file from being loaded correctly

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14340 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for loading a property file. The diff shows a change in how the InputStream is obtained to load a resource file. Originally, `ClassLoader.getSystemResourceAsStream` was used, but it has been replaced by the instance's class loader `thisLoader.getResourceAsStream`. This change aligns with the commit message, as it addresses how a resource file is located and loaded, which could directly affect whether a property file is loaded correctly. The previous implementation might not have correctly located the file if it was not within the system classpath but accessible via the instance's class loader context.

Therefore, the change is likely a bug fix since it modifies the resource loading mechanism to ensure successful retrieval, matching the commit message description.

The answer is **Buggy**."
weka,21385.json,1f61d35aae476dd90e8166897219ea0c8128e978,"@@ -1,173 +1,173 @@
   public void actionPerformed(ActionEvent e) {
     
     //JMenuItem m = (JMenuItem)e.getSource();
     
     if (e.getActionCommand() == null) {
       if (m_scaling == 0) {
 	repaint();
       }
       else {
 	animateScaling(m_nViewPos, m_nViewSize, m_scaling);
       }
     }
     else if (e.getActionCommand().equals(""Fit to Screen"")) {
       
       Dimension np = new Dimension();
       Dimension ns = new Dimension();
 
       getScreenFit(np, ns);
 
       animateScaling(np, ns, 10);
       
     }
     else if (e.getActionCommand().equals(""Center on Top Node"")) {
       
       int tpx = (int)(m_topNode.getCenter() * m_viewSize.width);   //calculate
       //the top nodes postion but don't adjust for where 
       int tpy = (int)(m_topNode.getTop() * m_viewSize.height);     //view is
       
       
       
       Dimension np = new Dimension(getSize().width / 2 - tpx, 
 				   getSize().width / 6 - tpy);
       
       animateScaling(np, m_viewSize, 10);
       
     }
     else if (e.getActionCommand().equals(""Auto Scale"")) {
       autoScale();  //this will figure the best scale value 
       //keep the focus on the middle of the screen and call animate
     }
     else if (e.getActionCommand().equals(""Visualize The Node"")) {
       //send the node data to the visualizer 
       if (m_focusNode >= 0) {
 	Instances inst;
 	if ((inst = m_nodes[m_focusNode].m_node.getInstances()) != null) {
 	  VisualizePanel pan = new VisualizePanel();
 	  pan.setInstances(inst);
 	  JFrame nf = new JFrame();
 	  nf.setSize(400, 300);
 	  nf.getContentPane().add(pan);
 	  nf.setVisible(true);
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
-					""availble Instances data for "" +
+					""available Instances data for "" +
 					""this Node."", ""Sorry!"",
 					JOptionPane.WARNING_MESSAGE); 
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" + 
 				      ""selected Node to perform "" +
 				      ""this operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE); 
       }
     }
     else if (e.getActionCommand().equals(""Create Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.ADD_CHILDREN, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decision Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Remove Child Nodes"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new 
 	    TreeDisplayEvent(TreeDisplayEvent.REMOVE_CHILDREN, 
 			     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""classify_child"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.CLASSIFY_CHILD, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""send_instances"")) {
       if (m_focusNode >= 0) {
 	if (m_listener != null) {
 	  //then send message to the listener
 	  m_listener.userCommand(new TreeDisplayEvent
 	    (TreeDisplayEvent.SEND_INSTANCES, 
 	     m_nodes[m_focusNode].m_node.getRefer()));
 	}
 	else {
 	  JOptionPane.showMessageDialog(this, ""Sorry, there is no "" + 
 					""available Decsion Tree to "" +
 					""perform this operation on."",
 					""Sorry!"", 
 					JOptionPane.WARNING_MESSAGE);
 	}
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Error, there is no "" +
 				      ""selected Node to perform this "" +
 				      ""operation on."", ""Error!"",
 				      JOptionPane.ERROR_MESSAGE);
       }
     }
     else if (e.getActionCommand().equals(""Accept The Tree"")) {
       if (m_listener != null) {
 	//then send message to the listener saying that the tree is done
 	m_listener.userCommand(new TreeDisplayEvent(TreeDisplayEvent.ACCEPT,
 						  null));
       }
       else {
 	JOptionPane.showMessageDialog(this, ""Sorry, there is no "" +
 				      ""available Decision Tree to "" +
 				      ""perform this operation on."",
 				      ""Sorry!"", 
 				      JOptionPane.WARNING_MESSAGE);
       }
     }
   }",Buggy,"fixed typo in error message


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3733 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states that a typo in an error message was fixed. Looking at the diff, the only change is correcting the spelling of ""availble"" to ""available"" in an error message. There are no other changes in the code logic or functionality.

This aligns perfectly with the commit message, which specifically mentions fixing a typo in an error message. Since the change is purely a typo correction and doesn't involve fixing any logic, functionality, or error-handling issues, it doesn't represent a bug fix in the code's behavior or execution.

Therefore, this change reflects a simple typographical correction rather than a bug fix. The conclusion is **NotBuggy**."
weka,26536.json,8a6133c51b18360a896a4e959c47ff6c31ed9425,"@@ -1,16 +1,12 @@
   protected void initializeWithPluginMetrics() {
     m_pluginMetrics.clear();
     List<AbstractEvaluationMetric> pluginMetrics = m_eval.getPluginMetrics();
     if (pluginMetrics != null && pluginMetrics.size() > 0) {
       for (AbstractEvaluationMetric m : pluginMetrics) {
-        if (m instanceof InformationRetrievalEvaluationMetric) {
-          List<String> statNames = m.getStatisticNames();
-          for (String s : statNames) {
-            m_pluginMetrics.put(s.toLowerCase(), m);
-          }
-        } else {
-          m_pluginMetrics.put(m.getMetricName().toLowerCase(), m);
+        List<String> statNames = m.getStatisticNames();
+        for (String s : statNames) {
+          m_pluginMetrics.put(s.toLowerCase(), m);
         }
       }
     }
   }",Buggy,"Fixed a small bug in the initializeWithPluginMetrics() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10890 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a small bug fix in the `initializeWithPluginMetrics()` method, and the code diff indicates a logical change within this method. Here's an analysis:

1. **Original Code Logic:**
   - The code differentiates between instances of `InformationRetrievalEvaluationMetric` and other metric types. 
   - For `InformationRetrievalEvaluationMetric`, it retrieves a list of statistic names from the metric and populates `m_pluginMetrics` with these statistic names converted to lowercase.
   - For other metrics, only the metric's name in lowercase is added to `m_pluginMetrics`.

2. **Modified Code Logic:**
   - The new logic removes the distinction between `InformationRetrievalEvaluationMetric` and other metric types.
   - Now, for all metric instances, it directly retrieves statistic names and populates `m_pluginMetrics` with these names in lowercase without checking the metric type.

3. **Bug Fix Justification:**
   - The commit simplifies the logic, suggesting that all metrics need to be treated similarly regarding how their statistics are stored.
   - The removal of specific handling for `InformationRetrievalEvaluationMetric` indicates that the prior code might have been incorrect in distinguishing metrics or the handling was unnecessary and led to unwanted behavior.

4. **Conclusion:**
   - The changes align with the commit message indicating a bug fix.
   - Given the context, it's reasonable to assert that the prior logic was either incorrect or caused an inconsistency, which the current changes aim to resolve.

Considering these aspects, the fix addresses a logical problem in the method, consistent with the commit message indicating a bug fix. Therefore, the conclusion is **Buggy**."
weka,21345.json,f05e160a2290644b91f34152ee153ba21fa688bd,"@@ -1,9 +1,21 @@
     public void setValueAt(Object aValue,
 			   int rowIndex,
 			   int columnIndex) {
 
       //      double value = ((Double) aValue).doubleValue();
       //      m_matrix.setElement(rowIndex, columnIndex, value);
-      m_matrix.setCell(rowIndex, columnIndex, aValue);
+      // try to parse it as a double first
+      Double val;
+      try {
+        val = new Double(((String)aValue));
+        double value = val.doubleValue();
+      } catch (Exception ex) {
+        val = null;
+      }
+      if (val == null) {
+        m_matrix.setCell(rowIndex, columnIndex, aValue);
+      } else {
+        m_matrix.setCell(rowIndex, columnIndex, val);
+      }
       fireTableCellUpdated(rowIndex, columnIndex);
     }",Buggy,"Fixed problem with setting cost values


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3268 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a problem with setting cost values, and the code changes reflect a fix for this issue. The original code directly set the cell value using `aValue`, but the updated code attempts to parse `aValue` as a `Double`. If parsing is successful, it uses the parsed `Double`, providing the correct numeric type for setting cost values, rather than a general `Object`. If parsing fails, it falls back to using the original `aValue`.

This change addresses a potential bug where `aValue` could be a string representing a number that wasn't being converted to a `Double` type, which could affect how cost values are utilized or displayed. The use of a try-catch block also improves error handling by ensuring that non-parsable values do not lead to unexpected exceptions but rather are handled gracefully by falling back to the original value.

Given these observations, the changes are indeed a bug fix related to handling the setting of cost values. Therefore, the conclusion is **Buggy**."
weka,32633.json,5bf47271c53b258212fcf284bc65101b75cd4309,"@@ -1,45 +1,45 @@
   protected static void loadPackageDirectory(File directory, boolean verbose)
       throws Exception {
     File[] contents = directory.listFiles();
 
     // make sure that jar files and lib directory get processed first
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith("".jar"")) {
         if (verbose) {
           System.out.println(""[Weka] loading "" + contents[i].getPath());
         }
         ClassloaderUtil.addFile(contents[i].getPath());
       } else if (contents[i].isDirectory()
           && contents[i].getName().equalsIgnoreCase(""lib"")) {
         // add any jar files in the lib directory to the classpath
         loadPackageDirectory(contents[i], verbose);
       }
     }
 
     // now any auxilliary files
     for (int i = 0; i < contents.length; i++) {
       if (contents[i].isFile() && contents[i].getPath().endsWith(""Beans.props"")) {
         // KnowledgeFlow plugin -- add the Beans.props file to the list of
         // bean plugin props
 
         KnowledgeFlowApp.addToPluginBeanProps(contents[i]);
         KnowledgeFlowApp.disposeSingleton();
 
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""Explorer.props"")) {
         // Explorer tabs plugin
         // process the keys in the properties file and append/add values
         processExplorerProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GUIEditors.props"")) {
         // Editor for a particular component
         processGUIEditorsProps(contents[i]);
       } else if (contents[i].isFile()
           && contents[i].getPath().endsWith(""GenericPropertiesCreator.props"")) {
         processGenericPropertiesCreatorProps(contents[i]);
       } else if (contents[i].isFile()
-          && contents[i].getParent().endsWith(""PluginManager.props"")) {
+          && contents[i].getPath().endsWith(""PluginManager.props"")) {
         processPluginManagerProps(contents[i]);
       }
     }
   }",Buggy,"Fixed a bug in the processing of PluginManager.props files.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9314 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change alters the condition for processing `PluginManager.props` files. The original condition checked if the parent directory's name ended with ""PluginManager.props,"" which would logically never be true for a file. The revised condition checks if the file's path ends with ""PluginManager.props,"" which is the correct way to identify the file. This indicates a clear logical bug fix in the file processing mechanism.

Given the provided commit message, which specifies a bug fix in the processing of `PluginManager.props` files, the code change directly addresses this issue by correcting the logical condition used to identify and process these files.

Thus, the changes align with the commit message and indicate a bug fix. Therefore, the conclusion is **Buggy**."
weka,30120.json,4223a444b35556add50503527e26b24f82ea6784,"@@ -1,6 +1,6 @@
   public Instances trainCV(int numFolds, int numFold, Random random) {
 
-    Instances train = trainCV(numFold, numFold);
+    Instances train = trainCV(numFolds, numFold);
     train.randomize(random);
     return train;
   }",Buggy,"Fixed a bug that I just introduced with my new code.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1850 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The diff shows that the parameter passed to the `trainCV` method was corrected from `numFold, numFold` to `numFolds, numFold`. The initial code likely caused incorrect behavior by repeating a parameter, likely leading to a logical error in fold handling for cross-validation. The change aligns with correcting a self-introduced bug in the code. Therefore, the answer is **Buggy**."
weka,20457.json,0fb7be37516d2f841ec1d8dc7949967049dd41f9,"@@ -1,5 +1,10 @@
   public void setText(String text) {
     m_currentContents = text;
-    m_combo.setSelectedItem(m_currentContents);
+    java.awt.Component theEditor = m_combo.getEditor().getEditorComponent();
+    if (theEditor instanceof JTextField) {
+      ((JTextField)theEditor).setText(text);
+    } else {
+      m_combo.setSelectedItem(m_currentContents);
+    }
     m_support.firePropertyChange("""", null, null);
   }",Buggy,"Fixed a bug in setting the value of the field.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@8116 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates there was an issue with setting a field's value, which is addressed by this change. The modification includes a conditional check to determine the type of editor component. If it's a `JTextField`, it sets the text directly on the editor component; otherwise, it falls back to setting the combo box's selected item.

This change suggests that there was a bug where the text might not be set correctly if the editor was specifically a `JTextField` and not just any item. This is indeed a bug fix related to ensuring the correct method of setting the text based on the component type. 

Hence, the evaluation of this change indicates a bug fix. The conclusion is **Buggy**."
weka,36417.json,5887d72aa9c3dc9d14f09c98ea60b54b3ea4a75c,"@@ -1,31 +1,40 @@
   public void processSecondary(Integer setNum, Integer maxSetNum, Data data,
     PairedDataHelper<weka.classifiers.Classifier> helper) throws WekaException {
 
     // trained classifier for this set number
     weka.classifiers.Classifier classifier =
       helper.getIndexedPrimaryResult(setNum);
 
     // test data
     Instances testSplit = data.getPrimaryPayload();
 
+    if (m_trainedClassifierHeader != null
+      && !testSplit.equalHeaders(m_trainedClassifierHeader)) {
+      if (!(m_trainedClassifier instanceof InputMappedClassifier)) {
+        throw new WekaException(
+          ""Structure of incoming data does not match ""
+            + ""that of the trained classifier"");
+      }
+    }
+
     // paired training data
     Instances trainingSplit =
       helper.getIndexedValueFromNamedStore(""trainingSplits"", setNum);
 
     getStepManager().logBasic(
       ""Dispatching model for set "" + setNum + "" out of "" + maxSetNum
         + "" to output"");
 
     Data batchClassifier =
       new Data(StepManager.CON_BATCH_CLASSIFIER, classifier);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET,
       trainingSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_TESTSET,
       testSplit);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, setNum);
     batchClassifier.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM,
       maxSetNum);
     batchClassifier
       .setPayloadElement(StepManager.CON_AUX_DATA_LABEL, getName());
     getStepManager().outputData(batchClassifier);
   }",Buggy,"Fixed a bug where the Classifier step was not checking for compatible structure structure between training and test sets in the case where the classifier is not an InputMappedClassifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14678 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug fix was implemented to address the issue where the Classifier step was not verifying compatible structures between training and test sets when the classifier is not an InputMappedClassifier.

Let's analyze the code changes:

1. **Structure Check Addition**:
   - A new conditional block is added to verify the compatibility between the structure of the test data and the structure used for trained classifiers.
   - It checks if `m_trainedClassifierHeader` is not null and whether the headers of the test data (`testSplit`) match the trained classifier header (`m_trainedClassifierHeader`).
   - If the structures do not match, and the classifier is not an instance of `InputMappedClassifier`, an exception (`WekaException`) is thrown with a relevant error message.

The code changes align well with the commit message. They add a necessary check for dataset structure compatibility, which was previously missing, and throw an exception if there's a mismatch. This directly addresses the bug described in the message about missing validation for compatible structures.

Considering the details provided in the commit message and the explicit addition of code to handle the described bug, the conclusion is:

**Buggy**"
weka,36360.json,a415ad7bb7e75ba3cda722d4c62bd27c01db961e,"@@ -1,78 +1,84 @@
   protected void createOffscreenPlot(Data data) {
     List<Instances> offscreenPlotData = new ArrayList<Instances>();
     Instances predictedI = data.getPrimaryPayload();
+    boolean colorSpecified = false;
 
-    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()) {
+    String additional = m_additionalOptions;
+    if (m_additionalOptions.length() > 0) {
+      additional = environmentSubstitute(additional);
+    }
+
+    if (!additional.contains(""-color"")
+      && m_offscreenRendererName.contains(""Weka Chart Renderer"")) {
+      // for WekaOffscreenChartRenderer only
+      if (additional.length() > 0) {
+        additional += "","";
+      }
+      if (predictedI.classIndex() >= 0) {
+        additional += ""-color="" + predictedI.classAttribute().name();
+      } else {
+        additional += ""-color=/last"";
+      }
+    } else {
+      colorSpecified = true;
+    }
+
+    if (predictedI.classIndex() >= 0 && predictedI.classAttribute().isNominal()
+      && !colorSpecified) {
       // set up multiple series - one for each class
       Instances[] classes = new Instances[predictedI.numClasses()];
       for (int i = 0; i < predictedI.numClasses(); i++) {
         classes[i] = new Instances(predictedI, 0);
         classes[i].setRelationName(predictedI.classAttribute().value(i));
       }
       for (int i = 0; i < predictedI.numInstances(); i++) {
         Instance current = predictedI.instance(i);
         classes[(int) current.classValue()].add((Instance) current.copy());
       }
       for (Instances classe : classes) {
         offscreenPlotData.add(classe);
       }
     } else {
       offscreenPlotData.add(new Instances(predictedI));
     }
 
     List<String> options = new ArrayList<String>();
-    String additional = m_additionalOptions;
-    if (m_additionalOptions.length() > 0) {
-      additional = environmentSubstitute(additional);
-    }
-
-    if (additional.contains(""-color"")) {
-      // for WekaOffscreenChartRenderer only
-      if (additional.length() > 0) {
-        additional += "","";
-      }
-      if (predictedI.classIndex() >= 0) {
-        additional += ""-color="" + predictedI.classAttribute().name();
-      } else {
-        additional += ""-color=/last"";
-      }
-    }
 
     String[] optionsParts = additional.split("","");
     for (String p : optionsParts) {
       options.add(p.trim());
     }
 
     // only need the x-axis (used to specify the attribute to plot)
     String xAxis = m_xAxis;
     xAxis = environmentSubstitute(xAxis);
 
     String width = m_width;
     String height = m_height;
     int defWidth = 500;
     int defHeight = 400;
     width = environmentSubstitute(width);
     height = environmentSubstitute(height);
 
     defWidth = Integer.parseInt(width);
     defHeight = Integer.parseInt(height);
 
     try {
       getStepManager().logDetailed(""Creating image"");
       BufferedImage osi =
         m_offscreenRenderer.renderHistogram(defWidth, defHeight,
           offscreenPlotData, xAxis, options);
 
       Data imageData = new Data(StepManager.CON_IMAGE, osi);
       String relationName = predictedI.relationName();
       if (relationName.length() > 10) {
         relationName = relationName.substring(0, 10);
       }
       imageData.setPayloadElement(StepManager.CON_AUX_DATA_TEXT_TITLE,
         relationName + "":"" + m_xAxis);
       getStepManager().outputData(imageData);
     } catch (Exception e1) {
       e1.printStackTrace();
     }
 
   }",Buggy,"Fixed a few bugs in offscreen rendering that affected stacked histograms

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13327 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the changes address bugs in offscreen rendering affecting stacked histograms. Let's analyze the code modifications to understand how they relate to the commit message:

1. **Introduction of `colorSpecified` Flag:** 
   - The change introduces a boolean flag `colorSpecified` to determine if the color option has been explicitly set by the user. This is relevant because color options can affect renderings when dealing with stacked histograms. 

2. **Reordering and Logic Adjustment:**
   - The logic for handling the `-color` option has been revised. Previously, a single block checked for `-color` presence and then appended it. Now, the logic has split: checks for `-color` option presence and whether the renderer is a ""Weka Chart Renderer"" are clearly separated.
   - The handling of additional options and color specifications is done upfront to ensure that the proper color options are applied before proceeding with the class-based data separation.

3. **Conditional Handling of Nominal Class Attributes:**
   - The updated code ensures that if a class index is valid and the class attribute is nominal, and a color option hasn't been specified, it correctly assigns a color based on class attributes or defaults. This is critical because histograms with multiple classes need distinct colors for visual clarity. 

4. **Data Handling:**
   - Instances are segregated by class and added for offscreen plotting, ensuring each class's data gets its color overlay.

The changes specifically address and improve how colors are handled in stacked histograms, which aligns with the commit message stating fixes for bugs affecting offscreen rendering. 

In conclusion, the modifications align well with the bug fix claim regarding rendering stacked histograms correctly. Thus, I determine the result is **Buggy**."
weka,9851.json,64d54abda520da9b2297ed56047ce86beda812b4,"@@ -1,113 +1,117 @@
   protected void makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, Instances test, FastVector modelError, int[][] sortedIndices,
       double[][] weights, double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini, boolean useErrorRate)
   throws Exception {
 
     if (BestFirstElements.size()==0) return;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     //BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return;
+      }
+
       BFTree nextSplitNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	  nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	  nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	  nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	  useHeuristic, useGini, useErrorRate);
       return;
 
     }
 
     // If gini gain or information gain is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
     }
 
     // gini gain or information gain is not 0
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       m_Attribute = att;
       if (att.isNumeric()) m_SplitValue = splitValue;
       else m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, m_Attribute,
 	  m_SplitValue, m_SplitString,
 	  sortedIndices, weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 	makeLeaf(train);
       }
 
       // split the node and cauculate error rate of this temporary tree
       else {
 	m_isLeaf = false;
 	m_Attribute = att;
 
 	makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, m_Attribute, useHeuristic, useGini);
 	for (int i=0; i<2; i++){
 	  m_Successors[i].makeLeaf(train);
 	}
 
 	Evaluation eval = new Evaluation(test);
 	eval.evaluateModel(root, test);
 	double error;
 	if (useErrorRate) error = eval.errorRate();
 	else error = eval.rootMeanSquaredError();
 	modelError.addElement(new Double(error));
       }
 
       if (BestFirstElements.size()!=0) {
 	FastVector nextSplitElement = (FastVector)BestFirstElements.elementAt(0);
 	BFTree nextSplitNode = (BFTree)nextSplitElement.elementAt(0);
 	nextSplitNode.makeTree(BestFirstElements, root, train, test, modelError,
 	    nextSplitNode.m_SortedIndices, nextSplitNode.m_Weights,
 	    nextSplitNode.m_Dists, nextSplitNode.m_ClassProbs,
 	    nextSplitNode.m_TotalWeight, nextSplitNode.m_Props, minNumObj,
 	    useHeuristic, useGini,useErrorRate);
       }
     }
   }",Buggy,"Fixed null pntr exception due to the fact that BFTree's recursion depended on a bug in FastVector


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a null pointer exception in BFTree's recursive method `makeTree`, related to an issue in FastVector that the previous behavior relied upon. The diff adds a condition to return immediately if `BestFirstElements` is empty after making a leaf, which prevents further recursive calls that could lead to a null pointer exception if `BestFirstElements` were empty.

Here is the step-by-step reasoning:

1. **Identify Problem**: The commit explicitly mentions fixing a null pointer exception which can occur when `BestFirstElements` is empty.

2. **Check Code Changes**: 
   - In the diff, a check `if (BestFirstElements.size() == 0) { return; }` is added after the `makeLeaf(train);` call if certain conditions hold.
   - This ensures that if `BestFirstElements` becomes empty, the method exits before trying to process an empty `BestFirstElements` that could lead to accessing a null or an invalid reference.

3. **Reflect on Purpose**: The added condition prevents the code from proceeding with a recursive call using an empty `BestFirstElements`.

4. **Related Concepts**:
   - **Null Pointer Exception**: Trying to access elements of an empty Vector could result in null pointer dereference as it attempts to access elements that aren't there.

5. **Conclusion**:
   - The commit provides a necessary safeguard against a potential null pointer exception by checking and handling the state of `BestFirstElements`, directly aligning with the commit message.

Given these observations, this change is consistent with the message describing a bug fix related to null pointer exceptions. Therefore, the assessment of this change is **Buggy**."
weka,28572.json,563360413e272237a4f3fe3033b8e50dfb132e27,"@@ -1,12 +1,18 @@
 	void replaceAtt(int nTargetNode, String sName, FastVector values) {
 		Attribute newAtt = new Attribute(sName, values);
 		if (m_Instances.classIndex() == nTargetNode) {
 			m_Instances.setClassIndex(-1);
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			m_Instances.setClassIndex(nTargetNode); */
+			
+			m_Instances.deleteAttributeAt(nTargetNode);
+			m_Instances.insertAttributeAt(newAtt, nTargetNode);
 			m_Instances.setClassIndex(nTargetNode);
 		} else {
-			m_Instances.insertAttributeAt(newAtt, nTargetNode);
-			m_Instances.deleteAttributeAt(nTargetNode + 1);
+			/*m_Instances.insertAttributeAt(newAtt, nTargetNode);
+			m_Instances.deleteAttributeAt(nTargetNode + 1); */
+		        m_Instances.deleteAttributeAt(nTargetNode);
+		        m_Instances.insertAttributeAt(newAtt, nTargetNode);
 		}
 	} // replaceAtt",Buggy,"Fixed a bug (caused indirectly by the change that disallows attributes with the same name in a set of instances) whereby an exception was generated by actions that mutated the values at a node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7834 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to a change in attribute mutation handling. The code previously inserted a new attribute and then deleted the next one, potentially leaving inconsistent states. The revised approach deletes the target node's attribute first, then inserts the new one—simplifying attribute replacement without shifting other attributes—avoiding the state that led to exceptions. This aligns with the commit message and addresses the described bug in attribute mutation. Therefore, the commit can be classified as a bug fix. The answer is **Buggy**."
weka,20177.json,b5bafa7c1b1d4a69d3060a9be3b02c4b280127b9,"@@ -1,10 +1,11 @@
   public static void addPlugin(String interfaceName, String name, 
       String concreteType) {
     if (PLUGINS.get(interfaceName) == null) {
       Map<String, String> pluginsOfInterfaceType = 
         new TreeMap<String, String>();
       pluginsOfInterfaceType.put(name, concreteType);
+      PLUGINS.put(interfaceName, pluginsOfInterfaceType);      
     } else {
       PLUGINS.get(interfaceName).put(name, concreteType);
     }
   }",Buggy,"Fixed a bug in the plugin registering mechanism.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7629 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code confirms the bug fix described in the commit message. Originally, when `PLUGINS.get(interfaceName)` returned null (indicating that no plugins were registered under that interface name), the new map `pluginsOfInterfaceType` was created and populated with the provided name and concreteType, but this new map was not added back to `PLUGINS`. The modification adds `PLUGINS.put(interfaceName, pluginsOfInterfaceType);`, ensuring that the newly created map is correctly registered under the given interface name in the `PLUGINS` map. This effectively fixes the bug in the plugin registering mechanism. The result is **Buggy**."
weka,21148.json,bad512e53bc1d955e4847e14296af551c9f28459,"@@ -1,50 +1,50 @@
   public void acceptDataSet(DataSetEvent e) {
 
     m_busy = true;
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Processing batch..."");
     }
 
     try {
       makeOutputStructure(new Instances(e.getDataSet(), 0));
     } catch (Exception ex) {
       String msg = statusMessagePrefix()
         + ""ERROR: unable to create output instances structure."";
       if (m_log != null) {
         m_log.statusMessage(msg);
         m_log.logMessage(""[SubstringLabeler] "" + ex.getMessage());
       }
       stop();
 
       ex.printStackTrace();
       m_busy = false;
       return;
     }
 
     Instances toProcess = e.getDataSet();
 
     for (int i = 0; i < toProcess.numInstances(); i++) {
       Instance current = toProcess.instance(i);
       Instance result = null;
       try {
         result = m_matches.makeOutputInstance(current, true);
       } catch (Exception ex) {
         ex.printStackTrace();
       }
 
       if (result != null) {
         // m_outputStructure.add(result);
         m_matches.getOutputStructure().add(result);
       }
     }
 
     if (m_log != null) {
       m_log.statusMessage(statusMessagePrefix() + ""Finished."");
     }
 
     // notify listeners
-    DataSetEvent d = new DataSetEvent(this, m_matches.getInputStructure());
+    DataSetEvent d = new DataSetEvent(this, m_matches.getOutputStructure());
     notifyDataListeners(d);
 
     m_busy = false;
   }",Buggy,"Fixed a bug in batch mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11956 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code change in the diff highlights a key modification:

- A `DataSetEvent` is created using `m_matches.getOutputStructure()` instead of `m_matches.getInputStructure()`.

The commit message indicates a bug fix, and the diff reflects this by correcting the event creation process. The previous logic may have incorrectly sent the input structure instead of the desired output structure, probably causing issues in ""batch mode"" processing. This modification ensures that the correct data structure is used within the event, aligning with the intention of fixing a bug.

Given the specific alteration in code that corrects the foundational logic of how data is handled in events, this change is highly indicative of a bug fix in the production logic of this batch mode procedure. 

Thus, the changes are accurately described by the commit message, and the nature of the changes suggests the correction of erroneous behavior. Therefore, the final analysis is **Buggy**."
weka,36953.json,aeff9067d2f9056edfc89df194af976e07e4bd40,"@@ -1,105 +1,106 @@
   public void processIncoming(Data data) throws WekaException {
     Object modelToSave = null;
     Instances modelHeader = null;
     Integer setNum = null;
     Integer maxSetNum = null;
 
     if (data.getConnectionName().equals(StepManager.CON_INCREMENTAL_CLASSIFIER)) {
       if (m_incrementalHeader == null
         && !getStepManager().isStreamFinished(data)) {
         m_incrementalHeader =
           ((Instance) data
             .getPayloadElement(StepManager.CON_AUX_DATA_TEST_INSTANCE))
             .dataset();
       }
       if (getStepManager().isStreamFinished(data)
         || (m_incrementalSaveSchedule > 0
           && m_counter % m_incrementalSaveSchedule == 0 && m_counter > 0)) {
         modelToSave =
           (weka.classifiers.Classifier) data
             .getPayloadElement(StepManager.CON_INCREMENTAL_CLASSIFIER);
+        modelHeader = m_incrementalHeader;
       }
     } else {
       modelToSave = data.getPayloadElement(data.getConnectionName());
       modelHeader =
         (Instances) data
           .getPayloadElement(StepManager.CON_AUX_DATA_TRAININGSET);
       setNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
       maxSetNum =
         (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
       if (modelHeader == null) {
         modelHeader =
           (Instances) data.getPayloadElement(StepManager.CON_AUX_DATA_TESTSET);
       }
     }
 
     if (modelToSave != null) {
       if (modelToSave instanceof UpdateableBatchProcessor) {
         try {
           // make sure model cleans up before saving
           ((UpdateableBatchProcessor) modelToSave).batchFinished();
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
 
       if (modelHeader != null) {
         modelHeader = new Instances(modelHeader, 0);
       }
 
       getStepManager().processing();
       String prefix = getStepManager().environmentSubstitute(m_filenamePrefix);
       String relationName =
         m_includeRelationName && modelHeader != null ? modelHeader
           .relationName() : """";
       String setSpec =
         maxSetNum != null && setNum != null ? ""_"" + setNum + ""_"" + maxSetNum
           + ""_"" : """";
 
       String modelName = modelToSave.getClass().getCanonicalName();
       modelName =
         modelName.substring(modelName.lastIndexOf(""."") + 1, modelName.length());
       String filename = """" + prefix + relationName + setSpec + modelName;
       filename = sanitizeFilename(filename);
 
       String dirName =
         getStepManager().environmentSubstitute(m_directory.toString());
       File tempFile = new File(dirName);
       filename = tempFile.getAbsolutePath() + File.separator + filename;
 
       getStepManager().logBasic(
         ""Saving model "" + modelToSave.getClass().getCanonicalName() + "" to ""
           + filename + "".model"");
       getStepManager().statusMessage(
         ""Saving model: "" + modelToSave.getClass().getCanonicalName());
 
       ObjectOutputStream oos = null;
       try {
         oos =
           new ObjectOutputStream(new BufferedOutputStream(new FileOutputStream(
             new File(filename + "".model""))));
         oos.writeObject(modelToSave);
         if (modelHeader != null) {
           oos.writeObject(modelHeader);
         }
         oos.close();
       } catch (Exception ex) {
         throw new WekaException(ex);
       } finally {
         if (data.getConnectionName() != StepManager.CON_INCREMENTAL_CLASSIFIER
           || getStepManager().isStreamFinished(data)) {
           getStepManager().finished();
         }
         if (oos != null) {
           try {
             oos.close();
           } catch (Exception ex) {
             throw new WekaException(ex);
           }
         }
       }
     }
 
     m_counter++;
   }",Buggy,"Fixed a bug where the training data header was not getting saved for incremental models

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13175 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug was fixed related to the training data header not being saved for incremental models. Let's examine the code changes to evaluate if they align with this claim.

1. **Code Analysis:**
   - Before the change, `modelHeader` was only set for non-incremental classifiers by obtaining it from the payload with `StepManager.CON_AUX_DATA_TRAININGSET`.
   - The diff adds a new line of code `modelHeader = m_incrementalHeader;` for the case where `data.getConnectionName().equals(StepManager.CON_INCREMENTAL_CLASSIFIER)`.
   - This addition makes sure `modelHeader`, representing the training data header, is populated with `m_incrementalHeader` for incremental models, which wasn't happening before.

2. **Implication of Changes:**
   - The new line ensures that when `modelToSave` is an incremental classifier, the `modelHeader` variable correctly captures the header of the training data associated with the incremental model.
   - This aligns with the commit message which states that the issue was the training data header not being saved for incremental models.

3. **Conclusion:**
   - Since the code change directly addresses the issue described in the commit message by ensuring the training data header is saved for incremental models, it clearly indicates a bug fix related to this functionality.

The changes are consistent with the commit message and indicate a correction of an oversight in handling header data for incremental models. Therefore, I conclude the commit as **Buggy**."
weka,9988.json,ff62ff2731fed53f3db748855b97c8c08a893dca,"@@ -1,19 +1,19 @@
   protected synchronized void trimZeroDistances(Neighborhood n) {    
-    int index = 0;
+    int index = n.m_neighbors.numInstances();
     for (int i = 0; i < n.m_neighbors.numInstances(); i++) {
       if (n.m_distances[i] > 0) {
         index = i;
         break;
       }
     }
     
     if (index > 0) {
       // trim zero distances
       for (int i = 0; i < index; i++) {
         n.m_neighbors.remove(0);
       }
       double[] newDist = new double[n.m_distances.length - index];
       System.arraycopy(n.m_distances, index, newDist, 0, newDist.length);
       n.m_distances = newDist;
     }
   }",Buggy,"Fixed a bug in trimZeroDistances()

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9723 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix in the `trimZeroDistances()` method. Let's analyze the changes:

1. **Original Issue**:
   - The `index` was initially set to 0, meaning if no positive distance is found, zero distances would not be trimmed because the method would remove instances up to `index` (which remained 0).

2. **Change Made**:
   - The `index` variable is updated to start at `n.m_neighbors.numInstances()`. This ensures if no positive distances are found throughout the loop, all neighbors with zero distances get removed, as `index` will then be equal to the number of instances, thus trimming all instances with zero distance.

3. **Logical Correction**:
   - The loop correctly assigns the first positive distance's index to the `index` variable, while if no positive distance is detected, all instances are meant to trim, as observed from the changes made.

The alteration aligns with the commit message indicating a bug fix because it delivers appropriate logic for trimming zero distances, addressing previous behavior where zero distances might not have been trimmed correctly depending on `index` assignment.

Therefore, considering the commit message and code changes, this was indeed a bug fix. The answer is **Buggy**."
weka,18815.json,0cd35ea8a7783c8ca9d16a71fc888af2d82c3e90,"@@ -1,69 +1,74 @@
   public static void main(String[] args) {
+
+    weka.core.logging.Logger.log(weka.core.logging.Logger.Level.INFO,
+      ""Logging started"");
+    WekaPackageManager.loadPackages(false, true, true);
+
     try {
       LookAndFeel.setLookAndFeel(WorkbenchDefaults.APP_ID,
         WorkbenchDefaults.APP_ID + "".lookAndFeel"", WorkbenchDefaults.LAF);
     } catch (IOException ex) {
       ex.printStackTrace();
     }
     weka.gui.GenericObjectEditor.determineClasses();
 
     try {
       if (System.getProperty(""os.name"").contains(""Mac"")) {
         System.setProperty(""apple.laf.useScreenMenuBar"", ""true"");
       }
       m_workbench = new WorkbenchApp();
       final javax.swing.JFrame jf =
         new javax.swing.JFrame(""Weka "" + m_workbench.getApplicationName());
       jf.getContentPane().setLayout(new java.awt.BorderLayout());
 
       Image icon =
         Toolkit.getDefaultToolkit().getImage(
           WorkbenchApp.class.getClassLoader().getResource(
             ""weka/gui/weka_icon_new_48.png""));
       jf.setIconImage(icon);
 
       jf.getContentPane().add(m_workbench, BorderLayout.CENTER);
       jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
       jf.pack();
       m_workbench.showMenuBar(jf);
       jf.setSize(1024, 768);
       jf.setVisible(true);
 
       if (args.length == 1) {
         System.err.println(""Loading instances from "" + args[0]);
         AbstractFileLoader loader = ConverterUtils.getLoaderForFile(args[0]);
         loader.setFile(new File(args[0]));
         m_workbench.getPerspectiveManager().getMainPerspective()
           .setInstances(loader.getDataSet());
       }
 
       Thread memMonitor = new Thread() {
         @Override
         public void run() {
           while (true) {
             // try {
             // System.out.println(""Before sleeping."");
             // Thread.sleep(10);
 
             if (m_Memory.isOutOfMemory()) {
               // clean up
               jf.dispose();
               m_workbench = null;
               System.gc();
 
               // display error
               System.err.println(""\ndisplayed message:"");
               m_Memory.showOutOfMemory();
               System.err.println(""\nexiting"");
               System.exit(-1);
             }
           }
         }
       };
 
       memMonitor.setPriority(Thread.MAX_PRIORITY);
       memMonitor.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed a weird class loading bug that affected loading RPlugin

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15118 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a ""weird class loading bug"" was fixed. The diff shows added logging and package loading before setting up the UI and error monitoring within the `main` method. Specifically:

1. **Logging Start**: The line `weka.core.logging.Logger.log(weka.core.logging.Logger.Level.INFO, ""Logging started"");` initializes logging, which could help trace class loading and execution flow, improving diagnosis in case of class loading issues.

2. **Package Manager Load**: The line `WekaPackageManager.loadPackages(false, true, true);` ensures that packages are loaded explicitly. This step could be essential for resolving class loading issues by making sure that necessary resources or plugins are pre-loaded.

Although the diff doesn't show explicit changes directly addressing class loading processes (like modifying class loaders or paths), logging initiation and explicit package loading are reasonable actions to tackle certain types of class loading anomalies. These changes align with the commit message aimed at fixing a class loading bug.

Considering this modification indirectly addresses the class loading aspect by ensuring necessary components are loaded at the correct time and adds logging for better traceability, the commit indeed reflects a fix to a class loading problem.

Therefore, this diff is consistent with the commit message as a bug fix. The verdict is **Buggy**."
weka,30927.json,95a3f1d8e4e0087ec7760ebed797b134337b1feb,"@@ -1,31 +1,32 @@
   protected static String makeOptionStr(AbstractFileLoader loader) {
     StringBuffer result;
     Option option;
 
     result = new StringBuffer(""\nUsage:\n"");
     result.append(""\t"" + loader.getClass().getName().replaceAll("".*\\."", """"));
-    if (loader instanceof OptionHandler) {
-      result.append("" [options]"");
-    }
     result.append("" <"");
     String[] ext = loader.getFileExtensions();
     for (int i = 0; i < ext.length; i++) {
       if (i > 0) {
         result.append("" | "");
       }
       result.append(""file"" + ext[i]);
     }
-    result.append("">\n"");
+    result.append("">"");
+    if (loader instanceof OptionHandler) {
+      result.append("" [options]"");
+    }
+    result.append(""\n"");
 
     if (loader instanceof OptionHandler) {
       result.append(""\nOptions:\n\n"");
       Enumeration<Option> enm = ((OptionHandler) loader).listOptions();
       while (enm.hasMoreElements()) {
         option = enm.nextElement();
         result.append(option.synopsis() + ""\n"");
         result.append(option.description() + ""\n"");
       }
     }
 
     return result.toString();
   }",Buggy,"Fixed output bug in command-line help: file name needs to be given *before* options.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12103 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message claims a bug fix in the command-line help output concerning the order of file names and options. The original code placed ""[options]"" before the file name in the usage string. The change moves ""[options]"" to after the file name, aligning with bug description. Correct usage order enhances user comprehension and conforms to conventional command-line syntax. Thus, the change fixes the output bug mentioned in the commit message. The conclusion is **Buggy**."
weka,12790.json,bfb54a334c1bdae9044c51ab3cd1319734e8cab4,"@@ -1,98 +1,100 @@
     public void buildClusterer(Instances data) throws Exception
   {
     reset();
     meanInstance = new DenseInstance(data.numAttributes());
     for (int i = 0; i < data.numAttributes(); i++)
       meanInstance.setValue(i, data.meanOrMode(i));
     numInstances = data.numInstances();
 
     kMeans.setDistanceFunction(distanceFunction);
     kMeans.setMaxIterations(maxIterations);
     //    kMeans.setInitializeUsingKMeansPlusPlusMethod(initializeWithKMeansPlusPlus);
-    kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    if (initializeWithKMeansPlusPlus) {
+      kMeans.setInitializationMethod(new weka.core.SelectedTag(SimpleKMeans.KMEANS_PLUS_PLUS, SimpleKMeans.TAGS_SELECTION));
+    }
 
     /**
      * step 1: iterate over all restarts and possible k values, record CH-scores
      */
     Random r = new Random(m_Seed);
     double meanCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     double maxCHs[] = new double[maxNumClusters + 1 - minNumClusters];
     int maxSeed[] = new int[maxNumClusters + 1 - minNumClusters];
 
     for (int i = 0; i < restarts; i++)
       {
         if (printDebug)
           System.out.println(""cascade> restarts: "" + (i + 1) + "" / "" + restarts);
 
         for (int k = minNumClusters; k <= maxNumClusters; k++)
           {
             if (printDebug)
               System.out.print(""cascade>  k:"" + k + "" "");
 
             int seed = r.nextInt();
             kMeans.setSeed(seed);
             kMeans.setNumClusters(k);
             kMeans.buildClusterer(data);
             double ch = getCalinskiHarabasz();
 
             int index = k - minNumClusters;
             meanCHs[index] = (meanCHs[index] * i + ch) / (double) (i + 1);
             if (i == 0 || ch > maxCHs[index])
               {
                 maxCHs[index] = ch;
                 maxSeed[index] = seed;
               }
 
             if (printDebug)
               System.out.println("" CH:"" + df.format(ch) + ""  W:""
                                  + df.format(kMeans.getSquaredError() / (double) (numInstances - kMeans.getNumClusters()))
                                  + "" (unweighted:"" + df.format(kMeans.getSquaredError()) + "")  B:""
                                  + df.format(getSquaredErrorBetweenClusters() / (double) (kMeans.getNumClusters() - 1))
                                  + "" (unweighted:"" + df.format(getSquaredErrorBetweenClusters()) + "") "");
           }
       }
     if (printDebug)
       {
         String s = ""cascade> max CH: [ "";
         for (int i = 0; i < maxSeed.length; i++)
           s += df.format(maxCHs[i]) + "" "";
         System.out.println(s + ""]"");
       }
     String s = ""cascade> mean CH: [ "";
     for (int i = 0; i < maxSeed.length; i++)
       s += df.format(meanCHs[i]) + "" "";
 
     finalMeanCH = s + ""]"";
     //    System.out.println(s + ""]"");
 
     /**
      * step 2: select k with best mean CH-score; select seed for max CH score for this k
      */
     int bestK = -1;
     double maxCH = -1;
     for (int k = minNumClusters; k <= maxNumClusters; k++)
       {
         int index = k - minNumClusters;
         if (bestK == -1 || meanCHs[index] > maxCH)
           {
             maxCH = meanCHs[index];
             bestK = k;
           }
       }
     if (manuallySelectNumClusters)
       {
         int selectedK = selectKManually(meanCHs, bestK);
         if (selectedK != -1)
           bestK = selectedK;
       }
     int bestSeed = maxSeed[bestK - minNumClusters];
 
     finalBestK = bestK;
     finalBestSeed = bestSeed;
     //    System.out.println(""cascade> k (yields highest mean CH): "" + bestK);
     //    System.out.println(""cascade> seed (highest CH for k="" + bestK + "") : "" + bestSeed);
 
     kMeans.setSeed(bestSeed);
     kMeans.setNumClusters(bestK);
     kMeans.buildClusterer(data);
   }",Buggy,"Fixed a small bug introduced with the last change.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10457 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a small bug was fixed, and the diff shows exactly how. The change adds an if-check to only set the initialization method to KMeans++ when `initializeWithKMeansPlusPlus` is true. Previously, it was always set to KMeans++, ignoring the `initializeWithKMeansPlusPlus` boolean. This could lead to incorrect behavior if the initialization method should not be KMeans++ under certain conditions.

### Analysis:
- **Logical Correction**: The original logic forcefully set the initialization method without respecting the `initializeWithKMeansPlusPlus` flag, which meant that it did not honor the intended behavior of conditionally setting the method based on the flag's value.
- **Condition Check Update**: The addition of the if-condition corrects this, allowing the program flow to reflect the intention stated by `initializeWithKMeansPlusPlus`. This ensures that the method is only used when it is deliberately desired, preventing unwanted behavior resulting from ignoring the flag.

This aligns well with the commit message and demonstrates a concrete bug fix rather than just a refactoring or optimization. Hence, the changes made in this diff indicate a bug fix as described in the commit message.

Therefore, the changes are **Buggy**."
weka,31032.json,4ab412e6138508ec825027ffab22b044aedd6a09,"@@ -1,9 +1,11 @@
   private void initTokenizer(StreamTokenizer tokenizer) {
     tokenizer.resetSyntax();         
     tokenizer.whitespaceChars(0, (' '-1));    
     tokenizer.wordChars(' ','\u00FF');
     tokenizer.whitespaceChars(',',',');
     tokenizer.whitespaceChars('\t','\t');
     tokenizer.commentChar('%');
+    tokenizer.quoteChar('""');
+    tokenizer.quoteChar('\'');
     tokenizer.eolIsSignificant(true);
   }",Buggy,"Fixed bug introduced in r4914, where handling of quoted strings got deleted.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5150 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug was introduced related to handling quoted strings. The code modifications in the diff reinstate functionality by adding `quoteChar('""')` and `quoteChar('\'')` to handle strings enclosed in quotes. This aligns with the commit message indicating a restoration of missing quoted string handling. Therefore, these changes aim to fix the described bug.

The changes focus on improving the code's ability to correctly parse and handle quoted strings, a sign of a bug fix. Given that the commit message and the code changes are consistent in addressing a bug about quoted string handling, the conclusion is **Buggy**."
weka,28754.json,4718fad5f7d375b9878d9d345d7cefc62c223c70,"@@ -1,25 +1,26 @@
   protected synchronized void completedClassifier(int iteration,
       boolean success) {
-    m_completed++;
 
     if (!success) {
       m_failed++;
       if (m_Debug) {
         System.err.println(""Iteration "" + iteration + "" failed!"");
       }
+    } else {
+      m_completed++;
     }
 
     if (m_completed + m_failed == m_Classifiers.length) {
       if (m_failed > 0) {
         if (m_Debug) {
           System.err.println(""Problem building classifiers - some iterations failed."");
         }
       }
 
       // have to shut the pool down or program executes as a server
       // and when running from the command line does not return to the
       // prompt
       m_executorPool.shutdown();
       block(false);
     }
   }",Buggy,"Fixed a bug that prevented the sum of completed and failed classifiers from equaling the total number of classifiers when failures occur. This bug only affected the case when one or more failures occurred.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6266 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes a bug where the sum of completed and failed classifiers didn't match the total classifiers when failures occurred. The diff shows the `m_completed` counter being incremented only if `success` is true, fixing the logic to accumulate correctly based on success/failure. This directly resolves the described bug. The answer is **Buggy**."
weka,36601.json,2b4d1e40b90f0675df910618594260c237945fec,"@@ -1,3 +1,3 @@
-  public boolean getStepMustRunSingleThreaded() {
+  public boolean stepMustRunSingleThreaded() {
     return getStepManager().getStepMustRunSingleThreaded();
   }",Buggy,"Fixed a bug that could cause Knowledge Flow files to fail to load. This was inadvertently introduced with the single threaded executor addition

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13700 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix related to Knowledge Flow files failing to load. The diff shows a minor change: renaming `getStepMustRunSingleThreaded` to `stepMustRunSingleThreaded`. This could be part of a broader fix, but the change alone doesn't reveal a direct bug fix. The method's name change doesn't align clearly with fixing a bug unless more context shows a call site inconsistency or logical error was corrected. Without additional information showing that this resolves a specific issue, the change appears more stylistic or a minor refactor. Thus, based on the provided diff alone:

The answer is **NotBuggy**."
weka,1770.json,81174005aadc513eaeddade0fcb41f7c02f7cf21,"@@ -1,106 +1,107 @@
   protected void makeTree(Instances data, int totalInstances,
     int[][] sortedIndices, double[][] weights, double[] classProbs,
     double totalWeight, double minNumObj, boolean useHeuristic)
     throws Exception {
 
     // if no instances have reached this node (normally won't happen)
     if (totalWeight == 0) {
       m_Attribute = null;
       m_ClassValue = Utils.missingValue();
       m_Distribution = new double[data.numClasses()];
       return;
     }
 
     m_totalTrainInstances = totalInstances;
     m_isLeaf = true;
+    m_Successors = null;
 
     m_ClassProbs = new double[classProbs.length];
     m_Distribution = new double[classProbs.length];
     System.arraycopy(classProbs, 0, m_ClassProbs, 0, classProbs.length);
     System.arraycopy(classProbs, 0, m_Distribution, 0, classProbs.length);
     if (Utils.sum(m_ClassProbs) != 0) {
       Utils.normalize(m_ClassProbs);
     }
 
     // Compute class distributions and value of splitting
     // criterion for each attribute
     double[][][] dists = new double[data.numAttributes()][0][0];
     double[][] props = new double[data.numAttributes()][0];
     double[][] totalSubsetWeights = new double[data.numAttributes()][2];
     double[] splits = new double[data.numAttributes()];
     String[] splitString = new String[data.numAttributes()];
     double[] giniGains = new double[data.numAttributes()];
 
     // for each attribute find split information
     for (int i = 0; i < data.numAttributes(); i++) {
       Attribute att = data.attribute(i);
       if (i == data.classIndex()) {
         continue;
       }
       if (att.isNumeric()) {
         // numeric attribute
         splits[i] = numericDistribution(props, dists, att, sortedIndices[i],
           weights[i], totalSubsetWeights, giniGains, data);
       } else {
         // nominal attribute
         splitString[i] = nominalDistribution(props, dists, att,
           sortedIndices[i], weights[i], totalSubsetWeights, giniGains, data,
           useHeuristic);
       }
     }
 
     // Find best attribute (split with maximum Gini gain)
     int attIndex = Utils.maxIndex(giniGains);
     m_Attribute = data.attribute(attIndex);
 
     m_train = new Instances(data, sortedIndices[attIndex].length);
     for (int i = 0; i < sortedIndices[attIndex].length; i++) {
       Instance inst = data.instance(sortedIndices[attIndex][i]);
       Instance instCopy = (Instance) inst.copy();
       instCopy.setWeight(weights[attIndex][i]);
       m_train.add(instCopy);
     }
 
     // Check if node does not contain enough instances, or if it can not be
     // split,
     // or if it is pure. If does, make leaf.
     if (totalWeight < 2 * minNumObj || giniGains[attIndex] == 0
       || props[attIndex][0] == 0 || props[attIndex][1] == 0) {
       makeLeaf(data);
     }
 
     else {
       m_Props = props[attIndex];
       int[][][] subsetIndices = new int[2][data.numAttributes()][0];
       double[][][] subsetWeights = new double[2][data.numAttributes()][0];
 
       // numeric split
       if (m_Attribute.isNumeric()) {
         m_SplitValue = splits[attIndex];
       } else {
         m_SplitString = splitString[attIndex];
       }
 
       splitData(subsetIndices, subsetWeights, m_Attribute, m_SplitValue,
         m_SplitString, sortedIndices, weights, data);
 
       // If split of the node results in a node with less than minimal number of
       // isntances,
       // make the node leaf node.
       if (subsetIndices[0][attIndex].length < minNumObj
         || subsetIndices[1][attIndex].length < minNumObj) {
         makeLeaf(data);
         return;
       }
 
       // Otherwise, split the node.
       m_isLeaf = false;
       m_Successors = new SimpleCart[2];
       for (int i = 0; i < 2; i++) {
         m_Successors[i] = new SimpleCart();
         m_Successors[i].makeTree(data, m_totalTrainInstances, subsetIndices[i],
           subsetWeights[i], dists[attIndex][i],
           totalSubsetWeights[attIndex][i], minNumObj, useHeuristic);
       }
     }
   }",Buggy,"Fixed problem where unprune() step caused root node to not be labeled a leaf when it really is a leaf. This only caused problems when unpruned tree consists of a single node that is a leaf node.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix to ensure that when an unpruned tree consists of a single node, it is correctly labeled as a leaf. Let's examine the code diff to verify this change:

1. **Initialization of `m_isLeaf` and `m_Successors`:** The line `m_isLeaf = true;` explicitly sets the current node to a leaf by default. The addition of `m_Successors = null;` ensures clarity that this node does not have any child nodes initially.

2. **Leaf Creation Conditions:**
   - In the condition where the node should be a leaf (`totalWeight < 2 * minNumObj || giniGains[attIndex] == 0 || props[attIndex][0] == 0 || props[attIndex][1] == 0`), the method `makeLeaf(data)` is called to establish the node as a leaf. This upholds the existing logic for node classification.
   - Moreover, if the split of the node results in less than the minimal number of instances (`subsetIndices[0][attIndex].length < minNumObj || subsetIndices[1][attIndex].length < minNumObj`), the node should also become a leaf, with `makeLeaf(data)` invoked before returning.

3. **Setting Node as Non-Leaf When Split:**
   - If none of the conditions for marking the node as a leaf are met, the split proceeds, setting `m_isLeaf = false;` and initializing successors (`m_Successors`).

The introduction of `m_Successors = null;` initially ensures that a single node, potentially unvisited by any splitting logic due to its status as inherently a leaf, is indeed classified correctly. The other changes in the code maintain the existing logic for when to subdivide a node.

This update aligns with the commit message's goal to rectify an issue with singular leaf node trees. It ensures the lone node is appropriately marked as a leaf when conditions dictate.

**Conclusion: Buggy**"
weka,17814.json,34abf3851dd64f8c49bdd8875debb6ec45310dc5,"@@ -1,27 +1,33 @@
   public void setInstancesFromFileQ() {
     
 //     if (m_IOThread == null) {
       int returnVal = m_FileChooser.showOpenDialog(this);
       if (returnVal == JFileChooser.APPROVE_OPTION) {
 	File selected = m_FileChooser.getSelectedFile();
 	
 	try
 	{
 	java.io.Reader r = new java.io.BufferedReader(
 				new java.io.FileReader(selected));
 	Instances i = new Instances(r);
 	setInstances(i);
 	
 	//dataFileLabel.setText(selected.getName());
-	dataFileLabel.setText(i.relationName());
+	String relationName = i.relationName();
+	String truncatedN = relationName;
+	if (relationName.length() > 25) {
+	  truncatedN = relationName.substring(0, 25) + ""..."";
+	}
+	dataFileLabel.setText(truncatedN);
+	dataFileLabel.setToolTipText(relationName);
 	} catch (Exception e)
 	{
 		JOptionPane.showMessageDialog(this,""Can't load at this time,\n""
 				    + ""currently busy with other IO"",
 				    ""Load Instances"",
 				    JOptionPane.WARNING_MESSAGE);
 		    e.printStackTrace();
 	
 	}
       }
   }",Buggy,"Fixed a bug that caused a widget layout problem when loading a dataset with a long relation name.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6482 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions a bug fix for a layout problem when a dataset has a long relation name. The code change aligns with this message by truncating relation names longer than 25 characters and appending ""..."" to avoid layout issues. Additionally, it sets the full relation name as a tooltip, preserving the complete information. These changes suggest a UI-related bug fix by preventing layout distortion caused by overly long strings. Therefore, the answer is **Buggy**."
weka,24248.json,3a2501745298eb32d9cb3350d07eb15e93716bb9,"@@ -1,13 +1,19 @@
   public void calculateDerived() {
 
     mean = Double.NaN;
     stdDev = Double.NaN;
     if (count > 0) {
       mean = sum / count;
+      stdDev = Double.POSITIVE_INFINITY;
       if (count > 1) {
 	stdDev = sumSq - (sum * sum) / count;
 	stdDev /= (count - 1);
+        if (stdDev < 0) {
+          System.err.println(""Warning: stdDev value = "" + stdDev 
+                             + "" -- rounded to zero."");
+          stdDev = 0;
+        }
 	stdDev = Math.sqrt(stdDev);
       }
     }
   }",Buggy,"Fixed some NaN problems with Std Devs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@714 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates problems with NaN values in standard deviations. The changes address these by initializing `stdDev` to `Double.POSITIVE_INFINITY` and adding a correction mechanism: it checks if `stdDev` is negative—which could occur due to floating-point precision issues—and rounds it to zero with a warning. This handling of potential negative values in standard deviation calculations aligns with the commit message, indicating a fix for issues related to NaN or mathematically incorrect values.

Therefore, the changes in the code are directly tackling the problem mentioned in the commit message. This diff demonstrates logical corrections and error-handling improvements relevant to the described problem.

The assessment of the changes aligns well with the intent described in the commit message, so we conclude with **Buggy**."
weka,23181.json,2e12f8aa73f943180eb6c082e92698d3281baf35,"@@ -1,8 +1,23 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     if (filter == null) {
       m_AssociatorEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     m_AssociatorEditor.setCapabilitiesFilter(filter);
+    
+    m_StartBut.setEnabled(true);
+    // Check capabilities
+    Capabilities currentFilter = m_AssociatorEditor.getCapabilitiesFilter();
+    Associator associator = (Associator) m_AssociatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (associator != null && currentFilter != null && 
+        (associator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)associator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5384 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to updating the enabled/disabled state of a start button when a new data set is set on the panel. The changes in the diff reflect this by implementing logic to enable the button and conditionally disable it based on capability checking.

1. **Setting Enabled State**: The line `m_StartBut.setEnabled(true);` ensures the start button is enabled initially whenever `updateCapabilitiesFilter` is called.

2. **Conditional Disabling**: The added logic checks the associator's capabilities against the current filter:
   - Retrieves the current capabilities filter and associator.
   - If the associator is a `CapabilitiesHandler`, it retrieves its capabilities.
   - Disables the button if the associator's capabilities do not match the filter.

This change directly aligns with the commit message by ensuring that the start button updates its state appropriately when the data set changes, correcting previously incorrect behavior. Therefore, the update is consistent with fixing a bug, as described in the commit message.

The conclusion is **Buggy**."
weka,21328.json,be43f6517961a36acbbc66eeb4fe2bc861f6c154,"@@ -1,8 +1,13 @@
   public Object getValue() {
     String path = getAsText();
     if (path != null && path.length() > 0) {
       return new File(path);
     }
 
-    return new File(""."");
+    JFileChooser embeddedEditor = (JFileChooser) m_fileEditor.getCustomEditor();
+    if (embeddedEditor.getFileSelectionMode() == JFileChooser.DIRECTORIES_ONLY) {
+      return new File(""."");
+    } else {
+      return new File("""");
+    }
   }",Buggy,"Fixed a bug where '.' was returned when the field was empty in the case where the editor has been configured for files only (rather than directories)

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13132 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The changes in the `getValue` method update the return behavior when the input path is empty. Initially, the method always returned `new File(""."")`, which represents the current directory, regardless of the editor's file selection mode. This behavior would be incorrect if the editor is configured for files only, not directories.

The fix involves checking the file selection mode of the `JFileChooser` embedded editor:
- If the mode is `DIRECTORIES_ONLY`, the method returns `new File(""."")`, which is consistent with directory selection.
- Otherwise, it returns `new File("""")` for cases when the editor is configured for file selection only, thus avoiding returning the current directory when the field is meant to represent files.

This change aligns with the commit message stating the fixed behavior when dealing with empty fields, specifically for the files-only configuration. Therefore, the modifications clearly address the described bug.

**Buggy**"
weka,6470.json,bae2e9e85c74323389279cac539860e8b88d49d2,"@@ -1,27 +1,26 @@
             public void run() {
               PythonSession session;
-              m_executeScriptBut.setEnabled(false);
               try {
                 session = PythonSession.acquireSession(PythonPanel.this);
-                m_logPanel.statusMessage(""Executing script..."");
                 List<String> outAndErr =
                   session.executeScript(m_scriptEditor.getText(),
                     m_debug.isSelected());
                 if (outAndErr.get(0).length() > 0) {
                   logMessage(outAndErr.get(0), null);
                 }
                 if (outAndErr.get(1).length() > 0) {
                   throw new WekaException(outAndErr.get(1));
                 }
                 refreshVarList(session);
                 checkDebug(session);
                 m_logPanel.statusMessage(""OK"");
               } catch (WekaException ex) {
                 ex.printStackTrace();
                 logMessage(null, ex);
                 m_logPanel.statusMessage(""An error occurred. See log."");
               } finally {
                 PythonSession.releaseSession(PythonPanel.this);
                 m_executeScriptBut.setEnabled(true);
+                revalidate();
               }
             }",Buggy,"Fixed a refresh bug that affected the variables list

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11810 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a refresh bug affecting the variables list. The code changes include adding `revalidate()` in the `finally` block after enabling the script execution button. The `revalidate()` call suggests an update to the user interface layout, likely to refresh or update the display correctly after the script execution, which can be necessary if the change to the variables list wasn't appearing due to the UI not being refreshed. This closely aligns with the commit message, as it suggests a refresh issue was present and solved here.

Hence, these changes indicate a bug fix. The conclusion is **Buggy**."
weka,31459.json,3b4b438af6d33017965dabcd7943e5de02464e2c,"@@ -1,29 +1,29 @@
   void forceDeleteAttributeAt(int position) {
 
     int index = locateIndex(position);
 
     m_NumAttributes--;
     if ((index >= 0) && (m_Indices[index] == position)) {
       int[] tempIndices = new int[m_Indices.length - 1];
       double[] tempValues = new double[m_AttValues.length - 1];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index);
       for (int i = index; i < m_Indices.length - 1; i++) {
 	tempIndices[i] = m_Indices[i + 1] - 1;
 	tempValues[i] = m_AttValues[i + 1];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     } else {
       int[] tempIndices = new int[m_Indices.length];
       double[] tempValues = new double[m_AttValues.length];
       System.arraycopy(m_Indices, 0, tempIndices, 0, index + 1);
       System.arraycopy(m_AttValues, 0, tempValues, 0, index + 1);
-      for (int i = index + 1; i < m_Indices.length - 1; i++) {
+      for (int i = index + 1; i < m_Indices.length; i++) {
 	tempIndices[i] = m_Indices[i] - 1;
 	tempValues[i] = m_AttValues[i];
       }
       m_Indices = tempIndices;
       m_AttValues = tempValues;
     }
   }",Buggy,"Fixed bug in SparseInstance in forceAttributeDelete.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1236 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates fixing a bug in `SparseInstance`'s `forceDeleteAttributeAt` method. The diff changes a loop's end condition in an ""else"" block from `i < m_Indices.length - 1` to `i < m_Indices.length`. 

Examining the provided code, it appears that the loop aims to process elements beyond `index + 1` in `m_Indices` and `m_AttValues` without performing a deletion. The original bug might've been due to incorrectly terminating one iteration early, skipping the last element's processing. This could lead to incorrect attribute deletions or incorrect management of attributes within the array.

Thus, by correctly iterating through the full range (`i < m_Indices.length`), the code correctly processes all relevant indices, resolving the identified bug. Therefore, the commit message is consistent with the code changes, which correct a logical error in array processing during attribute deletion.

This assessment concludes with **Buggy**."
weka,29293.json,64d11bc2714c068b7bdba2410a82a0d50c430b55,"@@ -1,24 +1,24 @@
   public void setOptions(String[] options) throws Exception {
     String tmpStr;
 
     tmpStr = Utils.getOption('R', options);
     if (tmpStr.length() != 0) {
       setCombinationRule(new SelectedTag(tmpStr, TAGS_RULES));
     } else {
       setCombinationRule(new SelectedTag(AVERAGE_RULE, TAGS_RULES));
     }
 
     m_classifiersToLoad.clear();
     while (true) {
       String loadString = Utils.getOption('P', options);
       if (loadString.length() == 0) {
         break;
       }
 
       m_classifiersToLoad.add(loadString);
     }
 
-    setDoNotPrintModels(Utils.getFlag(""-do-not-print"", options));
+    setDoNotPrintModels(Utils.getFlag(""do-not-print"", options));
 
     super.setOptions(options);
   }",Buggy,"Fixed an option handling bug

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13518 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The modification in the code involves changing the option flag string from ""-do-not-print"" to ""do-not-print"" when calling `Utils.getFlag`. This alteration corrects the syntax for option parsing, eliminating the incorrect requirement for the flag to have a hyphen.

Here's a step-by-step analysis of why this indicates a bug fix:

1. **Original Code**: 
   - `Utils.getFlag(""-do-not-print"", options);`: This suggests that the option for ""do-not-print"" was incorrectly being looked for with a hyphen prefixed to it.

2. **Revised Code**: 
   - `Utils.getFlag(""do-not-print"", options);`: Removes the hyphen, making it align with the typical approach in command line options handled by flags.

3. **Implication**: 
   - The change signifies that previously, the code likely failed to recognize the ""do-not-print"" option correctly unless it was mistakenly prefixed with a hyphen in the options array.
   - This was a functional bug as it could cause unexpected behavior by not setting flags correctly based on the provided options.

4. **Outcome**: 
   - The fix ensures that the intended flag is captured accurately, preserving and activating the corresponding logic in the program when ""do-not-print"" is provided.

This correction aligns with the commit message indicating the resolution of an option handling bug. Thus, the conclusion is **Buggy**."
weka,9850.json,feaeb8e65657f876489351748361cd4cbc9a7fe5,"@@ -1,104 +1,107 @@
   protected boolean makeTree(FastVector BestFirstElements, BFTree root,
       Instances train, int[][] sortedIndices, double[][] weights,
       double[][][] dists, double[] classProbs, double totalWeight,
       double[] branchProps, int minNumObj, boolean useHeuristic, boolean useGini)
   throws Exception {
 
     if (BestFirstElements.size()==0) return false;
 
     ///////////////////////////////////////////////////////////////////////
     // All information about the node to split (first BestFirst object in
     // BestFirstElements)
     FastVector firstElement = (FastVector)BestFirstElements.elementAt(0);
 
     // node to split
     BFTree nodeToSplit = (BFTree)firstElement.elementAt(0);
 
     // split attribute
     Attribute att = (Attribute)firstElement.elementAt(1);
 
     // info of split value or split string
     double splitValue = Double.NaN;
     String splitStr = null;
     if (att.isNumeric())
       splitValue = ((Double)firstElement.elementAt(2)).doubleValue();
     else {
       splitStr=((String)firstElement.elementAt(2)).toString();
     }
 
     // the best gini gain or information gain of this node
     double gain = ((Double)firstElement.elementAt(3)).doubleValue();
     ///////////////////////////////////////////////////////////////////////
 
     // If no enough data to split for this node or this node can not be split find next node to split.
     if (totalWeight < 2*minNumObj || branchProps[0]==0
 	|| branchProps[1]==0) {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.makeLeaf(train);
+      if (BestFirstElements.size() == 0) {
+        return false;
+      }
       BFTree nextNode = (BFTree)
       ((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
       return root.makeTree(BestFirstElements, root, train,
 	  nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	  nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	  nextNode.m_Props, minNumObj, useHeuristic, useGini);
     }
 
     // If gini gain or information is 0, make all nodes in the BestFirstElements leaf nodes
     // because these node sorted descendingly according to gini gain or information gain.
     // (namely, gini gain or information gain of all nodes in BestFirstEelements is 0).
     if (gain==0) {
       for (int i=0; i<BestFirstElements.size(); i++) {
 	FastVector element = (FastVector)BestFirstElements.elementAt(i);
 	BFTree node = (BFTree)element.elementAt(0);
 	node.makeLeaf(train);
       }
       BestFirstElements.removeAllElements();
       return false;
     }
 
     else {
       // remove the first element
       BestFirstElements.removeElementAt(0);
       nodeToSplit.m_Attribute = att;
       if (att.isNumeric()) nodeToSplit.m_SplitValue = splitValue;
       else nodeToSplit.m_SplitString = splitStr;
 
       int[][][] subsetIndices = new int[2][train.numAttributes()][0];
       double[][][] subsetWeights = new double[2][train.numAttributes()][0];
 
       splitData(subsetIndices, subsetWeights, nodeToSplit.m_Attribute,
 	  nodeToSplit.m_SplitValue, nodeToSplit.m_SplitString,
 	  nodeToSplit.m_SortedIndices, nodeToSplit.m_Weights, train);
 
       // if split will generate node(s) which has total weights less than m_minNumObj,
       // do not split
       int attIndex = att.index();
       if (subsetIndices[0][attIndex].length<minNumObj ||
 	  subsetIndices[1][attIndex].length<minNumObj) {
 
 	nodeToSplit.makeLeaf(train);
 	BFTree nextNode = (BFTree)
 	((FastVector)BestFirstElements.elementAt(0)).elementAt(0);
 	return root.makeTree(BestFirstElements, root, train,
 	    nextNode.m_SortedIndices, nextNode.m_Weights, nextNode.m_Dists,
 	    nextNode.m_ClassProbs, nextNode.m_TotalWeight,
 	    nextNode.m_Props, minNumObj, useHeuristic, useGini);
       }
 
       // split the node
       else {
 	nodeToSplit.m_isLeaf = false;
 	nodeToSplit.m_Attribute = att;
 
 	nodeToSplit.makeSuccessors(BestFirstElements,train,subsetIndices,
 	    subsetWeights,dists, nodeToSplit.m_Attribute,useHeuristic,useGini);
 
 	for (int i=0; i<2; i++){
 	  nodeToSplit.m_Successors[i].makeLeaf(train);
 	}
 
 	return true;
       }
     }
   }",Buggy,"Fixed a bug that would occur occasionally when PREPRUNING was selected

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6949 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The changes made in the code are consistent with the commit message, which states a bug fix when the ""PREPRUNING"" option is selected. Here's a breakdown:

1. **Contextual Understanding**: 
   - The method `makeTree` attempts to split nodes of a decision tree using a breadth-first approach (indicated by `FastVector BestFirstElements`).
   - If the conditions for splitting a node are not met (e.g., insufficient data), the node is converted to a leaf and the next node is considered.
   - The commit message's reference to a bug suggests errors in scenarios typically handled by prepruning logic, where nodes should not split under certain thresholds.

2. **Code Analysis**:
   - The existing code converts a node to a leaf and removes it from `BestFirstElements` when a split cannot occur and attempts to process the next node. 
   - **Addition (+):** An additional check was introduced:
     ```java
     if (BestFirstElements.size() == 0) {
       return false;
     }
     ```
     This ensures when the `BestFirstElements` is empty, the method exits early. This change is crucial because it prevents further processing on the assumption that there are elements left, which would lead to errors.
   - The rest of the logic branches dependent on `BestFirstElements` size assume there is at least one element to process, exemplifying a potential bug if this size isn't checked following a `removeElementAt` operation.

3. **Logical Correction**:
   - This fix effectively avoids errors when trying to access elements from an empty list after removing all nodes that couldn't split. This directly aligns with addressing 'occasionally occurring bugs,' likely resulting from boundary conditions related to the prepruning decision logic.

In conclusion, the changes made in the code align well with the provided commit message, as they address a specific bug occurring under certain conditions during the tree building process using prepruning. The fix ensures robustness by checking for an empty list before attempting further tree creation steps. Therefore, the conclusion is **Buggy**."
weka,27361.json,84effa0146bd9bb228110c26aaf7a40f959c9043,"@@ -1,3 +1,3 @@
-  public double [][] coefficents() {
+  public double [][] coefficients() {
     return m_Par;
   }",Buggy,"Fixed spelling mistake in method to get coefficients :-)


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a spelling correction for a method name. The diff shows the method name changing from `coefficents` to `coefficients`, which aligns with the message and accurately describes a simple spelling fix. This change aims for code clarity and correctness in the method naming convention, but it doesn't affect functional behavior or fix a bug; it only corrects a typographical error in the method's name.

Therefore, the answer is **NotBuggy**."
weka,13928.json,83d877a74c163b3242c845bd9ecc4d2857ef2fa1,"@@ -1,33 +1,37 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     int [] arrIdc = new int[m_BaseClassifiers.length+1];
     arrIdc[m_BaseClassifiers.length]=m_MetaFormat.numAttributes()-1;
     double [] classProbs = new double[m_BaseFormat.numClasses()];
     Instance newInst;
     double sum=0;
 
     for (int i = 0; i<m_MetaClassifiers.length; i++) {
       for (int j = 0; j<m_BaseClassifiers.length; j++)
           arrIdc[j]=m_BaseFormat.numClasses()*j+i;
 
-      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_makeIndicatorFilter.setAttributeIndex("""" + (m_MetaFormat.classIndex() + 1));
+      m_makeIndicatorFilter.setNumeric(true);
       m_makeIndicatorFilter.setValueIndex(i);
-
+      m_makeIndicatorFilter.setInputFormat(m_MetaFormat);
       m_makeIndicatorFilter.input(metaInstance(instance));
       m_makeIndicatorFilter.batchFinished();
       newInst = m_makeIndicatorFilter.output();
+
+      m_attrFilter.setAttributeIndicesArray(arrIdc);
+      m_attrFilter.setInvertSelection(true);
       m_attrFilter.setInputFormat(m_makeIndicatorFilter.getOutputFormat());
       m_attrFilter.input(newInst);
       m_attrFilter.batchFinished();
       newInst = m_attrFilter.output();
 
       classProbs[i]=m_MetaClassifiers[i].classifyInstance(newInst);
       if (classProbs[i]>1) { classProbs[i]=1; }
       if (classProbs[i]<0) { classProbs[i]=0; }
       sum+= classProbs[i];
     }
 
     if (sum!=0) Utils.normalize(classProbs,sum);
 
     return classProbs;
   }",Buggy,"Fixed problem resulting from changing Range.java


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1815 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message specifies a fix involving a problem due to changes in `Range.java`. The diff reveals transformations in the `distributionForInstance` method, noticeably adjusting the configuration and application of filters (`m_makeIndicatorFilter` and `m_attrFilter`). 

Key changes include:
- Adjustments to `m_makeIndicatorFilter`: It now sets the attribute index with `m_MetaFormat.classIndex() + 1` and uses `setNumeric(true)`. These changes ensure the filter processes the correct attribute and treats it numerically, potentially addressing mismatches due to prior changes in attribute management.
- `m_attrFilter` now uses `setInvertSelection(true)`, likely accounting for changes in the attributes to be ignored or included, addressing potential selection logic shifts.
- Added `setInputFormat(...)` calls ensure that both filters are newly configured based on updated formats, likely addressing format/capability mismatches introduced by `Range.java` changes.

These modifications collectively suggest corrections to maintain functional accuracy in attribute filtering and processing, presumably addressing inconsistencies raised by `Range.java` changes.

Given the focus on maintaining logical consistency with upstream alterations and stabilizing functionality, this commit's scope suggests it resolves issues introduced by prior changes, aligning with a bug fix.

Therefore, the answer is **Buggy**."
weka,18243.json,5e2b216f64dbf04497ebd70c8db41a5f21e4ae37,"@@ -1,83 +1,86 @@
   protected void setInstancesFromDBaseQuery() {
 
     try {
       if (m_InstanceQuery == null) {
 	m_InstanceQuery = new InstanceQuery();
       }
       String dbaseURL = m_InstanceQuery.getDatabaseURL();
       String username = m_InstanceQuery.getUsername();
       String passwd = m_InstanceQuery.getPassword();
       /*dbaseURL = (String) JOptionPane.showInputDialog(this,
 					     ""Enter the database URL"",
 					     ""Query Database"",
 					     JOptionPane.PLAIN_MESSAGE,
 					     null,
 					     null,
 					     dbaseURL);*/
      
       
       
       DatabaseConnectionDialog dbd= new DatabaseConnectionDialog(null,dbaseURL,username);
       dbd.setVisible(true);
       
       //if (dbaseURL == null) {
       if (dbd.getReturnValue()==JOptionPane.CLOSED_OPTION) {
 	m_FromLab.setText(""Cancelled"");
 	return;
       }
       dbaseURL=dbd.getURL();
       username=dbd.getUsername();
       passwd=dbd.getPassword();
       m_InstanceQuery.setDatabaseURL(dbaseURL);
       m_InstanceQuery.setUsername(username);
       m_InstanceQuery.setPassword(passwd);
       m_InstanceQuery.setDebug(dbd.getDebug());
       
       m_InstanceQuery.connectToDatabase();
       if (!m_InstanceQuery.experimentIndexExists()) {
 	System.err.println(""not found"");
 	m_FromLab.setText(""No experiment index"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       System.err.println(""found"");
       m_FromLab.setText(""Getting experiment index"");
       Instances index = m_InstanceQuery.retrieveInstances(""SELECT * FROM ""
 				       + InstanceQuery.EXP_INDEX_TABLE);
       if (index.numInstances() == 0) {
 	m_FromLab.setText(""No experiments available"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;	
       }
       m_FromLab.setText(""Got experiment index"");
 
       DefaultListModel lm = new DefaultListModel();
       for (int i = 0; i < index.numInstances(); i++) {
 	lm.addElement(index.instance(i).toString());
       }
       JList jl = new JList(lm);
       jl.setSelectedIndex(0);
       int result;
       // display dialog only if there's not just one result!
       if (jl.getModel().getSize() != 1) {
         ListSelectorDialog jd = new ListSelectorDialog(null, jl);
         result = jd.showDialog();
       }
       else {
         result = ListSelectorDialog.APPROVE_OPTION;
       }
       if (result != ListSelectorDialog.APPROVE_OPTION) {
 	m_FromLab.setText(""Cancelled"");
+        m_InstanceQuery.disconnectFromDatabase();
 	return;
       }
       Instance selInst = index.instance(jl.getSelectedIndex());
       Attribute tableAttr = index.attribute(InstanceQuery.EXP_RESULT_COL);
       String table = InstanceQuery.EXP_RESULT_PREFIX
 	+ selInst.toString(tableAttr);
       setInstancesFromDatabaseTable(table);
       
     } catch (Exception ex) {
        // 1. print complete stacktrace
        ex.printStackTrace();
        // 2. print message in panel
        m_FromLab.setText(""Problem reading database: '"" + ex.getMessage() + ""'"");
     }
   }",Buggy,"Fixed a couple of bugs where database connection was not being closed if close/cancel was pressed. Thanks to Miryam Gomez and Raquel Porras for the bug fix


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4243 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes a bug fix for properly closing database connections when a process is cancelled. The code changes align with this by adding calls to `m_InstanceQuery.disconnectFromDatabase()` in several places where methods could return early (due to cancelled operations or lack of data) to ensure the connection is closed. These additions prevent potential resource leaks, clearly indicating a correction of the described issue where the database connection was not being closed. Therefore, this change is in accordance with the commit message regarding bug fixes, and the complete stack trace logging in the catch block is a good practice that further enhances error handling.

Based on this analysis, I conclude the changes indicate a bug fix, so the answer is **Buggy**."
weka,19592.json,54c6ebfe7355bada5df3972c74469299c98fc417,"@@ -1,80 +1,82 @@
   protected void updateChart(double [] dataPoint) {
-    int iwidth = m_plotPanel.getWidth();
-    int iheight = m_plotPanel.getHeight();
+    //    int iwidth = m_plotPanel.getWidth();
+    //    int iheight = m_plotPanel.getHeight();
+    int iwidth = m_osi.getWidth(this);
+    int iheight = m_osi.getHeight(this);
 
     //    System.err.println(dataPoint[0]);
     if (dataPoint.length-1 != m_previousY.length) {
       m_previousY = new double [dataPoint.length-1];
       //      m_plotCount = 0;
       for (int i = 0; i < dataPoint.length-1; i++) {
 	m_previousY[i] = convertToPanelY(0);
       }
     }
 
     Graphics osg = m_osi.getGraphics();
     
     Graphics g = m_plotPanel.getGraphics();
 
     // paint the old scale onto the plot if a scale update has occured
     if (m_yScaleUpdate) {
       String maxVal = numToString(m_oldMax);
       String minVal = numToString(m_oldMin);
       String midVal = numToString((m_oldMax - m_oldMin) / 2.0);
       if (m_labelMetrics == null) {
 	m_labelMetrics = g.getFontMetrics(m_labelFont);
       }
       osg.setFont(m_labelFont);
       int wmx = m_labelMetrics.stringWidth(maxVal);
       int wmn = m_labelMetrics.stringWidth(minVal);
       int wmd = m_labelMetrics.stringWidth(midVal);
 
       int hf = m_labelMetrics.getAscent();
       osg.setColor(m_colorList[m_colorList.length-1]);
       osg.drawString(maxVal, iwidth-wmx, hf-2);
       osg.drawString(midVal, iwidth-wmd, (iheight / 2)+(hf / 2));
       osg.drawString(minVal, iwidth-wmn, iheight-1);
       m_yScaleUpdate = false;
       System.err.println(""Here"");
     }
 
     osg.copyArea(m_refreshWidth,0,iwidth-m_refreshWidth,
 		 iheight,-m_refreshWidth,0);
     osg.setColor(Color.black);
     osg.fillRect(iwidth-m_refreshWidth,0, iwidth, iheight);
 
     double pos;
     for (int i = 0; i < dataPoint.length-1; i++) {
       osg.setColor(m_colorList[(i % m_colorList.length)]);
       pos = convertToPanelY(dataPoint[i]);
       osg.drawLine(iwidth-m_refreshWidth, (int)m_previousY[i], 
 		   iwidth-1, (int)pos);
       m_previousY[i] = pos;
       if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 	// draw the actual y value onto the plot for this curve
 	String val = numToString(dataPoint[i]);
 	if (m_labelMetrics == null) {
 	  m_labelMetrics = g.getFontMetrics(m_labelFont);
 	}
 	int hf = m_labelMetrics.getAscent();
 	if (pos - hf < 0) {
 	  pos += hf;
 	}
 	int w = m_labelMetrics.stringWidth(val);
 	osg.setFont(m_labelFont);
 	osg.drawString(val, iwidth-w, (int)pos);
       }
     }
     
     // last element in the data point array contains the data point number
     if (dataPoint[dataPoint.length-1] % m_xValFreq == 0) {
 
       String xVal = """"+(int)dataPoint[dataPoint.length-1];
       osg.setColor(m_colorList[m_colorList.length-1]);
       int w = m_labelMetrics.stringWidth(xVal);
       osg.setFont(m_labelFont);
       osg.drawString(xVal, iwidth-w, iheight - 1);
     }
     g.drawImage(m_osi,0,0,m_plotPanel);
     //    System.err.println(""Finished"");
     //    m_plotCount++;
   }",Buggy,"Fixed scrolling problem under java 1.4 on Linux


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1484 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions fixing a scrolling problem specific to Java 1.4 on Linux. Let's analyze the changes to see if they align with this message:

1. **Width and Height Calculation**:
   - The original code retrieves the width and height directly from `m_plotPanel` using `getWidth()` and `getHeight()`.
   - The revised code comments out these lines and replaces them with width and height obtained from `m_osi.getWidth(this)` and `m_osi.getHeight(this)`.

2. **Potential Scrolling Issue**:
   - The change suggests that the width and height are now being calculated based on `m_osi`, which presumably is an off-screen image or buffer object. This could be related to addressing a graphical update or rendering issue, potentially specific to a Java version or operating system where direct panel dimensions do not give the expected results.
   - This adjustment may be crucial in managing rendering, refreshing, or scrolling issues that were present in Java 1.4 on Linux due to how the dimensions were initially calculated or buffered for rendering operations.

3. **Relevance to Scrolling Issue**:
   - Given that the commit message specifies a scrolling problem fix under Java 1.4 on Linux, this change to the calculation of dimensions is consistent with adjustments often needed to handle graphics operations correctly across different JRE implementations or platforms.

These changes indicate an attempt to address visual rendering, possibly affecting scrolling by ensuring a consistent drawing area when updating the chart. Therefore, this update is likely intended to resolve the described platform-specific issue. 

Thus, the changes align with the commit message indicating a fix for a problematic rendering behavior, leading me to conclude **Buggy**."
weka,30975.json,32ee55c07bd6bb0d5afd27817fc05d405bd35c4f,"@@ -1,50 +1,67 @@
   public void writeIncremental(Instance inst) throws IOException {
     int writeMode = getWriteMode();
     Instances structure = getInstances();
 
     if (getRetrieval() == BATCH || getRetrieval() == NONE) {
       throw new IOException(""Batch and incremental saving cannot be mixed."");
     }
 
     if (writeMode == WAIT) {
       if (structure == null) {
         setWriteMode(CANCEL);
         if (inst != null) {
-          throw new IOException(
-            ""Structure (header Information) has to be set "" + ""in advance"");
+          throw new IOException(""Structure (header Information) has to be set ""
+            + ""in advance"");
         }
       } else {
-        m_dictionaryBuilder.reset();
-        try {
-          m_dictionaryBuilder.setup(structure);
-        } catch (Exception ex) {
-          throw new IOException(ex);
-        }
-        setWriteMode(WRITE);
+        setWriteMode(STRUCTURE_READY);
       }
       writeMode = getWriteMode();
     }
+    if (writeMode == CANCEL) {
+      cancel();
+    }
+
+    if (writeMode == STRUCTURE_READY) {
+      m_dictionaryBuilder.reset();
+      try {
+        m_dictionaryBuilder.setup(structure);
+      } catch (Exception ex) {
+        throw new IOException(ex);
+      }
+      setWriteMode(WRITE);
+      writeMode = getWriteMode();
+    }
 
     if (writeMode == WRITE) {
       if (structure == null) {
         throw new IOException(""No instances information available."");
       }
 
       if (inst != null) {
         m_dictionaryBuilder.processInstance(inst);
       } else {
+        try {
+          m_dictionaryBuilder.finalizeDictionary();
+        } catch (Exception e) {
+          throw new IOException(e);
+        }
         if (retrieveFile() == null && getWriter() == null) {
           if (getSaveBinaryDictionary()) {
             throw new IOException(
               ""Can't output binary dictionary to standard out!"");
           }
           m_dictionaryBuilder.saveDictionary(System.out);
         } else {
-          m_dictionaryBuilder.saveDictionary(getWriter());
+          if (getSaveBinaryDictionary()) {
+            m_dictionaryBuilder.saveDictionary(m_binaryStream);
+          } else {
+            m_dictionaryBuilder.saveDictionary(getWriter());
+          }
         }
 
         resetStructure();
         resetWriter();
       }
     }
   }",Buggy,"Fixed a bug in incremental mode

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states ""Fixed a bug in incremental mode,"" and the diff indicates changes in handling write modes during the `writeIncremental` process. Here’s a breakdown of the changes that suggest a bug fix:

1. **Mode Initialization and Condition Handling**:
   - The original code combined several operations under the condition of `writeMode == WAIT`. It changed the mode to `CANCEL` if the structure was not set, or to `WRITE` if it was. The updated code introduces a new condition to set `writeMode` to `STRUCTURE_READY`, creating separation of concerns. This separation ensures that the proper initialization is completed before proceeding, likely addressing an ordering or logical error in mode handling.

2. **Introduction of New Mode `STRUCTURE_READY`**:
   - The introduction of `STRUCTURE_READY` as a distinct state allows the method to ensure that `m_dictionaryBuilder` is correctly set up before switching modes to `WRITE`. This step was likely missing or incomplete in the original implementation, which might have led to bugs if `m_dictionaryBuilder` wasn’t prepared before processing instances.

3. **Exception Handling and Resource Management**:
   - The diff adds a new exception handling block when finalizing the dictionary (`finalizeDictionary`). This suggests the original code might have missed capturing potential errors, particularly if an instance is `null`, indicating an oversight in resources or memory management.

4. **Ensuring Correct Output Stream**:
   - Enhanced conditions for saving the dictionary are now checking if `getSaveBinaryDictionary()` to determine the stream (binary vs. writer). This change points to addressing bugs related to incorrect output methods or corrupted files in specific scenarios.

The diff aligns well with the commit message stating a bug fix. The restructuring of logical flow within the method, the introduction of a distinct step to ensure correct setup, and the refinement of exception handling suggest that the change was necessary to correct issues within incremental processing.

Given these points, the changes do represent a bug fix. Therefore, the conclusion is **Buggy**."
weka,20298.json,2de75b28460973901afd12db84cb5288c20a8918,"@@ -1,24 +1,23 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
-
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7690 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix in setting the tooltip. The changes in the diff involve correcting a typo in the class name for a plugin retrieval: changing ""OffscreenChartRender"" to ""OffscreenChartRenderer."" This fix corrects the logic for obtaining the correct plugin instance, which directly affects the ability to set the tooltip correctly when a plugin renderer is selected. Without this fix, the tooltip might not have set correctly if the plugin was not found, causing a bug in displaying the tooltip. Additionally, the exception block has been removed, which might have previously caught an exception that masked this error. 

Given these changes, the updates to the code align with the commit message stating a bug fix. Therefore, the answer is **Buggy**."
weka,17871.json,e62dd28a920a99053a89d45c44b4520741764624,"@@ -1,91 +1,92 @@
   public static void main (String [] args) {
     try {
       if (args.length < 8) {
 	System.err.println(""Usage : BoundaryPanel <dataset> ""
 			   +""<class col> <xAtt> <yAtt> ""
 			   +""<base> <# loc/pixel> <kernel bandwidth> ""
 			   +""<display width> ""
 			   +""<display height> <classifier ""
 			   +""[classifier options]>"");
 	System.exit(1);
       }
       final javax.swing.JFrame jf = 
 	new javax.swing.JFrame(""Weka classification boundary visualizer"");
       jf.getContentPane().setLayout(new BorderLayout());
 
       System.err.println(""Loading instances from : ""+args[0]);
       java.io.Reader r = new java.io.BufferedReader(
 			 new java.io.FileReader(args[0]));
       final Instances i = new Instances(r);
       i.setClassIndex(Integer.parseInt(args[1]));
 
       //      bv.setClassifier(new Logistic());
       final int xatt = Integer.parseInt(args[2]);
       final int yatt = Integer.parseInt(args[3]);
       int base = Integer.parseInt(args[4]);
       int loc = Integer.parseInt(args[5]);
 
       int bandWidth = Integer.parseInt(args[6]);
       int panelWidth = Integer.parseInt(args[7]);
       int panelHeight = Integer.parseInt(args[8]);
 
       final String classifierName = args[9];
       final BoundaryPanel bv = new BoundaryPanel(panelWidth,panelHeight);
       bv.addActionListener(new ActionListener() {
 	  public void actionPerformed(ActionEvent e) {
 	    String classifierNameNew = 
 	      classifierName.substring(classifierName.lastIndexOf('.')+1, 
 				       classifierName.length());
 	    bv.saveImage(classifierNameNew+""_""+i.relationName()
 			 +""_X""+xatt+""_Y""+yatt+"".jpg"");
 	  }
 	});
 
       jf.getContentPane().add(bv, BorderLayout.CENTER);
       jf.setSize(bv.getMinimumSize());
       //      jf.setSize(200,200);
       jf.addWindowListener(new java.awt.event.WindowAdapter() {
 	  public void windowClosing(java.awt.event.WindowEvent e) {
 	    jf.dispose();
 	    System.exit(0);
 	  }
 	});
 
       jf.pack();
       jf.setVisible(true);
       //      bv.initialize();
       bv.repaint();
       
 
       String [] argsR = null;
       if (args.length > 10) {
+	System.err.println(""""+(args.length-10));
 	argsR = new String [args.length-10];
-	for (int j = 9; j < args.length; j++) {
+	for (int j = 10; j < args.length; j++) {
 	  argsR[j-10] = args[j];
 	}
       }
       Classifier c = Classifier.forName(args[9], argsR);
       KDDataGenerator dataGen = new KDDataGenerator();
       dataGen.setKernelBandwidth(bandWidth);
       bv.setDataGenerator(dataGen);
       bv.setNumSamplesPerRegion(loc);
       bv.setGeneratorSamplesBase(base);
       bv.setClassifier((DistributionClassifier)c);
       bv.setTrainingData(i);
       bv.setXAttribute(xatt);
       bv.setYAttribute(yatt);
 
       try {
 	// try and load a color map if one exists
 	FileInputStream fis = new FileInputStream(""colors.ser"");
 	ObjectInputStream ois = new ObjectInputStream(fis);
 	FastVector colors = (FastVector)ois.readObject();
 	bv.setColors(colors);	
       } catch (Exception ex) {
 	System.err.println(""No color map file"");
       }
       bv.start();
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Fixed bug in main method


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions fixing a bug in the main method. Analyzing the provided diff, there is a loop index correction in the for-loop that initializes `argsR`. Originally, the loop starts with `j = 9`, but it should start with `j = 10` as `args[9]` is the classifier name and should not be included in `argsR`. This correction ensures the correct arguments are passed to the `Classifier.forName` method, which would have improperly included the classifier name as an option if the bug remained. Additionally, a debugging line was added to print the number of arguments beyond the 10th positional argument, which is consistent with identifying or handling bugs.

By correcting the loop index, the change effectively resolves a bug that could result in incorrect configuration or runtime errors caused by mishandling command-line arguments. Given these observations, the changes are indeed indicative of a bug fix.

Thus, based on the alignment with the commit message and the functional changes that correct an error in handling array indices, I conclude the changes are **Buggy**."
weka,30018.json,044bb44ae261d7619c1f9d9616f4e996330d7043,"@@ -1,77 +1,75 @@
   protected List<String> checkForNativeLibs(Package toLoad, File packageDir) {
     List<String> jarsForClassloaderToIgnore = new ArrayList<>();
 
     if (toLoad.getPackageMetaDataElement(""NativeLibs"") != null) {
       String nativeLibs =
         toLoad.getPackageMetaDataElement(""NativeLibs"").toString();
       if (nativeLibs.length() > 0) {
         String[] jarsWithLibs = nativeLibs.split("";"");
         for (String entry : jarsWithLibs) {
           String[] jarAndEntries = entry.split("":"");
           if (jarAndEntries.length != 2) {
             System.err
               .println(""Was expecting two entries for native lib spec - ""
                 + ""jar:comma-separated lib paths"");
             continue;
           }
           String jarPath = jarAndEntries[0].trim();
           String[] libPathsInJar = jarAndEntries[1].split("","");
           List<String> libsToInstall = new ArrayList<>();
-          List<String> libsToAddToPath = new ArrayList<>();
           // look at named libs and check if they are already in
           // $WEKA_HOME/native-libs - don't extract a second time, but DO
           // add entries to java.library.path
           for (String lib : libPathsInJar) {
             String libName = lib.trim().replace(""\\"", ""/"");
             if (!nativeLibInstalled(libName.substring(
               libName.lastIndexOf(""/"") + 1, libName.length()))) {
-              libsToInstall.add(libName);
+              libsToInstall.add(libName.substring(
+                libName.lastIndexOf(""/"") + 1, libName.length()));
             }
-            libsToAddToPath.add(libName.substring(libName.lastIndexOf(""/"") + 1,
-              libName.length()));
           }
 
           if (libsToInstall.size() > 0) {
             try {
               installNativeLibs(packageDir, jarPath, libsToInstall);
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
           /*
            * if (libsToAddToPath.size() > 0) {
            * addNativeLibsToLibsProp(libsToAddToPath); }
            */
         }
       }
     }
 
     // now check to see if there is a native loader to inject into the
     // root class loader
     if (toLoad.getPackageMetaDataElement(""InjectLoader"") != null) {
       String injectDetails =
         toLoad.getPackageMetaDataElement(""InjectLoader"").toString();
       String[] entries = injectDetails.split("";"");
 
       for (String entry : entries) {
         String jarPath = entry.trim();
         boolean rootClassLoader = false;
         if (jarPath.startsWith(""root|"")) {
           jarPath = jarPath.replace(""root|"", """");
           rootClassLoader = true;
         }
         String ignoreJar = jarPath.replace(""\\"", ""/"");
         ignoreJar = ignoreJar.substring(ignoreJar.lastIndexOf(""/"") + 1);
 
         jarsForClassloaderToIgnore.add(ignoreJar);
         try {
           WekaPackageClassLoaderManager.injectAllClassesInJar(new File(
             packageDir.toString() + File.separator + jarPath.trim()));
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
     }
 
     return jarsForClassloaderToIgnore;
   }",Buggy,"Fixed a bug in the native lib installation routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix in the native library installation routine. The diff shows changes within the `checkForNativeLibs` method that affect how native libraries are handled:

1. **Modification of `libsToInstall`**: The logic for adding libraries to the `libsToInstall` list has been modified. Previously, the entire path was included, but the change ensures that only the library name (without the directory path) gets added to the list. This implies that there was probably a bug where the `installNativeLibs` method incorrectly handled library paths, expecting just the library names and not the full paths.

2. **Removal of `libsToAddToPath` logic**: The code dealing with `libsToAddToPath` has been commented out or removed. This suggests that this part might have been unnecessary or incorrectly implemented.

The changes directly relate to correctly identifying and installing native libraries from packages, which aligns with the commit message about fixing a bug in the native lib installation routine. The logic adjustment to correctly handle paths indicates that the previous implementation was flawed in its handling of library paths or installation, which would constitute a bug.

Thus, the changes align with a bug fix.

**Buggy**"
weka,3629.json,34ee50c28e6d5f6dc2ea9bff29e6d09545fb601e,"@@ -1,38 +1,38 @@
   public void start() throws WekaException {
     if (getStepManager().numIncomingConnections() == 0) {
       getStepManager().processing();
       ClassLoader orig = Thread.currentThread().getContextClassLoader();
       try {
         String jobName = ""WekaKF:"" + m_runningJob.getJobName();
         Thread.currentThread()
           .setContextClassLoader(this.getClass().getClassLoader());
 
         List<StepManager> outConns =
           getStepManager().getOutgoingConnections().get(""success"");
         if (outConns != null) {
           for (StepManager manager : outConns) {
             jobName += ""+"" + ((AbstractSparkJob) ((StepManagerImpl) manager)
               .getManagedStep()).getUnderlyingJob().getJobName();
           }
         }
         m_runningJob.setJobName(jobName);
         getStepManager()
           .logBasic(""Starting Spark job as start point: "" + jobName);
 
         // we are a start point. Assumption is that we're the *only* start point
         // as things will break down if there are more than one.
         try {
           m_sparkLogAppender = m_runningJob.initJob(null);
         } catch (Exception ex) {
-          ex.printStackTrace();
+          m_runningJob = null;
           throw new WekaException(ex);
         }
         m_currentContext = m_runningJob.getSparkContext();
         m_cachingStrategy = m_runningJob.getCachingStrategy();
         m_outputDirectory = m_runningJob.getSparkJobConfig().getOutputDir();
       } finally {
         Thread.currentThread().setContextClassLoader(orig);
       }
       runJob();
     }
   }",Buggy,"Fixed a bug that prevented spark step options from being altered after a job had been run and suffered a failure

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12960 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message suggests a fix for a bug related to Spark step options that couldn't be changed after a job failure. In the code diff, the change replaces the use of `ex.printStackTrace();` with `m_runningJob = null;` before throwing a `WekaException`. 

Let's analyze the change:

1. **Handling Failed Job**: Previously, when an exception was caught, the stack trace of the exception was printed, but the status of `m_runningJob` was not changed. This might leave the object in an invalid state if a subsequent operation attempted to modify or restart the same job.

2. **Setting `m_runningJob` to null**: By setting `m_runningJob` to `null`, it ensures that the failed job is essentially discarded, allowing for the possibility of reinitializing it later with new options if needed.

Connecting this to the commit message, the bug fix directly relates to enabling alterations to the Spark step options after a failure because clearing the `m_runningJob` ensures that a new job can be started fresh, avoiding using the possibly corrupt state left by the failed attempt.

Therefore, the change aligns well with the message and indicates a bug fix. The conclusion is **Buggy**."
weka,29409.json,c06f15670aa5dc47487a677160a087b8ab2d869a,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.1 $"");
+    return RevisionUtils.extract(""$Revision$"");
   }",Buggy,"Fixed a bug introduced by the improvements in derived fields where field definitions for inputs were not being set correctly.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5028 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a bug fix related to field definitions for inputs, but the diff only updates a string literal in the `getRevision` method, specifically removing the version number within the `extract` call. This change seems to be a refactoring or cleanup of the version information rather than a direct bug fix related to field definitions.

Given the lack of modifications in areas concerning field definitions or deriving fields, this code change does not align with the message about fixing a bug in input field definitions. Instead, it addresses a version control or documentation aspect unrelated to the described bug.

Therefore, the commit does not reflect the bug fix described in the message. The answer is **NotBuggy**."
weka,25760.json,b63980e747df9f6abe802dfdf83f00c171dec0ea,"@@ -1,17 +1,25 @@
     protected double[][] getCoefficients(){
 	double[][] coefficients = new double[m_numClasses][m_numericDataHeader.numAttributes() + 1];
 	for (int j = 0; j < m_numClasses; j++) {
 	    //go through simple regression functions and add their coefficient to the coefficient of
 	    //the attribute they are built on.
 	    for (int i = 0; i < m_numRegressions; i++) {
 		
 		double slope = m_regressions[j][i].getSlope();
 		double intercept = m_regressions[j][i].getIntercept();
 		int attribute = m_regressions[j][i].getAttributeIndex();
 		
 		coefficients[j][0] += intercept;
 		coefficients[j][attribute + 1] += slope;
 	    }
 	}
+        
+        // Need to multiply all coefficients by (J-1) / J
+        for (int j = 0; j < coefficients.length; j++) {
+          for (int i = 0; i < coefficients[0].length; i++) {
+            coefficients[j][i] *= (double)(m_numClasses - 1) / (double)m_numClasses;
+          }
+        }
+
 	return coefficients;
     }",Buggy,"Fixed bug in output of coefficients and intercept: they needed to be multiplied by (J-1)/J, where J is the number of classes. This affected LMT and SimpleLogistic.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3930 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug existed in the calculation of coefficients and intercepts, which needed adjustment by multiplying them by \((J-1)/J\), where \(J\) is the number of classes. The provided code diff shows the implementation of this correction by iterating through the coefficients array and applying this multiplication.

Here is a breakdown of the changes:
- The original code calculates the coefficients and intercepts from regression functions without any scaling factor.
- The bug fix involves scaling all coefficients by the factor \((J-1)/J\) to correct their output, aligning with the description in the commit message.

This change directly corresponds to the commit message claiming to fix the calculation error in coefficients and intercepts, as it applies the described mathematical correction.

Hence, the answer is **Buggy**."
weka,10869.json,d7b954bf6ad31fcb8c1e3fd0ed630c511674efa0,"@@ -1,103 +1,105 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         Matrix khatM = new UpperSymmDenseMatrix(m);
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khatM.set(i, j, m_Kernel.eval(i, j, m_Sample.instance(i)));
             }
         }
         m_Kernel.clean();
 
         if (m_Debug) {
             Matrix kbM = new DenseMatrix(n, m);
             for (int i = 0; i < n; i++) {
                 for (int j = 0; j < m; j++) {
                     kbM.set(i, j, m_Kernel.eval(-1, j, inputFormat.instance(i)));
                 }
             }
 
             // Calculate SVD of kernel matrix
             SVD svd = SVD.factorize(khatM);
 
             double[] singularValues = svd.getS();
             Matrix sigmaI = new UpperSymmDenseMatrix(m);
             for (int i = 0; i < singularValues.length; i++) {
                 if (singularValues[i] > SMALL) {
                     sigmaI.set(i, i, 1.0 / singularValues[i]);
                 }
             }
 
             System.err.println(""U :\n"" + svd.getU());
             System.err.println(""Vt :\n"" + svd.getVt());
             System.err.println(""Reciprocal of singular values :\n"" + sigmaI);
 
             Matrix pseudoInverse = svd.getU().mult(sigmaI, new DenseMatrix(m,m)).mult(svd.getVt(), new DenseMatrix(m,m));
 
             // Compute reduced-rank version
             Matrix khatr = kbM.mult(pseudoInverse, new DenseMatrix(n, m)).mult(kbM.transpose(new DenseMatrix(m, n)), new DenseMatrix(n,n));
 
             System.err.println(""Reduced rank matrix: \n"" + khatr);
         }
 
         // Compute weighting matrix
         if (getUseSVD()) {
             SVD svd = SVD.factorize(khatM);
             double[] e = svd.getS();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""U matrix :\n"" + svd.getU());
                 System.err.println(""Vt matrix :\n"" + svd.getVt());
                 System.err.println(""Singluar values \n"" + Utils.arrayToString(svd.getS()));
                 System.err.println(""Reciprocal of square root of singular values :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(svd.getVt(), new DenseMatrix(m,m));
         } else {
 
             SymmDenseEVD evd = SymmDenseEVD.factorize(khatM);
             double[] e = evd.getEigenvalues();
             Matrix dhatr = new UpperSymmDenseMatrix(e.length);
             for (int i = 0; i < e.length; i++) {
                 if (Math.sqrt(e[i]) > SMALL) {
                     dhatr.set(i, i, 1.0 / Math.sqrt(e[i]));
                 }
             }
             if (m_Debug) {
                 System.err.println(""Eigenvector matrix :\n"" + evd.getEigenvectors());
                 System.err.println(""Eigenvalues \n"" + Utils.arrayToString(evd.getEigenvalues()));
                 System.err.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
             }
             m_WeightingMatrix = dhatr.mult(evd.getEigenvectors().transpose(), new DenseMatrix(m,m));
         }
 
         if (m_Debug) {
             System.err.println(""Weighting matrix: \n"" + m_WeightingMatrix);
         }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances(inputFormat.relationName(), atts, 0);
-        d.setClassIndex(d.numAttributes() - 1);
+        if (hasClass) {
+          d.setClassIndex(d.numAttributes() - 1);
+        }
         return d;
     }",Buggy,"Bug fix: class index in output was set to last attribute even if input data did not have class.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14365 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting the class index in output when the input data lacks a class attribute. The code change in the diff addresses precisely this issue. Originally, the `setClassIndex()` method was always called with the last attribute index, regardless of whether the input data had a class attribute. The fix introduces a conditional (`if (hasClass)`) to ensure `setClassIndex()` is only called when the input data has a class attribute, thereby aligning with the bug description in the commit message.

The code change directly correlates with the mentioned bug fix, ensuring that the logic now properly handles scenarios where input data does not include a class attribute. This correction prevents potential misbehavior or exceptions when processing data that lacks a class attribute.

Given the commit message correctly describes the change, and the code modification directly addresses the issue, the conclusion is **Buggy**."
weka,10869.json,ff769b98dbd0a017a91ca677f90dc3750b239511,"@@ -1,67 +1,87 @@
     protected Instances determineOutputFormat(Instances inputFormat) throws Exception {
 
         // Sample subset of instances
         Filter filter = Filter.makeCopy(getFilter());
         filter.setInputFormat(inputFormat);
         m_Sample = Filter.useFilter(inputFormat, filter);
 
         // Compute kernel-based matrices for subset
         m_Kernel = Kernel.makeCopy(m_Kernel);
         m_Kernel.buildKernel(m_Sample);
         int m = m_Sample.numInstances();
         int n = inputFormat.numInstances();
         double[][] khat = new double[m][m];
         for (int i = 0; i < m; i++) {
             for (int j = i; j < m; j++) {
                 khat[i][j] = m_Kernel.eval(i, j, m_Sample.instance(i));
                 khat[j][i] = khat[i][j];
             }
         }
         Matrix khatM = new Matrix(khat);
-        /*double[][] kb = new double[m][n];
-        for (int i = 0; i < m; i++) {
-            for (int j = i; j < n; j++) {
-                kb[i][j] = m_Kernel.eval(-1, i, inputFormat.instance(i));
+
+        if (m_Debug) {
+            double[][] kb = new double[n][m];
+            for (int i = 0; i < n; i++) {
+                for (int j = 0; j < m; j++) {
+                    kb[i][j] = m_Kernel.eval(-1, j, inputFormat.instance(i));
+                }
             }
-        }
-        Matrix kbM = new Matrix(kb).transpose();*/
+            Matrix kbM = new Matrix(kb);
 
-        // Calculate SVD of kernel matrix
-        SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
+            // Calculate SVD of kernel matrix
+            SingularValueDecomposition svd = new SingularValueDecomposition(new Matrix(khat));
 
-        double[] singularValues = svd.getSingularValues();
-        Matrix sigmaI = new Matrix(m,m);
-        for (int i = 0; i < singularValues.length; i++) {
-            sigmaI.set(i, i, 1.0 / singularValues[i]);
+            double[] singularValues = svd.getSingularValues();
+            Matrix sigmaI = new Matrix(m, m);
+            for (int i = 0; i < singularValues.length; i++) {
+                if (singularValues[i] > 1e-6) {
+                    sigmaI.set(i, i, 1.0 / singularValues[i]);
+                }
+            }
+
+            System.out.println(""U :\n"" + svd.getU());
+            System.out.println(""V :\n"" + svd.getV());
+            System.out.println(""Reciprocal of singular values :\n"" + sigmaI);
+
+            Matrix pseudoInverse = svd.getV().times(sigmaI).times(svd.getU().transpose());
+
+            // Compute reduced-rank version
+            Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
+
+            System.out.println(""Reduced rank matrix: \n"" + khatr);
         }
 
-        m_WeightingMatrix = sigmaI.times(svd.getV().transpose());
-
-
-        /* Matrix pseudoInverse = svd.getV().transpose().times(sigmaI).times(svd.getU().transpose());
-
-        // Compute reduced-rank version
-        Matrix khatr = kbM.times(pseudoInverse).times(kbM.transpose());
-
-        // Get eigenvalues and eigenvectors of reduced-rank matrix
-        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatr);
+        // Get eigenvalues and eigenvectors
+        EigenvalueDecomposition evd = new EigenvalueDecomposition(khatM);
         double[] e = evd.getRealEigenvalues();
         Matrix dhatr = new Matrix(e.length, e.length);
         for (int i  = 0; i < e.length; i++) {
-            dhatr.set(i, i, 1./Math.sqrt(e[i]));
+            dhatr.set(i, i, 1.0/Math.sqrt(e[i]));
         }
-        m_WeightingMatrix = dhatr.times(evd.getV()); */
+        if (m_Debug) {
+            System.out.println(""Eigenvector matrix :\n"" + evd.getV());
+            System.out.println(""Eigenvalue matrix \n"" + evd.getD());
+            System.out.println(""Reciprocal of square root of eigenvalues :\n"" + dhatr);
+        }
+
+        //System.out.println(""Reconstructed matrix: \n"" + evd.getV().times(evd.getD()).times(evd.getV().inverse())
+
+        m_WeightingMatrix = dhatr.times(evd.getV().transpose());
+
+        if (m_Debug) {
+            System.out.println(""Weighting matrix: \n"" + m_WeightingMatrix);
+        }
 
         // Construct header for output format
         boolean hasClass = (inputFormat.classIndex() >= 0);
         ArrayList<Attribute> atts = new ArrayList<Attribute>(m + ((hasClass) ? 1 : 0));
         for (int i = 0; i < m; i++) {
             atts.add(new Attribute(""z"" + (i + 1)));
         }
         if (hasClass) {
             atts.add((Attribute) inputFormat.classAttribute().copy());
         }
         Instances d = new Instances("""", atts, 0);
         d.setClassIndex(d.numAttributes() - 1);
         return d;
     }",Buggy,"Several bug fixes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12516 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The provided diff suggests changes aimed at bug fixes, and the commit message indeed mentions ""Several bug fixes,"" indicating a focus on resolving various issues. Let's closely analyze the changes to understand their relevance to bug fixing:

1. **Matrix and SVD Adjustments**: 
   - The fix introduces a conditional check on singular values: `if (singularValues[i] > 1e-6)`. This prevents attempts to invert singular values that are effectively zero, which could have led to inaccurate results or runtime exceptions due to numerical instability.
   - This change is crucial for ensuring robustness during the computation of the singular value decomposition (SVD), so it is indicative of a bug fix related to handling edge cases in linear algebra computations.

2. **Eigenvalue Decomposition and Debugging**:
   - Eigenvalue decomposition is computed on `khatM` instead of `khatr`, indicating a possible correction in the logic that was previously commented out.
   - Added debug statements can help trace and verify matrix operations throughout the method. These additions can be instrumental in diagnosing and correcting errors during matrix computations.

3. **Debug Information**: 
   - Printing matrices and calculations when `m_Debug` is true can assist developers in identifying inconsistencies or bugs in the mathematical operations.
   - Console output for intermediate matrix computations, such as U, V matrices, and eigenvectors/values, can help diagnose where computation might go wrong, which is typically added to address or understand bugs.

4. **Formatting and Corrections**:
   - Changed and uncommented sections of code related to kernel operations and weighting matrix computations suggest a prior bug that might have been hidden or improperly handled in previous versions.
   
Overall, the changes are consistent with intended bug fixes, particularly related to matrix operation stability and correctness. Additionally, the debug print statements suggest an effort to make the code easier to verify, which implies the presence of previous bugs.

Given these points, the diff aligns with the commit message indication of bug fixes. Therefore, the conclusion is **Buggy**."
weka,18657.json,c9a6740cbebd93b5978b7e993e314995eb4db94f,"@@ -1,17 +1,27 @@
   public void insertInstance(int index, boolean notify) {
     if (!m_IgnoreChanges) {
       addUndoPoint();
     }
     double[] vals = new double[m_Data.numAttributes()];
+
+    // set any string or relational attribute values to missing
+    // in the new instance, just in case this is the very first
+    // instance in the dataset.
+    for (int i = 0; i < m_Data.numAttributes(); i++) {
+      if (m_Data.attribute(i).isString()
+        || m_Data.attribute(i).isRelationValued()) {
+        vals[i] = Utils.missingValue();
+      }
+    }
     Instance toAdd = new DenseInstance(1.0, vals);
     if (index < 0) {
       m_Data.add(toAdd);
     } else {
       m_Data.add(index, toAdd);
     }
     if (notify) {
       notifyListener(new TableModelEvent(this, m_Data.numInstances() - 1,
         m_Data.numInstances() - 1, TableModelEvent.ALL_COLUMNS,
         TableModelEvent.INSERT));
     }
   }",Buggy,"Fixed a bug in the insertion of new instances. Now sets the value of relational and string attributes in the new instance to missing, just in case the new instance is the very first one in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12708 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes fixing a bug related to setting values for relational and string attributes to missing when a new instance is inserted, especially if it is the first in the dataset. The code diff aligns with this explanation: it introduces a loop to set string and relational attribute values to missing in the initial instance, addressing unexpected default values. This change is critical when the dataset starts empty, ensuring proper initialization. This specific handling aligns with the described bug fix in the commit message.

The analysis shows that the code changes explicitly address the bug described in the commit message. Therefore, the conclusion is **Buggy**."
weka,33458.json,b7e34d731ed42ab40ad353a159d93fb4247a9bc9,"@@ -1,24 +1,26 @@
   public String substitute(String source) throws Exception {
     // Grab each variable out of the string
     int index = source.indexOf(""${"");
 
     while (index >= 0) {
       index += 2;
       int endIndex = source.indexOf('}');
       if (endIndex >= 0 && endIndex > index +1) {
         String key = source.substring(index, endIndex);
 
         // look this sucker up
         String replace = m_envVars.get(key);
         if (replace != null) {
           String toReplace = ""${"" + key + ""}"";
           source = source.replace(toReplace, replace);
         } else {
           throw new Exception(""[Environment] Variable "" 
                               + key + "" doesn't seem to be set."");
         }
+      } else {
+        break;
       }
       index = source.indexOf(""${"");
     }
     return source;
   }",Buggy,"Fixed a bug that caused an infinite loop in substitute(). Now uses a TreeMap instead of a HashMap so that keys are kept in sorted order.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5368 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions fixing an infinite loop issue in the `substitute()` method. The code change adds a `break` statement in the loop, which suggests that previously, if `endIndex` was not found as expected, the loop would continue indefinitely. This change intentionally interrupts the loop when no valid substitution pattern (i.e., a complete `${key}` structure) is found. The update matches the description in the commit message about correcting infinite looping behavior due to unmapped or improperly closed variables, which aligns with the identification and rectification of a bug.

Thus, the changes in the code align with the commit message regarding fixing a loop-related bug, indicating that this commit indeed addresses a software defect.

**Buggy**"
weka,24845.json,ceb6ad5f19eef0a321d4c4ad851a2bba76e1ebb0,"@@ -1,5 +1,4 @@
   public String globalInfo() {
     return ""Hierarchical clustering class.\n""
-      + ""Implements a number of classic agglomorative (i.e. bottom up) hierarchical clustering methods""
-      + ""based on ."";
+      + ""Implements a number of classic agglomerative (i.e., bottom up) hierarchical clustering methods."";
   }",Buggy,"Fixed bug in globalInfo() method.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13174 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions fixing a bug in the `globalInfo()` method. The diff reveals changes solely related to correcting a typographical error in the documentation string within the `globalInfo()` method. Specifically, ""agglomorative"" has been corrected to ""agglomerative,"" and a redundant phrase ""based on ."" has been removed, clarifying the method's descriptive text.

This alteration improves readability and correctness of the documentation but doesn't involve any functional or logical changes to the code. Therefore, while it corrects a typo, it doesn't qualify as a bug fix in the typical code execution context, such as error-handling updates, logical corrections, or exception handling improvements.

Given that the fix addresses a minor documentation typo without impacting code execution or logic, the change is not indicative of a true software bug fix.

The conclusion is **NotBuggy**."
weka,15454.json,be743132c9f71754ed911dce8ed0a797b9314166,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
-        ""\tNew field specification (name@type@value).\n""
+        ""\tNew field specification (name@type:value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
         ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed a bug in the help info for the -A option

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13337 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code modifies the help information string for the ""-A"" option from ""name@type@value"" to ""name@type:value"". This suggests there was a mistake or inconsistency in the documented expected format for this command-line option.

Let's analyze whether this indicates a bug fix:
1. **Documentation Correction**: The commit message states a bug in the help info, which directly correlates with the altered string in the code that provides user guidance.
2. **Inconsistency Correction**: If the intended format uses a colon (:) instead of an at symbol (@) to separate ""type"" and ""value"", retaining the wrong delimiter in documentation could confuse users and lead to misuse.
3. **Functional Impact**: Although this does not affect the runtime behavior of the application, incorrect help information can lead to user errors, which is effectively a usability bug.

Since the change directly aligns with correcting a documented oversight that could lead to incorrect user input, it is indeed a bug fix.

Thus, this change is deemed **Buggy**."
weka,15454.json,611c95b4ddf959002e0360d9c109a532143b3d2c,"@@ -1,18 +1,18 @@
   public Enumeration<Option> listOptions() {
 
     Vector<Option> newVector = new Vector<Option>(1);
 
     newVector
       .addElement(new Option(
         ""\tNew field specification (name@type@value).\n""
           + ""\t Environment variables may be used for any/all parts of the\n""
           + ""\tspecification. Type can be one of (numeric, nominal, string or date).\n""
           + ""\tThe value for date be a specific date string or the special string\n""
           + ""\t\""now\"" to indicate the current date-time. A specific date format\n""
           + ""\tstring for parsing specific date values can be specified by suffixing\n""
           + ""\tthe type specification - e.g. \""myTime@date:MM-dd-yyyy@08-23-2009\"".""
           + ""This option may be specified multiple times"", ""A"", 1,
-        ""-A <name:type:value>""));
+        ""-A <name@type@value>""));
 
     return newVector.elements();
   }",Buggy,"Fixed an error in the listOptions output

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12731 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that an error in the `listOptions` output was fixed. The change in the code is a correction in the format of the option string from ""-A <name:type:value>"" to ""-A <name@type@value>"". This aligns the documentation or intended output format with the actual expected input, possibly resolving user confusion or errors related to option parsing.

Considering this modification aligns the actual output with the intended output, this is indeed a bug fix. Therefore, the conclusion is **Buggy**."
weka,26007.json,6fcd46acab99a453150b3418dc7da518be7033d2,"@@ -1,43 +1,43 @@
   private String ruleToString() {
     StringBuffer text = new StringBuffer();
 
     if (m_splitAtts.length > 0) {
       text.append(""IF\n"");
 
       for (int i = m_splitAtts.length - 1; i >= 0; i--) {
 	text.append(""\t"" + m_covered.attribute(m_splitAtts[i]).name() + "" "");
 
 	if (m_relOps[i] == 0) {
 	  text.append(""<= "");
 	} else {
 	  text.append(""> "");
 	} 
 
 	text.append(Utils.doubleToString(m_splitVals[i], 1, 3) + ""\n"");
       } 
 
       text.append(""THEN\n"");
     } 
 
     if (m_ruleModel != null) {
       try {
 	text.append(m_ruleModel.printNodeLinearModel());
 	text.append("" ["" + m_numCovered/*m_covered.numInstances()*/);
 
 	if (m_globalAbsDev > 0.0) {
 	  text.append(""/""+Utils.doubleToString((100 * 
 						   m_ruleModel.
 						   rootMeanSquaredError() / 
-						   m_globalAbsDev), 1, 3) 
+						   m_globalStdDev), 1, 3) 
 		      + ""%]\n\n"");
 	} else {
 	  text.append(""]\n\n"");
 	} 
       } catch (Exception e) {
 	return ""Can't print rule"";
       } 
     } 
     
     //    System.out.println(m_instances);
     return text.toString();
   }",Buggy,"Fixed bug in rule output. Second number in the brackets at a leaf now reports the rms error as a percentage of the global standard deviation.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message specifies a bug fix regarding how the second number in brackets at a leaf node is reported. It should now represent the root mean square (RMS) error as a percentage of the global standard deviation, rather than the global absolute deviation.

Here's the breakdown of the changes:

1. **Original Code:**

   ```java
   text.append(""/""+Utils.doubleToString((100 * 
       m_ruleModel.rootMeanSquaredError() / 
       m_globalAbsDev), 1, 3) + ""%]\n\n"");
   ```

   - This line calculates the RMS error as a percentage of the global absolute deviation (`m_globalAbsDev`).

2. **Modified Code:**

   ```java
   text.append(""/""+Utils.doubleToString((100 * 
       m_ruleModel.rootMeanSquaredError() / 
       m_globalStdDev), 1, 3) + ""%]\n\n"");
   ```

   - The calculation has been corrected to use `m_globalStdDev`, the global standard deviation, instead of `m_globalAbsDev`.

3. **Analysis:**

   - The code change aligns with the commit message, which describes a bug fix in the reporting of the rule output.
   - The calculation of the percentage was incorrect in the original code and was using the wrong denominator.
   - The adjustment fixes this by using the standard deviation to calculate the RMS error percentage correctly.

Given the detailed context of the change and its alignment with the commit message’s explanation, it is clear that these changes address a bug in the logic for displaying rule output.

Thus, the change should be classified as **Buggy**."
weka,14982.json,e15ee656dfd1f2636611269ded7d3ce754f975c0,"@@ -1,80 +1,90 @@
   public int[] search (ASEvaluation ASEval, Instances data)
     throws Exception {
     int i, j;
 
     if (!(ASEval instanceof AttributeEvaluator)) {
       throw  new Exception(ASEval.getClass().getName() 
 			   + "" is not a"" 
 			   + ""Attribute evaluator!"");
     }
-    
-    if (ASEval instanceof AttributeTransformer) {
-      data = ((AttributeTransformer)ASEval).transformedHeader();
-    }
 
     m_numAttribs = data.numAttributes();
 
     if (ASEval instanceof UnsupervisedAttributeEvaluator) {
       m_hasClass = false;
     }
     else {
-      m_hasClass = true;
       m_classIndex = data.classIndex();
+      if (m_classIndex >= 0) {	
+	m_hasClass = true;
+      } else {
+	m_hasClass = false;
+      }
+    }
+
+    // get the transformed data and check to see if the transformer
+    // preserves a class index
+    if (ASEval instanceof AttributeTransformer) {
+      data = ((AttributeTransformer)ASEval).transformedHeader();
+      if (m_classIndex >= 0 && data.classIndex() >= 0) {
+	m_classIndex = data.classIndex();
+	m_hasClass = true;
+      }
     }
 
 
     m_startRange.setUpper(m_numAttribs - 1);
     if (!(getStartSet().equals(""""))) {
       m_starting = m_startRange.getSelection();
     }
     
     int sl=0;
     if (m_starting != null) {
       sl = m_starting.length;
     }
     if ((m_starting != null) && (m_hasClass == true)) {
       // see if the supplied list contains the class index
       boolean ok = false;
       for (i = 0; i < sl; i++) {
 	if (m_starting[i] == m_classIndex) {
 	  ok = true;
 	  break;
 	}
       }
       
       if (ok == false) {
 	sl++;
       }
     }
     else {
       if (m_hasClass == true) {
 	sl++;
       }
     }
 
 
     m_attributeList = new int[m_numAttribs - sl];
     m_attributeMerit = new double[m_numAttribs - sl];
 
     // add in those attributes not in the starting (omit list)
     for (i = 0, j = 0; i < m_numAttribs; i++) {
       if (!inStarting(i)) {
 	m_attributeList[j++] = i;
       }
     }
 
     AttributeEvaluator ASEvaluator = (AttributeEvaluator)ASEval;
 
     for (i = 0; i < m_attributeList.length; i++) {
       m_attributeMerit[i] = ASEvaluator.evaluateAttribute(m_attributeList[i]);
     }
 
     double[][] tempRanked = rankedAttributes();
     int[] rankedAttributes = new int[m_attributeList.length];
 
     for (i = 0; i < m_attributeList.length; i++) {
       rankedAttributes[i] = (int)tempRanked[i][0];
     }
 
     return  rankedAttributes;
   }",Buggy,"Fixed a bug with handling class index when used with attribute transformers


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2023 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix related to handling the class index when using attribute transformers. Let's analyze the changes:

1. **Initial Behavior Without Transformers**: 
   - The original code set up the class index handling before checking if `ASEval` was an `AttributeTransformer`.
   - The handling of `m_classIndex` was unconditional based on `ASEval` being an `AttributeEvaluator`, defaulting to the data’s class index separate from whether any transformations had occurred.

2. **Handling `AttributeTransformer`**:
   - The updated code delays this handling until after the transformer is considered.
   - If `ASEval` is an `AttributeTransformer`, the transformation is applied, and then the class index from the transformed data is reconsidered.
   - The pivotal changes ensure that after transformation, the presence and value of the class index are reassessed, indicating whether the class index is preserved or changed in the transformation.

3. **Boolean Logic Change**:
   - It adds a conditional that sets `m_hasClass` based on whether `m_classIndex` is valid (i.e., greater than or equal to zero), which is crucial for correctly managing the class index's presence post-transformation.

4. **Validation**:
   - Before this fix, there was an assumption that if a class index was set, it was always available post-transformation without checking.
   - Now, if a transformation occurs, the code checks `data.classIndex()` again, providing correct state information about the data's structure after the transformation.

Given these steps, the changes align with the commit message as a bug fix. The code now correctly manages and checks for the class index post-attribute transformation, ensuring that operations depending on class information work correctly regardless of prior transformations affecting header metadata.

Conclusively, these modifications point to an error-handling correction specifically tied to situations where transformations might alter the class index's usability. Thus, the evaluation of transformed data is refined to accurately reflect its structure, marking this as a **Buggy** fix."
weka,8085.json,908420a70784469c92b61a26142bb14cd748d5d5,"@@ -1,57 +1,59 @@
   public void finalizeTask() throws Exception {
     if (m_classifier == null) {
       throw new Exception(""No classifier has been set"");
     }
 
     if (m_classifier instanceof UpdateableClassifier
       && !m_batchTrainedIncremental) {
       // nothing to do except possibly down-sample predictions for
       // auc/prc
       if (m_predFrac > 0) {
         ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(
           m_predFrac, m_seed);
       }
 
       return;
     }
 
     m_trainingHeader.compactify();
 
     Instances test = m_trainingHeader;
     Random r = new Random(m_seed);
     test.randomize(r);
     if (test.classAttribute().isNominal() && m_totalFolds > 1) {
       test.stratify(m_totalFolds);
     }
 
     if (m_totalFolds > 1 && m_foldNumber >= 1) {
       test = test.testCV(m_totalFolds, m_foldNumber - 1);
     }
 
     m_numTestInstances = test.numInstances();
 
-    if (m_classifier instanceof BatchPredictor) {
+    if (m_classifier instanceof BatchPredictor
+      && ((BatchPredictor) m_classifier)
+        .implementsMoreEfficientBatchPrediction()) {
 
       // this method always stores the predictions for AUC, so we need to get
-      // rid of them if we're note doing any AUC computation
+      // rid of them if we're not doing any AUC computation
       m_eval.evaluateModel(m_classifier, test);
-      if (m_predFrac < 0) {
+      if (m_predFrac <= 0) {
         ((AggregateableEvaluationWithPriors) m_eval).deleteStoredPredictions();
       }
     } else {
       for (int i = 0; i < test.numInstances(); i++) {
         if (m_predFrac > 0) {
           m_eval.evaluateModelOnceAndRecordPrediction(m_classifier,
             test.instance(i));
         } else {
           m_eval.evaluateModelOnce(m_classifier, test.instance(i));
         }
       }
     }
 
     // down-sample predictions for auc/prc
     if (m_predFrac > 0) {
       ((AggregateableEvaluationWithPriors) m_eval).prunePredictions(m_predFrac,
         m_seed);
     }
   }",Buggy,"Fixed a bug (due to changes in BatchPredictor) that caused predictions for AUC, AUPRC to always be retained regardless of the user-specified fraction

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12342 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a specific bug fix involving predictions being retained regardless of user-specified fractions due to changes in `BatchPredictor`. The code diff introduces an additional condition to the `if` statement examining whether the classifier is a `BatchPredictor` and whether it implements more efficient batch predictions (`implementsMoreEfficientBatchPrediction()` method). This aligns with the message, suggesting this condition was missing before and caused predictions for AUC and AUPRC to be retained when they shouldn't have been. Additionally, the comparison to check if `m_predFrac < 0` has been changed to `m_predFrac <= 0`, which corrects the logic to accurately delete stored predictions when the prediction fraction is zero or negative, thus directly addressing the bug described.

Based on these observations, the changes in the code accurately address the bug mentioned in the commit message, focusing on correcting logical conditions for retaining predictions based on user input and classifier capabilities.

Thus, the changes indicate a bug fix, and the conclusion is **Buggy**."
weka,26472.json,aab16fcf5619d64b56a044bb7acaa276a2f2857b,"@@ -1,12 +1,15 @@
   public double matthewsCorrelationCoefficient(int classIndex) {
     double numTP = numTruePositives(classIndex);
     double numTN = numTrueNegatives(classIndex);
     double numFP = numFalsePositives(classIndex);
     double numFN = numFalseNegatives(classIndex);
     double n = (numTP * numTN) - (numFP * numFN);
     double d = (numTP + numFP) * (numTP + numFN) * (numTN + numFP)
         * (numTN + numFN);
     d = Math.sqrt(d);
+    if (d == 0) {
+      d = 1;
+    }
 
     return n / d;
   }",Buggy,"Fixed a bug in the MCC calculation - MCC should be zero if the denominator of the calculation is zero.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9101 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix regarding the MCC (Matthews Correlation Coefficient) calculation specifically when the denominator is zero. The change in the code ensures that if the calculated denominator (`d`) is zero, it is explicitly set to 1 instead of being zero, which would cause a division by zero error. This aligns with the commit message intention to prevent incorrect MCC calculations when the denominator is zero by returning a sensible default of zero.

Therefore, the change directly addresses the mentioned bug, by handling the case of a zero denominator gracefully, preventing potential runtime errors and ensuring the calculation follows the intended mathematical formulation. Consequently, I conclude that the change is indicative of resolving a bug.

**Buggy**"
weka,32917.json,db5c23a0208b68d40384c60c377e2e204c0b098e,"@@ -1,17 +1,17 @@
   public void setOptions(String[] options) throws Exception {
-    String nnSearchClass = Utils.getOption('D', options);
+    String nnSearchClass = Utils.getOption('A', options);
     if(nnSearchClass.length() != 0) {
       String nnSearchClassSpec[] = Utils.splitOptions(nnSearchClass);
       if(nnSearchClassSpec.length == 0) { 
         throw new Exception(""Invalid DistanceFunction specification string.""); 
       }
       String className = nnSearchClassSpec[0];
       nnSearchClassSpec[0] = """";
 
       setDistanceFunction( (DistanceFunction)
-                            Utils.forName( NearestNeighbourSearch.class, 
+                            Utils.forName( DistanceFunction.class, 
                                            className, nnSearchClassSpec) );
     }
     else 
       this.setDistanceFunction(new EuclideanDistance());  
   }",Buggy,"New Reorder filter. Improvement of docs for Copy filter. Improvements to ArffViewer. Bug fix in NearestNeighbourSearch option setting. All from Peter.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2325 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The diff indicates a change in how options are parsed and applied in the setOptions method. Originally, it was fetching a class name and options using the 'D' option by default and using it for the distance function class, which appears incorrect. The change updates this to correctly fetch with the 'A' option instead, suggesting the initial use of 'D' was a bug. Additionally, the corrected code specifies using the `DistanceFunction` class in `Utils.forName`, whereas previously it used `NearestNeighbourSearch.class`, indicating a potential type mismatch issue in dynamically loading classes based on user-supplied options. 

These changes align well with the commit message, which notes a bug fix in ""NearestNeighbourSearch option setting."" Both changes suggest errors in class option parsing and dynamic loading are addressed here.

Therefore, the changes indicate a bug fix. Concluding with **Buggy**."
weka,15986.json,a67dbdf599b532b4d05eec72b79cc0a92a58bb0f,"@@ -1,27 +1,29 @@
   public boolean setInputFormat(Instances instanceInfo) throws Exception {
     
     super.setInputFormat(instanceInfo);
 
     int[] attsToDelete = new int[instanceInfo.numAttributes()];
     int numToDelete = 0;
     for (int i=0; i<instanceInfo.numAttributes(); i++) {
-      if (i == instanceInfo.classIndex()) continue; // skip class
+      if ((i == instanceInfo.classIndex() && !m_invert)) {
+	continue; // skip class
+      }
       if (instanceInfo.attribute(i).type() == m_attTypeToDelete)
 	attsToDelete[numToDelete++] = i;
     }
 
     int[] finalAttsToDelete = new int[numToDelete];
     System.arraycopy(attsToDelete, 0, finalAttsToDelete, 0, numToDelete);
     
     m_attributeFilter.setAttributeIndicesArray(finalAttsToDelete);
     m_attributeFilter.setInvertSelection(m_invert);
     
     boolean result = m_attributeFilter.setInputFormat(instanceInfo);
     Instances afOutputFormat = m_attributeFilter.getOutputFormat();
     
     // restore old relation name to hide attribute filter stamp
     afOutputFormat.setRelationName(instanceInfo.relationName());
 
     setOutputFormat(afOutputFormat);
     return result;
   }",Buggy,"Fixed bug.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1805 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message is vague, but the code change aligns with a potential bug fix. Let's analyze the modifications step by step:

1. **Original Code:** The original loop iterates over attributes and skips the class attribute without considering the `m_invert` flag.
   
2. **Modified Code:** The change adds a condition that respects the `m_invert` flag when deciding to skip the class attribute.

3. **Bug Context:** In data processing or filtering, it's common to have flags like `m_invert` to control behavior inversion. Here, `m_invert` likely determines whether to delete or keep certain attributes, including the class attribute.

4. **Logical Correction:** The modified condition:
   ```java
   if ((i == instanceInfo.classIndex() && !m_invert))
   ```
   ensures the class attribute is skipped only when `m_invert` is `false` (the default or non-inverted mode), which fits common logic patterns where inverting a selection should have opposite effects.

5. **Potential Problem in Original Code:** Without this condition, the code wouldn't handle cases where `m_invert` is `true` properly, possibly leading to incorrect or unwanted attribute deletions.

The change enforces the intended logic regarding how class attributes should be treated when `m_invert` is active. The fix aligns with a common pattern used to address inversion logic in configurations or settings, thus making the application’s behavior more reliable under different configurations.

Considering the above, the update resolves a flaw related to the behavior of attribute deletion with respect to the class attribute. The commit message's vagueness is not ideal, but the change strongly indicates a bug fix based on logical corrections.

Therefore, I conclude the changes indicate a bug fix, and the appropriate conclusion is **Buggy**."
weka,10085.json,7d3176f1d203310d133e13abf01de23e6eb5784f,"@@ -1,7 +1,5 @@
-    public int[] postProcess(int[] attributeSet) {
+  public void clean() {
 
     // save memory
     m_trainInstances = new Instances(m_trainInstances, 0);
-
-    return attributeSet;
   }",Buggy,"Fixed a bug when generating a ranking via GreedyStepwise

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11854 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The diff shows a method named `postProcess` was replaced with `clean`. The original returned an `attributeSet`, but the new `clean` method doesn’t. It focuses on memory optimization by clearing `m_trainInstances`. The changes do not clearly align with fixing a bug directly related to generating a ranking via `GreedyStepwise`. The diff indicates code cleanup but not a bug fix. The answer is **NotBuggy**."
weka,30928.json,656caee9c08b0537064158bcef54389068beb1eb,"@@ -1,55 +1,57 @@
   public static void runFileLoader(AbstractFileLoader loader, String[] options) {
     // help request?
     try {
       String[] tmpOptions = options.clone();
       if (Utils.getFlag('h', tmpOptions)) {
         System.err.println(""\nHelp requested\n"" + makeOptionStr(loader));
         return;
       }
     } catch (Exception e) {
       // ignore it
     }
 
     if (options.length > 0) {
+      String fileName = options[0];
+      options[0] = """";
       if (loader instanceof OptionHandler) {
         // set options
         try {
           ((OptionHandler) loader).setOptions(options);
           // find file
           for (int i = 0; i < options.length; i++) {
             if (options[i].length() > 0) {
               options = new String[] { options[i] };
               break;
             }
           }
         } catch (Exception ex) {
           System.err.println(makeOptionStr(loader));
           System.exit(1);
         }
       }
 
       try {
-        loader.setFile(new File(options[0]));
+        loader.setFile(new File(fileName));
         // incremental
         if (loader instanceof IncrementalConverter) {
           Instances structure = loader.getStructure();
           System.out.println(structure);
           Instance temp;
           do {
             temp = loader.getNextInstance(structure);
             if (temp != null) {
               System.out.println(temp);
             }
           } while (temp != null);
         }
         // batch
         else {
           System.out.println(loader.getDataSet());
         }
       } catch (Exception ex) {
         ex.printStackTrace();
       }
     } else {
       System.err.println(makeOptionStr(loader));
     }
   }",Buggy,"Bug fix: file name wasn't removed from options array.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12104 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug related to a file name not being removed from the options array. The code change aligns with this by introducing `String fileName = options[0];` to store the file name, then setting `options[0] = """";` to effectively remove the file name from the options array. Later, when setting the file for the loader, it uses `fileName` instead of accessing `options[0]` directly. This ensures the options do not contain the file name when setting options for the loader, thus fixing the described bug. The change directly reflects the problem described in the commit message, where the file name had to be removed from the options array before configuring the loader with the options. The adjustments evidence logical corrections that address the bug.

Therefore, the conclusion is **Buggy**."
weka,21177.json,e137d2cd150f941d4ad96e9bcb3d3a27f7153b2e,"@@ -1,29 +1,33 @@
   protected int getIndexOfAttribute(Instances insts, String attName) {
     
+    if (attName == null) {
+      return -1;
+    }
+    
     // special first and last strings
     if (attName.equalsIgnoreCase(""/last"")) {
       return insts.numAttributes() - 1;
     }
     if (attName.equalsIgnoreCase(""/first"")) {
       return 0;
     }
     if (attName.startsWith(""/"")) {
       // try and parse remainder as a number
       String numS = attName.replace(""/"", """");
       try {
         int index = Integer.parseInt(numS);
         index--; // from 1-based to 0-based
         if (index >= 0 && index < insts.numAttributes()) {
           return index;
         }
       } catch (NumberFormatException e) {        
       }      
     }
     
     Attribute att = insts.attribute(attName);
     if (att != null) {
       return att.index();
     }
     
     return -1; // not found
   }",Buggy,"Small bug fix in the routine that finds attribute indexes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7639 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes a bug fix related to finding attribute indexes, and the diff introduces a null check for the attribute name. Previously, if `attName` was null, the code could throw a `NullPointerException`. By adding this check, the method safely returns -1 when `attName` is null, which is a logical enhancement that prevents potential runtime errors.

The change clearly addresses a potential bug by handling null input more gracefully. Since it adds defensive programming to prevent a possible exception-causing scenario, this aligns with the intent to fix a bug.

The correctness of this change reflects a bug fix, so the answer is **Buggy**."
weka,20328.json,8e2c885e0e2766ce262502a0ba98b112379f314f,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
       tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
-        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
+        Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRenderer"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a bug in the routine that sets the tool tip for additional options in plugin renderers.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7689 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to setting a tooltip for additional options in plugin renderers. The provided diff shows a change in the class name used in the `PluginManager.getPluginInstance` method:

- Original: `""weka.gui.beans.OffscreenChartRender""`
- Updated: `""weka.gui.beans.OffscreenChartRenderer""`

This correction likely addresses a typo or incorrect class name, which prevents the application from correctly instantiating the plugin and thus from setting the tooltip text as intended. The change aligns with the commit message, as it corrects the setup routine for the tooltip for additional options in plugin renderers.

This update directly fixes an issue where, due to an incorrect class name, the plugin instance could not be retrieved, and consequently, the tooltip might have not been set, or a default, possibly unhelpful, tooltip was used.

Therefore, this change does indeed indicate a bug fix.

**Buggy**"
weka,20328.json,68eef00749a6bf58cbca2344dd6f91e526407d21,"@@ -1,24 +1,24 @@
   private void setupRendererOptsTipText(JLabel optsLab) {
     String renderer = m_rendererCombo.getSelectedItem().toString();
     if (renderer.equalsIgnoreCase(""weka chart renderer"")) {
       // built-in renderer
       WekaOffscreenChartRenderer rcr = new WekaOffscreenChartRenderer();
       String tipText = rcr.optionsTipTextHTML();
-      tipText = tipText.replace(""<html>"", ""<html>Comma separate list of options:<br>"");
+      tipText = tipText.replace(""<html>"", ""<html>Comma separated list of options:<br>"");
       optsLab.setToolTipText(tipText);
     } else {
       try {
         Object rendererO = PluginManager.getPluginInstance(""weka.gui.beans.OffscreenChartRender"",
             renderer);
 
         if (rendererO != null) {
           String tipText = ((OffscreenChartRenderer)rendererO).optionsTipTextHTML();
           if (tipText != null && tipText.length() > 0) {
             optsLab.setToolTipText(tipText);
           }
         }
       } catch (Exception ex) {
 
       }
     }
   }",Buggy,"Fixed a spelling mistake in a tooltip.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7635 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates a correction of a spelling mistake in a tooltip. The code change replaces ""Comma separate list of options"" with ""Comma separated list of options."" This aligns perfectly with the commit message, as it addresses what is essentially a typographical error in a user-facing string, not a bug in the software's logic or functionality. 

There are no changes related to error handling, logical corrections, or exception-handling improvements typically seen in bug fixes. Instead, the change is purely a fix to the user interface text, reflecting an effort to correct text rather than software behavior.
  
Hence, since the change aligns with the commit message and does not indicate a bug fix, the conclusion is **NotBuggy**."
weka,3145.json,d00cb09c5d77936fe79ca4e6697fc4b8dde11de4,"@@ -1,50 +1,51 @@
   protected List<Instances> initializeWithRandomCenters(
     JavaRDD<Instance> dataset, Instances headerWithSummary, int numRuns,
     int numClusters) throws IOException, DistributedWekaException {
 
     Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     // sample all runs worth of initial centers in one hit
     // take twice as many as needed in case there are duplicates
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       try {
         seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
       } catch (NumberFormatException e) {
         // don't complain
       }
     }
 
     // oversample for > 1 cluster per run, so that we have some options if there
     // are duplicates in the list. numClusters == 1 will be used when seeding
     // the k-means|| initialization process
+    int oversampleFactor = numClusters > 1 ? 2 : 1;
     List<Instance> centerList =
-      dataset.takeSample(true, numClusters > 1 ? 2 : 1 * numRuns * numClusters,
-        seed);
+      dataset.takeSample(true, oversampleFactor * numRuns * numClusters,
+                         seed);
 
     // make sure that start points and header have been through any filters
     KMeansMapTask forFilteringOnly = new KMeansMapTask();
     try {
       forFilteringOnly.setOptions(Utils
         .splitOptions(environmentSubstitute(getKMeansMapTaskOpts())));
 
       // initialize sketches
       headerNoSummary = forFilteringOnly.init(headerWithSummary);
 
       for (int i = 0; i < centerList.size(); i++) {
         Instance filtered = forFilteringOnly.applyFilters(centerList.get(i));
         centerList.set(i, filtered);
       }
 
     } catch (Exception ex) {
       logMessage(ex);
       throw new DistributedWekaException(ex);
     }
 
     List<Instances> centreCandidates =
       KMeansMapTask.assignStartPointsFromList(numRuns, numClusters, centerList,
         headerNoSummary);
 
     return centreCandidates;
   }",Buggy,"Fixed a bug in the standard random initialization routine

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11926 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message mentions fixing a ""bug in the standard random initialization routine."" The code change involves modifying how samples are taken from the dataset to initialize cluster centers. Specifically, it introduces a new variable `oversampleFactor` and adjusts how the sample size is calculated:

```java
int oversampleFactor = numClusters > 1 ? 2 : 1;
List<Instance> centerList =
  dataset.takeSample(true, oversampleFactor * numRuns * numClusters, seed);
```

Previously, the sample size was determined by a ternary operator directly inline:

```java
dataset.takeSample(true, numClusters > 1 ? 2 : 1 * numRuns * numClusters, seed);
```

In the original code, the calculation `numClusters > 1 ? 2 : 1 * numRuns * numClusters` behaves unintentionally due to operator precedence. The multiplication happens only with `1 * numRuns * numClusters`, potentially leading to incorrect oversampling when `numClusters > 1`. 

Correcting this logic to use `oversampleFactor` clarifies the intention, ensuring that the oversampling factor is correctly applied when `numClusters > 1`. This correction likely fixes a bug where the initialization could have been done with fewer samples than intended when multiple clusters were involved, leading to potentially poor initialization due to duplicates.

Considering these factors, the code change aligns well with the commit message describing a fix in the random initialization, which implies a correction to ensure accurate sampling for cluster initialization. Therefore, the assessment is **Buggy**."
weka,26979.json,483a2a5d5184c1590893885c0a1be41649ba25de,"@@ -1,113 +1,113 @@
   public String toString() {
 
     StringBuffer text = new StringBuffer();
     int printed = 0;
 	
     if ((m_alpha == null) && (m_sparseWeights == null)) {
       return ""SMOreg : No model built yet."";
     }
     try {
       text.append(""SMOreg\n\n"");
 	    
       text.append(""Kernel used : \n"");
       if(m_useRBF) {
 	text.append(""  RBF kernel : K(x,y) = e^-("" + m_gamma + ""* <x-y,x-y>^2)"");
       } else if (m_exponent == 1){
 	text.append(""  Linear Kernel : K(x,y) = <x,y>"");
       } else {
 	if (m_featureSpaceNormalization) {
 	  if (m_lowerOrder){
 	    text.append(""  Normalized Poly Kernel with lower order : K(x,y) = (<x,y>+1)^"" + m_exponent + ""/"" + 
 			""((<x,x>+1)^"" + m_exponent + ""*"" + ""(<y,y>+1)^"" + m_exponent + "")^(1/2)"");		    
 	  } else {
 	    text.append(""  Normalized Poly Kernel : K(x,y) = <x,y>^"" + m_exponent + ""/"" + ""(<x,x>^"" + 
 			m_exponent + ""*"" + ""<y,y>^"" + m_exponent + "")^(1/2)"");
 	  }
 	} else {
 	  if (m_lowerOrder){
 	    text.append(""  Poly Kernel with lower order : K(x,y) = (<x,y> + 1)^"" + m_exponent);
 	  } else {
 	    text.append(""  Poly Kernel : K(x,y) = <x,y>^"" + m_exponent);		
 	  }
 	}
       }
       text.append(""\n\n"");
 
       // display the linear transformation
       String trans = """";
       if (m_filterType == FILTER_STANDARDIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(standardized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       } else if (m_filterType == FILTER_NORMALIZE) {
 	//text.append(""LINEAR TRANSFORMATION APPLIED : \n"");
 	trans = ""(normalized) "";
 	//text.append(trans + m_data.classAttribute().name() + ""  = "" + 
 	//	    m_Alin + "" * "" + m_data.classAttribute().name() + "" + "" + m_Blin + ""\n\n"");
       }
 
       // If machine linear, print weight vector
       if (!m_useRBF && m_exponent == 1.0) {
 	text.append(""Machine Linear: showing attribute weights, "");
 	text.append(""not support vectors.\n"");
 		
 	// We can assume that the weight vector is stored in sparse
 	// format because the classifier has been built
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	for (int i = 0; i < m_sparseWeights.length; i++) {
-	  if (i != (int)m_classIndex) {
+	  if (m_sparseIndices[i] != (int)m_classIndex) {
 	    if (printed > 0) {
 	      text.append("" + "");
 	    } else {
 	      text.append(""   "");
 	    }
 	    text.append(Utils.doubleToString(m_sparseWeights[i], 12, 4) +
 			"" * "");
 	    if (m_filterType == FILTER_STANDARDIZE) {
 	      text.append(""(standardized) "");
 	    } else if (m_filterType == FILTER_NORMALIZE) {
 	      text.append(""(normalized) "");
 	    }
 	    if (!m_checksTurnedOff) {
 	      text.append(m_data.attribute(m_sparseIndices[i]).name()+""\n"");
 	    } else {
 	      text.append(""attribute with index "" + 
 			  m_sparseIndices[i] +""\n"");
 	    }
 	    printed++;
 	  }
 	}
       } else {
 	text.append(""Support Vector Expansion :\n"");
 	text.append(trans + m_data.classAttribute().name() + "" =\n"");
 	printed = 0;
 	for (int i = 0; i < m_alpha.length; i++) {
 	  double val = m_alpha[i] - m_alpha_[i];
 	  if (java.lang.Math.abs(val) < 1e-4)
 	    continue;
 	  if (printed > 0) {
 	    text.append("" + "");
 	  } else {
 	    text.append(""   "");		    
 	  }
 	  text.append(Utils.doubleToString(val, 12, 4) 
 		      + "" * K[X("" + i + ""), X]\n"");
 	  printed++;
 	}
       }
       if (m_b > 0) {
 	text.append("" + "" + Utils.doubleToString(m_b, 12, 4));
       } else {
 	text.append("" - "" + Utils.doubleToString(-m_b, 12, 4));
       }
       if (m_useRBF || m_exponent != 1.0) {
 	text.append(""\n\nNumber of support vectors: "" + printed);
       }
       text.append(""\n\nNumber of kernel evaluations: "" + m_kernel.numEvals()+ ""\n"");
     } catch (Exception e) {
       return ""Can't print the classifier."";
     }
 
     return text.toString();
   }",Buggy,"Fixed bug in output of sparse linear machines (class index was not dealt with correctly (thanks, Bernhard)).


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2091 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix involving the class index handling in the output of sparse linear machines. The diff shows a critical update in how the class index is managed within a loop. Previously, the code checked if the loop index `i` equaled `m_classIndex`, which could lead to incorrect behavior if the class index wasn't directly mapped to the loop index position due to sparse representation. The updated code correctly checks if `m_sparseIndices[i]` equals `m_classIndex`, ensuring it accurately skips the class index attribute in sparse representation while printing attribute weights.

This modification is directly related to the commit message, addressing an erroneous handling of the class index, which aligns with the description of fixing a ""bug in output of sparse linear machines."" With this logic correction and accurate handling of sparse index arrays, the fix ensures the output is as expected.

Therefore, the changes in the diff reflect a genuine bug fix regarding the management of the class index, consistent with the described problem in the commit message.

Conclusion: **Buggy**"
weka,28766.json,78f99a6107b459a31b5ab63e2f403eb841974e27,"@@ -1,14 +1,19 @@
   public double[] distributionForInstance(Instance instance) throws Exception {
 
     double[] dist = new double[instance.numClasses()];
     switch (instance.classAttribute().type()) {
     case Attribute.NOMINAL:
-      dist[(int)classifyInstance(instance)] = 1.0;
+      double classification = classifyInstance(instance);
+      if (Instance.isMissingValue(classification)) {
+	return dist;
+      } else {
+	dist[(int)classification] = 1.0;
+      }
       return dist;
     case Attribute.NUMERIC:
       dist[0] = classifyInstance(instance);
       return dist;
     default:
       return dist;
     }
   }",Buggy,"Fixed bug that caused incorrect handling of unclassified instances in distributionForInstance()


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2006 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the bug fix addresses incorrect handling of ""unclassified instances"" in the `distributionForInstance` method. The provided code diff modifies the handling of instances classified as nominal types.

Before the change:
- The code directly sets the distribution at the index of the classified instance to 1.0. If `classifyInstance(instance)` returned a missing value, which is typically represented by a special value in WEKA like `Double.NaN`, it likely resulted in an `ArrayIndexOutOfBoundsException` or logical error.

After the change:
- The variable `classification` is used to store the result of `classifyInstance(instance)`.
- A check is introduced to handle missing classification values. If `Instance.isMissingValue(classification)` returns true, the method returns the distribution array as is, effectively handling unclassified instances correctly without attempting to set an index in the array.
- If the classification value is not missing, the index is set in the distribution array as before.

These changes aim to address the bug reported in the commit message, specifically ensuring that instances that cannot be classified (have missing classification values) are correctly handled without causing errors or unintended behavior.

Thus, the changes in the diff directly align with the bug fix described in the commit message, demonstrating logical corrections and error handling improvements in the code.

The conclusion is **Buggy**."
weka,28908.json,b842fc63d279fec85c6c091749c5a8b41f36f775,"@@ -1,25 +1,25 @@
   protected Instances metaFormat(Instances instances) throws Exception {
 
     FastVector attributes = new FastVector();
     Instances metaFormat;
     Attribute attribute;
     int i = 0;
 
     for (int k = 0; k < m_Classifiers.length; k++) {
       Classifier classifier = (Classifier) getClassifier(k);
       String name = classifier.getClass().getName();
       if (m_BaseFormat.classAttribute().isNumeric()) {
 	attributes.addElement(new Attribute(name));
       } else {
 	for (int j = 0; j < m_BaseFormat.classAttribute().numValues(); j++) {
 	  attributes.addElement(new Attribute(name + "":"" + 
 					      m_BaseFormat
 					      .classAttribute().value(j)));
 	}
       }
     }
-    attributes.addElement(m_BaseFormat.classAttribute());
+    attributes.addElement(m_BaseFormat.classAttribute().copy());
     metaFormat = new Instances(""Meta format"", attributes, 0);
     metaFormat.setClassIndex(metaFormat.numAttributes() - 1);
     return metaFormat;
   }",Buggy,"Apply bug fix from Alexander K. Seewald <alexsee@oefai.at>


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2237 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the diff involves replacing the line `attributes.addElement(m_BaseFormat.classAttribute());` with `attributes.addElement(m_BaseFormat.classAttribute().copy());`.

### Analysis:

1. **Original Issue:**
   - **Reference Problem:** The original code directly adds a reference to the class attribute of `m_BaseFormat` to the `attributes`. This approach can lead to unexpected issues if the attribute object is modified elsewhere, since the `metaFormat` would hold the reference rather than a separate instance.

2. **Applied Fix:**
   - **Creating a Separate Instance:** Using `m_BaseFormat.classAttribute().copy()` creates a new instance of the class attribute. This prevents unintended side-effects from modifications elsewhere in the code where the original attribute is used.
   - **Robustness Improvement:** The fix ensures the `metaFormat` instance has its own distinct class attribute, safeguarding it from external modifications and ensuring it behaves as expected regardless of changes to `m_BaseFormat`.

3. **Commit Message Relation:**
   - The commit message states that a bug fix from Alexander K. Seewald is being applied. The change directly relates to a bug fix, as it resolves an issue that could lead to incorrect behavior due to shared references of attributes.

### Conclusion:

This change indicates a bug fix by ensuring that a critical component (`classAttribute`) in the `metaFormat` is isolated from unintended modifications, referencing the intent described in the commit message. Therefore, the answer is **Buggy**."
weka,4826.json,6809d47f9b4ddab9de675679cc0cd2f38a66799c,"@@ -1,104 +1,103 @@
   protected void display(ArrayList<Prediction> preds, Attribute classAtt,
     int classValue) {
 
     if (preds == null) {
       JOptionPane.showMessageDialog(null, ""No data available for display!"");
       return;
     }
 
     // Remove prediction objects where either the prediction or the actual value are missing
     ArrayList<Prediction> newPreds = new ArrayList<>();
     for (Prediction p : preds) {
       if (!Utils.isMissingValue(p.actual()) && !Utils
         .isMissingValue(p.predicted())) {
         newPreds.add(p);
       }
     }
     preds = newPreds;
 
     ArrayList<Attribute> attributes = new ArrayList<>(1);
     attributes.add(new Attribute(""class_prob""));
     Instances data =
       new Instances(""class_probabilities"", attributes, preds.size());
 
     for (int i = 0; i < preds.size(); i++) {
       double[] inst =
         { ((NominalPrediction) preds.get(i)).distribution()[classValue] };
       data.add(new DenseInstance(preds.get(i).weight(), inst));
     }
 
     try {
       Discretize d = new Discretize();
       d.setUseEqualFrequency(true);
       d.setBins(
         Integer.max(1, (int) Math.round(Math.sqrt(data.sumOfWeights()))));
       d.setUseBinNumbers(true);
       d.setInputFormat(data);
       data = Filter.useFilter(data, d);
 
       int numBins = data.attribute(0).numValues();
       double[] sumClassProb = new double[numBins];
       double[] sumTrueClass = new double[numBins];
       double[] sizeOfBin = new double[numBins];
       for (int i = 0; i < data.numInstances(); i++) {
         int binIndex = (int) data.instance(i).value(0);
         sizeOfBin[binIndex] += preds.get(i).weight();
         sumTrueClass[binIndex] +=
           preds.get(i).weight() * ((((int) preds.get(i).actual())
             == classValue) ? 1.0 : 0.0);
         sumClassProb[binIndex] +=
           preds.get(i).weight() * ((NominalPrediction) preds.get(i))
             .distribution()[classValue];
       }
 
       ArrayList<Attribute> atts = new ArrayList<>(1);
       atts.add(new Attribute(""average_class_prob""));
       atts.add(new Attribute(""average_true_class_value""));
 
       // Collect data for plotting, making sure that 0,0 and 1,1 are included as invisible points
       Instances cdata =
         new Instances(""calibration_curve_data"", atts, numBins + 2);
       int[] shapeType = new int[numBins + 2];
       boolean[] connectPoint = new boolean[numBins + 2];
       for (int i = 0; i < numBins; i++) {
         double[] v = new double[2];
         v[0] = sumClassProb[i] / sizeOfBin[i];
         v[1] = sumTrueClass[i] / sizeOfBin[i];
         cdata.add(new DenseInstance(sizeOfBin[i], v));
         shapeType[i] = Plot2D.PLUS_SHAPE;
         connectPoint[i] = true;
       }
       double[] zero = new double[2];
       double[] one = new double[2];
       one[0] = 1.0;
       one[1] = 1.0;
       cdata.add(new DenseInstance(0.0, zero));
       cdata.add(new DenseInstance(0.0, one));
       shapeType[shapeType.length - 2] =
         -2; // Hack to make sure that corner points are invisible
       shapeType[shapeType.length - 1] = -2;
 
       PlotData2D plotInfo = new PlotData2D(cdata);
       plotInfo.setConnectPoints(connectPoint);
       plotInfo.setShapeType(shapeType);
-      plotInfo.setPlotName(
-        ""\""Calibration curve for class value \"" + classAtt.value(classValue)"");
+      plotInfo.setPlotName(""Calibration curve for class value "" + classAtt.value(classValue));
       VisualizePanel vp = new VisualizePanel();
       vp.setName(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       vp.setMasterPlot(plotInfo);
 
       JFrame frame = new JFrame(
         ""Calibration curve (x: estimated probability, y: observed probability) for ""
           + classAtt.value(classValue) + "" based on"" + "" "" + numBins
           + "" equal-frequency bins"");
       frame.setSize(1024, 800);
       frame.setContentPane(vp);
       frame.setVisible(true);
 
     } catch (Exception ex) {
       ex.printStackTrace();
     }
   }",Buggy,"Committed one small bug fix in name of Instances object. Removed one unnecessary import.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14764 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a small bug fix in the name of an Instances object, along with the removal of an unnecessary import. The diff shows a change in how the plot name is set, altering from embedding expressions within quotes to appending strings directly.

The original code:
```java
plotInfo.setPlotName(
    ""\""Calibration curve for class value \"" + classAtt.value(classValue)"");
```

was changed to:
```java
plotInfo.setPlotName(""Calibration curve for class value "" + classAtt.value(classValue));
```

This update changes the behavior of the string; previously, it was setting a literal string, including quotes, which was not the intention. The corrected line sets the name dynamically based on `classAtt.value(classValue)`. Thus, this is indeed a bug fix pertinent to setting the correct plot name based on the class attribute value.

Though the commit message also mentions an import removal, it is not visible in this diff segment. The observed change aligns with the intention described in the commit message to fix a bug, addressing an error likely present in the current plot name handling.

Therefore, this update indicates a bug fix as it addresses a logical error in string construction affecting runtime behavior.

**Buggy**"
weka,22844.json,0f8a283a8a350a9341d33f14127fb4d90fbe0b6c,"@@ -1,25 +1,40 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_ClassifierEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_ClassifierEditor.setCapabilitiesFilter(filterClass);
+    
+    // Check capabilities
+    m_StartBut.setEnabled(true);
+    Capabilities currentFilter = m_ClassifierEditor.getCapabilitiesFilter();
+    Classifier classifier = (Classifier) m_ClassifierEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (classifier != null && currentFilter != null && 
+        (classifier instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)classifier).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5382 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message clearly indicates a bug fix related to the enabled/disabled state of a ""start"" button. The code changes align well with the message:

1. **Context of Change:** The method `updateCapabilitiesFilter` is being modified. This function seems responsible for managing capabilities for some sort of classifier setup.

2. **Changes Made:**
   - An additional code block is introduced to determine whether the ""start"" button (`m_StartBut`) should be enabled based on the capabilities of a classifier and the provided filter.
   - Initially, the button is set to be enabled (`m_StartBut.setEnabled(true);`).
   - It then retrieves the current capabilities filter and the classifier.
   - It checks if the classifier supports the given filter either partially (`supportsMaybe`) or fully (`supports`).
   - If neither is true, it disables the ""start"" button (`m_StartBut.setEnabled(false);`).

3. **Bug Fix Validation:** The change ensures that the button's enabled state is contingent upon the classifier's capabilities in relation to the filter. If the classifier doesn't support the filter, the button is disabled, thus resolving the issue where the button state might not align with the current setup capabilities.

4. **Relevance of the Commit Message:** The message directly correlates with the changes, as they specifically address updating the button's state when a dataset is set on the panel.

As the diff shows a logical correction to maintain UI consistency based on the capabilities filter, it indeed seems to fix a bug regarding the UI state management of the start button.

Thus, the conclusion is **Buggy**."
weka,15202.json,e6a516aae06f40c5da24d36795d1ee48b4688fb5,"@@ -1,11 +1,17 @@
   public void setOptions(String[] options) throws Exception {
     
     setAttributeIndices(Utils.getOption('R', options));
     setInvertSelection(Utils.getFlag('V', options));
-    setClassName(Utils.getOption('C', options));
-    setMethodName(Utils.getOption('M', options));
+    String classString = Utils.getOption('C', options);
+    if (classString.length() != 0) {
+      setClassName(classString);
+    }
+    String methodString = Utils.getOption('M', options);
+    if (methodString.length() != 0) {
+      setMethodName(methodString);
+    }
 
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",Buggy,"Fixed some option-handling bugs.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2193 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that some option-handling bugs have been fixed. Analyzing the diff:

1. The `setClassName` and `setMethodName` methods previously used unchecked results from `Utils.getOption`, potentially leading to errors if empty strings are returned for missing options.
2. The fix introduces a condition that checks if the `classString` and `methodString` are non-empty before calling the respective setter methods.

This change addresses potential bugs where empty strings (indicating missing options) would be used inappropriately, which could lead to incorrect behavior or errors at runtime.

Given these considerations, the code changes align with the commit message’s claim that option-handling bugs are fixed, specifically with improvements to the logic that guards against inappropriate option handling.

Therefore, the conclusion is **Buggy**."
weka,23203.json,af433bd414a79a55cf517aeccf73cfc90d5c8089,"@@ -1,13 +1,14 @@
   public void setInstances(Instances inst) {
     m_Instances = inst;
     m_RelationNameLab.setText(m_Instances.relationName());
+    m_RelationNameLab.setToolTipText(m_Instances.relationName());
     m_NumInstancesLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + m_Instances.numInstances()));
     m_NumAttributesLab.setText("""" + m_Instances.numAttributes());
     m_sumOfWeightsLab.setText("""" + 
         ((m_showZeroInstancesAsUnknown && m_Instances.numInstances() == 0) 
             ? ""?"" 
             : """" + Utils.doubleToString(m_Instances.sumOfWeights(), 3)));
   }",Buggy,"Fixed a bug where the relation name would overlap the attributes. Now sets the tip text for the relation name label to be equal to the relation name (so that the full relation name can be seen when the mouse hovers over the name on the panel).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7229 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a UI issue where the relation name could be obscured by overlapping with attributes. The change in the code correctly aligns with this description, as it adds a tooltip to the relation name label. This allows users to see the full relation name when hovering over it with the mouse, resolving the problem mentioned in the commit message.

The update specifically addresses the issue by using `setToolTipText` with the full relation name, which is a standard method for providing additional context to users when UI elements might not display all information due to space constraints.

Therefore, the code modification aligns with the commit message's description of the bug fix related to UI display, and it's appropriate to conclude that this change is a bug fix.

**Buggy**"
weka,27297.json,334f1a26a1895da32f511cae40ee3143b87295d7,"@@ -1,18 +1,18 @@
   public double SVMOutput(Instance inst) throws Exception {
     
     double result = -m_b;
     // Is the machine linear?
     if (m_weights != null) {
       // Is weight vector stored in sparse format?
-      for (int i = 0; i < m_weights.length; i++) {
+      for (int i = 0; i < inst.numValues(); i++) {
 	if (inst.index(i) != m_classIndex) {
 	  result += m_weights[inst.index(i)] * inst.valueSparse(i);
 	}
       }
     } else {
       for (int i = m_supportVectors.getNext(-1); i != -1; i = m_supportVectors.getNext(i)) {
 	result += (m_alpha[i] - m_alphaStar[i]) * m_kernel.eval(-1, i, inst);
       }
     }
     return result;
   }",Buggy,"Fixed bug where a loop a loop iteration was not correct for sparse instances.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@6621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to incorrect loop iteration for sparse instances. The code change aligns with this description by adjusting the loop to iterate over the number of values in the instance (`inst.numValues()`) rather than the length of `m_weights`, which might not accurately reflect the required iteration count for sparse data. This correction ensures that only the non-zero elements of the instance are considered, preventing incorrect calculations when handling sparse data structures. This change directly addresses an iteration bug for sparse instances.

Therefore, the analysis leads to the conclusion that the code change is indeed a bug fix as described in the commit message. The appropriate conclusion for this change is **Buggy**."
weka,9635.json,d31aa1c73607a81824ab967b78e8bf672d9c1a1b,"@@ -1,110 +1,110 @@
   protected void purgeTasks(long purgeInterval) {
     Date now = new Date();
     long nowMilli = now.getTime();
     boolean doPurge = false;
 
     List<WekaTaskEntry> taskList = m_taskMap.getTaskList();
     for (WekaTaskEntry t : taskList) {
       NamedTask task = m_taskMap.getTask(t);
       doPurge = false;
       if (!(task instanceof Scheduled)) {
         // Date lastExecuted = getExecutionTime(t);
         Date lastExecuted = t.getLastExecution();
         if (lastExecuted != null) {
           if (task.getTaskStatus().getExecutionStatus() == TaskStatusInfo.PROCESSING) {
             // don't purge executing tasks!!
             continue;
           }
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       } else {
         Date lastExecuted = t.getLastExecution();
         Date nextExecution =
           ((Scheduled) task).getSchedule().nextExecution(lastExecuted);
         if (nextExecution == null && lastExecuted != null) {
           long milli = lastExecuted.getTime();
 
           // leave tasks that were sent to us from another server for twice as
           // long in
           // order to give the master a chance to tell us to purge them
           long pI =
             (t.getCameFromMaster() ? (purgeInterval * 2) : purgeInterval);
 
           if (nowMilli - milli > pI) {
             doPurge = true;
           }
         }
       }
 
       if (doPurge) {
         PostMethod post = null;
         InputStream is = null;
 
         try {
           String url = ""http://"" + getHostname() + "":"" + getPort();
           url = url.replace("" "", ""%20"");
           url += PurgeTaskServlet.CONTEXT_PATH;
-          url += ""/?name="" + t.toString();
+	  url += ""/?name="" + URLEncoder.encode(t.toString(), ""UTF-8"");
           url += ""&client=Y"";
 
           post = new PostMethod(url);
           post.setDoAuthentication(true);
           post.addRequestHeader(new Header(""Content-Type"", ""text/plain""));
 
           // Get HTTP client
           HttpClient client =
             ConnectionManager.getSingleton().createHttpClient();
           ConnectionManager.addCredentials(client, m_username, m_password);
 
           // Execute request
           int result = client.executeMethod(post);
           // System.out.println(""[WekaServer] Response from master server : "" +
           // result);
           if (result == 401) {
             System.err.println(""[WekaServer] Unable to purge task""
               + "" - authentication required.\n"");
           } else {
             // the response
             is = post.getResponseBodyAsStream();
             ObjectInputStream ois = new ObjectInputStream(is);
             Object response = ois.readObject();
             if (response.toString().startsWith(WekaServlet.RESPONSE_ERROR)) {
               System.err.println(""[WekaServer] A problem occurred while ""
                 + ""trying to purge task ("" + t.toString() + ""): \n"" + ""\t""
                 + response.toString());
             } else {
               System.out.println(""[WekaServer] purged task: "" + t.toString());
             }
           }
         } catch (Exception ex) {
           System.err
             .println(""[WekaServer] A problem occurred while ""
               + ""trying to purge task ("" + t.toString() + ""): ""
               + ex.getMessage());
           ex.printStackTrace();
         } finally {
           if (is != null) {
             try {
               is.close();
             } catch (IOException e) {
               e.printStackTrace();
             }
           }
 
           if (post != null) {
             post.releaseConnection();
           }
         }
       }
     }
   }",Buggy,"Fixed a bug in the purge task thread

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12578 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code diff involves using `URLEncoder.encode(t.toString(), ""UTF-8"")` instead of simply using `t.toString()` directly in the URL construction. This ensures that any special characters within the task name are properly encoded for inclusion in a URL, preventing potential issues with URL formatting or incorrect HTTP requests. Such issues could prevent the task from being purged if the URL was malformed.

The commit message states, ""Fixed a bug in the purge task thread,"" which aligns with this change, as incorrect URL formatting would indeed be a bug affecting the functionality of the purge task process.

Thus, this code modification fits the context of a bug fix as described in the commit message, addressing a potential issue with malformed URLs during the purge operation.

The answer is **Buggy**."
weka,33400.json,a69dfab4b09e9d52542306b240900fdbfb951df3,"@@ -1,145 +1,146 @@
   private Instance vectorizeInstance(Instance input, int[] offsetHolder,
     boolean retainStringAttValuesInMemory) throws Exception {
 
     if (!m_inputContainsStringAttributes) {
       return input;
     }
 
     if (m_inputFormat == null) {
       throw new Exception(""No input format available. Call setup() and ""
         + ""make sure a dictionary has been built first."");
     }
 
     if (m_consolidatedDict == null) {
       throw new Exception(""Dictionary hasn't been built or consolidated yet!"");
     }
 
     int indexOffset = 0;
     int classIndex = m_outputFormat.classIndex();
     Map<Integer, double[]> contained = new TreeMap<Integer, double[]>();
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (!m_selectedRange.isInRange(i)) {
         if (!m_inputFormat.attribute(i).isString()
           && !m_inputFormat.attribute(i).isRelationValued()) {
 
           // add nominal and numeric directly
           if (input.value(i) != 0.0) {
             contained.put(indexOffset, new double[] { input.value(i) });
           }
         } else {
           if (input.isMissing(i)) {
             contained.put(indexOffset, new double[] { Utils.missingValue() });
           } else if (m_inputFormat.attribute(i).isString()) {
             String strVal = input.stringValue(i);
             if (retainStringAttValuesInMemory) {
               double strIndex =
                 m_outputFormat.attribute(indexOffset).addStringValue(strVal);
               contained.put(indexOffset, new double[] { strIndex });
             } else {
               m_outputFormat.attribute(indexOffset).setStringValue(strVal);
               contained.put(indexOffset, new double[] { 0 });
             }
           } else {
             // relational
             if (m_outputFormat.attribute(indexOffset).numValues() == 0) {
               Instances relationalHeader =
                 m_outputFormat.attribute(indexOffset).relation();
 
               // hack to defeat sparse instances bug
               m_outputFormat.attribute(indexOffset).addRelation(
                 relationalHeader);
             }
             int newIndex =
               m_outputFormat.attribute(indexOffset).addRelation(
                 input.relationalValue(i));
             contained.put(indexOffset, new double[] { newIndex });
           }
         }
         indexOffset++;
       }
     }
 
     offsetHolder[0] = indexOffset;
 
     // dictionary entries
     for (int i = 0; i < m_inputFormat.numAttributes(); i++) {
       if (m_selectedRange.isInRange(i) && !input.isMissing(i)) {
         m_tokenizer.tokenize(input.stringValue(i));
 
         while (m_tokenizer.hasMoreElements()) {
           String word = m_tokenizer.nextElement();
           if (m_lowerCaseTokens) {
             word = word.toLowerCase();
           }
           word = m_stemmer.stem(word);
 
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount != null) {
             if (m_outputCounts) {
               double[] inputCount =
                 contained.get(idxAndDocCount[0] + indexOffset);
               if (inputCount != null) {
                 inputCount[0]++;
               } else {
                 contained.put(idxAndDocCount[0] + indexOffset,
                   new double[] { 1 });
               }
             } else {
               contained
                 .put(idxAndDocCount[0] + indexOffset, new double[] { 1 });
             }
           }
         }
       }
     }
 
     // TF transform
     if (m_TFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           val[0] = Math.log(val[0] + 1);
         }
       }
     }
 
     // IDF transform
     if (m_IDFTransform) {
       for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
         int index = e.getKey();
         if (index >= indexOffset) {
           double[] val = e.getValue();
           String word = m_outputFormat.attribute(index).name();
+          word = word.substring(m_Prefix.length());
           int[] idxAndDocCount = m_consolidatedDict.get(word);
           if (idxAndDocCount == null) {
             throw new Exception(""This should never occur"");
           }
           if (idxAndDocCount.length != 2) {
             throw new Exception(""Can't compute IDF transform as document ""
               + ""counts are not available"");
           }
           val[0] = val[0] * Math.log(m_count / (double) idxAndDocCount[1]);
         }
       }
     }
 
     double[] values = new double[contained.size()];
     int[] indices = new int[contained.size()];
     int i = 0;
     for (Map.Entry<Integer, double[]> e : contained.entrySet()) {
       values[i] = e.getValue()[0];
       indices[i++] = e.getKey().intValue();
     }
 
     Instance inst =
       new SparseInstance(input.weight(), values, indices,
         m_outputFormat.numAttributes());
     inst.setDataset(m_outputFormat);
 
     if (m_normalize) {
       normalizeInstance(inst, indexOffset);
     }
 
     return inst;
   }",Buggy,"Fixed a bug that affected the -P option when used in conjuction with the IDF transform

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12931 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that a bug fix was applied to the -P option when used with the IDF transform. Let's analyze the diff to see how it reflects this adjustment:

1. **Understanding of the Existing Process**: 
   - The IDF (Inverse Document Frequency) transform is typically applied to words/token entries to assess their importance relative to a document-having a particular token against the total number of documents.
   - From the code, IDF is computed with `val[0] * Math.log(m_count / (double) idxAndDocCount[1]);`, where `val[0]` represents term frequency (TF), `m_count` may denote the total count of documents, and `idxAndDocCount[1]` represents the document count containing that token.

2. **Inspection of the Code Change**:
   - The substring change that removes an `m_Prefix.length()` portion from `word` before using it to fetch data from `m_consolidatedDict` was added. 

3. **Effect of the Prefix Removal**:
   - This change signifies that previously, the `word` being looked up in the dictionary was possibly prefixed, leading to lookup failures or incorrect IDF calculations when prefixed tokens didn't match entries in `m_consolidatedDict`.
   - By stripping the prefix, the code now correctly matches tokens in the dictionary, allowing accurate IDF computations.

4. **Correlation with Bug Fix**:
   - The prefix adjustment directly addresses a bug where the incorrect matching of token entries could lead to exceptions or inaccurate IDF values, aligning with the commit message indicating a bug fix.

Hence, this code modification clearly reflects a bug fix enhancement for the IDF transform functionality encompassed within the commit message. Therefore, the conclusion is **Buggy**."
weka,3374.json,b97c9f7d4a47bf18c449215421f95140d8c6c5ea,"@@ -1,222 +1,222 @@
   public boolean runJobWithContext(JavaSparkContext sparkContext)
     throws IOException, DistributedWekaException {
 
     m_currentContext = sparkContext;
     setJobStatus(JobStatus.RUNNING);
     boolean success = false;
 
     if (m_env == null) {
       m_env = Environment.getSystemWide();
     }
 
     // Make sure that we save out to a subdirectory of the output
     // directory
     String outputPath = environmentSubstitute(m_sjConfig.getOutputDir());
     outputPath = addSubdirToPath(outputPath, OUTPUT_SUBDIR);
 
     JavaRDD<Instance> inputData = null;
     Instances headerWithSummary = null;
     if (getDataset(TRAINING_DATA) != null) {
       inputData = getDataset(TRAINING_DATA).getDataset();
       headerWithSummary = getDataset(TRAINING_DATA).getHeaderWithSummary();
       logMessage(""RDD<Instance> dataset provided: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     if (inputData == null && headerWithSummary == null) {
       logMessage(""[Randomly shuffle data] Invoking ARFF Job..."");
       m_arffHeaderJob.setEnvironment(m_env);
       m_arffHeaderJob.setLog(getLog());
       m_arffHeaderJob.setStatusMessagePrefix(m_statusMessagePrefix);
       m_arffHeaderJob.setCachingStrategy(getCachingStrategy());
 
       // header job necessary?
       success = m_arffHeaderJob.runJobWithContext(sparkContext);
 
       if (!success) {
         setJobStatus(JobStatus.FAILED);
         statusMessage(""Unable to continue - creating the ARFF header failed!"");
         logMessage(""[Randomly shuffle data] Unable to continue - creating the ARFF header failed!"");
         return false;
       }
 
       Dataset d = m_arffHeaderJob.getDataset(TRAINING_DATA);
 
       headerWithSummary = d.getHeaderWithSummary();
       inputData = d.getDataset();
       logMessage(""Fetching RDD<Instance> dataset from ARFF job: ""
         + inputData.partitions().size() + "" partitions."");
     }
 
     /*
      * int minSlices = 1; if
      * (!DistributedJobConfig.isEmpty(m_sjConfig.getMinInputSlices())) { try {
      * minSlices = Integer
      * .parseInt(environmentSubstitute(m_sjConfig.getMinInputSlices())); } catch
      * (NumberFormatException e) { } }
      */
 
     /*
      * if (!m_cleanOutputDir) { // check for existing chunk files... String
      * pathPlusChunk = outputPath + ""/part-00000"";
      * logMessage(""[Randomly shuffle data] Checking output directory: "" +
      * outputPath); if (SparkJob.checkFileExists(pathPlusChunk)) {
      * logMessage(""[Randomly shuffle data] Output directory is populated "" +
      * ""with randomly shuffled chunk files already - "" + ""no need to execute."");
      * 
      * loadShuffledDataFiles(outputPath, sparkContext,
      * CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary),
      * minSlices); return true; } }
      */
 
     /*
      * // TODO revisit at some stage... Current assumption: if you // have
      * output from this job as serialized instances then you // are happy with
      * the shuffling and will not want to re-shuffle if
      * (m_sjConfig.getSerializedInput()) { throw new DistributedWekaException(
      * ""Randomly shuffling serialized Instance "" +
      * ""input is not supported yet.""); }
      */
 
     // clean the output directory
     SparkJob.deleteDirectory(outputPath);
 
     String inputFile = environmentSubstitute(m_sjConfig.getInputFile());
 
     int seed = 1;
     if (!DistributedJobConfig.isEmpty(getRandomSeed())) {
       seed = Integer.parseInt(environmentSubstitute(getRandomSeed()));
     }
     final Instances headerNoSummary =
       CSVToARFFHeaderReduceTask.stripSummaryAtts(headerWithSummary);
 
     try {
       WekaClassifierSparkJob.setClassIndex(
         environmentSubstitute(m_classAttribute), headerNoSummary,
         !m_dontDefaultToLastAttIfClassNotSpecified);
 
     } catch (Exception e) {
       logMessage(e);
       throw new DistributedWekaException(e);
     }
 
     // find summary attribute for class (if set, otherwise just use the first
     // numeric on nominal attribute). We're using this simply to find out
     // the total number of instances in the dataset
     String className = null;
     if (headerNoSummary.classIndex() >= 0) {
       className = headerNoSummary.classAttribute().name();
     } else {
       for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
         if (headerNoSummary.attribute(i).isNumeric()
           || headerNoSummary.attribute(i).isNominal()) {
           className = headerNoSummary.attribute(i).name();
           break;
         }
       }
     }
     Attribute summaryClassAtt =
       headerWithSummary
         .attribute(CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX
           + className);
     if (summaryClassAtt == null) {
       throw new DistributedWekaException(
         ""Was unable to find the summary attribute for "" + ""the class: ""
           + className);
     }
 
     int totalNumInstances = 0;
     int numFoldSlices = 10;
     // summary attribute for getting the total number of instances
     Attribute summaryAttOrig = null;
     for (int i = 0; i < headerNoSummary.numAttributes(); i++) {
       if (headerNoSummary.attribute(i).isNumeric()
         || headerNoSummary.attribute(i).isNominal()) {
         summaryAttOrig = headerNoSummary.attribute(i);
         break;
       }
     }
     String summaryName = summaryAttOrig.name();
     Attribute summaryAtt =
       headerWithSummary
         .attribute(
           CSVToARFFHeaderMapTask.ARFF_SUMMARY_ATTRIBUTE_PREFIX + summaryName);
     if (summaryAtt == null) {
       logMessage(""[RandomizedDataSparkJob] Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
       throw new DistributedWekaException(""Was unable to find the summary ""
         + ""attribute for attribute: "" + summaryName);
     }
 
-    if (summaryAtt.isNominal()) {
+    if (summaryAttOrig.isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     if (DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())
       && DistributedJobConfig.isEmpty(getNumInstancesPerShuffledSplit())) {
       logMessage(""[RandomizedDataSparkJob] Must specify either the number of ""
         + ""splits or the number of instances per split"");
       throw new DistributedWekaException(""Must specify either the number of ""
         + ""splits or the number of instances per split"");
     }
 
     if (!DistributedJobConfig.isEmpty(getNumRandomlyShuffledSplits())) {
       numFoldSlices =
         Integer.parseInt(environmentSubstitute(getNumRandomlyShuffledSplits()));
     } else {
       int numInsts = 0;
       try {
         numInsts =
           Integer
             .parseInt(environmentSubstitute(getNumInstancesPerShuffledSplit()));
       } catch (NumberFormatException ex) {
         throw new DistributedWekaException(ex);
       }
 
       if (numInsts <= 0) {
         throw new DistributedWekaException(
           ""Number of instances per split must "" + ""be > 0"");
       }
 
       if (numInsts > totalNumInstances) {
         throw new DistributedWekaException(""Can't have more instances per split ""
           + ""than there are instances in the dataset!"");
       }
       double nc = (double) totalNumInstances / numInsts;
       nc = Math.ceil(nc);
       numFoldSlices = (int) nc;
     }
     logMessage(""[Randomly shuffle] creating "" + numFoldSlices + "" splits."");
 
     if (headerNoSummary.attribute(className).isNominal()) {
       NominalStats stats = NominalStats.attributeToStats(summaryClassAtt);
       for (String label : stats.getLabels()) {
         totalNumInstances += stats.getCount(label);
       }
     } else {
       NumericStats stats = NumericStats.attributeToStats(summaryClassAtt);
       totalNumInstances =
         (int) stats.getStats()[ArffSummaryNumericMetric.COUNT.ordinal()];
     }
 
     logMessage(""[Randomly shuffle data] Num slices = "" + numFoldSlices);
 
     final Random random = new Random(seed);
     for (int i = 0; i < 20; i++) {
       random.nextInt();
     }
 
     performRandomShuffle(inputData, outputPath, numFoldSlices, random,
       headerWithSummary, headerNoSummary.classIndex());
 
     setJobStatus(JobStatus.FINISHED);
 
     return true;
   }",Buggy,"Fixed a bug in the calculation of the total number of instances in the dataset

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11588 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug related to the calculation of the total number of instances in a dataset has been fixed. Let's examine the code changes to assess whether they align with this claim.

1. The modified code section involves the method for calculating `totalNumInstances`.
2. The original code uses `summaryAtt.isNominal()` to determine whether to use nominal statistics (`NominalStats`) or numeric statistics (`NumericStats`) for calculating the total number of instances.
3. The change updates the condition to use `summaryAttOrig.isNominal()` instead of `summaryAtt.isNominal()`.

Given the context of the method:
- `summaryAtt` is an attribute with summary information, and `summaryAttOrig` appears to be the original attribute without summary information.
- Using `summaryAttOrig.isNominal()` ensures that the decision of using nominal or numeric statistics is based on the original attribute's type rather than the summary attribute's type.
- This change is significant because it corrects which attribute type the logic checks against, which is crucial for accurate statistics calculations (i.e., determining the count of instances accurately).

The change fixes potential errors in the total number of instances calculation due to incorrect logic in checking the attribute type. Therefore, this change aligns well with the commit message as it addresses a bug in calculating the total number of instances in the dataset.

Given this analysis, the commit changes reflect a bug fix, and the appropriate label is **Buggy**."
weka,4830.json,9202bc66506c46f20388951597366af165de4928,"@@ -1,76 +1,77 @@
   public void processIncoming(Data data) throws WekaException {
 
     if (isStopRequested()) {
       getStepManager().interrupted();
       return;
     }
 
     getStepManager().processing();
 
     Instances predictedInsts = data.getPrimaryPayload();
     int maxSetNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM);
     int setNum =
       (Integer) data.getPayloadElement(StepManager.CON_AUX_DATA_SET_NUM);
 
     if (maxSetNum > 1 && getPoolSets()) {
       if (m_isReset) {
         m_pooledData = new Instances(predictedInsts);
       } else {
         m_pooledData.addAll(predictedInsts);
       }
     }
 
     m_isReset = false;
 
     if (getPoolSets() && setNum < maxSetNum) {
+      getStepManager().finished();
       return;
     }
 
     if (getPoolSets() && maxSetNum > 1) {
       predictedInsts = m_pooledData;
     }
 
     if (predictedInsts.classIndex() < 0) {
       throw new WekaException(""No class set in the predicted data!"");
     }
 
     int numAttributes = predictedInsts.numAttributes();
     int numClasses = predictedInsts.classAttribute().numValues();
 
     // we always produce a curve for the first label. The user can
     // always choose a label by using a ClassValuePicker step
     Attribute classAtt = predictedInsts.classAttribute();
     Attribute predictedLabelProbAtt =
       predictedInsts.attribute(numAttributes - numClasses);
 
     ArrayList<Prediction> preds = new ArrayList<>();
     for (int i = 0; i < predictedInsts.numInstances(); i++) {
       Instance current = predictedInsts.instance(i);
       double[] dist = new double[numClasses];
       dist[0] = current.value(predictedLabelProbAtt.index());
       double actual = current.classValue();
 
       preds.add(new NominalPrediction(actual, dist, current.weight()));
     }
 
     try {
       Instances curveInsts =
         CalibrationCurveUtils
           .getCalibrationCurveAsInstances(preds, classAtt, 0);
       curveInsts.setRelationName(""__"" + curveInsts.relationName());
 
       Instance zero = curveInsts.remove(curveInsts.numInstances() - 2);
       zero.setWeight(-1);
       curveInsts.add(0, zero);
       curveInsts.lastInstance().setWeight(-1);
       Data output = new Data(StepManager.CON_DATASET, curveInsts);
       output.setPayloadElement(StepManager.CON_AUX_DATA_SET_NUM, 1);
       output.setPayloadElement(StepManager.CON_AUX_DATA_MAX_SET_NUM, 1);
       getStepManager().outputData(output);
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     getStepManager().finished();
   }",Buggy,"Small bug fix in reporting finished state for step

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14772 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a small bug fix in reporting the finished state for a step. The code diff reflects this change with the addition of a call to `getStepManager().finished();` inside a conditional block:

```java
if (getPoolSets() && setNum < maxSetNum) {
      getStepManager().finished();
      return;
}
```

This suggests that in certain conditions when the step processing was being pooled, the finished state was not correctly reported. By adding `getStepManager().finished();` in this block, the code now signals that processing has indeed been successfully completed for that particular scenario before returning, which aligns with the commit message describing a ""small bug fix in reporting finished state for step.""

This change clearly addresses an issue where the finished state wasn't being recorded, thus confirming that this is indeed a bug fix in the process control logic. Therefore, the commit aligns well with the message and indeed indicates a bug fix.

The correct conclusion is **Buggy**."
weka,36414.json,81a1e225e8da398eb2d17f911d2de0b246ac22aa,"@@ -1,3 +1,3 @@
     void setUpdateIncrementalClassifier(boolean update) {
-    m_updateIncrementalClassifier = true;
+    m_updateIncrementalClassifier = update;
   }",Buggy,"Fixed a bug in the set method for updating an incremental classifier

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13045 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message clearly indicates that a bug was fixed in the `set` method for updating an incremental classifier. 

Analyzing the code changes:

- The method `setUpdateIncrementalClassifier` is supposed to use the parameter `update` to set the value of `m_updateIncrementalClassifier`.
- In the original code, `m_updateIncrementalClassifier` was always being set to `true`, which would ignore the `update` argument and likely cause incorrect behavior.
- The change corrects this error by setting `m_updateIncrementalClassifier` to the value of `update`, which aligns with the intended functionality described in the commit message.

This change is clearly related to fixing a bug where the method did not respect its input parameter, leading to incorrect behavior.

Thus, this change should be classified as **Buggy**."
weka,15017.json,a9246f58e2844ebadfa2c2b12f3f4d15755f5006,"@@ -1,58 +1,59 @@
   protected Instances process(Instances instances) throws Exception {
     // initializing necessary?
     if (!m_Initialized) {
       // do we have a file to initialize with?
       if ((getInitFile() != null) && getInitFile().isFile()) {
 	DataSource source = new DataSource(getInitFile().getAbsolutePath());
 	Instances data = source.getDataSet();
 	m_InitFileClassIndex.setUpper(data.numAttributes() - 1);
 	data.setClassIndex(m_InitFileClassIndex.getIndex());
 	initFilter(data);
       }
       else {
 	initFilter(instances);
       }
     }
 
     // apply filters
     if (m_Missing != null)
       instances = Filter.useFilter(instances, m_Missing); 
     if (m_NominalToBinary != null)
       instances = Filter.useFilter(instances, m_NominalToBinary); 
     if (m_ActualFilter != null)
       instances = Filter.useFilter(instances, m_ActualFilter);
 
     // backup class attribute and remove it
     double[] classes = instances.attributeToDoubleArray(instances.classIndex());
     int classIndex = instances.classIndex();
+    Attribute classAttribute = (Attribute)instances.classAttribute().copy();
     instances.setClassIndex(-1);
     instances.deleteAttributeAt(classIndex);
 
     // generate new header
     FastVector atts = new FastVector();
     for (int j = 0; j < m_NumTrainInstances; j++)
       atts.addElement(new Attribute(""Kernel "" + j));
-    atts.addElement(new Attribute(""Class""));
+    atts.addElement(classAttribute);
     Instances result = new Instances(""Kernel"", atts, 0);
     result.setClassIndex(result.numAttributes() - 1);
 
     // compute matrix
     for (int i = 0; i < instances.numInstances(); i++) {
       double[] k = new double[m_NumTrainInstances + 1];
       
       for (int j = 0; j < m_NumTrainInstances; j++) {
 	double v = m_ActualKernel.eval(-1, j, instances.instance(i));
 	k[j] = v;
       }
       k[k.length - 1] = classes[i];
 
       // create new instance
       Instance in = new DenseInstance(1.0, k);
       result.add(in);    
     }
 
     if (!isFirstBatchDone())
       setOutputFormat(result);
     
     return result;
   }",Buggy,"Fixed bug in KernelFilter: now works with nominal class attributes as well.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@9561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states a bug fix in *KernelFilter* for handling nominal class attributes. The code diff shows changes to retain the nominal class attribute from the original data, replacing it with a method that preserves its properties:

1. **Preservation of Class Attributes**: The addition of `Attribute classAttribute = (Attribute)instances.classAttribute().copy();` and replacing `atts.addElement(new Attribute(""Class""));` with `atts.addElement(classAttribute);` shows that the code is now retaining information about the class attribute from the original dataset, including its nominal properties. This change is critical for datasets using nominal class attributes, ensuring they are not accidentally converted to incompatible types or lost.

2. **Bug Fix Alignment**: The modification directly addresses the issue mentioned in the commit message, focusing on proper handling of nominal class attributes. This implies the previous implementation likely failed to correctly process such attributes, potentially treating them as generic classes without their specific nominal characteristics.

3. **Handling and Processing**: The rest of the logic remains unchanged, ensuring the core processing is consistent while the bug fix ensures correct setup and processing regarding class attributes.

Given this analysis, the changes indicate a specific bug fix related to ensuring proper class attribute handling, particularly for nominal attributes, aligning with the commit message detailing. Hence, this deserves the conclusion **Buggy**."
weka,36274.json,8072e35dd2cf69509b7d8d77a0a509d3ff3f5202,"@@ -1,82 +1,86 @@
   protected static void addStepJSONtoFlowArray(JSONNode stepArray,
     StepManagerImpl stepManager) throws WekaException {
 
     JSONNode step = stepArray.addObjectArrayElement();
     step.addPrimitive(""class"", stepManager.getManagedStep().getClass()
       .getCanonicalName());
     // step.addPrimitive(STEP_NAME, stepManager.getManagedStep().getName());
     JSONNode properties = step.addObject(PROPERTIES);
     try {
       Step theStep = stepManager.getManagedStep();
       BeanInfo bi = Introspector.getBeanInfo(theStep.getClass());
       PropertyDescriptor[] stepProps = bi.getPropertyDescriptors();
 
       for (PropertyDescriptor p : stepProps) {
         if (p.isHidden() || p.isExpert()) {
           continue;
         }
 
         String name = p.getDisplayName();
         Method getter = p.getReadMethod();
         Method setter = p.getWriteMethod();
         if (getter == null || setter == null) {
           continue;
         }
         boolean skip = false;
         for (Annotation a : getter.getAnnotations()) {
           if (a instanceof NotPersistable) {
             skip = true;
             break;
           }
         }
         if (skip) {
           continue;
         }
 
         Object[] args = {};
         Object propValue = getter.invoke(theStep, args);
         if (propValue == null) {
           properties.addNull(name);
         } else if (propValue instanceof Boolean) {
           properties.addPrimitive(name, (Boolean) propValue);
         } else if (propValue instanceof Integer || propValue instanceof Long) {
           properties.addPrimitive(name,
             new Integer(((Number) propValue).intValue()));
         } else if (propValue instanceof Double) {
           properties.addPrimitive(name, (Double) propValue);
         } else if (propValue instanceof Number) {
           properties.addPrimitive(name,
             new Double(((Number) propValue).doubleValue()));
         } else if (propValue instanceof weka.core.converters.Loader) {
           addLoader(name, (weka.core.converters.Loader) propValue, properties);
         } else if (propValue instanceof weka.core.converters.Saver) {
           addSaver(name, (weka.core.converters.Saver) propValue, properties);
         } else if (propValue instanceof OptionHandler) {
           addOptionHandler(name, (OptionHandler) propValue, properties);
         } else if (propValue instanceof Enum) {
           addEnum(name, (Enum) propValue, properties);
+        } else if (propValue instanceof File) {
+          String fString = propValue.toString();
+          fString = fString.replace('\\', '/');
+          properties.addPrimitive(name, fString);
         } else {
           properties.addPrimitive(name, propValue.toString());
         }
       }
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
 
     JSONNode connections = step.addObject(CONNECTIONS);
     for (Map.Entry<String, List<StepManager>> e : stepManager.m_connectedByTypeOutgoing
       .entrySet()) {
       String connName = e.getKey();
       JSONNode connTypeArray = connections.addArray(connName);
       for (StepManager c : e.getValue()) {
         connTypeArray.addArrayElement(c.getName());
       }
     }
 
     if (stepManager.getStepVisual() != null) {
       String coords =
         """" + stepManager.getStepVisual().getX() + "",""
           + stepManager.getStepVisual().getY();
       step.addPrimitive(COORDINATES, coords);
     }
   }",Buggy,"Fixed a bug that affected the parsing of step properties involving files containing Windows separator charactors

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12964 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The given diff provides evidence of a bug fix related to handling file paths in properties. The original code's absence of special handling for `File` objects could lead to incorrect path representation, especially on systems using different directory separators, like Windows.

Here's the specific analysis:

1. **Problem Context**: The commit message mentions fixing a parsing issue with step properties involving files, particularly when using Windows separator characters (`\`).

2. **Code Change**: 
   - The addition of the `if` block that checks if `propValue` is an instance of `File`.
   - Converts the file path string representation from Windows-style separators (`\`) to Unix-style (`/`) using `replace('\\', '/')`.

3. **Impact Analysis**:
   - Handling `File` objects specifically by normalizing path separators is significant, especially for cross-platform compatibility.
   - Without this change, file paths may not be parsed or serialized correctly when properties are processed on different operating systems - a notable bug.

Overall, the added code ensures uniformity in path separators, preventing potential bugs in systems managing file properties. Therefore, this code change addresses a bug in the system, consistent with the commit message.

Conclusion: **Buggy**."
weka,26684.json,f36aad315fb901cb71922f7ae08459999978c9d9,"@@ -1,65 +1,83 @@
   public Instances getCurve(FastVector predictions, int classIndex) {
 
     if ((predictions.size() == 0) ||
         (((NominalPrediction)predictions.elementAt(0))
          .distribution().length <= classIndex)) {
       return null;
     }
 
     double totPos = 0, totNeg = 0;
     double [] probs = getProbabilities(predictions, classIndex);
 
     // Get distribution of positive/negatives
     for (int i = 0; i < probs.length; i++) {
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(i);
       if (pred.actual() == Prediction.MISSING_VALUE) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with missing class value"");
         continue;
       }
       if (pred.weight() < 0) {
         System.err.println(getClass().getName() 
                            + "" Skipping prediction with negative weight"");
         continue;
       }
       if (pred.actual() == classIndex) {
         totPos += pred.weight();
       } else {
         totNeg += pred.weight();
       }
     }
 
     Instances insts = makeHeader();
     int [] sorted = Utils.sort(probs);
     TwoClassStats tc = new TwoClassStats(totPos, totNeg, 0, 0);
+    double threshold = 0;
+    double cumulativePos = 0;
+    double cumulativeNeg = 0;
     for (int i = 0; i < sorted.length; i++) {
+
+      if ((i == 0) || (probs[sorted[i]] > threshold)) {
+	tc.setTruePositive(tc.getTruePositive() - cumulativePos);
+	tc.setFalseNegative(tc.getFalseNegative() + cumulativePos);
+	tc.setFalsePositive(tc.getFalsePositive() - cumulativeNeg);
+	tc.setTrueNegative(tc.getTrueNegative() + cumulativeNeg);
+	threshold = probs[sorted[i]];
+	insts.add(makeInstance(tc, threshold));
+	cumulativePos = 0;
+	cumulativeNeg = 0;
+	if (i == sorted.length - 1) {
+	  break;
+	}
+      }
+
       NominalPrediction pred = (NominalPrediction)predictions.elementAt(sorted[i]);
+
       if (pred.actual() == Prediction.MISSING_VALUE) {
-        System.err.println(getClass().getName()
-                           + "" Skipping prediction with missing class value"");
-        continue;
+	System.err.println(getClass().getName()
+			   + "" Skipping prediction with missing class value"");
+	continue;
       }
       if (pred.weight() < 0) {
-        System.err.println(getClass().getName() 
-                           + "" Skipping prediction with negative weight"");
-        continue;
+	System.err.println(getClass().getName() 
+			   + "" Skipping prediction with negative weight"");
+	continue;
       }
       if (pred.actual() == classIndex) {
-        tc.setTruePositive(tc.getTruePositive() - pred.weight());
-        tc.setFalseNegative(tc.getFalseNegative() + pred.weight());
+	cumulativePos += pred.weight();
       } else {
-        tc.setFalsePositive(tc.getFalsePositive() - pred.weight());
-        tc.setTrueNegative(tc.getTrueNegative() + pred.weight());
+	cumulativeNeg += pred.weight();
       }
+
       /*
       System.out.println(tc + "" "" + probs[sorted[i]] 
                          + "" "" + (pred.actual() == classIndex));
       */
-      if ((i != (sorted.length - 1)) &&
+      /*if ((i != (sorted.length - 1)) &&
           ((i == 0) ||  
           (probs[sorted[i]] != probs[sorted[i - 1]]))) {
         insts.add(makeInstance(tc, probs[sorted[i]]));
-      }
+	}*/
     }
     return insts;
   }",Buggy,"Fixed bug in ThresholdCurve, which resulted in one instance being on the wrong side of the threshold.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2134 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The provided code diff and commit message indicate a bug fix in the `ThresholdCurve` class. Here's a step-by-step analysis of the changes:

1. **Commit Message Analysis**: The message specifies that the bug involved an instance being incorrectly evaluated relative to a threshold. The fix aims to adjust this behavior.

2. **Code Diff Analysis**:
   - **Initialization of Variables**: New variables `threshold`, `cumulativePos`, and `cumulativeNeg` have been added and initialized to zero. These are used to track cumulative positives and negatives up to a given threshold.
   
   - **Loop Adjustments**: Within the main loop over `sorted`, logic has been added to update the `TwoClassStats` object whenever a new threshold is encountered. This is done by recalculating true positives, false negatives, false positives, and true negatives based on the cumulative counts computed so far.
   
   - **Condition Addition**: A condition `(i == 0) || (probs[sorted[i]] > threshold)` has been added to delineate when threshold statistics should be updated, ensuring correctness in the positioning of instances relative to the threshold.
   
   - **Cumulative Calculation**: For each prediction, the code now accumulates weights into `cumulativePos` and `cumulativeNeg` instead of directly updating the `TwoClassStats`. These accumulators represent the count of positive or negative predictions for the current threshold.

3. **Logic Fix**: This code changes the logic for creating and updating statistical data (true positive, false positive rates) by accounting for cumulative counts at specific threshold boundaries rather than doing so for each individual probability. This is crucial for correctly interpreting instances at the threshold, ensuring accurate division between positive and negative predictions.

Overall, the changes align with the commit message and clearly target a bug fix related to how instances are placed concerning a decision threshold. Therefore, the code changes specifically address the bug described in the commit message with the following improvements:

- They handle cases where multiple instances share the same probability value correctly.
- They ensure that statistics are updated only at necessary threshold transitions.

Hence, the changes correspond to a bug fix. The answer is **Buggy**."
weka,27269.json,8e4d3f359b3b7e7182d587aaab8f838daae5c8fa,"@@ -1,11 +1,14 @@
   public double eval(int id1, int id2, Instance inst1) 
     throws Exception {
 	
-    double div = Math.sqrt(super.eval(id1, id1, inst1) * 
-			   super.eval(id2, id2, m_data.instance(id2)));
-    if(div != 0){
+
+    double div = Math.sqrt(super.eval(id1, id1, inst1) * ((m_keys != null)
+                           ? super.eval(id2, id2, m_data.instance(id2))
+                           : super.eval(-1, -1, m_data.instance(id2))));
+
+    if(div != 0){      
       return super.eval(id1, id2, inst1) / div;
     } else {
       return 0;
     }
-  }

+  }",Buggy,"Fixed a bug in NormalizedPolyKernel that resulted in an attempted access to the cache for test instances


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2490 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message refers to a bug involving cache access for test instances. The change alters how `super.eval` is called for the second ID, `id2`. Specifically, if `m_keys` is `null`, it uses a default ID of `-1` when calling `super.eval` on `m_data.instance(id2)`, presumably indicating a fallback behavior for test instances where `id2` might not be available in a cache.

This conditional change implies a fix to avoid an improper or unintended cache access that could occur during evaluation by forcing a different code path when the `m_keys` cache is not available. This alteration aligns well with the commit's bug fix reference, indicating logical correction related to how the method handles different scenarios regarding instance access, which was not correctly managed before.

Thus, the code change correctly addresses the described bug related to the cache access, which is confirmed by the change in logic to prevent incorrect accesses, likely resulting in errors or misbehaviors. Therefore, the answer is **Buggy**."
weka,30125.json,4f854d24790f7ba7429f74355b69542147801667,"@@ -1,105 +1,104 @@
   public String toSummaryString() {
 
     StringBuffer result = new StringBuffer();
     result.append(""Relation Name:  "").append(relationName()).append('\n');
     result.append(""Num Instances:  "").append(numInstances()).append('\n');
     result.append(""Num Attributes: "").append(numAttributes()).append('\n');
     result.append('\n');
 
     result.append(Utils.padLeft("""", 5)).append(Utils.padRight(""Name"", 25));
     result.append(Utils.padLeft(""Type"", 5)).append(Utils.padLeft(""Nom"", 5));
     result.append(Utils.padLeft(""Int"", 5)).append(Utils.padLeft(""Real"", 5));
     result.append(Utils.padLeft(""Missing"", 12));
     result.append(Utils.padLeft(""Unique"", 12));
     result.append(Utils.padLeft(""Dist"", 6)).append('\n');
     Instances temp = new Instances(this);
     int total = temp.numInstances();
     for (int i = 0; i < numAttributes(); i++) {
       Attribute a = attribute(i);
       temp.sort(i);
       int intCount = 0, realCount = 0, missingCount = 0;
       int distinctCount = 0, uniqueCount = 0, currentCount = 0;
       double prev = Instance.missingValue();
       for (int j = 0; j < temp.numInstances(); j++) {
 	Instance current = temp.instance(j);
 	if (current.isMissing(i)) {
 	  missingCount = temp.numInstances() - j;
 	  break;
 	}
 	if (Utils.eq(current.value(i), prev)) {
 	  currentCount++;
 	} else {
 	  distinctCount++;
 	  if (currentCount == 1) {
 	    uniqueCount++;
 	  }
 	  if (currentCount > 0) {
 	    if (Utils.eq(prev, (double)((int)prev))) {
 	      intCount += currentCount;
 	    } else {
 	      realCount += currentCount;
 	    }
 	  }
 	  currentCount = 1;
 	  prev = current.value(i);
 	}
       }
       if (currentCount == 1) {
 	uniqueCount++;
       }
       if (currentCount > 0) {
 	if (Utils.eq(prev, (double)((int)prev))) {
 	  intCount += currentCount;
 	} else {
 	  realCount += currentCount;
 	}
       }
-      
       result.append(Utils.padLeft("""" + (i + 1), 4)).append(' ');
       result.append(Utils.padRight(a.name(), 25)).append(' ');
-      int percent;
+      long percent;
       switch (a.type()) {
       case Attribute.NOMINAL:
 	result.append(Utils.padLeft(""Nom"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.NUMERIC:
 	result.append(Utils.padLeft(""Num"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       case Attribute.STRING:
 	result.append(Utils.padLeft(""Str"", 4)).append(' ');
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       default:
 	result.append(Utils.padLeft(""???"", 4)).append(' ');
 	result.append(Utils.padLeft("""" + 0, 3)).append(""% "");
-	percent = 100 * intCount / total;
+	percent = Math.round(100.0 * intCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
-	percent = 100 * realCount / total;
+	percent = Math.round(100.0 * realCount / total);
 	result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
 	break;
       }
       result.append(Utils.padLeft("""" + missingCount, 5)).append("" /"");
-      percent = 100 * missingCount / total;
+      percent = Math.round(100.0 * missingCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + uniqueCount, 5)).append("" /"");
-      percent = 100 * uniqueCount / total;
+      percent = Math.round(100.0 * uniqueCount / total);
       result.append(Utils.padLeft("""" + percent, 3)).append(""% "");
       result.append(Utils.padLeft("""" + distinctCount, 5)).append(' ');
       result.append('\n');
     }
     return result.toString();
   }",Buggy,"- Fixed small rounding problem in summary method.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@105 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The changes in the diff focus on improving the precision of percentage calculations by using `Math.round` on floating-point division results. The previous code likely suffered from truncation due to integer division, which could lead to inaccuracies and a ""rounding problem"" when reporting percentages. 

By changing the calculation to use `Math.round(100.0 * value / total)`, the code now properly calculates the percentage value by multiplying by 100.0 (to ensure floating-point arithmetic) and rounding to the nearest whole number, which eliminates the truncation issue.

This correction aligns directly with the commit message that mentions fixing a small rounding problem in a summary method. The change improves the accuracy of the reported statistics.

Given this analysis, the changes address a specific bug related to rounding and numerical accuracy, confirming the intent mentioned in the commit message.

Therefore, the conclusion is **Buggy**."
weka,24450.json,d1fccde7b3f541c040561dbcfd7c68890d26bfad,"@@ -1,37 +1,37 @@
   public double logDensity(double[] valuePassed) {
     double[] value = valuePassed.clone();
     double logProb = 0;
     // calculate mean subtractions
     double[] subtractedMean = new double[value.length];
     for (int i = 0; i < value.length; i++) {
       subtractedMean[i] = value[i] - mean[i];
     }
-
+    value = subtractedMean.clone();
     double[][] L = this.chol.getL().getArray();
     int n = this.chol.getL().getRowDimension();
     // Solve L*Y = B;
     for (int k = 0; k < this.chol.getL().getRowDimension(); k++) {
       for (int i = 0; i < k; i++) {
         value[k] -= value[i] * L[k][i];
       }
 
       value[k] /= L[k][k];
     }
 
     // Solve L'*X = Y;
     for (int k = n - 1; k >= 0; k--) {
       for (int i = k + 1; i < n; i++) {
         value[k] -= value[i] * L[i][k];
       }
       value[k] /= L[k][k];
     }
 
     // compute dot product
     double innerProduct = 0;
     // do a fast dot product
     for (int i = 0; i < value.length; i++) {
       innerProduct += value[i] * subtractedMean[i];
     }
     logProb = lnconstant - innerProduct * 0.5;
     return logProb;
   }",Buggy,"Bug fixes from Uday

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10459 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the method `logDensity` involves assigning `subtractedMean` to `value` before using it in a series of operations. Originally, `value` was being directly manipulated, but now it utilizes the modified `subtractedMean`, which likely reflects the intended adjustment for mean subtraction before performing further calculations. This change appears to be a logical fix to correctly process the data. Given that this update corrects how the data should be handled, it is indeed aligning with a bug-related change.

Thus, the answer is **Buggy**."
weka,15350.json,21d3ace55220b46f80de8bfd5ac1a94760fe142b,"@@ -1,39 +1,38 @@
   protected Instances process(Instances instances) throws Exception {
 
     Instances result = getOutputFormat();
 
     for (Instance inst : instances) {
       if (instances.numAttributes() < result.numAttributes()) { // Do we actually need to add an attribute?
         double[] newVals = new double[result.numAttributes()];
         for (int i = 0; i < inst.numValues(); i++) {
           newVals[inst.index(i)] = inst.valueSparse(i);
         }
         String value = """";
         for (int i = 0; i < inst.numAttributes(); i++) {
           if (instances.attribute(i).isNominal() && m_Attributes.isInRange(i) && i != instances.classIndex()) {
             if (Utils.isMissingValue(newVals[i])) {
               value = null;
               break;
             } else {
               value += (value.length() > 0) ? ""_x_"" + instances.attribute(i).value((int) newVals[i]) :
                       instances.attribute(i).value((int) newVals[i]);
             }
           }
         }
         if (value == null) {
           newVals[newVals.length - 1] = Double.NaN;
         } else {
           newVals[newVals.length - 1] = result.attribute(result.numAttributes() - 1).indexOfValue(value);;
         }
-        if (inst instanceof DenseInstance) {
-          result.add(new DenseInstance(inst.weight(), newVals));
-        } else {
-          result.add(new SparseInstance(inst.weight(), newVals));
-        }
+        Instance newInst = inst.copy(newVals);
+        copyValues(newInst, false, inst.dataset(), result);
+        result.add(newInst);
       } else {
+        copyValues(inst, false, inst.dataset(), result);
         result.add(inst);
       }
     }
 
     return result;
   }",Buggy,"Bug fix: CartesianProduct should now work correctly with string and relational attributes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15075 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The code change in the diff appears to address how a new `Instance` is created and added to `result`. Originally, a distinction was made between `DenseInstance` and `SparseInstance`. The revised approach simplifies this by using `inst.copy(newVals)` to create a copy of the instance with the modified values. The change also includes a call to `copyValues(newInst, false, inst.dataset(), result);`, which suggests that attributes (possibly string or relational) are correctly preserved or transferred to the new instance format.

The commit message states a bug fix related to the handling of string and relational attributes in the `CartesianProduct`. The change in the diff supports this intent by ensuring attributes are copied correctly, which aligns with the commit's claim that `CartesianProduct` should now work correctly with string and relational attributes.

The changes imply an error-handling update in processing instances, which could resolve an issue where attributes were not correctly copied or managed, potentially causing incorrect results when attributes were used in Cartesian products.

Given these observations, the change is directly related to a bug fix as described in the commit message. Therefore, the conclusion is **Buggy**."
weka,32640.json,f8fa07eb4f3b6e38a4952f907df8574bab292919,"@@ -1,3 +1,3 @@
   public static synchronized void loadPackages(boolean verbose) {
-    loadPackages(verbose, true, true);
+    loadPackages(verbose, false, true);
   }",Buggy,"Standard loadPackages() call was not invoking full class discovery due to a bug introduced with the latest changes - fixed.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10392 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates fixing a bug that prevents the full class discovery process during a `loadPackages()` call. The diff shows a change in the parameters passed to the `loadPackages` method. Initially, the second boolean argument was `true`, but it was modified to `false`.

The alteration of this boolean parameter likely aligns with correcting the behavior described in the commit message, where the standard `loadPackages()` call was not performing as intended due to recent changes. The boolean parameter could be controlling a feature pertinent to the class discovery process, and changing its value might have resolved an issue introduced in the latest changes.

Thus, the modifications in the code align well with the described bug fix in the commit message, as they directly enable the ""full class discovery"" intended by the method.

Therefore, the changes can be concluded as **Buggy** since they address a functionality issue as described in the commit message."
weka,32072.json,e68cbb019cd730299b6a056ba88d78a04e771a90,"@@ -1,18 +1,17 @@
   protected void initialize() {
-    String part;
+    String part = """";
     URLClassLoader sysLoader;
     URL[] urls;
 
     m_Cache = new Hashtable<String, HashSet<String>>();
 
     sysLoader = (URLClassLoader) getClass().getClassLoader();
     urls = sysLoader.getURLs();
     for (URL url : urls) {
+      part = url.toString();
       if (VERBOSE) {
         System.out.println(""Classpath-part: "" + part);
       }
-
-      part = url.toString();
       initFromClasspathPart(part);
     }
   }",Buggy,"Fixed a compilation problem that occurs when VERBOSE is set to true.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11202 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change moves the initialization of the variable `part` to the point just before its first use, and ensures it has a default empty value to prevent potential uninitialized variable compilation issues when `VERBOSE` is true. The commit message aligns with the code modification, as it addresses a compilation problem specifically related to a scenario when `VERBOSE` is set to true. The issue was likely that if `VERBOSE` was true, `part` might be used before being assigned when printing, leading to a compilation error. This aligns with fixing a bug related to compilation rather than runtime logic issues.

Therefore, the conclusion is **Buggy**."
weka,21163.json,65f17814a1d6158b5dc0b85d70c32f30efca4908,"@@ -1,18 +1,26 @@
   public EventSetDescriptor [] getEventSetDescriptors() {
     try {
       EventSetDescriptor [] esds =  
       { new EventSetDescriptor(PredictionAppender.class, 
-			       ""dataSet"",
-			       DataSourceListener.class,
-			       ""acceptDataSet""),
-	new EventSetDescriptor(DataSource.class, 
-			       ""instance"",
-			       InstanceListener.class,
-			       ""acceptInstance"")
-         };
+	  ""dataSet"",
+	  DataSourceListener.class,
+      ""acceptDataSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""instance"",
+	  InstanceListener.class,
+      ""acceptInstance""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""trainingSet"",
+	  TrainingSetListener.class,
+      ""acceptTrainingSet""),
+      new EventSetDescriptor(PredictionAppender.class, 
+	  ""testSet"",
+	  TestSetListener.class,
+      ""acceptTestSet"")
+      };
       return esds;
     } catch (Exception ex) {
       ex.printStackTrace();
     }
     return null;
   }",Buggy,"Fixed a small bug and added event sets for training at test set events.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@3814 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states that it fixes a small bug and adds event sets for training and test set events. The code changes reflect these descriptions: new event descriptors for ""trainingSet"" and ""testSet"" are added, which aligns with the ""added event sets"" part of the message. However, the diff doesn't specify what the ""small bug"" is, and without more context, it's unclear if the existing descriptors were part of the bug fix. The message could refer to an existing issue with the event descriptors, but based solely on the diff, the primary change is the addition of new event sets.

Since we cannot definitively identify and verify a specific bug fix from the code changes provided, the conclusion is **NotBuggy**."
weka,32626.json,1d67da514cfac522915affd97cf003f12258f2f5,"@@ -1,46 +1,47 @@
   protected static void establishMirror() {
     if (m_offline) {
       return;
     }
 
     try {
       String mirrorListURL =
-        ""http://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
+        ""https://www.cs.waikato.ac.nz/ml/weka/packageMetaDataMirror.txt"";
 
       URLConnection conn = null;
       URL connURL = new URL(mirrorListURL);
 
       if (PACKAGE_MANAGER.setProxyAuthentication(connURL)) {
         conn = connURL.openConnection(PACKAGE_MANAGER.getProxy());
       } else {
         conn = connURL.openConnection();
       }
 
       conn.setConnectTimeout(10000); // timeout after 10 seconds
       conn.setReadTimeout(10000);
 
       BufferedReader bi =
         new BufferedReader(new InputStreamReader(conn.getInputStream()));
 
       REP_MIRROR = bi.readLine();
 
       bi.close();
       if (REP_MIRROR != null && REP_MIRROR.length() > 0) {
         // use the mirror if it is different from the primary repo
         // and the user hasn't specified an explicit repo via the
         // property
         if (!REP_MIRROR.equals(PRIMARY_REPOSITORY) && !USER_SET_REPO) {
 
           log(weka.core.logging.Logger.Level.INFO,
             ""[WekaPackageManager] Package manager using repository mirror: ""
               + REP_MIRROR);
 
           REP_URL = new URL(REP_MIRROR);
         }
       }
     } catch (Exception ex) {
+      ex.printStackTrace();
       log(weka.core.logging.Logger.Level.WARNING,
         ""[WekaPackageManager] The repository meta data mirror file seems ""
           + ""to be unavailable ("" + ex.getMessage() + "")"");
     }
   }",Buggy,"Package mirror settings now accessed via https instead of http. Fixes an error that gets printed to the log

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@14973 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes switching package mirror settings from HTTP to HTTPS to fix a log error. Let's analyze the changes:

1. **Protocol Change**: 
   - The change updates the `mirrorListURL` from ""http"" to ""https"", which aligns with the commit message. Switching to HTTPS can prevent errors due to HTTP connections being potentially blocked or failing due to security reasons.

2. **Error Handling**:
   - An exception is caught, and the stack trace is printed using `ex.printStackTrace()`. This enhances error traceability and complements the log message by providing more detailed information about the error, which can assist in debugging.

The change is relevant to both the commit message and the enhancement of application robustness by addressing connectivity issues with the mirror URL. Using HTTPS ensures secure communication and potentially resolves issues if the HTTP endpoint is deprecated or causes warnings/messages due to the lack of encryption.

The alterations made support the intent described in the commit message, addressing a bug related to erroneous log entries due to protocol issues.

Thus, this change qualifies as a bug fix. The conclusion is **Buggy**."
weka,22555.json,6b82052500cf75da83431ce041e21658720d1816,"@@ -1,21 +1,21 @@
   public void layoutEditor() {
     m_stepToBlockBox.setEditable(true);
 
     StepManager sm = getStepToEdit().getStepManager();
     List<StepManagerImpl> flowSteps =
       getMainPerspective().getCurrentLayout().getFlow().getSteps();
     for (StepManagerImpl smi : flowSteps) {
       m_stepToBlockBox.addItem(smi.getName());
     }
 
     JPanel p = new JPanel(new BorderLayout());
-    p.setBorder(BorderFactory.createTitledBorder(""Choose class attribute""));
+    p.setBorder(BorderFactory.createTitledBorder(""Choose step to wait for""));
     p.add(m_stepToBlockBox, BorderLayout.NORTH);
 
     add(p, BorderLayout.CENTER);
 
     String userSelected = ((Block) getStepToEdit()).getStepToWaitFor();
     if (userSelected != null) {
       m_stepToBlockBox.setSelectedItem(userSelected);
     }
   }",Buggy,"Fixed a bug in a label

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12606 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit involves a simple change in a UI label from ""Choose class attribute"" to ""Choose step to wait for"". This suggests an update to a more appropriate label rather than a direct bug fix impacting the logic, functionality, or stability of the program's execution. The change appears to address a mislabeling in the graphical user interface, reflecting a correction in the user interface rather than addressing a functional bug. 

While the commit message does mention fixing a bug, the diff reveals that this fix is cosmetic or usability-related, not one that affects program behavior at a code-execution level.

Given this reasoning, the most fitting conclusion for this commit is **NotBuggy**."
weka,10201.json,2b9d972bd34de6f7911f39d55f81ab6194452ebe,"@@ -1,14 +1,16 @@
   public Instance getNextInstance(Instances structure) throws IOException {
     if (getRetrieval() == BATCH) {
       throw new IOException(""Cannot mix getting instances in both incremental and batch modes"");
     }
     m_structure = structure;
     setRetrieval(INCREMENTAL);
 
     //Have we read all the data?
     if ((m_currentTimeSlot == 0 && m_dataSet.TDIM == 0) || (m_currentTimeSlot < m_dataSet.TDIM)) {
-      return new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      Instance inst = new SparseInstance(1.0, make1Darray(m_currentTimeSlot++));
+      inst.setDataset(m_structure);
+      return inst;
      } else {
       return null;
     }
   }",Buggy,"Fixed bug in incremental loading mode: instance was not assigned to a dataset.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12106 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message specifies a bug fix related to the assignment of an instance to a dataset in the incremental loading mode. The code change directly addresses this by creating an `Instance` and then calling `inst.setDataset(m_structure)`. This ensures that the newly created `Instance` is correctly associated with the given dataset, which wasn't happening before. The change prevents potential errors when manipulating `Instance` objects that require knowing their dataset context.

The commit message and the code modification strongly align, as the code fixes the exact issue described in the message. This update is indeed a correction of faulty behavior pertinent to the commit's stated bug.

The answer is **Buggy**."
weka,23962.json,f48889f6b6237d7493f1e78fd7c3c417b636f27f,"@@ -1,20 +1,24 @@
   protected String doubleToString(double d, int prec) {
     String result;
     int currentPrec;
     int i;
 
     result = Utils.doubleToString(d, prec);
 
+    if (prec <= 0 || Double.isInfinite(d) || Double.isNaN(d)) {
+      return result;
+    }
+
     // decimal point?
     if (result.indexOf(""."") == -1) {
       result += ""."";
     }
 
     // precision so far?
     currentPrec = result.length() - result.indexOf(""."") - 1;
     for (i = currentPrec; i < prec; i++) {
       result += ""0"";
     }
 
     return result;
   }",Buggy,"Fixed bug in output of experiment results occurring when number was NaN (or infinite). In that case, no decimal point and 0s should be added. Also, decimal point is no longer printed if user requests 0 decimal places (e.g., if the user requests precision 0).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13955 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that it fixes a bug related to handling NaN or infinite numbers and ensures no decimal point if the user requests zero precision or if the number is NaN/infinite. The changes align with this description:

1. **NaN/Infinite Check**: The code introduces a check for `Double.isInfinite(d)` and `Double.isNaN(d)`. If either is true, it returns the result immediately, avoiding unnecessary formatting with decimals or zeros, which matches the described bug about NaN or infinite values.

2. **Zero Precision Check**: The condition `prec <= 0` ensures that when the user requests zero decimal precision, the function returns early without adding a decimal point or zeros.

3. **Changes Align with Description**: The code modifications correspond to the conditions described in the commit message, ensuring proper handling of special cases (NaN and infinite) and respecting user precision requests.

These modifications fulfill the fix described in the commit message, correcting the handling of edge cases in the `doubleToString` function.

Therefore, the changes clearly indicate a bug fix as per the commit message. So, the conclusion is **Buggy**."
weka,17296.json,9e4a9dc3de5bc88ccad6654a7df72b7c8807bb97,"@@ -1,76 +1,86 @@
   private void determineBounds() {
      double value,min,max;
     
     if (m_plotInstances != null && 
 	m_plotInstances.numAttributes() > 0 &&
 	m_plotInstances.numInstances() > 0) {
       // x bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_xIndex).isNominal()) {
 	m_minX = 0;
 	m_maxX = m_plotInstances.attribute(m_xIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_xIndex)) {
 	    value = m_plotInstances.instance(i).value(m_xIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+	
 	m_minX = min; m_maxX = max;
 	if (min == max) {
 	  m_maxX += 0.05;
 	  m_minX -= 0.05;
 	}
       }
 
       // y bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
       if (m_plotInstances.attribute(m_yIndex).isNominal()) {
 	m_minY = 0;
 	m_maxY = m_plotInstances.attribute(m_yIndex).numValues()-1;
       } else {
 	for (int i=0;i<m_plotInstances.numInstances();i++) {
 	  if (!m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	    value = m_plotInstances.instance(i).value(m_yIndex);
 	    if (value < min) {
 	      min = value;
 	    }
 	    if (value > max) {
 	      max = value;
 	    }
 	  }
 	}
 	
+	// handle case where all values are missing
+	if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
 	m_minY = min; m_maxY = max;
 	if (min == max) {
 	  m_maxY += 0.05;
 	  m_minY -= 0.05;
 	}
       }
       
       // colour bounds
       min=Double.POSITIVE_INFINITY;
       max=Double.NEGATIVE_INFINITY;
 
       for (int i=0;i<m_plotInstances.numInstances();i++) {
 	if (!m_plotInstances.instance(i).isMissing(m_cIndex)) {
 	  value = m_plotInstances.instance(i).value(m_cIndex);
 	  if (value < min) {
 	    min = value;
 	  }
 	  if (value > max) {
 	    max = value;
 	  }
 	}
       }
+
+      // handle case where all values are missing
+      if (min == Double.POSITIVE_INFINITY) min = max = 0.0;
+
       m_minC = min; m_maxC = max;
     }
   }",Buggy,"Fixed bug caused when all numeric values are missing.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1220 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the bug was caused when all numeric values are missing. The code changes directly address this by checking if `min` remains `Double.POSITIVE_INFINITY`, which indicates that all values were missing since no actual value set `min` to a real number. When this condition is true, both `min` and `max` are set to `0.0` to handle the scenario gracefully and avoid potential issues with undefined values being used later. 

This fix is applied consistently in the x, y, and color bounds calculation, which appropriately addresses the bug described in the commit message. The presence of these checks suggests an error-handling improvement specifically targeted at the bug outlined in the commit message.

Therefore, the changes in the diff align closely with the commit message, identifying a correction for a flaw in handling completely missing numeric values. As such, the commit represents a bug fix. The answer is **Buggy**."
weka,20037.json,af57a617ece4a265b0e23698d4b5ab07ebd17dc5,"@@ -1,12 +1,13 @@
   public boolean eventGeneratable(String eventName) {
     if (m_listenee == null) {
       return false;
     }
 
     if (m_listenee instanceof EventConstraints) {
-      if (!((EventConstraints)m_listenee).eventGeneratable(""classifier"")) {
+      if (!((EventConstraints)m_listenee).
+	  eventGeneratable(""batchClassifier"")) {
 	return false;
       }
     }
     return true;
   }",Buggy,"Fixed eventGeneratable bug


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1631 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a fix for a bug related to ""eventGeneratable."" Observing the code changes, the check for the event name has been corrected from ""classifier"" to ""batchClassifier."" This suggests that the original event name was incorrect, potentially causing certain events not to be generated when they should have been. Modifying the string indicates that the logic of the check aligns with the intended functionality.

The change is directly relevant to the commit message since it addresses an error in the logic related to event generation, which matches the description of a bug fix. The correction ensures that the eventGeneratable method functions as expected, enabling the right events based on correct identifiers.

Given that this change directly rectifies a logic error related to event generation, it aligns with the bug fix described in the commit message.

**Buggy**"
weka,17545.json,9d199c8ab7badc5fe3d5f2680f4caf570d1806f2,"@@ -1,132 +1,133 @@
   private void paintData(Graphics gx) {
 
     for (int j=0;j<m_plots.size();j++) {
       PlotData2D temp_plot = (PlotData2D)(m_plots.elementAt(j));
 
       for (int i=0;i<temp_plot.m_plotInstances.numInstances();i++) {
 	if (temp_plot.m_plotInstances.instance(i).isMissing(m_xIndex) ||
 	    temp_plot.m_plotInstances.instance(i).isMissing(m_yIndex)) {
 	} else {
 	  double x = (temp_plot.m_pointLookup[i][0] + 
 		      temp_plot.m_pointLookup[i][2]);
 	  double y = (temp_plot.m_pointLookup[i][1] + 
 		      temp_plot.m_pointLookup[i][3]);
 
 	  double prevx = 0;
 	  double prevy = 0;
 	  if (i > 0) {
 	    prevx = (temp_plot.m_pointLookup[i - 1][0] + 
 			temp_plot.m_pointLookup[i - 1][2]);
 	    prevy = (temp_plot.m_pointLookup[i - 1][1] + 
 			temp_plot.m_pointLookup[i - 1][3]);
 	  }
 
 	  int x_range = (int)x - m_XaxisStart;
 	  int y_range = (int)y - m_YaxisStart;
 
 	  if (x_range >= 0 && y_range >= 0) {
 	    if (m_drawnPoints[x_range][y_range] == i 
 		|| m_drawnPoints[x_range][y_range] == 0
 		|| temp_plot.m_displayAllPoints == true) {
 	      m_drawnPoints[x_range][y_range] = i;
 	      if (temp_plot.m_plotInstances.attribute(m_cIndex).isNominal()) {
 		if (temp_plot.m_plotInstances.attribute(m_cIndex).numValues() >
 		    m_colorList.size() && 
 		    !temp_plot.m_useCustomColour) {
 		  extendColourMap(temp_plot.m_plotInstances.
 				  attribute(m_cIndex).numValues());
 		}
 
 		Color ci;
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  ci = Color.gray;
 		} else {
 		  int ind = (int)temp_plot.m_plotInstances.instance(i).
 		    value(m_cIndex);
 		  ci = (Color)m_colorList.elementAt(ind);
 		}
 
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);	    
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
-
+		       drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+				     temp_plot.m_shapeType[i],gx);
 		    } else {
-		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
+		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      } else {
 		double r;
 		Color ci = null;
 		if (!temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  r = (temp_plot.m_plotInstances.instance(i).
 		       value(m_cIndex) - m_minC) / (m_maxC - m_minC);
 		  r = (r * 240) + 15;
 		  ci = new Color((int)r,150,(int)(255-r));
 		} else {
 		  ci = Color.gray;
 		}
 		if (!temp_plot.m_useCustomColour) {
 		  gx.setColor(ci);
 		} else {
 		  gx.setColor(temp_plot.m_customColour);
 		}
 		if (temp_plot.m_plotInstances.instance(i).
 		    isMissing(m_cIndex)) {
 		  if (temp_plot.m_connectPoints[i] == true) {
 		    drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  } else {
 		    drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				  MISSING_SHAPE,gx);
 		  }
 		} else {
 		  if (temp_plot.m_shapeType[i] == CONST_AUTOMATIC_SHAPE) {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,
 				    temp_plot.m_shapeSize[i],j,gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],j,gx);
 		    }
 		  } else {
 		    if (temp_plot.m_connectPoints[i] == true) {
 		      drawDataPoint(x,y,prevx,prevy,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    } else {
 		      drawDataPoint(x,y,temp_plot.m_shapeSize[i],
 				    temp_plot.m_shapeType[i],gx);
 		    }
 		  }
 		}
 	      }
 	    }
 	  }
 	}
       }
     }
   }",Buggy,"Fixed bug that caused some lines to be drawn when visualizing
classifier errors.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@977 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The given code change in `paintData` method reveals a logical correction related to how points are drawn on the graphics context. Specifically:

1. **Logical Error Fix:** 
   - The existing code incorrectly determined when to use the `drawDataPoint` method with or without the previous coordinates (`prevx`, `prevy`). This could have resulted in unintended lines being drawn when `temp_plot.m_connectPoints[i]` was `false`.
   - The change corrected the control flow such that drawing with `prevx` and `prevy` only occurs if `temp_plot.m_connectPoints[i]` is `true`.

2. **Relevance to Commit Message:**
   - The commit mentions fixing a bug regarding unintended lines during the visualization of classifier errors. The diff aligns with this, showing conditional drawing logic adjustments to prevent such cases.

The changes indeed correct an error related to graphics rendering based on conditional logic. Thus, the update is consistent with the commit message indicating a bug fix in how lines are drawn when visualizing data points. 

Therefore, I conclude the assessment with **Buggy**."
weka,30087.json,4eeaa9f5f013cff1e7007f66a4529ac662b22751,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
     for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
-    if (m_ClassIndex >= position) {
+    if (m_ClassIndex > position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt(): class index was incremented when class attribute was replaced.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12100 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message specifies a bug fix in the `replaceAttributeAt()` method where the class index was incorrectly incremented when a class attribute was replaced.

- The diff shows a change in logic from `if (m_ClassIndex >= position)` to `if (m_ClassIndex > position)`. 
- Originally, the class index incremented if it was positioned at or after the given position.
- The change makes it increment only if the class index is strictly after the position, which correctly avoids unnecessary increments when replacing the attribute at the class index itself.

This modification addresses the bug as described in the commit message, aligning the logic with the expected behavior when replacing attributes. Hence, this change is indeed fixing a bug, as described in the message.

The conclusion is **Buggy**."
weka,30087.json,e4ae148238732024ecdbbd5e7b3ec87d4457ebb0,"@@ -1,46 +1,46 @@
   public void replaceAttributeAt(/* @non_null@ */Attribute att, int position) {
 
     if ((position < 0) || (position > m_Attributes.size())) {
       throw new IllegalArgumentException(""Index out of range"");
     }
 
     // Does the new attribute have a different name?
     if (!att.name().equals(m_Attributes.get(position).name())) {
 
       // Need to check if attribute name already exists
       Attribute candidate = attribute(att.name());
       if ((candidate != null) && (position != candidate.index())) {
         throw new IllegalArgumentException(""Attribute name '"" + att.name()
           + ""' already in use at position #"" + 
           attribute(att.name()).index());
       }
     }
     att = (Attribute) att.copy();
     att.setIndex(position);
 
     ArrayList<Attribute> newList = new ArrayList<Attribute>(m_Attributes.size());
     HashMap<String, Integer> newMap = new HashMap<String, Integer>((int) ((m_Attributes.size() + 1) / 0.75));
     for (int i = 0 ; i < position; i++) {
       Attribute oldAtt = m_Attributes.get(i);
       newList.add(oldAtt);
       newMap.put(oldAtt.name(), i);
     }
     newList.add(att);
     newMap.put(att.name(), position);
-    for (int i = position; i < m_Attributes.size(); i++) {
+    for (int i = position + 1; i < m_Attributes.size(); i++) {
       Attribute newAtt = (Attribute) m_Attributes.get(i);
       newList.add(newAtt);
       newMap.put(newAtt.name(), i);
     }
     m_Attributes = newList;
     m_NamesToAttributeIndices = newMap;
 
     for (int i = 0; i < numInstances(); i++) {
       instance(i).setDataset(null);
       instance(i).setMissing(position);
       instance(i).setDataset(this);
     }
     if (m_ClassIndex >= position) {
       m_ClassIndex++;
     }
   }",Buggy,"Fixed bug in replaceAttributeAt() in Instances that I introduced in the modifications I made today.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11339 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that a bug fix was made to the method `replaceAttributeAt()`, which was introduced earlier in the day. The code diff reflects a change in how the loop iterates over the `m_Attributes` list beyond the specified position. 

Initially, the loop starts from the given `position`, which means it would incorrectly handle the attributes that should maintain their existing positions after the replaced attribute. This could result in the newly set attribute overwriting an existing one, effectively creating a data integrity issue.

The modification changes the loop's starting point from `position` to `position + 1`. This ensures that the attribute being replaced is correctly substituted at `position`, and all subsequent attributes are adjusted properly, preserving their order and references in the hashmap.

The changes directly address a logic error in the original method, preventing incorrect behavior when replacing an attribute at a given index. Thus, this aligns with the commit message that specifies a bug fix.

The adjustments reflect logical corrections indicative of a bug fix, which allows the method to correctly replace an attribute without inadvertently altering the list or hashmap negatively.

The answer is **Buggy**."
weka,29873.json,c5e5a38b7cb456c56a091a2581ec85b5e9c97afa,"@@ -1,16 +1,16 @@
   public double convictionForRule(AprioriItemSet premise,
 				   AprioriItemSet consequence,
 				   int premiseCount,
 				   int consequenceCount) {
     double num = 
-      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) *
+      (double)premiseCount * (double)(m_totalTransactions - consequenceCount) /
        (double)m_totalTransactions;
     double denom = 
       ((premiseCount - consequence.m_counter)+1);
     
     if (num < 0 || denom < 0) {
       System.err.println(""*** ""+num+"" ""+denom);
       System.err.println(""premis count: ""+premiseCount+"" consequence count ""+consequenceCount+"" total trans ""+m_totalTransactions);
     }
     return num / denom;
   }",Buggy,"Fixed bug in the computation of the conviction measure


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2297 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The change in the code modifies an arithmetic operator, replacing a multiplication (`*`) with a division (`/`), which alters the logic of the conviction computation.

- The `num` variable represents part of the conviction measure computation.
- Originally, `num` used a multiplication: `(double)(m_totalTransactions - consequenceCount) * (double)m_totalTransactions`.
- The change replaces this multiplication with a division: `(double)(m_totalTransactions - consequenceCount) / (double)m_totalTransactions`.

This alteration reflects a significant logical correction in how the conviction measure is calculated. This is likely to resolve an incorrect computation of conviction as indicated in the commit message, directly aligning with a bug fix in numerical calculations.

Based on these observations, the code change aligns with the commit message, indicating a fix to a bug in the conviction computation logic. Therefore, this change is correctly identified as a bug fix.

The answer is **Buggy**."
weka,25075.json,51f46d90e1a9e13dbc38b52a664024090bef89d4,"@@ -1,68 +1,98 @@
   protected void adjustCanopies(double[] densities) {
     if (m_numClustersRequested < 0) {
       assignCanopiesToCanopyCenters();
 
       m_trainingData = new Instances(m_canopies, 0);
       return;
     }
 
     // more canopies than requested?
     if (m_canopies.numInstances() > m_numClustersRequested) {
       int[] sortedIndexes = Utils.stableSort(densities);
 
       Instances finalCanopies = new Instances(m_canopies, 0);
       int count = 0;
       for (int i = sortedIndexes.length - 1; count < m_numClustersRequested; i--) {
         finalCanopies.add(m_canopies.instance(sortedIndexes[i]));
         count++;
       }
 
       m_canopies = finalCanopies;
+      List<double[][]> tempCanopyCenters = new ArrayList<double[][]>();
+      List<double[]> tempT2Dists = new ArrayList<double[]>();
+      List<double[]> tempMissings = new ArrayList<double[]>();
+
+      // make sure that the center sums, densities and missing counts are
+      // aligned with the new canopy list
+      count = 0;
+      for (int i = sortedIndexes.length - 1; count < finalCanopies
+        .numInstances(); i--) {
+        tempCanopyCenters.add(m_canopyCenters.get(sortedIndexes[i]));
+        tempT2Dists.add(m_canopyT2Density.get(sortedIndexes[i]));
+        tempMissings.add(m_canopyNumMissingForNumerics.get(sortedIndexes[i]));
+        count++;
+      }
+      m_canopyCenters = tempCanopyCenters;
+      m_canopyT2Density = tempT2Dists;
+      m_canopyNumMissingForNumerics = tempMissings;
+
     } else if (m_canopies.numInstances() < m_numClustersRequested
       && m_trainingData != null && m_trainingData.numInstances() > 0) {
 
       // make up the difference with randomly selected instances (if possible)
       Random r = new Random(getSeed());
       for (int i = 0; i < 10; i++) {
         r.nextInt();
       }
       HashMap<DecisionTableHashKey, Integer> initC = new HashMap<DecisionTableHashKey, Integer>();
       DecisionTableHashKey hk = null;
 
       // put the existing canopies in the lookup
       for (int i = 0; i < m_canopies.numInstances(); i++) {
         try {
           hk = new DecisionTableHashKey(m_canopies.instance(i),
             m_canopies.numAttributes(), true);
 
           initC.put(hk, null);
         } catch (Exception e) {
           e.printStackTrace();
         }
       }
 
       for (int j = m_trainingData.numInstances() - 1; j >= 0; j--) {
         int instIndex = r.nextInt(j + 1);
         try {
           hk = new DecisionTableHashKey(m_trainingData.instance(instIndex),
             m_trainingData.numAttributes(), true);
         } catch (Exception e) {
           e.printStackTrace();
         }
         if (!initC.containsKey(hk)) {
-          m_canopies.add(m_trainingData.instance(instIndex));
+          Instance newInstance = m_trainingData.instance(instIndex);
+          m_canopies.add(newInstance);
+
+          double[] density = new double[1];
+          density[0] = 1.0;
+          m_canopyT2Density.add(density);
+
+          double[][] center = new double[newInstance.numAttributes()][0];
+          double[] numMissingNumerics = new double[newInstance.numAttributes()];
+          updateCanopyCenter(newInstance, center, numMissingNumerics);
+          m_canopyCenters.add(center);
+          m_canopyNumMissingForNumerics.add(numMissingNumerics);
+
           initC.put(hk, null);
         }
         m_trainingData.swap(j, instIndex);
 
         if (m_canopies.numInstances() == m_numClustersRequested) {
           break;
         }
       }
     }
 
     assignCanopiesToCanopyCenters();
 
     // save memory
     m_trainingData = new Instances(m_canopies, 0);
   }",Buggy,"Fixed a bug that affected aggregation of Canopy clusterers - canopy center sums were not getting adjusted after the final number of canopies was determined.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10597 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message describes a bug affecting the correct adjustment of canopy center sums after determining the final number of canopies. The code changes in the diff reflect this:

1. The original code did not adjust `m_canopyCenters`, `m_canopyT2Density`, and `m_canopyNumMissingForNumerics` to align with the final set of canopies.
   
2. The introduced code now creates temporary lists `tempCanopyCenters`, `tempT2Dists`, and `tempMissings` that only include relevant elements from the original lists, corresponding to the adjusted list of canopies (`finalCanopies`).

3. The changes also address situations where there are fewer canopies than requested by randomly adding new canopies from `m_trainingData`. The new canopies are properly initialized with a density, a center, and a count of missing numeric values.

These changes align well with the commit message claiming a bug fix. They ensure that the state of canopy-related data structures corresponds accurately to the actual number of finalized canopies, solving the misalignment problem.

Thus, these changes are indicative of a bug fix. Therefore, the appropriate conclusion is **Buggy**."
weka,37063.json,2a4806e3c2bd860a4e7b0496b5b05ff72dfdf46b,"@@ -1,84 +1,85 @@
   public void start() throws WekaException {
     if (getStepManager().numOutgoingConnections() > 0) {
       weka.datagenerators.DataGenerator generator = getDataGenerator();
       if (getStepManager()
         .numOutgoingConnectionsOfType(StepManager.CON_DATASET) > 0) {
         getStepManager().processing();
         StringWriter output = new StringWriter();
         try {
           generator.setOutput(new PrintWriter(output));
           getStepManager().statusMessage(""Generating..."");
           getStepManager().logBasic(""Generating data"");
           weka.datagenerators.DataGenerator.makeData(generator,
             generator.getOptions());
           Instances instances =
             new Instances(new StringReader(output.toString()));
 
           if (!isStopRequested()) {
             Data outputData = new Data(StepManager.CON_DATASET, instances);
             getStepManager().outputData(outputData);
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         if (isStopRequested()) {
           getStepManager().interrupted();
         } else {
           getStepManager().finished();
         }
       } else {
         // streaming case
         try {
           if (!generator.getSingleModeFlag()) {
             throw new WekaException(""Generator does not support ""
               + ""incremental generation, so cannot be used with ""
               + ""outgoing 'instance' connections"");
           }
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
         String stm =
           getName() + ""$"" + hashCode() + 99 + ""| overall flow throughput -|"";
         m_flowThroughput =
           new StreamThroughput(stm, ""Starting flow..."",
             ((StepManagerImpl) getStepManager()).getLog());
 
         try {
           getStepManager().logBasic(""Generating..."");
           generator.setDatasetFormat(generator.defineDataFormat());
 
           for (int i = 0; i < generator.getNumExamplesAct(); i++) {
+            m_flowThroughput.updateStart();
             getStepManager().throughputUpdateStart();
             if (isStopRequested()) {
               getStepManager().interrupted();
               return;
             }
 
             // over all examples to be produced
             Instance inst = generator.generateExample();
             m_incrementalData.setPayloadElement(StepManager.CON_INSTANCE, inst);
             getStepManager().throughputUpdateEnd();
 
             getStepManager().outputData(m_incrementalData);
             m_flowThroughput.updateEnd(((StepManagerImpl) getStepManager())
               .getLog());
           }
 
           if (isStopRequested()) {
             ((StepManagerImpl) getStepManager()).getLog().statusMessage(
               stm + ""remove"");
             getStepManager().interrupted();
             return;
           }
           m_flowThroughput.finished(((StepManagerImpl) getStepManager())
             .getLog());
 
           // signal end of input
           m_incrementalData.clearPayload();
           getStepManager().throughputFinished(m_incrementalData);
         } catch (Exception ex) {
           throw new WekaException(ex);
         }
       }
     }
   }",Buggy,"Fixed a bug in the overall throughput status

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13209 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message addresses a bug fix related to throughput status. In the diff, there is an addition of a call to `m_flowThroughput.updateStart()` at the beginning of the loop iterating over generated examples. This change suggests a modification in how throughput is tracked and updated, ensuring the correct start time is recorded for each iteration during data generation. This aligns with fixing a bug related to throughput status tracking. The code modification involves enhancing the timing mechanism to ensure accurate throughput calculations, which is consistent with the commit message.

Therefore, the change aligns with the described bug fix in the commit message, and the changes relate to correcting the throughput status update logic.

The conclusion is **Buggy**."
weka,33600.json,4fbfba6d434318852d50cf855de7282fd21ceae3,"@@ -1,62 +1,63 @@
   public boolean isCompatibleBaseSystem() throws Exception {
 
     String baseSystemName = m_packageManager.getBaseSystemName();
     String systemVersion = m_packageManager.getBaseSystemVersion().toString();
     // System.err.println(""Base system version "" + systemVersion);
 
-    String dependencies = getPackageMetaDataElement(""Depends"").toString();
+    String dependencies = getPackageMetaDataElement(""Depends"") == null
+      ? null : getPackageMetaDataElement(""Depends"").toString();
     if (dependencies == null) {
       return true;
     }
-
+    
     boolean ok = true;
     StringTokenizer tok = new StringTokenizer(dependencies, "","");
     while (tok.hasMoreTokens()) {
       String nextT = tok.nextToken().trim();
       String[] split = splitNameVersion(nextT);
       if (split[0].startsWith(baseSystemName.toLowerCase())) {
         // check the system version
         if (split[1] != null) {
           if (split.length == 3) {
             VersionPackageConstraint.VersionComparison constraint =
               VersionPackageConstraint.getVersionComparison(split[1]);
             if (!VersionPackageConstraint.checkConstraint(systemVersion,
               constraint, split[2])) {
               ok = false;
               break;
             }
           } else {
             // construct a ""dummy"" package for the base system
             Map<String, String> baseMap = new HashMap<String, String>();
             baseMap.put(""PackageName"", ""weka"");
 
             baseMap.put(""Version"", systemVersion);
             Package basePackage =
               new DefaultPackage(null, m_packageManager, baseMap);
 
             VersionRangePackageConstraint versionRConstraint =
               new VersionRangePackageConstraint(basePackage);
             VersionPackageConstraint.VersionComparison comp1 =
               VersionPackageConstraint.getVersionComparison(split[1]);
             VersionPackageConstraint.VersionComparison comp2 =
               VersionPackageConstraint.getVersionComparison(split[3]);
 
             versionRConstraint.setRangeConstraint(split[2], comp1, split[4],
               comp2);
 
             if (!versionRConstraint.checkConstraint(basePackage)) {
               ok = false;
               break;
             }
           }
           /*
            * int comparisonResult =
            * VersionPackageConstraint.compare(systemVersion, split[2]); ok =
            * versionOK(split[1], comparisonResult); if (!ok) { break; }
            */
         }
       }
     }
 
     return ok;
   }",Buggy,"Fixed a bug in DefaultPackage that would result in npe when checking dependencies for a malformed Description.props (that did not list at least a base Weka dependency).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@15152 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states the fix is related to preventing a NullPointerException when checking dependencies in the `DefaultPackage`. The code change addresses this by updating the way `dependencies` is assigned. Previously, it was directly converted to a string, but if `getPackageMetaDataElement(""Depends"")` returned `null`, this would cause a NullPointerException.

The updated line:
```java
String dependencies = getPackageMetaDataElement(""Depends"") == null
  ? null : getPackageMetaDataElement(""Depends"").toString();
```

This change ensures that if `getPackageMetaDataElement(""Depends"")` returns `null`, `dependencies` is explicitly set to `null` rather than calling `toString()` on a `null` object, which would lead to a NullPointerException.

Given that this change matches the description in the commit message and addresses a specific NullPointerException, it is indeed a bug fix.

Therefore, the conclusion is **Buggy**."
weka,17605.json,d6ac3ed81063bb608413aa9d346e6e8e8b868874,"@@ -1,220 +1,221 @@
 	  public void mouseClicked(MouseEvent e) {
 	    
 	    if ((m_sIndex == 2 || m_sIndex == 3) && 
 		(m_createShape || 
 		 (e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK)) {
 	      if (m_createShape) {
 		//then it has been started already.
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
-		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
+		if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK &&
+                    !e.isAltDown()) {
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribX(e.getX())));
 		  
 		  m_shapePoints.addElement(new 
 		    Double(m_plot2D.convertToAttribY(e.getY())));
 		  
 		  m_newMousePos.width = e.getX();
 		  m_newMousePos.height = e.getY();
 		  g.drawLine((int)Math.ceil
 			     (m_plot2D.convertToPanelX
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 2)).
 			       doubleValue())),
 			     
 			     (int)Math.ceil
 			     (m_plot2D.convertToPanelY
 			      (((Double)m_shapePoints.
 				elementAt(m_shapePoints.size() - 1)).
 			       doubleValue())),
 			     m_newMousePos.width, m_newMousePos.height);
 		  
 		}
 		else if (m_sIndex == 3) {
 		  //then extend the lines to infinity 
 		  //(100000 or so should be enough).
 		  //the area is selected by where the user right clicks 
 		  //the mouse button
 		  
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 5) {
 		    double cx = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()));
 		    
 		    double cx2 = Math.ceil
 		      (m_plot2D.convertToPanelX
 		       (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue())) - 
 		      cx;
 		    
 		    cx2 *= 50000;
 		    
 		    double cy = Math.ceil
 		      (m_plot2D.
 		       convertToPanelY(((Double)m_shapePoints.
 					elementAt(m_shapePoints.size() - 3)).
 				       doubleValue()));
 		    double cy2 = Math.ceil
 		      (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					  elementAt(m_shapePoints.size() - 1)).
 					  doubleValue())) - cy;
 		    cy2 *= 50000;
 			    
 		    
 		    double cxa = Math.ceil(m_plot2D.convertToPanelX
 					   (((Double)m_shapePoints.
 					     elementAt(3)).
 					    doubleValue()));
 		    double cxa2 = Math.ceil(m_plot2D.convertToPanelX
 					    (((Double)m_shapePoints.
 					      elementAt(1)).
 					     doubleValue())) - cxa;
 		    cxa2 *= 50000;
 		    
 		    
 		    double cya = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(4)).
 			doubleValue()));
 		    double cya2 = Math.ceil
 		      (m_plot2D.convertToPanelY
 		       (((Double)m_shapePoints.elementAt(2)).
 			doubleValue())) - cya;
 		    
 		    cya2 *= 50000;
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cxa2 + cxa)), 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cy2 + cy)), 
 		       m_shapePoints.size() - 1);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribX(cx2 + cx)), 
 		       m_shapePoints.size() - 2);
 		    
 		    m_shapePoints.setElementAt
 		      (new Double(m_plot2D.convertToAttribY(cya2 + cya)), 2);
 		    
 		    
 		    //determine how infinity line should be built
 		    
 		    cy = Double.POSITIVE_INFINITY;
 		    cy2 = Double.NEGATIVE_INFINITY;
 		    if (((Double)m_shapePoints.elementAt(1)).
 			doubleValue() > 
 			((Double)m_shapePoints.elementAt(3)).
 			doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt(2)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt(4)).
 			  doubleValue()) {
 			cy = ((Double)m_shapePoints.elementAt(2)).
 			  doubleValue();
 		      }
 		    }
 		    if (((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 2)).doubleValue() > 
 			((Double)m_shapePoints.elementAt
 			 (m_shapePoints.size() - 4)).doubleValue()) {
 		      if (((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 3)).
 			  doubleValue() == 
 			  ((Double)m_shapePoints.elementAt
 			   (m_shapePoints.size() - 1)).doubleValue()) {
 			cy2 = ((Double)m_shapePoints.lastElement()).
 			  doubleValue();
 		      }
 		    }
 		    m_shapePoints.addElement(new Double(cy));
 		    m_shapePoints.addElement(new Double(cy2));
 		    
 		    if (!inPolyline(m_shapePoints, m_plot2D.convertToAttribX
 				    (e.getX()), 
 				    m_plot2D.convertToAttribY(e.getY()))) {
 		      Double tmp = (Double)m_shapePoints.
 			elementAt(m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(m_shapePoints.lastElement(), 
 			 m_shapePoints.size() - 2);
 		      m_shapePoints.setElementAt
 			(tmp, m_shapePoints.size() - 1);
 		    }
 		    
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		  
 		}
 		else {
 		  //then close the shape
 		  m_createShape = false;
 		  if (m_shapePoints.size() >= 7) {
 		    m_shapePoints.addElement(m_shapePoints.elementAt(1));
 		    m_shapePoints.addElement(m_shapePoints.elementAt(2));
 		    if (m_shapes == null) {
 		      m_shapes = new FastVector(4);
 		    }
 		    m_shapes.addElement(m_shapePoints);
 			   
 		    m_submit.setText(""Submit"");
 		    m_submit.setActionCommand(""Submit"");
 		    
 		    m_submit.setEnabled(true);
 		  }
 		  m_shapePoints = null;
 		  PlotPanel.this.repaint();
 		}
 		g.dispose();
 		//repaint();
 	      }
 	      else if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK) {
 		//then this is the first point
 		m_createShape = true;
 		m_shapePoints = new FastVector(17);
 		m_shapePoints.addElement(new Double(m_sIndex));
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribX(e.getX()))); //the new point
 		m_shapePoints.addElement(new 
 		  Double(m_plot2D.convertToAttribY(e.getY())));
 		m_newMousePos.width = e.getX();      //the temp mouse point
 		m_newMousePos.height = e.getY();
 
 		Graphics g = m_plot2D.getGraphics();
 		g.setColor(Color.black);
 		g.setXORMode(Color.white);
 		g.drawLine((int)Math.ceil
 			   (m_plot2D.convertToPanelX(((Double)m_shapePoints.
 					     elementAt(1)).doubleValue())),
 			   (int)Math.ceil
 			   (m_plot2D.convertToPanelY(((Double)m_shapePoints.
 					     elementAt(2)).doubleValue())),
 			   m_newMousePos.width, m_newMousePos.height);
 		g.dispose();
 	      }
 	    }
 	    else {
 	      if ((e.getModifiers() & InputEvent.BUTTON1_MASK) == 
 		  InputEvent.BUTTON1_MASK) {
 		
 		m_plot2D.searchPoints(e.getX(),e.getY(), false);
 	      } else {
 		m_plot2D.searchPoints(e.getX(), e.getY(), true);
 	      }
 	    }
 	  }",Buggy,"Fixed bug that prevented polyline and polygon selection options from working with single button mice. The Alt key can now be held down to simulate button 2


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@2621 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message outlines a bug fix enabling polyline and polygon selection for single-button mice by using the Alt key to simulate the second mouse button. The code changes directly support this:

1. **Change with Alt Key**: The line `if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK)` was modified to `if ((e.getModifiers() & e.BUTTON1_MASK) == e.BUTTON1_MASK && !e.isAltDown())`. This logical condition now checks if the Alt key is not pressed. It suggests that another part of the code probably considers Alt to trigger a different functionality, enabling simulation of the second mouse button with a single-button mouse. 

2. **Context of Bug**: The condition alteration directly addresses the need as described in the commit message, implying that the Alt key combination is now part of the operational logic to differentiate behaviors based on mouse input.

3. **Functionality Alignment**: Both the original message and code changes align, focusing on making certain interactions available to single-button mouse users by leveraging the Alt key.

These changes clearly address a bug described in the commit message, correcting the user interface's interaction model according to the hardware constraints (single-button mice). Therefore, this commit is a **Buggy**."
weka,23551.json,0f79454f25618e64781b12a24a482ef8a0441bca,"@@ -1,44 +1,44 @@
   public Instance generateExample() throws Exception {
     Instance result;
     Random rand;
     double x;
     double y;
     double[] atts;
     Instance inst;
 
-    result = null;
     rand = getRandom();
 
     if (m_DatasetFormat == null) {
       throw new Exception(""Dataset format not defined."");
     }
 
     // random x
     x = rand.nextDouble();
     // fit into range
     x = x * (getMaxRange() - getMinRange()) + getMinRange();
 
     // generate y
     atts = new double[1];
     atts[0] = x;
     inst = new DenseInstance(1.0, atts);
+    inst.setDataset(m_RawData);
     m_Filter.input(inst);
     m_Filter.batchFinished();
     inst = m_Filter.output();
 
     // noise
     y = inst.value(1) + getAmplitude() * m_NoiseRandom.nextGaussian()
       * getNoiseRate() * getNoiseVariance();
 
     // generate attributes
     atts = new double[m_DatasetFormat.numAttributes()];
 
     atts[0] = x;
     atts[1] = y;
     result = new DenseInstance(1.0, atts);
 
     // dataset reference
     result.setDataset(m_DatasetFormat);
 
     return result;
   }",Buggy,"Fixed problem that caused assertion to fail when executing unit test.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11504 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates that the change fixes a problem causing an assertion failure during a unit test. The code modification involves setting the dataset of an `Instance` object `inst` to `m_RawData` right after its creation. This is a critical change; it aligns instances with the expected input format of the filter, crucial for correct processing. Without setting the dataset, the filter might not function properly, leading to unexpected behavior and assertion failures during tests.

This change likely resolves an issue that caused discrepancies expected during test execution. The intent and nature of this change demonstrate that the problem involved test failures, potentially due to incorrect instance handling or filter processing, which has now been corrected.

Given the explicit nature of this change, the reasoning aligns with fixing a bug related to an assertion failure. Therefore, the conclusion is **Buggy**."
weka,21037.json,05cc4c3b0752474605f5ea11f2c85e4a9c3d6043,"@@ -1,16 +1,37 @@
   public boolean eventGeneratable(String eventName) {
 
-    if (!m_listeneeTypes.contains(eventName)) {
-      return false;
+    if (eventName.equals(""instance"")) {
+
+      if (!m_listeneeTypes.contains(eventName)) {
+        return false;
+      }
+
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints
+          && !((EventConstraints) listenee).eventGeneratable(eventName)) {
+          return false;
+        }
+      }
     }
 
-    for (Object listenee : m_listenees.values()) {
-      if (listenee instanceof EventConstraints) {
-        if (!((EventConstraints) listenee).eventGeneratable(eventName)) {
-          return false;
+    if (eventName.equals(""dataSet"") || eventName.equals(""trainingSet"")
+      || eventName.equals(""testSet"")) {
+
+      if (!m_listeneeTypes.contains(""dataSet"")
+        && !m_listeneeTypes.contains(""trainingSet"")
+        && !m_listeneeTypes.contains(""testSet"")) {
+        return false;
+      }
+      for (Object listenee : m_listenees.values()) {
+        if (listenee instanceof EventConstraints) {
+          if (!((EventConstraints) listenee).eventGeneratable(""dataSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""trainingSet"")
+            && !((EventConstraints) listenee).eventGeneratable(""testSet"")) {
+            return false;
+          }
         }
       }
     }
 
     return true;
   }",Buggy,"Fixed a bug in the specification of event types produced by Appender and in the routine that determines whether a particular event type can be generated at a given point in time.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10149 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message indicates a bug fix related to event types generated by an ""Appender"" and their generatability at certain times.

Upon examining the `eventGeneratable` method diff:

1. **Previous Logic**: The original code only checked if the event type was in `m_listeneeTypes` and whether any listenee (implementing `EventConstraints`) can generate it. If not, it returned false.

2. **Updated Logic**:
   - If `eventName` equals ""instance"":
     - The new logic retains the check to see if ""instance"" is in `m_listeneeTypes`.
     - It adds a loop to verify if each listenee can generate the ""instance"" event by casting them to `EventConstraints`.

   - If `eventName` equals ""dataSet"", ""trainingSet"", or ""testSet"":
     - The code checks if any of the three event types exist in `m_listeneeTypes`.
     - It further confirms if any `listenee` can generate the ""dataSet"", ""trainingSet"", or ""testSet"" events.

The update ensures special handling for different types of events, likely fixing the bug related to incorrect handling of these event types.

Therefore, this change reflects a bug fix in the handling and specification of event types, aligning it with the commit message's claim. The answer is **Buggy**."
weka,23139.json,50e14ddcea9246c4b6c83142b493163b1e9b862c,"@@ -1,27 +1,42 @@
   protected void updateCapabilitiesFilter(Capabilities filter) {
     Instances 		tempInst;
     Capabilities 	filterClass;
 
     if (filter == null) {
       m_AttributeEvaluatorEditor.setCapabilitiesFilter(new Capabilities(null));
       m_AttributeSearchEditor.setCapabilitiesFilter(new Capabilities(null));
       return;
     }
     
     if (!ExplorerDefaults.getInitGenericObjectEditorFilter())
       tempInst = new Instances(m_Instances, 0);
     else
       tempInst = new Instances(m_Instances);
     tempInst.setClassIndex(m_ClassCombo.getSelectedIndex());
 
     try {
       filterClass = Capabilities.forInstances(tempInst);
     }
     catch (Exception e) {
       filterClass = new Capabilities(null);
     }
     
     // set new filter
     m_AttributeEvaluatorEditor.setCapabilitiesFilter(filterClass);
     m_AttributeSearchEditor.setCapabilitiesFilter(filterClass);
+    
+    m_StartBut.setEnabled(true);
+    // check capabilities...
+    Capabilities currentFilter = m_AttributeEvaluatorEditor.getCapabilitiesFilter();
+    ASEvaluation evaluator = (ASEvaluation) m_AttributeEvaluatorEditor.getValue();
+    Capabilities currentSchemeCapabilities =  null;
+    if (evaluator != null && currentFilter != null && 
+        (evaluator instanceof CapabilitiesHandler)) {
+      currentSchemeCapabilities = ((CapabilitiesHandler)evaluator).getCapabilities();
+      
+      if (!currentSchemeCapabilities.supportsMaybe(currentFilter) &&
+          !currentSchemeCapabilities.supports(currentFilter)) {
+        m_StartBut.setEnabled(false);
+      }
+    }
   }",Buggy,"Fixed a bug where the enabled/disabled state of the start button was not being updated when a new data set was set on this panel.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5385 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The diff shows changes in the `updateCapabilitiesFilter` method, specifically adding logic to update the enabled/disabled state of `m_StartBut`. The added code first enables the start button and then checks if the current evaluator's capabilities support the given filter using `supportsMaybe` and `supports`. If neither support is present, the start button is disabled. This ensures that the state of the start button reflects whether the current setup can handle the capabilities, addressing a bug where the button might not update properly when a new data set with different capabilities is set. The commit message accurately describes this fix. Therefore, the answer is **Buggy**."
xerces2-j,5527.json,2c3c54c0011dbb81897f1f1484b2a1da2d9bce5a,"@@ -1,3 +1,3 @@
     protected boolean versionSupported(String version) {
-        return version.equals(""1.1"");
+        return (version.equals(""1.1"") || version.equals(""1.0""));
     } // versionSupported(String):  boolean",Buggy,"fix for bug 18789; thanks to Neil Delima for the patch


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319325 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions a fix for bug 18789 with a patch provided by Neil Delima. The code change expands the `versionSupported` method to also recognize ""1.0"" as a supported version in addition to ""1.1"". 

### Analysis:
1. **Bug Context**: The commit message specifies a bug fix, implying that prior behavior was incorrect or incomplete.
2. **Behavior Modification**: By adding support for ""1.0"", the fix likely addresses a bug where a valid version (""1.0"") was incorrectly rejected or caused issues.
3. **Potential Impact**: If ""1.0"" versions were expected to be accepted everywhere ""1.1"" versions are, this change corrects a previous omission.

The modification directly aligns with the typical need to extend functionality to accommodate expected inputs, addressing a specific bug scenario. Thus, the provided patch corrects a logical gap in version handling.

Conclusively, given the context of the commit message and the change made:

**Buggy**"
xerces2-j,1404.json,4e11453e19d58d4481d2367028b0cb86c0deafeb,"@@ -1,62 +1,66 @@
 	protected void checkUnboundNamespacePrefixedNode (Node node) throws IOException{
 
 		if (fNamespaces) {
 
    			if (DEBUG) {
 			    System.out.println(""==>serializeNode(""+node.getNodeName()+"") [Entity Reference - Namespaces on]"");
 				System.out.println(""==>Declared Prefix Count: "" + fNSBinder.getDeclaredPrefixCount());
 				System.out.println(""==>Node Name: "" + node.getNodeName());
 				System.out.println(""==>First Child Node Name: "" + node.getFirstChild().getNodeName());
 				System.out.println(""==>First Child Node Prefix: "" + node.getFirstChild().getPrefix());
 				System.out.println(""==>First Child Node NamespaceURI: "" + node.getFirstChild().getNamespaceURI());			
 			}
 
 		
 			Node child, next;
 	        for (child = node.getFirstChild(); child != null; child = next) {
 	            next = child.getNextSibling();
 			    if (DEBUG) {
 			        System.out.println(""==>serializeNode(""+child.getNodeName()+"") [Child Node]"");
 			        System.out.println(""==>serializeNode(""+child.getPrefix()+"") [Child Node Prefix]"");
 	            }    
 	
 		 	    //If a NamespaceURI is not declared for the current
 		 	    //node's prefix, raise a fatal error.
 		 	    String prefix = child.getPrefix();
+                prefix = (prefix == null || 
+                        prefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(prefix);
 		 	    if (fNSBinder.getURI(prefix) == null && prefix != null) {
 					fatalError(""The replacement text of the entity node '"" 
 								+ node.getNodeName()  
 								+ ""' contains an element node '"" 
 								+ child.getNodeName() 
 								+ ""' with an undeclared prefix '"" 
 								+ prefix + ""'."");
 		 	    }	
 
 				if (child.getNodeType() == Node.ELEMENT_NODE) {
 					
 					NamedNodeMap attrs = child.getAttributes();
 					
 					for (int i = 0; i< attrs.getLength(); i++ ) {
 						
 				 	    String attrPrefix = attrs.item(i).getPrefix();
+                        attrPrefix = (attrPrefix == null || 
+                                attrPrefix.length() == 0) ? XMLSymbols.EMPTY_STRING : fSymbolTable.addSymbol(attrPrefix);
 				 	    if (fNSBinder.getURI(attrPrefix) == null && attrPrefix != null) {
 							fatalError(""The replacement text of the entity node '"" 
 										+ node.getNodeName()  
 										+ ""' contains an element node '"" 
 										+ child.getNodeName() 
 										+ ""' with an attribute '"" 
 										+ attrs.item(i).getNodeName() 										
 										+ ""' an undeclared prefix '"" 
 										+ attrPrefix + ""'."");
 				 	    }	
 						
 					}	
 
 				}
 					
 				if (child.hasChildNodes()) {
 					checkUnboundNamespacePrefixedNode(child);
 				}	
 	        }
 		}    
 	}",Buggy,"Fixing JIRA Bug #1043:
http://issues.apache.org/jira/browse/XERCESJ-1043

Looking up a namespace URI from a NamespaceContext requires that we pass in the interned
String from the SymbolTable as the prefix. We were failing to do that when checking for unbound
prefixes. Should be fixed now.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320220 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message refers to a fix for a bug regarding the handling of namespace URIs, where the code needs to pass in interned strings from a `SymbolTable` as prefixes when looking up namespaces. The issue could cause undeclared prefix errors when checking for unbound prefixes without using interned Strings.

The diff shows:
1. The addition of logic to use `fSymbolTable.addSymbol(prefix)` to intern strings for prefixes and attribute prefixes. This ensures that the lookup for namespace URIs is performed with the correct, interned strings.
2. If a prefix is null or empty, it is replaced with `XMLSymbols.EMPTY_STRING` before adding to the `SymbolTable`, ensuring stability in operations where the absence of a prefix is denoted by an empty string.

The modifications align well with the commit message and the described bug. This change ensures that namespace URI lookups are accurate by properly utilizing interned strings from the `SymbolTable`. Such changes rectify the issue of failing to detect or declare prefixes due to not using interned strings, which is the core of the mentioned bug.

Thus, the changes in the diff do indeed indicate a bug fix as per the commit message.

**Buggy**."
xerces2-j,8183.json,a01d7a8db8b4fb1ff2a2f8c62736b56bb2d84d63,"@@ -1,12 +1,14 @@
-    public boolean equals(XMLGrammarDescription desc) {
+    public boolean equals(Object descObj) {
+        if(!(descObj instanceof XMLGrammarDescription)) return false;
+        XMLGrammarDescription desc = (XMLGrammarDescription)descObj;
         if (!getGrammarType().equals(desc.getGrammarType())) {
             return false;
         }
         if (fTargetNamespace != null && fTargetNamespace.equals(((XSDDescription)desc).getTargetNamespace())) {
             return true;
         }
         else if (fTargetNamespace == null && ((XSDDescription)desc).getTargetNamespace() == null) {
             return true;
         }
         return false;
     }",Buggy,"fix a couple of bugs with schema preparsing:  make sure the error reporter knows about schemas; and make sure XSDDescription .equals method actually works in contexts such as Hashtables (the parameter should be an Object, not an XMLGrammarDescription).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318470 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions fixing bugs related to schema preparsing, specifically the proper functioning of the equals method in the XSDDescription class. The code diff aligns with this by modifying the equals method to accept an Object instead of an XMLGrammarDescription, which is essential for the method to work correctly in Hashtable contexts. The change includes an instance check with instanceof and appropriate casting, ensuring that the equals method behaves correctly by comparing XMLGrammarDescription instances.

This modification directly addresses the issue described: ensuring that the equals method works as expected when used in collections like Hashtables, which rely on equals to handle a general Object type. The previous version of the method wouldn't correctly override the Object class's equals method, causing potential logical errors when attempting to find instances in collections.

Given the nature of the change, which corrects a potential bug in an essential method used for object comparison in data structures, the assessment of this commit should be **Buggy**."
xerces2-j,8285.json,f33e487490b168dd0b29e7df099a1832fb79ee79,"@@ -1,84 +1,85 @@
     private int scanMixed(QName element) throws Exception {
 
         int valueIndex = -1;  // -1 is special value for #PCDATA
         int prevNodeIndex = -1;
         boolean starRequired = false;
         int[] valueSeen = new int[32];
         int valueCount = 0;
         boolean dupAttrType = false;
         int nodeIndex = -1;
 
         while (true) {
             if (fValidationEnabled) {
                 for (int i=0; i<valueCount;i++) {
                     if ( valueSeen[i] == valueIndex) {
                         dupAttrType = true;
                         break;
                     }
                 }
             }
             if (dupAttrType && fValidationEnabled) {
                 reportRecoverableXMLError(XMLMessages.MSG_DUPLICATE_TYPE_IN_MIXED_CONTENT,
                                           XMLMessages.VC_NO_DUPLICATE_TYPES,
                                           valueIndex);
                 dupAttrType = false;
 
             }
             else {
                 try {
                     valueSeen[valueCount] = valueIndex;
                 }
                 catch (ArrayIndexOutOfBoundsException ae) {
                     int[] newArray = new int[valueSeen.length*2];
                     System.arraycopy(valueSeen,0,newArray,0,valueSeen.length);
+                    valueSeen = newArray;
                     valueSeen[valueCount] = valueIndex;
                 }
                 valueCount++;
 
                 nodeIndex = fDTDGrammar.addUniqueLeafNode(valueIndex);
             }
 
             checkForPEReference(false);
             if (!fEntityReader.lookingAtChar('|', true)) {
                 if (!fEntityReader.lookingAtChar(')', true)) {
                     reportFatalXMLError(XMLMessages.MSG_CLOSE_PAREN_REQUIRED_IN_MIXED,
                                         XMLMessages.P51_CLOSE_PAREN_REQUIRED,
                                         element.rawname);
                     return -1;
                 }
                 decreaseParenDepth();
                 if (nodeIndex == -1) {
                     nodeIndex = prevNodeIndex;
                 } else if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 if (fEntityReader.lookingAtChar('*', true)) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_ZERO_OR_MORE, nodeIndex);
                 } else if (starRequired) {
                     reportFatalXMLError(XMLMessages.MSG_MIXED_CONTENT_UNTERMINATED,
                                         XMLMessages.P51_UNTERMINATED,
                                         fStringPool.toString(element.rawname),
                                         fDTDGrammar.getContentSpecNodeAsString(nodeIndex));
                     return -1;
                 }
                 return nodeIndex;
             }
             if (nodeIndex != -1) {
                 if (prevNodeIndex != -1) {
                     nodeIndex = fDTDGrammar.addContentSpecNode(XMLContentSpec.CONTENTSPECNODE_CHOICE, prevNodeIndex, nodeIndex);
                 }
                 prevNodeIndex = nodeIndex;
             }
             starRequired = true;
             checkForPEReference(false);
             checkForElementTypeWithPEReference(fEntityReader, ')', fElementRefQName);
             valueIndex = fElementRefQName.rawname;
             if (valueIndex == -1) {
                 reportFatalXMLError(XMLMessages.MSG_ELEMENT_TYPE_REQUIRED_IN_MIXED_CONTENT,
                                     XMLMessages.P51_ELEMENT_TYPE_REQUIRED,
                                     element.rawname);
                 return -1;
             }
         }
 
     } // scanMixed(QName):int",Buggy,"Bug - fix is to assign the array reference


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315690 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix associated with assigning the array reference. In the provided code diff, we can observe that the original code did not properly handle the expansion of the `valueSeen` array. It attempted to continue using the old array even after detecting an `ArrayIndexOutOfBoundsException`. The change assigns the new resized array back to `valueSeen`, ensuring that subsequent operations use the expanded array.

Here's the breakdown of the change:
- Initially, the `valueSeen` array is created with a fixed size.
- When `valueCount` exceeds this initial size, an `ArrayIndexOutOfBoundsException` is caught, and a new array with double the size of the original is created.
- The critical fix in this diff is the assignment `valueSeen = newArray;` which ensures the newly created array is used moving forward, preventing further `ArrayIndexOutOfBoundsException` occurrences.

This fix addresses a bug where the program could crash or behave incorrectly if the size of `valueSeen` exceeded 32 without assigning the larger array reference properly to `valueSeen`.

Given the nature of this change, it does indeed constitute a bug fix as per the commit message. The original code had a clear potential for runtime errors due to improper array handling, which has now been rectified.

Therefore, the evaluation can be concluded as **Buggy**."
xerces2-j,5598.json,c08116e59d73beba99136a5c0d0b920e74a6c891,"@@ -1,33 +1,33 @@
     synchronized RangeToken getCaseInsensitiveToken() {
         if (this.icaseCache != null)
             return this.icaseCache;
             
         RangeToken uppers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < this.ranges.length;  i += 2) {
             for (int ch = this.ranges[i];  ch <= this.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     uppers.addRange(ch, ch);
                 else {
                     char uch = Character.toUpperCase((char)ch);
                     uppers.addRange(uch, uch);
                 }
             }
         }
         RangeToken lowers = this.type == Token.RANGE ? Token.createRange() : Token.createNRange();
         for (int i = 0;  i < uppers.ranges.length;  i += 2) {
             for (int ch = uppers.ranges[i];  ch <= uppers.ranges[i+1];  ch ++) {
                 if (ch > 0xffff)
                     lowers.addRange(ch, ch);
                 else {
-                    char uch = Character.toUpperCase((char)ch);
+                    char uch = Character.toLowerCase((char)ch);
                     lowers.addRange(uch, uch);
                 }
             }
         }
         lowers.mergeRanges(uppers);
         lowers.mergeRanges(this);
         lowers.compactRanges();
 
         this.icaseCache = lowers;
         return lowers;
     }",Buggy,"Fix a problem with range tokens where lower case characters where not added (in a case insensitive mode)

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@827769 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message specifies the issue of lower case characters not being added in a case-insensitive mode. In the code diff, the change from `Character.toUpperCase((char)ch)` to `Character.toLowerCase((char)ch)` within the second for-loop addresses this problem. Initially, the code added only uppercase representations even when lowercase was expected, leading to incomplete case-insensitive functionality. By changing the code to correctly process and add lowercase characters to the `lowers` range token, the bug related to the incomplete case-insensitive character mapping is resolved.

The diff aligns perfectly with the commit message, which mentions fixing a problem with range tokens for lowercase characters in case-insensitive mode. This modification improves the logic by ensuring that both upper and lower case characters are appropriately considered, effectively solving the identified problem.

Conclusion: **Buggy**."
xerces2-j,2798.json,c12fd694626841e15a23993537a61d1ef75260c4,"@@ -1,3 +1,6 @@
     public TypeInfo getSchemaTypeInfo(){
-      return this;
+        if(needsSyncData()) {
+            synchronizeData();
+        }
+        return this;
     }",Buggy,"A TypeInfo.getTypeName(...) bug fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320508 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a bug fix for `TypeInfo.getTypeName(...)`, and the code change occurs in a method `getSchemaTypeInfo()`. The change adds a conditional check with `needsSyncData()` and a call to `synchronizeData()`, suggesting the need for ensuring data consistency or synchronization before returning the object. This addresses a potential logical flaw where data might be accessed in an inconsistent or unsynchronized state. Even though the method names in the message and the diff do not exactly match, the modification aligns with the context of a bug fix potentially related to ensuring correct type information retrieval. Therefore, the conclusion is **Buggy**."
xerces2-j,4988.json,860cb3b9c2ad281211d6d52beb871b2439720fa5,"@@ -1,3 +1,6 @@
     public static Element getParent(Element elem) {
-        return (Element)elem.getParentNode();
+        Node parent = elem.getParentNode();
+        if (parent instanceof Element)
+            return (Element)parent;
+        return null;
     } // getParent(Element):Element",Buggy,"Fixed some small bugs. Now we can have a sequence of two elements.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317613 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code change made in the `getParent` method addresses potential issues when the parent node is not an instance of `Element`. The original implementation directly cast the parent node to an `Element`, which could result in a `ClassCastException` if the parent node was not an `Element`. 

With the updated implementation, a check is in place to determine if the parent node is indeed an instance of `Element`. If it is, the parent node is returned after casting; otherwise, the method returns `null`. This modification prevents runtime exceptions and ensures that `getParent` only returns `Element` instances or `null`, aligning with expected behavior when the parent is not an `Element`.

Regarding the commit message ""Fixed some small bugs. Now we can have a sequence of two elements,"" it mentions the fixing of small bugs. While the message does not explicitly link to the method `getParent`, the code change indeed corrects a probable bug related to improper casting.

Therefore, given the nature of the change and the context provided by the commit message, the conclusion is **Buggy**."
xerces2-j,4757.json,c7c7140fbc0baf115a90871a337515ebc33c7265,"@@ -1,45 +1,45 @@
     public XMLInputSource resolveEntity(XMLResourceIdentifier resourceIdentifier)
             throws XNIException, IOException {
         
         if (fEntityResolver != null) {
             
             String pubId = resourceIdentifier.getPublicId();
-            String sysId = resourceIdentifier.getExpandedSystemId();
+            String sysId = resourceIdentifier.getLiteralSystemId();
             String baseURI = resourceIdentifier.getBaseSystemId();
             String name = null;
             if (resourceIdentifier instanceof XMLDTDDescription) {
                 name = ""[dtd]"";
             }
             else if (resourceIdentifier instanceof XMLEntityDescription) {
                 name = ((XMLEntityDescription) resourceIdentifier).getEntityName();
             }
             
             // When both pubId and sysId are null, the user's entity resolver
             // can do nothing about it. We'd better not bother calling it.
             // This happens when the resourceIdentifier is a GrammarDescription,
             // which describes a schema grammar of some namespace, but without
             // any schema location hint. -Sg
             if (pubId == null && sysId == null) {
                 return null;
             }
             
             // Resolve using EntityResolver2
             try {
                 InputSource inputSource = 
                     fEntityResolver.resolveEntity(name, pubId, baseURI, sysId);
                 return (inputSource != null) ? createXMLInputSource(inputSource, baseURI) : null;
             }
             // error resolving entity
             catch (SAXException e) {
                 Exception ex = e.getException();
                 if (ex == null) {
                     ex = e;
                 }
                 throw new XNIException(ex);
             }   
         }
         
         // unable to resolve entity
         return null;
         
     } // resolveEntity(XMLResourceIdentifier):XMLInputSource",Buggy,"Fixing a bug. The systemId passed to EntityResolver2.resolveEntity may be an absolute or relative URI.
That is it should be the literal system identifier, not the expanded one which resolved from the base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320213 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the handling of system identifiers in entity resolution. The code change modifies how the system identifier is retrieved from the `XMLResourceIdentifier` instance. Instead of using `getExpandedSystemId()`, it now uses `getLiteralSystemId()`. 

### Analyzing Changes:

1. **Code Logic:**
   - Original Code: `String sysId = resourceIdentifier.getExpandedSystemId();`
   - Modified Code: `String sysId = resourceIdentifier.getLiteralSystemId();`

2. **Behavioral Impact:**
   - **Original Implementation:** It retrieves the expanded system identifier, which is a URI that has potentially been resolved relative to a base URI. This means the identifier may not reflect the original input.
   - **Modified Implementation:** It retrieves the literal system identifier, which reflects the original input and may be absolute or relative. This change ensures that the system ID passed to `EntityResolver2.resolveEntity` is not modified or resolved against a base URI, aligning with the commit message's intent.

3. **Commit Message Alignment:** 
   - The commit message explains that the systemId should be the literal identifier rather than the expanded one. The code change directly reflects this message, making the identifier used consistent with the expectation of the `EntityResolver2` interface. 

4. **Exception Handling:** The existing exception handling around the entity resolution call remains unchanged, suggesting no additional error types are introduced or fixed here.

### Conclusion:

The code change aligns with the commit message and fixes a bug related to how the system identifier is resolved and passed during XML entity resolution. The update ensures that the `resolveEntity` method receives the correct form of the system identifier. Therefore, this commit is categorized as a **Buggy**."
xerces2-j,1574.json,474a683ae11a84aaa7da8861708189c9d0c5d551,"@@ -1,11 +1,14 @@
     public boolean isNonEscapingElement( String tagName )
     {
         int i;
 
-        if ( _nonEscapingElements == null )
-            return false;
+        if ( _nonEscapingElements == null ) {
+            // non escaping was not set
+            // by default output value for elements as unescaped
+            return true;
+        }
         for ( i = 0 ; i < _nonEscapingElements.length ; ++i )
             if ( _nonEscapingElements[ i ].equals( tagName ) )
                 return true;
         return false;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=2389


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318097 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The modification in the code changes the behavior when `_nonEscapingElements` is `null`. Previously, the method returned `false`, indicating that the element should not be considered ""non-escaping"" by default. After the change, it now returns `true`, meaning that when `_nonEscapingElements` is `null`, elements are treated as non-escaping by default.

The commit message references a bug fix, specifically linked to a bug report (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=2389). Although the details of the bug report are not provided in the diff, the change in behavior suggests it addresses an issue where elements were incorrectly escaped when `_nonEscapingElements` was not set. The adjustment aligns the method's behavior with expected outcomes likely discussed in the bug report.

Therefore, the changes in the diff seem to correct a bug related to the handling of element escaping, as inferred from the commit message's link to a specific bug report. This aligns with the nature of a bug fix addressing erroneous logic, and the explanation is consistent with a bug fix relevant to XML processing behavior.

Conclusively, the nature of the change and its contextual correspondence with the commit message suggest that the modification is indeed correcting a bug. Thus, the conclusion is **Buggy**."
xerces2-j,5596.json,1543bbc70bee5fdf7d5abd54532f9b21e7617de5,"@@ -1,86 +1,86 @@
     protected void intersectRanges(Token token) {
         RangeToken tok = (RangeToken)token;
         if (tok.ranges == null || this.ranges == null)
             return;
         this.icaseCache = null;
         this.sortRanges();
         this.compactRanges();
         tok.sortRanges();
         tok.compactRanges();
 
         int[] result = new int[this.ranges.length+tok.ranges.length];
         int wp = 0, src1 = 0, src2 = 0;
         while (src1 < this.ranges.length && src2 < tok.ranges.length) {
             int src1begin = this.ranges[src1];
             int src1end = this.ranges[src1+1];
             int src2begin = tok.ranges[src2];
             int src2end = tok.ranges[src2+1];
             if (src1end < src2begin) {          // Not overlapped
                                                 // src1: o-----o
                                                 // src2:         o-----o
                                                 // res:  empty
                                                 // Reuse src2
                 src1 += 2;
             } else if (src1end >= src2begin
                        && src1begin <= src2end) { // Overlapped
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // src2:      o----o
                                                 // src2:          o----o
                                                 // src2:  o------------o
-                if (src2begin <= src2begin && src1end <= src2end) {
+                if (src2begin <= src1begin && src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:  o------------o
                                                 // res:     o--------o
                                                 // Reuse src2
                     result[wp++] = src1begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else if (src2begin <= src1begin) {
                                                 // src1:    o--------o
                                                 // src2:  o----o
                                                 // res:     o--o
                                                 // Reuse the rest of src1
                     result[wp++] = src1begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                     src2 += 2;
                 } else if (src1end <= src2end) {
                                                 // src1:    o--------o
                                                 // src2:          o----o
                                                 // res:           o--o
                                                 // Reuse src2
                     result[wp++] = src2begin;
                     result[wp++] = src1end;
                     src1 += 2;
                 } else {
                                                 // src1:    o--------o
                                                 // src2:      o----o
                                                 // res:       o----o
                                                 // Reuse the rest of src1
                     result[wp++] = src2begin;
                     result[wp++] = src2end;
                     this.ranges[src1] = src2end+1;
                 }
             } else if (src2end < src1begin) {
                                                 // Not overlapped
                                                 // src1:          o-----o
                                                 // src2: o----o
                 src2 += 2;
             } else {
                 throw new RuntimeException(""Token#intersectRanges(): Internal Error: [""
                                            +this.ranges[src1]
                                            +"",""+this.ranges[src1+1]
                                            +""] & [""+tok.ranges[src2]
                                            +"",""+tok.ranges[src2+1]
                                            +""]"");
             }
         }
         while (src1 < this.ranges.length) {
             result[wp++] = this.ranges[src1++];
             result[wp++] = this.ranges[src1++];
         }
         this.ranges = new int[wp];
         System.arraycopy(result, 0, this.ranges, 0, wp);
                                                 // this.ranges is sorted and compacted.
     }",Buggy,"Fixing JIRA Bug #1224:
http://issues.apache.org/jira/browse/XERCESJ-1224

Correcting a typo in intersectRanges(). Thanks to Dave Brosius 
for catching this error and suggesting how to correct it.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@504431 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates fixing a bug related to a typo in the `intersectRanges()` method. Upon examining the diff, the original line contains a logical error:

```java
if (src2begin <= src2begin && src1end <= src2end)
```

The condition `src2begin <= src2begin` is always true and therefore does not make logical sense. The corrected line:

```java
if (src2begin <= src1begin && src1end <= src2end)
```

This change correctly checks if the range from `src1begin` to `src1end` falls within the range from `src2begin` to `src2end`, fixing the oversight.

Given the description in the commit message and the change in the code, it is clear that a bug was present and has been fixed regarding the range intersection condition. This fix likely corrects the logic of the program to ensure accurate range intersection results.

Therefore, the changes made in this diff do align with the commit message, indicating a bug fix. The assessment is **Buggy**."
xerces2-j,3365.json,163dc12c725a4e9bc243970c82016cb18998c25c,"@@ -1,34 +1,37 @@
     public TimeZone getTimeZone(int defaultZoneoffset) {
         TimeZone result = null;
         int zoneoffset = getTimezone();
 
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             zoneoffset = defaultZoneoffset;
         }
         if (zoneoffset == DatatypeConstants.FIELD_UNDEFINED) {
             result = TimeZone.getDefault();
         } else {
             // zoneoffset is in minutes. Convert to custom timezone id format.
             char sign = zoneoffset < 0 ? '-' : '+';
             if (sign == '-') {
                 zoneoffset = -zoneoffset;
             }
             int hour = zoneoffset / 60;
             int minutes = zoneoffset - (hour * 60);
 
             // Javadoc for java.util.TimeZone documents max length
             // for customTimezoneId is 8 when optional ':' is not used.
             // Format is 
             // ""GMT"" ('-'|''+') (digit digit?) (digit digit)?
             //                   hour          minutes
             StringBuffer customTimezoneId = new StringBuffer(8);
             customTimezoneId.append(""GMT"");
             customTimezoneId.append(sign);
             customTimezoneId.append(hour);
             if (minutes != 0) {
+                if (minutes < 10) {
+                    customTimezoneId.append('0');
+                }
                 customTimezoneId.append(minutes);
             }
             result = TimeZone.getTimeZone(customTimezoneId.toString());
         }
         return result;
     }",Buggy,"Fixing JIRA Bug #1243:
http://issues.apache.org/jira/browse/XERCESJ-1243

When we're building the TimeZone string if minutes < 10 we need to prepend a zero
to conform to the format expected by TimeZone.getTimeZone().


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@524223 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message notes a specific bug (JIRA Bug #1243) related to time zone string formatting. The code changes address this by adding a leading zero for time zone minutes when they are less than 10. This aligns with the expected format for TimeZone.getTimeZone(), ensuring correct behavior. This change is clearly a bug fix as described in the commit message, addressing a format-related issue.

Thus, The answer is **Buggy**."
xerces2-j,4419.json,0414900df065f25d451157d2c0f4d9ed5079d3a8,"@@ -1,13 +1,18 @@
     public void startParameterEntity (String name,
     XMLResourceIdentifier identifier,
     String encoding,
     Augmentations augs) throws XNIException {
         if (DEBUG_EVENTS) {
             System.out.println (""==>startParameterEntity: ""+name);
             if (DEBUG_BASEURI) {
                 System.out.println (""   expandedSystemId: ""+identifier.getExpandedSystemId ());
                 System.out.println (""   baseURI:""+ identifier.getBaseSystemId ());
             }
         }
+        if (augs != null && fInternalSubset != null && 
+            !fInDTDExternalSubset && 
+            Boolean.TRUE.equals(augs.getItem(Constants.ENTITY_SKIPPED))) {
+            fInternalSubset.append(name).append("";\n"");
+        }
         fBaseURIStack.push (identifier.getExpandedSystemId ());
     }",Buggy,"Fixing JIRA Bug #1114:
http://issues.apache.org/jira/browse/XERCESJ-1114

If a parameter entity is skipped the parameter entity reference 
should be written into the internal subset string.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@441668 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states a fix for JIRA Bug #1114, describing an issue where a skipped parameter entity's reference should be written into the internal subset string. 

The code changes are consistent with this description. 

1. **Additional Check and Logic:**
   - `if (augs != null && fInternalSubset != null && !fInDTDExternalSubset && Boolean.TRUE.equals(augs.getItem(Constants.ENTITY_SKIPPED)))`: This condition checks if augmentations are present, the internal subset is initialized, the current context is not the external subset, and the entity was indeed skipped.
   - `fInternalSubset.append(name).append("";\n"")`: If these conditions are met, the name of the skipped parameter entity is appended to the `fInternalSubset` with a semicolon, followed by a newline, which implements the desired behavior as described in the bug report.

These changes directly target the behavior described in the bug, representing an error-handling update specific to skipped parameters, aligning with the commit message.

Thus, these modifications clearly indicate a bug fix. **Buggy**."
xerces2-j,2331.json,c8ed9f7617b7aef095f3e03f97aefda48972f356,"@@ -1,178 +1,197 @@
     public short compareTreePosition(Node other) {
         // Questions of clarification for this method - to be answered by the
         // DOM WG.   Current assumptions listed - LM
         // 
         // 1. How do ENTITY nodes compare?  
         //    Current assumption: TREE_POSITION_DISCONNECTED, as ENTITY nodes 
         //    aren't really 'in the tree'
         //
         // 2. How do NOTATION nodes compare?
         //    Current assumption: TREE_POSITION_DISCONNECTED, as NOTATION nodes
         //    aren't really 'in the tree'
         //
         // 3. Are TREE_POSITION_ANCESTOR and TREE_POSITION_DESCENDANT     
         //    only relevant for nodes that are ""part of the document tree""?   
         //     <outer>
         //         <inner  myattr=""true""/>
         //     </outer>
         //    Is the element node ""outer"" considered an ancestor of ""myattr""?
         //    Current assumption: No.                                     
         //
         // 4. How do children of ATTRIBUTE nodes compare (with eachother, or  
         //    with children of other attribute nodes with the same element)    
         //    Current assumption: Children of ATTRIBUTE nodes are treated as if 
         //    they they are the attribute node itself, unless the 2 nodes 
         //    are both children of the same attribute. 
         //
         // 5. How does an ENTITY_REFERENCE node compare with it's children? 
         //    Given the DOM, it should precede its children as an ancestor. 
         //    Given ""document order"",  does it represent the same position?     
         //    Current assumption: An ENTITY_REFERENCE node is an ancestor of its
         //    children.
         //
         // 6. How do children of a DocumentFragment compare?   
         //    Current assumption: If both nodes are part of the same document 
         //    fragment, there are compared as if they were part of a document. 
 
         
         // If the nodes are the same...
         if (this==other) 
           return (TREE_POSITION_SAME_NODE | TREE_POSITION_EQUIVALENT);
         
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         short thisType = this.getNodeType();
         short otherType = other.getNodeType();
 
         // If either node is of type ENTITY or NOTATION, compare as disconnected
         if (thisType == Node.ENTITY_NODE || 
             thisType == Node.NOTATION_NODE ||
             otherType == Node.ENTITY_NODE ||
             otherType == Node.NOTATION_NODE ) {
           return TREE_POSITION_DISCONNECTED; 
         }
 
         // Find the ancestor of each node, and the distance each node is from 
         // its ancestor.
         // During this traversal, look for ancestor/descendent relationships 
         // between the 2 nodes in question. 
         // We do this now, so that we get this info correct for attribute nodes 
         // and their children. 
 
         Node node; 
         Node thisAncestor = this;
         Node otherAncestor = other;
         int thisDepth=0;
         int otherDepth=0;
         for (node=this; node != null; node = node.getParentNode()) {
             thisDepth +=1;
             if (node == other) 
               // The other node is an ancestor of this one.
               return (TREE_POSITION_ANCESTOR | TREE_POSITION_PRECEDING);
             thisAncestor = node;
         }
 
         for (node=other; node!=null; node=node.getParentNode()) {
             otherDepth +=1;
             if (node == this) 
               // The other node is a descendent of the reference node.
               return (TREE_POSITION_DESCENDANT | TREE_POSITION_FOLLOWING);
             otherAncestor = node;
         }
         
        
         Node thisNode = this;
         Node otherNode = other;
 
         int thisAncestorType = thisAncestor.getNodeType();
         int otherAncestorType = otherAncestor.getNodeType();
 
         // if the ancestor is an attribute, get owning element. 
         // we are now interested in the owner to determine position.
 
         if (thisAncestorType == Node.ATTRIBUTE_NODE)  {
            thisNode = ((AttrImpl)thisAncestor).getOwnerElement();
         }
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
            otherNode = ((AttrImpl)otherAncestor).getOwnerElement();
         }
 
         // Before proceeding, we should check if both ancestor nodes turned
         // out to be attributes for the same element
         if (thisAncestorType == Node.ATTRIBUTE_NODE &&  
             otherAncestorType == Node.ATTRIBUTE_NODE &&  
             thisNode==otherNode)              
             return TREE_POSITION_EQUIVALENT;
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
  
         // Note:  the following 2 loops are quite close to the ones above.
         // May want to common them up.  LM.
         if (thisAncestorType == Node.ATTRIBUTE_NODE) {
             thisDepth=0;
             for (node=thisNode; node != null; node=node.getParentNode()) {
                 thisDepth +=1;
                 if (node == otherNode) 
                   // The other node is an ancestor of the owning element
+                  {
                   return TREE_POSITION_PRECEDING;
+                  }
                 thisAncestor = node;
             }
         }
 
         // Now, find the ancestor of the owning element, if the original
         // ancestor was an attribute
         if (otherAncestorType == Node.ATTRIBUTE_NODE) {
             otherDepth=0;
             for (node=otherNode; node != null; node=node.getParentNode()) {
                 otherDepth +=1;
                 if (node == thisNode) 
                   // The other node is a descendent of the reference 
                   // node's element
                   return TREE_POSITION_FOLLOWING;
                 otherAncestor = node;
             }
         }
 
         // thisAncestor and otherAncestor must be the same at this point,  
         // otherwise, we are not in the same tree or document fragment
         if (thisAncestor != otherAncestor) 
           return TREE_POSITION_DISCONNECTED; 
 
-        // Determine which node is of the greatest depth.  
+      
+        // Go up the parent chain of the deeper node, until we find a node 
+        // with the same depth as the shallower node
+
         if (thisDepth > otherDepth) {
           for (int i=0; i<thisDepth - otherDepth; i++)
             thisNode = thisNode.getParentNode();
+          // Check if the node we have reached is in fact ""otherNode"". This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""precedes"" this.
+          if (thisNode == otherNode) 
+            return TREE_POSITION_PRECEDING;
         }
+ 
         else {
           for (int i=0; i<otherDepth - thisDepth; i++)
             otherNode = otherNode.getParentNode();
+          // Check if the node we have reached is in fact ""thisNode"".  This can
+          // happen in the case of attributes.  In this case, otherNode 
+          // ""follows"" this.
+          if (otherNode == thisNode) 
+            return TREE_POSITION_FOLLOWING;
         }
-          
+             
         // We now have nodes at the same depth in the tree.  Find a common 
         // ancestor.                                   
         Node thisNodeP, otherNodeP;
         for (thisNodeP=thisNode.getParentNode(),
                   otherNodeP=otherNode.getParentNode();
              thisNodeP!=otherNodeP;) {
              thisNode = thisNodeP;
              otherNode = otherNodeP;
              thisNodeP = thisNodeP.getParentNode();
              otherNodeP = otherNodeP.getParentNode();
         }
 
+        // At this point, thisNode and otherNode are direct children of 
+        // the common ancestor.  
         // See whether thisNode or otherNode is the leftmost
+
         for (Node current=thisNodeP.getFirstChild(); 
                   current!=null;
                   current=current.getNextSibling()) {
                if (current==otherNode) {
                  return TREE_POSITION_PRECEDING;
                }
                else if (current==thisNode) {
                  return TREE_POSITION_FOLLOWING;
                }
         }
         // REVISIT:  shouldn't get here.   Should probably throw an 
         // exception
         return 0;
 
     }",Buggy,"Fix for bug 13054


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318777 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a bug. The changes in the diff add checks that cover scenarios where nodes are attributes with the same owner, ensuring correct tree position comparisons. These updates likely prevent incorrect comparisons of node positions within a DOM tree. The fixes involve logic to properly identify ancestor-descendant relationships and account for special cases with attributes. The focus on correcting tree positioning logic suggests addressing functional errors, evident from added guards and handling. Therefore, the changes align with the provided commit message and indicate a bug fix. The answer is **Buggy**."
xerces2-j,4598.json,1ab22779af45496ed09d0d35aa50271c6e881b33,"@@ -1,3 +1,4 @@
     public void setLocale(Locale locale) throws XNIException {
+    	super.setLocale(locale);
         fErrorReporter.setLocale(locale);
     } // setLocale(Locale)",Buggy,"fix for bug id 5927. Thanks to 'Taki' for the solution.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a specific bug (bug id 5927) and attributes the solution to 'Taki'. The code change adds a call to `super.setLocale(locale);` before setting the locale of the `fErrorReporter` object. 

This change suggests resolving an issue where the superclass's locale-setting logic might not have been previously invoked, potentially leading to inconsistent locale settings or incorrect behavior in classes up the inheritance hierarchy. By ensuring that the superclass's method is called, the fix allows the entire object hierarchy to properly update their locale, tackling a likely oversight or logical error from before.

Given this context and the fact that the commit message directly references fixing a ""bug"", it is reasonable to conclude that this change indeed addresses a bug-related issue.

Thus, the appropriate conclusion is **Buggy**."
xerces2-j,1891.json,61370c8cd1f67817077ae8b356b0f69747471746,"@@ -1,72 +1,74 @@
     protected boolean matchChildSequence(QName element, int event)
             throws XNIException {
     	
         // need to resize fCurrentChildSequence
         if (fCurrentChildDepth >= fCurrentChildSequence.length) {
             int tmpCurrentChildSequence[] = new int[fCurrentChildSequence.length];
             System.arraycopy(fCurrentChildSequence, 0, tmpCurrentChildSequence,
                     0, fCurrentChildSequence.length);
 
             // Increase the size by a factor of 2 (?)
             fCurrentChildSequence = new int[fCurrentChildDepth * 2];
             System.arraycopy(tmpCurrentChildSequence, 0, fCurrentChildSequence,
                     0, tmpCurrentChildSequence.length);
         }
 
         //     
         if (fIsResolveElement) {
             // start
             if (event == XPointerPart.EVENT_ELEMENT_START) {
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildDepth++;
 
                 // reset the current child position 
                 fCurrentChildPosition = 1;
 
                 //if (!fSchemeNameFound) {
                 if ((fCurrentChildDepth <= fFoundDepth) || (fFoundDepth == 0)) {
                     if (checkMatch()) {
                         fIsElementFound = true;
                         fFoundDepth = fCurrentChildDepth;
                     } else {
                         fIsElementFound = false;
                         fFoundDepth = 0;
                     }
                 }
 
             } else if (event == XPointerPart.EVENT_ELEMENT_END) {
                 if (fCurrentChildDepth == fFoundDepth) {
                     fIsElementFound = true;
                 } else if (((fCurrentChildDepth < fFoundDepth) && (fFoundDepth != 0))
                         || ((fCurrentChildDepth > fFoundDepth) // or empty element found
                         && (fFoundDepth == 0))) {
                     fIsElementFound = false;
                 }
 
                 // reset array position of last child
                 fCurrentChildSequence[fCurrentChildDepth] = 0;
 
                 fCurrentChildDepth--;
                 fCurrentChildPosition = fCurrentChildSequence[fCurrentChildDepth] + 1;
                 
             } else if (event == XPointerPart.EVENT_ELEMENT_EMPTY) {
 
                 fCurrentChildSequence[fCurrentChildDepth] = fCurrentChildPosition;
                 fCurrentChildPosition++;
 
                 // Donot check for empty elements if the empty element is 
                 // a child of a found parent element 
-                //if (!fIsElementFound) {
-                    if (checkMatch()) {
-                        fIsElementFound = true;
+                if (checkMatch()) {
+                    if (!fIsElementFound) {
                         fWasOnlyEmptyElementFound = true;
                     } else {
-                        fIsElementFound = false;
+                        fWasOnlyEmptyElementFound = false;
                     }
-                //} 
-                
+                    fIsElementFound = true;
+                } else {
+                    fIsElementFound = false;
+                    fWasOnlyEmptyElementFound = false;
+                }  
             }
         }
 
         return fIsElementFound;
     }",Buggy,"Fixing JIRA Bug #1134: http://issues.apache.org/jira/browse/XERCESJ-1134

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@415823 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions a specific JIRA bug fix. The code changes indicate a focus on correctly handling events related to matching element sequences, particularly when dealing with empty elements. The changes move the setting of `fIsElementFound` out of a conditional block and adjust the logic regarding `fWasOnlyEmptyElementFound`. This is likely addressing a logic error in the conditions for setting flags relating to found elements, possibly aligning behavior with intended logic when encountering empty elements.

### Key Observations:
1. **Logical Corrections:** The adjustments seem to refine the logical flow when updating `fIsElementFound` and `fWasOnlyEmptyElementFound`, indicating a potential correction in how element matching states are updated under various conditions.
2. **Specific Conditions:** The removal and reorganization of if-else constructs suggest correcting what to do when different events in element processing occur (e.g., start, end, empty), which is likely an error previously observed under certain conditions.
3. **Commit Message Consistency:** The commit message directly calls out a bug ID, suggesting the change is a specific fix associated with a reported issue.

Given these observations, the code changes align well with rectifying a bug, likely related to incorrect state management when matching element sequences. Therefore, the analysis strongly suggests that this modification aims at fixing a bug.

**Conclusion:** **Buggy**"
xerces2-j,7694.json,60697c3db7e640a090be94027b7b5e35be8de0e4,"@@ -1,19 +1,21 @@
     public synchronized XSObjectList getAnnotations() {
         if(fAnnotations != null) 
             return fAnnotations;
 
         // do this in two passes to avoid inaccurate array size
         int totalAnnotations = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             totalAnnotations += fGrammarList[i].fNumAnnotations;
         }
         XSAnnotationImpl [] annotations = new XSAnnotationImpl [totalAnnotations];
         int currPos = 0;
         for (int i = 0; i < fGrammarCount; i++) {
             SchemaGrammar currGrammar = fGrammarList[i];
-            System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
-            currPos += currGrammar.fNumAnnotations;
+            if (currGrammar.fNumAnnotations > 0) {
+                System.arraycopy(currGrammar.fAnnotations, 0, annotations, currPos, currGrammar.fNumAnnotations);
+                currPos += currGrammar.fNumAnnotations;
+            }
         }
         fAnnotations = new XSObjectListImpl(annotations, annotations.length);
         return fAnnotations;
     }",Buggy,"Fixing Jira Bug #968:
http://nagoya.apache.org/jira/browse/XERCESJ-968

If no annotations exist for a SchemaGrammar the array
of annotations is never created. For such schema docs
we were passing null to System.arrayCopy which caused
a NullPointerException to be thrown. This is now fixed.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319923 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a `NullPointerException` when no annotations exist in a `SchemaGrammar`. The code change addresses this by adding a conditional check before calling `System.arraycopy()`. Specifically, it ensures that `currGrammar.fNumAnnotations` is greater than zero before attempting to copy annotations, thus preventing a null pointer situation.

The diffs' logic aligns with the commit message's intention to prevent an exception by handling cases where no annotations are present. The additions effectively correct the behavior that caused the bug as defined in the Jira entry.

Therefore, this change represents a clear bug fix for the described issue. Hence, the answer is **Buggy**."
xerces2-j,2219.json,5a14bab0762a851069a18eebd7f278b943f88c00,"@@ -1,20 +1,20 @@
     public Object getTypeInfo(int nodeIndex) {
         if (nodeIndex == -1) {
             return null;
         }
 
         int chunk = nodeIndex >> CHUNK_SHIFT;
         int index = nodeIndex & CHUNK_MASK;
         
         
-        Object value = fNodeValue[chunk][index];
+        Object value = fNodeValue[chunk] != null ? fNodeValue[chunk][index] : null;
         if (value != null) {
             fNodeValue[chunk][index] = null;
             RefCount c = (RefCount) fNodeValue[chunk][CHUNK_SIZE];
             c.fCount--;
             if (c.fCount == 0) {
                 fNodeValue[chunk] = null;
             }
         }
         return value;
     }",Buggy,"Fix a bug introduced in the new getTypeInfo method (DOM L3)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318962 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change involves checking if `fNodeValue[chunk]` is `null` before attempting to access `[index]`. This prevents potential `NullPointerException` when `fNodeValue[chunk]` is `null`. The modification addresses a logical flaw in the method that could occur if a `null` value were unexpectedly encountered, which aligns with the bug fix described in the commit message.

Therefore, the change is a correction addressing a bug in the `getTypeInfo` method, and the commit message accurately describes this correction. The logical improvement aligns with the description of fixing a newly introduced bug.

The conclusion is **Buggy**."
xerces2-j,4324.json,53ec54f2822c25f6815aa81b3b2eb8f46dd3caf5,"@@ -1,91 +1,91 @@
     protected void configurePipeline() {
         if (fCurrentDVFactory != fDatatypeValidatorFactory) {
             fCurrentDVFactory = fDatatypeValidatorFactory;
             // use XML 1.0 datatype library
             setProperty(DATATYPE_VALIDATOR_FACTORY, fCurrentDVFactory);
         }
 
         // setup DTD pipeline
         if (fCurrentDTDScanner != fDTDScanner) {
-			fCurrentDTDScanner = fDTDScanner;
+            fCurrentDTDScanner = fDTDScanner;
             setProperty(DTD_SCANNER, fCurrentDTDScanner);
             setProperty(DTD_PROCESSOR, fDTDProcessor);
-            fDTDScanner.setDTDHandler(fDTDProcessor);
-            fDTDProcessor.setDTDSource(fDTDScanner);
-            fDTDProcessor.setDTDHandler(fDTDHandler);
-            if (fDTDHandler != null) {
-                 fDTDHandler.setDTDSource(fDTDProcessor);
-            }
+        }
+        fDTDScanner.setDTDHandler(fDTDProcessor);
+        fDTDProcessor.setDTDSource(fDTDScanner);
+        fDTDProcessor.setDTDHandler(fDTDHandler);
+        if (fDTDHandler != null) {
+            fDTDHandler.setDTDSource(fDTDProcessor);
+        }
 
-            fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
-            fDTDProcessor.setDTDContentModelSource(fDTDScanner);
-            fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
-            if (fDTDContentModelHandler != null) {
-                fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
-            }            
+        fDTDScanner.setDTDContentModelHandler(fDTDProcessor);
+        fDTDProcessor.setDTDContentModelSource(fDTDScanner);
+        fDTDProcessor.setDTDContentModelHandler(fDTDContentModelHandler);
+        if (fDTDContentModelHandler != null) {
+            fDTDContentModelHandler.setDTDContentModelSource(fDTDProcessor);
         }
 
         // setup document pipeline
         if (fFeatures.get(NAMESPACES) == Boolean.TRUE) {
             if (fCurrentScanner != fNamespaceScanner) {
                 fCurrentScanner = fNamespaceScanner;
                 setProperty(DOCUMENT_SCANNER, fNamespaceScanner);
                 setProperty(DTD_VALIDATOR, fDTDValidator);
             }
             fNamespaceScanner.setDTDValidator(fDTDValidator);
             fNamespaceScanner.setDocumentHandler(fDTDValidator);
             fDTDValidator.setDocumentSource(fNamespaceScanner);
             fDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fDTDValidator);
             }
             fLastComponent = fDTDValidator;
         } else {
             // create components
             if (fNonNSScanner == null) {
                 fNonNSScanner = new XMLDocumentScannerImpl();
                 fNonNSDTDValidator = new XMLDTDValidator();
                 // add components
                 addComponent((XMLComponent) fNonNSScanner);
                 addComponent((XMLComponent) fNonNSDTDValidator);
             }
             if (fCurrentScanner != fNonNSScanner) {
                 fCurrentScanner = fNonNSScanner;
                 setProperty(DOCUMENT_SCANNER, fNonNSScanner);
                 setProperty(DTD_VALIDATOR, fNonNSDTDValidator);
             }
 
             fNonNSScanner.setDocumentHandler(fNonNSDTDValidator);
             fNonNSDTDValidator.setDocumentSource(fNonNSScanner);
             fNonNSDTDValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fNonNSDTDValidator);
             }
             fLastComponent = fNonNSDTDValidator;
         }
 
         // add XML Schema validator if needed
         if (fFeatures.get(XMLSCHEMA_VALIDATION) == Boolean.TRUE) {
             // If schema validator was not in the pipeline insert it.
             if (fSchemaValidator == null) {
                 fSchemaValidator = new XMLSchemaValidator();
                 // add schema component
                 setProperty(SCHEMA_VALIDATOR, fSchemaValidator);
                 addCommonComponent(fSchemaValidator);
-				fSchemaValidator.reset(this);
+                fSchemaValidator.reset(this);
                 // add schema message formatter
                 if (fErrorReporter.getMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN) == null) {
                     XSMessageFormatter xmft = new XSMessageFormatter();
                     fErrorReporter.putMessageFormatter(XSMessageFormatter.SCHEMA_DOMAIN, xmft);
                 }
 
             }
             fLastComponent.setDocumentHandler(fSchemaValidator);
             fSchemaValidator.setDocumentSource(fLastComponent);
             fSchemaValidator.setDocumentHandler(fDocumentHandler);
             if (fDocumentHandler != null) {
                 fDocumentHandler.setDocumentSource(fSchemaValidator);
             }
             fLastComponent = fSchemaValidator;
         }
     } // configurePipeline()",Buggy,"Fixing a bug in XML10 configurePipeline: we should always setup DTD pipeline (was only
setup if scanner has not been changed)


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319532 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the bug relates to the setup of the DTD pipeline, which was previously only being configured if the DTD scanner had not been changed. This has been updated so that the DTD pipeline is always set up, regardless of changes to the DTD scanner.

Here's the reasoning for the bug fix determination:

1. **Logic Correction**: Previously, the DTD pipeline setup (lines setting `fDTDScanner`, `fDTDProcessor`, and `fDTDHandler`) only occurred within the `if` block checking if the current DTD scanner was different from the expected scanner (`fCurrentDTDScanner != fDTDScanner`). This meant that if `fCurrentDTDScanner` was already set correctly, the DTD pipeline was not reconfigured even though it should have been.

2. **Bug Fix Confirmation**: The code change moves crucial DTD pipeline setup steps (e.g., invoking `setDTDHandler`) outside of the conditional block, ensuring that these operations always occur.

3. **Aligns with Commit Message**: The changes directly address the issue described in the commit message, ensuring the DTD pipeline is set up regardless of changes to the scanner.

Overall, the changes indicate a logical correction to ensure the consistent setup of the DTD pipeline, which was previously incorrect under certain conditions. This action rectifies the identified bug. The answer is **Buggy**."
xerces2-j,4808.json,70b3c80f83b7fcd277b5905ccde6e49f5e6274bb,"@@ -1,4 +1,4 @@
     public String getType(String qname) {
         int index = getIndex(qname);
-        return index != -1 ? fAttributes[index].type : null;
+        return index != -1 ? getType(index): null;
     } // getType(String):String",Buggy,"fixing bug 15584.  Thanks to Michael Glavassevich for pointing out this bug in a SAX context; it was equally an XNI bug, however.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319115 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code change modifies the return logic for `getType(String qname)`. Previously, it accessed the `type` directly from `fAttributes` using an index, but now it uses a method call `getType(index)` instead. This change suggests that the method `getType(int index)` potentially handles additional logic that was not considered in the direct access approach. This could address a bug related to how the attribute type was previously determined when using `qname`.

Given the commit message notes a fix for a bug identified in both SAX and XNI contexts, it's reasonable to conclude that this method adjustment corrects an underlying consistency or logic error with attribute type retrieval.

Therefore, this change aligns with a bug fix, as it likely addresses the incorrectly reported `type` when using qualified names, as pointed out by Michael Glavassevich. The fix ensures consistent handling as needed in this context.

Thus, the conclusion is **Buggy**."
xerces2-j,2281.json,f905e13bf111fee815eabfc522f4dfc0949fb422,"@@ -1,3 +1,3 @@
     public void setType(XSTypeDefinition type) {
-        type = type;
-    }

+        this.type = type;
+    }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=17064
Patch submitter: Michael Glavassevich


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319093 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change corrects an assignment within the `setType` method. Previously, the method had a no-op self-assignment (assigning `type = type;`), meaning it did not set the intended instance variable `this.type`. The correction makes the method correctly assign the passed `type` to `this.type`, indicating a bug related to object state management. The commit message also references a specific bug report in the Apache issue tracker. Given these insights, the changes align with fixing a bug related to the proper setting of an object's type. Therefore, the changes are:

1. Correcting logical error: The incorrect self-assignment is replaced by a proper assignment to the instance variable.
2. Potentially fixing a bug: The change ensures the instance variable `type` is correctly updated, addressing the faulty logic.

Thus, the conclusion is **Buggy**."
xerces2-j,3095.json,955ef6b2ca713901bd702935c131a09390af1389,"@@ -1,9 +1,8 @@
     int indexOf(Node child, Node parent) {
-        Node node;
-        int i = 0;
         if (child.getParentNode() != parent) return -1;
-        for(node = child; node!= null; node=node.getPreviousSibling()) {
+        int i = 0;
+        for(Node node = parent.getFirstChild(); node!= child; node=node.getNextSibling()) {
             i++;
         }
         return i;
     }",Buggy,"applied patch from Lynn Monson:
fixed a bug in indexOf which was off by 1


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316616 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that the change fixes an off-by-one bug in the `indexOf` method. Let's analyze the changes:

1. **Original Code**:  
   - The method initializes `node` to null and `i` to 0.
   - It checks if the child's parent is not the given parent, returning -1 if true.
   - It iterates backward from the child using `getPreviousSibling()` until `node` is null, incrementing `i`.

2. **Modified Code**:  
   - `i` is initialized before the loop.
   - The loop iterates from the parent's first child using `getFirstChild()` and `getNextSibling()`.
   - The loop increments `i` until it encounters the `child`.

**Analysis**:
- The original code attempts to find `child` by moving backward from it, which may incorrectly count nodes that shouldn't be included, potentially causing an off-by-one error.
- The modified code correctly counts siblings by iterating forward from the first child of the parent, which aligns with typical zero-based indexing for positioning within a list or sequence.

The change aligns well with the commit message's description of fixing an off-by-one error, as the new approach ensures accurate counting from the start of the sibling list.

Hence, the changes are consistent with the commit message indicating a bug fix. Therefore, the answer is **Buggy**."
xerces2-j,3693.json,b23b90432763872d07b931251896abce3e1e90e6,"@@ -1,39 +1,39 @@
         public InputSource resolveEntity(String name, String publicId, 
                 String baseURI, String systemId) throws SAXException, IOException {
             if (fEntityResolver != null) {
                 LSInput lsInput = fEntityResolver.resolveResource(XML_TYPE, null, publicId, systemId, baseURI);
                 if (lsInput != null) {
                     final String pubId = lsInput.getPublicId();
                     final String sysId = lsInput.getSystemId();
                     final String baseSystemId = lsInput.getBaseURI();
                     final Reader charStream = lsInput.getCharacterStream();
                     final InputStream byteStream = lsInput.getByteStream();
                     final String data = lsInput.getStringData();
                     final String encoding = lsInput.getEncoding();
 
                     /**
                      * An LSParser looks at inputs specified in LSInput in
                      * the following order: characterStream, byteStream,
                      * stringData, systemId, publicId. For consistency
                      * with the DOM Level 3 Load and Save Recommendation
                      * use the same lookup order here.
                      */
                     InputSource inputSource = new InputSource();
                     inputSource.setPublicId(pubId);
-                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(systemId, baseSystemId) : systemId);
+                    inputSource.setSystemId((baseSystemId != null) ? resolveSystemId(sysId, baseSystemId) : sysId);
                     
                     if (charStream != null) {
                         inputSource.setCharacterStream(charStream);
                     }
                     else if (byteStream != null) {
                         inputSource.setByteStream(byteStream);
                     }
                     else if (data != null && data.length() != 0) {
                         inputSource.setCharacterStream(new StringReader(data));
                     }
                     inputSource.setEncoding(encoding);
                     return inputSource;
                 }
             }
             return null;
         }",Buggy,"Fixing a minor bug. The InputSource returned from the resolution forwarder should 
contain the system identifier returned from the application's LSResourceResolver 
not the one which was passed to the resolveEntity() method.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@374971 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The described change in the Git diff directly correlates with the commit message regarding a bug fix. The modification adjusts the argument passed to the method `resolveSystemId` from the original `systemId` (passed to the `resolveEntity` method) to `sysId` (the system identifier from `LSInput`).

Previously, the code was mistakenly using the originally passed system identifier to construct the `InputSource`. The correct behavior, as stated in the commit message, is to use the system identifier obtained from the application's `LSResourceResolver`. This ensures the correct system identifier is used, fixing the bug described in the commit message.

This change involves fixing a logical error related to resource resolution, aligning with the intention in the commit message to address a bug about the system identifier.

Therefore, the answer is **Buggy**."
xerces2-j,1803.json,6c7a16c3202d0e50e4cd05449e9a5a9963f80e1b,"@@ -1,6 +1,7 @@
     public int hashCode() {
         if (uri != null) {
-            return uri.hashCode() + localpart.hashCode();
+            return uri.hashCode() + 
+                ((localpart != null) ? localpart.hashCode() : 0);
         }
-        return rawname.hashCode();
+        return (rawname != null) ? rawname.hashCode() : 0;
     } // hashCode():int",Buggy,"Fixing Jira Bug #997:
http://nagoya.apache.org/jira/browse/XERCESJ-997

Fixing an NPE which may occur in the hashCode method
when either the localpart or rawname fields are null. This
seems to only be a problem for external users of this class.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320004 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code change directly addresses the potential NullPointerException by adding null checks for the `localpart` and `rawname` fields in the `hashCode` method. This change ensures that if either `localpart` or `rawname` is null, a default value of 0 is used, thus preventing a null value from causing the exception. This change aligns perfectly with the commit message, which specifies fixing the NPE issue.

The explicit mention of fixing a known bug (Jira Bug #997) and the nature of the code changes focusing on null checks to avoid null-pointer exceptions indicate that this commit is indeed a bug fix.

**Buggy**"
xerces2-j,3399.json,0df32768beedf6778651639c24d234d5b70f3b6e,"@@ -1,30 +1,33 @@
     private static void organizeParts(
         String whole,
         String[] parts,
         int[] partsIndex,
         int len,
         String tokens)
         throws IllegalArgumentException {
 
         int idx = tokens.length();
         for (int i = len - 1; i >= 0; i--) {
+            if (parts[i] == null) {
+                throw new IllegalArgumentException(whole);
+            }
             int nidx =
                 tokens.lastIndexOf(
                     parts[i].charAt(parts[i].length() - 1),
                     idx - 1);
             if (nidx == -1) {
                 throw new IllegalArgumentException(whole);
                 // ,partsIndex[i]+parts[i].length()-1);
             }
 
             for (int j = nidx + 1; j < idx; j++) {
                 parts[j] = null;
             }
             idx = nidx;
             parts[idx] = parts[i];
             partsIndex[idx] = partsIndex[i];
         }
         for (idx--; idx >= 0; idx--) {
             parts[idx] = null;
         }
     }",Buggy,"Fixing JIRA Bug #1416: http://issues.apache.org/jira/browse/XERCESJ-1416. An NPE could occur when the value of the duration is invalid. Check for this condition and throw the correct exception.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@906803 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly indicates that a fix for a bug leading to a NullPointerException (NPE) is addressed. The code change introduces a check to see if any elements in the `parts` array are `null` before attempting operations on them. If a `null` part is encountered, it now throws an `IllegalArgumentException` with the provided `whole` string before proceeding further in the loop.

Here is the breakdown of how this code change aligns with the commit message and indicates a bug fix:
1. **Bug Identification**: The commit message refers to a specific JIRA bug about a potential NPE when processing invalid duration values.
2. **Code Change Analysis**: The added null check ensures the code does not attempt to access the `charAt` method on a `null` object, which is what would cause an NPE.
3. **Exception Handling Improvement**: The change replaces a runtime exception (NPE) with a domain-relevant `IllegalArgumentException`, which is more meaningful and intended for this situation.

Taken together, these points clearly indicate the change is meant to address a bug that can cause an application crash due to an uncaught exception in Java. The fix ensures the code behaves as expected in cases where it previously might have failed unpredictably.

The code changes align with the commit message in explicitly fixing a bug by adding precise error-checking logic.

Therefore, the conclusion is **Buggy**."
xerces2-j,3918.json,3260ba200fec37c5d074e5bb96ebd856e494ebe9,"@@ -1,14 +1,22 @@
     public void setFeature(String name, boolean value)
         throws ParserConfigurationException, SAXNotRecognizedException, 
 		SAXNotSupportedException
     {
         // XXX This is ugly.  We have to collect the features and then
         // later create an XMLReader to verify the features.
         if (features == null) {
             features = new Hashtable();
         }
         features.put(name, new Boolean(value));
 
         // Test the feature by possibly throwing SAX exceptions
-        newSAXParserImpl();
+        try {
+            newSAXParserImpl();
+        } catch (SAXNotSupportedException e) {
+            features.remove(name);
+            throw e;
+        } catch (SAXNotRecognizedException e) {
+            features.remove(name);
+            throw e;
+        }
     }",Buggy,"Fix setFeature() bug provided by Kohsuke Kawaguchi


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317014 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix to the `setFeature()` method. The diff adds exception handling for `SAXNotSupportedException` and `SAXNotRecognizedException`. If an exception is thrown while testing the feature with `newSAXParserImpl()`, the code now removes the feature from the `features` dictionary, ensuring no invalid feature settings persist. This enhances robustness by preventing invalid states. This change aligns well with the intention to fix a bug, as it corrects improper feature handling. Thus, the answer is **Buggy**."
xerces2-j,5468.json,7b91d52436909c5274ee9d683e37e15b4d73c03b,"@@ -1,111 +1,116 @@
     public void print(Node node) {
 
         // is there anything to do?
         if ( node == null ) {
             return;
         }
 
         int type = node.getNodeType();
         switch ( type ) {
         // print document
         case Node.DOCUMENT_NODE: {
                 if ( !canonical ) {
                     String  Encoding = this.getWriterEncoding();
                     if ( Encoding.equalsIgnoreCase( ""DEFAULT"" ) )
                         Encoding = ""UTF-8"";
                     else if ( Encoding.equalsIgnoreCase( ""Unicode"" ) )
                         Encoding = ""UTF-16"";
                     else
                         Encoding = MIME2Java.reverse( Encoding );
 
                     out.println(""<?xml version=\""1.0\"" encoding=\""""+
                                 Encoding + ""\""?>"");
                 }
-                print(((Document)node).getDocumentElement());
+                //print(((Document)node).getDocumentElement());
+                
+                NodeList children = node.getChildNodes(); 
+                for ( int iChild = 0; iChild < children.getLength(); iChild++ ) { 
+                    print(children.item(iChild)); 
+                } 
                 out.flush();
                 break;
             }
 
             // print element with attributes
         case Node.ELEMENT_NODE: {
                 out.print('<');
                 out.print(node.getNodeName());
                 Attr attrs[] = sortAttributes(node.getAttributes());
                 for ( int i = 0; i < attrs.length; i++ ) {
                     Attr attr = attrs[i];
                     out.print(' ');
                     out.print(attr.getNodeName());
                     out.print(""=\"""");
                     out.print(normalize(attr.getNodeValue()));
                     out.print('""');
                 }
                 out.print('>');
                 NodeList children = node.getChildNodes();
                 if ( children != null ) {
                     int len = children.getLength();
                     for ( int i = 0; i < len; i++ ) {
                         print(children.item(i));
                     }
                 }
                 break;
             }
 
             // handle entity reference nodes
         case Node.ENTITY_REFERENCE_NODE: {
                 if ( canonical ) {
                     NodeList children = node.getChildNodes();
                     if ( children != null ) {
                         int len = children.getLength();
                         for ( int i = 0; i < len; i++ ) {
                             print(children.item(i));
                         }
                     }
                 } else {
                     out.print('&');
                     out.print(node.getNodeName());
                     out.print(';');
                 }
                 break;
             }
 
             // print cdata sections
         case Node.CDATA_SECTION_NODE: {
                 if ( canonical ) {
                     out.print(normalize(node.getNodeValue()));
                 } else {
                     out.print(""<![CDATA["");
                     out.print(node.getNodeValue());
                     out.print(""]]>"");
                 }
                 break;
             }
 
             // print text
         case Node.TEXT_NODE: {
                 out.print(normalize(node.getNodeValue()));
                 break;
             }
 
             // print processing instruction
         case Node.PROCESSING_INSTRUCTION_NODE: {
                 out.print(""<?"");
                 out.print(node.getNodeName());
                 String data = node.getNodeValue();
                 if ( data != null && data.length() > 0 ) {
                     out.print(' ');
                     out.print(data);
                 }
-                out.print(""?>"");
+                out.println(""?>"");
                 break;
             }
         }
 
         if ( type == Node.ELEMENT_NODE ) {
             out.print(""</"");
             out.print(node.getNodeName());
             out.print('>');
         }
 
         out.flush();
 
     } // print(Node)",Buggy,"Fix submitted by Ed Stub. Fixes problem reported by Stephane.RAULT@r2isa... where we
are no reporting PI's


PR:
Obtained from:
Submitted by:
Reviewed by:


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316112 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix regarding processing instruction (PI) reporting. In the diff:

1. The handling of `Node.DOCUMENT_NODE` is changed. Previously, only the document element was printed. The new code loops through all children of the document node, allowing each to be processed individually. This change ensures that processing instructions (PIs), which are not necessarily part of the document element, are included in the output.

2. The code handling `Node.PROCESSING_INSTRUCTION_NODE` has been modified to use `out.println(""?>"");` instead of `out.print(""?>"");`. This introduces a newline after printing the processing instruction, which may aid in readability or meet a format requirement.

These changes directly relate to the commit message about fixing an issue where processing instructions weren't being reported. Therefore, the modifications align with the description of the problem and represent a bug fix.

Thus, the correct assessment of this commit is **Buggy**."
xerces2-j,684.json,d1fd0668a071ddcf44c6bdf1b50030bab12d18ee,"@@ -1,25 +1,26 @@
     public synchronized void deleteRow( int index )
     {
         Node    child;
         
         child = getFirstChild();
         while ( child != null )
         {
             if ( child instanceof HTMLTableRowElement )
             {
                 if ( index == 0 )
                 {
                     removeChild ( child );
                     return;
                 }
+                --index;
             }
             else
             if ( child instanceof HTMLTableSectionElementImpl )
             {
                 index = ( (HTMLTableSectionElementImpl) child ).deleteRowX( index );
                 if ( index < 0 )
                     return;
             }
             child = child.getNextSibling();
         }
     }",Buggy,"Fixing bug in HTML DOM (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=18744). Deletion of rows other than row 0 were failing siliently.

Thanks to Brett Sutton for the fix.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319291 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the ""deletion of rows other than row 0"" failing silently. The change to the `deleteRow` method addresses this issue by decrementing the `index` whenever an `HTMLTableRowElement` is encountered but not removed (i.e., when `index` is not 0).

Here's the step-by-step reasoning:

1. **Initial Behavior Analysis**:
   - The original logic only removed the first found row if `index` was 0. It did not properly reduce the `index` for subsequent matching HTMLTableRowElements, leading to a failure to delete rows other than the first one specified.

2. **Bug Identification**:
   - The code failed when trying to reference rows besides index 0 because, after encountering an HTMLTableRowElement, it immediately returned without adjusting the index for further traversal.

3. **Code Change Description**:
   - By decrementing `--index` whenever an `HTMLTableRowElement` is encountered (and not removed due to `index` being non-zero), it ensures the correct row eventually reaches `index` 0, triggering the removal.

4. **Outcome**:
   - This alteration effectively fixes the described silent failure, allowing the function to correctly navigate and decrement through table row elements until the specified row is reached and deleted.

Therefore, the change in the code accurately reflects the bug fix described in the commit message. The fix properly addresses the failure of the deletion operation for row indices other than 0. Thus, the conclusion is **Buggy**."
xerces2-j,5580.json,af847434ae5a2c3408cae3c4b738622800c93e4f,"@@ -1,43 +1,43 @@
     private static void buildCaseInsensitiveMap() {
         caseInsensitiveMap = new int[INITIAL_CHUNK_COUNT][][];
         for (int i=0; i<INITIAL_CHUNK_COUNT; i++) {
             caseInsensitiveMap[i] = new int[CHUNK_SIZE][];
         }
         
         int lc, uc;
         for (int i=0; i<0x10000; i++) {
-            lc = Character.toLowerCase(i);
-            uc = Character.toUpperCase(i);
+            lc = Character.toLowerCase((char) i);
+            uc = Character.toUpperCase((char) i);
 
             // lower/upper case value is not the same as code point
             if (lc != uc || lc != i) {
                 int[] map = new int[2];
                 int index = 0;
 
                 if (lc != i) {
                     map[index++] = lc;
                     map[index++] = LOWER_CASE_MATCH;
                     int[] lcMap = getMapping(lc);
                     if (lcMap != null) {
                         map = updateMap(i, map, lc, lcMap, LOWER_CASE_MATCH);
                     }
                 }
                 
                 if (uc != i) {
                     if (index == map.length) {
                         map = expandMap(map, 2);
                     }
                     map[index++] = uc;
                     map[index++] = UPPER_CASE_MATCH;
                     int[] ucMap = getMapping(uc);
                     if (ucMap != null) {
                         map = updateMap(i, map, uc, ucMap, UPPER_CASE_MATCH);
                     }
                 }
                 
                 set(i, map);
             }
         }
 
         mapBuilt = Boolean.TRUE;
     }",Buggy,"Fixing compilation errors on Java SE 1.4 and below. Character.toLowerCase(int) and Character.toUpperCase(int) didn't exist until Java SE 5.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@834593 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates fixing compilation issues on Java SE 1.4 and earlier. In these versions, `Character.toLowerCase(int)` and `Character.toUpperCase(int)` do not exist, as they were introduced in Java 5. Instead, using `Character.toLowerCase(char)` and `Character.toUpperCase(char)` resolves the issue. The change in the code modifies method calls where the integer variable `i` is cast to a character `char`. This aligns with the commit message, which aims to restore compatibility with earlier Java versions by using available method signatures.

The change does not indicate a logical bug in runtime behavior but addresses a compatibility issue that prevented the code from compiling. This ensures the code will compile in older Java environments without altering the intended functionality of converting character cases.

Given that the change described in the commit message accurately reflects the diff shown, and it resolves a specific issue related to compilation rather than runtime logic errors, it focuses on compatibility rather than bug fixing. Therefore, based on the criteria for bug-related changes, the conclusion is **NotBuggy**."
xerces2-j,7513.json,13892ec89f2b5203650caf1d7b5355df433dacfd,"@@ -1,74 +1,74 @@
         private void mergeSchemaGrammars(SchemaGrammar cachedGrammar, SchemaGrammar newGrammar) {
 
             /** Add new top-level element declarations. **/
             XSNamedMap map = newGrammar.getComponents(XSConstants.ELEMENT_DECLARATION);
             int length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSElementDecl decl = (XSElementDecl) map.item(i);
-                if (cachedGrammar.getElementDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalElementDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalElementDecl(decl);
                 }
             }
             
             /** Add new top-level attribute declarations. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeDecl decl = (XSAttributeDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeDecl(decl);
                 }
             }
             
             /** Add new top-level type definitions. **/
             map = newGrammar.getComponents(XSConstants.TYPE_DEFINITION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSTypeDefinition decl = (XSTypeDefinition) map.item(i);
                 if (cachedGrammar.getGlobalTypeDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalTypeDecl(decl);
                 }
             }
             
             /** Add new top-level attribute group definitions. **/
             map = newGrammar.getComponents(XSConstants.ATTRIBUTE_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSAttributeGroupDecl decl = (XSAttributeGroupDecl) map.item(i);
-                if (cachedGrammar.getAttributeDeclaration(decl.getName()) == null) {
+                if (cachedGrammar.getGlobalAttributeGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalAttributeGroupDecl(decl);
                 }
             }
             
             /** Add new top-level model group definitions. **/
             map = newGrammar.getComponents(XSConstants.MODEL_GROUP);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSGroupDecl decl = (XSGroupDecl) map.item(i);
                 if (cachedGrammar.getGlobalGroupDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalGroupDecl(decl);
                 }
             }
             
             /** Add new top-level notation declarations. **/
             map = newGrammar.getComponents(XSConstants.NOTATION_DECLARATION);
             length = map.getLength();
             for (int i = 0; i < length; ++i) {
                 XSNotationDecl decl = (XSNotationDecl) map.item(i);
                 if (cachedGrammar.getGlobalNotationDecl(decl.getName()) == null) {
                     cachedGrammar.addGlobalNotationDecl(decl);
                 }
             }
             
             /** 
              * Add all annotations. Since these components are not named it's
              * possible we'll add duplicate components. There isn't much we can
              * do. It's no worse than XMLSchemaLoader when used as an XSLoader.
              */
             XSObjectList annotations = newGrammar.getAnnotations();
             length = annotations.getLength();
             for (int i = 0; i < length; ++i) {
                 cachedGrammar.addAnnotation((XSAnnotationImpl) annotations.item(i));
             }
             
         }",Buggy,"Fixing a few minor bugs.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The changes in the diff introduce a series of method replacements for checking if specific schema components already exist within the `cachedGrammar`. The original calls like `getElementDeclaration`, `getAttributeDeclaration`, and `getAttributeDeclaration` have been replaced with more specific calls like `getGlobalElementDecl`, `getGlobalAttributeDecl`, and `getGlobalAttributeGroupDecl`.

These modifications are indicative of bug fixes because:

1. **Correct Functionality**: The methods changed likely result in checking the correct global declarations within the `cachedGrammar`. This implies that there was possibly a misalignment in logic where non-global declarations were being checked or added inappropriately, which could lead to incorrect behavior in schema grammar merging.

2. **Consistency and Specificity**: By explicitly using methods that suggest they are intended for global declarations, the code becomes more precise. This reduces the potential for logical errors in processing schema components.

3. **Aligns with Commit Message**: The message indicates fixing ""minor bugs"", and these changes align well with such a description. The changes suggest an enhancement of accuracy when merging grammar components, which would be vital in a schema validation context.

Overall, these revisions represent logical corrections to how different types of declarations are handled, ensuring the grammar merging process functions as intended without unintentionally duplicating or missing components due to incorrect method usage.

Therefore, this diff is consistent with the commit message about fixing minor bugs, indicating it is an actual bug fix. The conclusion is **Buggy**."
xerces2-j,2176.json,b16dd097ce2ebce1533edaab4e6743b08da63bfe,"@@ -1,26 +1,26 @@
     public int createDeferredDocumentType(String rootElementName,
                                           String publicId, String systemId) {
 
         // create node
         int nodeIndex = createNode(Node.DOCUMENT_TYPE_NODE);
         int chunk     = nodeIndex >> CHUNK_SHIFT;
         int index     = nodeIndex & CHUNK_MASK;
 
         // added for DOM2: createDoctype factory method includes
         // name, publicID, systemID
 
         // create extra data node
         int extraDataIndex = createNode((short)0); // node type unimportant
         int echunk = extraDataIndex >> CHUNK_SHIFT;
         int eindex = extraDataIndex & CHUNK_MASK;
 
         // save name, public id, system id
         setChunkValue(fNodeName, rootElementName, chunk, index);
-        setChunkValue(fNodeValue, publicId, chunk, eindex);
-        setChunkValue(fNodeURI, systemId, chunk, eindex);
+        setChunkValue(fNodeValue, publicId, chunk, index);
+        setChunkValue(fNodeURI, systemId, chunk, index);
         setChunkIndex(fNodeExtra, extraDataIndex, chunk, index);
 
         // return node index
         return nodeIndex;
 
     } // createDeferredDocumentType(String,String,String):int",Buggy,"Fixed bug that caused namespace information to be lost
in the deferred DOM.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317632 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"In the provided code diff, there's a fix that addresses how public and system IDs are assigned to a new node in the `createDeferredDocumentType` method.

### Analysis:
- **Original Logic:** The original logic stored `publicId` and `systemId` into `fNodeValue` and `fNodeURI` using the `eindex` obtained from `extraDataIndex`. This implies that the extra data node was being used for storing these values.
- **Modified Logic:** The fix changed the logic to store `publicId` and `systemId` into `fNodeValue` and `fNodeURI` using `index`, which is associated with the `nodeIndex`. This means these values are now directly connected to the document type node itself, instead of using the additional node for these associations.

### Explanation:
The change ensures namespace-related information (`publicId` and `systemId`) is directly associated with the correct document type node instead of being stored in an extra, possibly unrelated data node. This aligns with the commit message stating that the bug fix prevents namespace information from being lost.

By correcting the indices used for storing these values, the code ensures that the `publicId` and `systemId` are associated correctly with the node representing the document type, thereby retaining necessary namespace information.

### Conclusion:
This change directly addresses a bug concerning the incorrect association of data, which resulted in namespace information being lost. Therefore, the change aligns with the commit message, indicating a bug fix.

The conclusion is **Buggy**."
xerces2-j,5971.json,b3530fffbb47d1dc2dab5e538c013f388b4e142f,"@@ -1,26 +1,31 @@
         protected void resolveExternalSubsetAndRead()
             throws IOException, XNIException {
             
             fDTDDescription.setValues(null, null, fEntityManager.getCurrentResourceIdentifier().getExpandedSystemId(), null);
             fDTDDescription.setRootName(fElementQName.rawname);
             XMLInputSource src = fExternalSubsetResolver.getExternalSubset(fDTDDescription);
             
             if (src != null) {
                 fDoctypeName = fElementQName.rawname;
                 fDoctypePublicId = src.getPublicId();
                 fDoctypeSystemId = src.getSystemId();
                 // call document handler
                 if (fDocumentHandler != null) {
                     // This inserts a doctypeDecl event into the stream though no 
                     // DOCTYPE existed in the instance document.
                     fDocumentHandler.doctypeDecl(fDoctypeName, fDoctypePublicId, fDoctypeSystemId, null);
                 }
                 try {
-                    fDTDScanner.setInputSource(src);
-                    while (fDTDScanner.scanDTDExternalSubset(true));
+                    if (fValidationManager == null || !fValidationManager.isCachedDTD()) {
+                        fDTDScanner.setInputSource(src);
+                        while (fDTDScanner.scanDTDExternalSubset(true));
+                    }
+                    else {
+                        fDTDScanner.setInputSource(null);
+                    }
                 }
                 finally {
                     fEntityManager.setEntityHandler(XMLDocumentScannerImpl.this);
                 }
             }
         } // resolveExternalSubsetAndRead()",Buggy,"Fixing a bug reported by Ritu Raj Tiwari on xerces-j-dev. When resolving
an external subset if there's already a DTD in the grammar pool do not
read the input source.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320234 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff addresses a bug where an external subset was being resolved and read even if a corresponding DTD already existed in the grammar pool. This unnecessary action could lead to performance inefficiencies or incorrect behavior if the input source shouldn't be re-read.

1. **Logic Correction**: The new code checks if the `fValidationManager` is null or if `fValidationManager.isCachedDTD()` returns false before reading the DTD input source. This condition prevents reading from the input source if a DTD is already cached, aligning the functionality with avoiding redundant reads when a DTD is present.

2. **Error and Exception Handling**: The revised code includes additional logic (an else statement setting `fDTDScanner.setInputSource(null);`) to handle cases where the DTD should not be read again. It ensures that the input source isn't incorrectly set in these scenarios.

3. **Relevance to the Commit Message**: The changes directly address the bug described in the commit message by ensuring that external DTD subsets are not read when a cached version is available, which matches the intended resolution of the problem.

Overall, the commit message accurately reflects the changes made in the code. The modification addresses a specific logical bug, preventing unwanted reading of input sources when a DTD already exists in the grammar pool.

Thus, the conclusion is **Buggy**."
xerces2-j,2755.json,2b2593eef2e2d2c5352fbc9b4e356053a93e34ab,"@@ -1,6 +1,12 @@
     Node matchNodeOrParent(Node node) {
+        // Additions and removals in the underlying data structure may occur
+        // before any iterations, and in this case the reference_node is null.
+        if (fCurrentNode == null) return null;
+        
+        // check if the removed node is an _ancestor_ of the 
+        // reference node
         for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Bug fix: http://nagoya.apache.org/bugzilla/show_bug.cgi?id=13062


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318813 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message references a specific bug report in a bug tracking system. The code diff adds a check to return null if `fCurrentNode` is null before entering the for loop. This behavior likely addresses a bug where operations on a null `fCurrentNode` could previously result in unexpected behavior or errors (e.g., a NullPointerException) when performing iterations or node comparisons. This change introduces a safeguard by ensuring the method exits early if `fCurrentNode` hasn't been initialized or is otherwise null. 

The change is directly related to enhancing error handling and ensuring correct functionality under a specific scenario (when `fCurrentNode` is null), consistent with a bug fix.

Thus, this change aligns well with the commit message, which explicitly states that it addresses a bug. Therefore, the appropriate assessment is **Buggy**."
xerces2-j,2755.json,7f856ddb5537fc5e98c71ff23700a48bac7f652d,"@@ -1,6 +1,6 @@
     Node matchNodeOrParent(Node node) {
-        for (Node n = node; n != fRoot; n = n.getParentNode()) {
+        for (Node n = fCurrentNode; n != fRoot; n = n.getParentNode()) {
             if (node == n) return n;
         }
         return null;
     }",Buggy,"Applying patch suggested by Joe Kesselman. This fixes the following bug:
http://nagoya.apache.org/bugzilla/show_bug.cgi?id=6888


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318267 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The provided diff shows a small but significant change in the for loop condition of the method `matchNodeOrParent`. Originally, the loop began with `Node n = node`, but Joe Kesselman's patch changes it to start with `Node n = fCurrentNode`.

Here's the reasoning step-by-step:

1. **Understanding the Change**: The loop iterates over parent nodes. Initially, the search begins from the `node` passed as an argument, with each iteration moving to the parent node `n.getParentNode()`.
   
2. **Modification Analysis**: Changing the starting node of the loop from `node` to `fCurrentNode` indicates a logic correction. The intention seems to be ensuring the search for the matching node begins from a specific node in the current state or context (`fCurrentNode`) rather than the passed parameter (`node`). 

3. **Implications**: This change can correct scenarios where the search should always start from a particular point (`fCurrentNode`) rather than wherever the method is called with (`node`). It potentially fixes issues where the intended matching fails because the search starts at an incorrect initial node.

4. **Bug Fix Confirmation**: Given the reference to a specific bug report (http://nagoya.apache.org/bugzilla/show_bug.cgi?id=6888) in the commit message and the logical correction implied by the code change, it is clear that this is a bug fix. 

The commit message and code changes are aligned and indicative of a bug fix. Therefore, the conclusion is **Buggy**."
xerces2-j,4545.json,3893a210786fea6e8236c54214287515c5333708,"@@ -1,8 +1,9 @@
     public void setFeature(String featureId, boolean state)
         throws XMLConfigurationException {
         if (featureId.equals(XINCLUDE_FEATURE)) {
             fXIncludeEnabled = state;
+            fConfigUpdated = true;
             return;
         }
         super.setFeature(featureId,state);
     }",Buggy,"Fixing a bug. It was possible that the XIncludeHandler could be used without being properly reset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@320336 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that a bug was fixed in the handling of the XIncludeHandler, specifically regarding a failure to reset. The code diff shows an update to the `setFeature` method. It sets a new boolean flag, `fConfigUpdated`, to `true` when the `XINCLUDE_FEATURE` is enabled or disabled. This suggests that the internal configuration state is marked as updated whenever this feature's state changes.

The commit relates to ensuring that any changes in the feature's state are noted and likely require the handler to reset or reconfigure accordingly. This supports the idea that the handler could have been used in an updated state without proper initialization beforehand, which is the issue the commit intends to fix.

The changes align with the commit message and address the described problem. Therefore, this indicates a bug fix. The answer is **Buggy**."
xerces2-j,376.json,9768ff6cd6cd57357deb3c4be70cb29bb476e3d2,"@@ -1,15 +1,16 @@
     public String getJavaEncoding() {
         String javaEncoding = null;
         String mimeEncoding = getMimeEncoding();
 
         if (mimeEncoding != null) {
             if (mimeEncoding.equals( ""DEFAULT"" ))
                 javaEncoding =  ""UTF8"";
             else if (mimeEncoding.equalsIgnoreCase( ""UTF-16"" ))
                 javaEncoding = ""Unicode"";
             else
                 javaEncoding = EncodingMap.getIANA2JavaMapping( mimeEncoding );    
-        } else   // Should never return null
+        } 
+        if(javaEncoding == null)   // Should never return null
             javaEncoding = ""UTF8"";
         return(javaEncoding);
     }",Buggy,"fix for bug #6008


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318133 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The code diff changes the handling of `javaEncoding` by adding a condition that assigns a default encoding of ""UTF8"" if `javaEncoding` is still `null` after attempting to map it from `mimeEncoding`. Previously, if `mimeEncoding` was `null`, `javaEncoding` defaulted to ""UTF8"", but didn't confirm that `EncodingMap.getIANA2JavaMapping(mimeEncoding)` didn't also return `null`. 

The new condition ensures that `getJavaEncoding()` never returns `null`, which aligns with the comment ""// Should never return null"". This adjustment likely fixes issues relating to `null` being returned in some edge cases, potentially leading to NullPointerExceptions or other logic errors elsewhere in the program.

Given these considerations, the change corrects a logical error (ensuring no unexpected `null` return), which correlates with fixing a reported bug. Thus, this code change is a bug fix.

The answer is **Buggy**."
xerces2-j,5806.json,f6fbb6219a5d2f0cfd71f23812b4571049b5613b,"@@ -1,93 +1,97 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
             if (this.chardata == '{' && this.offset < this.regexlen) {
 
                 int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
 
                 if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
 
                     min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
+                        if (min < 0)
+                            throw ex(""parser.quantifier.5"", this.offset);
                     }
                 }
                 else {
                     throw ex(""parser.quantifier.1"", this.offset);
                 }
 
                 max = min;
                 if (ch == ',') {
 
                    if (off >= this.regexlen) {
                        throw ex(""parser.quantifier.3"", this.offset);
                    }
                    else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
 
                         max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
+                            if (max < 0)
+                                throw ex(""parser.quantifier.5"", this.offset);
                         }
 
                         if (min > max)
                             throw ex(""parser.quantifier.4"", this.offset);
                    }
                    else { // assume {min,}
                         max = -1;           
                     }
                 }
 
                if (ch != '}')
                    throw ex(""parser.quantifier.2"", this.offset);
 
                if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
 
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17415: Regexes with large min/max not handled correctly.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319162 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The provided commit message describes fixing a bug related to regular expressions with large minimum and maximum values. Examining the code changes, we notice:

1. **Error Handling for Negative Values**: 
   - The code introduces two new checks for both the `min` and `max` values to ensure they are not negative. If they are found to be negative, an exception is thrown with the message: `""parser.quantifier.5"", this.offset`.

2. **Validation of Min and Max**: 
   - Previously, there might not have been a check to prevent `min` from becoming negative as digits were processed, which might occur due to integer overflow. The new exception handling ensures that if `min` or `max` fall into an invalid state (i.e., negative due to overflow), a controlled exception is raised.

3. **Fix for Specific Bug**: 
   - Given the context of the commit message and code changes, a bug that caused the parser to incorrectly handle very large numbers (due to overflow) has been addressed. This aligns with the commit message's indication of fixing a bug (specifically identified as bug 17415).

These changes clearly suggest an intention to fix an inherent issue dealing with large numbers in regex quantifiers, indicated by the commit message's emphasis on handling regexes correctly, especially those with large minimum and maximum values.

Therefore, I conclude the changes in the code provide a solution to a known bug, aligning with the commit message's description. Thus, the result is **Buggy**."
xerces2-j,5806.json,218c54d8d80c22fd4d9f45f8013e14eb23838699,"@@ -1,89 +1,93 @@
     Token parseFactor() throws ParseException {        
         int ch = this.read();
         Token tok;
         switch (ch) {
           case T_CARET:         return this.processCaret();
           case T_DOLLAR:        return this.processDollar();
           case T_LOOKAHEAD:     return this.processLookahead();
           case T_NEGATIVELOOKAHEAD: return this.processNegativelookahead();
           case T_LOOKBEHIND:    return this.processLookbehind();
           case T_NEGATIVELOOKBEHIND: return this.processNegativelookbehind();
 
           case T_COMMENT:
             this.next();
             return Token.createEmpty();
 
           case T_BACKSOLIDUS:
             switch (this.chardata) {
               case 'A': return this.processBacksolidus_A();
               case 'Z': return this.processBacksolidus_Z();
               case 'z': return this.processBacksolidus_z();
               case 'b': return this.processBacksolidus_b();
               case 'B': return this.processBacksolidus_B();
               case '<': return this.processBacksolidus_lt();
               case '>': return this.processBacksolidus_gt();
             }
                                                 // through down
         }
         tok = this.parseAtom();
         ch = this.read();
         switch (ch) {
           case T_STAR:  return this.processStar(tok);
           case T_PLUS:  return this.processPlus(tok);
           case T_QUESTION: return this.processQuestion(tok);
           case T_CHAR:
-            if (this.chardata == '{') {
-                                                // this.offset -> next of '{'
-                int off = this.offset;
+            if (this.chardata == '{' && this.offset < this.regexlen) {
+
+                int off = this.offset;          // this.offset -> next of '{'
                 int min = 0, max = -1;
-                if (off >= this.regexlen)  break;
-                ch = this.regex.charAt(off++);
-                if (ch != ',' && (ch < '0' || ch > '9'))  break;
-                if (ch != ',') {                // 0-9
-                    min = ch-'0';
+
+                if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
+
+                    min = ch -'0';
                     while (off < this.regexlen
                            && (ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {
                         min = min*10 +ch-'0';
-                        ch = -1;
                     }
-                    if (ch < 0)  break;
                 }
-                //if (off >= this.regexlen)  break;
+                else {
+                    throw ex(""parser.quantifier.1"", this.offset);
+                }
+
                 max = min;
                 if (ch == ',') {
-                    if (off >= this.regexlen
-                        || ((ch = this.regex.charAt(off++)) < '0' || ch > '9')
-                        && ch != '}')
-                        break;
-                    if (ch == '}') {
-                        max = -1;           // {min,}
-                    } else {
-                        max = ch-'0';       // {min,max}
+
+                   if (off >= this.regexlen) {
+                       throw ex(""parser.quantifier.3"", this.offset);
+                   }
+                   else if ((ch = this.regex.charAt(off++)) >= '0' && ch <= '9') {                       
+
+                        max = ch -'0';       // {min,max}
                         while (off < this.regexlen
                                && (ch = this.regex.charAt(off++)) >= '0'
                                && ch <= '9') {
                             max = max*10 +ch-'0';
-                            ch = -1;
                         }
-                        if (ch < 0)  break;
-                        //if (min > max)
-                        //    throw new ParseException(""parseFactor(): min > max: ""+min+"", ""+max);
+
+                        if (min > max)
+                            throw ex(""parser.quantifier.4"", this.offset);
+                   }
+                   else { // assume {min,}
+                        max = -1;           
                     }
                 }
-                if (ch != '}')  break;
-                                                // off -> next of '}'
-                if (this.checkQuestion(off)) {
+
+               if (ch != '}')
+                   throw ex(""parser.quantifier.2"", this.offset);
+
+               if (this.checkQuestion(off)) {  // off -> next of '}'
                     tok = Token.createNGClosure(tok);
                     this.offset = off+1;
                 } else {
                     tok = Token.createClosure(tok);
                     this.offset = off;
                 }
+
                 tok.setMin(min);
                 tok.setMax(max);
                 //System.err.println(""CLOSURE: ""+min+"", ""+max);
                 this.next();
             }
         }
         return tok;
     }",Buggy,"Fixing bugs 17417: Regex {min,max} with min > max not rejected.
Many thanks to Khaled Noaman for the patch.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319159 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly indicates fixing a bug related to processing regex quantifiers with incorrect min and max values. The code changes corroborate this by adding specific checks and throwing exceptions when necessary conditions aren't met, such as when `min` exceeds `max` or required characters are missing in the quantifier expression `{min,max}`. Here's a breakdown of the changes:

1. **Adding Bounds and Validation:** The code now checks if it remains within the regex length when parsing numbers using `this.offset < this.regexlen`. It ensures parsing halts appropriately rather than indexing beyond intended lengths.

2. **Min and Max Validations:** The original logic only checked for parsing termination conditions. It now confirms if `min` and `max` integers are sensible. E.g., `if (min > max)` raises an exception explicitly, which directly addresses the commit message's mentioned bug.

3. **Exception Handling Improvements:** The modifications throw specific exceptions (`parser.quantifier.XX`) if parsing does not meet expectations, enhancing robustness over merely breaking out of parsing.

4. **Inferencing Assumptions:** Previously implicit assumptions, like treating a trailing comma as having max values continuing to infinity (`max = -1`), have been made explicit, improving code understandability.

Given that these changes align with fixing incorrect behavior (min > max not being caught) and align with error-handling improvements, they indicate a bug fix.

Thus, the conclusion is **Buggy**."
xerces2-j,7034.json,ba7898b39060234e9c88e9e06c0dd5fbed52a9fb,"@@ -1,12 +1,13 @@
     public void startDocument() throws SAXException {
         fNeedPushNSContext = true;
+        fNamespaceContext.reset();
         try {
             fSchemaDOMParser.startDocument(fSAXLocatorWrapper, null, fNamespaceContext, null);
         }
         catch (XMLParseException e) {
             convertToSAXParseException(e);
         }
         catch (XNIException e) {
             convertToSAXException(e);
         }
     }",Buggy,"Fixing a bug. We weren't reseting the NamespaceContext which can cause all sorts
of problems including the loss of some namespace decls on XSAnnotation components.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@644489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message and the code modifications both address a bug related to the namespace context not being reset. The diff shows that the `fNamespaceContext.reset()` method is added at the beginning of the `startDocument()` method. This change ensures that the namespace context is properly reset at the start of a document, preventing potential issues with namespace declarations, as described in the commit message.

The change is directly related to the bug fix mentioned in the commit message, where not resetting the namespace context could lead to problems, such as loss of namespace declarations. This is a typical bug fix in managing the state within XML parsing logic, specifically making sure that the state is correctly initialized at the start of processing a new document.

Given the alignment between the commit message and the code change, and the nature of the fix, this clearly indicates a bug fix. Therefore, the changes indicate a bug fix, so the conclusion is **Buggy**."
xerces2-j,3375.json,883dc18e1cd70be3f4dc6fcaf48ddcdfe4c2a99d,"@@ -1,19 +1,19 @@
         private int parseInt(int minDigits, int maxDigits)
             throws IllegalArgumentException {
             int vstart = vidx;
-            while (isDigit(peek()) && (vidx - vstart) <= maxDigits) {
+            while (isDigit(peek()) && (vidx - vstart) < maxDigits) {
                 vidx++;
             }
             if ((vidx - vstart) < minDigits) {
                 // we are expecting more digits
                 throw new IllegalArgumentException(value); //,vidx);
             }
 
             // NumberFormatException is IllegalArgumentException            
             //           try {
             return Integer.parseInt(value.substring(vstart, vidx));
             //            } catch( NumberFormatException e ) {
             //                // if the value is too long for int, NumberFormatException is thrown
             //                throw new IllegalArgumentException(value,vstart);
             //            }
         }",Buggy,"Fixing a bug. parseInt() was allowing maxDigits + 1 which caused 
XMLGregorianCalendar to accept bogus dates like ""2007-008-003"".

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@565088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a bug in the parseInt method, where it allowed one more digit than intended, leading to the acceptance of incorrect date formats. The code change directly addresses this by adjusting the while loop condition from `<= maxDigits` to `< maxDigits`. This correction ensures the loop stops reading digits once `maxDigits` is reached, preventing the creation of invalid date objects.

The change aligns very well with the commit message, clearly indicating a fix to a condition that previously allowed for invalid date strings by accepting an extra digit. This update improves the validation logic precisely as described in the commit message, addressing a bug in logic regarding the number of digits processed.

Therefore, the analysis confirms that the changes made in the code match the intent of the commit message, and they rectify a specific incorrect behavior as noted in the message. The conclusion is **Buggy**."
xerces2-j,6542.json,3574857d4a4ddaf8aabec99216c178a5a6089a19,"@@ -1,46 +1,46 @@
     private void setOrdered(){
 
-        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all ﾷprimitiveﾷ types {value} is as specified in the table in Fundamental Facets (C.1).
+        // When {variety} is atomic, {value} is inherited from {value} of {base type definition}. For all ""primitive"" types {value} is as specified in the table in Fundamental Facets (C.1).
         if(fVariety == VARIETY_ATOMIC){
             this.fOrdered = fBase.fOrdered;
         }
 
         // When {variety} is list, {value} is false.
         else if(fVariety == VARIETY_LIST){
             this.fOrdered = ORDERED_FALSE;
         }
 
         // When {variety} is union, the {value} is partial unless one of the following:
         // 1. If every member of {member type definitions} is derived from a common ancestor other than the simple ur-type, then {value} is the same as that ancestor's ordered facet.
         // 2. If every member of {member type definitions} has a {value} of false for the ordered facet, then {value} is false.
         else if(fVariety == VARIETY_UNION){
             int length = fMemberTypes.length;
             // REVISIT: is the length possible to be 0?
             if (length == 0) {
                 this.fOrdered = ORDERED_PARTIAL;
                 return;
             }
             // we need to process the first member type before entering the loop
             short ancestorId = getPrimitiveDV(fMemberTypes[0].fValidationDV);
             boolean commonAnc = ancestorId != DV_ANYSIMPLETYPE;
             boolean allFalse = fMemberTypes[0].fOrdered == ORDERED_FALSE;
             // for the other member types, check whether the value is false
             // and whether they have the same ancestor as the first one
             for (int i = 1; i < fMemberTypes.length && (commonAnc || allFalse); i++) {
                 if (commonAnc)
                     commonAnc = ancestorId == getPrimitiveDV(fMemberTypes[i].fValidationDV);
                 if (allFalse)
                     allFalse = fMemberTypes[i].fOrdered == ORDERED_FALSE;
             }
             if (commonAnc) {
                 // REVISIT: all member types should have the same ordered value
                 //          just use the first one. Can we assume this?
                 this.fOrdered = fMemberTypes[0].fOrdered;
             } else if (allFalse) {
                 this.fOrdered = ORDERED_FALSE;
             } else {
                 this.fOrdered = ORDERED_PARTIAL;
             }
         }
 
     }//setOrdered",Buggy,"fixing bug 16714


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319088 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message references a bug fix, and the diff reflects a potential logic error or oversight in the `setOrdered` method that needed correction.

Here's the analysis of the code changes:

1. **Change of Comment Syntax**:
   - The change in comment syntax: `ﾷprimitiveﾷ` to `""primitive""`. This is likely not contributing to a bug fix directly, but it makes the code comments more readable and accurate, using standard quotes instead of special characters.

2. **Handling of `fOrdered`**:
   - The core logic for setting `fOrdered` based on `fVariety` appears unchanged. However, by addressing the readability of comments, it might be an indication of clarification that was needed to resolve bug #16714.

3. **Functionality**:
   - The intention here seems to be documenting the logic decisions in `setOrdered` better, thereby possibly addressing an explanatory bug or a misinterpretation concerning the ordinality of types in a union or list.

While there might not be an explicit logic change in the `setOrdered` method itself, the changes aim to enhance clarity possibly due to a misunderstanding or misinterpretation previously, which aligns with resolving a bug.

Thus, given the provided information and the focus on bug #16714, it is reasonable to conclude:

**Buggy**"
xerces2-j,7850.json,d6e418eec78c8b253b3f353be05259d353faa58a,"@@ -1,12 +1,12 @@
     private CMNode multiNodes(CMNode node, int num, boolean copyFirst) {
         if (num == 0) {
             return null;
         }
         if (num == 1) {
             return copyFirst ? copyNode(node) : node;
         }
         int num1 = num/2;
         return fNodeFactory.getCMBinOpNode(XSModelGroupImpl.MODELGROUP_SEQUENCE,
                                            multiNodes(node, num1, copyFirst),
-                                           multiNodes(node, num-num1, false));
+                                           multiNodes(node, num-num1, true));
     }",Buggy,"Fixing a bug introduced by a previous commit when trying to balance the
syntax tree.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319270 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The diff shows a change in the `multiNodes` method, where the third parameter of the second call to `multiNodes` has been changed from `false` to `true`. This parameter seems to control whether the first argument `node` is copied when `num` is greater than 1. The commit message suggests that this change is related to fixing a bug that was introduced in an earlier commit, specifically with balancing the syntax tree.

The issue could lie in how nodes are copied or referenced within the syntax tree. Using `true` instead of `false` for the second `multiNodes` call likely ensures that the correct structure of the syntax tree is maintained by making sure nodes are copied correctly rather than reusing them, which could be critical in maintaining an accurate and balanced syntax tree.

Overall, this change directly addresses logical behavior by affecting how the tree nodes are constructed, thus validating the commit message claiming a bug fix. Hence, the analysis of this code diff aligns with the message: it is fixing a previously introduced bug within the tree balancing operation.

Given these considerations, we conclude that this change is indeed a bug fix. Therefore, the answer is **Buggy**."
xerces2-j,8612.json,841a543458d7b19286dbcc32207d8978ccf195f5,"@@ -1,19 +1,19 @@
     protected void init() {
         if(fValidation || fDynamicValidation) {
             super.init();
             // now overwrite some entries in parent:
 
             try {
                 fValID       = fDatatypeValidatorFactory.getBuiltInDV(""XML11ID"");
-                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
+                fValIDRef    = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREF"");
                 fValIDRefs   = fDatatypeValidatorFactory.getBuiltInDV(""XML11IDREFS"");
-                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENSymbol"");
+                fValNMTOKEN  = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKEN"");
                 fValNMTOKENS = fDatatypeValidatorFactory.getBuiltInDV(""XML11NMTOKENS"");
 
             }
             catch (Exception e) {
                 // should never happen
                 e.printStackTrace(System.err);
             }
         }
     } // init()",Buggy,"Fixing 2 bugs:
1) Bugzilla# 18429, NPE thrown validating NMTOKEN. Thanks to Neil Delima for the patch.
2) S production is unchanged in XML 1.1, so shouldn't override isSpace()


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319269 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message references two bug fixes, with Bugzilla# 18429 addressing a NullPointerException in NMTOKEN validation. Aided by Neil Delima, this fix pertains to `fValNMTOKEN` using the correct built-in datatype validator, `XML11NMTOKEN`, rather than `XML11NMTOKENSymbol`. Also, it fixes `fValIDRef` using `XML11IDREF` instead of mistakenly duplicated `XML11IDREFS`. These adjustments correct datatype mappings, preventing potential validation errors such as NPEs and ensuring proper XML object validation as specified. This corresponds directly to a bug fix since it addresses validation issues by correcting mappings to datatype validators, potentially averting runtime exceptions and ensuring XML1.1's accuracy in processing certain entities/passwords. The environmental context depicted clarifies a sincere intent to patch existing validation hurdles relevant to XML processing, depicted by more inclusive setups.

This thorough coordination ensures that system behaviors better align with documentation and contractually agreed XML processing, thus directly resolving identified bug territory. Therefore, these code modifications align with the commit message, reflecting bug fixes accurately as described in the entry. The analysis concludes that this work denotes a **Buggy** classification, affirming its alignment in pinpointing and resolving bug-related issues and explicitly addressing the NPE concern, duly explicated through the elucidated changes in NMTOKEN validation logic."
xerces2-j,27.json,874363b725dcbddf23ed1f1d07d9a6fcd6e3ca4c,"@@ -1,46 +1,47 @@
     public void attributeDecl(String elementName, String attributeName,
                               String type, String[] enumeration,
                               String defaultType, XMLString defaultValue, 
+                              XMLString nonNormalizedDefaultValue,
                               Augmentations augs) throws XNIException {
 
         printIndent();
         fOut.print(""attributeDecl("");
         fOut.print(""elementName="");
         printQuotedString(elementName);
         fOut.print(',');
         fOut.print(""attributeName="");
         printQuotedString(attributeName);
         fOut.print(',');
         fOut.print(""type="");
         printQuotedString(type);
         fOut.print(',');
         fOut.print(""enumeration="");
         if (enumeration == null) {
             fOut.print(""null"");
         }
         else {
             fOut.print('{');
             for (int i = 0; i < enumeration.length; i++) {
                 printQuotedString(enumeration[i]);
                 if (i < enumeration.length - 1) {
                     fOut.print(',');
                 }
             }
             fOut.print('}');
         }
         fOut.print(',');
         fOut.print(""defaultType="");
         printQuotedString(defaultType);
         fOut.print(',');
         fOut.print(""defaultValue="");
         if (defaultValue == null) {
             fOut.print(""null"");
         }
         else {
             printQuotedString(defaultValue.ch, defaultValue.offset,
                               defaultValue.length);
         }
         fOut.println(')');
         fOut.flush();
 
     } // attributeDecl(String,String,String,String[],String,XMLString)",Buggy,"corrected attributeDecl signature in samples.
Thanks to John Spitzer for pointing that out (bug#6447).


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318206 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message claims to address a bug by correcting the `attributeDecl` method signature. The diff reveals the addition of a new parameter, `XMLString nonNormalizedDefaultValue`, to the method signature. This alteration likely resolves a discrepancy between expected inputs and the method's previous signature, possibly leading to issues during execution if callers expected a different method signature matching this or if the method intended to process additional information that mistakenly was not received before.

Adding this parameter suggests a correction to align with class/interface definitions or expected computational logic, addressing identified functionality inadequacies (as indicated by referencing a bug report in the commit message).

The changes are consistent with bug correction (bug#6447), potentially addressing incorrect method behavior or invocation issues arising from mismatched expectations about parameters. Thus, the update described reflects a genuine bug fix.

Therefore, the changes are appropriately classified as **Buggy**."
xerces2-j,7660.json,c034fce51f8f78ab929b74f4a902deb10034459a,"@@ -1,5 +1,5 @@
     static final Field[] resize(Field[] oldArray, int newSize) {
         Field[] newArray = new Field[newSize];
-        System.arraycopy(oldArray, 0, newArray, 0, newSize);
+        System.arraycopy(oldArray, 0, newArray, 0, oldArray.length);
         return newArray;
     }",Buggy,"Fixing bug 9022: arrayCopy should take oldArray.length, instead of newSize.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318392 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly references a bug (bug 9022) and describes the problem with the `arrayCopy` method. The code was copying too many elements, specified by `newSize`, into the `newArray`. This could lead to out-of-bounds exceptions or incomplete arrays being copied if `newSize` was too small or excessively large. The fix changes the copy length to use `oldArray.length`, ensuring that only the available existing elements in `oldArray` are copied. This aligns with the commit message stating a bug fix. The answer is **Buggy**."
xerces2-j,938.json,0dde63af37b11224e5bc46928596405440cd0107,"@@ -1,15 +1,8 @@
     public Node cloneNode( boolean deep )
     {
         HTMLDocumentImpl    clone;
-        NodeImpl            node;
 
         clone = new HTMLDocumentImpl();
-        if ( deep ) {
-            node = (NodeImpl) getFirstChild();
-            while ( node != null ) {
-                clone.appendChild( clone.importNode( node, true ) );
-                node = (NodeImpl) node.getNextSibling();
-            }
-        }
+        cloneNode(clone, deep);
         return clone;
     }",Buggy,"Fixing JIRA Bug #1021:
http://issues.apache.org/jira/browse/XERCESJ-1021

DocumentType nodes cannot be imported. Instead of attempting to loop
over the children of the Document node we should delegate to the
cloneNode(CoreDocumentImpl,boolean) method on DocumentImpl which
knows how to do the right thing. This should also fix ID assignment
and possibly other things.

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@418366 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that the changes fix a specific JIRA bug regarding the importing of DocumentType nodes. The code diff reveals a shift from a manual deep-copy process using iteration over child nodes to delegation via the cloneNode method. The modified approach likely ensures that the DocumentNode is handled correctly under the hood, addressing issues mentioned in ID assignment and other potential problems.

The change aligns with the commit message by replacing a custom loop mechanism with a method presumed to handle core document issues comprehensively.

Since the commit message and code changes indicate a bug fix concerning node cloning and ensuring correct ID assignments, I conclude **Buggy**."
xerces2-j,8262.json,ef4f12df6ceef58d0976e59c42d72c11836172c8,"@@ -1,4 +1,10 @@
     public void setInputSource(XMLInputSource inputSource) throws IOException {
+        if (inputSource == null) {
+            // no system id was available
+            fDTDHandler.startDTD(null, null);
+            fDTDHandler.endDTD(null);
+            return;
+        }
         fEntityManager.setEntityHandler(this);
         fEntityManager.startDTDEntity(inputSource);
     } // setInputSource(XMLInputSource)",Buggy,"Fixing bug http://nagoya.apache.org/bugzilla/show_bug.cgi?id=11176


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318583 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message references a specific bug ID, indicating a purposeful bug fix. The code change handles a null `inputSource` by calling appropriate methods on `fDTDHandler` and returning immediately, which prevents potential null pointer exceptions and handles an edge case previously unhandled. The presence of error-handling code and safeguarding logic suggests this change corrects a flaw. The conclusion is **Buggy**."
xerces2-j,1373.json,51be532c652cf9a20ca7e950b34b2f00f7dafdd3,"@@ -1,4 +1,4 @@
     public void printText( StringBuffer text )
     {
-        _text.append( text );
+        _text.append( text.toString() );
     }",Buggy,"fixing bug 15768.  According to the javadoc for StringBuffer and String, this is what the original code would have compiled to; this fix simply makes sure that, if compiled under JDK 1.4, Xerces will work with previous JDKs


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318938 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for bug 15768, focusing on compatibility across different JDK versions. The modification in the code changes `_text.append(text)` to `_text.append(text.toString())`. This change ensures that `append` is called with a `String`, aligning behavior across various Java versions, particularly pre-JDK 1.4.

The code change:
- Converts the `StringBuffer` to its `String` representation explicitly with `text.toString()`. 
- Ensures compatibility with JDK versions where method resolution could differ based on whether a `StringBuffer` or `String` argument is passed.

Given the context provided by the commit message and the nature of the code change, it clearly addresses a backward compatibility issue, which can be viewed as a bug in the context of maintaining consistent behavior across different Java environments.

Hence, the changes align with the commit message in addressing a specific issue related to Java version compatibility, and thus this represents a bug fix.

The answer is **Buggy**."
xerces2-j,5333.json,5ba592aa9007de9e5392ebab7095bfd0cfa63824,"@@ -1,13 +1,13 @@
     protected boolean sameBaseURIAsIncludeParent() {
         String parentBaseURI = getIncludeParentBaseURI();
         String baseURI = fCurrentBaseURI.getExpandedSystemId();
         // REVISIT: should we use File#sameFile() ?
         //          I think the benefit of using it is that it resolves host names
         //          instead of just doing a string comparison.
         // TODO: [base URI] is still an open issue with the working group.
         //       They're deciding if xml:base should be added if the [base URI] is different in terms
         //       of resolving relative references, or if it should be added if they are different at all.
         //       Revisit this after a final decision has been made.
         //       The decision also affects whether we output the file name of the URI, or just the path.
-        return parentBaseURI.equals(baseURI);
+        return parentBaseURI != null && parentBaseURI.equals(baseURI);
     }",Buggy,"Partial fix for Bug #24992. Fix NPE which would occur if an 
include parent has no base URI.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319611 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change in this code snippet directly aligns with the commit message. The code alters a method that checks if two URIs are the same. Initially, it directly called `equals` on `parentBaseURI`, which could lead to a NullPointerException (NPE) if `parentBaseURI` was null. The modification adds a null check before the `equals` method call. This change prevents the method from throwing an NPE when `parentBaseURI` is null, exactly as described in the commit message, which mentions fixing an NPE when an include parent has no base URI.

This alteration corrects an error by ensuring the code gracefully handles the scenario where `parentBaseURI` is null, preventing a runtime exception. Therefore, it aligns with the description of a bug fix.

The code and the commit message are consistent in addressing a specific bug concerning handling a null base URI. The fix accounts for error handling, a common aspect of bug fixes.

Based on this analysis, I conclude that this change is **Buggy**."
hibernate-orm,14257.json,8463057b85db928fe8d3aa0b4379bc5b0f255c71,"@@ -1,110 +1,123 @@
 	private void addSetter(ClassFile classfile, final Method[] setters) throws CannotCompileException {
 		ConstPool cp = classfile.getConstPool();
 		int target_type_index = cp.addClassInfo( this.targetBean.getName() );
 		String desc = GET_SETTER_DESC;
 		MethodInfo mi = new MethodInfo( cp, GENERATED_SETTER_NAME, desc );
 
 		Bytecode code = new Bytecode( cp, 4, 6 );
+		StackMapTable stackmap = null;
 		/* | this | bean | args | i | raw bean | exception | */
 		if ( setters.length > 0 ) {
 			int start, end; // required to exception table
 			// iconst_0 // i
 			code.addIconst( 0 );
 			// istore_3 // store i
 			code.addIstore( 3 );
 			// aload_1 // load the bean
 			code.addAload( 1 );
 			// checkcast // cast the bean into a raw bean
 			code.addCheckcast( this.targetBean.getName() );
 			// astore 4 // store the raw bean
 			code.addAstore( 4 );
 			/* current stack len = 0 */
 			// start region to handling exception (BulkAccessorException)
 			start = code.currentPc();
 			int lastIndex = 0;
 			for ( int i = 0; i < setters.length; ++i ) {
 				if ( setters[i] != null ) {
 					int diff = i - lastIndex;
 					if ( diff > 0 ) {
 						// iinc 3, 1
 						code.addOpcode( Opcode.IINC );
 						code.add( 3 );
 						code.add( diff );
 						lastIndex = i;
 					}
 				}
 				/* current stack len = 0 */
 				// aload 4 // load the raw bean
 				code.addAload( 4 );
 				// aload_2 // load the args
 				code.addAload( 2 );
 				// iconst_i
 				code.addIconst( i );
 				// aaload
 				code.addOpcode( Opcode.AALOAD );
 				// checkcast
 				Class[] setterParamTypes = setters[i].getParameterTypes();
 				Class setterParamType = setterParamTypes[0];
 				if ( setterParamType.isPrimitive() ) {
 					// checkcast (case of primitive type)
 					// invokevirtual (case of primitive type)
 					this.addUnwrapper( classfile, code, setterParamType );
 				}
 				else {
 					// checkcast (case of reference type)
 					code.addCheckcast( setterParamType.getName() );
 				}
 				/* current stack len = 2 */
 				String rawSetterMethod_desc = RuntimeSupport.makeDescriptor( setters[i] );
 				if ( !this.targetBean.isInterface() ) {
 					// invokevirtual
 					code.addInvokevirtual( target_type_index, setters[i].getName(), rawSetterMethod_desc );
 				}
 				else {
 					// invokeinterface
 					Class[] params = setters[i].getParameterTypes();
 					int size;
 					if ( params[0].equals( Double.TYPE ) || params[0].equals( Long.TYPE ) ) {
 						size = 3;
 					}
 					else {
 						size = 2;
 					}
 
 					code.addInvokeinterface( target_type_index, setters[i].getName(), rawSetterMethod_desc, size );
 				}
 			}
 
 			// end region to handling exception (BulkAccessorException)
 			end = code.currentPc();
 			// return
 			code.addOpcode( Opcode.RETURN );
 			/* current stack len = 0 */
 			// register in exception table
 			int throwableType_index = cp.addClassInfo( THROWABLE_CLASS_NAME );
-			code.addExceptionHandler( start, end, code.currentPc(), throwableType_index );
+			int handler_pc = code.currentPc();
+			code.addExceptionHandler( start, end, handler_pc, throwableType_index );
 			// astore 5 // store exception
 			code.addAstore( 5 );
 			// new // BulkAccessorException
 			code.addNew( BULKEXCEPTION_CLASS_NAME );
 			// dup
 			code.addOpcode( Opcode.DUP );
 			// aload 5 // load exception
 			code.addAload( 5 );
 			// iload_3 // i
 			code.addIload( 3 );
 			// invokespecial // BulkAccessorException.<init>
 			String cons_desc = ""(Ljava/lang/Throwable;I)V"";
 			code.addInvokespecial( BULKEXCEPTION_CLASS_NAME, MethodInfo.nameInit, cons_desc );
 			// athrow
 			code.addOpcode( Opcode.ATHROW );
+			StackMapTable.Writer writer = new StackMapTable.Writer(32);
+			int[] localTags = { StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.OBJECT, StackMapTable.INTEGER };
+			int[] localData = { cp.getThisClassInfo(), cp.addClassInfo(""java/lang/Object""),
+                        	cp.addClassInfo(""[Ljava/lang/Object;""), 0};
+			int[] stackTags = { StackMapTable.OBJECT };
+			int[] stackData = { throwableType_index };
+			writer.fullFrame(handler_pc, localTags, localData, stackTags, stackData);
+			stackmap = writer.toStackMapTable(cp);
 		}
 		else {
 			// return
 			code.addOpcode( Opcode.RETURN );
 		}
-
-		mi.setCodeAttribute( code.toCodeAttribute() );
+		CodeAttribute ca = code.toCodeAttribute();
+		if (stackmap != null) {
+			ca.setAttribute(stackmap);
+		}
+		mi.setCodeAttribute( ca );
 		mi.setAccessFlags( AccessFlag.PUBLIC );
 		classfile.addMethod( mi );
 	}",Buggy,"HHH-7850 BulkAccessorFactory Java 7 verify error resolved by JASSIST-163 BulkAccessorFactory.java.diff2 patch from Shigeru Chiba.
",Buggy,"The commit message details a bug fix for a Java 7 verification error, citing a specific issue (HHH-7850) linked to a patch (JASSIST-163) from Shigeru Chiba applied to `BulkAccessorFactory`.

Examining the diff:

1. **Bytecode Modifications**: 
   - Code related to exception handling is altered. The `int handler_pc` assignment ensures consistent usage across the handler block.
   - A `StackMapTable` is added to appropriately manage Java's stack map frames, necessary for Java 7's stricter bytecode verification process. It ensures JVM can accurately predict stack behavior during execution and exception handling.
   
2. **Stack Map Table**:
   - A `StackMapTable.Writer` is created and populated with local and stack state data (`localTags`, `localData`, `stackTags`, `stackData`), ensuring compliance with Java 7+ verification requirements.
   - If `stackmap` is not `null`, it's set as an attribute on the `CodeAttribute`, maintaining compatibility with Java 7's bytecode verification needs.

The changes effectively address Java 7's stricter verification processes, resolving the verify error issue stated in the commit message. The modifications align with the message and suggest a bug fix due to the correction of bytecode verifiability. Thus, the conclusion is **Buggy**."
hibernate-orm,15873.json,8fb35947ff6525a8286295f2959cd5a71502b2f9,"@@ -1,3 +1,3 @@
 	public String getQuerySequencesString() {
-		return ""select table_name from information_schema.TABLES where table_type='SEQUENCE'"";
+		return ""select table_name from information_schema.TABLES where table_schema = database() and table_type = 'SEQUENCE'"";
 	}",Buggy,"HHH-13373 fix the bug that 'different sequence names in differnt dbs' breaks SequenceInformationExtractorMariaDBDatabaseImpl
",Buggy,"The commit message refers to a bug fix regarding sequence names in different databases affecting `SequenceInformationExtractorMariaDBDatabaseImpl`. The diff updates the SQL query in `getQuerySequencesString()` method to include `table_schema = database()`. 

Explanation:
- Original SQL query retrieves sequence names from `information_schema.TABLES` without considering the database/schema context. This could return sequences from all schemas, leading to conflicts or errors when sequences with the same name exist in different schemas.
- Updated query adds a condition to ensure sequences are only selected from the current database, using `table_schema = database()`. This change limits the scope to the current schema, preventing issues arising from identical sequence names across different databases.

This modification aligns with the commit message, which describes a fix related to handling sequence names specific to different databases. This is clearly a bug fix as it corrects the erroneous behavior in querying sequences.

Thus, the conclusion is **Buggy**."
hibernate-orm,15816.json,fc6312a73b99c8edf2af32122165cc8b31b79aa7,"@@ -1,3 +1,8 @@
 	public String getQuerySequencesString() {
-		return null ;
+		if ( supportsSequences() ) {
+			return ""select SEQUENCENAME from SYS.SYSSEQUENCES"";
+		}
+		else {
+			throw new MappingException( ""Derby does not support sequence prior to release 10.6.1.0"" );
+		}
 	}",Buggy,"HHH-10110 - Fix DerbyDialect#getQuerySequencesString() causing error during schema update
",Buggy,"The commit message states a fix for an error in `DerbyDialect#getQuerySequencesString()` that occurs during schema updates. Here's the analysis of the changes:

1. **supportsSequences Check**: The updated code introduces a check for `supportsSequences()`, which likely returns a boolean indicating whether the current Derby version supports sequences. This aligns with the message specifying a fix for sequence handling errors.

2. **Query Execution**: If sequences are supported, the method now returns a SQL query string to select sequence names from `SYS.SYSSEQUENCES`. This seems like a corrective measure to ensure the correct query is returned when sequences are available.

3. **Exception Handling**: If sequences are not supported, the code throws a `MappingException`. This exception includes a clear message about the lack of sequence support in Derby versions prior to 10.6.1.0. This exception handling prevents attempts to query sequences on unsupported versions, likely contributing to resolving the error mentioned in the commit.

These changes strategically address the problem in `getQuerySequencesString` by returning the appropriate query string or raising an informed exception when sequences aren't supported. Both align with the commit message, indicating these are indeed bug fixes to prevent errors during schema updates.

Given these are corrections to prevent and handle erroneous behavior in specific versions, the answer is **Buggy**."
hibernate-orm,22027.json,b33c6d435780a96d6f911fbf8a9d796f10c7af0d,"@@ -1,15 +1,17 @@
-	public void onLoad(final LoadEvent event,
-					   final LoadEventListener.LoadType loadType) throws HibernateException {
+	public void onLoad(
+			final LoadEvent event,
+			final LoadEventListener.LoadType loadType) throws HibernateException {
 
 		final EntityPersister persister = getPersister( event );
 
 		if ( persister == null ) {
 			throw new HibernateException( ""Unable to locate persister: "" + event.getEntityClassName() );
 		}
 
 		final Class idClass = persister.getIdentifierType().getReturnedClass();
-		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) )
+		if ( idClass != null && !idClass.isInstance( event.getEntityId() ) ) {
 			checkIdClass( persister, event, loadType, idClass );
+		}
 
 		doOnLoad( persister, event, loadType );
 	}",Buggy,"HHH-10366 - Fix checkstyle error
",NotBuggy,"The commit message indicates a fix for a checkstyle error, which typically involves code formatting or style rather than functional changes. The provided diff modifies the code formatting by adjusting the method parameter alignment and adding braces to an `if` statement. These changes enhance readability and align with coding standards but do not change the underlying logic or address any functional bugs.

Since the intent and nature of the changes match the commit message focused on style, and there's no indication of a bug fix related to functionality, I conclude that the changes are **NotBuggy**."
hibernate-orm,15530.json,220365600cd7692ede1977d366966a5828dc749c,"@@ -1,7 +1,14 @@
 	public String getProcessedSql() {
-		boolean useLimitOffset = supportsLimit() && supportsLimitOffset()
-				&& LimitHelper.hasFirstRow( selection ) && LimitHelper.hasMaxRows( selection );
-		return dialect.getLimitString(
-				sql, useLimitOffset ? LimitHelper.getFirstRow( selection ) : 0, getMaxOrLimit()
-		);
+		if (LimitHelper.useLimit(this, selection)) {
+			// useLimitOffset: whether ""offset"" is set or not;
+			// if set, use ""LIMIT offset, row_count"" syntax;
+			// if not, use ""LIMIT row_count""
+			boolean useLimitOffset = LimitHelper.hasFirstRow(selection);
+
+			return new StringBuilder(sql.length() + 20).append(sql)
+							.append(useLimitOffset ? "" limit ?, ?"" : "" limit ?"").toString();
+		}
+		else {
+			return sql; // or return unaltered SQL
+		}
 	}",Buggy,"HHH-7716 Fixed a bug in CUBRIDLimitHandler. Now correctly processes
LIMIT
clause
in SQL.
",Buggy,"The commit message indicates a bug fix in the CUBRIDLimitHandler related to processing the LIMIT clause in SQL statements. The code changes support this claim through several modifications:

1. **Improved Logic**: The new implementation introduces `LimitHelper.useLimit(this, selection)` to determine whether or not to apply a LIMIT clause. This potentially encapsulates more logic than the previous approach, which simply checked for both `supportsLimit()` and `supportsLimitOffset()` within the method.

2. **OFFSET Handling**: The updated code explicitly checks for OFFSET with `LimitHelper.hasFirstRow(selection)` to decide on the syntax `LIMIT offset, row_count` versus `LIMIT row_count`. This distinction was less transparent in the original code.

3. **StringBuilder for SQL Construction**: Instead of using `dialect.getLimitString` with various parameters, the new code constructs the SQL statement using a `StringBuilder`. This directly appends the appropriate LIMIT syntax to the SQL. This not only improves clarity but may also provide minor performance benefits.

4. **Return Unaltered SQL**: The condition to return the unaltered SQL string (`sql`) when limits are not used is made explicit. This safeguards against inadvertently modifying SQL that doesn't need a limit, aligning with the claim of correctly processing the LIMIT clause.

These changes align well with the commit message as they address a specific bug related to how LIMIT clauses are constructed and applied in SQL. The updated logic and condition handling ensure that LIMIT clauses are correctly appended or not, adhering to SQL syntax requirements.

Therefore, the changes in the diff indeed indicate a bug fix relevant to the commit message, leading to the conclusion of **Buggy**."
hibernate-orm,26062.json,6a14aeeba79d3536a893f63154c0b697db3f3711,"@@ -1,13 +1,15 @@
 	private void processExpression(ReadWriteExpression annotation) {
 		String nonNullLogicalColumnName = logicalColumnName != null ? logicalColumnName : """"; //use the default for annotations 
-		if ( annotation != null && annotation.forColumn().equals( nonNullLogicalColumnName ) ) {
+		if ( annotation != null &&
+				( StringHelper.isEmpty( annotation.forColumn() )
+						|| annotation.forColumn().equals( nonNullLogicalColumnName ) ) ) {
 			readExpression = annotation.read();
 			if ( StringHelper.isEmpty( readExpression ) ) {
 				readExpression = null;
 			}
 			writeExpression = annotation.write();
 			if ( StringHelper.isEmpty( writeExpression ) ) {
 				writeExpression = null;
 			}
 		}
 	}",Buggy,"HHH-4510 Fix bug where explicit @Column would not match @ReadWriteExpresion with empty forColumn

git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@20747 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The code change modifies the `processExpression` method to handle cases where the `@ReadWriteExpression` annotation's `forColumn` is an empty string. The original logic required `annotation.forColumn()` to exactly match `nonNullLogicalColumnName`, which could lead to a mismatch if `forColumn` was left empty, intending to match any column or use a default behavior.

Here's a step-by-step breakdown of the change:

1. **Original Logic**: The condition only passes if the `forColumn()` exactly equals `nonNullLogicalColumnName`.

2. **Updated Logic**: 
   - It first checks if `annotation.forColumn()` is empty using `StringHelper.isEmpty`.
   - If `forColumn()` is empty, the condition passes immediately.
   - Otherwise, it checks for equality with `nonNullLogicalColumnName`.

3. **Purpose of Change**: This update allows `@ReadWriteExpression` annotations with an empty `forColumn` to apply broadly when no specific column match is required, thus resolving a bug where the annotations would not correctly apply in such scenarios.

4. **Relation to Commit Message**: The commit message explicitly states that it fixes a bug regarding the `@Column` not matching `@ReadWriteExpression` with an empty `forColumn`. The code change directly addresses this issue by modifying the condition to handle the empty string scenario.

Given this context, the changes in the code align with the commit message and indeed fix a bug described in the commit message. Hence, the correct classification of this commit is **Buggy**."
hibernate-orm,4331.json,f5e10c29ebf103df4de7ea7b528702f0d2d9fc2a,"@@ -1,9 +1,18 @@
 	public void destroy() {
 		if ( !active ) {
 			return;
 		}
 		active = false;
 		destroy( classLoaderServiceBinding );
 		destroy( strategySelectorBinding );
 		destroy( integratorServiceBinding );
+
+		if ( childRegistries != null ) {
+			for(ServiceRegistry serviceRegistry : childRegistries) {
+				if(serviceRegistry instanceof ServiceRegistryImplementor) {
+					ServiceRegistryImplementor serviceRegistryImplementor = (ServiceRegistryImplementor) serviceRegistry;
+					serviceRegistryImplementor.destroy();
+				}
+			}
+		}
 	}",Buggy,"HHH-10907 - Fix connection leak problem in hibernate-core tests
",Buggy,"The commit message states that this change addresses a connection leak in hibernate-core tests. In the diff, the destroy method is enhanced to also iterate over childRegistries and invoke destroy on each, provided it is an instance of ServiceRegistryImplementor. This addition suggests that previously, these child registries might have been left undisposed, leading to potential resource leaks. The changes involve ensuring all resources are appropriately released, directly addressing the issue of resource leakage, which is typically associated with connection leaks. 

Given the context and the nature of the changes—introducing logic to ensure thorough cleanup—the commit aligns well with the message indicating a fix for a connection leak. Therefore, the conclusion is **Buggy**."
hibernate-orm,3407.json,8b9f171a034f7604853d4c4bc1ffa78a8e2991fe,"@@ -1,54 +1,55 @@
 	private void cleanseUniqueKeyMap() {
 		// We need to account for a few conditions here...
 		// 	1) If there are multiple unique keys contained in the uniqueKeys Map, we need to deduplicate
 		// 		any sharing the same columns as other defined unique keys; this is needed for the annotation
 		// 		processor since it creates unique constraints automagically for the user
 		//	2) Remove any unique keys that share the same columns as the primary key; again, this is
 		//		needed for the annotation processor to handle @Id @OneToOne cases.  In such cases the
 		//		unique key is unnecessary because a primary key is already unique by definition.  We handle
 		//		this case specifically because some databases fail if you try to apply a unique key to
 		//		the primary key columns which causes schema export to fail in these cases.
 		if ( uniqueKeys.isEmpty() ) {
 			// nothing to do
 			return;
 		}
 		else if ( uniqueKeys.size() == 1 ) {
 			// we have to worry about condition 2 above, but not condition 1
 			final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeys.entrySet().iterator().next();
 			if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 				uniqueKeys.remove( uniqueKeyEntry.getKey() );
 			}
 		}
 		else {
 			// we have to check both conditions 1 and 2
 			final Iterator<Map.Entry<String,UniqueKey>> uniqueKeyEntries = uniqueKeys.entrySet().iterator();
 			while ( uniqueKeyEntries.hasNext() ) {
 				final Map.Entry<String,UniqueKey> uniqueKeyEntry = uniqueKeyEntries.next();
 				final UniqueKey uniqueKey = uniqueKeyEntry.getValue();
 				boolean removeIt = false;
 
 				// condition 1 : check against other unique keys
 				for ( UniqueKey otherUniqueKey : uniqueKeys.values() ) {
 					// make sure its not the same unique key
 					if ( uniqueKeyEntry.getValue() == otherUniqueKey ) {
 						continue;
 					}
 					if ( otherUniqueKey.getColumns().containsAll( uniqueKey.getColumns() )
 							&& uniqueKey.getColumns().containsAll( otherUniqueKey.getColumns() ) ) {
 						removeIt = true;
 						break;
 					}
 				}
 
 				// condition 2 : check against pk
 				if ( isSameAsPrimaryKeyColumns( uniqueKeyEntry.getValue() ) ) {
 					removeIt = true;
 				}
 
 				if ( removeIt ) {
-					uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					//uniqueKeys.remove( uniqueKeyEntry.getKey() );
+					uniqueKeyEntries.remove();
 				}
 			}
 
 		}
 	}",Buggy,"HHH-7446 bug-fix
",Buggy,"The commit changes how unique keys are removed from `uniqueKeys`. Originally, `uniqueKeys.remove()` within an iteration could cause `ConcurrentModificationException`. Switching to `uniqueKeyEntries.remove()` removes via iterator, avoiding exceptions. This change addresses an error-prone logic and is a typical bug fix in Java collections manipulation. Thus, the changes align with the commit message, purposefully correcting behavior that would lead to a runtime error.

Therefore, the appropriate conclusion here is **Buggy**."
hibernate-orm,29945.json,8e6fcce523698018a1e9952a8cf3a78485458ac7,"@@ -1,25 +1,25 @@
 	private void internalSetValue(T value) {
 		if ( procedureParameter.getMode() != ParameterMode.IN && procedureParameter.getMode() != ParameterMode.INOUT ) {
 			throw new IllegalStateException( ""Can only bind values for IN/INOUT parameters : "" + procedureParameter );
 		}
 
 		if ( procedureParameter.getParameterType() != null ) {
 			if ( value == null ) {
 				if ( !procedureParameter.isPassNullsEnabled() ) {
-					throw new IllegalArgumentException( ""The parameter with the ["" +
+					throw new IllegalArgumentException( ""The parameter "" +
 							( procedureParameter.getName() != null
-									? procedureParameter.getName() + ""] name""
-									: procedureParameter.getPosition() + ""] position"" )
+									? ""named ["" + procedureParameter.getName() + ""]""
+									: ""at position ["" + procedureParameter.getPosition() + ""]"" )
 							+ "" was null. You need to call ParameterRegistration#enablePassingNulls(true) in order to pass null parameters."" );
 				}
 			}
 			else if ( !procedureParameter.getParameterType().isInstance( value ) &&
 					!procedureParameter.getHibernateType().getReturnedClass().isInstance( value ) ) {
 				throw new IllegalArgumentException( ""Bind value ["" + value + ""] was not of specified type ["" + procedureParameter
 						.getParameterType() );
 			}
 		}
 
 		this.value = value;
 		this.isBound = true;
 	}",Buggy,"HHH-12905 Improve the error message and update the tests accordingly

Also fix a loose end in the MySQL test: at least with MariaDB, using
a bit(1) as datatype for boolean does not work: it always return true
even if you set it to 0. Using either boolean or tinyint(1) solves
the issue.

As I'm not sure older versions of MySQL supports a real boolean type I
used a tinyint(1).
",NotBuggy,"The commit message indicates an improvement to the error message and changes regarding MySQL data types, but this diff only relates to the error message formatting.

1. **Error Message Improvement**: The format change in the `IllegalArgumentException` message makes it clearer by explicitly stating whether the parameter is named or positional and uses consistent brackets. This aligns with the first part of the commit message focusing on better error messages.

2. **Bug Fix Indication**: Adjusting an error message typically doesn't fix a bug in the functional sense. It improves usability and debugging ease but doesn't change the program's correctness or handle an incorrect state caused by a bug.

3. **No MySQL Data Type Change in the Diff**: The diff doesn't show changes to data types or usage of `tinyint(1)`, as mentioned in the commit message. Hence, that part isn't related to the code changes presented.

The commit message and the code changes align in terms of error message improvement but don't indicate a functional or logical bug fix. The error message enhancement is more about developer experience rather than fixing erroneous behavior.

The final assessment is **NotBuggy** since the diff shows no logical or error-handling corrections that would indicate fixing a bug."
hibernate-orm,9074.json,50b788266339ac7f22f2f43dc011378e7b0b0aad,"@@ -1,33 +1,33 @@
 	private EnumValueMapper interpretParameters(Properties parameters) {
 		if ( parameters.containsKey( NAMED ) ) {
 			final boolean useNamed = ConfigurationHelper.getBoolean( NAMED, parameters );
 			if ( useNamed ) {
 				return new NamedEnumValueMapper();
 			}
 			else {
 				return new OrdinalEnumValueMapper();
 			}
 		}
 
 		if ( parameters.containsKey( TYPE ) ) {
 			final int type = Integer.decode( (String) parameters.get( TYPE ) );
 			if ( isNumericType( type ) ) {
 				return new OrdinalEnumValueMapper();
 			}
 			else if ( isCharacterType( type ) ) {
-				return new OrdinalEnumValueMapper();
+				return new NamedEnumValueMapper();
 			}
 			else {
 				throw new HibernateException(
 						String.format(
 								Locale.ENGLISH,
 								""Passed JDBC type code [%s] not recognized as numeric nor character"",
 								type
 						)
 				);
 			}
 		}
 
 		// the fallback
 		return new OrdinalEnumValueMapper();
 	}",Buggy,"HHH-10766 Resolve mapping 'type' parameter error
",Buggy,"The diff shows a change in behavior for handling the 'type' parameter. Specifically, when `isCharacterType(type)` is true, the code now returns `NamedEnumValueMapper()` instead of `OrdinalEnumValueMapper()`. This implies that there was an error in the mapping logic where a character type was incorrectly mapped to an ordinal value mapper. The commit message ""Resolve mapping 'type' parameter error"" suggests this logical change is indeed a correction of a past mistake in parameter handling.

Here's a step-by-step analysis:

1. **Existing Structure**: The function returns a specific `EnumValueMapper` based on the parameters.
2. **Parameter Check**: It checks if the `parameters` contain specific keys (`NAMED` and `TYPE`).
3. **Behavior for `NAMED`**: If `NAMED` is true, `NamedEnumValueMapper` is used; otherwise, `OrdinalEnumValueMapper`.
4. **Handling `TYPE` Parameter**:
   - When `isNumericType(type)` is true, `OrdinalEnumValueMapper` is returned.
   - Previously, even if `isCharacterType(type)` was true, `OrdinalEnumValueMapper` was returned, which likely caused a mapping error.
5. **Correction**: The updated code correctly assigns `NamedEnumValueMapper` for character types, aligning behavior with expected mapping logic for enums based on type.

This change is a clear logical correction to ensure appropriate handling of the 'type' parameter, specifically addressing the incorrect mapping when a character type is given. Thus, the commit message aligns well with the changes in the diff.

Therefore, the commit fixes a bug related to parameter mapping, and the changes indicate a bug fix. The conclusion is **Buggy**."
presto,26733.json,d9c1e2084c70835d5feecf645ed8836064809fc2,"@@ -1,18 +1,18 @@
     private void startDiscovery()
     {
         discoveryService.scheduleWithFixedDelay(() -> {
             try {
                 // jitter to avoid overloading database and overloading the backup store
-                SECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
+                MILLISECONDS.sleep(ThreadLocalRandom.current().nextLong(1, organizationDiscoveryIntervalMillis));
 
                 log.info(""Running shard organizer..."");
                 submitJobs(discoverAndInitializeTablesToOrganize());
             }
             catch (InterruptedException e) {
                 Thread.currentThread().interrupt();
             }
             catch (Throwable t) {
                 log.error(t, ""Error running shard organizer"");
             }
         }, 0, organizationDiscoveryIntervalMillis, TimeUnit.MILLISECONDS);
     }",Buggy,"fix organizationDiscoveryIntervalMillis bug
",Buggy,"The code change updates the time unit used for sleeping in a randomly generated interval from seconds to milliseconds, which aligns with the variable `organizationDiscoveryIntervalMillis`. Using seconds in the original code means that intervals intended to be milliseconds would be incorrectly interpreted as seconds, potentially causing longer delays than intended. This adjustment corrects the interpretation of the interval length, fixing a bug in the timing logic.

Since the change directly addresses the timing issue, the modification is coherent with the commit message, and the change indeed fixes a bug related to how the interval was previously being processed.

The final assessment is **Buggy**."
presto,16973.json,83ee1b402bab99302d7c271fcd5dc8a560c027dc,"@@ -1,14 +1,14 @@
     private void loadCatalog(File file)
             throws Exception
     {
         log.info(""-- Loading catalog %s --"", file);
         Map<String, String> properties = new HashMap<>(loadProperties(file));
 
         String connectorName = properties.remove(""connector.name"");
-        checkState(connectorName != null, ""Catalog configuration %s does not contain conector.name"", file.getAbsoluteFile());
+        checkState(connectorName != null, ""Catalog configuration %s does not contain connector.name"", file.getAbsoluteFile());
 
         String catalogName = Files.getNameWithoutExtension(file.getName());
 
         connectorManager.createConnection(catalogName, connectorName, ImmutableMap.copyOf(properties));
         log.info(""-- Added catalog %s using connector %s --"", catalogName, connectorName);
     }",Buggy,"Fix typo in error message about connector.name
",NotBuggy,"The commit message indicates a fix for a typo in an error message regarding `connector.name`. The code change corrects ""conector.name"" to ""connector.name"", aligning perfectly with the stated intent of the commit, which is primarily cosmetic rather than functional. There are no changes related to logic or error handling that suggest a bug fix beyond correcting the typo.

Since the alteration only involves text in an error message and doesn't impact code functionality related to error detection or handling, it does not indicate a bug fix in terms of software behavior.

Therefore, the conclusion is **NotBuggy**."
presto,14423.json,8c469bda0657f41c146796ea2ccaaea6dfa552e6,"@@ -1,4 +1,4 @@
     public static boolean isPrefixSubnetOf(@SqlType(StandardTypes.IPPREFIX) Slice first, @SqlType(StandardTypes.IPPREFIX) Slice second)
     {
-        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first));
+        return between(ipSubnetMin(second), ipSubnetMin(first), ipSubnetMax(first)) && between(ipSubnetMax(second), ipSubnetMin(first), ipSubnetMax(first));
     }",Buggy,"Fix bug in IS_SUBNET_OF function
",Buggy,"The code change involves modifying the `isPrefixSubnetOf` function by adding an additional condition to the `return` statement. Initially, the function checked if `ipSubnetMin(first)` was between `ipSubnetMin(second)` and `ipSubnetMax(first)`. However, the added condition also checks if `ipSubnetMax(second)` is between `ipSubnetMin(first)` and `ipSubnetMax(first)`.

Analyzing this change:
1. **Logical Correction**: The change ensures that not only the minimum boundary of the second subnet is within the first subnet, but also the maximum boundary of the second subnet is within the first. This is crucial for determining if one subnet is wholly contained within another, which aligns with the concept of one subnet being ""a subnet of"" another.

2. **Bug-Related**: The modification suggests that a logical error existed in the original function's implementation. Previously, it might have inaccurately determined subnet containment by only considering one boundary. This fix ensures both boundaries are considered, correcting the logical oversight.

3. **Commit Message Alignment**: The commit message explicitly states ""Fix bug in IS_SUBNET_OF function"", and the code change directly pertains to this function with an improvement that resolves a logical issue in subnet evaluation.

Given these points, it's evident that the commit addresses a bug related to the incorrect logical evaluation of subnets. Therefore, the conclusion is **Buggy**."
presto,8688.json,97566626dd63db55ee08b48b3e366d6d9453badb,"@@ -1,39 +1,39 @@
     private SymbolStatsEstimate normalizeSymbolStats(Symbol symbol, SymbolStatsEstimate symbolStats, PlanNodeStatsEstimate stats, TypeProvider types)
     {
         if (symbolStats.isUnknown()) {
             return SymbolStatsEstimate.unknown();
         }
 
         double outputRowCount = stats.getOutputRowCount();
         double distinctValuesCount = symbolStats.getDistinctValuesCount();
         double nullsFraction = symbolStats.getNullsFraction();
 
         if (!isNaN(distinctValuesCount)) {
-            Type type = requireNonNull(types.get(symbol), () -> ""No stats for symbol "" + symbol);
+            Type type = requireNonNull(types.get(symbol), () -> ""type is missing for symbol "" + symbol);
             double maxDistinctValuesByLowHigh = maxDistinctValuesByLowHigh(symbolStats, type);
             if (distinctValuesCount > maxDistinctValuesByLowHigh) {
                 distinctValuesCount = maxDistinctValuesByLowHigh;
             }
 
             if (distinctValuesCount > outputRowCount) {
                 distinctValuesCount = outputRowCount;
             }
 
             double nonNullValues = outputRowCount * (1 - nullsFraction);
             if (distinctValuesCount > nonNullValues) {
                 double difference = distinctValuesCount - nonNullValues;
                 distinctValuesCount -= difference / 2;
                 nonNullValues += difference / 2;
                 nullsFraction = 1 - nonNullValues / outputRowCount;
             }
         }
 
         if (distinctValuesCount == 0.0) {
             return SymbolStatsEstimate.zero();
         }
 
         return SymbolStatsEstimate.buildFrom(symbolStats)
                 .setDistinctValuesCount(distinctValuesCount)
                 .setNullsFraction(nullsFraction)
                 .build();
     }",Buggy,"Fix error message in StatsNormalizer
",NotBuggy,"The commit message indicates a fix for an error message in the `StatsNormalizer`. The change involved updates a lambda expression's error message when requiring a non-null type. The update changes the message from ""No stats for symbol"" to ""type is missing for symbol"", which clarifies the error context.

The code change affects the error message generation but does not alter the program's behavior or logic. It's primarily an improvement in error reporting for developers diagnosing issues.

Since the change improves the accuracy of the error message but doesn't alter functionality or fix incorrect logic, this is more related to error message clarity rather than a bug fix in the code logic or behavior itself.

Hence, the determination based on the provided information is **NotBuggy**."
presto,26619.json,3325ab0064b5fd93af18c1d5e87170f5d66bae28,"@@ -1,10 +1,7 @@
     public void deleteRows(Block rowIds)
     {
-        if (rowsToDelete == null) {
-            rowsToDelete = new BitSet(Ints.checkedCast(recordReader.getFileRowCount()));
-        }
         for (int i = 0; i < rowIds.getPositionCount(); i++) {
             long rowId = BIGINT.getLong(rowIds, i);
             rowsToDelete.set(Ints.checkedCast(rowId));
         }
     }",Buggy,"Fix bug in delete for Raptor

When no rows match for delete, rowsToDelete is not set causing a NPE.
Set it in the constructor because we already know the maximum size.
We might over allocate for cases where no rows are selected for
deletion, but this is a short lived bit vector so it should be low
impact.
",Buggy,"The commit message explains a bug where `rowsToDelete` is not initialized in certain cases, leading to a `NullPointerException`. The fix involves removing the lazy initialization of `rowsToDelete` within the `deleteRows` method. Although the exact initialization isn't shown here, the message indicates it's initialized in the constructor instead. By preemptively allocating the `BitSet`, any potential `NullPointerException` during the use of `rowsToDelete` is avoided.

The relevance between the code changes and the commit message is clear—they both address a bug related to the initialization of `rowsToDelete`. Furthermore, the error described (a potential `NullPointerException`) is a typical bug. Given the above analysis, the changes indicate a bug fix as explained in the commit message.

Therefore, the answer is **Buggy**."
presto,14862.json,9adc687000043241a0c0ecb1b56ef6b810c2a430,"@@ -1,23 +1,26 @@
     public static Block bigintDistinct(@SqlType(""array(bigint)"") Block array)
     {
         if (array.getPositionCount() == 0) {
             return array;
         }
 
         boolean containsNull = false;
         LongSet set = new LongOpenHashSet(array.getPositionCount());
         BlockBuilder distinctElementBlockBuilder = BIGINT.createBlockBuilder(new BlockBuilderStatus(), array.getPositionCount());
         for (int i = 0; i < array.getPositionCount(); i++) {
-            if (!containsNull && array.isNull(i)) {
-                containsNull = true;
-                distinctElementBlockBuilder.appendNull();
+            if (array.isNull(i)) {
+                if (!containsNull) {
+                    containsNull = true;
+                    distinctElementBlockBuilder.appendNull();
+                }
                 continue;
             }
+
             long value = BIGINT.getLong(array, i);
             if (set.add(value)) {
                 BIGINT.writeLong(distinctElementBlockBuilder, value);
             }
         }
 
         return distinctElementBlockBuilder.build();
     }",Buggy,"Fix null-handling bug in ArrayDistinctFunction
",Buggy,"The commit message states that a null-handling bug in `ArrayDistinctFunction` is being fixed. The code change involves modifying how null values in an array are processed. 

1. **Original Code**: 
   - The code initially checks if `array.isNull(i)` and, if so, sets `containsNull` to `true` and appends `null` to the `distinctElementBlockBuilder`.
   - Once a null is encountered, it continues to the next iteration.

2. **Modified Code**: 
   - It now first checks if `array.isNull(i)`.
   - Only if `containsNull` is `false`, it sets `containsNull` to `true` and appends `null` to `distinctElementBlockBuilder`.
   - This prevents adding multiple null values by continuing immediately after handling the first null.

By rearranging the null-checking logic and only permitting a single `null` in the distinct results, the update effectively resolves an error where multiple nulls could be erroneously recorded. This aligns with the commit message indicating a fix for null handling.

The changes directly address a potential logical bug regarding null handling in the `bigintDistinct` function, thus making the code changes a clear bug fix. Therefore, the conclusion is **Buggy**."
presto,20594.json,2d9e768a03b5c9d804825c9a489383465f373801,"@@ -1,8 +1,10 @@
         public PlanWithProperties visitFilter(FilterNode node, PreferredProperties preferredProperties)
         {
-            if (node.getSource() instanceof TableScanNode) {
+            if (node.getSource() instanceof TableScanNode && metadata.isLegacyGetLayoutSupported(session, ((TableScanNode) node.getSource()).getTable())) {
+                // If isLegacyGetLayoutSupported, then we can continue with legacy predicate pushdown logic.
+                // Otherwise, we leave the filter as is in the plan as it will be pushed into the TableScan by filter pushdown logic in the connector.
                 return planTableScan((TableScanNode) node.getSource(), node.getPredicate());
             }
 
             return rebaseAndDeriveProperties(node, planChild(node, preferredProperties));
         }",Buggy,"Fix bug AddExchanges dropping filter when pushdown is enabled
",Buggy,"The code diff updates the logic in `visitFilter` by adding a condition that checks `isLegacyGetLayoutSupported` before pushing down the filter. This indicates a fix for cases where filters were improperly dropped due to the absence of legacy layout support, causing incorrect query plans when pushdown was enabled. This aligns with the commit message describing a bug fix concerning filters being dropped. The answer is **Buggy**."
presto,30019.json,9e755c76e9d8e3a5e0a486002b3898be9d6d5d25,"@@ -1,14 +1,13 @@
     public void close()
     {
         closed = true;
-
         // use try with resources to close everything properly
-        try (ResultSet resultSet = this.resultSet;
+        try (Connection connection = this.connection;
                 Statement statement = this.statement;
-                Connection connection = this.connection) {
+                ResultSet resultSet = this.resultSet) {
             // do nothing
         }
         catch (SQLException e) {
             throw Throwables.propagate(e);
         }
     }",Buggy,"Fix ""Connection already closed"" error in JdbcRecordCursor

The resultset, statement and connection were being closed in
the wrong order.
",Buggy,"The commit message indicates a fix for a ""Connection already closed"" error by rearranging the order of closing resources. The diff shows that the order of `try-with-resources` has indeed been updated. The original order had the `ResultSet` closed before `Statement` and `Connection`, potentially leading to the error stated in the commit message since some database resources need them closed in a specific order. The revised order initializes the `Connection` first and closes it last, which is the correct order to avoid the error.

The connection should remain open until the `ResultSet` and `Statement` have completed their operations. By correcting the order, this change directly addresses the ""Connection already closed"" issue mentioned in the commit message. Thus, it aligns perfectly with the description provided.

Therefore, the changes in the diff do reflect a bug fix as described in the commit message. The answer is **Buggy**."
presto,32288.json,1eb20ea3faeb66b18c4c9300b4a6c8c3190ca6ce,"@@ -1,8 +1,9 @@
     public List<SchemaTableName> listTables(ConnectorSession session, String schemaNameOrNull)
     {
-        checkArgument(schemaNameOrNull == null || schemaNameOrNull.equals(SCHEMA_NAME),
-                ""Only '%s' schema is supported"", SCHEMA_NAME);
+        if (schemaNameOrNull != null && !schemaNameOrNull.equals(SCHEMA_NAME)) {
+            return ImmutableList.of();
+        }
         return tables.values().stream()
                 .map(BlackHoleTableHandle::toSchemaTableName)
                 .collect(toList());
     }",Buggy,"Fix listing tables in blackhole connector

Listing tables in a non-existent schema should not be an error.
",Buggy,"The commit message states that listing tables in a non-existent schema should not result in an error. The original code enforces that the schema name must either be null or exactly equal to `SCHEMA_NAME`; otherwise, it throws an error via `checkArgument`. 

In the changed code, if the schema name is not `null` and does not match `SCHEMA_NAME`, it returns an empty list instead of throwing an error. This aligns with the commit message indicating that attempting to list tables in a non-existent schema should not be treated as an error but should simply return no tables.

This change corrects the behavior when listing tables in non-existent schemas, which is a logical correction, indicative of a bug fix.

Consequently, the alignment between the commit message and the code changes strongly suggests the change fixes an intended behavior that was originally treated incorrectly. Therefore, the conclusion is **Buggy**."
presto,32344.json,5bc9087c740ce60796c3b92131fe7c2a09bc04f7,"@@ -1,4 +1,10 @@
     public List<File> getFiles(SchemaTableName table)
     {
-        return cachedFiles.getUnchecked(table);
+        try {
+            return cachedFiles.getUnchecked(table);
+        }
+        catch (UncheckedExecutionException e) {
+            throwIfInstanceOf(e.getCause(), PrestoException.class);
+            throw e;
+        }
     }",Buggy,"Fix error categorization in local file connector

Exceptions are wrapped by LoadingCache and must be unwrapped.
",Buggy,"The given commit message states ""Fix error categorization in local file connector,"" specifically addressing the need to unwrap exceptions that are wrapped by `LoadingCache`. The code modification reflects this statement.

Here's the step-by-step analysis:

1. **Original Code:**
   - The original code directly returns the result of `cachedFiles.getUnchecked(table)`.
   - This method could throw an `UncheckedExecutionException` if an exception occurs during the cache loading.

2. **Modified Code:**
   - The revised code now includes a `try-catch` block surrounding the call to `cachedFiles.getUnchecked(table)`.
   - If an `UncheckedExecutionException` is caught, the cause of this exception is checked.
   - If the cause is an instance of `PrestoException`, it rethrows the cause itself using `throwIfInstanceOf`, else it rethrows the original wrapped exception.

3. **Error Handling:**
   - The `throwIfInstanceOf(e.getCause(), PrestoException.class);` call is a mechanism to unwrap the cause and throw it if it's a specific exception (`PrestoException` here), allowing more precise error handling.

4. **Alignment with Commit Message:**
   - The commit message explains that error categorization has been improved by unwrapping the `LoadingCache` exceptions.
   - The code changes show exactly this: the `UncheckedExecutionException` is unwrapped to properly handle and categorize the underlying exception (`PrestoException`).

This change reflects a bug fix because it corrects how exceptions are propagated and categorized, preventing the potential mishandling of specific exception types wrapped by `LoadingCache`.

Therefore, the changes in the code match the commit message's explanation and indicate a bug fix in error handling. Thus, the conclusion is **Buggy**."
presto,28248.json,dce842fd5d590740f0f7ca26ec5c7eb192e64640,"@@ -1,11 +1,16 @@
         protected Node visitNegativeExpression(NegativeExpression node, Context<C> context)
         {
             if (!context.isDefaultRewrite()) {
                 Node result = nodeRewriter.rewriteNegativeExpression(node, context.get(), TreeRewriter.this);
                 if (result != null) {
                     return result;
                 }
             }
 
+            Expression child = rewrite(node.getValue(), context.get());
+            if (child != node.getValue()) {
+                return new NegativeExpression(child);
+            }
+
             return node;
         }",Buggy,"Fix bug when rewriting NegativeExpression

It was discarding the rewritten subexpression of an arithmetic negation
",Buggy,"The commit message indicates that there was a bug related to the discarding of a rewritten subexpression in the process of rewriting a `NegativeExpression`. The diff clearly shows that after the call to `rewrite(node.getValue(), context.get())`, the `child` expression is checked against `node.getValue()`. If they are not the same, i.e., if `child` has been rewritten, a new `NegativeExpression` is created with `child` as its argument and returned.

Previously, if `child` was different from `node.getValue()`, the resultant rewritten expression would simply be discarded as the function returned `node`. The changes make sure that if the subexpression is rewritten, it gets encapsulated in a new `NegativeExpression`, aligning with the fix suggested in the commit message.

Thus, this change fixes the intended bug as described, correcting the logic to ensure that rewritten subexpressions are properly incorporated into the `NegativeExpression`. Therefore, the conclusion is **Buggy**."
presto,8271.json,d560b6e644e49cab94fa9076ec62406cd62afa68,"@@ -1,12 +1,16 @@
         public InputStream handle(Request request, com.facebook.airlift.http.client.Response response)
         {
             try {
+                if (response.getStatusCode() != HTTP_OK) {
+                    String result = new BufferedReader(new InputStreamReader(response.getInputStream())).lines().collect(Collectors.joining(""\n""));
+                    throw new PrestoException(DRUID_BROKER_RESULT_ERROR, result);
+                }
                 if (APPLICATION_JSON.equals(response.getHeader(CONTENT_TYPE))) {
                     return response.getInputStream();
                 }
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Response received was not of type "" + APPLICATION_JSON);
             }
             catch (IOException e) {
                 throw new PrestoException(DRUID_BROKER_RESULT_ERROR, ""Unable to read response from worker"", e);
             }
         }",Buggy,"Fix unhandled HTTP response error for druid client
",Buggy,"The change introduces a check for an HTTP status code other than HTTP_OK and throws a `PrestoException` with the response content if the status code indicates an error. This is a clear change in error handling for unexpected HTTP responses, ensuring that errors do not go unnoticed and are handled properly. This aligns perfectly with the commit message stating a fix for an unhandled HTTP response error for the druid client.

Considering the adjustment to the error-handling logic that directly addresses a potentially buggy situation, I conclude that the changes indicate a bug fix. The answer is **Buggy**."
presto,22102.json,37377fdfa6f208809b77185abca3b1d0bdcb2f92,"@@ -1,72 +1,73 @@
     public static Map<List<RowExpression>, Boolean> getExpressionsPartitionedByCSE(Collection<? extends RowExpression> expressions)
     {
         if (expressions.isEmpty()) {
             return ImmutableMap.of();
         }
 
         CommonSubExpressionCollector expressionCollector = new CommonSubExpressionCollector();
         expressions.forEach(expression -> expression.accept(expressionCollector, null));
         Set<RowExpression> cse = expressionCollector.cseByLevel.values().stream().flatMap(Set::stream).collect(toImmutableSet());
 
         if (cse.isEmpty()) {
             return expressions.stream().collect(toImmutableMap(ImmutableList::of, m -> false));
         }
 
         ImmutableMap.Builder<List<RowExpression>, Boolean> expressionsPartitionedByCse = ImmutableMap.builder();
         SubExpressionChecker subExpressionChecker = new SubExpressionChecker(cse);
         Map<Boolean, List<RowExpression>> expressionsWithCseFlag = expressions.stream().collect(Collectors.partitioningBy(expression -> expression.accept(subExpressionChecker, null)));
         expressionsWithCseFlag.get(false).forEach(expression -> expressionsPartitionedByCse.put(ImmutableList.of(expression), false));
 
         List<RowExpression> expressionsWithCse = expressionsWithCseFlag.get(true);
         if (expressionsWithCse.size() == 1) {
             RowExpression expression = expressionsWithCse.get(0);
             expressionsPartitionedByCse.put(ImmutableList.of(expression), true);
             return expressionsPartitionedByCse.build();
         }
 
         List<Set<RowExpression>> cseDependency = expressionsWithCse.stream()
                 .map(expression -> subExpressions(expression).stream()
                         .filter(cse::contains)
                         .collect(toImmutableSet()))
                 .collect(toImmutableList());
 
         boolean[] merged = new boolean[expressionsWithCse.size()];
 
         int i = 0;
         while (i < merged.length) {
             while (i < merged.length && merged[i]) {
                 i++;
             }
             if (i >= merged.length) {
                 break;
             }
             merged[i] = true;
             ImmutableList.Builder<RowExpression> newList = ImmutableList.builder();
             newList.add(expressionsWithCse.get(i));
             Set<RowExpression> dependencies = new HashSet<>();
             Set<RowExpression> first = cseDependency.get(i);
             dependencies.addAll(first);
             int j = i + 1;
             while (j < merged.length) {
                 while (j < merged.length && merged[j]) {
                     j++;
                 }
                 if (j >= merged.length) {
                     break;
                 }
                 Set<RowExpression> second = cseDependency.get(j);
                 if (!Sets.intersection(dependencies, second).isEmpty()) {
                     RowExpression expression = expressionsWithCse.get(j);
                     newList.add(expression);
                     dependencies.addAll(second);
                     merged[j] = true;
+                    j = i + 1;
                 }
                 else {
                     j++;
                 }
             }
             expressionsPartitionedByCse.put(newList.build(), true);
         }
 
         return expressionsPartitionedByCse.build();
     }",Buggy,"Fixing a bug in common sub expression partitioning

The previous implementation could group expressions differently with different order or exprsesions.
",Buggy,"The commit message describes fixing a bug in common subexpression (CSE) partitioning, specifically addressing differing grouping results due to expression order. The provided diff modifies part of a complex CSE partitioning logic.

Here's the relevant assessment of the change:

1. **Context Understanding**: The method `getExpressionsPartitionedByCSE` partitions a collection of `RowExpression` objects based on shared common subexpressions. The goal is to create a consistent partition regardless of input order, ensuring deterministic results.

2. **Change Analysis**:
   - The modified loop structure for merging expressions with common dependencies involves resetting the inner loop index `j` back to `i + 1` whenever a merge occurs (`j = i + 1;`). Such a change is logical when new dependencies are introduced, necessitating a re-evaluation of unconsidered expressions to ensure all possible groupings are accounted for.
   - This prevents omissions in forming partitions, addressing the described bug of inconsistent grouping based on order.

3. **Commit Relevance**: The adjustment directly relates to the commit message by purporting to fix how expressions are grouped. Prior approaches would lead to inconsistently partitioned results contingent on input order, which deviates from the intended behavior of deterministic CSE partitioning.

4. **Identifying Bug Fix**: 
   - **Logical Correction**: The reset of `j` ensures all potential partitions are considered correctly, accounting for any new dependencies that arise dynamically.
   - **Reiteration**: The process described implies previously missed groupings due to limitations in the old logic.

Conclusively, these changes showcase a direct fix to a logical issue as per the commit message, intended to ensure deterministic behavior in expression partitioning. Hence, the conclusion is **Buggy**."
presto,10913.json,e6f650722f4041eade54638d355de2b9f7a9e280,"@@ -1,4 +1,4 @@
     public boolean isFinalQueryInfo()
     {
-        return state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
+        return state.equals(QueryState.FAILED) || state.isDone() && getAllStages(outputStage).stream().allMatch(StageInfo::isFinalStageInfo);
     }",Buggy,"fix the client hang problem after query failed
",Buggy,"Analyzing the code change:

- **Original Code Logic**: The original condition for `isFinalQueryInfo()` checks if the `state` is ""done"" and all stages are ""final.""
- **Changed Logic**: The new condition adds an additional check to consider the query final if the `state` equals `QueryState.FAILED`, in addition to continuing with the previous condition.

**Commit Message Analysis**:
- The commit message states ""fix the client hang problem after query failed,"" implying that there was an issue with the system hanging when a query failure occurred.

**Relevance to Commit Message**:
- The code modification is directly related to the commit message. By considering the query final when the state is ""FAILED,"" the system likely avoids hanging by recognizing that the query processing should be concluded. This change would have been necessary to fix a situation where failure did not mark the query as final, potentially causing the system to wait indefinitely for stages to complete that never actually reach a final state because the query is already in a failed state.

**Conclusion**:
- The change aligns well with fixing a bug where the system failed to terminate correctly upon a query failure. This optimal handling of the query states directly addresses the potential hang issue outlined in the commit message.

Thus, this commit is likely addressing a bug fix, and the correct conclusion is **Buggy**."
presto,7159.json,130527265fd8aebee395ed2412f4058d0e269f51,"@@ -1,84 +1,83 @@
         public PlanNode visitProject(ProjectNode project, Void context)
         {
             if (!(project.getSource() instanceof TableScanNode)) {
                 return visitPlan(project, context);
             }
 
             TableScanNode tableScan = (TableScanNode) project.getSource();
             if (!isParquetDereferenceEnabled(session, tableScan.getTable())) {
                 return visitPlan(project, context);
             }
 
             Map<RowExpression, Subfield> dereferenceToNestedColumnMap = extractDereferences(
                     session,
                     rowExpressionService.getExpressionOptimizer(),
                     new HashSet<>(project.getAssignments().getExpressions()));
             if (dereferenceToNestedColumnMap.isEmpty()) {
                 return visitPlan(project, context);
             }
 
-            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().values().stream()
-                    .map(columnHandle -> (HiveColumnHandle) columnHandle)
-                    .collect(toMap(HiveColumnHandle::getName, identity()));
+            Map<String, HiveColumnHandle> regularHiveColumnHandles = tableScan.getAssignments().entrySet().stream()
+                    .collect(toMap(e -> e.getKey().getName(), e -> (HiveColumnHandle) e.getValue()));
 
             List<VariableReferenceExpression> newOutputVariables = new ArrayList<>(tableScan.getOutputVariables());
             Map<VariableReferenceExpression, ColumnHandle> newAssignments = new HashMap<>(tableScan.getAssignments());
 
             Map<RowExpression, VariableReferenceExpression> dereferenceToVariableMap = new HashMap<>();
 
             for (Map.Entry<RowExpression, Subfield> dereference : dereferenceToNestedColumnMap.entrySet()) {
                 Subfield nestedColumn = dereference.getValue();
                 RowExpression dereferenceExpression = dereference.getKey();
 
                 // Find the nested column Hive Type
                 HiveColumnHandle regularColumnHandle = regularHiveColumnHandles.get(nestedColumn.getRootName());
                 if (regularColumnHandle == null) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""]'s base column "" + nestedColumn.getRootName() + "" is not present in table scan output"");
                 }
 
                 Optional<HiveType> nestedColumnHiveType = regularHiveColumnHandles.get(nestedColumn.getRootName())
                         .getHiveType()
                         .findChildType(
                                 nestedColumn.getPath().stream()
                                         .map(p -> ((Subfield.NestedField) p).getName())
                                         .collect(Collectors.toList()));
 
                 if (!nestedColumnHiveType.isPresent()) {
                     throw new IllegalArgumentException(""nested column ["" + nestedColumn + ""] type is not present in Hive column type"");
                 }
 
                 String pushdownColumnName = pushdownColumnNameForSubfield(nestedColumn);
                 // Create column handle for nested column
                 HiveColumnHandle nestedColumnHandle = new HiveColumnHandle(
                         pushdownColumnName,
                         nestedColumnHiveType.get(),
                         dereferenceExpression.getType().getTypeSignature(),
                         -1,
                         SYNTHESIZED,
                         Optional.of(""nested column pushdown""),
                         ImmutableList.of(nestedColumn),
                         Optional.empty());
 
                 VariableReferenceExpression newOutputVariable = variableAllocator.newVariable(pushdownColumnName, dereferenceExpression.getType());
                 newOutputVariables.add(newOutputVariable);
                 newAssignments.put(newOutputVariable, nestedColumnHandle);
 
                 dereferenceToVariableMap.put(dereferenceExpression, newOutputVariable);
             }
 
             TableScanNode newTableScan = new TableScanNode(
                     idAllocator.getNextId(),
                     tableScan.getTable(),
                     newOutputVariables,
                     newAssignments,
                     tableScan.getCurrentConstraint(),
                     tableScan.getEnforcedConstraint());
 
             Assignments.Builder newProjectAssignmentBuilder = Assignments.builder();
             for (Map.Entry<VariableReferenceExpression, RowExpression> entry : project.getAssignments().entrySet()) {
                 RowExpression newExpression = rewriteWith(new DereferenceExpressionRewriter(dereferenceToVariableMap), entry.getValue());
                 newProjectAssignmentBuilder.put(entry.getKey(), newExpression);
             }
 
             return new ProjectNode(idAllocator.getNextId(), newTableScan, newProjectAssignmentBuilder.build(), project.getLocality());
         }",Buggy,"Fix base column name not present error when dereference pushdown enabled
",Buggy,"This commit addresses a specific error related to ""base column name not present"" when dereference pushdown is enabled. Here's a step-by-step analysis of the changes:

1. **Context Change**: The initial part of the method checks if the source node is a TableScanNode and whether dereference pushdown is enabled.

2. **Modified Logic**: 
    - The diff changes how `regularHiveColumnHandles` is populated. The previous version mapped a column handle's name to itself. The updated version associates a variable's name (from `tableScan.getAssignments().entrySet()`) with its corresponding Hive column handle. This matches base columns more accurately with their data in the table scan, addressing scenarios where the name might differ from the column itself.
   
3. **Error Addressed**: 
   - The updated code explicitly throws an `IllegalArgumentException` if a nested column's base column root name isn’t in `regularHiveColumnHandles`. The commit message indicates fixing a ""base column name not present"" error, and this condition addresses that.

4. **Dereference Mapping**: 
   - It creates a new map (`dereferenceToVariableMap`) to track the association between dereference expressions and their variables, crucial for correctly handling dereferenced columns post-fix.

5. **Column Handle Creation**:
   - For each dereference, it checks and retrieves the precise hive type from the column handle. If it’s missing, another `IllegalArgumentException` is thrown, verifying column type presence aligns with the fix requirements.

6. **Downstream Adjustments**:
   - It ensures new `VariableReferenceExpression` instances are correctly allocated and adds them to `newOutputVariables` and `newAssignments`, preparing them for inclusion in the `TableScanNode`.

7. **Outcome**:
   - These logical corrections ensure that the column pushdown mechanism correctly matches base columns and their respective types in scenarios involving nested columns, thus addressing a potential bug where base columns could be misconfigured or missing.

The code changes, error handling, condition additions, and more precise mapping collectively address a ""base column name not present"" error linked to dereference pushdown logic, strongly suggesting the commit fixes a bug. Therefore, the conclusion is **Buggy**."
presto,22204.json,b88a9a91b24500d3c46df8cd613783effdc9d70c,"@@ -1,34 +1,34 @@
     private static Map<LambdaDefinitionExpression, CompiledLambda> generateMethodsForLambda(
             ClassDefinition containerClassDefinition,
             CallSiteBinder callSiteBinder,
             CachedInstanceBinder cachedInstanceBinder,
             List<RowExpression> expressions,
             Metadata metadata,
             SqlFunctionProperties sqlFunctionProperties,
             String methodNamePrefix,
             Set<LambdaDefinitionExpression> existingCompiledLambdas)
     {
         Set<LambdaDefinitionExpression> lambdaExpressions = expressions.stream()
                 .map(LambdaExpressionExtractor::extractLambdaExpressions)
                 .flatMap(List::stream)
                 .filter(lambda -> !existingCompiledLambdas.contains(lambda))
                 .collect(toImmutableSet());
         ImmutableMap.Builder<LambdaDefinitionExpression, CompiledLambda> compiledLambdaMap = ImmutableMap.builder();
 
-        int counter = 0;
+        int counter = existingCompiledLambdas.size();
         for (LambdaDefinitionExpression lambdaExpression : lambdaExpressions) {
             CompiledLambda compiledLambda = LambdaBytecodeGenerator.preGenerateLambdaExpression(
                     lambdaExpression,
                     methodNamePrefix + ""lambda_"" + counter,
                     containerClassDefinition,
                     compiledLambdaMap.build(),
                     callSiteBinder,
                     cachedInstanceBinder,
                     metadata,
                     sqlFunctionProperties);
             compiledLambdaMap.put(lambdaExpression, compiledLambda);
             counter++;
         }
 
         return compiledLambdaMap.build();
     }",Buggy,"Fix compiler error in LambdaBytecodeGenerator

When there are lambda expressions from different SQL functions, and they are
compiled into the same class due to CSE, we need to make sure the generated
function names are always unique.
",Buggy,"The commit message describes a fix for a compiler error involving lambda expressions from different SQL functions. The error occurs when these are compiled into the same class due to Common Subexpression Elimination (CSE) without unique function names.

Looking at the code changes:
- The `int counter` initialization has been modified from starting at 0 to starting at the size of `existingCompiledLambdas`.
- This change ensures that new lambda functions are always appended with a unique suffix. Starting from the size of `existingCompiledLambdas` avoids potential conflicts with previously compiled lambda expressions.

These changes directly address the issue mentioned in the commit message by ensuring unique naming of generated functions, which aligns with fixing a compiler error resulting from name collisions during class generation.

Given these considerations, the code changes effectively address a bug (the mentioned compiler error due to missing unique function names in specific cases), thus the conclusion is **Buggy**."
presto,1269.json,4cb4d192f1cc8e0da404521946d7566738d45c00,"@@ -1,6 +1,6 @@
         private void prepareDropPartition(SchemaTableName schemaTableName, List<String> partitionValues)
         {
             metastoreDeleteOperations.add(new IrreversibleMetastoreOperation(
-                    format(""drop partition %s.%s %s"", schemaTableName, schemaTableName.getTableName(), partitionValues),
+                    format(""drop partition %s.%s %s"", schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues),
                     () -> delegate.dropPartition(schemaTableName.getSchemaName(), schemaTableName.getTableName(), partitionValues, true)));
         }",Buggy,"Fix SemiTransactionalHiveMetastore error message
",Buggy,"The code diff involves a change in the error message generated when dropping a partition. The original code concatenates the schemaTableName object twice, likely causing incorrect messages. The fix replaces the second instance of schemaTableName with schemaTableName.getSchemaName(), ensuring the correct representation of schema name and table name in messages. Correct error messages aid debugging, indicating a fix of logical error. Hence, the conclusion is **Buggy**."
presto,17974.json,0498c0df5aff14c9f7f88db1fe2e43ed3470aa5b,"@@ -1,12 +1,16 @@
     private static Optional<TaskInfo> findFailedTask(StageInfo stageInfo)
     {
+        if (stageInfo == null) {
+            return Optional.empty();
+        }
+
         for (StageInfo subStage : stageInfo.getSubStages()) {
             Optional<TaskInfo> task = findFailedTask(subStage);
             if (task.isPresent()) {
                 return task;
             }
         }
         return stageInfo.getTasks().stream()
                 .filter(taskInfo -> taskInfo.getState() == TaskState.FAILED)
                 .findFirst();
     }",Buggy,"Fix NPE when query fails during parsing/planning

Commit 135626b793b42841c77f6b8c8b7fe80fac8ffc46 introduced a bug
where queries that fail during parsing/analysis/planning
throw an NPE when the completion event is processed.

This is due to an incorrect assumption that QueryInfos always
have an output stage, which is not true for queries that fail
before execution starts.
",Buggy,"The commit message states that the code change addresses a NullPointerException (NPE) when queries fail during parsing, analysis, or planning phases. The modification in the git diff reflects that additional null checks for `stageInfo` are introduced before proceeding with the logic that processes `StageInfo` objects. By checking if `stageInfo` is `null` at the beginning of the `findFailedTask` method and returning an `Optional.empty()`, the change prevents further processing on a `null` object, which directly addresses the described NPE and aligns with preventing assumptions that QueryInfos always have an output stage.

Hence, this change does align with the commit message and identifies a bug-related correction involving handling potentially null references, which helps prevent NPEs.

Thus, the conclusion is **Buggy**."
presto,22343.json,913c3f81cdc1a8dbc3201382b38d5e429f43fa7f,"@@ -1,36 +1,37 @@
     public BytecodeNode generateExpression(Signature signature, BytecodeGeneratorContext context, Type rowType, List<RowExpression> arguments)
     {
         BytecodeBlock block = new BytecodeBlock().setDescription(""Constructor for "" + rowType.toString());
         CallSiteBinder binder = context.getCallSiteBinder();
         Scope scope = context.getScope();
         List<Type> types = rowType.getTypeParameters();
 
         block.comment(""BlockBuilder blockBuilder = new InterleavedBlockBuilder(types, new BlockBuilderStatus(), 1);"");
         Variable blockBuilder = scope.createTempVariable(BlockBuilder.class);
         Binding typesBinding = binder.bind(types, List.class);
         block.append(blockBuilder.set(
                 newInstance(InterleavedBlockBuilder.class, loadConstant(typesBinding), newInstance(BlockBuilderStatus.class), constantInt(1))));
 
         for (int i = 0; i < arguments.size(); ++i) {
             Type fieldType = types.get(i);
             Class<?> javaType = fieldType.getJavaType();
             if (javaType == void.class) {
                 block.comment(i + ""-th field type of row is undefined"");
                 block.append(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop());
             }
             else {
                 Variable field = scope.createTempVariable(javaType);
-                block.comment(""Generate + "" + i + ""-th field of row"");
+                block.comment(""Clean wasNull and Generate + "" + i + ""-th field of row"");
+                block.append(context.wasNull().set(constantFalse()));
                 block.append(context.generate(arguments.get(i)));
                 block.putVariable(field);
                 block.append(new IfStatement()
                         .condition(context.wasNull())
                         .ifTrue(blockBuilder.invoke(""appendNull"", BlockBuilder.class).pop())
                         .ifFalse(constantType(binder, fieldType).writeValue(blockBuilder, field).pop()));
             }
         }
         block.comment(""put (Block) blockBuilder.build(); wasNull = false;"");
         block.append(blockBuilder.invoke(""build"", Block.class));
         block.append(context.wasNull().set(constantFalse()));
         return block;
     }",Buggy,"Fix bug when constructing ROW with null values

Not all generators set wasNull, causing following values
(generated from one generator that also doesn't set wasNull) to be null.
",Buggy,"This diff addresses a bug related to the setting of the `wasNull` flag when generating a row expression. Here's a step-by-step analysis:

1. **Context Setting**: The commit message indicates there was a bug due to some generators not setting `wasNull`, causing subsequent generated values to be null incorrectly.

2. **Code Changes**:
   - In the modified code, before generating each field (unless it's undefined), the `wasNull` flag is explicitly reset to `false` using `context.wasNull().set(constantFalse())`.
   - This change ensures that the `wasNull` status from a previous operation does not incorrectly affect the current field generation. 

3. **Logical Correction**: 
   - The original code potentially left the `wasNull` flag in an incorrect state from one generation to the next, leading to unexpected behavior when building rows with null values.
   - By explicitly resetting `wasNull` before generating each field, the code ensures that each field's generation begins with a clean slate, thereby fixing any issues with rows constructed with null values.

This change directly aligns with the commit message, which describes a bug fix involving null handling during row construction. The reset of the `wasNull` flag corrects the error and aligns with the described issue.

Conclusion: **Buggy** (This is a bug fix)."
presto,1944.json,d439d475c1eb6674ff8a9fb75ee08565217bc39c,"@@ -1,8 +1,8 @@
     public boolean isSingleValue()
     {
         return !low.isLowerUnbounded() &&
                 !high.isUpperUnbounded() &&
                 low.getBound() == Marker.Bound.EXACTLY &&
                 high.getBound() == Marker.Bound.EXACTLY &&
-                low.getValue() == high.getValue();
+                low.getValue().equals(high.getValue());
     }",Buggy,"Fix Range bug in SPI
",Buggy,"The change in this diff is from using `==` to using `.equals()` for comparing `low.getValue()` and `high.getValue()`. 

1. **Understanding the Change**: 
   - The `==` operator checks for reference equality, which means it was checking if both `low.getValue()` and `high.getValue()` point to the same object in memory.
   - Using `.equals()` checks for logical equality, meaning it determines if the values contained in the objects are equivalent.

2. **Reason for the Change**: 
   - The original use of `==` would only work if `low.getValue()` and `high.getValue()` were pointing to the same object, not if they were different objects logically representing the same value. This can lead to incorrect results when `low` and `high` have equivalent but different instances.

3. **Implication of the Change**: 
   - This change ensures that `isSingleValue()` correctly identifies a single value range by logically comparing the values rather than checking if they are the same instance.
   - It is likely fixing a bug where logically equivalent bounds were not recognized as such, leading to incorrect behavior in the range logic.

Therefore, based on the analysis, the commit likely fixes a bug relating to how bounds were evaluated in the range logic of the SPI. The change aligns with the commit message indicating a bug fix. The assessment is **Buggy**."
presto,17855.json,18088bed1462b2cf8f46ab44293b3791136588d3,"@@ -1,8 +1,7 @@
     public String toString()
     {
         return Objects.toStringHelper(this)
                 .add(""tableHandle"", tableHandle)
-                .add(""filters"", filters)
                 .add(""addresses"", addresses)
                 .toString();
     }",Buggy,"System connector predicate pushdown bug fix
",NotBuggy,"The commit message indicates a bug fix related to ""predicate pushdown,"" but the diff only removes the inclusion of the `filters` field in the `toString` method output. This change affects the logging or debugging output and does not appear directly related to bug fixing in predicate pushdown functionality. As no clear logical error or exception-handling correction is evident from this change alone, the modifications do not appear to address a functional bug.

The commit message suggests a bug fix, but the code change shown does not support this claim. The removal of the `filters` field from the `toString` method's output likely contributes to avoiding unnecessary logging but doesn't fix a functional bug.

Thus, based on the given information, the conclusion is **NotBuggy**."
presto,3309.json,c798add760c213bafe85a1f7934e54b5fe0b8728,"@@ -1,4 +1,4 @@
     public int getPositionCount()
     {
-        return blocks[0].getPositionCount();
+        return positionCount;
     }",Buggy,"Fix bug in count aggregation from inline view

This query now works:

SELECT COUNT(*) FROM (SELECT ... ) x
",Buggy,"The commit message indicates a bug fix related to count aggregation from an inline view. In the code diff, the `getPositionCount()` method is modified to return `positionCount` instead of `blocks[0].getPositionCount()`. This change likely addresses a bug where the previous implementation incorrectly assumed that the position count could be derived from the first block alone. This aligns with the commit message, suggesting that the new implementation should correctly count all positions represented by `positionCount`, thus aligning with the expected behavior for count aggregation queries.

Given that this involves correcting the count logic, this change aligns with the commit message's description of fixing a bug. Therefore, the conclusion is **Buggy**."
presto,12870.json,f20d677b8203f10728649c8f4b038f68a08dd909,"@@ -1,4 +1,4 @@
     private static List<ParameterMetadata> createInputParameterMetadata(Type value, Type key)
     {
-        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
+        return ImmutableList.of(new ParameterMetadata(STATE), new ParameterMetadata(NULLABLE_BLOCK_INPUT_CHANNEL, value), new ParameterMetadata(BLOCK_INPUT_CHANNEL, key), new ParameterMetadata(BLOCK_INDEX));
     }",Buggy,"Fix null-handling bug in min_by/max_by

Null keys were previously incorrectly handled. They were effectively treated
as default value of the type for comparison in the input function.
",Buggy,"The commit message indicates a fix in handling null values for certain keys. The code change modifies a parameter in the ImmutableList from `NULLABLE_BLOCK_INPUT_CHANNEL` to `BLOCK_INPUT_CHANNEL` for the `key`. This directly ties into adjusting how nulls are handled, suggesting the original code treated null keys incorrectly by assuming they should be comparable as non-nullable.

The message and the change align in indicating that this modification is aimed at a bug related to null handling, consistent with clarifying the processing of null keys in the `min_by/max_by` functions. This is a common type of bug fix, focusing on improving logic related to null value processing and comparisons.

Hence, the conclusion for this change is **Buggy**."
presto,16287.json,c5b89795521472323d392daa860240d64d3876f0,"@@ -1,35 +1,59 @@
     static DecodedBlockNode decodeBlock(BlockFlattener flattener, Closer blockLeaseCloser, Block block)
     {
         BlockLease lease = flattener.flatten(block);
         blockLeaseCloser.register(lease::close);
         Block decodedBlock = lease.get();
 
+        long estimatedSizeInBytes = decodedBlock.getLogicalSizeInBytes();
+
         if (decodedBlock instanceof ArrayBlock) {
             ColumnarArray columnarArray = ColumnarArray.toColumnarArray(decodedBlock);
-            return new DecodedBlockNode(columnarArray, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarArray.getElementsBlock())));
+            Block childBlock = columnarArray.getElementsBlock();
+            return new DecodedBlockNode(
+                    columnarArray,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    columnarArray.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof MapBlock) {
             ColumnarMap columnarMap = ColumnarMap.toColumnarMap(decodedBlock);
-            return new DecodedBlockNode(columnarMap, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, columnarMap.getKeysBlock()), decodeBlock(flattener, blockLeaseCloser, columnarMap.getValuesBlock())));
+            Block keyBlock = columnarMap.getKeysBlock();
+            Block valueBlock = columnarMap.getValuesBlock();
+            return new DecodedBlockNode(
+                    columnarMap,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, keyBlock), decodeBlock(flattener, blockLeaseCloser, valueBlock)),
+                    columnarMap.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RowBlock) {
             ColumnarRow columnarRow = ColumnarRow.toColumnarRow(decodedBlock);
             ImmutableList.Builder<DecodedBlockNode> children = ImmutableList.builder();
             for (int i = 0; i < columnarRow.getFieldCount(); i++) {
-                children.add(decodeBlock(flattener, blockLeaseCloser, columnarRow.getField(i)));
+                Block childBlock = columnarRow.getField(i);
+                children.add(decodeBlock(flattener, blockLeaseCloser, childBlock));
             }
-            return new DecodedBlockNode(columnarRow, children.build());
+            return new DecodedBlockNode(columnarRow, children.build(), columnarRow.getRetainedSizeInBytes(), estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof DictionaryBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((DictionaryBlock) decodedBlock).getDictionary())));
+            Block dictionary = ((DictionaryBlock) decodedBlock).getDictionary();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, dictionary)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
         if (decodedBlock instanceof RunLengthEncodedBlock) {
-            return new DecodedBlockNode(decodedBlock, ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, ((RunLengthEncodedBlock) decodedBlock).getValue())));
+            Block childBlock = ((RunLengthEncodedBlock) decodedBlock).getValue();
+            return new DecodedBlockNode(
+                    decodedBlock,
+                    ImmutableList.of(decodeBlock(flattener, blockLeaseCloser, childBlock)),
+                    decodedBlock.getRetainedSizeInBytes(),
+                    estimatedSizeInBytes);
         }
 
-        return new DecodedBlockNode(decodedBlock, ImmutableList.of());
+        return new DecodedBlockNode(decodedBlock, ImmutableList.of(), block.getRetainedSizeInBytes(), estimatedSizeInBytes);
     }",Buggy,"Fix estimatedSerializedSizeInBytes for RLE and Dictionary Blocks

When a block passed to OptimizedPartitionedOutputOperator is a RLE or
Dictionary block, we used to estimated the serialized size using
getLogicalSize() which returns the size of the block after inflation.
However the child block of the RLE or Dictionary Block was using plain
sizeInBytes without considering it is going to be expanded. This
commit fixes this problem by adding a scale factor to estimate how many
times the child blocks are going to be expanded.
",Buggy,"The commit message details fixing a bug in estimating serialized block sizes for RLE and Dictionary Blocks. The diff supports this by incorporating a more accurate estimation of serialized sizes. It introduces adjustments for different block types to consider their expanded size using the `getLogicalSizeInBytes()` method.

- The `estimatedSizeInBytes` variable is initialized to log the logical size of the decoded block.
- This estimated size is now passed to `DecodedBlockNode` instances for all block types like `ArrayBlock`, `MapBlock`, `RowBlock`, `DictionaryBlock`, and `RunLengthEncodedBlock`.
- These changes harmonize the size estimation to account for the actual expansion of these blocks, aligning with the commit message.

This update is a clear attempt at correcting logic related to size estimation, resolving the mentioned bug. Hence, the commit is accurately described as a bug fix.

The assessment leads to the conclusion: **Buggy**."
presto,21104.json,58e78de1c0cde073cc472bb7b2d4cf52155e15e3,"@@ -1,4 +1,8 @@
         public StreamProperties visitTopN(TopNNode node, List<StreamProperties> inputProperties)
         {
+            // Partial TopN doesn't guarantee that stream is ordered
+            if (node.getStep().equals(TopNNode.Step.PARTIAL)) {
+                return Iterables.getOnlyElement(inputProperties);
+            }
             return StreamProperties.ordered();
         }",Buggy,"Fix stream properties for partial TopN

This is a bug fix, that seems to could never occur.
",Buggy,"The provided code diff adds a condition to check if a `TopNNode` is in the `PARTIAL` step. For partial steps, the new code returns the input properties instead of assuming the stream is ordered. This change directly addresses a potential issue in handling different processing stages of `TopNNode`. Without this correction, the system might incorrectly assume that a stream resulting from a partial TopN operation is ordered, which could cause incorrect behavior or outputs.

The commit message correctly identifies this change as a bug fix. Even though it notes that the bug might not occur frequently, the modification ensures that the system behaves correctly in all cases. Therefore, based on the information provided, the changes in the code diff align with the commit message to fix a bug related to stream properties.

Hence, the conclusion is **Buggy**."
presto,18169.json,86913bdc9e40dd112bda300f21ae0f95e1330515,"@@ -1,4 +1,4 @@
         public Type getType(int field)
         {
-            return delegate.getType(field);
+            return delegate.getType(toDelegateField(field));
         }",Buggy,"Fix MappedRecordCursor bug
",Buggy,"The commit message indicates a fix for a ""MappedRecordCursor bug."" The code change modifies the `getType` method to use `toDelegateField(field)` instead of calling `delegate.getType(field)` directly.

Here's the analysis:
1. **Problem Area**: The original code directly accesses a field type from a delegate, which may not correctly handle field mappings.
2. **Change Made**: The fix applies `toDelegateField(field)` before fetching the type, suggesting the function likely maps or translates the field index appropriately.
3. **Reasoning**: If `toDelegateField` correctly handles necessary transformations or mappings for the field index, applying it before accessing the type ensures correct delegation. Without this transformation, type lookups could be incorrect, leading to bugs like returning wrong or unexpected types.
4. **Bug Fix**: The change provides evidence of fixing a logical error in how field indices were handled in the method, aligning with the commit message's claim.

Hence, the change addresses an issue in field handling within `MappedRecordCursor` and qualifies as a bug fix.

The answer is **Buggy**."
presto,26959.json,90e7efd07b37da0ff012bb99798ed37b4173661b,"@@ -1,85 +1,83 @@
     private String toPredicate(int columnIndex, Domain domain)
     {
         String columnName = columnsNames.get(columnIndex);
         Type type = types.get(columnIndex);
 
         if (domain.getRanges().isNone() && domain.isNullAllowed()) {
             return columnName + "" IS NULL"";
         }
 
         if (domain.getRanges().isAll() && !domain.isNullAllowed()) {
             return columnName + "" IS NOT NULL"";
         }
 
         // Add disjuncts for ranges
         List<String> disjuncts = new ArrayList<>();
         List<Comparable<?>> singleValues = new ArrayList<>();
         for (Range range : domain.getRanges()) {
             checkState(!range.isAll()); // Already checked
-            Comparable<?> lowValue = range.getLow().getValue();
             if (range.isSingleValue()) {
-                singleValues.add(lowValue);
+                singleValues.add(range.getLow().getValue());
             }
             else {
                 List<String> rangeConjuncts = new ArrayList<>();
                 if (!range.getLow().isLowerUnbounded()) {
-                    Object bindValue = getBindValue(columnIndex, lowValue);
+                    Object bindValue = getBindValue(columnIndex, range.getLow().getValue());
                     switch (range.getLow().getBound()) {
                         case ABOVE:
                             rangeConjuncts.add(toBindPredicate(columnName, "">""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, "">=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             throw new IllegalStateException(""Low Marker should never use BELOW bound: "" + range);
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getLow().getBound());
                     }
                 }
                 if (!range.getHigh().isUpperUnbounded()) {
-                    Comparable<?> highValue = range.getHigh().getValue();
-                    Object bindValue = getBindValue(columnIndex, highValue);
+                    Object bindValue = getBindValue(columnIndex, range.getHigh().getValue());
                     switch (range.getHigh().getBound()) {
                         case ABOVE:
                             throw new IllegalStateException(""High Marker should never use ABOVE bound: "" + range);
                         case EXACTLY:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<=""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         case BELOW:
                             rangeConjuncts.add(toBindPredicate(columnName, ""<""));
                             bindValues.add(ValueBuffer.create(columnIndex, type, bindValue));
                             break;
                         default:
                             throw new AssertionError(""Unhandled bound: "" + range.getHigh().getBound());
                     }
                 }
                 // If rangeConjuncts is null, then the range was ALL, which should already have been checked for
                 checkState(!rangeConjuncts.isEmpty());
                 disjuncts.add(""("" + Joiner.on("" AND "").join(rangeConjuncts) + "")"");
             }
         }
 
         // Add back all of the possible single values either as an equality or an IN predicate
         if (singleValues.size() == 1) {
             disjuncts.add(toBindPredicate(columnName, ""=""));
             bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, getOnlyElement(singleValues))));
         }
         else if (singleValues.size() > 1) {
             disjuncts.add(columnName + "" IN ("" + Joiner.on("","").join(limit(cycle(""?""), singleValues.size())) + "")"");
             for (Comparable<?> singleValue : singleValues) {
                 bindValues.add(ValueBuffer.create(columnIndex, type, getBindValue(columnIndex, singleValue)));
             }
         }
 
         // Add nullability disjuncts
         checkState(!disjuncts.isEmpty());
         if (domain.isNullAllowed()) {
             disjuncts.add(columnName + "" IS NULL"");
         }
 
         return ""("" + Joiner.on("" OR "").join(disjuncts) + "")"";
     }",Buggy,"Fix bug for domain with lower unbounded range
",Buggy,"Analyzing the given diff and commit message, we need to determine if these code changes indicate a bug fix related to ""a domain with a lower unbounded range.""

### Code Changes Analysis
1. **Low Value Handling:**
   - **Old Code:** Uses `Comparable<?> lowValue = range.getLow().getValue();` for handling low range values.
   - **New Code:** Inline usage of `range.getLow().getValue()` directly in both single value and range processing without creating a separate variable.

2. **High Value Handling:**
   - Changed `Comparable<?> highValue = range.getHigh().getValue();` to directly utilize `range.getHigh().getValue()` inline.

3. **No Change in Logic:**
   - The logical flow regarding handling low and high range boundaries remains consistent, where the code switches between different comparison operators based on the boundary type (ABOVE, EXACTLY, BELOW).

4. **Focus on Single Value and Range Boundaries:**
   - The change primarily focuses on directly accessing range values in place, which subtly aligns with ensuring the handling of unbounded values directly.

### Relation to Commit Message
- **""Fix bug for domain with lower unbounded range"":** 
  - The changes seem to focus on cleaner and more consistent access to the values within ranges.
  - However, there is no explicit addition or modification directly hinting at managing unbound lower ranges, other than potentially avoiding side effects from variable re-use.

### Conclusion
The commit message refers to fixing a bug related to handling domains with lower unbounded ranges. The diff suggests enhanced direct access to range values, which could aid readability and reduce bugs related to value manipulations. The refactor might implicitly be related to the mentioned bug but without explicit logic change to definitely address lower unbounded ranges directly visible.

Despite the indirect nature, considering it aligns with potential cleanup to facilitate better handling of such ranges and prevent possible misuse, it can correlate with a bug fix intention. 

Hence, I conclude: **Buggy**."
presto,1063.json,097536e42a4f29b25497ec5c782ef60cabdf71ef,"@@ -1,26 +1,31 @@
     public <V> Callable<V> wrap(Callable<V> callable)
     {
         return () -> {
             try (TimeStat.BlockTimer ignored = time.time()) {
                 return callable.call();
             }
             catch (Exception e) {
                 if (e instanceof MetaException) {
                     metastoreExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 if (e instanceof TException) {
+                    if (e instanceof TBase) {
+                        // This exception is an API response and not a server error
+                        throw e;
+                    }
+
                     thriftExceptions.update(1);
                     // Need to throw here instead of falling through due to JDK-8059299
                     totalFailures.update(1);
                     throw e;
                 }
 
                 totalFailures.update(1);
                 throw e;
             }
         };
     }",Buggy,"Fix Hive metastore client API stats

The stats were counting API response exceptions as server errors.
",Buggy,"The commit message specifies a correction in how API response exceptions are counted within the Hive metastore client. Previously, all `TException` instances were being counted as server-related errors, which was incorrect when the exception was an API response.

The logic has now been updated to include a crucial step:
- The code now checks if the caught `TException` is a subclass of `TBase`. If it is, it identifies the exception as an API response rather than a server error. 
- In such cases, the code now correctly avoids updating `thriftExceptions` and `totalFailures`. Instead, it simply rethrows the exception.

These changes address the incorrect accounting of API response exceptions by introducing specific exception-handling logic that better distinguishes between true server errors and expected API responses signaled through exceptions.

This accurately reflects the commit message, confirming an alignment between the stated bug—incorrectly counting API exceptions as server errors—and the code changes. Thus, the changes provide clear evidence of a bug fix.

The answer is **Buggy**."
presto,2919.json,54dd54cb681af311e9b6fb6910ae004dfa71988b,"@@ -1,8 +1,8 @@
     public SingleArrayBlockWriter beginBlockEntry()
     {
         if (currentEntryOpened) {
-            throw new IllegalStateException(""Expected current entry to be closed but was closed"");
+            throw new IllegalStateException(""Expected current entry to be closed but was opened"");
         }
         currentEntryOpened = true;
         return new SingleArrayBlockWriter(values, values.getPositionCount());
     }",Buggy,"Minor exception message fix for ArrayBlockBuilder

When attempting to begin an entry on an already opened block writer,
the error message does not accurately reflect the current state.
",Buggy,"The commit message states that it fixes an incorrect exception message. The code change corrects the message from ""but was closed"" to ""but was opened,"" aligning it with the actual condition being checked: `currentEntryOpened`. This adjustment clarifies the state of the `currentEntryOpened` if the exception is thrown, improving error diagnostics.

While the correction of the exception message doesn't directly fix a logical or functional bug in program execution, it rectifies an inaccurate error indication, which can aid in debugging by providing the correct context. Since this change corrects a potential source of confusion in error handling (even though it does not affect program logic), it aligns with fixing minor errors related to exception handling.

Therefore, the code changes reflect a kind of bug fix related to the accuracy of internal error reporting information. The conclusion is **Buggy**."
presto,37880.json,954e4714ac1aac177bbf00d5641088cc7eb15289,"@@ -1,8 +1,8 @@
     public int getPartition(Object key)
     {
         int partition = requireNonNull((Integer) key, ""key is null"");
         if (!(partition >= 0 && partition < numPartitions)) {
-            throw new IllegalArgumentException(""invalid partition: %s"" + partition);
+            throw new IllegalArgumentException(format(""Unexpected partition: %s. Total number of partitions: %s."", partition, numPartitions));
         }
         return partition;
     }",Buggy,"Fix error message in IntegerIdentityPartitioner
",NotBuggy,"The commit message states that it fixes an error message in the `IntegerIdentityPartitioner`. The diff shows a change in the exception message thrown when an invalid partition is supplied. The new message provides more detailed information, including the total number of partitions, which enhances clarity.

- **Enhanced Error Message**: The previous message was concise but didn't give enough context about the failure. The updated message specifies that it's an ""Unexpected partition"" and includes the total number of partitions, which can help the user diagnose the issue more effectively. 
- **Format Correction**: The old message used incorrect format syntax (a simple concatenation instead of using a formatted string via `format`). The new message correctly employs `String.format` for placeholder replacement, although it appears inlined as `format`, which assumes that import or method context handles this.

There is no logical or functional modification beyond this change in the error message, aligning perfectly with the commit message that explicitly describes this as a fix to the error message itself.

Thus, the changes do not address any underlying logic or error in the code apart from improving the messaging. Therefore, the conclusion is **NotBuggy** because the change is purely about improving information conveyance rather than fixing functionality."
presto,5480.json,3d776ab3a9b34d155caa2097ed5e0a82c3f9ca39,"@@ -1,6 +1,7 @@
     private static Class<? extends InputFormat<?, ?>> getInputFormatClass(JobConf conf, String inputFormatName)
             throws ClassNotFoundException
     {
         Class<?> clazz = conf.getClassByName(inputFormatName);
-        return (Class<? extends InputFormat<?, ?>>) clazz.asSubclass(InputFormat.class);
+        // TODO: remove redundant cast to Object after IDEA-118533 is fixed
+        return (Class<? extends InputFormat<?, ?>>) (Object) clazz.asSubclass(InputFormat.class);
     }",Buggy,"Fix bogus error message in IDEA 13
",NotBuggy,"The change adds a cast to `Object` before casting `clazz.asSubclass(InputFormat.class)` to `Class<? extends InputFormat<?, ?>>`. This likely addresses a problematic compile-time error in IDEA 13 (likely related to type inference or casting behavior), even if the previous code could have been theoretically correct. The commit message refers to a bogus error message, not a functional bug. Since this doesn't affect runtime logic and aims to mitigate an error message issue, it doesn't represent a bug fix in code execution.

Hence, the answer is **NotBuggy**."
presto,14661.json,a5a04929e51756cac167c14cf73399a0df74f4c5,"@@ -1,4 +1,5 @@
     public static long valueAtQuantileBigint(@SqlType(""qdigest(bigint)"") Slice input, @SqlType(StandardTypes.DOUBLE) double quantile)
     {
+        checkCondition(quantile >= 0 && quantile <= 1, INVALID_FUNCTION_ARGUMENT, ""Quantile should be within bounds [0, 1], was: "" + quantile);
         return new QuantileDigest(input).getQuantile(quantile);
     }",Buggy,"Fix invalid quantile to be a user error
",Buggy,"The provided diff shows the addition of a condition to check that the `quantile` parameter falls within the range [0, 1]. If the `quantile` value is out of this range, an exception with the error code `INVALID_FUNCTION_ARGUMENT` is thrown, and a descriptive error message is provided. Prior to this change, if an invalid quantile was provided, it could lead to undesirable behavior or errors somewhere within the `getQuantile` method or downstream from it without a clear user-facing message or indication of what went wrong.

Analyzing step by step:

1. **Addition of Validation**: The addition of `checkCondition` introduces a precondition to ensure `quantile` is within acceptable bounds ([0, 1]). This is crucial because quantiles inherently must be within this range to be meaningful.

2. **Improvement in Error Reporting**: The change in the code provides a clearer, more descriptive error message to the user if the input is not valid, categorizing it as a user error (`INVALID_FUNCTION_ARGUMENT`).

3. **Commit Message Relevance**: The commit message states, ""Fix invalid quantile to be a user error,"" which aligns perfectly with the code change that converts potential generic errors or incorrect behavior due to invalid input into a well-defined user error.

4. **Indication of a Bug Fix**: The change prevents logical errors when the quantile is not valid by checking and handling the situation explicitly. The invalid quantile might have led to incorrect functionality or crashes previously, which this change aims to address. This aligns with the introduction of error handling capability as a bug fix.

Therefore, based on the above analysis, the change addresses a specific bug related to input validation and handling, translating it into a user error, which is intended by the commit message. The conclusion is **Buggy**."
presto,2512.json,ff82d4c25dcaeb742f1eb3470a04e89a8d2ede7b,"@@ -1,5 +1,5 @@
     public Block getRegion(int positionOffset, int length)
     {
         checkValidRegion(positionCount, positionOffset, length);
-        return new DictionaryBlock(idsOffset + positionOffset, length, dictionary, ids, false, randomDictionaryId());
+        return new DictionaryBlock(idsOffset + positionOffset, length, getDictionary(), ids, false, getDictionarySourceId());
     }",Buggy,"Fix bug in DictionaryBlock getRegion
",Buggy,"The commit message states that the change fixes a bug in the `DictionaryBlock`'s `getRegion` function. The modification in the code alters the arguments passed to the `DictionaryBlock` constructor:

1. `dictionary` was replaced with `getDictionary()`.
2. `randomDictionaryId()` was replaced with `getDictionarySourceId()`.

These modifications suggest a fix for potentially incorrect dictionary or identifier usage in the original implementation. Using `getDictionary()` instead of a possibly outdated instance variable ensures the most accurate dictionary state, and `getDictionarySourceId()` might provide a stable and correct identifier over using a random one, which can be critical for position tracking or block identity across different operations.

The changes align well with the intent to fix a bug mentioned in the commit message. Therefore, the conclusion is **Buggy**."
presto,22974.json,eb4e582942793631a8de8cf6e6e1c06ce3053d75,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = factories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(delegate.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in SessionPropertyDefaults
",NotBuggy,"The diff modifies a method to improve the error message by adding the missing parameter (`name`) to the `checkState` method. This will provide more details in the error message, specifically indicating which session property configuration manager is unregistered. This change involves enhancing an error message, improving debugging, but it does not indicate a fix to incorrect behavior in logic. Given that the commit message specifically mentions fixing an ""error message,"" and the diff aligns with enhancing the message, it primarily focuses on improving output clarity rather than resolving incorrect logic or behavior.

Therefore, although the change is beneficial for clarity, it doesn't fix a bug in functionality; it fixes an error message. Thus, I will assess this change as **NotBuggy**."
presto,22974.json,59d297913ff6641b852bd236161f237a3f9e3af7,"@@ -1,8 +1,8 @@
     public void setConfigurationManager(String name, Map<String, String> properties)
     {
         SessionPropertyConfigurationManagerFactory factory = sessionPropertyConfigurationManagerFactories.get(name);
-        checkState(factory != null, ""Session property configuration manager %s is not registered"");
+        checkState(factory != null, ""Session property configuration manager '%s' is not registered"", name);
 
         SessionPropertyConfigurationManager manager = factory.create(properties, configurationManagerContext);
         checkState(sessionPropertyConfigurationManager.compareAndSet(null, manager), ""sessionPropertyConfigurationManager is already set"");
     }",Buggy,"Fix error message in QuerySessionSupplier
",NotBuggy,"The change in the code replaces a placeholder in an error message with assignment of a variable (`name`), which suggests the intention to provide more specific feedback upon failure. Originally, the error message lacked the specific session property configuration manager name, which could lead to confusion when diagnosing issues.

While the commit message states ""Fix error message"" which is accurately reflected in the change, it does not directly indicate a bug in terms of functionality. Instead, it enhances clarity and aids debugging. The error-handling improvement is related to communication rather than correcting a logical flaw or improper behavior in code execution, as the functionality of the method remains unchanged.

Given this analysis, the change aligns with the commit message, but it involves improving the error message for better clarity rather than fixing a bug. Thus, the conclusion is **NotBuggy**."
presto,31633.json,318f9bf12b4402074f5f8caf0845a121c026f0df,"@@ -1,8 +1,8 @@
     public Object getObjectValue(ConnectorSession session, Block block, int position)
     {
         if (block.isNull(position)) {
             return null;
         }
 
-        return BingTile.decode(block.getLong(0, 0));
+        return BingTile.decode(block.getLong(position, 0));
     }",Buggy,"Fix getObjectValue in BingTileType

This change fixes a bug in decoding arrays of bing tiles.
",Buggy,"The commit message claims to fix a bug in decoding arrays of Bing tiles. The code change modifies the `getObjectValue` method to correctly pass the `position` parameter when calling `block.getLong()`, instead of hardcoding `0` as the position index.

### Analysis:
- **Original Code Issue**: The original code always uses `block.getLong(0, 0)` to decode the tile, which only works if the position is always zero. This approach would incorrectly decode or fail to decode Bing tiles if an array or block contains multiple entries, and a position other than zero is provided.
- **Modification**: The change replaces the fixed position `0` with the `position` argument of the method. This allows `getLong(position, 0)` to correctly fetch the long value from any specified position in the block.
- **Effect**: This change ensures that the method `getObjectValue` correctly retrieves and decodes Bing tiles from any given position within the block, rather than assuming the first position. This certainly fixes the described bug.

The change described in the commit message accurately reflects the modification in the code, aligning with a bug fix in handling decoding of Bing tiles from different positions in an array or block.

**Conclusion**: The changes indicate a bug fix in the handling of array positions, as described in the commit message. Therefore, the conclusion is **Buggy**."
netty,5669.json,b39ffed042844adecaf0a4fc4e9a2f53edaa111d,"@@ -1,15 +1,15 @@
         private void record(int actualReadBytes) {
-            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) {
+            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT)]) {
                 if (decreaseNow) {
                     index = max(index - INDEX_DECREMENT, minIndex);
                     nextReceiveBufferSize = SIZE_TABLE[index];
                     decreaseNow = false;
                 } else {
                     decreaseNow = true;
                 }
             } else if (actualReadBytes >= nextReceiveBufferSize) {
                 index = min(index + INDEX_INCREMENT, maxIndex);
                 nextReceiveBufferSize = SIZE_TABLE[index];
                 decreaseNow = false;
             }
         }",Buggy,"Fix incorrect calculation of next buffer size in AdaptiveRecvByteBufAllocator (#9555)

Motivation:

Due a bug we did not always correctly calculate the next buffer size in AdaptiveRecvByteBufAllocator.

Modification:

Fix calculation and add unit test

Result:

Correct calculation is always used. 
",Buggy,"The diff modifies the condition that checks whether `actualReadBytes` is less than or equal to an entry in `SIZE_TABLE`. The original code seems to have a fencepost error, as it decrements the index twice when checking the size table (`index - INDEX_DECREMENT - 1`). This could have led to an incorrect assessment of whether the buffer size needs adjustment, resulting in erroneous buffer size calculations.

By changing this condition to `index - INDEX_DECREMENT`, the calculation now correctly references the appropriate size in the `SIZE_TABLE` when deciding to reduce the buffer size. This correction aligns with the commit message's claim of fixing an incorrect buffer size calculation.

Additionally, the commit message indicates that a unit test was added to ensure correctness, although it isn't shown in the provided diff. This indicates a proactive step to prevent future regressions.

Given this analysis, the code change does address an incorrect calculation and aligns with the description provided in the commit message, indicating a resolved bug in the logic for determining the buffer size.

The conclusion is **Buggy**."
netty,10635.json,799350c369e68462b61c6aef97db2a33ea937434,"@@ -1,63 +1,67 @@
     public void updateDependencyTree(int childStreamId, int parentStreamId, short weight, boolean exclusive) {
         if (weight < MIN_WEIGHT || weight > MAX_WEIGHT) {
             throw new IllegalArgumentException(String.format(
                     ""Invalid weight: %d. Must be between %d and %d (inclusive)."", weight, MIN_WEIGHT, MAX_WEIGHT));
         }
         if (childStreamId == parentStreamId) {
             throw new IllegalArgumentException(""A stream cannot depend on itself"");
         }
 
         State state = state(childStreamId);
         if (state == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             state = new State(childStreamId);
             stateOnlyRemovalQueue.add(state);
             stateOnlyMap.put(childStreamId, state);
         }
 
         State newParent = state(parentStreamId);
         if (newParent == null) {
             // If there is no State object that means there is no Http2Stream object and we would have to keep the
             // State object in the stateOnlyMap and stateOnlyRemovalQueue. However if maxStateOnlySize is 0 this means
             // stateOnlyMap and stateOnlyRemovalQueue are empty collections and cannot be modified so we drop the State.
             if (maxStateOnlySize == 0) {
                 return;
             }
             newParent = new State(parentStreamId);
             stateOnlyRemovalQueue.add(newParent);
             stateOnlyMap.put(parentStreamId, newParent);
+            // Only the stream which was just added will change parents. So we only need an array of size 1.
+            List<ParentChangedEvent> events = new ArrayList<ParentChangedEvent>(1);
+            connectionState.takeChild(newParent, false, events);
+            notifyParentChanged(events);
         }
 
         // if activeCountForTree == 0 then it will not be in its parent's pseudoTimeQueue and thus should not be counted
         // toward parent.totalQueuedWeights.
         if (state.activeCountForTree != 0 && state.parent != null) {
             state.parent.totalQueuedWeights += weight - state.weight;
         }
         state.weight = weight;
 
         if (newParent != state.parent || (exclusive && newParent.children.size() != 1)) {
             final List<ParentChangedEvent> events;
             if (newParent.isDescendantOf(state)) {
                 events = new ArrayList<ParentChangedEvent>(2 + (exclusive ? newParent.children.size() : 0));
                 state.parent.takeChild(newParent, false, events);
             } else {
                 events = new ArrayList<ParentChangedEvent>(1 + (exclusive ? newParent.children.size() : 0));
             }
             newParent.takeChild(state, exclusive, events);
             notifyParentChanged(events);
         }
 
         // The location in the dependency tree impacts the priority in the stateOnlyRemovalQueue map. If we created new
         // State objects we must check if we exceeded the limit after we insert into the dependency tree to ensure the
         // stateOnlyRemovalQueue has been updated.
         while (stateOnlyRemovalQueue.size() > maxStateOnlySize) {
             State stateToRemove = stateOnlyRemovalQueue.poll();
             stateToRemove.parent.removeChild(stateToRemove);
             stateOnlyMap.remove(stateToRemove.streamId);
         }
     }",Buggy,"Fix HTTP/2 dependency tree corruption

Motivation:

Chrome was randomly getting stuck loading the tiles examples.
Investigation showed that the Netty flow controller thought it had
nothing to send for the connection even though some streams has queued
data and window available.

Modifications:

Fixed an accounting error where an implicitly created parent was not
being added to the dependency tree, thus it and all of its children were
orphaned from the connection's tree and would never have data written.

Result:

Fixes #6621
",Buggy,"The commit message describes fixing a bug related to HTTP/2 dependency tree corruption where streams with queued data were not being processed correctly due to an erroneous dependency tree structure.

Analyzing the git diff:

1. **Accounting Error Fix**: The code changes address an error where an implicitly created parent was not added to the dependency tree. By adding this parent to the tree (`connectionState.takeChild(newParent, false, events);`), the link between the parent and child in the dependency tree structure is established, which was missing prior to this change.

2. **Notification**: After adding the parent, the code now calls `notifyParentChanged(events);` to update relevant parties about the new dependency tree configuration. This is important for maintaining consistency within the system.

3. **List of Events**: The modification introduces a `ParentChangedEvent` list, which facilitates the tracking of changes as new parents are added.

These changes directly resolve the issue described in the commit message: by ensuring that new parent nodes are integrated into the tree, it prevents their children from being orphaned and ensures queued data can be processed correctly, fixing the dependency tree corruption.

Thus, the modifications align well with the commit message and represent a bug fix regarding the HTTP/2 dependency tree. Therefore, the conclusion is **Buggy**."
netty,536.json,6fc7c589f023b069ac5f4df8a6766f0b501487fb,"@@ -1,10 +1,11 @@
         DatagramPacket newDatagramPacket(ByteBuf buffer, InetSocketAddress localAddress) throws UnknownHostException {
             final InetAddress address;
-            if (scopeId != 0) {
-                address = Inet6Address.getByAddress(null, addr, scopeId);
+            if (addrLen == ipv4Bytes.length) {
+                System.arraycopy(addr, 0, ipv4Bytes, 0, addrLen);
+                address = InetAddress.getByAddress(ipv4Bytes);
             } else {
-                address = InetAddress.getByAddress(addr);
+                address = Inet6Address.getByAddress(null, addr, scopeId);
             }
             return new DatagramPacket(buffer.writerIndex(count),
                     localAddress, new InetSocketAddress(address, port));
         }",Buggy,"Correctly handle ipv6 mapped ipv4 addresses when using recvmmsg (#9541)


Motivation:

394a1b3485000c211595aff7495c4f863972af29 introduced the possibility to use recvmmsg(...) but did not correctly handle ipv6 mapped ip4 addresses to make it consistent with other transports.

Modifications:

- Correctly handle ipv6 mapped ipv4 addresses by only copy over the relevant bytes
- Small improvement on how to detect ipv6 mapped ipv4 addresses by using memcmp and not byte by byte compare
- Adjust test to cover this bug

Result:

Correctly handle ipv6 mapped ipv4 addresses
",Buggy,"The commit message clearly describes the changes, which involve correcting handling of IPv6 mapped IPv4 addresses. Here's how the changes align with the message:

1. **Bug Fix Identification**:
   - The commit message indicates that previously introduced functionality did not correctly handle IPv6 mapped IPv4 addresses consistently.
   - The code change addresses this issue by distinguishing between IPv4 and IPv6 mapped addresses.

2. **Code Changes**:
   - The code now checks if `addrLen` equals the length of `ipv4Bytes`. If so, it copies the address bytes into `ipv4Bytes` and creates an `InetAddress` instead of an `Inet6Address`, which ensures only the relevant bytes are carried over for IPv4.
   - The fallback unchanged logic uses `Inet6Address` with a scope ID if it matches an IPv6 pattern.

3. **Improvements**:
   - The commit mentions using `memcmp` for better detection, indicating a performance or logical enhancement compared to byte-by-byte comparison.

4. **Test Adjustments**:
   - The commit message notes test adjustments to ensure that this bug is covered, although specific test changes are not visible in the provided diff.

These changes directly address the issue mentioned in the commit message by updating the behavior of how IPv6 mapped IPv4 addresses are handled, thereby correcting a bug.

Given this analysis, the changes implemented in the commit align with the commit message, indicating a fix intended to correct a misbehavior regarding IPv6 mapped IPv4 addresses.

The conclusion is **Buggy**."
netty,15758.json,f0a3f849f7d0e028eb97e672a3cbf52648b98617,"@@ -1,26 +1,27 @@
     public ByteBuf writeZero(int length) {
         if (length == 0) {
             return this;
         }
 
+        ensureWritable(length);
         checkIndex(writerIndex, length);
 
         int nLong = length >>> 3;
         int nBytes = length & 7;
         for (int i = nLong; i > 0; i --) {
             writeLong(0);
         }
         if (nBytes == 4) {
             writeInt(0);
         } else if (nBytes < 4) {
             for (int i = nBytes; i > 0; i --) {
                 writeByte((byte) 0);
             }
         } else {
             writeInt(0);
             for (int i = nBytes - 4; i > 0; i --) {
                 writeByte((byte) 0);
             }
         }
         return this;
     }",Buggy,"Fix a bug in AbstractByteBuf.writeZero() where the capacity is not auto-expanded
",Buggy,"The commit message indicates a fix for a bug in the `AbstractByteBuf.writeZero()` method where the buffer's capacity wasn't auto-expanded. Looking at the diff, the change introduces a call to `ensureWritable(length)` at the start of the method, which likely checks the buffer's ability to accommodate the specified number of zero bytes and expands it if necessary before writing.

This modification addresses the problem of potential buffer overflow or unsuccessful writes when the buffer doesn’t have enough space for the intended operation. Without this fix, writing zeros could fail or lead to unexpected behavior if the buffer didn't expand to accommodate the specified length.

Thus, this change aligns with the commit message, which describes the bug fix of automatically expanding the buffer's capacity as needed. The introduction of `ensureWritable(length)` is a clear indication of error avoidance and operational correctness improvement.

Hence, the changes in this commit can be concluded to be all about fixing a bug. Therefore, the conclusion should be **Buggy**."
netty,14984.json,dcc39e5b21eda7bdd9563ed86253a693923f6432,"@@ -1,13 +1,13 @@
     protected String format(ChannelHandlerContext ctx, String eventName, Object firstArg, Object secondArg) {
         if (secondArg == null) {
             return formatSimple(ctx, eventName, firstArg);
         }
 
         String chStr = ctx.channel().toString();
         String arg1Str = String.valueOf(firstArg);
         String arg2Str = secondArg.toString();
         StringBuilder buf = new StringBuilder(
-                chStr.length() + 1 + eventName + 2 + arg1Str.length() + 2 + arg2Str.length());
+                chStr.length() + 1 + eventName.length() + 2 + arg1Str.length() + 2 + arg2Str.length());
         buf.append(chStr).append(' ').append(eventName).append("": "").append(arg1Str).append("", "").append(arg2Str);
         return buf.toString();
     }",Buggy,"Fixes a LoggingHandler#format method with two arguments

Motivation:
Bug in capacity calculation: occurs auto convert to string instead of sum up.

Modifications:
Use `eventName.length()` in sum.

Result:
Less trash in logs.
",Buggy,"The commit message indicates there was a bug involving the wrong capacity calculation for a `StringBuilder` due to mistakenly adding `eventName` directly rather than its length. This could result in incorrect buffer sizing and potentially inefficient memory allocation. The fix correctly uses `eventName.length()` for the capacity calculation, aligning the code with the intended logic and the commit message. This change directly addresses a bug in how string concatenation capacity was calculated. Therefore, the changes indicate a bug fix.

**Buggy**"
netty,2881.json,338e1a991c5d15695bd654a7b8d2bd4008e868ae,"@@ -1,51 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
                         // After setting the state to NONE, look at the tasks queue one more time.
                         // If it is empty, then we can return from this method.
                         // Otherwise, it means the producer thread has called execute(Runnable)
                         // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
                         // There are two possible scenarios when this happen
                         //
                         // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
                         //    is successfully setting the state to SUBMITTED. This mean the producer
                         //    will call / has called executor.execute(this). In this case, we can just return.
                         // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
                         //    returns false. In this case, the producer thread won't call executor.execute.
                         //    In this case, we need to change the state to RUNNING and keeps running.
                         //
                         // The above cases can be distinguished by performing a
                         // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
-                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                        if (tasks.isEmpty() || !state.compareAndSet(NONE, RUNNING)) {
                             return; // done
                         }
                     }
                 }
             }
         }",Buggy,"Fix a bug introduced by 79706357c73ded02615d0445db7503b646ff9547 which can cause thread to spin in an infinite loop. (#9579)

Motivation:
peek() is implemented in a similar way to poll() for the mpsc queue, thus it is more like a consumer call.
It is possible that we could have multiple thread call peek() and possibly one thread calls poll() at at the same time.
This lead to multiple consumer scenario, which violates the multiple producer single consumer condition and could lead to spin in an infinite loop in peek()

Modification:
Use isEmpty() instead of peek() to check if task queue is empty

Result:
Dont violate the mpsc semantics.
",Buggy,"The commit message clearly states that there is a bug in the current implementation that can cause a thread to enter an infinite loop due to improper handling of ""peek()"" in a multiple producer single consumer (MPSC) queue scenario. This leads to potential violations of MPSC semantics when multiple threads call ""peek()"" along with one calling ""poll()"". 

### Code Analysis

1. **Original Code**: The condition `if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING))` involves ""peek()"". The behavior of ""peek()"" in a multi-threaded environment can introduce incorrect assumptions about queue emptiness when another thread might concurrently modify the queue.

2. **Modification**: The modification changes the condition to `if (tasks.isEmpty() || !state.compareAndSet(NONE, RUNNING))`. The ""isEmpty()"" method is a safer check in the context of this problem because it directly checks the queue's emptiness without exposing the intermediate value that ""peek()"" might return, which aligns with ensuring consumer semantics are not violated.

3. **Problem Description**: By using ""isEmpty()"", the code avoids the spin in an infinite loop caused by the peek operation, which is prone to race conditions.

4. **Semantics and Correctness**: The change ensures that the consumer semantics of the queue (checking emptiness) are respected, which is crucial for preventing infinite loops and maintaining proper task processing by correctly identifying when tasks have been exhausted.

The change directly addresses a logical flaw that could lead to incorrect and undesired behavior (infinite loop), as described in both the commit message and the corresponding code logic. 

### Conclusion
Given these points, the change aligns with the commit message to fix a bug involving potential infinite loop scenarios due to incorrect usage of ""peek()"". This correction ensures that MPSC queue semantics are upheld. Therefore, the commit is indeed a bug fix.

**Buggy**"
netty,2881.json,79706357c73ded02615d0445db7503b646ff9547,"@@ -1,34 +1,51 @@
         public void run() {
             if (!state.compareAndSet(SUBMITTED, RUNNING)) {
                 return;
             }
             for (;;) {
                 int i = 0;
                 try {
                     for (; i < maxTaskExecutePerRun; i++) {
                         Runnable task = tasks.poll();
                         if (task == null) {
                             break;
                         }
                         safeExecute(task);
                     }
                 } finally {
                     if (i == maxTaskExecutePerRun) {
                         try {
                             state.set(SUBMITTED);
                             executor.execute(this);
                             return; // done
                         } catch (Throwable ignore) {
                             // Reset the state back to running as we will keep on executing tasks.
                             state.set(RUNNING);
                             // if an error happened we should just ignore it and let the loop run again as there is not
                             // much else we can do. Most likely this was triggered by a full task queue. In this case
                             // we just will run more tasks and try again later.
                         }
                     } else {
                         state.set(NONE);
-                        return; // done
+                        // After setting the state to NONE, look at the tasks queue one more time.
+                        // If it is empty, then we can return from this method.
+                        // Otherwise, it means the producer thread has called execute(Runnable)
+                        // and enqueued a task in between the tasks.poll() above and the state.set(NONE) here.
+                        // There are two possible scenarios when this happen
+                        //
+                        // 1. The producer thread sees state == NONE, hence the compareAndSet(NONE, SUBMITTED)
+                        //    is successfully setting the state to SUBMITTED. This mean the producer
+                        //    will call / has called executor.execute(this). In this case, we can just return.
+                        // 2. The producer thread don't see the state change, hence the compareAndSet(NONE, SUBMITTED)
+                        //    returns false. In this case, the producer thread won't call executor.execute.
+                        //    In this case, we need to change the state to RUNNING and keeps running.
+                        //
+                        // The above cases can be distinguished by performing a
+                        // compareAndSet(NONE, RUNNING). If it returns ""false"", it is case 1; otherwise it is case 2.
+                        if (tasks.peek() == null || !state.compareAndSet(NONE, RUNNING)) {
+                            return; // done
+                        }
                     }
                 }
             }
         }",Buggy,"Fix race condition in the NonStickyEventExecutorGroup (#8232)

Motivation:

There was a race condition between the task submitter and task executor threads such that the last Runnable submitted may not get executed. 

Modifications:

The bug was fixed by checking the task queue and state in the task executor thread after it saw the task queue was empty.

Result:

Fixes #8230",Buggy,"The commit message indicates that a race condition was fixed in `NonStickyEventExecutorGroup`. Let's analyze the changes to see if they address the issue as described:

1. **Old Behavior:**
   - The code sets the state to `NONE` after finishing task execution (`tasks.poll()` loop).
   - If a new task was enqueued between the poll and state reset, it could potentially stay unexecuted.

2. **New Behavior:**
   - After setting the state to `NONE`, the code now checks the task queue (`tasks.peek()`) and uses `compareAndSet(NONE, RUNNING)` to determine if there is a discrepancy.
   - If tasks were added and the state is still NONE, it changes the state back to RUNNING, allowing the loop to continue fetching.

3. **Scenarios Addressed:**
   - **Case 1:** Another thread successfully sets the state to SUBMITTED. The check `compareAndSet(NONE, RUNNING)` returns false, meaning execution can safely stop because the executor will run the task.
   - **Case 2:** No other thread updated the state, and tasks exist, the state is set to RUNNING, continuing task execution to ensure the last submitted task is run.

The change directly addresses the race condition by ensuring that any new task added after the poll does not remain unexecuted, aligning with the commit message. Therefore, this change indeed fixes a bug related to concurrency.

The conclusion is **Buggy** because the changes correct a race condition, ensuring no tasks are skipped inadvertently."
netty,17549.json,20894bc99e28191cc3926ae95a23f5c7d3969a33,"@@ -1,4 +1,3 @@
     public ByteBuffer internalNioBuffer(int index, int length) {
-        // Do not mess with the internal buffer's byte order.
-        return buf.nioBuffer(index, length).order(order);
+        return buf.internalNioBuffer(index, length).duplicate().order(order);
     }",Buggy,"Fix a bug in internalNioBuffer() implementations of derived buffers

- A user can create multiple duplicates of a buffer and access their internal NIO buffers. (e.g. write multiple duplicates to multiple channels assigned to different event loop.)  Because the derived buffers' internalNioBuffer() simply delegates the call to the original buffer, all derived buffers and the original buffer's internalNioBuffer() will return the same buffer, which will lead to a race condition.
- Fixes #1739
",Buggy,"The commit message describes a concurrency issue with the internalNioBuffer() method in derived buffers that can lead to a race condition. The diff shows that this method now duplicates the buffer returned by internalNioBuffer().

Here's how the change relates to the bug:

1. **Original Issue**: The issue was that multiple duplicates of a buffer shared the same internal NIO buffer, leading to potential race conditions, as they could be modified concurrently when used across different threads or event loops.

2. **Code Change**: By calling `.duplicate()`, the fix ensures each derived buffer gets a distinct duplicate of the ByteBuffer, not affecting the original buffer. This action prevents shared access to the same buffer instance, thereby eliminating the race condition described.

3. **Commit Message Alignment**: The commit message aligns with the code changes, describing the specific problem (race condition due to shared buffers) and referencing a related issue number (#1739). The change directly addresses the described problem by providing separate buffer instances.

Given the clear alignment and explanation in the commit message for a concurrency-related bug fix, the conclusion is **Buggy**."
netty,2272.json,cd3254df88b60476dc04b39915d3d70c200eb6f4,"@@ -1,6 +1,7 @@
         private static void reachabilityFence0(Object ref) {
             if (ref != null) {
-                // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
-                synchronized (ref) { }
+                synchronized (ref) {
+                    // Empty synchronized is ok: https://stackoverflow.com/a/31933260/1151521
+                }
             }
         }",Buggy,"Update to new checkstyle plugin (#8777) (#8780)

Motivation:

We need to update to a new checkstyle plugin to allow the usage of lambdas.

Modifications:

- Update to new plugin version.
- Fix checkstyle problems.

Result:

Be able to use checkstyle plugin which supports new Java syntax.",NotBuggy,"The presented commit message details an update to a checkstyle plugin and the resolution of subsequent checkstyle issues. The diff indicates a formatting change in which a comment has been repositioned within an empty synchronized block. This alteration appears to address a layout warning or requirement from the updated checkstyle plugin, rather than aiming to correct a logical error or defect in the code.

The update resolves a checkstyle formatting problem but does not indicate a bug fix related to software behavior or functionality. The change is a matter of code style consistency as per the checkstyle plugin's guidelines, which supports the claim in the commit message that checkstyle problems were fixed.

In conclusion, this commit does not indicate a bug fix. It rather involves style-related adjustments to ensure compliance with the updated plugin's rules. Thus, the assessment of this change is **NotBuggy**."
netty,1691.json,32746c53c1089550f79bd2d6eeba4d65dab1f2d3,"@@ -1,24 +1,24 @@
-    static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
+    private static byte padWithZeros(byte b, int lowOrderBitsToPreserve) {
         switch (lowOrderBitsToPreserve) {
         case 0:
             return 0;
         case 1:
-            return (byte) (0x01 & b);
+            return (byte) (0x80 & b);
         case 2:
-            return (byte) (0x03 & b);
+            return (byte) (0xC0 & b);
         case 3:
-            return (byte) (0x07 & b);
+            return (byte) (0xE0 & b);
         case 4:
-            return (byte) (0x0F & b);
+            return (byte) (0xF0 & b);
         case 5:
-            return (byte) (0x1F & b);
+            return (byte) (0xF8 & b);
         case 6:
-            return (byte) (0x3F & b);
+            return (byte) (0xFC & b);
         case 7:
-            return (byte) (0x7F & b);
+            return (byte) (0xFE & b);
         case 8:
             return b;
         default:
             throw new IllegalArgumentException(""lowOrderBitsToPreserve: "" + lowOrderBitsToPreserve);
         }
     }",Buggy,"EDNS Client Subnet is not encoded correctly when source prefix length is not a multiple of 8.

Motivation:
When the ECS source prefix length is not a mutiple of 8, the last byte the address inside the
ECS OPT record is not padded properly.

Modifications:
DefaultDnsRecordEncoder.padWithZeros(...) was modified to add padding from the least
significant bits.

Result:
ECS encoding bug fixed.
",Buggy,"The commit message clearly describes an error in encoding when the source prefix length isn't a multiple of 8, particularly affecting how the last byte is encoded. The diff modifies how bits are masked to pad bytes, ensuring padding occurs from the most significant bits, which aligns with the problem described.

Here is a breakdown of the changes made:

1. **Visibility Change**: The method `padWithZeros` was changed from `static` to `private static`. This is a scope adjustment and does not relate directly to the bug fix but may intend to encapsulate functionality within the class.

2. **Bitmask Modifications**: Each case in the switch statement updates the bitmask applied to the byte `b`:

   - Original version applies masks like `0x01`, `0x03`, `0x07`, ..., preserving low-order bits.
   - Updated version applies masks like `0x80`, `0xC0`, `0xE0`, ..., which preserve the higher-order bits.

   This change addresses the highlighted issue in the commit message, where padding wasn't occurring properly. By shifting the mask towards the higher-order bits, the encoding now correctly handles cases where the prefix length isn't precisely byte-aligned.

3. **Alignment with Commit Message**: The changes directly target the bug described in the commit message by modifying the padding logic to correctly account for bit significance.

Given these points, this diff addresses a tangible bug with bitwise operations affecting encoding, and the changes align with the problem described in the commit message. Therefore, the conclusion is **Buggy**."
netty,13070.json,a29532df43c9db08db5dbc83f30bb7bd2e55a596,"@@ -1,12 +1,12 @@
     private static Method updateByteBuffer(Checksum checksum) {
         if (PlatformDependent.javaVersion() >= 8) {
             try {
                 Method method = checksum.getClass().getDeclaredMethod(""update"", ByteBuffer.class);
-                method.invoke(method, ByteBuffer.allocate(1));
+                method.invoke(checksum, ByteBuffer.allocate(1));
                 return method;
             } catch (Throwable ignore) {
                 return null;
             }
         }
         return null;
     }",Buggy,"Fix ByteBufChecksum optimisation for CRC32 and Adler32 (#9242)

Motivation:

Because of a simple bug in ByteBufChecksum#updateByteBuffer(Checksum),
ReflectiveByteBufChecksum is never used for CRC32 and Adler32, resulting
in direct ByteBuffers being checksummed byte by byte, which is
undesriable.

Modification:

Fix ByteBufChecksum#updateByteBuffer(Checksum) method to pass the
correct argument to Method#invoke(Checksum, ByteBuffer).

Result:

ReflectiveByteBufChecksum will now be used for Adler32 and CRC32 on
Java8+ and direct ByteBuffers will no longer be checksummed on slow
byte-by-byte basis.",Buggy,"The commit message states a bug fix in `ByteBufChecksum#updateByteBuffer(Checksum)`, which prevented `ReflectiveByteBufChecksum` from being utilized effectively. The diff shows the code changing the invocation to use the correct instance (`checksum` instead of `method`). The initial code attempted to invoke `method` on itself, incorrect for Java reflection. Fixing this allows proper usage for CRC32/Adler32 buffer checksums, enhancing performance by avoiding byte-by-byte processing. Thus, the change correlates with the commit message, suggesting a genuine bug fix. The answer is **Buggy**."
netty,239.json,1cc104e1c02f5ee54979d8dfcba5aa4fb20d3503,"@@ -1,3 +1,3 @@
-    public static SctpServerChannel open() {
+    public static SctpServerChannel open() throws IOException {
         return null;
     }",Buggy,"Fix a compilation error
",Buggy,"The diff shows that the `open()` method signature has been changed to include `throws IOException`. This adjustment addresses a compilation error likely caused by the need for the `open()` method to handle potential `IOException` that might occur, which was not properly declared before. Including the throws clause rectifies this oversight by explicitly signaling that the method can produce an `IOException`, thereby preventing the compilation error related to unchecked exceptions or unhandled exceptions.

The alignment between the commit message and the change is direct, as adding the `throws IOException` declaration is necessary to eliminate a compilation error related to exception handling. This update directly addresses a compilation issue, which, while not always categorized solely as a bug, relates closely to resolving a defect in code compliance with the Java exception handling mechanism.

Taking the nature of Java exception handling into account, the correction implies a bug in exception declaration, reinforcing a proper API contract. Hence, the change aligns with fixing a technical error (compilation error) and facilitates type-safe exception handling.

Therefore, this change indicates a fix for a compilation error, which can be considered a defect resolution. Thus, we conclude with **Buggy**."
netty,1341.json,a4c96483d1e83981a1e0860d3a6f71fbe21d500c,"@@ -1,18 +1,20 @@
     private static boolean anyInterfaceSupportsIpV6() {
         try {
             Enumeration<NetworkInterface> interfaces = NetworkInterface.getNetworkInterfaces();
             while (interfaces.hasMoreElements()) {
                 NetworkInterface iface = interfaces.nextElement();
                 Enumeration<InetAddress> addresses = iface.getInetAddresses();
                 while (addresses.hasMoreElements()) {
-                    if (addresses.nextElement() instanceof Inet6Address) {
+                    InetAddress inetAddress = addresses.nextElement();
+                    if (inetAddress instanceof Inet6Address && !inetAddress.isAnyLocalAddress() &&
+                        !inetAddress.isLoopbackAddress() && !inetAddress.isLinkLocalAddress()) {
                         return true;
                     }
                 }
             }
         } catch (SocketException e) {
             logger.debug(""Unable to detect if any interface supports IPv6, assuming IPv4-only"", e);
             // ignore
         }
         return false;
     }",Buggy,"Fix a bug where making IPv6 DnsQuestion when it's not supported (#10170)

Motivation:
Related https://github.com/line/armeria/issues/2463
Here is an example that an NIC has only link local address for IPv6.
```
$ ipaddr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if18692: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1460 qdisc noqueue
    link/ether 1a:5e:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff
    inet 10.xxx.xxx.xxx/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::xxxx:xxxx:xxxx:xxxx/64 scope link
       valid_lft forever preferred_lft forever
```
If the NICs have only local or link local addresses, We should not send IPv6 DNS queris.

Modification:
- Ignore link-local IPv6 addresses which may exist even on a machine without IPv6 network.

Result:
- `DnsNameResolver` does not send DNS queries for AAAA when IPv6 is not available.",Buggy,"The commit message and the code changes both address the issue of incorrectly sending IPv6 DNS queries when only link-local addresses are present. The `anyInterfaceSupportsIpV6()` method previously returned `true` for any IPv6 address, including link-local and loopback addresses. The updated code specifically checks for these types of addresses using `!inetAddress.isAnyLocalAddress()`, `!inetAddress.isLoopbackAddress()`, and `!inetAddress.isLinkLocalAddress()`. By excluding these addresses, the change ensures that the method only returns `true` if an interface has a globally routable IPv6 address, thus preventing unnecessary and incorrect IPv6 DNS queries (AAAA).

This aligns well with the commit message, as the described bug is indeed about ensuring that AAAA queries aren't sent when IPv6 connectivity isn't actually supported beyond link-local. The change improves the conditional logic to correctly determine the availability of suitable IPv6 support.

Therefore, the changes made in the code correspond precisely to the issue described in the commit message, indicating a clear bug fix. Thus, the conclusion is **Buggy**."
netty,17654.json,c0bbde48b78163a394ae3fd63f98b6b6c56acb25,"@@ -1,3 +1,3 @@
     public ByteBuffer nioBuffer(int index, int length) {
-        return (ByteBuffer) buffer.duplicate().position(index).position(index + length);
+        return (ByteBuffer) buffer.duplicate().position(index).limit(index + length);
     }",Buggy,"[#1852] Fix bug in UnpooledDirectByteBuf.nioBuffer(...) implementation
",Buggy,"The change in the code is indicative of a bug fix related to the `UnpooledDirectByteBuf.nioBuffer(...)` method. Here's the step-by-step reasoning:

1. **Code Analysis**:
   - The method `nioBuffer` receives `index` and `length` as parameters.
   - The original code mistakenly sets the position twice: first to `index` and then updates it to `index + length`.
   - The corrected code sets the position to `index` and the limit to `index + length`.

2. **Understanding the Bug**:
   - In the original code, setting the position twice likely leads to an incorrectly sized slice of the buffer because the final position is not where the intended slice starts.
   - The correct approach involves setting the position to the start of the desired range (`index`) and then setting the limit to the end of the range (`index + length`), effectively slicing the buffer correctly from `index` to `index + length`.

3. **Commit Message**:
   - The message clearly indicates a bug fix within a specific method, `nioBuffer`, which is directly addressed by this diff.

4. **Logical Correction**:
   - The logical error in buffer manipulation is corrected. Now, the method behaves as expected, providing a correctly delimited section of the buffer. 

The changes directly address a logic error, aligning with the commit message that specifically mentions a bug fix in the implementation of `UnpooledDirectByteBuf.nioBuffer(...)`. Therefore, the conclusion is **Buggy**."
netty,12409.json,6983f704c0882dd322bc1cf0c7c1c635e4b05ea5,"@@ -1,3 +1,5 @@
     public ByteBuf readBytes(ByteBuf dst) {
-        throw new UnreplayableOperationException();
+        checkReadableBytes(dst.writableBytes());
+        buffer.readBytes(dst);
+        return this;
     }",Buggy,"Fix unreplayble error
",Buggy,"In this diff, an exception `UnreplayableOperationException` that was being thrown is replaced with a method call to `checkReadableBytes(dst.writableBytes())` followed by executing `buffer.readBytes(dst)`. This change indicates the following:

1. **Error Handling Improvement**: The original method was designed to throw an exception outright, indicating that the operation was previously considered not replayable or feasible under certain conditions. By removing this exception and adding `checkReadableBytes`, it suggests an intention to validate certain conditions (such as sufficient readable bytes) before proceeding with the read operation. This is a typical approach when fixing a bug related to incorrect handling of input or state.

2. **Correction for Unhandled Case**: The change shows an effort to ensure that a specific condition is checked (i.e., enough readable bytes), making the function's logic more robust and likely preventing the erroneous situation that was liable to cause the `UnreplayableOperationException`.

3. **Maintaining Operational Flow**: The method now completes successfully by continuing to read bytes into the destination `ByteBuf` and returning `this`. This suggests a move toward enabling the function as part of a fluent API pattern or to ensure the operation can complete under valid conditions.

Overall, the changes in the code address an operational bug by refining error handling and ensuring correct preconditions are checked before carrying out the operation. Hence, this aligns with a bug fix as described. Therefore, the appropriate conclusion here is **Buggy**."
netty,9557.json,97d07253e2933dbca0d08cd19072503506bda863,"@@ -1,12 +1,12 @@
     public static SpdySessionStatus valueOf(int code) {
         switch (code) {
         case 0:
             return OK;
         case 1:
             return PROTOCOL_ERROR;
-        case 11:
+        case 2:
             return INTERNAL_ERROR;
         }
 
         return new SpdySessionStatus(code, ""UNKNOWN ("" + code + ')');
     }",Buggy,"SPDY: fixed session status (internal error misdocumented)
",Buggy,"The commit message mentions a bug fix regarding a misdocumented internal error for a SPDY session status. The diff changes the switch case from `case 11:` to `case 2:` for the `INTERNAL_ERROR`. This modification corrects the mapping of a status code to the correct session status, which aligns perfectly with the commit message. Such a change indicates a logical correction in how session statuses are determined, thus fixing a bug.

The alteration ensures that the correct status code is used, likely addressing an issue where the wrong error status was being returned. This update is essential, particularly in protocols like SPDY where precise error handling is crucial for correct communication and troubleshooting.

Therefore, the code change indicates a bug fix. The appropriate conclusion is **Buggy**."
argouml,1941.json,045f33e22637950982cc1193584f80301860683e,"@@ -1,7 +1,7 @@
     public void notationTextChanged(NotationTextEvent event) {
         this.setText(event.getText());
         this.setUnderline(event.isUnderlined());
-        this.setUnderline(event.isBold());
-        this.setItalic(event.isBold());
+        this.setBold(event.isBold());
+        this.setItalic(event.isItalic());
         this.damage();
     }",Buggy,"Fix some cut and paste errors

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19262 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a fix for cut-and-paste errors. The diff changes incorrect method calls for setting text attributes. Initially, setUnderline(event.isBold()) mistakenly duplicates setBold's task, and setItalic(event.isBold()) misassigns 'bold' property to 'italic'. Each is replaced with correct operations—setBold(event.isBold()) and setItalic(event.isItalic())—matching intended behavior. Therefore, this direct mapping to the commit message showcases a logical correction addressing functional errors, confirming a bug-related change aligned with the provided commit message. Thus, the answer is **Buggy**."
argouml,1718.json,126f591fd6961fb320d035184050b65257130479,"@@ -1,6 +1,8 @@
     public boolean doesAccept(Object objectToAccept) {
         if (Model.getFacade().isAClassifier(objectToAccept)) {
             return true;
+        } else if (Model.getFacade().isAComment(objectToAccept)) {
+        	return true;
         }
         return false;
     }",Buggy,"Fixed a bug where comments could not be added in Seq2 diagrams.

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15998 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message states a bug fix related to comments in Seq2 diagrams. The provided code change involves modifying the `doesAccept` method to allow comments to be accepted by adding a condition to check if `objectToAccept` is a comment, using `Model.getFacade().isAComment(objectToAccept)`. This aligns with the commit message about enabling comments, suggesting the original omission prevented comments in Seq2 diagrams. Consequently, this change indeed addresses the bug specified in the commit message. Therefore, the conclusion is **Buggy**."
docx4j,8366.json,9f08fd2eac1d042d1fe3b2e70b2918f551ea677e,"@@ -1,50 +1,50 @@
     public static Document CxnSpToSVG(CxnSp cxnSp) {
     	
     	// Geometrical transforms
     	CTTransform2D xfrm = cxnSp.getSpPr().getXfrm();
     	Box b = new Box(xfrm.getOff().getX(), xfrm.getOff().getY(),
-    			xfrm.getExt().getCx(), xfrm.getExt().getCx() );
+    			xfrm.getExt().getCx(), xfrm.getExt().getCy() );
     	
     	if (xfrm.getRot()!=0) {
     		b.rotate(xfrm.getRot());
     	}
     	if (xfrm.isFlipH() ) {
     		b.flipH();
     	}
     	if (xfrm.isFlipV() ) {
     		b.flipV();
     	}
     	
     	// Convert from EMU to pixels
     	b.toPixels();
 
     	// Wrap in a div positioning it on the page
     	Document document = createDocument();
 		Element xhtmlDiv = document.createElement(""div"");
 		// Firefox needs the following; Chrome doesn't
 		xhtmlDiv.setAttribute(""style"", 
 				""position: absolute; width:100%; height:100%; left:0px; top:0px;"");		
 		Node n = document.appendChild(xhtmlDiv);
     	
     	// Convert the object itself to SVG
 		Svg svg = oFactory.createSvg();
     	Line line = oFactory.createLine();
     	svg.getSVGDescriptionClassOrSVGAnimationClassOrSVGStructureClass().add(line);
     	
     	line.setX1(b.getOffset().getXAsString() );
     	line.setY1(b.getOffset().getYAsString() );
     	
     	Point otherEnd = b.getOtherCorner();
     	
     	line.setX2( otherEnd.getXAsString() );
     	line.setY2( otherEnd.getYAsString() );
 
     	line.setStyle(""stroke:rgb(99,99,99)"");
     	// You can't see the line in Midori, unless you specify the color.
     	// width eg stroke-width:2 is optional
     	
     	Document d2 = XmlUtils.marshaltoW3CDomDocument(svg, jcSVG);   
     	XmlUtils.treeCopy(d2, n);
     	return document;
     	
     }",Buggy,"Fix typo bug.
",Buggy,"The commit message indicates a ""typo bug"" fix. In the code change, the line:

```java
Box b = new Box(xfrm.getOff().getX(), xfrm.getOff().getY(),
    			xfrm.getExt().getCx(), xfrm.getExt().getCx() );
```

is corrected to:

```java
Box b = new Box(xfrm.getOff().getX(), xfrm.getOff().getY(),
    			xfrm.getExt().getCx(), xfrm.getExt().getCy() );
```

The original code erroneously used `xfrm.getExt().getCx()` for both width and height parameters, effectively causing the width and height to be identical, which is likely not the intended behavior. By changing the second use of `xfrm.getExt().getCx()` to `xfrm.getExt().getCy()`, the code correctly represents both dimensions: width (Cx) and height (Cy).

This change aligns with the commit message indicating a ""typo bug,"" as the error appears to be a simple misplacement of a method call due to typographical mistake, and the fix resolves it.

Thus, the nature of the change indicates a correction of a defect in how dimensions were handled, implying a bug fix. The answer is **Buggy**."
docx4j,5289.json,e0638645ae07b75593d031844af2c84f5b7a4960,"@@ -1,50 +1,50 @@
 	public OpcPackage get() throws Docx4JException {
 		
 		long startTime = System.currentTimeMillis();				
 
 		// 1. Get [Content_Types].xml
 		ContentTypeManager ctm = new ContentTypeManager();
 		try {
 			InputStream is = partStore.loadPart(""[Content_Types].xml"");		
 			ctm.parseContentTypesFile(is);
 		} catch (Docx4JException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		} catch (NullPointerException e) {
 			throw new Docx4JException(""Couldn't get [Content_Types].xml from ZipFile"", e);
 		}
 		
 		// .. now find the name of the main part
 		String partName = ""_rels/.rels"";
 		RelationshipsPart rp = getRelationshipsPartFromZip(null, partName);
 		if (rp==null) {
 			throw new Docx4JException(""_rels/.rels appears to be missing from this package!"");
 		}
 		
 		String mainPartName = PackageRelsUtil.getNameOfMainPart(rp);
 		String pkgContentType = ctm.getContentType(new PartName(""/"" + mainPartName));
 
 		// 2. Create a new Package; this'll return the appropriate subclass
 		OpcPackage p = ctm.createPackage(pkgContentType);
 		log.info(""Instantiated package of type "" + p.getClass().getName() );
 		p.setPartStore(partStore);
 
 		p.setRelationships(rp);
 		rp.setSourceP(p); //
 		
 		// 5. Now recursively 
 //		(i) create new Parts for each thing listed
 //		in the relationships
 //		(ii) add the new Part to the package
 //		(iii) cross the PartName off unusedZipEntries
 		addPartsFromRelationships(p, rp, ctm );
 
 		// 6.
 		registerCustomXmlDataStorageParts(p);
 		
-		partStore.finishLoad();
+		partStore.finishSave();
 		
 		long endTime = System.currentTimeMillis();
 		log.info(""package read;  elapsed time: "" + Math.round((endTime-startTime)) + "" ms"" );
 		 
 		 return p;
 	}",Buggy,"Fix compilation errors in some classes
",NotBuggy,"The commit message indicates that the changes address compilation errors. Reviewing the diff, a significant change has been made: 

- The method call `partStore.finishLoad();` was replaced with `partStore.finishSave();`.

This adjustment likely resolves a compilation issue by correcting a method name or aligning the method call with the correct interface or expected operation. The change does not directly suggest fixing a runtime bug or logical error but rather ensuring the code compiles successfully by invoking the correct method. There's no apparent bug fix relating to execution or logic, just a correction from `finishLoad` to `finishSave`.

Therefore, the change aligns with the commit message, addressing compilation errors, not bugs during runtime, indicating **NotBuggy**."
docx4j,5988.json,e43c7ce6875ed0292eea114d1f12f5b9bed3e549,"@@ -1,30 +1,31 @@
 	public static Part newPartForContentType(String contentType, String partName)
 	throws InvalidFormatException, PartUnrecognisedException {
 		
-		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)) {
+		if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)
+				|| contentType.equals(ContentTypes.PRESENTATIONML_TEMPLATE) ) {
 			return new MainPresentationPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE)) {
 			return new SlidePart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_MASTER)) {
 			return new SlideMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_SLIDE_LAYOUT)) {
 			return new SlideLayoutPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TABLE_STYLES)) {
 			return new TableStylesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_PRES_PROPS)) {
 			return new PresentationPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_VIEW_PROPS)) {
 			return new ViewPropertiesPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_TAGS)) {
 			return new TagsPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_HANDOUT_MASTER)) {
 			return new HandoutMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_MASTER)) {
 			return new NotesMasterPart(new PartName(partName));
 		} else if (contentType.equals(ContentTypes.PRESENTATIONML_NOTES_SLIDE)) {
 			return new NotesSlidePart(new PartName(partName));
 		} else {
 			throw new PartUnrecognisedException(""No subclass found for "" 
 					+ partName + "" (content type '"" + contentType + ""')"");					
 		}
 	}",Buggy,"Arnaud Kleinpeter's patch of 10 April, fixing error when trying to open a .potx file (powerpoint
template).
",Buggy,"The commit message clearly states that the patch fixes an error when trying to open a .potx file, which is a PowerPoint template. Looking at the code diff, we can see an additional condition was added to an if statement:

```java
if (contentType.equals(ContentTypes.PRESENTATIONML_MAIN)
        || contentType.equals(ContentTypes.PRESENTATIONML_TEMPLATE)) {
    return new MainPresentationPart(new PartName(partName));
}
```

The new condition checks if `contentType` equals `ContentTypes.PRESENTATIONML_TEMPLATE`. Prior to this change, the code did not recognize `ContentTypes.PRESENTATIONML_TEMPLATE`, which likely mapped to .potx files, leading to an error because the appropriate Part couldn't be created, and no subclass was found.

By extending the check to include this template type, the code can now properly handle .potx files by returning the appropriate `MainPresentationPart`. This resolves the error described in the commit message, which was due to an unrecognized or unsupported content type associated with .potx files.

Given the change in the conditional logic to accommodate an additional file type and the intention stated in the commit message to fix an error, this change reflects a bug fix related to handling a specific file type, .potx files.

Therefore, the assessment of the commit as described aligns with a genuine bug fix, and the conclusion is:

**Buggy**"
docx4j,6677.json,e7a3747180c1121e45343a31225052ae7dc68b01,"@@ -1,7 +1,6 @@
 	public Node toNode(AbstractWmlConversionContext context, Model model, TransformState state, Document doc) throws TransformerException {
 	HyperlinkModel hyperlinkModel = (HyperlinkModel)model;
 	Node ret = null;
 		ret = toNode(context, hyperlinkModel, doc);
-		XmlUtils.treeCopy(hyperlinkModel.getContent().getChildNodes(), ret);
 		return ret;
 	}",Buggy,"Fixed several problems converting docx to pdf

Fixes:
- removed treeCopy from AbstractHyperlinkWriter.java to prevent double
links creation;
- added creation of TblPr to prevent NPE and moving of P created for
<caption> tag in nestedTableHierarchyFix() method in XHTMLimporter.java.
So if one generates docx from html and then pdf from this docx
everything will work.
",Buggy,"The commit message indicates a fix where removing treeCopy avoids doubling links. The diff supports this; it removes a line that copies a node tree, preventing undesired duplicate hyperlink nodes. This matches the commit description about fixing link duplication issues. The answer is **Buggy**."
docx4j,5744.json,ec6c723c0edc1ad95aa484b4da21adf68c9e5f49,"@@ -1,27 +1,27 @@
 	public void registerInContentTypeManager() {
 		
 		ContentTypeManager ctm = this.getPackage().getContentTypeManager(); 
 		if (type.equals(AltChunkType.Xhtml) ) {
 			ctm.addDefaultContentType(""xhtml"", ""application/xhtml+xml"");
 		} else if (type.equals(AltChunkType.Mht) ) {
-			ctm.addDefaultContentType(""mht"", ""multipart/related"");
+			ctm.addDefaultContentType(""mht"", ""message/rfc822"");
 		} else if (type.equals(AltChunkType.Xml) ) {
 			ctm.addDefaultContentType(""xml"", ""application/xml"");
 		} else if (type.equals(AltChunkType.TextPlain) ) {
 			ctm.addDefaultContentType(""txt"", ""text/plain"");
 		} else if (type.equals(AltChunkType.WordprocessingML) ) { //Docx
 			// In case we're being added to a docm/dotx/dotm
 			ctm.addDefaultContentType(""docx"", ContentTypes.WORDPROCESSINGML_DOCUMENT);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabled) ) {
 			ctm.addDefaultContentType(""docm"", ContentTypes.WORDPROCESSINGML_DOCUMENT_MACROENABLED);
 		} else if (type.equals(AltChunkType.OfficeWordTemplate) ) {
 			ctm.addDefaultContentType(""dotx"", ContentTypes.WORDPROCESSINGML_TEMPLATE);
 		} else if (type.equals(AltChunkType.OfficeWordMacroEnabledTemplate) ) {
 			ctm.addDefaultContentType(""dotm"", ContentTypes.WORDPROCESSINGML_TEMPLATE_MACROENABLED);
 		} else if (type.equals(AltChunkType.Rtf) ) {
 			ctm.addDefaultContentType(""rtf"", ""text/rtf"");
 		} else if (type.equals(AltChunkType.Html) ) {
 			ctm.addDefaultContentType(""html"", ""text/html"");
 		}
 		
 	}",Buggy,"Bug fix: Change content type for mht to ""message/rfc822"" (Zoltan Luspai)
",Buggy,"The commit message specifies a bug fix in changing the content type for ""mht"" from ""multipart/related"" to ""message/rfc822"". The code diff reflects this change in the registerInContentTypeManager method. This clearly aligns with the commit message and indicates a bug fix related to accurately setting the content type for MHT files. Therefore, the modifications in the code are directly related to fixing the specified bug.

Thus, this change can be concluded as **Buggy**."
docx4j,6106.json,e89bdbce6981005eb9b5b3c2c5893d0f1561ac1f,"@@ -1,20 +1,25 @@
 	public static String getNameOfMainPart(RelationshipsPart packageRels) throws Docx4JException  {
 		
 		// find rel of type officeDocument
 		for (Relationship rel : packageRels.getRelationships().getRelationship() ) {
 			
 			if (rel.getType().equals(
 					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument"") ) {
 				return rel.getTarget();
 			} 
 			else if (rel.getType().equals(
 					""http://schemas.microsoft.com/office/2006/relationships/graphicFrameDoc"") ) {
 				// v:shape/@o:gfxdata
 				return rel.getTarget();
 			} 
+			else if (rel.getType().equals(
+					""http://schemas.openxmlformats.org/officeDocument/2006/relationships/diagramLayout"") ) {
+				// Glox
+				return rel.getTarget();
+			} 
 //			else {
 //				System.out.println(rel.getType());
 //			}
 		}
 		throw new Docx4JException(""No relationship of type officeDocument"");
 	}",Buggy,"Bug fix: make ParseGlox sample work.
",Buggy,"The diff involves modifying a method to correctly identify an additional relationship type related to ""Glox"" diagrams. Previously, the method only returned the target for two specific relationship types. The change adds another `else if` block to handle the ""diagramLayout"" relationship type, allowing the method to return the appropriate target for Glox, which is likely related to the referenced ""ParseGlox sample.""

This change makes the method more comprehensive in handling different document relationship types, specifically fixing an issue where the method would not return the target for a Glox-related relationship, potentially causing the ParseGlox sample to fail. Given that the commit message mentions making the ParseGlox sample work, this indicates that the issue addressed by the code change was indeed a bug in the previous functionality of handling document relationships.

Thus, the changes in the code align with the commit message, and the update can be appropriately categorized as a bug fix related to the handling of Glox diagrams. The conclusion is **Buggy**."
docx4j,5455.json,8d2fa40ea092e0c98a6e8648a80184c0dfe88ed7,"@@ -1,62 +1,68 @@
 	public void removePart(PartName partName) {
 		
 		log.info(""trying to removePart "" + partName.getName() );
 		
 		if (partName == null)
 			throw new IllegalArgumentException(""partName was null"");
 		
 		Part part = getPackage().getParts().get(partName);
 		
 		if (part!=null) {
 
 			// Remove the relationship for which it is a target from here
 			// Throw an error if this can't be found!
 			Relationship relToBeRemoved = null;
 //			for (Relationship rel : relationshipsByID.values() ) {
 			for (Relationship rel : relationships.getRelationship() ) {
+				
+				if (rel.getTargetMode() !=null
+						&& rel.getTargetMode().equals(""External"") ) {
+					// This method can't be used to remove external resources
+					continue;
+				}
 								
 				URI resolvedTargetURI = null;
 
 				try {
 					resolvedTargetURI = org.docx4j.openpackaging.URIHelper
 							.resolvePartUri(sourceP.partName.getURI(), new URI(
 									rel.getTarget()));
 				} catch (URISyntaxException e) {
 					log.error(""Cannot convert "" + rel.getTarget()
 							+ "" in a valid relationship URI-> ignored"", e);
 				}		
 
 				log.debug(""Comparing "" + resolvedTargetURI + "" == "" + partName.getName());
 				
 				if (partName.getName().equals(resolvedTargetURI.toString()) ) { // was rel.getTargetURI()
 					
 					log.info(""True - will delete relationship with target "" + rel.getTarget());
 					relToBeRemoved = rel; // Avoid java.util.ConcurrentModificationException
 					break;
 				}
 				
 			}
 			if (relToBeRemoved==null) {
 				// The Part may be in the package somewhere, but its not
 				// a target of this relationships part!
 				throw new IllegalArgumentException(partName + "" is not a target of "" + this.partName );
 			} else {
 				removeRelationship(relToBeRemoved);				
 			}
 						
 			// Remove parts it references
 			if (part.getRelationshipsPart()!=null) {
 				part.getRelationshipsPart().removeParts();
 				
 				// part.setRelationships(null);  // Unnecessary
 			}			
 
 			// Remove from Content Type Manager
 				// TODO			
 			
 			// Delete the specified part from the package.
 			getPackage().getParts().remove(partName);						
 		}
 
 //		this.isDirty = true;
 	}",Buggy,"Bug fix in removePart - ignore external resources.
",Buggy,"The commit message states that the bug fix involves ignoring external resources when removing a part. The code change reflects this by adding a check to skip processing any relationship flagged as ""External"" within the `removePart` method. 

Here's a step-by-step analysis:

1. **Part Removal Logic:** The `removePart` method is used to remove a part from a package.

2. **Relationship Handling:** It iterates over relationships associated with a part to identify and remove the correct part.

3. **New Logic for External Resources:** 
   - A conditional statement is added to check if a relationship's `TargetMode` is ""External"".
   - If it is ""External"", the method continues to the next iteration, effectively ignoring external resources.
   
4. **Bug Fix Context:** 
   - The commit message accurately describes a bug fix where external resources should not be removed using this method.
   - The change prevents erroneous removal attempts or processing of external parts.

The code change directly implements the functionality described in the commit message, indicating a logical correction related to how different types of relationships are treated when removing parts. This aligns with the typical characteristics of a bug fix: correcting logic to handle specific cases appropriately. 

Thus, this change should be concluded as **Buggy**."
docx4j,3230.json,71277942a6f5c967ca96c8454953e8c6697dffbb,"@@ -1,9 +1,14 @@
 	public void setXslFO(Element foElement) {
 		
 		if (((Color)this.getObject()).getVal()!=null ) {
-			foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			if (((Color)this.getObject()).getVal().equals(""auto"")) {
+				// set it to black
+				foElement.setAttribute(FO_NAME, ""black"");				
+			} else {
+				foElement.setAttribute(FO_NAME, ""#"" + ((Color)this.getObject()).getVal());
+			}
 		} else {
 			//
 		}
 
 	}",Buggy,"bug fix: XSL FO doesn't like font color #auto
",Buggy,"The commit message describes a bug fix regarding XSL FO's handling of the font color ""auto"". The code change incorporates a conditional check that explicitly handles the case when the color value is ""auto"". If the value is ""auto"", it sets the font color to ""black"" instead. This directly addresses the issue described in the commit message, ensuring that an unsupported color value does not get set in the XSL FO, which aligns with the specified bug fix.

Therefore, the diff indicates a bug fix action as described by the commit message. The appropriate conclusion for this set of changes is **Buggy**."
docx4j,18108.json,b7d10c89810f8fb9ba39216b7630f82f493bfdee,"@@ -1,126 +1,125 @@
 	public static void main(String[] args) throws Docx4JException {
 		
 		// Input file
 		String inputfilepath = System.getProperty(""user.dir"") + ""/sample-docs/pptx/pptx-chart.pptx"";
 		
 		// The names of the parts which will be edited
 		// Alter these to match what is in your input pptx
 		// .. the chart
 		String chartPartName = ""/ppt/charts/chart1.xml"";
 		// .. the xlsx
 		String xlsPartName = ""/ppt/embeddings/Microsoft_Excel_Sheet1.xlsx"";
-//		String xlsPartName = ""/ppt/embeddings/Microsoft_Office_Excel_Worksheet1.xlsx"";
 		
 		// Output file
 		String outputfilepath = System.getProperty(""user.dir"") 
 				+ ""/OUT_EditEmbeddedCharts-"" 
 				+ System.currentTimeMillis() + "".pptx"";
 		
 		// Values to change
 		Random rand = new Random();
 
 		String firstValue  = String.valueOf(rand.nextInt(99));
 		String secondValue = String.valueOf(rand.nextInt(99));
 		
 		// Open the PPT template file
 		PresentationMLPackage ppt = (PresentationMLPackage) OpcPackage
 			.load(new java.io.File(inputfilepath));
 
 		/*
 		 * Get the Chart object and update the values. Afterwards, we'll update 
 		 * the associated spreadsheet so that the data is synchronized.
 		 */
 		Chart chart = (Chart) ppt.getParts().get(new PartName(chartPartName));
 		
 		List<Object> objects = chart.getJaxbElement().getChart().getPlotArea()
 				.getAreaChartOrArea3DChartOrLineChart();
 		
 		for (Object object : objects) {
 			
 			if (object instanceof CTBarChart) {
 
 				List<CTBarSer> ctBarSers = ((CTBarChart) object).getSer();
 				
 				for (CTBarSer ctBarSer : ctBarSers)
 				{
 					List<CTNumVal> ctNumVals = ctBarSer.getVal().getNumRef().getNumCache().getPt();
 					for (CTNumVal ctNumVal : ctNumVals)
 					{
 						System.out.println(""ctNumVal Val BEFORE: "" + ctNumVal.getV());
 						if (ctNumVal.getIdx() == 0) {
 							ctNumVal.setV(firstValue);
 						}
 						else if (ctNumVal.getIdx() == 1) {
 							ctNumVal.setV(secondValue);	
 						}
 						System.out.println(""ctNumVal Val AFTER: "" + ctNumVal.getV());
 					}
 				}
 			}
 		}
 				
 		/*
 		 * Get the spreadsheet and find the cell values that need to be updated
 		 */
 		
 		EmbeddedPackagePart epp  = (EmbeddedPackagePart) ppt
 			.getParts().get(new PartName(xlsPartName));
 		
 		if (epp==null) {
 			throw new Docx4JException(""Could find EmbeddedPackagePart: "" + xlsPartName);
 		}
 		
 		InputStream is = BufferUtil.newInputStream(epp.getBuffer());
 		
 		SpreadsheetMLPackage spreadSheet = (SpreadsheetMLPackage) SpreadsheetMLPackage.load(is);
 
 		Map<PartName,Part> partsMap = spreadSheet.getParts().getParts();		 
 		Iterator<Entry<PartName, Part>> it = partsMap.entrySet().iterator();
 
 		while(it.hasNext()) {
 			Map.Entry<PartName, Part> pairs = it.next();
 			
 			if (partsMap.get(pairs.getKey()) instanceof WorksheetPart) {
 				
 				WorksheetPart wsp = (WorksheetPart) partsMap.get(pairs.getKey()) ;
 				
 				List<Row> rows = wsp.getJaxbElement().getSheetData().getRow();
 
 				for (Row row : rows) {
 					List<Cell> cells = row.getC();
 					for (Cell cell : cells)
 					{
 						if (cell.getR().equals(""B2"") && cell.getV() != null) {
 							System.out.println(""B2 CELL VAL: "" + cell.getV());
 							// change the B2 cell value
+							cell.setT(STCellType.STR);
 							cell.setV(firstValue);
 						}
 						else if (cell.getR().equals(""B3"") && cell.getV() != null) {
 							System.out.println(""B3 CELL VAL: "" + cell.getV());
 							// Change the B3 cell value
-							cell.setV(secondValue); 
+							cell.setT(STCellType.STR);
+							cell.setV(secondValue);
 						}
 					}					
 				}
 			}
 		}
 
 		/*
 		 * Convert the Spreadsheet to a binary format, set it on the 
 		 * EmbeddedPackagePart, add it back onto the deck and save to a file.
 		 *  
 		 */		
 		ByteArrayOutputStream baos = new ByteArrayOutputStream();
 		
 		SaveToZipFile saver = new SaveToZipFile(spreadSheet);
 
 		saver.save(baos);
 		epp.setBinaryData(baos.toByteArray());
 
-		ppt.addTargetPart(epp);
-
 		// Write the new file to disk
 		ppt.save(new java.io.File(outputfilepath));
 
 		System.out.println(""\n\n done .. saved "" + outputfilepath);
 	}",Buggy,EditEmbeddedCharts - bug fixes,Buggy,"The commit message indicates ""bug fixes"" but is vague on specifics. The code changes include:

1. **Updating Cell Types**: The code now explicitly sets the cell type to `STCellType.STR` before updating the cell value. This ensures that the values are interpreted as strings. If the cell type was previously incorrect, this change would fix potential errors in processing or displaying the cell values.

2. **Removal of Redundant Code**: The line `ppt.addTargetPart(epp);` has been removed. If this line was unnecessary or caused issues (e.g., adding the part twice), removing it could fix a bug.

3. **Commenting Out a Line**: An unused line for an alternative embedded workbook path was commented out. While this change doesn't directly fix a bug, it clarifies the primary path used.

Both changes have the potential to address specific issues: handling cell type correctly prevents misinterpretation of data, and removing redundant code might correct incorrect behavior or resource handling. These modifications align with the stated commit purpose of ""bug fixes,"" particularly in contexts where cell type or redundant actions could lead to errors.

Given these observations and the focus on potential bug resolution, the changes are consistent with fixing errors in the codebase. Thus, the answer is **Buggy**."
docx4j,5328.json,2b240431a940e8529960d06423db48c7122090ec,"@@ -1,67 +1,70 @@
 	public void addPartsFromRelationships(ZipOutputStream out,  RelationshipsPart rp )
 	 throws Docx4JException {
 		
 //		for (Iterator it = rp.iterator(); it.hasNext(); ) {
 //			Relationship r = (Relationship)it.next();
 //			log.info(""For Relationship Id="" + r.getId() + "" Source is "" + r.getSource().getPartName() + "", Target is "" + r.getTargetURI() );
 		for ( Relationship r : rp.getRelationships().getRelationship() ) {
 			
 			log.debug(""For Relationship Id="" + r.getId() 
 					+ "" Source is "" + rp.getSourceP().getPartName() 
 					+ "", Target is "" + r.getTarget() );
 			
-//			if (!r.getTargetMode().equals(TargetMode.INTERNAL) ) {
+			if (r.getType().equals(Namespaces.HYPERLINK)) {				
+				continue;  // whether internal or external								
+			}
+			
 			if (r.getTargetMode() != null
 					&& r.getTargetMode().equals(""External"") ) {
 				
 				// ie its EXTERNAL
 				// As at 1 May 2008, we don't have a Part for these;
 				// there is just the relationship.
 
 				log.warn(""Encountered external resource "" + r.getTarget() 
 						   + "" of type "" + r.getType() );
 				
 				// So
 				continue;				
 			}
 			
 			try {
 				//String resolvedPartUri = URIHelper.resolvePartUri(r.getSourceURI(), r.getTargetURI() ).toString();
 
 				String resolvedPartUri = URIHelper.resolvePartUri(rp.getSourceURI(), new URI(r.getTarget() ) ).toString();		
 				
 				// Now drop leading ""/'
 				resolvedPartUri = resolvedPartUri.substring(1);				
 				
 				// Now normalise it .. ie abc/def/../ghi
 				// becomes abc/ghi
 				// Maybe this isn't necessary with a zip file,
 				// - ZipFile class may be smart enough to do it.
 				// But it is certainly necessary in the JCR case.
 //				target = (new java.net.URI(target)).normalize().toString();
 //				log.info(""Normalised, it is "" + target );				
 				
 //				Document contents = getDocumentFromZippedPart( zf,  target);
 				
 				if (!false) {
 					log.debug(""Getting part /"" + resolvedPartUri );
 					
 					Part part = p.getParts().get(new PartName(""/"" + resolvedPartUri));
 					
 					if (part==null) {
 						log.error(""Part "" + resolvedPartUri + "" not found!"");
 					} else {
 						log.debug(part.getClass().getName() );
 					}
 					
 					savePart(out, part);
 					
 				}
 					
 			} catch (Exception e) {
 				throw new Docx4JException(""Failed to add parts from relationships"", e);				
 			}
 		}
 		
 		
 	}",Buggy,"Bug fix: save file containing internal hyperlink rel
",Buggy,"The commit message indicates a bug fix related to saving files with internal hyperlink relationships. The code change supports this by modifying the logic to exclude processing any hyperlinks (`Namespaces.HYPERLINK`), whether internal or external. This is achieved by adding a check for the relationship's type and skipping it if it matches `Namespaces.HYPERLINK`. The original condition only dealt with the target mode being ""External"" without considering hyperlinks. The update aligns with the commit message, addressing the bug by ensuring hyperlinks are correctly handled and not processed further, preventing possible errors with hyperlink relationships.

Given this analysis, it's clear the changes align with a bug fix in handling hyperlinks in documents. Therefore, the correct conclusion is **Buggy**."
docx4j,6285.json,712217fb5f02ee12b18281bf0abe099e8e2cfe55,"@@ -1,32 +1,29 @@
 	private RelationshipsPart getRelationshipsPartFromXmlPackage(Base p, String partName) 
 			throws Docx4JException {
 		
-		RelationshipsPart thePart = null;
+		RelationshipsPart rp = null;
 		
 		try {
 			
 			org.docx4j.xmlPackage.Part part = parts.get(partName);
 			
 			if (part == null) {
 				return null;
 			}
 			
 			org.w3c.dom.Element el = part.getXmlData().getAny();
 			
-			RelationshipsPart rp = new RelationshipsPart(new PartName(partName) );
+			rp = new RelationshipsPart(new PartName(partName) );
 			// PartName already starts with a '/', so no need to add it
 			rp.setSourceP(p);
-			rp.unmarshal(el);
 			
-//			// Convert it to a Dom4J element
-//			thePart = new RelationshipsPart( p, new PartName( partName), convertW3CtoDom4J(el) );
-//				// PartName already starts with a '/', so no need to add it
-			
+			rp.setRelationships( (Relationships)rp.unmarshal(el) );
+						
 		} catch (Exception e) {
 			e.printStackTrace();
 			throw new Docx4JException(""Error getting document from XmlPackage:"" + partName, e);
 			
 		} 
 		
-		return thePart;
+		return rp;
 	}",Buggy,"bug fix in getRelationshipsPartFromXmlPackage
",Buggy,"The commit message indicates a bug fix in the method getRelationshipsPartFromXmlPackage. Let's analyze the changes:

1. **Variable Renaming and Reassignment:**
   - The variable `thePart` is renamed to `rp`, and its initialization is moved outside the try block for accessibility throughout the method. Initially, `thePart` was unused, leading to a potential issue if the part was successfully instantiated but not returned.

2. **Unmarshal Adjustment:**
   - The line `rp.unmarshal(el);` is replaced with `rp.setRelationships((Relationships)rp.unmarshal(el));`. This change ensures the unmarshalled relationships are set correctly within the `RelationshipsPart` object, instead of simply invoking unmarshal without utilizing the output, possibly fixing an issue with the relationships not being properly assigned.

3. **Return Value Correction:**
   - The method initially returned `thePart`, which was always `null`. The update returns the correctly initialized and populated `rp`.

The changes clearly aim to address functionality that was previously incorrect or not fully implemented, resulting in a bug where the `RelationshipsPart` was not effectively retrieved or populated. Therefore, the alterations reflect a bug fix.

The conclusion is **Buggy**."
mockito,494.json,881c1d9cace557e64641d9b104c59b2b0c86ee4c,"@@ -1,8 +1,7 @@
     public static IMockitoConfiguration getConfig() {
         if (!initialized) {
-            //TODO check email of mockito group
             throw new IllegalStateException(""Something went wrong. GlobalConfiguration should be initialised by now.\n"" +
                 ""Please report issue at http://mockito.org or write an email to mockito@googlegroups.com"");
         }
         return globalConfiguration;
     }",Buggy,"removed TODOs,
fixed the minor problem of breaking the invocation string when no args given

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401161
",NotBuggy,"The commit message mentions removing TODOs and fixing an issue regarding the invocation string. However, the provided diff only shows the removal of a TODO comment without any functional code changes related to the invocation string or argument handling. The removal of comments and presence of a throw statement doesn't align with a description of fixing a problem related to invocation strings with no arguments.

Since there is no observable change in logic or behavior related to fixing the invocation string bug, this commit diff does not demonstrate a bug fix in the provided context. The main change here is organizational or cleanup by removing a TODO comment. 

Thus, the evaluation of whether this diff provides a bug fix shows:

- **Comment removal**: A TODO comment is removed, providing no functional impact on the code.
- **No invocation string fix**: There's no evidence in the diff of a change that handles invocation strings sans arguments.

Given the information available, the conclusion is **NotBuggy**. This diff primarily reflects code cleanup rather than addressing or fixing a bug as described in the message."
mongo-java-driver,3587.json,e53bde1953e3e4d12809ce6c27c69a1eb8a4b8dd,"@@ -1,3 +1,4 @@
     public Binary readBinaryData(final String name) {
-        return readBinaryData(name);
+        verifyName(name);
+        return readBinaryData();
     }",Buggy,"fixed infinite recursion error: test forthcoming
",Buggy,"The commit message indicates a fix for an ""infinite recursion error."" In the diff, the function `readBinaryData` initially calls itself with the parameter `name`, which would indeed cause infinite recursion as the method continuously calls itself without a base case or exit condition.

The code change replaces the self-recursive call with a call to `verifyName(name)` followed by `readBinaryData()` without parameters. This suggests that instead of recursively calling itself, the method now performs a name verification through `verifyName(name)` and then calls another method `readBinaryData()`, which likely does not include the previously recursive call.

Such a modification aligns directly with addressing infinite recursion by removing the self-referential aspect of the method. Consequently, the change indicates a genuine bug fix as described in the commit message.

Thus, I conclude with **Buggy**."
mongo-java-driver,2935.json,6fa5b4b1073a1df6d2a70d2fef360622bae6fbea,"@@ -1,14 +1,15 @@
     public void apply(final ClassModelBuilder<?> classModelBuilder) {
         for (PropertyModelBuilder<?> propertyModelBuilder : classModelBuilder.getPropertyModelBuilders()) {
             if (!(propertyModelBuilder.getPropertyAccessor() instanceof PropertyAccessorImpl)) {
                 throw new CodecConfigurationException(format(""The SET_PRIVATE_FIELDS_CONVENTION is not compatible with ""
                         + ""propertyModelBuilder instance that have custom implementations of org.bson.codecs.pojo.PropertyAccessor: %s"",
                         propertyModelBuilder.getPropertyAccessor().getClass().getName()));
             }
             PropertyAccessorImpl<?> defaultAccessor = (PropertyAccessorImpl<?>) propertyModelBuilder.getPropertyAccessor();
             PropertyMetadata<?> propertyMetaData = defaultAccessor.getPropertyMetadata();
-            if (!propertyMetaData.isDeserializable() && isPrivate(propertyMetaData.getField().getModifiers())) {
+            if (!propertyMetaData.isDeserializable() && propertyMetaData.getField() != null
+                    && isPrivate(propertyMetaData.getField().getModifiers())) {
                 setPropertyAccessor(propertyModelBuilder);
             }
         }
     }",Buggy,"Fix NPE error with ConventionSetPrivateField

JAVA-2951
",Buggy,"The commit message states that it fixes a NullPointerException (NPE) error, which is consistent with the code changes made in the diff. The original code calls `propertyMetaData.getField().getModifiers()` without checking if `getField()` returns null, which could lead to an NPE. The updated code adds a null check with `propertyMetaData.getField() != null`, thus preventing the potential NPE when trying to access methods on a potentially null object.

This change directly addresses a bug related to nullability checks, and the commit message accurately reflects the adjustments made in the code. Therefore, the modifications clearly indicate a bug fix.

The answer is **Buggy**."
mongo-java-driver,3321.json,9d17beae8f95df45c855e8284e96a6ddbdca07b7,"@@ -1,26 +1,28 @@
     public static UUID decodeBinaryToUuid(final byte[] data, final byte type, final UuidRepresentation uuidRepresentation) {
         if (data.length != 16) {
             throw new BsonSerializationException(String.format(""Expected length to be 16, not %d."", data.length));
         }
 
         if (type == BsonBinarySubType.UUID_LEGACY.getValue()) {
             switch(uuidRepresentation) {
                 case C_SHARP_LEGACY:
                     reverseByteArray(data, 0, 4);
                     reverseByteArray(data, 4, 2);
                     reverseByteArray(data, 6, 2);
                     break;
                 case JAVA_LEGACY:
                     reverseByteArray(data, 0, 8);
                     reverseByteArray(data, 8, 8);
                     break;
                 case PYTHON_LEGACY:
-                case STANDARD:
                     break;
+                case STANDARD:
+                    throw new BSONException(""Can not decode a subtype 3 (UUID legacy) BSON binary when the decoder is configured to use "" +
+                            ""the standard UUID representation"");
                 default:
                     throw new BSONException(""Unexpected UUID representation"");
             }
         }
 
         return new UUID(readLongFromArrayBigEndian(data, 0), readLongFromArrayBigEndian(data, 8));
     }",Buggy,"Fix UUID decoder bug

The UUID decoder now does NOT allow decoding of a subtype 3 (legacy UUID) BSON Binary value
when the UUID decoder's UUID representation is STANDARD.
",Buggy,"The commit message indicates a bug fix involving how the UUID decoder handles legacy BSON Binary values. The code change adds a case to throw an exception for the STANDARD UUID representation when encountering a subtype 3 (legacy UUID). This ensures an illegal operation is correctly prevented, aligning with the message. The change involves improved error handling for specific conditions, thus solving the bug described. The answer is **Buggy**."
mongo-java-driver,2112.json,381a79da3eab8c3d819826af40b0ae39dd08ae08,"@@ -1,3 +1,3 @@
-    public void pipe(BSONReader reader) {
+    public void pipe(final BSONReader reader) {
         pipeDocument(reader);
     }",Buggy,"Fixed checkstyle errors
",NotBuggy,"The commit message indicates that the changes are related to fixing checkstyle errors, and the diff shows a minor stylistic update—adding the `final` modifier to a method parameter. This change is consistent with resolving style issues rather than fixing any functional bugs. There are no indications of error handling updates, logical corrections, or exception-handling improvements, which are typical signs of bug-related changes.

Therefore, the changes align with the commit message, which does not indicate a bug fix, but rather a style correction. The changes are not indicative of resolving any erroneous behavior or logical issues in the code functionality.

The conclusion is **NotBuggy**."
mongo-java-driver,685.json,0a759943e1d5a8b971798db82e8a89330545e558,"@@ -1,8 +1,3 @@
     public GridFSInputFile createFile(final File file) throws IOException {
-        FileInputStream fileInputStream = new FileInputStream(file);
-        try {
-            return createFile(fileInputStream, file.getName(), true);
-        } finally {
-            fileInputStream.close();
-        }
+        return createFile(new FileInputStream(file), file.getName(), true);
     }",Buggy,"Fixed GridFS.createFile bug

The underlying call handles the closure of the stream correctly

JAVA-1813
",Buggy,"The change simplifies the method by removing the try-finally block, which manually closed the `FileInputStream`. The commit message indicates that the underlying `createFile` method now handles the stream closure correctly, thus eliminating the need for explicit closure in this method. This change corrects a potential resource leak bug if `createFile` did not close the stream properly before.

Therefore, this modification aligns with the commit message stating a fix for an issue in `GridFS.createFile`, specifically regarding stream handling, which indicates a bug fix. Hence, the conclusion is **Buggy**."
junit5,2294.json,01eb2e4e3ba37aedc2c8f578fcc77a55ef305cd5,"@@ -1,3 +1,9 @@
 	private static String toString(Object obj) {
-		return (obj instanceof Class ? getCanonicalName((Class<?>) obj) : String.valueOf(obj));
+		if (obj instanceof Class) {
+			return getCanonicalName((Class<?>) obj);
+		}
+		if (obj instanceof Object[]) {
+			return Arrays.toString((Object[]) obj);
+		}
+		return StringUtils.nullSafeToString(obj);
 	}",Buggy,"Fix bug in AssertionUtils.toString(Object)

Prior to this commit, AssertionUtils.toString(Object) printed an array via
the array's toString() method which resulted in non-user-friendly output.

This commit addresses this issue by printing arrays using Arrays.toString(),
which produces human readable output.

This is a prerequisite for #961.

Issue: #1030
",Buggy,"The commit message describes a fix within the `AssertionUtils.toString(Object)` method. The diff supports this by replacing the prior implementation, which called `String.valueOf(obj)` for any object, including arrays, resulting in raw memory address output for arrays, with a more human-readable representation using `Arrays.toString((Object[]) obj)` for array instances. This change specifically improves the output's readability, aligning with the commit message's goal of making array outputs more user-friendly. The inclusion of `StringUtils.nullSafeToString(obj)` further ensures a safe string conversion for non-array, non-class objects, likely providing better null handling or additional string conversion logic.

Given these changes improve the functionality as described by the commit and address an issue (Issue #1030), this commit is indeed a bug fix related to how arrays are represented in string conversion.

Thus, the conclusion is **Buggy**."
Essentials,1046.json,880ec1b3d953f1aef72ac925fe524799e4df8898,"@@ -1,13 +1,13 @@
 	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
 	{
 		charge(sender);
 		sender.sendMessage(Util.format(""gcmax"", (Runtime.getRuntime().maxMemory() / 1024 / 1024)));
 		sender.sendMessage(Util.format(""gcmin"", (Runtime.getRuntime().freeMemory() / 1024 / 1024)));
 		for (World w : server.getWorlds())
 		{
 			sender.sendMessage(
 					(w.getEnvironment() == World.Environment.NETHER ? ""Nether"" : ""World"") + "" \"""" + w.getName() + ""\"": ""
 					+ w.getLoadedChunks().length + Util.i18n(""gcchunks"")
-					+ w.getEntities().size() + Util.i18n(""entities""));
+					+ w.getEntities().size() + Util.i18n(""gcentities""));
 		}
 	}",Buggy,"Fix translation error in gc

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1465 e251c2fe-e539-e718-e476-b85c1f46cddb
",Buggy,"The change in the diff corrects a key used in an internationalization function, Util.i18n(), changing ""entities"" to ""gcentities"". This indicates a fix for a likely translation error where the wrong key was causing incorrect or missing text translations in one part of the application. The commit message, ""Fix translation error in gc,"" is consistent with this modification.

Given the commit message and code changes, it's clear that this change fixes a translation error, which is indeed a form of bug fix in terms of application behavior (though it deals specifically with user-facing text rather than logic errors or exceptions).

Therefore, the analysis of the diff and commit message confirms that the update is addressing a bug related to a translation error.

The final conclusion is **Buggy**."
Essentials,2593.json,4c78ab4f2a3eb4d598564069aa5a21e0380de29d,"@@ -1,28 +1,29 @@
 	private List<String> sort(List<String> permList) {
 		
 		List<String> result = new ArrayList<String>();
 		
 		for (String key : permList) {
 			String a = key.charAt(0) == '-'? key.substring(1):key;
 			Map<String, Boolean> allchildren = GroupManager.BukkitPermissions.getAllChildren(a, new HashSet<String>());
 			if (allchildren != null) {
 
 				ListIterator<String> itr = result.listIterator();
 				
 				while (itr.hasNext()){
 					String node = (String) itr.next();
 					String b = node.charAt(0) == '-'? node.substring(1):node;
 					
+					// Insert the parent node before the child
 					if (allchildren.containsKey(b)) {
-						itr.previous();
-						itr.add(key);
+						itr.set(key);
+						itr.add(node);
 						break;
 					}
 				}
 			}
 			if (!result.contains(key))
 				result.add(key);
 		}
 		
 		return result;
 	}",Buggy,fix for an iterator error if there is only one element in the array.,Buggy,"The commit message states that this change fixes an iterator error when there's only one element in an array. In this code, the iteration over `result` checks if each node has a parent in `allchildren`. The change replaces the previous call to `itr.previous(); itr.add(key);` with `itr.set(key); itr.add(node);`.

Analyzing the previous approach:
- `itr.previous(); itr.add(key);` would move the iterator backwards and insert `key` before the last returned element. This is complex and can cause issues like ConcurrentModificationException if not handled carefully, especially with one element, leading to the potential iterator error hinted at in the commit message.

The new approach with `itr.set(key); itr.add(node);` replaces the last element (`node`) with `key` and ensures that `node` is immediately reinserted after `key`. This is simpler and likely resolves issues related to one-element arrays since `set` and `add` operations would comfortably handle a zero-length iteration.

From this, the code change addresses an iterator logic error, particularly when modifying the list during iteration when there is only one element, thus satisfying the commit message.

Concluding, the changes align with the commit message, indicating a single-element iterator bug fix. The assessment is **Buggy**."
Essentials,1458.json,4823712f47bfb7f64e22b8633b05e3f6801919c5,"@@ -1,13 +1,16 @@
 		public void onEntityDamageByEntity(final EntityDamageByEntityEvent event)
 		{
 			if (event.getCause() != DamageCause.ENTITY_ATTACK || event.getEntity().getType() != EntityType.PLAYER)
 			{
 				return;
 			}
 			final Entity damager = event.getDamager();
-			final User user = ess.getUser(damager);
-			if (user.isJailed())
+			if (damager.getType() == EntityType.PLAYER)
 			{
-				event.setCancelled(true);
+				final User user = ess.getUser(damager);
+				if (user != null && user.isJailed())
+				{
+					event.setCancelled(true);
+				}
 			}
 		}",Buggy,"Fixing up NPE bug in jails (implemented in Dev2.9.163)
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) related to jails. The diff modifies the code by adding a check to ensure `damager` is of type `PLAYER` before attempting to retrieve a `User` object associated with it. Then, it checks if the `user` object is non-null before calling `isJailed()`. 

These changes directly address a potential NPE that could occur if `damager` was not a player or if `ess.getUser(damager)` returned null. By adding these null and type checks, the risk of NPEs is mitigated, which aligns with the commit message.

The changes match the description in the commit message, indicating a proactive approach to handling null values and preventing the application from crashing, which are common characteristics of a bug fix.

Thus, the changes support fixing an NPE bug as described in the commit message. Therefore, the conclusion is **Buggy**."
Essentials,832.json,67b5b4e06b5a5952d86afe241001c0f5ba589ed4,"@@ -1,4 +1,5 @@
 	private boolean isAuthor(BookMeta bmeta, String player)
 	{
-		return bmeta.getAuthor().equalsIgnoreCase(player);
+		String author = bmeta.getAuthor();
+		return author != null && author.equalsIgnoreCase(player);
 	}",Buggy,"Fix minor /book bug (Null author)
",Buggy,"The commit message states a minor bug fix related to a null author in a book. The code diff reveals a change in the `isAuthor` method, where there is now a null check for the author before calling `equalsIgnoreCase(player)`. Previously, if `bmeta.getAuthor()` returned null, calling `equalsIgnoreCase` would have thrown a `NullPointerException`. The update prevents this possible exception by ensuring that `author` is not null before performing the comparison.

This change directly addresses a potential bug (a `NullPointerException` when the author is null), aligning with the commit message claiming to fix an issue with a ""Null author.""

Therefore, the changes in the diff align with the commit message and indicate a bug fix related to error-handling regarding a null author. Based on this analysis, the conclusion is **Buggy**."
wicket,4568.json,89c5acbf41b98ab890ed489bbe15a5702da4595c,"@@ -1,4 +1,4 @@
 	public IModel<T> getModel()
 	{
-		return null;
+		return model;
 	}",Buggy,"fix small bug

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1159660 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change in the code is in the method `getModel()`, which previously returned `null` but now returns `model`. This change suggests a bug fix because returning `null` from a method intended to return a model likely resulted in errors or incorrect behavior in the application's logic. By returning `model` instead, the method now behaves as expected, providing the necessary data/model.

Thus, the code change aligns with a small bug fix, as indicated by the commit message. The alteration shifts the method's output from a potentially problematic `null` to an actual object reference, likely correcting a logical flaw in the application.

Therefore, the conclusion is **Buggy** because it addresses and fixes a functional issue in the software."
wicket,10114.json,f78f8eb142782d7931d91aec9a77037a8582f6cd,"@@ -1,5 +1,6 @@
 	public void renderHead(IHeaderResponse response)
 	{
 		super.renderHead(response);
-		response.render(CssHeaderItem.forReference(PageViewCSSResourceReference.get()));
+		response.render(
+			CssHeaderItem.forReference(new CssResourceReference(PageView.class, ""pageview.css"")));
 	}",Buggy,"WICKET-6737: fixed compilation error after deleting class
",Buggy,"The commit message states that a compilation error was fixed following the deletion of a class. The code change relates to CSS resource reference handling: it switches from using `PageViewCSSResourceReference.get()` to a `new CssResourceReference(PageView.class, ""pageview.css"")`.

Here's the analysis:

1. **Intent of Change**: The change implies that `PageViewCSSResourceReference` either no longer exists or is inappropriate due to a class deletion. By directly referencing `PageView.class`, the code ensures it uses a valid, existing class for resource specification.

2. **Nature of Fix**: Compilation errors arise when the Java compiler can't resolve a symbol because something is missing—in this case, likely due to the deletion of the `PageViewCSSResourceReference` class. The adjustment made corrects this by providing an alternative method of referencing the CSS resource.

3. **Alignment with Commit Message**: The commit message's reference to resolving a compilation error directly matches the context of replacing a non-existing class reference with an existing one, ensuring code can compile successfully.

The code change resolves a compilation error likely resulting from the absence of a previously referenced class. While this isn't a runtime bug fix, compilation errors prevent the code from functioning altogether, and thus addressing them is critical for operability.

Therefore, given its necessity for the application to compile correctly, I conclude this change reflects a bug fix. The final assessment is **Buggy**."
wicket,7162.json,8c62ff32ea564e1efb3b9fc84b75fe783508732e,"@@ -1,39 +1,41 @@
 	public boolean equalTo(final MarkupStream that)
 	{
 		// While a has more markup elements
 		while (this.hasMore())
 		{
 			// Get an element from each
-			final MarkupElement thisElement = this.next();
-			final MarkupElement thatElement = that.next();
+			final MarkupElement thisElement = this.get();
+			final MarkupElement thatElement = that.get();
 			
 			// and if the elements are not equal
 			if (thisElement != null && thatElement != null)
 			{
 				if (!thisElement.equalTo(thatElement))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
 			else
 			{
 				// If one element is null,
 				if (!(thisElement == null && thatElement == null))
 				{
 					// fail the comparison
 					return false;
 				}
 			}
+			this.next();
+			that.next();
 		}
 
 		// If we've run out of markup elements in b
 		if (!that.hasMore())
 		{
 			// then the two streams match perfectly
 			return true;
 		}
 
 		// Stream b had extra elements
 		return false;
 	}",Buggy,"fixed error in DiffUtil (MarkupStream.equalsTo). It failed to compare markup streams with just one MarkupElement.

git-svn-id: https://svn.apache.org/repos/asf/incubator/wicket/trunk@552187 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes fixing an error in MarkupStream.equalTo that failed when comparing streams with a single MarkupElement. The issue relates to the order of method calls within the comparison loop. Initially, `this.next()` and `that.next()` were used before checking equality, which meant retrieving and moving past the current element. The change replaces these calls with `this.get()` and `that.get()`, ensuring the current elements are retrieved without advancing the iterator. The advancement is explicitly managed by adding `this.next()` and `that.next()` after the comparison logic.

Analyzing the diff:

- **Original Code**: Used `next()` to retrieve and advance in the streams, potentially missing the comparison of the last single element correctly. This would have led to the observed issue when streams had exactly one `MarkupElement`.
- **Modified Code**: Uses `get()` to fetch elements without altering the stream position, ensuring the subsequent comparison is done properly. The position is incremented only after the check, resolving the issue.

This modification directly addresses the issue described in the commit message: failing to compare streams correctly when they contain just one element. The changes align well with the commit message, indicating a bug fix.

The answer is **Buggy**."
wicket,2888.json,e72ed7e22a7d002070ffe4a1d3e7daf514351c20,"@@ -1,14 +1,19 @@
 	public Serializable getCacheKey()
 	{
 		Class<?> scope = getScope();
 		String currentStyle = getCurrentStyle();
 		Locale currentLocale = getCurrentLocale();
 		
+		 IResourceStream packageResource = Application.get()
+			.getResourceSettings()
+			.getResourceStreamLocator()
+			.locate(scope, absolutePath, currentStyle, variation, currentLocale, null, false);
+		 
 		// if resource stream can not be found do not cache
-		if (exists(scope, absolutePath, currentLocale, currentStyle, variation))
+		if (packageResource != null)
 		{
 			return new CacheKey(scopeName, absolutePath, currentLocale, currentStyle, variation);
 		}
 
 		return null;
 	}",Buggy,"WICKET-6061 improved getChacheKey to fix problems with absolute path of package resources
",Buggy,"The commit message indicates an improvement to the getCacheKey method to fix issues with the absolute path of package resources. The code diff reflects this by changing the cache key determination logic:

1. **Previous Logic**: The original code uses the `exists` method to decide whether to cache a resource. This method likely checks if a resource exists based purely on the parameters provided.

2. **Updated Logic**: The revised code utilizes `locate` from the `getResourceStreamLocator` to try and retrieve a resource stream, which likely involves more than just existence checking and considers the specific resource stream's actual retrievability.

3. **Conditional Update**: The condition changes from checking existence to checking if `packageResource` (the located resource stream) is non-null. The change suggests that the cache should only be generated if the resource stream can be actually located (is found), rather than merely existing.

These modifications imply a logical correction in the caching decision process, directly related to handling resources and their paths correctly. Given that this aligns with the commit message about fixing problems with an absolute path and improves the determination logic for caching resources, this change indicates a bug fix.

Thus, this change is **Buggy**."
wicket,6423.json,835db0bf267a6c1b713f9801237c68cba043e9af,"@@ -1,5 +1,15 @@
 	public Border remove(final Component component)
 	{
-		getBodyContainer().remove(component);
+		if (component == body)
+		{
+			// when the user calls foo.add(getBodyContainer()) this method will be called with it to
+			// clear body container's old parent, in which case we do not want to redirect to body
+			// container but to border's old remove.
+			super.remove(body);
+		}
+		else
+		{
+			getBodyContainer().remove(component);
+		}
 		return this;
 	}",Buggy,"fixes a bug where border's getbodycontainer is left as a child of border even if it was added to another container
",Buggy,"The commit message describes a bug fix where the `getBodyContainer` could incorrectly remain a child of the `Border` even after reassignment to another container. The code diff addresses this issue by adding a check: if `component` is the `body`, it calls `super.remove(body)` instead of `getBodyContainer().remove(component)`. This modification correctly differentiates between removing the `body` container itself and removing components from the `body` container, ensuring proper parent-child relationships.

The changes in the code align with the commit message and indicate a logical correction in the handling of component removal, resolving the described bug.

Therefore, the conclusion is **Buggy**."
wicket,9455.json,7480012fafc537e405b6c0a624c1b76f39d56f41,"@@ -1,29 +1,29 @@
 	public static Number getMinValue(Class<? extends Number> numberType)
 	{
 		Number result;
 		if (Integer.class == numberType || int.class == numberType)
 		{
 			result = Integer.MIN_VALUE;
 		}
 		else if (Long.class == numberType || long.class == numberType) {
 			result = Long.MIN_VALUE;
 		}
 		else if (Float.class == numberType || float.class == numberType) {
 			result = Float.MIN_VALUE;
 		}
 		else if (Double.class == numberType || double.class == numberType) {
 			result = Double.MIN_VALUE;
 		}
 		else if (Byte.class == numberType || byte.class == numberType) {
 			result = Byte.MIN_VALUE;
 		}
 		else if (Short.class == numberType || short.class == numberType) {
 			result = Short.MIN_VALUE;
 		}
 		else { // null of any other Number
-			LOG.debug(""'{}' has no maximum value. Falling back to Double.MIN_VALUE."", numberType);
+			LOG.debug(""'{}' has no minimum value. Falling back to Double.MIN_VALUE."", numberType);
 			result = Double.MIN_VALUE;
 		}
 
 		return result;
 	}",Buggy,"Fix a copy/paste error
",Buggy,"The diff involves a change in a log message within a method `getMinValue`. The log message is corrected from stating a ""maximum value"" to a ""minimum value,"" aligning with the method's purpose to return minimum numerical values. This is a clear indication of a copy/paste error in documentation rather than logic.

While this change itself does not alter the program's behavior, it does correct a misleading log message, which can be considered a minor bug or oversight in text. Since the commit focuses on correcting this error, based on the message and change, it constitutes a fix for a documentation-related bug.

Therefore, the answer is **Buggy**."
vraptor4,95.json,d57d1be4185c180ec3a99dab938a0cfaec66516c,"@@ -1,33 +1,33 @@
 	public <T> T forwardTo(final Class<T> type) {
 		return proxifier.proxify(type, new MethodInvocation<T>() {
 
 			@Override
 			public Object intercept(T proxy, Method method, Object[] args, SuperMethod superMethod) {
 				try {
 					logger.debug(""Executing {}"", method);
 					ControllerMethod old = methodInfo.getControllerMethod();
 					methodInfo.setControllerMethod(DefaultControllerMethod.instanceFor(type, method));
 					Object methodResult = method.invoke(container.instanceFor(type), args);
 					methodInfo.setControllerMethod(old);
 
 					Type returnType = method.getGenericReturnType();
 					if (!(returnType == void.class)) {
 						request.setAttribute(extractor.nameFor(returnType), methodResult);
 					}
-					if (response.isCommitted() || result.used()) {
+					if (response.isCommitted()) {
 						logger.debug(""Response already commited, not forwarding."");
 						return null;
 					}
 					String path = resolver.pathFor(DefaultControllerMethod.instanceFor(type, method));
 					logger.debug(""Forwarding to {}"", path);
 					request.getRequestDispatcher(path).forward(request, response);
 					return null;
 				} catch (InvocationTargetException e) {
 					propagateIfPossible(e.getCause());
 					throw new ProxyInvocationException(e);
 				} catch (Exception e) {
 					throw new ProxyInvocationException(e);
 				}
 			}
 		});
 	}",Buggy,"Fixing problem with forward.
",Buggy,"The commit message ""Fixing problem with forward"" suggests that the issue related to forwarding behavior is being addressed. Let's analyze the code changes:

1. **Original Code**:
   - Contains a check: `if (response.isCommitted() || result.used())`, which evaluates both if the HTTP response has already been committed or if the result has been used, before proceeding with forwarding.

2. **Modified Code**:
   - The modified line changes this check to: `if (response.isCommitted())`, removing the check for `result.used()`.

3. **Implications of the Change**:
   - The check for `result.used()` is removed, which means that forwarding now only depends on whether the response is already committed.
   - This change likely fixes an issue where forwarding might be incorrectly blocked due to a prior result usage check, which could have been overly restrictive if `response.isCommitted()` was a sufficient condition on its own.

4. **Evaluation**:
   - By removing the `result.used()` condition, the code caters purely to the state of the HTTP response. This likely aligns with intended forward behavior to ensure it only cares if the response has been committed, not the `result`'s state.
   - The commit successfully addresses a problem directly related to the forwarding logic, which aligns with the commit message mentioning fixing a problem with forward.

Based on this analysis, the changes in the code align with the intent of fixing a ""forward problem"" as indicated by the commit message, clearly suggesting a correction to behavior. Therefore, I conclude that this commit indicates a bug fix.

**Buggy**"
vraptor4,1079.json,cff51ec6e04f0bcb011c05510d72b8c4c22b1b66,"@@ -1,4 +1,13 @@
 	public <T extends View> T use(Class<T> view) {
+		if(view.isAssignableFrom(Results.json()) && validator.hasErrors()) {
+	        throw new IllegalStateException(
+	                ""There are validation errors and you forgot to specify where to go. Please add in your method ""
+	                        + ""something like:\n""
+	                        + ""validator.onErrorUse(page()).of(AnyController.class).anyMethod();\n""
+	                        + ""or any view that you like.\n""
+	                        + ""If you didn't add any validation error, it is possible that a conversion error had happened."");
+	    }
+		
 		responseCommitted = true;
 		return container.instanceFor(view);
 	}",Buggy,"Fixing problem: validation ignored on redirects.
",Buggy,"The commit message indicates a problem with validation being ignored on redirects. The provided code diff introduces a check for JSON results: before using a view, it verifies if there are any validator errors. If errors are present and not handled, it throws an `IllegalStateException` with a detailed message prompting the developer to specify a redirection path on validation errors.

Here's a breakdown of why this change corresponds to a bug fix:

1. **Error Checking**: The new code adds a condition that checks if `view` is of type JSON and if there are validation errors (`validator.hasErrors()`).

2. **Error Handling**: When both conditions are met (JSON view and validation errors present), it raises an `IllegalStateException`. The exception message instructs the developer to handle validation errors properly by adding a redirection.

3. **Commit Message Alignment**: The commit message explicitly states ""fixing problem: validation ignored on redirects"", which aligns with the code change ensuring that unhandled validation errors do not lead to unexpected redirects.

4. **Prevention of Future Errors**: Without this check, the previous implementation likely bypassed important validation error handling, leading to unexpected or erroneous states for users. This change ensures that validation errors are noticed and handled before proceeding with a JSON view, addressing the core issue mentioned in the commit message.

Given these points, the modifications clearly relate to correcting a bug concerning validation and redirect behavior, hence the assessment is **Buggy**."
flink,4079.json,1863bb88e4f2388a2f91b8000bcb3ca1e44d352b,"@@ -1,9 +1,8 @@
 	static SupplierWithException<BufferPool, IOException> createBufferPoolFactory(
 			BufferPoolFactory bufferPoolFactory,
 			int networkBuffersPerChannel,
 			int floatingNetworkBuffersPerGate,
 			int size,
 			ResultPartitionType type) {
-		int maxNumberOfMemorySegments = type.isBounded() ? floatingNetworkBuffersPerGate : Integer.MAX_VALUE;
-		return () -> bufferPoolFactory.createBufferPool(0, maxNumberOfMemorySegments);
+		return () -> bufferPoolFactory.createBufferPool(0, floatingNetworkBuffersPerGate);
 	}",Buggy,"[FLINK-14872][runtime] Temporary fix for potential deadlock problem when tasks read from blocking ResultPartitions. (#10472)

This commit implements a temporary fix for the potential deadlock problem reported in FLINK-14872. The problem itself is not solved completely, however the possibility of deadlock is largely reduced. We leave the proper fix of this problem to the future version.",Buggy,"The commit message discusses a temporary fix for a potential deadlock problem in tasks reading from blocking `ResultPartitions`. The provided code diff changes the logic for determining the number of maximum memory segments, directly passing `floatingNetworkBuffersPerGate` instead of conditionally assigning it or `Integer.MAX_VALUE` based on whether the `type` is bounded.

Here's the analysis step by step:

1. **Code Change Analysis**:
   - The original code uses a conditional to set `maxNumberOfMemorySegments`:
     - If the `type` is bounded, it sets it to `floatingNetworkBuffersPerGate`.
     - Otherwise, it sets it to `Integer.MAX_VALUE`.
   - The updated code directly uses `floatingNetworkBuffersPerGate` without a condition.

2. **Impact of Code Change**:
   - The original approach dynamically decides the maximum number of memory segments based on the bounded nature of the `type`.
   - By removing the condition and always using `floatingNetworkBuffersPerGate`, the logic changes such that it may prevent excessive memory allocation, which could be a source of the deadlock.
   - While this change does not solve the deadlock issue entirely, it helps control resource allocation which is mentioned as part of the deadlock mitigation.

3. **Commit Message Correlation**:
   - The commit message mentions it's a temporary fix for reducing the likelihood of deadlocks.
   - The change in the code corresponds directly to this by altering resource limits (memory segments), which likely affects task blocking behavior.
   - Although the message states the complete resolution is deferred, this fix addresses part of the problem in line with reducing deadlock possibility.

Given the modifications directly relate to memory management, a potential source of deadlock in concurrent programming, this code suggests a bug-related change, fulfilling the criteria of the commit message. Therefore, the conclusion is:

**Buggy**"
flink,18562.json,d8c74d20970e6cf379165ffafe8550a5b98d7c4d,"@@ -1,16 +1,19 @@
 		public boolean equals(final Object obj) {
+			if (obj == this) {
+				return true;
+			}
 
 			if (obj instanceof FSKey) {
 				final FSKey key = (FSKey) obj;
 
 				if (!this.scheme.equals(key.scheme)) {
 					return false;
 				}
 
 				if ((this.authority == null) || (key.authority == null)) {
 					return this.authority == null && key.authority == null;
 				}
 				return this.authority.equals(key.authority);
 			}
 			return false;
 		}",Buggy,"[FLINK-1766]Fix the bug of equals function of FSKey

The equals function in org.apache.flink.core.fs.FileSystem.FSKey should first confirm whether obj == this, if obj is the same object.It should return true

Author: hongsibao <hongsibao@huawei.com>

Closes #511 from matadorhong/Bug_Flink_1776 and squashes the following commits:

1ad06d7 [hongsibao] Fix the code format problem
431fc4b [hongsibao] Fix the code format problem
ab0ae5e [hongsibao] [FLINK-1766]Fix the bug of equals function of FSKey
",Buggy,"The commit message states a bug fix in the equals function of FSKey. The modified code adds an identity check, `if (obj == this)`, at the start of the equals method. This is a common optimization and correction in an equals method. It quickly handles the trivial case where the two objects being compared are the same instance, returning true immediately. 

This change aligns perfectly with the commit message, indicating a bug fix by improving the correctness of the equals method. Thus, the code change directly fixes the described issue in the commit message. The diff signifies addressing a logical oversight by adding a standard equality check.

Therefore, the appropriate conclusion for this evaluation is **Buggy**."
flink,2817.json,cbfb807d65b68b2b6157e1b1d42606123ea499ad,"@@ -1,8 +1,7 @@
 	public void stop() throws Exception {
 		LOG.info(""Stopping ZooKeeperLeaderRetrievalService."");
 
 		client.getConnectionStateListenable().removeListener(connectionStateListener);
 
 		cache.close();
-		client.close();
 	}",Buggy,"[FLINK-5190] [runtime] fix ZooKeeperLeaderRetrievalService close the zk client when stopping bug
",Buggy,"The commit message mentions fixing a bug in the `ZooKeeperLeaderRetrievalService` related to closing the ZooKeeper client when stopping. The code diff shows the removal of `client.close();` from the `stop` method.

Here's a breakdown of the situation:

1. **Commit Message Analysis**: The message indicates that there is an issue with closing the ZooKeeper client in the stop process which is supposed to be fixed.

2. **Code Analysis**: The code diff removes `client.close();` from the `stop()` method.

3. **Understanding the Change**:
   - Typically, removing a close call might seem contradictory to the intention of properly stopping resources or services. However, it’s possible the bug involves a situation where closing the client here is either redundant (handled elsewhere) or erroneous (e.g., causing exceptions or being closed prematurely).

4. **Bug Analysis**:
   - If the client closure is managed differently or at a different lifecycle stage, removing it here could indeed resolve a bug.
   - Additionally, if the close operation on the client causes unforeseen issues during stop, this code change aligns with the commit message's intention to fix a bug.

Given the context provided in the commit message and the nature of the change, it suggests that an incorrect handling of the client closure during stopping was causing issues, which warrants its removal in this location.

Therefore, the answer is **Buggy**."
flink,43722.json,f0ed58c6a580db0966104b81491d08d25d1ff57e,"@@ -1,32 +1,33 @@
 		public void reduce(Iterable<T> records, Collector<T> out) {
 			final AggregationFunction<Object>[] aggFunctions = this.aggFunctions;
 			final int[] fieldPositions = this.fieldPositions;
 
 			// aggregators are initialized from before
 
 			T current = null;
 			final Iterator<T> values = records.iterator();
 			while (values.hasNext()) {
 				current = values.next();
 
 				for (int i = 0; i < fieldPositions.length; i++) {
 					Object val = current.productElement(fieldPositions[i]);
 					aggFunctions[i].aggregate(val);
 				}
 			}
 
 			Object[] fields = new Object[serializer.getArity()];
+			int length = serializer.getArity();
 			// First copy all tuple fields, then overwrite the aggregated ones
-			for (int i = 0; i < fieldPositions.length; i++) {
-				fields[0] = current.productElement(i);
+			for (int i = 0; i < length; i++) {
+				fields[i] = current.productElement(i);
 			}
 			for (int i = 0; i < fieldPositions.length; i++) {
 				Object aggVal = aggFunctions[i].getAggregate();
 				fields[fieldPositions[i]] = aggVal;
 				aggFunctions[i].initializeAggregate();
 			}
 
 			T result = serializer.createInstance(fields);
 
 			out.collect(result);
 		}",Buggy,"Fix Bug in ScalaAggregate Operator and add ITCase
",Buggy,"The commit message indicates a bug fix in the ScalaAggregate Operator. The diff in the provided code shows a change in the way tuple fields are copied. Originally, the code erroneously used fields[0] for all initial field copies, overwriting the same position repeatedly. This was incorrect, leading to potential data corruption or logical flaws in aggregated results. 

The updated code fixes this by iterating over all elements of the tuple and copying them correctly to their respective positions in the fields array. This fix aligns with the commit message stating a bug fix in the ScalaAggregate Operator. As this correction resolves a clear logical issue in the data processing inside the reduce method, this change is indeed a bug fix.

Therefore, the changes reflect a bug fix, so the conclusion is **Buggy**."
flink,25849.json,0081fb2ef2bd03d06a786dd8988865d2ff6168c2,"@@ -1,5 +1,5 @@
 		public void combine(Iterable<Tuple3<K1, K2, IN>> values, Collector<Tuple3<K1, K2, IN>> out) throws Exception {
 			iter.set(values.iterator());
 			coll.set(out);
-			this.wrappedFunction.combine(iter, coll);
+			((GroupCombineFunction)this.wrappedFunction).combine(iter, coll);
 		}",Buggy,"[FLINK-2135] Fix faulty cast to GroupReduceFunction

This closes #769
",Buggy,"The commit message claims to fix a faulty cast to GroupReduceFunction. The changes in the code replace a method call on `wrappedFunction` with a cast to `GroupCombineFunction` before calling `combine`. This indicates that the `wrappedFunction` needed to be specifically treated as a `GroupCombineFunction` rather than relying on its current type. Misuse of an incorrect class type could lead to runtime errors, therefore, addressing this casting issue is consistent with a bug fix.

Here is the step-by-step analysis:

1. **Problem Identification**: Originally, the method `combine` is called on `this.wrappedFunction` without ensuring that it is explicitly recognized as a `GroupCombineFunction`. This could potentially lead to issues if `wrappedFunction` wasn’t already an instance of `GroupCombineFunction`, possibly resulting in a `ClassCastException` at runtime if the types did not align as expected.

2. **Modification**: By explicitly casting `this.wrappedFunction` to `GroupCombineFunction`, it ensures the method `combine` being called is specific to the `GroupCombineFunction` interface. This indicates the need for such a cast was identified during execution, revealing a bug where the expected interface was not being used correctly.

3. **Commit Message & Code Alignment**: The commit message accurately describes the code change. The change is intended to fix a type-casting issue which could lead to bugs during the execution of the program by ensuring the correct method is invoked on the appropriate interface type.

Given this understanding, the change corrects a casting flaw which, if left unaddressed, could lead to erroneous behavior. Therefore, the appropriate conclusion based on the analysis of the diff and commit message is **Buggy**."
flink,17423.json,6624562982c9d57bebba8cb4b574b8ed28640a0d,"@@ -1,15 +1,17 @@
 	public static DeweyNumber fromString(final String deweyNumberString) {
 		String[] splits = deweyNumberString.split(""\\."");
 
-		if (splits.length == 0) {
+		if (splits.length == 1) {
 			return new DeweyNumber(Integer.parseInt(deweyNumberString));
-		} else {
+		} else if (splits.length > 0) {
 			int[] deweyNumber = new int[splits.length];
 
 			for (int i = 0; i < splits.length; i++) {
 				deweyNumber[i] = Integer.parseInt(splits[i]);
 			}
 
 			return new DeweyNumber(deweyNumber);
+		} else {
+			throw new IllegalArgumentException(""Failed to parse "" + deweyNumberString + "" as a Dewey number"");
 		}
 	}",Buggy,"[FLINK-13043] [Library / CEP] Fix the bug of parsing Dewey number from string

This closes #8936
",Buggy,"This diff addresses a bug in the method fromString for parsing Dewey numbers. Here's a step-by-step analysis:

1. **Initial Check**: 
   - The original code checks if `splits.length == 0`, but this condition is logically incorrect given that the `split(""\\."")` will always result in at least one element in the `splits` array even if `deweyNumberString` is empty (resulting in an array with a single empty string).
   - The updated code checks `splits.length == 1` to handle cases where there's no dot in `deweyNumberString`, meaning it's a single-part Dewey number.

2. **Else Clause Adjustment**: 
   - Originally, the else block did not specifically handle an invalid case, potentially resulting in an incorrect Dewey number construction when input was not properly formatted.
   - The updated code includes a condition `else if (splits.length > 0)` for correct Dewey number parsing and adds a new `else` clause to throw an `IllegalArgumentException` for invalid input that results in an unexpected case, which previously went unchecked.

3. **Bug Fix Justification**:  
   - The change to `splits.length == 1` addresses the logical error of parsing a single-part Dewey number.
   - The added exception handling (`IllegalArgumentException`) improves robustness by ensuring invalid input is appropriately flagged, preventing potentially erroneous Dewey number objects from being created.

Overall, the modifications align exactly with fixing a parsing bug as described in the commit message, correctly handling cases for both valid single-part and multi-part Dewey numbers and throwing an exception for illogical input forms.

Therefore, the answer is **Buggy**."
flink,18090.json,624cb64a462765419bb0f81e2a50b3cec1c877f6,"@@ -1,25 +1,22 @@
 	public void copyNormalizedKey(MemorySegment target, int offset, int len) {
+		// note that the char is an unsigned data type in java and consequently needs
+		// no code that transforms the signed representation to an offsetted representation
+		// that is equivalent to unsigned, when compared byte by byte
 		if (len == 2) {
 			// default case, full normalized key
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 		}
 		else if (len <= 0) {
 		}
 		else if (len == 1) {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
 		}
 		else {
-			int highByte = ((value >>> 8) & 0xff);
-			highByte -= Byte.MIN_VALUE;
-			target.put(offset, (byte) highByte);
-			target.put(offset + 1, (byte) ((value) & 0xff));
+			target.put(offset,     (byte) ((value >>> 8) & 0xff));
+			target.put(offset + 1, (byte) ((value      ) & 0xff));
 			for (int i = 2; i < len; i++) {
 				target.put(offset + i, (byte) 0);
 			}
 		}
 	}",Buggy,"Fixed erroneous normalized key generation in PactCharacter.
",Buggy,"The commit message states that an error in normalized key generation has been fixed. The code changes reflect this message accurately by removing unnecessary transformations that adjusted for signed-byte representation. The logic has been simplified to directly place bytes without offsetting, aligning with the statement that `char` is unsigned in Java and doesn't need such adjustments. This indicates a correction in logic for generating normalized keys correctly, which would be considered a bug fix.

Thus, these modifications are indeed a bug fix. The conclusion is **Buggy**."
flink,18559.json,f04b32b02900181db82c6c518ffbd6430a0a369a,"@@ -1,40 +1,40 @@
 	private static HashMap<String, FileSystemFactory> loadFileSystems() {
 		final HashMap<String, FileSystemFactory> map = new HashMap<>();
 
 		// by default, we always have the local file system factory
 		map.put(""file"", new LocalFileSystemFactory());
 
 		LOG.debug(""Loading extension file systems via services"");
 
 		try {
 			ServiceLoader<FileSystemFactory> serviceLoader = ServiceLoader.load(FileSystemFactory.class);
 			Iterator<FileSystemFactory> iter = serviceLoader.iterator();
 
 			// we explicitly use an iterator here (rather than for-each) because that way
 			// we can catch errors in individual service instantiations
 
 			//noinspection WhileLoopReplaceableByForEach
 			while (iter.hasNext()) {
 				try {
 					FileSystemFactory factory = iter.next();
 					String scheme = factory.getScheme();
 					map.put(scheme, factory);
 					LOG.debug(""Added file system {}:{}"", scheme, factory.getClass().getName());
 				}
 				catch (Throwable t) {
 					// catching Throwable here to handle various forms of class loading
 					// and initialization errors
 					ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
-					LOG.error(""Failed to load a file systems via services"", t);
+					LOG.error(""Failed to load a file system via services"", t);
 				}
 			}
 		}
 		catch (Throwable t) {
 			// catching Throwable here to handle various forms of class loading
 			// and initialization errors
 			ExceptionUtils.rethrowIfFatalErrorOrOOM(t);
 			LOG.error(""Failed to load additional file systems via services"", t);
 		}
 
 		return map;
 	}",Buggy,"[hotfix] [core] Fix lots of checkstyle errors in core.fs
",NotBuggy,"The given diff adjusts a logged error message to correct a grammatical mistake, changing ""file systems"" to ""file system."" The commit message refers to fixing checkstyle errors, which generally indicates addressing formatting, naming conventions, and similar code style issues. This specific change addresses a minor textual error in a log message, which is consistent with style and correctness improvements rather than functional changes or bug fixes in logic. There are no indications of bug fixes related to logical errors, exception handling, or error-handling logic improvements in the presented diff.

Therefore, this code change aligns well with the commit message regarding fixing checkstyle errors but does not suggest a bug fix related to a functional aspect of the code itself.

The conclusion based on the analysis is **NotBuggy**."
flink,11393.json,84b39dcb50167368d45882a88842760237fa3560,"@@ -1,19 +1,23 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
 		}
+		catch (LinkageError e) {
+			// hadoop classes are not in the classpath
+			LOG.debug(""Cannot determine user/group information using Hadoop utils. "" +
+					""Hadoop classes not loaded or compatible"", e);
+		}
 		catch (Throwable t) {
-			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
-				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
-			}
+			// some other error occurred that we should log and make known
+			LOG.warn(""Error while accessing user/group information via Hadoop utils."", t);
 		}
 		
 		String user = System.getProperty(""user.name"");
 		if (user == null) {
 			user = UNKNOWN;
 			if (LOG.isDebugEnabled()) {
 				LOG.debug(""Cannot determine user/group information for the current user."");
 			}
 		}
 		return user;
 	}",Buggy,"[hotfix] Fix error messages in EnvironmentInformation when accessing user information
",Buggy,"The commit message specifies fixing error messages in a method that retrieves user information. The diff addresses error handling improvements by adding distinctions between exceptions. Specifically:

1. A `LinkageError` catch block is introduced to log cases when Hadoop classes are not present, improving error clarity when a critical library is unavailable.
2. The existing catch block for `Throwable` now differentiates between expected and unexpected exceptions. Logging for unexpected conditions is upgraded from debug to warn, while ClassNotFoundException is no longer explicitly handled, improving error visibility.

These changes are aimed at more informative and precise logging in cases of failure, aligning with the commit message's intention to fix error messages. This qualifies as a bug fix related to error handling and messaging.

The assessment of the message and corresponding code change indicates a **Buggy** conclusion as it addresses error handling related to accessing user information."
flink,11393.json,e45534cf2275f6918294c81de5377146c243904c,"@@ -1,12 +1,19 @@
 	public static String getUserRunning() {
 		try {
 			return UserGroupInformation.getCurrentUser().getShortUserName();
-		} catch (Throwable t) {
-			if (LOG.isDebugEnabled()) {
-				LOG.debug(""Cannot determine user/group information for the current user."", t);
-			} else {
-				LOG.info(""Cannot determine user/group information for the current user."");
-			}
-			return UNKNOWN;
 		}
+		catch (Throwable t) {
+			if (LOG.isDebugEnabled() && !(t instanceof ClassNotFoundException)) {
+				LOG.debug(""Cannot determine user/group information using Hadoop utils."", t);
+			}
+		}
+		
+		String user = System.getProperty(""user.name"");
+		if (user == null) {
+			user = UNKNOWN;
+			if (LOG.isDebugEnabled()) {
+				LOG.debug(""Cannot determine user/group information for the current user."");
+			}
+		}
+		return user;
 	}",Buggy,"Fix problems with missing files/dependencies with cdh-4 distribution.
",Buggy,"The commit message indicates a fix for issues related to missing files or dependencies, likely in context of a specific distribution. This code change modifies error handling in `getUserRunning` by capturing `ClassNotFoundException` separately, and if the Hadoop utils fail, it defaults to a system property to get the user name. The refinements both handle missing dependencies/class issues better and ensure fallback logic, which aligns with fixing missing dependency problems.

Therefore, this commit addresses error handling and logical corrections related to dependencies as described in the message. The conclusion is **Buggy**."
flink,3887.json,4d693c4fbc5e6f3ff34ccb3cb3a1d9f35d6bbd76,"@@ -1,25 +1,27 @@
 	private void writeAndCloseBufferConsumer(BufferConsumer bufferConsumer) throws IOException {
 		try {
 			final Buffer buffer = bufferConsumer.build();
 			try {
 				if (canBeCompressed(buffer)) {
 					final Buffer compressedBuffer = parent.bufferCompressor.compressToIntermediateBuffer(buffer);
 					data.writeBuffer(compressedBuffer);
-					compressedBuffer.recycleBuffer();
+					if (compressedBuffer != buffer) {
+						compressedBuffer.recycleBuffer();
+					}
 				} else {
 					data.writeBuffer(buffer);
 				}
 
 				numBuffersAndEventsWritten++;
 				if (buffer.isBuffer()) {
 					numDataBuffersWritten++;
 				}
 			}
 			finally {
 				buffer.recycleBuffer();
 			}
 		}
 		finally {
 			bufferConsumer.close();
 		}
 	}",Buggy,"[FLINK-15166][runtime] Fix the bug of wrongly recycling uncompressed buffer

For blocking shuffle data compression, the compressed intermediate buffer is recycled after it is written out. However when the data can not be compressed, the returned buffer is the original buffer which should not be recycled.

This commit fixes the bug of wrongly recycling uncompressed buffer by comparing the returned buffer with the original buffer.",Buggy,"The commit message describes a bug where uncompressed buffers are wrongly recycled when they shouldn't be. In the provided code diff, we see an additional condition: `if (compressedBuffer != buffer)`. This checks whether the compressed buffer differs from the original buffer. If they are different, only then is the `compressedBuffer` recycled. This change ensures that only genuinely new buffers created during compression are recycled, addressing the bug described in the commit message by preventing the original buffer from being erroneously recycled if it is returned as the compressed buffer when no compression actually occurred.

The code diff directly aligns with the commit message by fixing the improper recycling of uncompressed buffers through a logical check. Hence, the change indeed indicates a bug fix.

The assessment is **Buggy**."
flink,18028.json,0f28095f9f79b49d45905e61709ff4fab6fedd54,"@@ -1,8 +1,8 @@
 	public Appendable append(CharSequence csq, int start, int end) {
 		final int otherLen = end - start;
 		grow(this.len + otherLen);
-		for (int pos = start; pos < len; pos++)
+		for (int pos = start; pos < end; pos++)
 			this.value[this.len + pos] = csq.charAt(pos);
 		this.len += otherLen;
 		return this;
 	}",Buggy,"Fixed bug in PactString append.
",Buggy,"The commit message indicates a bug fix in the `PactString append` method. Let's analyze the code change:

- The loop was previously using `len` as its upper boundary in `for (int pos = start; pos < len; pos++)`, which would have resulted in an incorrect range or potential runtime error since `len` could exceed the intended `end`.
- The corrected code now iterates up to `end` by changing the condition to `for (int pos = start; pos < end; pos++)`.

This adjustment ensures the loop correctly processes the intended substring from `start` to `end`. This correction aligns with the commit message describing a bug fix in the `append` method. Therefore, the changes indicate a bug fix.

**Buggy**"
flink,40862.json,6cf15c29b1bf3bb3ddcf69157079f9e5b185d9c8,"@@ -1,5 +1,4 @@
 	public SingleOutputStreamOperator<OUT, ?> reduce(ReduceFunction<OUT> reducer) {
 		return dataStream.addFunction(""groupReduce"", reducer, getTypeWrapper(reducer),
-				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition))
-				.partitionBy(keyPosition);
+				getTypeWrapper(reducer), new GroupReduceInvokable<OUT>(reducer, keyPosition));
 	}",Buggy,"[streaming] Minor bug and license header fixes
",Buggy,"Analyzing the commit message and the code diff:

1. **Commit Message**: It mentions ""Minor bug and license header fixes"". This indicates that the changes might address small issues, one of which could be a bug.

2. **Code Analysis**: 
   - The diff removes the `partitionBy(keyPosition)` method call in the chain.
   - This change suggests that the partitioning logic was mistakenly applied or not required here. 

3. **Bug Indication**:
   - Removing `partitionBy()` might resolve an unintended or erroneous behavior such as incorrect partitioning of stream processing, which could lead to incorrect processing results or performance issues. 

The change corrects behavior, consistent with fixing minor bugs. The answer is **Buggy**."
flink,18045.json,6e9b2848d5fabace5c6ef491c87c562eed9b5f43,"@@ -1,27 +1,26 @@
 	public void copy(DataInputView in, DataOutputView target) throws IOException {
 		int len = in.readUnsignedByte();
 		target.writeByte(len);
 
 		if (len >= HIGH_BIT) {
 			int shift = 7;
 			int curr;
 			len = len & 0x7f;
 			while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
 				len |= (curr & 0x7f) << shift;
 				shift += 7;
 				target.writeByte(curr);
 			}
 			len |= curr << shift;
+			target.writeByte(curr);
 		}
 
 		for (int i = 0; i < len; i++) {
 			int c = in.readUnsignedByte();
 			target.writeByte(c);
-			if (c >= HIGH_BIT) {
-				int curr;
-				while ((curr = in.readUnsignedByte()) >= HIGH_BIT) {
-					target.writeByte(curr);
-				}
+			while (c >= HIGH_BIT) {
+				c = in.readUnsignedByte();
+				target.writeByte(c);
 			}
 		}
 	}",Buggy,"[FLINK-1336] [core] Fix bug in StringValue binary copy method
",Buggy,"The diff shows changes in the `copy` method of a class, likely responsible for handling binary serialization of a `StringValue`. Here's a detailed analysis:

1. **Original Logic**:
   - Reads a byte (`len`) from `DataInputView`.
   - If the byte is 128 or greater (`len >= HIGH_BIT`), it performs additional processing to read a variable-length integer:
     - Continues reading bytes, applying a bitmask, and shifting until a byte smaller than `HIGH_BIT` is found.
     - During this process, each byte read is also written into `DataOutputView`.
   - After determining the length (`len`), it reads `len` bytes from the input and writes each to the output.
   - If a byte is greater than or equal to `HIGH_BIT`, additional bytes are read and written until a byte smaller than `HIGH_BIT` is found.

2. **Changes Made**:
   - Adds `target.writeByte(curr);` when reading the final byte of the variable-length integer.
   - Modifies logic to continuously write bytes that are `>= HIGH_BIT` after the main `for` loop.

3. **Bug Fix Analysis**:
   - The primary bug appears to be related to writing bytes when handling bytes that are `>= HIGH_BIT`. The original code writes these bytes separately, which is error-prone and inefficient.
   - The fix ensures that all relevant bytes are correctly written to the output, maintaining the intended copy logic, particularly for variable-length integers and characters with high bits set.

The changes align with the commit message, indicating a fix for the `StringValue` binary copy method. The logic has been improved to handle high-bit bytes correctly and consistently, addressing issues in the original implementation.

Therefore, the conclusion is **Buggy**."
flink,26584.json,32440227eceb122008906878136156ffa839efa6,"@@ -1,3 +1,4 @@
 	public String toString() {
-		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism()) + "") : "" + getIdString();
+		return ""Local Environment (DOP = "" + (getDegreeOfParallelism() == -1 ? ""default"" : getDegreeOfParallelism())
+				+ ""Number task manager = "" + getNumTaskManager() + "") : "" + getIdString();
 	}",Buggy,"Fixed free memory calculation bug in case where multiple task manager run on the same jvm.
",NotBuggy,"The change in the code modifies the `toString` method to include the number of task managers in the output string. This modification does not relate to memory calculation directly. The commit message specifies a bug fix in free memory calculation for multiple task managers running on the same JVM, but there is no evidence of memory handling or management being altered or corrected in the provided diff. The updated string output is informational and does not align with the described memory calculation concern.

Therefore, the code change does not directly relate to fixing a bug described in the commit message, which is about memory calculation. The answer is **NotBuggy**."
flink,19363.json,7e10e2b7ba967332740e1058efa7310fe2834123,"@@ -1,22 +1,24 @@
 	public String toString()
 	{
 		if (this.indexes.size() == 0) {
 			return ""(none)"";
 		}
 		final StringBuffer buf = new StringBuffer();
 		for (int i = 0; i < indexes.size(); i++) {
 			if (buf.length() == 0) {
 				buf.append(""["");
 			}
 			else {
 				buf.append("","");
 			}
 			buf.append(this.indexes.get(i));
-			buf.append("":"");
-			buf.append(this.types.get(i).getName());
+			if (this.types.get(i) != null) {
+				buf.append("":"");
+				buf.append(this.types.get(i).getName());
+			}
 			buf.append("":"");
 			buf.append(this.orders.get(i).name());
 		}
 		buf.append(""]"");
 		return buf.toString();
 	}",Buggy,"Fixed miscellaneous minor bugs in compiler.
",Buggy,"The changes in the code handle a potential `NullPointerException` when accessing `this.types.get(i).getName()`. Previously, the code always called `getName()` without checking for `null`, which would cause a crash if `types.get(i)` returned `null`. The fix introduces a null check before accessing `getName()`.

This update aligns with the commit message ""Fixed miscellaneous minor bugs in compiler,"" as it corrects a small bug related to handling potentially null values, which is a common source of minor bugs in software.

The reasoning for considering this a bug fix involves:
1. **Error Handling Improvement**: Adding a null check to prevent a potential `NullPointerException`.
2. **Conditionally Appending**: Only appending `:` and the name if the type is not `null`.

Therefore, these changes clearly indicate a minor bug fix, as stated in the commit message. Thus, the correct conclusion is **Buggy**."
flink,4441.json,81a7837a942668c23c795e5bd8a68c4d17009f85,"@@ -1,32 +1,35 @@
 	public void close() throws IOException {
 		// atomically set the close flag
 		synchronized (this.closeLock) {
 			if (this.closed) {
 				return;
 			}
 			this.closed = true;
 			
 			try {
 				// wait until as many buffers have been returned as were written
 				// only then is everything guaranteed to be consistent.
 				while (this.requestsNotReturned.get() > 0) {
 					try {
 						// we add a timeout here, because it is not guaranteed that the
 						// decrementing during buffer return and the check here are deadlock free.
 						// the deadlock situation is however unlikely and caught by the timeout
 						this.closeLock.wait(1000);
 						checkErroneous();
 					}
 					catch (InterruptedException iex) {
 						throw new IOException(""Closing of asynchronous file channel was interrupted."");
 					}
 				}
+
+				// Additional check because we might have skipped the while loop
+				checkErroneous();
 			}
 			finally {
 				// close the file
 				if (this.fileChannel.isOpen()) {
 					this.fileChannel.close();
 				}
 			}
 		}
 	}",Buggy,"[FLINK-1545] [runtime][tests] Fixes AsynchronousFileIOChannelsTest.testExceptionForwardsToClose by introducing additional error check in AsynchronousFileIOChannel.close method

This closes #399
",Buggy,"The commit message indicates that the changes are intended to fix a specific test by introducing an additional error check during the closure of a file channel in the AsynchronousFileIOChannel class.

Here's an analysis of the changes:

1. **Synchronized Block**: The method is wrapped in a synchronized block using `closeLock` to ensure that the close operation is thread-safe.

2. **Error Handling in the Loop**: Inside the loop, the method is already calling `checkErroneous()` while waiting for requests to be returned. This method appears to be related to checking for errors during the asynchronous operation.

3. **Additional Error Check**: The primary change introduced in this diff is the addition of another `checkErroneous()` call immediately after the loop. This addition ensures that if the loop is bypassed (e.g., if `this.requestsNotReturned.get()` is zero from the outset), errors are still checked, which might have been missed otherwise.

4. **Exception Handling**: Any `InterruptedException` during the wait is caught and rethrown as an `IOException`, which is appropriate for I/O-related operations, maintaining the method’s contract specified by its signature.

The inclusion of an additional error check immediately outside the loop directly addresses the potential issue where the error-checking logic could be skipped if the loop does not execute. This ensures that any errors during the file channel operations are not missed, even if no iterations of the loop are made.

Given the nature of these changes, they align well with typical bug fix scenarios where exception handling is improved to cover all code paths, which in this case is intended to ensure that errors are consistently checked and properly propagated during the closing process.

Thus, the changes are indeed indicative of a bug fix related to error handling, consistent with the commit message.

**Buggy**"
flink,25651.json,ce822bf7f5ec80df5d5a749b1439320af3fb8b18,"@@ -1,4 +1,7 @@
 	public <R> DeltaIteration<T, R> iterateDelta(DataSet<R> workset, int maxIterations, int... keyPositions) {
+		Preconditions.checkNotNull(workset);
+		Preconditions.checkNotNull(keyPositions);
+		
 		Keys.ExpressionKeys<T> keys = new Keys.ExpressionKeys<T>(keyPositions, getType(), false);
 		return new DeltaIteration<T, R>(getExecutionEnvironment(), getType(), this, workset, keys, maxIterations);
 	}",Buggy,"[FLINK-1254] [compiler] Fix compiler bug for pipeline breaker placement

This closes #216
",Buggy,"The commit message indicates that this change addresses a bug in the placement of pipeline breakers in the Flink compiler. 

The code changes introduce `Preconditions.checkNotNull()` checks for the `workset` and `keyPositions` parameters. This ensures that the method `iterateDelta` is not called with `null` arguments, which could potentially lead to runtime exceptions or incorrect behavior if null values were previously allowed and not properly handled.

Ensuring non-null arguments at the start of a method is a common technique for defensive programming. These changes improve error handling by preventing a method from proceeding with invalid inputs, which aligns with fixing a bug related to unexpected behavior during execution.

Given the context and changes provided, this commit indeed appears to address a bug related to ensuring valid arguments, thereby aligning with the commit message's goal of fixing a compiler bug. 

The assessment is **Buggy**."
flink,18032.json,1bfeeaead158051cb5f78f403c4878fc89596cc8,"@@ -1,23 +1,21 @@
 	public void write(final DataOutput out) throws IOException {
 		final int maxBit = 0x1 << 7;
 
 		int len = this.value.length();
 
 		while (len >= maxBit) {
 			out.write(len | maxBit);
 			len >>= 7;
 		}
 		out.write(len);
 
 		for (int i = 0; i < this.value.length(); i++) {
 			int c = this.value.charAt(i);
 
-			if (c < maxBit)
-				out.write(c);
-			else
-				while (c >= maxBit) {
-					out.write(c | maxBit);
-					c >>= 7;
-				}
+			while (c >= maxBit) {
+				out.write(c | maxBit);
+				c >>= 7;
+			}
+			out.write(c);
 		}
 	}",Buggy,"Fixed Serialization Bug in PactString
",Buggy,"The commit message claims a serialization bug fix in `PactString`. The diff modifies character serialization logic in the `write` method. Originally, characters less than `maxBit` (128) were directly written out, which might cause inconsistencies if subsequent serialization required handling higher bits shifted versions of `c`.

- The updated logic consistently handles each character byte by repeatedly shifting and writing a portion combined with `maxBit` until the character's value fits under `maxBit`, ensuring proper serialization. 
- The consistent write operation for every character eliminates previous conditional inconsistencies that could lead to serialization bugs.

Given these adjustments for character serialization, the changes align with the commit message indicating a fix for a serialization bug. Thus, the conclusion is **Buggy**."
flink,22768.json,3854552ceefd2b2b9c0e2a9b6152a7fcb69153fe,"@@ -1,26 +1,32 @@
 	public static String unresolvedHostToNormalizedString(String host) {
 		// Return loopback interface address if host is null
 		// This represents the behavior of {@code InetAddress.getByName } and RFC 3330
 		if (host == null) {
 			host = InetAddress.getLoopbackAddress().getHostAddress();
 		} else {
 			host = host.trim().toLowerCase();
+			if (host.startsWith(""["") && host.endsWith(""]"")) {
+				String address = host.substring(1, host.length() - 1);
+				if (IPAddressUtil.isIPv6LiteralAddress(address)) {
+					host = address;
+				}
+			}
 		}
 
 		// normalize and valid address
 		if (IPAddressUtil.isIPv6LiteralAddress(host)) {
 			byte[] ipV6Address = IPAddressUtil.textToNumericFormatV6(host);
 			host = getIPv6UrlRepresentation(ipV6Address);
 		} else if (!IPAddressUtil.isIPv4LiteralAddress(host)) {
 			try {
 				// We don't allow these in hostnames
 				Preconditions.checkArgument(!host.startsWith("".""));
 				Preconditions.checkArgument(!host.endsWith("".""));
 				Preconditions.checkArgument(!host.contains("":""));
 			} catch (Exception e) {
 				throw new IllegalConfigurationException(""The configured hostname is not valid"", e);
 			}
 		}
 
 		return host;
 	}",Buggy,"[FLINK-12840] [core] Fix network utils to work with ipv6 correctly

  - Fixes problems around akka configuration parsing with some IPv6 literals
  - Fixes an issue with address parsing and validation with some Ipv6 literals

This closes #8734
",Buggy,"The commit message indicates this change is part of a fix for IPv6-related issues in the network utilities.

Analyzing the provided diff:

1. **IPv6 Bracket Handling**: The modification deals specifically with IPv6 addresses enclosed in square brackets, which is a common representation format, especially in URLs (e.g., `[IPv6]:port`). The added code removes these brackets when the content within them is identified as a valid IPv6 literal. This is consistent with the IPv6 parsing and configuration stated in the commit message.

2. **Address Validation and Normalization**:
   - The condition `if (host.startsWith(""["") && host.endsWith(""]""))` ensures that only when an IPv6 literal is enclosed in brackets, it is stripped of these, to be further correctly processed by utilities like `IPAddressUtil.isIPv6LiteralAddress`.
   - The function continues the validation for both IPv6 and IPv4, ensuring the input remains compliant with expectations and behaves correctly across different address inputs.

These changes directly address the issues with IPv6 literal parsing and validation as mentioned in the commit message. This suggests that there was indeed a bug concerning the handling of IPv6 addresses, specifically when they were present in their bracketed form, and this diff corrects that handling.

Therefore, the relevance and nature of the changes indicate a bug fix related to IPv6 handling, aligning with the commit message.

The conclusion is **Buggy**."
flink,28054.json,6e5954e8a03ad5d440447a57098976b0250f4f72,"@@ -1,43 +1,45 @@
 	private RexNode convertOver(List<Expression> children) {
 		List<Expression> args = children;
 		Expression agg = args.get(0);
 		SqlAggFunction aggFunc = agg.accept(new SqlAggFunctionVisitor(typeFactory));
 		RelDataType aggResultType = typeFactory.createFieldTypeFromLogicalType(
 				fromDataTypeToLogicalType(((ResolvedExpression) agg).getOutputDataType()));
 
 		// assemble exprs by agg children
 		List<RexNode> aggExprs = agg.getChildren().stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 
 		// assemble order by key
 		Expression orderKeyExpr = args.get(1);
 		Set<SqlKind> kinds = new HashSet<>();
 		RexNode collationRexNode = createCollation(orderKeyExpr.accept(this), RelFieldCollation.Direction.ASCENDING,
 				null, kinds);
 		ImmutableList<RexFieldCollation> orderKey = ImmutableList
 				.of(new RexFieldCollation(collationRexNode, kinds));
 
 		// assemble partition by keys
 		List<RexNode> partitionKeys = args.subList(4, args.size()).stream().map(expr -> expr.accept(this))
 				.collect(Collectors.toList());
 		// assemble bounds
 		Expression preceding = args.get(2);
-		boolean isPhysical = ((ResolvedExpression) preceding).getOutputDataType().equals(DataTypes.BIGINT());
+		boolean isPhysical = LogicalTypeChecks.hasRoot(
+				fromDataTypeToLogicalType(((ResolvedExpression) preceding).getOutputDataType()),
+				LogicalTypeRoot.BIGINT);
 		Expression following = args.get(3);
 		RexWindowBound lowerBound = createBound(preceding, SqlKind.PRECEDING);
 		RexWindowBound upperBound = createBound(following, SqlKind.FOLLOWING);
 
 		// build RexOver
 		return relBuilder.getRexBuilder().makeOver(
 				aggResultType,
 				aggFunc,
 				aggExprs,
 				partitionKeys,
 				orderKey,
 				lowerBound,
 				upperBound,
 				isPhysical,
 				true,
 				false,
 				false);
 	}",Buggy,"[FLINK-13107][table-planner-blink] Fix Bug to check whether OverCall is RowMode or RangeMode.
",Buggy,"The commit message states that this change is about fixing a bug related to checking whether ""OverCall"" is in ""RowMode"" or ""RangeMode"". The diff indeed modifies a critical part of that decision logic. The original code checks if the `preceding` expression's data type is `DataTypes.BIGINT()` to determine if it's a physical interval. The updated code improves on this by using the `LogicalTypeChecks.hasRoot` method to check if the logical type's root is `LogicalTypeRoot.BIGINT`, which is more robust since it directly focuses on the logical type rather than relying on a specific DataType constant. 

This kind of improvement aligns with fixing a bug because:

1. It addresses the logical verification of the interval type more accurately, allowing the system to correctly determine whether the ""OverCall"" is physical or logical, which could have been misclassified previously.

2. By correctly identifying the interval type, it ensures the system applies appropriate handling and resources according to the mode being used, which could prevent errors that arise from misunderstanding the mode.

Thus, this change corrects a potential misclassification bug in the behavior of the function. Therefore, the assessment is **Buggy**."
flink,22909.json,b01641bcc13631b0db82c54143670613babb7c0c,"@@ -1,79 +1,79 @@
 	public NumberSequenceIterator[] split(int numPartitions) {
 		if (numPartitions < 1) {
 			throw new IllegalArgumentException(""The number of partitions must be at least 1."");
 		}
 		
 		if (numPartitions == 1) {
 			return new NumberSequenceIterator[] { new NumberSequenceIterator(current, to) };
 		}
 		
 		// here, numPartitions >= 2 !!!
 		
 		long elementsPerSplit;
 		
-		if (to - current >= 0) {
-			elementsPerSplit = (to - current) / numPartitions;
+		if (to - current + 1 >= 0) {
+			elementsPerSplit = (to - current + 1) / numPartitions;
 		}
 		else {
 			// long overflow of the range.
 			// we compute based on half the distance, to prevent the overflow.
 			// in most cases it holds that: current < 0 and to > 0, except for: to == 0 and current == Long.MIN_VALUE
 			// the later needs a special case
 			final long halfDiff; // must be positive
 			
 			if (current == Long.MIN_VALUE) {
 				// this means to >= 0
 				halfDiff = (Long.MAX_VALUE/2+1) + to/2;
 			} else {
 				long posFrom = -current;
 				if (posFrom > to) {
 					halfDiff = to + ((posFrom - to) / 2);
 				} else {
 					halfDiff = posFrom + ((to - posFrom) / 2);
 				}
 			}
 			elementsPerSplit = halfDiff / numPartitions * 2;
 		}
 		
 		if (elementsPerSplit < Long.MAX_VALUE) {
 			// figure out how many get one in addition
-			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current;
+			long numWithExtra = -(elementsPerSplit * numPartitions) + to - current + 1;
 			
 			// based on rounding errors, we may have lost one)
 			if (numWithExtra > numPartitions) {
 				elementsPerSplit++;
 				numWithExtra -= numPartitions;
 				
 				if (numWithExtra > numPartitions) {
 					throw new RuntimeException(""Bug in splitting logic. To much rounding loss."");
 				}
 			}
 			
 			NumberSequenceIterator[] iters = new NumberSequenceIterator[numPartitions];
 			long curr = current;
 			int i = 0;
 			for (; i < numWithExtra; i++) {
 				long next = curr + elementsPerSplit + 1;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1);
 				curr = next;
 			}
 			for (; i < numPartitions; i++) {
 				long next = curr + elementsPerSplit;
-				iters[i] = new NumberSequenceIterator(curr, next);
+				iters[i] = new NumberSequenceIterator(curr, next-1, true);
 				curr = next;
 			}
 			
 			return iters;
 		}
 		else {
 			// this can only be the case when there are two partitions
 			if (numPartitions != 2) {
 				throw new RuntimeException(""Bug in splitting logic."");
 			}
 			
 			return new NumberSequenceIterator[] {
 				new NumberSequenceIterator(current, current + elementsPerSplit),
 				new NumberSequenceIterator(current + elementsPerSplit, to)
 			};
 		}
 	}",Buggy,"Fixed bug in splitting logic of number sequence iterator.
",Buggy,"The commit message states ""Fixed bug in splitting logic of number sequence iterator."" Let's analyze the code changes to determine if they reflect a bug fix:

1. **Correcting Range Calculations:**
   - The original formula for `elementsPerSplit` calculation was `(to - current) / numPartitions`. The updated formula is `(to - current + 1) / numPartitions`. The addition of `1` suggests capturing the inclusive end of the range (`to`). This change rectifies an off-by-one error, which occurs frequently in range-based calculations.

2. **Rounding Error Adjustments:**
   - The calculation of `numWithExtra` has similarly been adjusted from `-(elementsPerSplit * numPartitions) + to - current` to include the `+1` (`-(elementsPerSplit * numPartitions) + to - current + 1`). This aligns the rounding with the intended inclusive logic.

3. **Adjustments to Iterator Ranges:**
   - The logic for assigning `next` to the iterators was changed from `new NumberSequenceIterator(curr, next)` to `new NumberSequenceIterator(curr, next - 1)` for partitions, adjusting the endpoint to be inclusive of the `current` element, aligning comparisons and iterator behavior with `current` potentially being less than `next`.
   
4. **Inclusion Check:**
   - A new boolean `true` has been added to one of the `NumberSequenceIterator` constructors, suggesting a flag to control behavior, most likely associated with the inclusive/exclusive nature of the range end. This helps encapsulate the inclusive adjustment directly within the iterator logic.

All these modifications indicate the resolution of a particular off-by-one error generally tied to inclusive versus exclusive range handling, which aligns with the commit message mentioning a bug in the splitting logic of the number sequence iterator.

Thus, we conclude this change reflects a bug fix, leading to the conclusion: **Buggy**."
flink,36898.json,6dcf74f022f4a127260b0e8d52b772fb28fa2249,"@@ -1,5 +1,10 @@
 	public void setCosts(Costs nodeCosts) {
-		// do not account for any cost, regardless of what the estimator
-		// calculates for our shipping strategies
-		super.setCosts(new Costs());
+		// the plan enumeration logic works as for regular two-input-operators, which is important
+		// because of the branch handling logic. it does pick redistributing network channels
+		// between the sink and the sink joiner, because sinks joiner has a different DOP than the sink.
+		// we discard any cost and simply use the sum of the costs from the two children.
+		
+		Costs totalCosts = getInput1().getSource().getCumulativeCosts().clone();
+		totalCosts.addCosts(getInput2().getSource().getCumulativeCosts());
+		super.setCosts(totalCosts);
 	}",Buggy,"Fixed erroneous costs handling in sink joiner.
",Buggy,"The commit message indicates a fix for erroneous costs handling in a ""sink joiner."" The provided code change addresses this by altering how costs are gathered and set. Previously, costs were reset to zero, disregarding any dynamic calculations. The update now sums the costs from two input sources, which aligns with correcting an error in cost handling logic.

Examining the code changes:
- The original logic discarded all computed costs by setting costs directly to a new empty `Costs()` object.
- The revised logic calculates the sum of cumulative costs from two child nodes.
- This change corrects the handling of costs, ensuring they reflect the true computational costs from inputs rather than being zeroed out.

This modification aligns with the commit message claim of fixing an erroneous costs handling issue, showing a clear logical correction to improve how cost calculations are performed.

Thus, based on the evidence of logical error correction in handling costs, the answer is **Buggy**."
flink,9963.json,750325e74c1cd91006b251f18f3ed8acf90c2a91,"@@ -1,3 +1,8 @@
 		public void jobLeaderLostLeadership(final JobID jobId, final UUID oldJobLeaderId) {
-			ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+			runAsync(new Runnable() {
+				@Override
+				public void run() {
+					ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId);
+				}
+			});
 		}",Buggy,"[FLINK-5893] [RM] Fix the bug of race condition for removing previous JobManagerRegistration in ResourceManager

This closes #3399.
",Buggy,"The commit message indicates a bug fix related to a race condition in removing a previous JobManagerRegistration in the ResourceManager. Analyzing the diff:

- The change moves the call to `ResourceManager.this.jobLeaderLostLeadership(jobId, oldJobLeaderId)` inside a `runAsync` method.
- This suggests that the original code potentially experienced race conditions due to synchronous execution.
- By executing the operation asynchronously, it prevents potential race conditions between threads that might have been causing failures or inconsistencies.

The change aligns with the commit message's mention of fixing a race condition, indicating attention to concurrency issues in the ResourceManager. This suggests a resolution of a bug caused by improper synchronization or concurrency control. Concluding, the correct assessment is **Buggy**."
flink,14028.json,0f88c392b6ad6f91aed33e157bdc9df6f613f09d,"@@ -1,3 +1,3 @@
 	public TableSource<Row> projectFields(int[] fields) {
-		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, null);
+		return new ParquetTableSource(path, parquetSchema, parquetConfig, recursiveEnumeration, fields, predicate);
 	}",Buggy,"[FLINK-15361][parquet] ParquetTableSource should pass predicate in projectFields

fix the problem, when after projectFields, ParquetTableSource will loose predicates.

this closes #10660.
",Buggy,"The commit message indicates that there was a bug related to losing predicates after applying projectFields in the ParquetTableSource. The code change reflects this fix by modifying the method projectFields to include the existing predicate when creating a new instance of ParquetTableSource. Previously, the predicate was set to null, which would result in losing any filtering conditions applied to the table source. By passing the existing predicate, the code ensures that the predicates are preserved even when fields are projected.

The alignment between the commit message and code change is accurate. The update addresses a specific bug where predicates were unintentionally discarded during field projection.

Therefore, the changes are indicative of a bug fix. The assessment is **Buggy**."
flink,27787.json,37f67b7d394ccb3355ebc995af1c3fee04ce060f,"@@ -1,8 +1,8 @@
 	public Expression[] accumulateExpressions() {
 		Expression[] accExpressions = new Expression[1 + operands().length];
-		// sequence = if (lastValues equalTo orderKeys) sequence else sequence + 1
-		accExpressions[0] = ifThenElse(orderKeyEqualsExpression(), sequence, plus(sequence, literal(1L)));
+		// sequence = if (lastValues equalTo orderKeys and sequence != 0) sequence else sequence + 1
+		accExpressions[0] = ifThenElse(and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L)))), sequence, plus(sequence, literal(1L)));
 		Expression[] operands = operands();
 		System.arraycopy(operands, 0, accExpressions, 1, operands.length);
 		return accExpressions;
 	}",Buggy,"[FLINK-14053][table-planner-blink] Fix DenseRankAggFunction first row bug.

We should consider the possibility that first row's order by key is equal to the initial last value.

This closes #9966
",Buggy,"The commit message states that a bug in the `DenseRankAggFunction` concerning the handling of the first row was fixed. The code change adjusts the `accumulateExpressions` method by modifying the condition for updating the `sequence`. 

Previously, the `sequence` was updated if the `lastValues` were not equal to the `orderKeys`. The update introduces an additional condition: the sequence should not equal zero. This change handles the edge case where the first row's order keys are the same as the initial last values and ensures that the sequence only increments if it is non-zero, preventing incorrect sequence reset or increment.

This modification aligns with the commit message and addresses a logical error in rank calculation for certain cases, thereby fixing a bug.

The answer is **Buggy**."
flink,7316.json,c59f4836fecc4069b2cffeae8dd81e50ea5d5e73,"@@ -1,4 +1,4 @@
 	public String getDescription() {
-		return ""String value that specifies the termination mode. Supported values are: "" +
-			StringUtils.toQuotedListString(TerminationMode.values()) + '.';
+		return ""String value that specifies the termination mode. The only supported value is: \"""" +
+			TerminationMode.CANCEL.name().toLowerCase() + ""\""."";
 	}",Buggy,"[FLINK-13136][docs] Fix documentation error about stopping job with restful api

This closes #9013.
",NotBuggy,"The commit message mentions fixing a documentation error regarding stopping jobs using a RESTful API. The code change modifies a description string to accurately reflect that only one termination mode, ""cancel,"" is supported, instead of listing multiple values.

Given that this change updates a string for documentation purposes and aligns it with the actual functionality, it's not fixing a bug in code execution but correcting a textual error for clarity. This supports the commit message that talks specifically about fixing documentation, not a functional bug.

Thus, the changes in the code are documentation-related rather than an execution bug fix. Therefore, the conclusion is **NotBuggy**."
flink,13283.json,2cb7bb96001f9780a27a880245382958448151c4,"@@ -1,3 +1,5 @@
 	public void setItemCount(long itemCount) throws IOException {
-		writeVarLongCount(out, itemCount);
+		if (itemCount > 0) {
+			writeVarLongCount(out, itemCount);
+		}
 	}",Buggy,"Fix bug in avro serialization for empty collections.
Add additional test for avro.
",Buggy,"The diff shows a change in the `setItemCount` method. Previously, `writeVarLongCount(out, itemCount);` was called unconditionally, which may not have been appropriate for an `itemCount` of zero. The modification introduces a check to ensure that `writeVarLongCount(out, itemCount);` is only called when `itemCount` is greater than zero. 

This aligns with the commit message indicating a fix for Avro serialization issues with empty collections. By not performing the write operation when `itemCount` is zero, the code prevents unnecessary serialization that could lead to erroneous data handling, particularly for empty collections. This change corrects a logic issue, preventing inappropriate or unnecessary data writes.

Given that this update specifically addresses a mistaken behavior tied to the handling of empty collections during serialization, it is indeed a bug fix.

Therefore, the conclusion is **Buggy**."
flink,34466.json,a86bce5a176144e06d0120b804f3af986c325ebf,"@@ -1,18 +1,18 @@
 	private static int getMillis(String dateStr) {
 		int length = dateStr.length();
 		if (length == 19) {
 			// ""1999-12-31 12:34:56"", no milli second left
 			return 0;
 		} else if (length == 21) {
 			// ""1999-12-31 12:34:56.7"", return 7
 			return Integer.parseInt(dateStr.substring(20)) * 100;
 		} else if (length == 22) {
 			// ""1999-12-31 12:34:56.78"", return 78
 			return Integer.parseInt(dateStr.substring(20)) * 10;
 		} else if (length >= 23 && length <= 26) {
 			// ""1999-12-31 12:34:56.123"" ~ ""1999-12-31 12:34:56.123456""
-			return Integer.parseInt(dateStr.substring(20, 23)) * 10;
+			return Integer.parseInt(dateStr.substring(20, 23));
 		} else {
 			return 0;
 		}
 	}",Buggy,"[FLINK-12553][table-runtime-blink] Fix bug that SqlDateTimeUtils#parseToTimeMillis doesn't parse millisecond correctly

This closes #8483
",Buggy,"The change in this diff corrects how milliseconds are parsed from date strings with varying lengths. The original implementation multiplied the milliseconds by 10, which was incorrect. The updated code removes this multiplication, accurately extracting the milliseconds without extra scaling. This aligns perfectly with the commit message stating a fix for incorrect millisecond parsing. Therefore, the fix addresses a bug.

The answer is **Buggy**."
flink,22688.json,7b6b5a2e019866bf8fdd993775eab410e22f0f5d,"@@ -1,11 +1,14 @@
 	public static Object deserializeObject(byte[] bytes, ClassLoader cl) throws IOException, ClassNotFoundException {
 		ObjectInputStream oois = null;
+		final ClassLoader old = Thread.currentThread().getContextClassLoader();
 		try {
+			Thread.currentThread().setContextClassLoader(cl);
 			oois = new ClassLoaderObjectInputStream(new ByteArrayInputStream(bytes), cl);
 			return oois.readObject();
 		} finally {
+			Thread.currentThread().setContextClassLoader(old);
 			if (oois != null) {
 				oois.close();
 			}
 		}
 	}",Buggy,"Fixes a bug where Thread.currentThread().getContextClassLoader() does not return the user code class loader within object deserialization.
",Buggy,"The commit message describes a bug related to deserialization and class loaders. The code change saves the current thread's context class loader, sets it to use the provided class loader `cl` during deserialization, and restores the original class loader afterward. This change ensures that the correct class loader is used during deserialization, aligning with the bug description in the commit message. The modifications address potential issues where the wrong class loader might have been used, causing deserialization failures or incorrect loading of classes.

The use of `Thread.currentThread().setContextClassLoader(cl);` to temporarily switch to the correct class loader before deserialization, and the restoration of the original class loader after deserialization, effectively resolves the described bug.

Thus, the changes in the code align with the commit message and describe a bug fix. The answer is **Buggy**."
flink,22189.json,e28b62e0e2973b01ad5b08ce319aaf0e7ce4c087,"@@ -1,11 +1,11 @@
 	public MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) {
-		return new MethodVisitor(Opcodes.ASM4) {
+		return new MethodVisitor(Opcodes.ASM5) {
 
 			@Override
 			public void visitFieldInsn(int op, String owner, String name, String desc) {
 				if (op == Opcodes.GETFIELD && name.equals(this0Name)) {
 					isThis0Accessed = true;
 				}
 			}
 		};
 	}",Buggy,"[FLINK-3143] update Closure Cleaner's ASM references to ASM5

- This solves errors with reflectasm using Scala 2.11 and Java 8

This closes #1445.
",NotBuggy,"This code change updates the ASM version from ASM4 to ASM5. The commit message mentions the motivation behind this update: resolving errors using Scala 2.11 and Java 8. While this change might fix compatibility or integration issues, it does not clearly address a logic-based bug in the code itself. Nonetheless, ensuring compatibility with newer systems can be crucial in preventing runtime failures, which could be argued as a form of preventative bug fix. However, since the essence of the change is more about updating a dependency for compatibility rather than fixing a bug in the program logic, the conclusion leans towards a version update rather than a direct bug fix.

**NotBuggy**"
flink,36539.json,73b5b3dd81e2a146592d9623f44ceff3d8c035fa,"@@ -1,6 +1,6 @@
 	public void setDegreeOfParallelism(int degreeOfParallelism) {
 		if (degreeOfParallelism < 1) {
-			throw new IllegalArgumentException();
+			throw new IllegalArgumentException(""Degree of parallelism of "" + degreeOfParallelism + "" is invalid."");
 		}
 		this.degreeOfParallelism = degreeOfParallelism;
 	}",Buggy,"Fix error with invalid config values for degree of parallelism.
",NotBuggy,"The commit message indicates a fix for an error related to invalid configuration values for the degree of parallelism. The code change enhances the exception message thrown when an invalid value is provided. While this modification improves clarity by providing a more descriptive message, it does not change the logic or fix a bug related to how invalid values are handled. It provides better error information but does not involve a bug fix in the sense of correcting a logical error or issue within the function's operation.

Given that there's no change in error detection or handling, nor any correction of a flawed logic path, the change is strictly about improving the error message. Therefore, this change could be considered more as an enhancement or improvement, rather than a direct bug fix.

The conclusion is **NotBuggy**."
flink,1954.json,f28c28643df43633439b2c99c383e1ed01319ea3,"@@ -1,9 +1,9 @@
 	public <T extends Value> Class<? extends ConvergenceCriterion<T>> getConvergenceCriterion() {
 		@SuppressWarnings(""unchecked"")
-		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) 
+		Class<? extends ConvergenceCriterion<T>> clazz = (Class<? extends ConvergenceCriterion<T>>) (Class<?>) 
 							this.config.getClass(ITERATION_CONVERGENCE_CRITERION, null, ConvergenceCriterion.class);
 		if (clazz == null) {
 			throw new NullPointerException();
 		}
 		return clazz;
 	}",Buggy,"Fixed compiler error in maven for TaskConfig.java
",Buggy,"The commit message notes a compilation error fixed in TaskConfig.java. The change involves a type casting adjustment in the Java code. By adding `(Class<?>)`, the code resolves a likely ""incompatible types"" error during compilation, allowing a safer cast to `Class<? extends ConvergenceCriterion<T>>`. Such a modification directly addresses a compilation issue. Therefore, the commit is related to fixing a compiler error, suggesting a correction in type casting to prevent compile-time type errors.

Therefore, based on this reasoning, the changes align with the commit message concerning fixing errors, and I conclude with **Buggy**."
flink,2537.json,5e498dc9e763a5daa3867456f657e19cd08fbb66,"@@ -1,13 +1,11 @@
-		private void handleCompletedFuture(T value, Throwable throwable) {
+		private void handleCompletedFuture(int index, T value, Throwable throwable) {
 			if (throwable != null) {
 				completeExceptionally(throwable);
 			} else {
-				int index = nextIndex.getAndIncrement();
-
 				results[index] = value;
 
 				if (numCompleted.incrementAndGet() == numTotal) {
 					complete(Arrays.asList(results));
 				}
 			}
 		}",Buggy,"[FLINK-12021] Deploy execution in topological sorted order

Due to changes how the slot futures are completed and due to the fact that the
ResultConjunctFuture does not maintain the order in which the futures were specified,
it could happen that executions were not deployed in topological order. This commit
fixes this problem by changing the ResultConjunctFuture so that it maintains the order
of the specified futures in its result collection.

This closes #8060.
",Buggy,"The commit message discusses ensuring that executions are deployed in topological order, which was problematic due to unordered completion of futures. The code changes reflect this by adding an `index` parameter to `handleCompletedFuture`. This allows results to be stored in the order intentions specified, directly addressing the issue. The modifications align well with the message by fixing the ordering problem and maintain proper sequence in execution deployment. The changes effectively address a bug related to the ordering of operations.

Therefore, the result is **Buggy**."
eclipseJdt,3793.json,2f6e64cbbbc83dfd41e2988704a66b1a58193e8d,"@@ -1,3 +1,3 @@
 	public boolean isTypeAccess() {
-		return false;
+		return !this.haveReceiver;
 	}",Buggy,"Fixed Bug 424226 - [1.8] Cannot use static method from an interface in
static method reference ",Buggy,"The commit message indicates a bug fix related to using static methods from an interface in a static method reference. The code change adjusts the return value of the `isTypeAccess` method from a constant `false` to a negated boolean `!this.haveReceiver`, suggesting a logical error in checking type access. The presence of `this.haveReceiver` suggests that the code now conditionally returns `true` or `false` based on whether a receiver exists, aligning with the commit's reference to a bug related to method references.

Given this adjustment, it's likely that the original implementation incorrectly handled certain scenarios, thereby manifesting the bug described in the commit message. The change directly affects the logic in determining access type, which could be crucial for resolving the mentioned bug.

Therefore, since the change is correcting a logical flaw as described in the commit message, the conclusion is **Buggy**."
eclipseJdt,651.json,969e112c287880aaa166b5c77a56bd62cba0db4b,"@@ -1,7 +1,11 @@
 private static IJavaProject getJavaProject(IPath path, IJavaModel model) {
-	IJavaProject project = model.getJavaProject(path.lastSegment());
+	String lastSeg = path.lastSegment();
+	if (lastSeg == null) {
+		lastSeg = path.toOSString();
+	}
+	IJavaProject project = model.getJavaProject(lastSeg);
 	if (project.exists()) {
 		return project;
 	}
 	return null;
 }",Buggy,"Fix for Bug 464339 When finding references, Java Search fails with
NullPointerException",Buggy,"The commit message specifies a fix for a NullPointerException when finding references within Java Search. The code diff adds a null check for `path.lastSegment()`, assigning the whole path string if it's null. This prevents the NullPointerException when `lastSegment()` returns null. Thus, the code changes directly address the issue mentioned in the commit message by ensuring that a null value does not propagate to the `getJavaProject` method.

The commit's changes align with the description of fixing a NullPointerException bug. Consequently, the analysis concludes with **Buggy**."
eclipseJdt,20760.json,5666cb170a5113b959f09f9a106548eceb94f3e9,"@@ -1,17 +1,23 @@
 	private boolean isPrimaryType(String name, IType type, boolean partialMatch) {
 		/*
 		 * Please have a look at: NameLookup#NameLookup
 		 * The HashTable this.typesInWorkingCopies contains values which are HashTables themselves.
 		 * The values of these HashTables are either of IType or IType[].
 		 * These values are types belonging to a compilation unit. Please check:
 		 * CompilationUnit#getTypes().
 		 * Therefore the parents of these types would be compilation units.
 		 */
 		ICompilationUnit cu = (ICompilationUnit) type.getParent();
 		String cuName = cu.getElementName().substring(0, cu.getElementName().lastIndexOf('.'));
+		/*
+		 * Secondary types along with primary types have their parent as the compilation unit.
+		 * The names of the primary type would match with their compilation unit.
+		 */
+		if (!cuName.equals(type.getElementName()))
+			return false;
 		if (partialMatch) {
 			return cuName.regionMatches(0, name, 0, name.length());
 		} else {
 			return cuName.equals(name);
 		}
 	}",Buggy,"Fixed Bug 431501 - NameLookup#findType API finds secondary types with
secondaryType and partial match set

Change-Id: Iad7904647729b2f282d039856b6ecee5325ebedb
Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>
",Buggy,"The purpose of this change is to ensure that only primary types that exactly match the compilation unit's name are considered valid. This is indicated by the added code that returns `false` if the compilation unit name doesn’t match the type's element name, which helps to differentiate primary types from secondary ones more effectively. The original code could mistakenly identify a secondary type as a primary type if the match criteria weren't strictly defined to consider the full name, leading to incorrect behavior.

The commit message states fixing Bug 431501 by correctly identifying primary versus secondary types, aligning with this logic correction. This improvement to type identification directly addresses potential issues with incorrect type matches.

Thus, the changes described in the diff are consistent with fixing the described bug in the commit message. The code effectively improves the logic for identifying primary and secondary types, preventing unintended partial matches with secondary types.

Therefore, this change qualifies as a bug fix and the conclusion is **Buggy**."
eclipseJdt,1127.json,fe3ac1852503ee5f3d26919dd323775bfa197648,"@@ -1,14 +1,17 @@
 public PossibleMatch[] getPossibleMatches(IPackageFragmentRoot[] roots) {
 	PossibleMatch[] result = new PossibleMatch[this.elementCount];
 	int index = 0;
+	HashSet<IPath> processedHash = new HashSet<>();
 	for (int i = 0, length = roots.length; i < length; i++) {
-		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(roots[i].getPath());
-		if (possibleMatches != null) {
+		IPath path = roots[i].getPath();
+		ObjectVector possibleMatches = (ObjectVector) this.rootsToPossibleMatches.get(path);
+		if (possibleMatches != null && !processedHash.contains(path)) {
 			possibleMatches.copyInto(result, index);
 			index += possibleMatches.size();
+			processedHash.add(path);
 		}
 	}
 	if (index < this.elementCount)
 		System.arraycopy(result, 0, result = new PossibleMatch[index], 0, index);
 	return result;
 }",Buggy,"Fix for bug 478360 - [1.9][search] AIOOBE  while searching for java.base
methods",Buggy,"This update addresses an ArrayIndexOutOfBoundsException (AIOOBE) while searching for Java methods by ensuring each root is processed once. The addition of a HashSet to track processed paths prevents duplicate processing, which might have caused the exception. The commit clearly targets a bug fix, as it corrects the control flow to avoid errors during the search process. Therefore, this change aligns with the commit message describing a bug fix related to this issue. The conclusion is **Buggy**."
eclipseJdt,3485.json,5df55b1d083f2bad66acc93a2ff019e72423c2c9,"@@ -1,12 +1,20 @@
 	public static IndexLocation createIndexLocation(URL url) {
 		URL localUrl;
 		try {
 			localUrl = FileLocator.resolve(url);
 		} catch (IOException e) {
 			return null;
 		}
 		if (localUrl.getProtocol().equals(""file"")) { //$NON-NLS-1$
-			return new FileIndexLocation(url, new File(localUrl.getPath()));
+			File localFile = null;
+			try {
+				URI localFileURI = new URI(localUrl.toExternalForm());
+				localFile = new File(localFileURI);
+			}
+			catch(Exception ex) {
+				localFile = new File(localUrl.getPath());
+			}
+			return new FileIndexLocation(url, localFile);
 		}
 		return new JarIndexLocation(url, localUrl);
 	}",Buggy,"Fix for bug 397818 - An IndexLocation is not created properly if the
pre-build indexes are located in a directory with a space
",Buggy,"The commit message notes a bug fix related to index location issues caused by directory spaces. The diff adds robust handling of URLs by converting them to URIs, to accurately capture possible spaces, using URI-based File creation. It falls back to traditional methods if URI fails. This aligns with the bug description and shows a bug-related fix. The answer is **Buggy**."
eclipseJdt,3849.json,e14a67b89e92e257eae6e75fc0c7b3046b4c9f63,"@@ -1,5 +1,6 @@
 public Object reusableJSRTarget() {
-	if (this.constant != Constant.NotAConstant)
+	if (this.constant != Constant.NotAConstant && (this.implicitConversion & TypeIds.BOXING) == 0) {
 		return this.constant;
+	}
 	return null;
 }",Buggy,Fix for bug 394718 - VerifyError: Inconsistent stackmap frames,Buggy,"The commit message states that the changes fix a bug related to a VerifyError due to inconsistent stackmap frames, which occurs when the JVM perceives discrepancies between expected and actual stack states at method call/return points. The Java modification checks an additional condition: the absence of a ""boxing"" conversion, before returning the constant.

Let's analyze this step-by-step:

1. Original Code: Returns `this.constant` if `this.constant` is not `Constant.NotAConstant`.
2. Modified Code: Adds a condition to the existing check, ensuring that the `implicitConversion` does not involve boxing before returning the constant.

By adding this condition, the code now ensures additional constraints are met, possibly accounting for a scenario where stackmap frame inconsistency might occur due to implicit boxing. In scenarios involving boxed primitive types, the absence of this condition might indeed lead to inconsistent stackmap frames, thus resulting in a `VerifyError`.

This additional check appears to be directly addressing a state where the implicit conversion of a value would lead to such a verification error during runtime.

Given the context provided by the commit message and the nature of the code change, this update is addressing a logical error (preventing a `VerifyError`) related explicitly to the handling of boxed types.

Conclusion: This change fixes a bug as described in the commit message, making it **Buggy**."
eclipseJdt,2849.json,48e3ddc97a372ba17e3a8fb05bd8c143189dd168,"@@ -1,21 +1,21 @@
 	public static final char[] concat(char[] first, char[] second, char[] third, char[] fourth) {
 		if (first == null)
 			return concat(second, third, fourth);
 		if (second == null)
 			return concat(first, third, fourth);
 		if (third == null)
 			return concat(first, second, fourth);
 		if (fourth == null)
 			return concat(first, second, third);
 
 		int length1 = first.length;
 		int length2 = second.length;
 		int length3 = third.length;
 		int length4 = fourth.length;
 		char[] result = new char[length1 + length2 + length3 + length4];
 		System.arraycopy(first, 0, result, 0, length1);
 		System.arraycopy(second, 0, result, length1, length2);
 		System.arraycopy(third, 0, result, length1 + length2, length3);
-		System.arraycopy(third, 0, result, length1 + length2 + length3, length4);
+		System.arraycopy(fourth, 0, result, length1 + length2 + length3, length4);
 		return result;
 	}",Buggy,"Bug 481796 - Fix ArrayIndexOutOfBoundsException

Change-Id: Ib1cded7b613262929f0c906a04bd69a523311141
Signed-off-by: Stefan Xenos <sxenos@gmail.com>
",Buggy,"The diff corrects an error in a line where the third array was copied twice instead of copying the fourth one. This would result in an `ArrayIndexOutOfBoundsException` if the length of the designated space for the fourth array did not match the length of the third array, or cause logical errors otherwise. The commit message indicates fixing an `ArrayIndexOutOfBoundsException`, and this change directly addresses such a potential issue by ensuring the correct array is copied into the intended position in the result array. Therefore, the modifications are in line with bug fixing as described in the commit message.

**Buggy**"
eclipseJdt,5117.json,8f577b9f934a073bf0b3684c3935f2cd08a1660a,"@@ -1,8 +1,8 @@
 	public StringBuffer printExpression(int indent, StringBuffer output) {
-		output.append(""NAryStringLiteral{""); //$NON-NLS-1$
+		output.append(""StringLiteralConcatenation{""); //$NON-NLS-1$
 		for (int i = 0, max = this.counter; i < max; i++) {
 			this.literals[i].printExpression(indent, output);
 			output.append(""+\n"");//$NON-NLS-1$
 		}
 		return output.append('}');
 	}",Buggy,"HEAD - Fix bug in formatting string literal concatenation
",Buggy,"The diff involves modifying a string literal used in the `printExpression` method from ""NAryStringLiteral"" to ""StringLiteralConcatenation"". This change aligns the output more accurately with the actual purpose or nature of the concatenated string expression, which might address a bug related to incorrect or misleading log or debug messages. The mention of ""Fix bug in formatting string literal concatenation"" in the commit message aligns with this change, suggesting that the previous output was incorrect or misleading.

However, the change is purely textual and does not involve fixing any functional defects or computational errors. Therefore, if the intent was solely to correct misleading output in logs, it can be considered a bug fix related to incorrect informational messages.

Given the current change context and commit message, this update is likely addressing a bug related to the clarity of debugging or logging information rather than fixing an underlying logic flaw in code execution.

Therefore, this change is in line with the commit message, which describes a fix in the formatting of string literal concatenation. Thus, we conclude with **Buggy**."
eclipseJdt,20784.json,653036e82ad47971c36ae720862ebb4740be149f,"@@ -1,12 +1,13 @@
 char[][] fullExclusionPatternChars() {
 	try {
+		if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;
 		ClasspathEntry entry = (ClasspathEntry)getRawClasspathEntry();
 		if (entry == null) {
 			return null;
 		} else {
 			return entry.fullExclusionPatternChars();
 		}
 	} catch (JavaModelException e) { 
 		return null;
 	}
 }",Buggy,"Fix for bug 28489
",Buggy,"The change introduces a new condition within the method `fullExclusionPatternChars`, specifically checking `if (this.getKind() != IPackageFragmentRoot.K_SOURCE) return null;`. This indicates the method now returns `null` early if the package fragment root's kind is not `K_SOURCE`. 

The inclusion of this condition suggests that the previous implementation lacked this check, which could indeed lead to unintended behavior (e.g., attempting operations on inappropriate fragment kinds and potentially causing errors downstream). The fact that the modification is in line with a bug fix for ""bug 28489"" further supports the notion that this condition addresses that particular issue.

Overall, this additional condition rectifies the logic by ensuring the method only proceeds when applicable, which aligns with the concept of a bug fix. Therefore, the changes align with the commit message stating a bug fix and clearly correct a logical flaw in the system.

The final evaluation is: **Buggy**."
eclipseJdt,1595.json,0a908cd3dc3279066d94660987f6e5450b72486e,"@@ -1,54 +1,55 @@
 private MethodBinding getMethodBinding(MethodPattern methodPattern, TypeBinding declaringTypeBinding) {
 	MethodBinding result;
 	char[][] parameterTypes = methodPattern.parameterSimpleNames;
 	if (parameterTypes == null) return null;
 	int paramTypeslength = parameterTypes.length;
 	ReferenceBinding referenceBinding = (ReferenceBinding) declaringTypeBinding;
 	MethodBinding[] methods = referenceBinding.getMethods(methodPattern.selector);
 	int methodsLength = methods.length;
 	TypeVariableBinding[] refTypeVariables = referenceBinding.typeVariables();
 	int typeVarLength = refTypeVariables==null ? 0 : refTypeVariables.length;
 	List <MethodBinding> possibleMethods = new ArrayList<MethodBinding>(methodsLength);
 	for (int i=0; i<methodsLength; i++) {
 		TypeBinding[] methodParameters = methods[i].parameters;
 		int paramLength = methodParameters==null ? 0 : methodParameters.length;
 		TypeVariableBinding[] methodTypeVariables = methods[i].typeVariables;
 		int methTypeVarLength = methodTypeVariables==null ? 0 : methodTypeVariables.length;
 		boolean found = false;
 		if (methodParameters != null && paramLength == paramTypeslength) {
 			for (int p=0; p<paramLength; p++) {
-				if (CharOperation.equals(methodParameters[p].sourceName(), parameterTypes[p])) {
+				TypeBinding parameter = methodParameters[p];
+				if (matchParams(methodPattern, p, parameter)) {
 					// param erasure match
 					found = true;
 				} else {
 					// type variable
 					found = false;
 					if (refTypeVariables != null) {
 						for (int v=0; v<typeVarLength; v++) {
 							if (!CharOperation.equals(refTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found && methodTypeVariables != null) {
 						for (int v=0; v<methTypeVarLength; v++) {
 							if (!CharOperation.equals(methodTypeVariables[v].sourceName, parameterTypes[p])) {
 								found = false;
 								break;
 							}
 							found = true;
 						}
 					}
 					if (!found) break;
 				}
 			}
 		}
 		if (found) {
 			possibleMethods.add(methods[i]);
 		}
 	}
 	result =  getMostApplicableMethod(possibleMethods, methodPattern);
 	return result;
 }",Buggy,Fix for Bug 521240: Search ignores the qualifier of the parameter type,Buggy,"The commit message states that the purpose of the change is to fix a bug where the search ignores the qualifier of the parameter type. The code modification introduces a change in how parameters are compared within method bindings.

1. **Change Analysis**:
   - The original code directly compared the source names of method parameters to the expected parameter names using `CharOperation.equals`.
   - The revised code replaces this comparison with a call to a new method `matchParams(methodPattern, p, parameter)`. This indicates a move to a potentially more complex matching logic that might consider qualifiers or additional attributes of the `parameter`.

2. **Understanding the Bug Context**:
   - The bug specified in the message mentions that the search function was ignoring some qualifiers of parameter types. This suggests that the previous parameter comparison might have been overly simplistic—possibly ignoring necessary details like fully-qualified names.
   - Introducing `matchParams` implies that it performs a more comprehensive check, which could rectify the oversight where only simple names were compared, not considering the full context or qualifier.

3. **Conclusion**:
   - The adjustment made in the code directly addresses the potential cause of Bug 521240 as described in the commit message. By using a specialized method for parameter comparison, it likely ensures that qualifiers aren't ignored, thereby fixing the specific issue of parameter type matching.

Therefore, the changes in the code align well with the commit message, indicating a fix for the specified bug. The judgment here is **Buggy**."
eclipseJdt,1134.json,b7bd88d9bc859369d4d17c37d109bd3331837ba7,"@@ -1,7 +1,8 @@
 public int match(MessageSend msgSend, MatchingNodeSet nodeSet)  {
 	if ((msgSend.bits & ASTNode.InsideJavadoc) == 0) return IMPOSSIBLE_MATCH;
+	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (this.pattern.declaringSimpleName == null || CharOperation.equals(msgSend.selector, this.pattern.declaringSimpleName)) {
 		return nodeSet.addMatch(msgSend, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 	}
 	return IMPOSSIBLE_MATCH;
 }",Buggy,"Fixed bug 381567: [search] Unexpected results from SearchEngine#search
Conflicts:
",Buggy,"The commit message mentions a bug fix related to search functionality. The code change introduces an additional check: `if (!this.pattern.findReferences)`. This modification suggests that there's a condition where the search should not continue, potentially addressing a bug where the search engine returned unexpected results when it was not supposed to search for references. This aligns well with the described issue in the commit message, indicating an error-handling update that prevents incorrect matches. Therefore, the change is consistent with fixing a bug related to search behavior.

Thus, the review leads to the conclusion: **Buggy**."
eclipseJdt,5656.json,905dec82e916c38704735fea38fad8cba693cc45,"@@ -1,3 +1,3 @@
 public TypeBinding clone(TypeBinding outerType) {
-	throw new IllegalStateException(); // shouldn't get here.
+	return this; // shouldn't get here.
 }",Buggy,"Fixed Bug 427105 - [1.8][builder] Differences between incremental and
full builds in method contract verification in the presence of type
annotations",Buggy,"The original code throws an `IllegalStateException`, indicating that execution should not reach that point in certain conditions, typically implying an exceptional state. The modification now returns `this` instead of throwing an exception, which indicates a change in how this unexpected state is handled.

This change could be related to a bug fix because:

1. **Behavior Change**: The change from throwing an exception to returning `this` could mean that the code was erroneously entering this state during normal operation, and this exception thrown caused incorrect behavior or crashes during incremental and full builds.

2. **Context from the Commit Message**: The commit message mentions fixing differences between incremental and full builds, specifically related to method contract verification in the presence of type annotations. This implies that the issue might have been discrepancies in behavior between the two types of builds due to the handling of type annotations, which could potentially trigger this block of code. By removing the exception, the error is handled more gracefully, potentially resolving inconsistencies between incremental and full builds.

Given these considerations, the change aligns with the commit message in indicating a bug fix related to build behavior differences. The fix appears to adjust handling to a more stable state without causing an exception under unexpected conditions.

Therefore, the conclusion is **Buggy**."
eclipseJdt,1327.json,84f1aab618cdc74f8c6aecebdc0fb845b80d6368,"@@ -1,4 +1,6 @@
 protected void consumeMemberValuePair() {
 	super.consumeMemberValuePair();
-	this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	if ((this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0) {
+		this.patternLocator.match((MemberValuePair) this.astStack[this.astPtr], this.nodeSet);
+	}
 }",Buggy,"Fix for Bug 435480 [1.8][search] search in method reference expressions
finds annotation element name",Buggy,"The code modification introduces a conditional statement around the pattern matching operation. The condition `(this.patternFineGrain & ~IJavaSearchConstants.METHOD_REFERENCE_EXPRESSION) != 0` specifically checks if certain flags are set before proceeding with matching the `MemberValuePair`. This suggests that earlier behavior, which always attempted to match without checking flags, could incorrectly include method reference expressions in search results. This aligns with the commit message indicating a fix for an issue related to searches in method reference expressions.

The addition of this condition effectively prevents unnecessary or incorrect matching operations, thereby addressing the bug as described in the commit message.

Given the focus on logical correction and filtering unnecessary actions, the change suggests a bug fix focused on improving search functionality related to method reference expressions. Therefore, the assessment is **Buggy**."
eclipseJdt,165.json,fe024fd7797ce73b0a83969f685073606863b79e,"@@ -1,23 +1,23 @@
 	public PrimitiveTypeImpl getPrimitiveType(TypeKind kind)
 	{
 		switch (kind) {
 		case BOOLEAN:
 			return PrimitiveTypeImpl.BOOLEAN;
 		case BYTE:
 			return PrimitiveTypeImpl.BYTE;
 		case CHAR:
 			return PrimitiveTypeImpl.CHAR;
 		case DOUBLE:
 			return PrimitiveTypeImpl.DOUBLE;
 		case FLOAT:
 			return PrimitiveTypeImpl.FLOAT;
 		case INT:
 			return PrimitiveTypeImpl.INT;
 		case LONG:
 			return PrimitiveTypeImpl.LONG;
 		case SHORT:
 			return PrimitiveTypeImpl.SHORT;
 		default:
-			throw new IllegalStateException();
+			throw new IllegalArgumentException();
 		}
 	}",Buggy,"Fixes Bug 427943 - The method
org.eclipse.jdt.internal.compiler.apt.model.Factory.getPrimitiveType
does not throw IllegalArgumentException

Change-Id: I652c422b80ef6192955ef508387f9fd0fac31a7e
Signed-off-by: Harry Terkelsen <het@google.com>",Buggy,"The commit message specifies fixing a bug related to the `getPrimitiveType` method not throwing an `IllegalArgumentException`. The code change makes the method throw an `IllegalArgumentException` instead of an `IllegalStateException` when the `TypeKind` does not match any expected case. This aligns with the standard use of `IllegalArgumentException` for invalid input parameters, indicating a bug fix related to exception handling. The code modification directly corresponds to the described bug fix in the commit message. Thus, the assessment is **Buggy**."
eclipseJdt,1253.json,025c833d41053b8599d25c53d05e92e37975873f,"@@ -1,18 +1,29 @@
 protected int resolveLevelForType(char[] qualifiedPattern, TypeBinding type) {
 	if (qualifiedPattern == null) return ACCURATE_MATCH;
 	if (type == null || !type.isValidBinding()) return INACCURATE_MATCH;
 
 	// Type variable cannot be specified through pattern => this kind of binding cannot match it (see bug 79803)
 	if (type.isTypeVariable()) return IMPOSSIBLE_MATCH;
 
+	if (type instanceof IntersectionTypeBinding18) {
+		int result = IMPOSSIBLE_MATCH, prev = IMPOSSIBLE_MATCH;
+		IntersectionTypeBinding18 i18 = (IntersectionTypeBinding18) type;
+		for (ReferenceBinding ref : i18.intersectingTypes) {
+			result = resolveLevelForType(qualifiedPattern, ref);
+			if (result == ACCURATE_MATCH) return result; 
+			if (result == IMPOSSIBLE_MATCH) continue;
+			if (prev == IMPOSSIBLE_MATCH) prev = result;
+		}
+		return prev;
+	}
 	// NOTE: if case insensitive search then qualifiedPattern is assumed to be lowercase
 
 	char[] qualifiedPackageName = type.qualifiedPackageName();
 	char[] qualifiedSourceName = qualifiedSourceName(type);
 	char[] fullyQualifiedTypeName = qualifiedPackageName.length == 0
 		? qualifiedSourceName
 		: CharOperation.concat(qualifiedPackageName, qualifiedSourceName, '.');
 	return CharOperation.match(qualifiedPattern, fullyQualifiedTypeName, this.isCaseSensitive)
 		? ACCURATE_MATCH
 		: IMPOSSIBLE_MATCH;
 }",Buggy,"Fix for Bug 485805 [1.8][search] Search engine throws
UnsupportedOperationException when searching for subclass implementors

Change-Id: I28b09ffc6e446e2967c4dbf077816ef2df83d2ab",Buggy,"The commit message indicates a fix for a bug where the search engine throws an `UnsupportedOperationException` when searching for subclass implementors. The code changes in the diff involve specific handling for `IntersectionTypeBinding18` types, which was not considered previously.

The problematic scenario was likely due to the fact that the `resolveLevelForType` method did not account for `IntersectionTypeBinding18` bindings. This would lead to scenarios where the search engine encountered unhandled types that resulted in exceptions, such as `UnsupportedOperationException`, when searching for subclass implementors.

The new code introduces a conditional block that checks if the `type` is an instance of `IntersectionTypeBinding18`. It then iterates over each intersecting type, applying the same search logic (`resolveLevelForType`) recursively. If any of these types yield an `ACCURATE_MATCH`, the function returns immediately with that result. Otherwise, if all checked types result in `IMPOSSIBLE_MATCH`, it returns accordingly unless a previous result was better.

This change logically aligns with the commit message, as it adds necessary handling for a special type of binding in the search, thus preventing the UnsupportedOperationException by ensuring all binding types are correctly processed. Consequently, these changes do reflect a bug fix as described in the commit message.

The answer is **Buggy**."
eclipseJdt,2496.json,7d2b09ebfd4cb99d1f345eedcae879729e8aff7e,"@@ -1,51 +1,53 @@
 	public MemoryAccessLog getReportFor(long address, int size) {
 		List<Tag> tags = new ArrayList<>();
 		tags.addAll(this.operationStack);
-		int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
-		int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
-		long currentWrite = this.timer;
 
 		List<MemoryOperation> operations = new ArrayList<>();
-		do {
-			long nextAddress = this.buffer0[currentPosition];
-			int nextArgument = this.buffer1[currentPosition];
-			byte nextOp = this.operation[currentPosition];
-
-			switch (nextOp) {
-				case POP_OPERATION: {
-					tags.add(getTagForId(nextArgument));
-					break;
-				}
-				case PUSH_OPERATION: {
-					tags.remove(tags.size() - 1);
-					break;
-				}
-				default: {
-					boolean isMatch = false;
-					if (address < nextAddress) {
-						long diff = nextAddress - address;
-						if (diff < size) {
-							isMatch = true;
-						}
-					} else {
-						long diff = address - nextAddress;
-						if (diff < nextArgument) {
-							isMatch = true;
-						}
+		if (this.buffer0 != null) {
+			int pointerToStart = (this.insertionPosition + this.buffer0.length - this.currentEntries) % this.buffer0.length;
+			int currentPosition = (this.insertionPosition + this.buffer0.length - 1) % this.buffer0.length;
+			long currentWrite = this.timer;
+			do {
+				long nextAddress = this.buffer0[currentPosition];
+				int nextArgument = this.buffer1[currentPosition];
+				byte nextOp = this.operation[currentPosition];
+	
+				switch (nextOp) {
+					case POP_OPERATION: {
+						tags.add(getTagForId(nextArgument));
+						break;
 					}
-
-					if (isMatch) {
-						List<Tag> stack = new ArrayList<>();
-						stack.addAll(tags);
-						MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
-								nextArgument, stack);
-						operations.add(nextOperation);
+					case PUSH_OPERATION: {
+						tags.remove(tags.size() - 1);
+						break;
 					}
-
-					currentWrite--;
+					default: {
+						boolean isMatch = false;
+						if (address < nextAddress) {
+							long diff = nextAddress - address;
+							if (diff < size) {
+								isMatch = true;
+							}
+						} else {
+							long diff = address - nextAddress;
+							if (diff < nextArgument) {
+								isMatch = true;
+							}
+						}
+	
+						if (isMatch) {
+							List<Tag> stack = new ArrayList<>();
+							stack.addAll(tags);
+							MemoryOperation nextOperation = new MemoryOperation(nextOp, currentWrite, nextAddress,
+									nextArgument, stack);
+							operations.add(nextOperation);
+						}
+	
+						currentWrite--;
+					}
 				}
-			}
-			currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
-		} while (currentPosition != pointerToStart);
+				currentPosition = (currentPosition + this.buffer0.length - 1) % this.buffer0.length;
+			} while (currentPosition != pointerToStart);
+		}
 		return new MemoryAccessLog(operations);
 	}",Buggy,"Bug 514089 - Build a tool to help debug index corruption

Fix NPE when corruption is detected and the buffer size is 0.

Change-Id: I4163e9f294c96582b201bdf09593602ab1807d6e
",Buggy,"The commit message mentions fixing an NPE (Null Pointer Exception) when corruption is detected and the buffer size is 0. In the code diff, a null check is added for `this.buffer0`, which was previously absent. Before this change, if `this.buffer0` was null, dereferencing it would lead to an NPE, especially when trying to access its length or elements. Adding this check prevents the method from trying to use a null buffer, which aligns with fixing the bug described in the commit message.

Thus, the changes indeed appear to fix a bug related to a Null Pointer Exception. The message and the code modifications are consistent with the claim of a bug fix.

The answer is **Buggy**."
eclipseJdt,15048.json,30961650a033c638f3a768401431bcd77f1a4058,"@@ -1,26 +1,29 @@
 	private void removeUnresolvedBindings(org.eclipse.jdt.internal.compiler.ast.TypeDeclaration type) {
 		final MemberTypeDeclaration[] memberTypes = type.memberTypes;
 		if (memberTypes != null) {
 			for (int i = 0, max = memberTypes.length; i < max; i++){
 				removeUnresolvedBindings(memberTypes[i]);
 			}
 		}
 		if (type.binding != null && (type.binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 			type.binding = null;
-			final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
-			if (fields != null) {
-				for (int i = 0, max = fields.length; i < max; i++){
+		}
+		
+		final org.eclipse.jdt.internal.compiler.ast.FieldDeclaration[] fields = type.fields;
+		if (fields != null) {
+			for (int i = 0, max = fields.length; i < max; i++){
+				if (fields[i].binding != null && (fields[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					fields[i].binding = null;
 				}
 			}
 		}
-
+	
 		final AbstractMethodDeclaration[] methods = type.methods;
 		if (methods != null) {
 			for (int i = 0, max = methods.length; i < max; i++){
 				if (methods[i].binding !=  null && (methods[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0) {
 					methods[i].binding = null;
 				}
 			}
 		}
 	}",Buggy,"Fix for bug 40804
",Buggy,"The commit message indicates a fix for bug 40804, and the diff shows changes in handling unresolved bindings within a type declaration in a Java compiler context.

Here's a step-by-step analysis of the changes:

1. **Moved Field Handling Out of Binding Nullification Check**: Originally, the nullification check for `type.binding` included inside it the logic for handling the fields (if `fields` were not null). This has been changed to handle fields directly outside of the check on `type.binding`.

2. **Added Field Binding Nullification Check**: The updated code now includes a check to specifically nullify field bindings with unresolved modifiers (`if (fields[i].binding != null && (fields[i].binding.modifiers & CompilerModifiers.AccUnresolved) != 0)`). Beforehand, this check was partially embedded within an outer binding nullification check.

3. **Continued Method Binding Handling**: The check for unresolved method bindings remains unchanged. Methods are evaluated similarly, indicating a consistency in handling bindings.

**Reasons for Alignment with Commit Message and Bug Fix Nature:**

- **Bug Fix Indication**: The fact that only unresolved field bindings are nullified now matches the pattern applied to both methods and fields, suggesting this approach was intended to avoid failing to clean up unresolved (and potentially problematic) bindings.

- **Improved Logic Handling**: Addressing unresolved binding states comprehensively across fields, types, and members aligns with a bug fix scope, as it likely resolves prior errors related to unresolved bindings remaining intact unintentionally.

- **Directly Mentioned Issue**: While the specific bug number (40804) or its detailed symptoms aren't shown here, the method's changes focus heavily on refining handling and adjustment of bindings — a core bug-related correction when dealing with compiler internals.

The changes align well with the commit message since they aim to fix a bug related to handling unresolved compiler bindings, maintaining a consistent strategy across different member types. Consequently, the answer is **Buggy**."
eclipseJdt,18985.json,4b7408bf1768e4fe08faa711e23ea5c6fe36d225,"@@ -1,6 +1,6 @@
 public String toString() {
-	return ""State for "" + this.javaProjectName
-		+ "" (#"" + this.buildNumber
+	return ""State for "" + this.javaProjectName //$NON-NLS-1$
+		+ "" (#"" + this.buildNumber //$NON-NLS-1$
 			+ "" @ "" + new Date(this.lastStructuralBuildTime) //$NON-NLS-1$
 				+ "")""; //$NON-NLS-1$
 }",Buggy,"HEAD - Fix Clean-up pass 1 problems
",NotBuggy,"The change in the code is related to adding `//$NON-NLS-1$` comments to string literals in the `toString()` method. These comments are typically used in Java projects to signal that a particular string literal should not be externalized (i.e., it does not need to be translated for internationalization/localization purposes).

The commit message ""Fix Clean-up pass 1 problems"" does not explicitly mention a bug fix; rather, it suggests a code clean-up or refactoring. The code modification is a non-functional change intended for clean code practices or for tooling support during internationalization efforts. This change does not address bug fixes such as error-handling updates, logical corrections, or exception handling, and does not fix any defects within the functionality of the application.

Therefore, based on the commit message and the nature of the change, I conclude that this is **NotBuggy**."
eclipseJdt,4719.json,6fe04df602475d9f13e955fcfd38124da359e84a,"@@ -1,28 +1,34 @@
 public void computeConversion(Scope scope, TypeBinding runtimeTimeType, TypeBinding compileTimeType) {
 	if (runtimeTimeType == null || compileTimeType == null)
 		return;
-	if ((this.bits & Binding.FIELD) != 0 && this.binding != null && this.binding.isValidBinding()) {
-		// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
-		FieldBinding field = (FieldBinding) this.binding;
-		FieldBinding originalBinding = field.original();
-		TypeBinding originalType = originalBinding.type;
-		// extra cast needed if field type is type variable
-		if (originalType.leafComponentType().isTypeVariable()) {
-	    	TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
-	    		? compileTimeType  // unboxing: checkcast before conversion
-	    		: runtimeTimeType;
-	        this.genericCast = originalType.genericCast(scope.boxing(targetType));
-	        if (this.genericCast instanceof ReferenceBinding) {
+	if (this.binding != null && this.binding.isValidBinding()) {
+		TypeBinding originalType = null;
+		if ((this.bits & Binding.FIELD) != 0) {
+			// set the generic cast after the fact, once the type expectation is fully known (no need for strict cast)
+			FieldBinding field = (FieldBinding) this.binding;
+			FieldBinding originalBinding = field.original();
+			originalType = originalBinding.type;
+		} else if ((this.bits & Binding.LOCAL) != 0) {
+			LocalVariableBinding local = (LocalVariableBinding) this.binding;
+			originalType = local.type;
+		}
+		// extra cast needed if field/local type is type variable
+		if (originalType != null && originalType.leafComponentType().isTypeVariable()) {
+			TypeBinding targetType = (!compileTimeType.isBaseType() && runtimeTimeType.isBaseType())
+					? compileTimeType  // unboxing: checkcast before conversion
+							: runtimeTimeType;
+			this.genericCast = originalType.genericCast(scope.boxing(targetType));
+			if (this.genericCast instanceof ReferenceBinding) {
 				ReferenceBinding referenceCast = (ReferenceBinding) this.genericCast;
 				if (!referenceCast.canBeSeenBy(scope)) {
-		        	scope.problemReporter().invalidType(this,
-		        			new ProblemReferenceBinding(
-								CharOperation.splitOn('.', referenceCast.shortReadableName()),
-								referenceCast,
-								ProblemReasons.NotVisible));
+					scope.problemReporter().invalidType(this,
+							new ProblemReferenceBinding(
+									CharOperation.splitOn('.', referenceCast.shortReadableName()),
+									referenceCast,
+									ProblemReasons.NotVisible));
 				}
-	        }
+			}
 		}
 	}
 	super.computeConversion(scope, runtimeTimeType, compileTimeType);
 }",Buggy,"Fixed Bug 416480 - Error in bytecode generated by ECJ compiler leads to
IncompatibleClassChangeError",Buggy,"The commit message indicates a fix for a specific bug (Bug 416480) related to a bytecode error that causes an `IncompatibleClassChangeError`. The code changes reflect an update to the `computeConversion` method, extending the handling of `genericCast` to include both fields (as before) and local variables (a new addition). Here's the breakdown of the changes:

1. **Handling Different Bindings**:
   - Originally, only `FieldBinding` was accounted for when setting `genericCast`.
   - Now, `LocalVariableBinding` is also considered when setting `genericCast`.

2. **Correct Handling of Type Variables**:
   - The logic for adding an extra cast when dealing with type variables has been extended to include local variables, which aligns with addressing the compatibility error indicated by the bug report.

3. **Improved Error Reporting**:
   - The check for visibility and error reporting remains the same but now applies to both fields and local variables.

The overall changes expand the supported scenarios to correctly handle casts, which likely prevents the `IncompatibleClassChangeError`. The presence of additional logic for local variables helps ensure that the compile and runtime types align properly, addressing the bytecode generation issue. These updates are significant enough to conclude that the changes are indeed fixing the bug mentioned in the commit message.

Therefore, the assessment is **Buggy**."
eclipseJdt,25728.json,ab7dabe7a80a22f7a1c8f6edce715e69ee26d4d5,"@@ -1,34 +1,36 @@
 	protected static boolean hasEmptyName(TypeReference reference, ASTNode assistNode) {
 		if (reference == null) return false;
 
-		if (reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
+		// https://bugs.eclipse.org/bugs/show_bug.cgi?id=397070
+		if (reference != assistNode &&
+				reference.sourceStart <= assistNode.sourceStart && assistNode.sourceEnd <= reference.sourceEnd) return false;
 
 		if (reference instanceof CompletionOnSingleTypeReference ||
 				reference instanceof CompletionOnQualifiedTypeReference ||
 				reference instanceof CompletionOnParameterizedQualifiedTypeReference) {
 			char[][] typeName = reference.getTypeName();
 			if (typeName[typeName.length - 1].length == 0) return true;
 		}
 		if (reference instanceof ParameterizedSingleTypeReference) {
 			ParameterizedSingleTypeReference parameterizedReference = (ParameterizedSingleTypeReference) reference;
 			TypeReference[] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (hasEmptyName(typeArguments[i], assistNode)) return true;
 				}
 			}
 		} else if (reference instanceof ParameterizedQualifiedTypeReference) {
 			ParameterizedQualifiedTypeReference parameterizedReference = (ParameterizedQualifiedTypeReference) reference;
 			TypeReference[][] typeArguments = parameterizedReference.typeArguments;
 			if (typeArguments != null) {
 				for (int i = 0; i < typeArguments.length; i++) {
 					if (typeArguments[i] != null) {
 						for (int j = 0; j < typeArguments[i].length; j++) {
 							if (hasEmptyName(typeArguments[i][j], assistNode)) return true;
 						}
 					}
 				}
 			}
 		}
 		return false;
 	}",Buggy,"Fix for bug 397070 - On completion in implements/extends clause,
supertype/interface-signature(s) can not be retrieved from enclosing
type",Buggy,"The commit message states a fix related to completion in implements/extends clause. The change introduces a crucial condition in the `hasEmptyName` method: it ensures that the `reference` and `assistNode` are not the same. This condition prevents a case where an `ASTNode` could incorrectly be considered a `TypeReference` of itself, which would likely lead to incorrect logic execution and failure in retrieving the correct type or interface names.

Adding this additional check addresses the underlying issue where the boundaries of `reference` and `assistNode` were not sufficient to differentiate when they are actually the same node, thereby fixing the bug described in the commit message.

Thus, the code changes align with the message that indicates a bug fix, especially given the specific reference to a bug ID. The provided commentary linking to a bug tracker page further supports this as a bug fix.

The assessment of the change is **Buggy**."
eclipseJdt,20900.json,c79924988b83a4410dc68ac4bbc9ed2e5112cc61,"@@ -1,3 +1,3 @@
-	public static IClasspathContainer getUserLibrary(String name) {
-		return (IClasspathContainer) getLibraryMap().get(name);
+	public static UserLibrary getUserLibrary(String name) {
+		return (UserLibrary) getLibraryMap().get(name);
 	}",Buggy,"Fix bug: invalid getUserLibrary return
",Buggy,"The commit message indicates a fix for an incorrect return type in the getUserLibrary method. The change in the code diff updates the return type from IClasspathContainer to UserLibrary. This suggests that the original implementation likely led to runtime errors when the method was expected to return a UserLibrary object but returned an IClasspathContainer instead. Changing the cast ensures the method returns the correct type, thus aligning with the commit message indicating a bug fix related to an ""invalid getUserLibrary return.""

The reasoning for this conclusion involves:

1. **Type Correction**: Modifying the return type from IClasspathContainer to UserLibrary suggests a bug because the original code possibly returned an object of the wrong type, causing exceptions or incorrect behavior when consumers expected a UserLibrary.

2. **Consistent with Commit Message**: The commit message explicitly mentions fixing a bug with the return type of getUserLibrary. The code change directly addresses this issue.

3. **Potential Runtime Errors**: Incorrect casting can lead to ClassCastException or erroneous data handling, which this change resolves by ensuring the return type matches expected usage.

Therefore, the change directly aligns with the commit message and appears to correct a semantic error in the code. Thus, the appropriate conclusion is **Buggy**."
eclipseJdt,14646.json,8bf6b68ac4b26f2f99e3e6e3e77b19496440917e,"@@ -1,12 +1,11 @@
 	public ITypeBinding getGenericTypeOfWildcardType() {
 		switch (this.binding.kind()) {
 			case Binding.WILDCARD_TYPE :
-			case Binding.INTERSECTION_TYPE :
 				WildcardBinding wildcardBinding = (WildcardBinding) this.binding;
 				if (wildcardBinding.genericType != null) {
 					return this.resolver.getTypeBinding(wildcardBinding.genericType);
 				}
 				break;
 		}
 		return null;
 	}",Buggy,"HEAD - Fixed bug 341759: NPE in ITypeBinding#getName() for intersection type
",Buggy,"The commit message indicates a bug fix for a NullPointerException (NPE) when calling `ITypeBinding#getName()` for intersection types. Here's the analysis of the code change:

1. **Context of the Change**:
   - The method `getGenericTypeOfWildcardType()` is intended to retrieve the generic type of a wildcard type. It uses a switch statement to check the kind of binding.

2. **Specific Change Made**:
   - The line `case Binding.INTERSECTION_TYPE :` has been removed from the switch statement.

3. **Analysis of the Change**:
   - Intersection types were handled similarly to wildcard types in the original code. However, the logic might not have been suitable for intersection types, potentially leading to a NullPointerException if `wildcardBinding.genericType` was null or incorrectly accessed.
   - By removing the `INTERSECTION_TYPE` case, the method no longer treats intersection types as having generic types. Thus, it avoids the possibility of dereferencing `null` and encountering an NPE for such cases.

4. **Relation to Commit Message**:
   - The change directly addresses the commit message, which states a fix for NPE in `ITypeBinding#getName()` for intersection types. The diff shows a removal of inappropriate handling in the method that could lead to such exceptions.

Based on the analysis, the change resolves a potential bug (an NPE) by removing an inappropriate case in the logic that didn't apply correctly to intersection types. Therefore, the assessment is **Buggy**."
eclipseJdt,19389.json,ae30fd6df4fe6e776883eda11e8cb81aefe2c34c,"@@ -1,29 +1,26 @@
 protected IBuffer openBuffer(IProgressMonitor pm, Object info) throws JavaModelException {
 
 	// create buffer
-	IBuffer buffer = this.owner.createBuffer(this);
-	if (buffer == null) return null;
+	IBuffer buffer = BufferManager.createBuffer(this);
 
 	// set the buffer source
-	if (buffer.getCharacters() == null) {
-		IBuffer classFileBuffer = this.classFile.getBuffer();
-		if (classFileBuffer != null) {
-			buffer.setContents(classFileBuffer.getCharacters());
-		} else {
-			// Disassemble
-			IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
-			Disassembler disassembler = new Disassembler();
-			String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
-			buffer.setContents(contents);
-		}
+	IBuffer classFileBuffer = this.classFile.getBuffer();
+	if (classFileBuffer != null) {
+		buffer.setContents(classFileBuffer.getCharacters());
+	} else {
+		// Disassemble
+		IClassFileReader reader = ToolFactory.createDefaultClassFileReader(this.classFile, IClassFileReader.ALL);
+		Disassembler disassembler = new Disassembler();
+		String contents = disassembler.disassemble(reader, Util.getLineSeparator("""", getJavaProject()), ClassFileBytesDisassembler.WORKING_COPY); //$NON-NLS-1$
+		buffer.setContents(contents);
 	}
 
 	// add buffer to buffer cache
 	BufferManager bufManager = getBufferManager();
 	bufManager.addBuffer(buffer);
 
 	// listen to buffer changes
 	buffer.addBufferChangedListener(this);
 
 	return buffer;
 }",Buggy,"HEAD - Fixed bug 337935: Test failures when run as an IDE (org.eclipse.sdk.ide)
",Buggy,"The commit message ""Fixed bug 337935"" suggests a bug fix related to test failures in an IDE context. Let's analyze the code changes to see how they align with this message:

1. **Buffer Creation Logic**: 
   - The original code uses `this.owner.createBuffer(this)` to create a buffer, while the revised code uses `BufferManager.createBuffer(this)`. This alteration might address issues with how buffers are instantiated, potentially fixing bugs related to buffer ownership or creation in IDE contexts.

2. **Buffer Content Setting**: 
   - The code originally checks if `buffer.getCharacters() == null` before setting contents. The revised code assumes the buffer always requires the content to be set from `classFileBuffer` or from disassembly, removing this conditional check.
   - This change could fix bugs where buffers were mistakenly left uninitialized, causing test failures.

3. **General Cleanup**:
   - The code simplifies the logic flow, which reduces potential error-prone states, likely improving reliability and preventing bugs encountered when running tests within an IDE.

Given these considerations, the code modifications indicate a bug fix by addressing buffer initialization and streamlining the content-setting logic. Thus, these changes align well with the commit message claiming a fix for test failures.

The answer is **Buggy**."
eclipseJdt,25842.json,4c94b3d7ef7a5503607858476a6477496a59c95b,"@@ -1,5 +1,5 @@
-protected void consumeLambdaHeader() {
-	super.consumeLambdaHeader();
+protected void consumeNestedLambda() {
+	super.consumeNestedLambda();
 	LambdaExpression lexp = (LambdaExpression) this.astStack[this.astPtr];
 	pushOnElementStack(K_LAMBDA_EXPRESSION_DELIMITER, EXPRESSION_BODY, lexp);
 }",Buggy,"Fixed Bug 430026 - [1.8] Lambda parameter has wrong parent if it
declares its type",Buggy,"The commit message references a bug fix for a specific issue (Bug 430026) with lambda parameters having the wrong parent. The code change renames a method from `consumeLambdaHeader()` to `consumeNestedLambda()`, and it adjusts the corresponding call to the superclass from `consumeLambdaHeader()` to `consumeNestedLambda()`. 

This modification suggests that the logic intended for handling nested lambdas is now explicitly handled through a different method call in the superclass structure. It implies that the previous method might have been incorrectly invoked, causing issues with lambda handling, such as assigning the wrong parent to a lambda parameter when the parameter declares its type.

Given the specificity of the modification and how it relates to the handling of lambda expression components, especially in the context of bug fixing mentioned in the commit message, these changes seem to address the problem described. The rename and adjustment in method behavior appear directly associated with resolving the mentioned bug.

Therefore, these changes indicate a bug fix. The conclusion is **Buggy**."
eclipseJdt,27032.json,b308074bc5b31f3a5857e47f87565296fbba4a15,"@@ -1,34 +1,39 @@
     public static EclipseDeclarationImpl createDeclaration(IBinding binding, BaseProcessorEnv env)
     {
         if(binding == null) return null;
        
         switch(binding.getKind())
         {
     	case IBinding.TYPE:
     		final ITypeBinding typeBinding = (ITypeBinding)binding;
         	if( typeBinding.isAnonymous() || typeBinding.isArray() || 
     			typeBinding.isWildcardType() || typeBinding.isPrimitive() )       
                 throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         	if( typeBinding.isTypeVariable() )
 	        	return new TypeParameterDeclarationImpl(typeBinding, env);
         	else
         		return createReferenceType(typeBinding, env);
         case IBinding.VARIABLE:
         	final IVariableBinding varBinding = (IVariableBinding)binding;            
             if(varBinding.isEnumConstant())
                 return new EnumConstantDeclarationImpl(varBinding, env);
             else
                 return new FieldDeclarationImpl(varBinding, env);
         case IBinding.METHOD:
             final IMethodBinding method = (IMethodBinding)binding;
             if( method.isConstructor() )
                 return new ConstructorDeclarationImpl(method, env);
             final ITypeBinding declaringType = method.getDeclaringClass();
             if( declaringType != null && declaringType.isAnnotation() )
                 return new AnnotationElementDeclarationImpl(method, env);
             else
-                return new MethodDeclarationImpl(method, env);             
+                return new MethodDeclarationImpl(method, env);
+        case IBinding.PACKAGE:
+            // https://bugs.eclipse.org/bugs/show_bug.cgi?id=352949
+            // Don't throw an exception, but just return null. 
+        	// apt also doesn't return a value
+        	return null;
         default:
             throw new IllegalStateException(""failed to create declaration from "" + binding); //$NON-NLS-1$
         }     
     }",Buggy,"Fix for bug 352949: Impossible to process package-level annotations with
an AnnotationProcessor",Buggy,"The diff modifies the `createDeclaration` method to handle `IBinding.PACKAGE` by returning null instead of throwing an exception. This change aligns with the commit message, which refers to a fix for a bug where package-level annotations could not be processed with an AnnotationProcessor. The added comment refers to the specific bug ID, indicating careful tracking of issue resolution.

Before this change, encountering a package-level annotation binding would trigger an exception, disrupting the processing workflow. By safely returning null, the method becomes compatible with the handling of package-level annotations, consistent with other elements where returning null implies non-applicability rather than an error.

Thus, the change directly addresses a bug in how package-level annotations were processed, correctly reflecting the commit message aim of fixing bug 352949. Therefore, this commit indicates a bug fix.

**Buggy**"
eclipseJdt,14260.json,ea916b0462fec6996e619f84c30f348e2877686a,"@@ -1,15 +1,23 @@
 	protected int retrieveEndOfRightParenthesisPosition(int start, int end) {
 		this.scanner.resetTo(start, end);
 		try {
 			int token;
+			int count = 0;
 			while ((token = this.scanner.getNextToken()) != TerminalTokens.TokenNameEOF) {
 				switch(token) {
 					case TerminalTokens.TokenNameRPAREN:
-						return this.scanner.currentPosition;
+						count--;
+						if (count <= 0) return this.scanner.currentPosition;
+						 break;
+					case TerminalTokens.TokenNameLPAREN:
+						count++;
+						//$FALL-THROUGH$
+					default:
+						break;
 				}
 			}
 		} catch(InvalidInputException e) {
 			// ignore
 		}
 		return -1;
 	}",Buggy,"Fix for bug 399600 - ASTConverter#retrieveEndOfRightParenthesisPosition
fails for certain cases",Buggy,"The commit message states a fix for a specific bug related to the ASTConverter's method retrieveEndOfRightParenthesisPosition. The changes in the code introduce a counter, `count`, to balance matching parentheses. Previously, encountering a right parenthesis `)` immediately returned the position. Now, the code only returns when the inner parentheses counter reaches zero, effectively tracking nested structures and ensuring only the corresponding right parenthesis for the initial left parenthesis is considered.

This aligns well with the commit message, indicating a logical correction in handling nested parentheses. Therefore, this is indeed a bug fix addressing the mentioned issue with incorrect parsing.

The conclusion is **Buggy**."
eclipseJdt,11577.json,3d11e595fd95f6b5ceb0fd10d1d7aa7d98828a7b,"@@ -1,56 +1,68 @@
 public void generateSyntheticEnclosingInstanceValues(BlockScope currentScope, ReferenceBinding targetType, Expression enclosingInstance, ASTNode invocationSite) {
 	// supplying enclosing instance for the anonymous type's superclass
 	ReferenceBinding checkedTargetType = targetType.isAnonymousType() ? (ReferenceBinding)targetType.superclass().erasure() : targetType;
 	boolean hasExtraEnclosingInstance = enclosingInstance != null;
 	if (hasExtraEnclosingInstance
 			&& (!checkedTargetType.isNestedType() || checkedTargetType.isStatic())) {
 		currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		return;
 	}
 
 	// perform some emulation work in case there is some and we are inside a local type only
 	ReferenceBinding[] syntheticArgumentTypes;
 	if ((syntheticArgumentTypes = targetType.syntheticEnclosingInstanceTypes()) != null) {
 
 		ReferenceBinding targetEnclosingType = checkedTargetType.enclosingType();
 		long compliance = currentScope.compilerOptions().complianceLevel;
 
 		// deny access to enclosing instance argument for allocation and super constructor call (if 1.4)
 		// always consider it if complying to 1.5
 		boolean denyEnclosingArgInConstructorCall;
 		if (compliance <= ClassFileConstants.JDK1_3) {
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression;
 		} else if (compliance == ClassFileConstants.JDK1_4){
 			denyEnclosingArgInConstructorCall = invocationSite instanceof AllocationExpression
 				|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess();
-		} else {
+		} else if (compliance < ClassFileConstants.JDK1_7) {
 			//compliance >= JDK1_5
 			denyEnclosingArgInConstructorCall = (invocationSite instanceof AllocationExpression
 					|| invocationSite instanceof ExplicitConstructorCall && ((ExplicitConstructorCall)invocationSite).isSuperAccess())
 				&& !targetType.isLocalType();
+		} else {
+			//compliance >= JDK1_7
+			if (invocationSite instanceof AllocationExpression) {
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType();
+			} else if (invocationSite instanceof ExplicitConstructorCall && 
+					((ExplicitConstructorCall)invocationSite).isSuperAccess()) {
+				MethodScope enclosingMethodScope = currentScope.enclosingMethodScope();
+				denyEnclosingArgInConstructorCall = !targetType.isLocalType() && enclosingMethodScope != null
+						&& enclosingMethodScope.isConstructorCall; 
+			} else {
+				denyEnclosingArgInConstructorCall = false;
+			}
 		}
 
 		boolean complyTo14 = compliance >= ClassFileConstants.JDK1_4;
 		for (int i = 0, max = syntheticArgumentTypes.length; i < max; i++) {
 			ReferenceBinding syntheticArgType = syntheticArgumentTypes[i];
 			if (hasExtraEnclosingInstance && TypeBinding.equalsEquals(syntheticArgType, targetEnclosingType)) {
 				hasExtraEnclosingInstance = false;
 				enclosingInstance.generateCode(currentScope, this, true);
 				if (complyTo14){
 					dup();
 					invokeObjectGetClass(); // will perform null check
 					pop();
 				}
 			} else {
 				Object[] emulationPath = currentScope.getEmulationPath(
 						syntheticArgType,
 						false /*not only exact match (that is, allow compatible)*/,
 						denyEnclosingArgInConstructorCall);
 				generateOuterAccess(emulationPath, invocationSite, syntheticArgType, currentScope);
 			}
 		}
 		if (hasExtraEnclosingInstance){
 			currentScope.problemReporter().unnecessaryEnclosingInstanceSpecification(enclosingInstance, checkedTargetType);
 		}
 	}
 }",Buggy,"Fixed bug 373371 - [compiler] JDT Compiler reports an error whereas
javac compiles without problem

Change-Id: I1ca89bd73228aa19ee351e12ef64c474cbc58175
",Buggy,"The commit message specifies fixing a bug related to the JDT Compiler erroneously flagging an error. The diff shows modifications in handling enclosing instance arguments based on the compliance level in the method `generateSyntheticEnclosingInstanceValues`. Originally, only two main scenarios for compliance levels were considered: Java 1.3 and Java 1.4+, with an undifferentiated path between Java 1.5 and 1.6. The introduced change specifically addresses compliance for Java 1.7 and beyond by refining the conditions under which enclosing arguments are denied. This modification aligns with ensuring that the compiler does not misreport errors compared to Java versions that allow more flexible handling of enclosing instances.

The specific update for compliance handling ensures that constructor call checks take into account whether the context is a constructor itself. This adjustment is likely ensuring the correctness of error reporting when using constructs specific to newer Java versions and aligns with correcting compiler behavior to match `javac`.

Overall, the change is addressing the underlying logic flaw that caused the compiler to misreport errors. Therefore, these changes are consistent with the commit message about fixing a bug. Based on the analysis, this commit is a bug fix.

**Buggy**"
eclipseJdt,877.json,48d4662ccc94304b71283abf02dc80106ec8ea24,"@@ -1,21 +1,22 @@
 private void readIndexMap() {
 	try {
 		char[] indexMaps = org.eclipse.jdt.internal.compiler.util.Util.getFileCharContent(this.indexNamesMapFile, null);
 		char[][] names = CharOperation.splitOn('\n', indexMaps);
 		if (names.length >= 3) {
 			// First line is DiskIndex signature (see writeIndexMapFile())
 			String savedSignature = DiskIndex.SIGNATURE;
 			if (savedSignature.equals(new String(names[0]))) {
 				for (int i = 1, l = names.length-1 ; i < l ; i+=2) {
 					IndexLocation indexPath = IndexLocation.createIndexLocation(new URL(new String(names[i])));
+					if (indexPath == null) continue;
 					this.indexLocations.put(new Path(new String(names[i+1])), indexPath );
 					this.indexStates.put(indexPath, REUSE_STATE);
 				}
 			}		
 		}
 	} catch (IOException ignored) {
 		if (VERBOSE)
 			Util.verbose(""Failed to read saved index file names""); //$NON-NLS-1$
 	}
 	return;
 }",Buggy,"Fix for Bug 420590
JavaIndexTests.testExistentIndexAfterRestart failed with error: NPE in
SimpleLookupTable.put
",Buggy,"The commit message states a fix for a NullPointerException (NPE) in SimpleLookupTable.put, encountered during JavaIndexTests.testExistentIndexAfterRestart.

Examining the code diff, we see a change in the `readIndexMap` method. The change adds a null check for `indexPath`. The line `if (indexPath == null) continue;` was introduced to ensure that `indexPath` is not null before proceeding to use it in the subsequent lines, specifically before calling `this.indexLocations.put`.

This change directly addresses a potential scenario where `IndexLocation.createIndexLocation` could return a null value, leading to an NPE when trying to insert a null key into `this.indexLocations` or `this.indexStates`.

The connection between the commit message and the code change is clear: the added null check serves to prevent the NPE described in the bug report.

Therefore, the changes correspond precisely with a bug fix as described in the commit message. The conclusion is **Buggy**."
eclipseJdt,1436.json,0f1f2b20ab9c02179f9ab0cd2826219f39372374,"@@ -1,6 +1,8 @@
 public int match(ReferenceExpression node, MatchingNodeSet nodeSet) {
 	if (!this.pattern.findReferences) return IMPOSSIBLE_MATCH;
 	if (!matchesName(this.pattern.selector, node.selector)) return IMPOSSIBLE_MATCH;
+	if (node.selector != null &&  Arrays.equals(node.selector, org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init))
+		return IMPOSSIBLE_MATCH; // :: new
 	nodeSet.mustResolve = true;
 	return nodeSet.addMatch(node, this.pattern.mustResolve ? POSSIBLE_MATCH : ACCURATE_MATCH);
 }",Buggy,"Fix for [Bug][458614][1.8][search] Constructor reference not found in
search",Buggy,"The commit message indicates a fix for a bug related to constructor references not being found in a search function. The code change introduces a new condition to check if `node.selector` is not null and matches a specific constant `org.eclipse.jdt.internal.compiler.codegen.ConstantPool.Init`, which represents constructor references (`:: new`). If this condition is true, it returns `IMPOSSIBLE_MATCH`, handling a previously unrecognized case appropriately.

This correction likely addresses the bug by ensuring constructor references are correctly identified and excluded from matches when the condition applies. Thus, the code change directly relates to the bug described in the commit message.

The assessment of these modifications, with an added check specifically tailored to improve matching logic and handle constructor references, confirms the approach to fixing the described issue. 

This corresponds with what you would expect in a bug fix for constructor reference matching, aligning entirely with the commit message.

**Buggy**"
eclipseJdt,23118.json,01fe9b211e794ff01834ebe0e40bd02cb1110d75,"@@ -1,30 +1,39 @@
 public static char[] getSignatureSimpleName(char[] typeSignature) {
 	if(typeSignature == null) return CharOperation.NO_CHAR;
 
 	char[] qualifiedType = Signature.toCharArray(typeSignature);
 
 	int dotCount = 0;
 	indexFound: for(int i = 0; i < typeSignature.length; i++) {
 		switch(typeSignature[i]) {
 			case C_DOT:
 				dotCount++;
 				break;
 			case C_GENERIC_START:
 				break indexFound;
 			case C_DOLLAR:
 				break indexFound;
 		}
 	}
 
 	if(dotCount > 0) {
+		int typeStart = 0;
 		for(int i = 0; i < qualifiedType.length; i++) {
-			if(qualifiedType[i] == '.') {
-				dotCount--;
+			switch (qualifiedType[i]) {
+				case '.':
+					dotCount--;
+					break;
+				case ' ':
+					typeStart = i+1;
+					break;
 			}
 			if(dotCount <= 0) {
-				return CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				char[] simpleName = CharOperation.subarray(qualifiedType, i + 1, qualifiedType.length);
+				if (typeStart > 0 && typeStart < qualifiedType.length)
+					return CharOperation.concat(CharOperation.subarray(qualifiedType, 0, typeStart), simpleName);
+				return simpleName;
 			}
 		}
 	}
 	return qualifiedType;
 }",Buggy,"Bug 463533 - Signature.getSignatureSimpleName() returns different
results for resolved and unresolved extends

Change-Id: Iaa8b4273dab596b84734326ad99f775bbe74a330
Signed-off-by: Stephan Herrmann <stephan.herrmann@berlin.de>",Buggy,"The commit message indicates a fix for a bug related to `Signature.getSignatureSimpleName()` returning incorrect results based on whether a type is resolved or unresolved regarding generic parameters.

Let's analyze the code changes:

1. The change introduces additional logic for handling spaces within the type signature. The code now keeps track of typeStart when a space is detected and uses this index to potentially include preceding elements in the returned simple name. The previous logic did not consider spaces or the position to start capturing part of the signature.

2. The additional check `if (typeStart > 0 && typeStart < qualifiedType.length)` uses this new space positioning logic to adjust which parts of the `qualifiedType` are included in the resulting name.

The modification aims to unify signature parsing under different scenarios (resolved vs unresolved) by ensuring parts of the signature before a space (or type start) are appropriately included. This adjustment suggests that dissimilar results were due to missing elements when spaces were involved, likely reflecting either resolved or unresolved signature details.

The changes align with the commit message's intent of addressing a bug concerning variance in method behavior under specific conditions and improving consistency in the returned simplistic signature string. Therefore, based on this analysis, the changes can be confidently concluded as a bug fix:
   
**Buggy**"
eclipseJdt,26368.json,04a3935ecc2f9e24622bcfddda7aa95a0643fdd7,"@@ -1 +1,10 @@
-    public String toString(){ return _arrayBinding.toString(); }

+    public String toString(){ 
+    	final ITypeBinding elementType = _arrayBinding.getElementType();
+    	final StringBuilder buffer = new StringBuilder();
+    	String name = elementType.getQualifiedName();
+    	buffer.append(name);
+		for( int i=0, dim = _arrayBinding.getDimensions(); i<dim; i++ )
+			buffer.append(""[]""); //$NON-NLS-1$
+		
+		return buffer.toString();
+    }",Buggy,"CR244832 | Annotation member that returns Class[] produces error in generated ControlBean

fixed ArrayType toString()

drt: apt.core
",Buggy,"The commit message suggests a fix for a specific issue related to annotation members returning a `Class[]`. The problem likely relates to how array types are represented in string form, potentially causing issues in the code generation process for ControlBeans.

In the provided diff, the `toString` method for some array binding object is modified. The previous implementation simply delegated the `toString` method to `_arrayBinding` itself, which might not have accurately represented array types, especially multi-dimensional arrays.

The newly introduced implementation constructs a more precise representation of the array type. It:
1. Retrieves the element type of the array.
2. Uses a `StringBuilder` to construct a string representation of the type.
3. Appends the element type's qualified name.
4. Appends `[]` for each dimension of the array, accommodating multi-dimensional arrays properly.

This modification ensures that the generated string accurately reflects the type of arrays, including their dimensions, which likely resolves the described issue with `Class[]` annotation members producing errors. It aligns with the commit message and addresses a bug related to the string representation of array types, which could have caused errors in generated code.

Thus, the changes indicate a bug fix, and the conclusion is **Buggy**."
eclipseJdt,1082.json,41e3b1756b128250deb0f9b82843aa520278c691,"@@ -1,53 +1,53 @@
 void matchReportReference(Expression expr, int lastIndex, TypeBinding refBinding, MatchLocator locator) throws CoreException {
 
 	// Look if there's a need to special report for parameterized type
 	if (refBinding.isParameterizedType() || refBinding.isRawType()) {
 
 		// Try to refine accuracy
 		ParameterizedTypeBinding parameterizedBinding = (ParameterizedTypeBinding)refBinding;
 		updateMatch(parameterizedBinding, this.pattern.getTypeArguments(), this.pattern.hasTypeParameters(), 0, locator);
 		
 		// See whether it is necessary to report or not
 		if (match.getRule() == 0) return; // impossible match
 		boolean report = (this.isErasureMatch && match.isErasure()) || (this.isEquivalentMatch && match.isEquivalent()) || match.isExact();
 		if (!report) return;
 
 		// Make a special report for parameterized types if necessary
 		 if (refBinding.isParameterizedType() && this.pattern.hasTypeArguments())  {
 			TypeReference typeRef = null;
 			TypeReference[] typeArguments = null;
 			if (expr instanceof ParameterizedQualifiedTypeReference) {
 				typeRef = (ParameterizedQualifiedTypeReference) expr;
 				typeArguments = ((ParameterizedQualifiedTypeReference) expr).typeArguments[lastIndex];
 			}
 			else if (expr instanceof ParameterizedSingleTypeReference) {
 				typeRef = (ParameterizedSingleTypeReference) expr;
 				typeArguments = ((ParameterizedSingleTypeReference) expr).typeArguments;
 			}
 			if (typeRef != null) {
 				locator.reportAccurateParameterizedTypeReference(match, typeRef, lastIndex, typeArguments);
 				return;
 			}
 		}
 	} else if (this.pattern.hasTypeArguments()) { // binding has no type params, compatible erasure if pattern does
 		match.setRule(SearchPattern.R_ERASURE_MATCH);
 	}
 
 	// Report match
 	if (expr instanceof ArrayTypeReference) {
 		locator.reportAccurateTypeReference(match, expr, this.pattern.simpleName);
 		return;
 	}
 	if (refBinding.isLocalType()) {
-		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=84049
+		// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=82673
 		LocalTypeBinding local = (LocalTypeBinding) refBinding;
 		IJavaElement focus = ((InternalSearchPattern)pattern).focus;
 		if (focus != null && local.enclosingMethod != null && focus.getParent().getElementType() == IJavaElement.METHOD) {
 			IMethod method = (IMethod) focus.getParent();
 			if (!CharOperation.equals(local.enclosingMethod.selector, method.getElementName().toCharArray())) {
 				return;
 			}
 		}
 	}
 	locator.report(match);
 }",Buggy,"Fix bug reference
",NotBuggy,"The commit message states ""Fix bug reference,"" suggesting an update to a bug identifier within the code.

- The only change within the provided diff is the update of a comment from `// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=84049` to `// see bug https://bugs.eclipse.org/bugs/show_bug.cgi?id=82673`.
- This change updates the reference to the correct bug report, which might improve code documentation and traceability but does not modify the logic, behavior, or functionality of the code.

The change exclusively involves altering a comment and does not address any logical errors or functional issues in the code itself, indicating no direct bug fix within the application's logic.

Therefore, based on this diff and the commit message, the change is addressing a documentation aspect rather than fixing an issue in the code logic or behavior itself. Hence, the assessment is **NotBuggy**."
eclipseJdt,25078.json,71d81627399572ca242308eb9b1f8436f445d7df,"@@ -1,9 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
 		TypeBinding type = super.resolveType(scope);
-		if (type instanceof PolyTypeBinding)
+		if (type == null || type instanceof ProblemReferenceBinding || type instanceof PolyTypeBinding)
 			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 440731 - [1.8][code select] Hover, F3 doesn't work for method
reference in method invocation of overloaded method

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a bug related to code selection and navigation in the presence of method references within overloaded methods. The code change introduces additional checks for `null` and `ProblemReferenceBinding` conditions when resolving types. These conditions likely avoid incorrect behavior or errors when navigating code in these scenarios.

Here’s the reasoning step by step:

1. **Original Condition**: The original condition only checked `type instanceof PolyTypeBinding`.
2. **New Condition**: The new condition checks for `type == null`, `type instanceof ProblemReferenceBinding`, and `type instanceof PolyTypeBinding`.
3. **Bug Context**: The message describes a problem with method references and code navigation (hover, F3) likely failing when certain method reference types are encountered.
4. **Error Handling Improvement**: Handling `null` values and `ProblemReferenceBinding` suggests that these scenarios were causing issues in code selection or navigation, which are now accounted for to prevent unexpected behavior.

The modification aligns well with the commit message, indicating that these changes address the stated bug by handling more type resolution cases that could lead to failure in navigation actions like hover or the use of F3.

Final conclusion: The change indicates a bug fix, so the answer is **Buggy**."
eclipseJdt,25078.json,a5e431ef5dd3e4ca66d16fb63b6f792ceeb0643b,"@@ -1,7 +1,9 @@
 	public TypeBinding resolveType(BlockScope scope) {
-		super.resolveType(scope);
+		TypeBinding type = super.resolveType(scope);
+		if (type instanceof PolyTypeBinding)
+			return type;
 		MethodBinding method = getMethodBinding();
 		if (method != null && method.isValidBinding() && !method.isSynthetic())
 			throw new SelectionNodeFound(this.actualMethodBinding);
 		throw new SelectionNodeFound();
 	}",Buggy,"Fixed Bug 424110 - [1.8][hovering] Hover, F3 does not work for method
reference in method invocation",Buggy,"The commit message indicates a fix for a bug where hovering and navigating code (Hover, F3) did not work properly for method references within method invocations. The change made to `resolveType(BlockScope scope)` involves checking if the `TypeBinding` returned by `super.resolveType(scope)` is an instance of `PolyTypeBinding`, and if so, it returns early. This adjustment addresses the likely cause of the bug by accommodating polymorphic method reference handling. This allows the method reference to be resolved correctly, thereby addressing the described issue.

The change directly aligns with fixing incorrect behavior related to method reference resolution. By ensuring specific types are returned correctly, the method invocation can be handled without throwing exceptions prematurely, which is necessary for the navigation feature to function correctly.

Thus, the change appears to address the described bug. Therefore, the assessment of this diff, based on the commit message and code changes, is **Buggy**."
eclipseJdt,3790.json,293a53a19363893dc72bcd9c4bf117c4e143553f,"@@ -1,3 +1,3 @@
 	public TypeBinding[] genericTypeArguments() {
-		return null;
+		return this.resolvedTypeArguments;
 	}",Buggy,"Fixed Bug 420580 - [1.8][compiler] ReferenceExpression drops explicit
type arguments",Buggy,"The diff changes the method `genericTypeArguments()` from returning `null` to returning `this.resolvedTypeArguments`. This adjustment suggests the fix of an issue related to handling or using generic type arguments in the context of reference expressions in the compiler. The commit message refers specifically to a bug fix (Bug 420580), stating that the explicit type arguments were dropped, which implies that the change directly addresses that issue by returning the resolved type arguments instead of null, thus maintaining those arguments.

Based on this analysis, the change effectively aligns with the commit message that indicates a bug fix. Therefore, the answer is **Buggy**."
eclipseJdt,144.json,63326b7a3054b32190c64b8268487d927040f016,"@@ -1,4 +1,4 @@
 	public Element getEnclosingElement()
 	{
-		return null;
+		return getGenericElement();
 	}",Buggy,"HEAD - Fixed bug 342470: javax.lang.model.element.Element.getEnclosingElement() doesn't return null for type parameter
",Buggy,"The commit message explicitly states that the commit fixes a bug where `javax.lang.model.element.Element.getEnclosingElement()` was incorrectly returning null for a type parameter, which is not the expected behavior.

In the code diff, we see a change in the method `getEnclosingElement()`, where the hardcoded `return null;` has been replaced with `return getGenericElement();`. This modification suggests that instead of the method always returning null, it now returns the result of `getGenericElement()`. This change aligns with the commit message, as it addresses the incorrectly returned null value and corrects it to provide the actual enclosing element.

Since the change directly corrects the logic of what the method should return, this is a clear indication of a bug fix.

Thus, the assessment of the change with respect to the commit message leads to the conclusion: **Buggy**."
eclipseJdt,11825.json,ee5e3843269e8af70318f7535175e87d19c752b4,"@@ -1,10 +1,11 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
 			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
+			: binding.isPrivate() ? MethodHandleRefKindInvokeSpecial
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 439889 - [1.8][compiler] [lambda] Deserializing lambda fails
with IllegalArgumentException: ""Invalid lambda deserialization""",Buggy,"The commit message indicates fixing a bug related to lambda deserialization, involving `IllegalArgumentException`. The diff modifies `literalIndexForMethodHandle` by adding a private method case `MethodHandleRefKindInvokeSpecial` for non-interface classes. This change properly handles private methods, addressing potential deserialization issues. The update aligns with the bug fix intent in the commit message. Therefore, the changes indicate a bug fix. The answer is **Buggy**."
eclipseJdt,11825.json,6a493caae74ed7f294c44634abfb786fa2111d3d,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : binding.isPrivate() ? MethodHandleRefKindInvokeSpecial : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421797 - [1.8][compiler] ClassFormatError with default 
methods & I.super.foo() syntax ",Buggy,"The commit message states that the fix addresses a `ClassFormatError` related to default methods and the `I.super.foo()` syntax. The change in the code updates the logic that determines the `referenceKind` for method handles. Specifically, it introduces an additional check for `binding.isPrivate()` when the method is an instance method of an interface. This ensures that the correct method handle reference kind (`MethodHandleRefKindInvokeSpecial`) is used for private methods in interfaces, which is necessary to handle default and private methods correctly according to the Java Language Specification.

This change directly relates to a bug fix because incorrect handling of reference kinds could lead to `ClassFormatError`, especially when handling default methods and `I.super.foo()` syntax, which are new features in Java 8. By adding the condition for private interface methods, it resolves the described issue.

Therefore, the code change aligns well with the commit message and the problem it describes, indicating a bug fix.

**Buggy**"
eclipseJdt,11825.json,ffd96339198fcdfc38a0c44d4da9dfd0a59d21fc,"@@ -1,10 +1,10 @@
 	public int literalIndexForMethodHandle(MethodBinding binding) {
 		boolean isInterface = binding.declaringClass.isInterface();
 		int referenceKind =
-			isInterface ? MethodHandleRefKindInvokeInterface
+			isInterface ? binding.isStatic() ? MethodHandleRefKindInvokeStatic : MethodHandleRefKindInvokeInterface
 			: binding.isConstructor() ? MethodHandleRefKindNewInvokeSpecial
 			: binding.isStatic() ? MethodHandleRefKindInvokeStatic
 			: MethodHandleRefKindInvokeVirtual;
 		
 		return literalIndexForMethodHandle(referenceKind, binding.declaringClass, binding.selector, binding.signature(), isInterface);
 	}",Buggy,"Fixed Bug 421712 - [1.8][compiler] java.lang.NoSuchMethodError with
lambda expression in interface default method.",Buggy,"The diff modifies the handling of method references in the `literalIndexForMethodHandle` method, particularly for interface methods. Originally, the code always used `MethodHandleRefKindInvokeInterface` for interface methods. The change adds a condition to use `MethodHandleRefKindInvokeStatic` if the interface method is static. This change addresses incorrect behavior when a lambda expression is used in an interface static method, which could lead to a `java.lang.NoSuchMethodError`, particularly if the method reference kind was improperly identified. The modification provides more accurate handling of static methods within interfaces by correctly distinguishing between static and non-static interface methods.

The commit message indicates that the issue fixed is aligned with Bug 421712, focusing on lambda expressions in interface default methods. The diff resolves a potential source of `NoSuchMethodError` by ensuring the correct method handle reference kind is used.

Based on the alignment of the code changes with the described error and the bug fix in the commit message, this change indicates a bug fix. Therefore, the conclusion is **Buggy**."
eclipseJdt,27787.json,6c76d390a016d2bc042688766f03adca637f8318,"@@ -1,4 +1,9 @@
 	public int run(InputStream in, OutputStream out, OutputStream err, String... arguments) {
-		boolean succeed = new Main(new PrintWriter(new OutputStreamWriter(out)), new PrintWriter(new OutputStreamWriter(err)), true/*systemExit*/, null/*options*/, null/*progress*/).compile(arguments);
+		boolean succeed = new Main(
+				new PrintWriter(new OutputStreamWriter(out != null ? out : System.out)),
+				new PrintWriter(new OutputStreamWriter(err != null ? err : System.err)),
+				true/* systemExit */,
+				null/* options */,
+				null/* progress */).compile(arguments);
 		return succeed ? 0 : -1;
 	}",Buggy,"Fixed Bug 426434 - EclipseCompiler#run() crashes when null out/err
passed by client",Buggy,"The commit message addresses a specific bug where the EclipseCompiler#run() method crashes if null values are passed for the out or err output streams by the client. The code modification aligns with this message by adding null checks for the `out` and `err` parameters. If either is null, it substitutes the standard System.out or System.err respectively, preventing a NullPointerException that might cause the method to crash.

This change clearly indicates a bug fix as it directly resolves the issue described in the commit message. It improves the robustness of the method by handling cases where clients might pass null for output streams, ensuring the function does not crash.

Hence, the changes are a direct fix for the bug described, and we can conclude with **Buggy**."
eclipseJdt,16316.json,f372eb4169324cad87b7cff6d25b5939cf1574c9,"@@ -1,4 +1,4 @@
 	public boolean isLocalTypeDeclaration() {
 		ASTNode parent = getParent();
-		return (parent instanceof Block);
+		return (parent instanceof TypeDeclarationStatement);
 	}",Buggy,"Fix TypeDeclaration.isLocalTypeDeclaration (bug #10468)
",Buggy,"The change focuses on the method `isLocalTypeDeclaration`. Originally, it checks if the parent node of a type declaration is a `Block`, indicating a local type declaration. However, the revised code changes the parent check from `Block` to `TypeDeclarationStatement`, which likely indicates a more specific or correct representation of what constitutes a local type declaration in this context.

Let's analyze step by step:

1. **Original Code**: The method checks whether the parent of a type declaration is a `Block`. A `Block` could encompass more than what constitutes a local type declarative context, potentially causing incorrect behavior where types are incorrectly classified as local.

2. **Modified Code**: Switching the check to `TypeDeclarationStatement` suggests that the type declaration's locality is more accurately determined by being immediately inside a `TypeDeclarationStatement`, not just any `Block`. This correction likely addresses situations where non-local types were erroneously marked as local, or vice versa, thus potentially causing incorrect behaviors in type handling.

3. **Bug Fix Nature**: The commit message explicitly references a bug (#10468), indicating prior erroneous behavior in determining local type declarations. Changing the check reflects a precise fix aligning with an accurate semantic context.

Considering that the adjustment aligns with the purpose of correcting functionality surrounding type declarations, and the commit message clearly indicates that this change addresses a specific bug, we can conclude that this modification indeed pertains to a bug fix.

Therefore, the conclusion is **Buggy**."
eclipseJdt,5374.json,8739fac6b0362a32fbe52c714ff661e202a84e1e,"@@ -1,31 +1,31 @@
 public void resolve(BlockScope upperScope) {
 	// special scope for secret locals optimization.
 	this.scope = new BlockScope(upperScope);
 	TypeBinding type = this.expression.resolveType(this.scope);
-	if (type == null)
-		return;
-	switch (type.id) {
-		case T_boolean :
-		case T_char :
-		case T_float :
-		case T_double :
-		case T_byte :
-		case T_short :
-		case T_int :
-		case T_long :
-			this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
-			break;
-		case T_void :
-			this.scope.problemReporter().illegalVoidExpression(this.expression);
-			break;
-		case T_null :
-			this.scope.problemReporter().invalidNullToSynchronize(this.expression);
-			break;
+	if (type != null) {
+		switch (type.id) {
+			case T_boolean :
+			case T_char :
+			case T_float :
+			case T_double :
+			case T_byte :
+			case T_short :
+			case T_int :
+			case T_long :
+				this.scope.problemReporter().invalidTypeToSynchronize(this.expression, type);
+				break;
+			case T_void :
+				this.scope.problemReporter().illegalVoidExpression(this.expression);
+				break;
+			case T_null :
+				this.scope.problemReporter().invalidNullToSynchronize(this.expression);
+				break;
+			}
+			//continue even on errors in order to have the TC done into the statements
+			this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
+			this.scope.addLocalVariable(this.synchroVariable);
+			this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
+			this.expression.computeConversion(this.scope, type, type);
 	}
-	//continue even on errors in order to have the TC done into the statements
-	this.synchroVariable = new LocalVariableBinding(SecretLocalDeclarationName, type, ClassFileConstants.AccDefault, false);
-	this.scope.addLocalVariable(this.synchroVariable);
-	this.synchroVariable.setConstant(Constant.NotAConstant); // not inlinable
-	this.expression.computeConversion(this.scope, type, type);
 	this.block.resolveUsing(this.scope);
 }",Buggy,"Bug 465048 - Bindings are null inside synchronized blocks

Resolve the block-part of a synchronized block, even if the synchronized
variable can't be resolved.

Change-Id: I3ed513fa21b10ae9c7fa39f1a94fe85fb98e5eb9
Signed-off-by: Carmi Grushko <carmi@google.com>
",Buggy,"The commit message addresses a bug where bindings become null inside synchronized blocks. The code change ensures the block part of a synchronized block is resolved, even if there are issues with the synchronized variable. Previously, if the type was `null`, the function would return immediately, which could leave the block unresolved and lead to a bug where bindings are null. By wrapping the type handling in a check and continuing with block resolution even if issues are encountered, the change appears to fix this bug. The commit aligns with changes made to account for unresolved types properly while proceeding with block resolution.

Thus, the conclusion is **Buggy**."
eclipseJdt,25918.json,8986600df161cd7584749fb6a28497a308fa862c,"@@ -1,5 +1,3 @@
 public boolean requireExtendedRecovery() {
-	if (this.assistNode instanceof TypeReference || this.assistNode instanceof CompletionOnKeyword2)
-		return false;
 	return lastIndexOfElement(K_LAMBDA_EXPRESSION_DELIMITER) >= 0;
 }",Buggy,"Fixed Bug 427463 - [1.8][content assist] No completions available in
throw statement within lambda body ",Buggy,"Analyzing the provided Git diff and commit message:

1. **Commit Message**: It states a specific bug fix, ""Bug 427463,"" related to content assist not offering completions in a throw statement within a lambda body.

2. **Code Changes**: The method `requireExtendedRecovery` has been modified by removing two conditions:
   - `this.assistNode instanceof TypeReference`
   - `this.assistNode instanceof CompletionOnKeyword2`

   Now, the function only returns true if `lastIndexOfElement(K_LAMBDA_EXPRESSION_DELIMITER) >= 0`.

3. **Potential Bug Indicated**: 
   - The change in logic suggests that the previous conditions were incorrectly preventing the method from returning true in some circumstances, likely when completion suggestions should have been offered within lambda expressions. 
   - By removing these potentially restrictive conditions, the behavior aligns more with expected functionality (providing completions in lambda expressions).

Thus, this change is directly related to resolving a bug concerning content assist for throw statements within lambda bodies. Therefore, the modification is consistent with the commit message and indicates a bug fix.

The answer is **Buggy**."
eclipseJdt,17024.json,c554caa948bbe051a5aeee635338c74832d8067f,"@@ -1,16 +1,15 @@
 	public static Token fromCurrent(Scanner scanner, int currentToken) {
 		int start = scanner.getCurrentTokenStartPosition();
 		int end = scanner.getCurrentTokenEndPosition();
 		if (currentToken == TokenNameCOMMENT_LINE) {
 			// don't include line separator
-			String source = scanner.getCurrentTokenString();
-			for (int i = source.length() - 1; i > 0; i--) {
-				char c = source.charAt(i);
+			while(end >= start) {
+				char c = scanner.source[end];
 				if (c != '\r' && c != '\n')
 					break;
 				end--;
 			}
 		}
 		Token token = new Token(start, end, currentToken);
 		return token;
 	}",Buggy,"Fixed bug 471090: Java Code Formatter breaks code if single line
comments contains unicode escape

Change-Id: Id2c901c7853e4062182163ef714afc0e43833baf
Signed-off-by: Till Brychcy <register.eclipse@brychcy.de>
",Buggy,"The commit message states a bug fix related to the Java Code Formatter improperly handling Unicode escapes in single-line comments. 

Examining the code diff:

1. **Original Code**:
   - Retrieves the token's source as a string.
   - Iterates over the source string backwards to adjust the `end` position, ensuring line separators are not included.

2. **Modified Code**:
   - Uses a more direct approach by accessing the `scanner.source` character array instead of converting it to a string, which likely avoids issues with Unicode escapes when comments are parsed.
   - The loop condition checks positions directly to trim line-end characters (`\r` or `\n`).

This updated strategy avoids potential problems arising from String manipulation of Unicode sequences, aligning well with the noted bug fix in the commit message description. Given that this change pertains to improving the handling of comments parsing in relation to specific characters that could disrupt formatting, and directly addresses the reported issue, it is indicative of a bug fix.

Hence, the assessment is **Buggy**."
eclipseJdt,1305.json,63969f0e42bcdfc0a055ac0f3d2f47fa48cf3e29,"@@ -1,7 +1,9 @@
 protected void consumeCastExpressionLL1WithBounds() {
 	super.consumeCastExpressionLL1WithBounds();
 	if ((this.patternFineGrain & IJavaSearchConstants.CAST_TYPE_REFERENCE) != 0) {
 		CastExpression castExpression = (CastExpression) this.expressionStack[this.expressionPtr];
-		this.patternLocator.match(castExpression.type, this.nodeSet);
+		TypeReference[] typeReferences = ((IntersectionCastTypeReference) castExpression.type).typeReferences;
+		for (int i = 0, length = typeReferences.length; i < length; i++)
+			this.patternLocator.match(typeReferences[i], this.nodeSet);
 	}
 }",Buggy,"Fixed Bug 427537 - [1.8][search] CCE with search match location set to
cast type and intersection casts",Buggy,"The commit message outlines a fix for a ClassCastException (CCE) that arises in a scenario involving cast expressions and intersection types. The provided diff addresses this issue by modifying the `consumeCastExpressionLL1WithBounds` method to correctly handle intersection cast types.

Originally, the code assumed that `castExpression.type` would be a single `TypeReference`. This assumption led to a ClassCastException when `castExpression.type` was, in fact, an `IntersectionCastTypeReference` that contains multiple `TypeReference` objects within it. The change in the code iterates over each type reference within the `IntersectionCastTypeReference` and applies the `match` operation to each one individually. This accurately handles the complex type structure and prevents the ClassCastException that was previously occurring.

This change aligns precisely with the commit message, which mentions a specific bug related to casting and intersection types, and describes the correction that prevents the exception from being thrown.

Therefore, this change clearly represents a bug fix, as confirmed by the updated handling of type references within an intersection cast scenario.

The verdict for this commit is **Buggy**."
eclipseJdt,17644.json,f7608e255b7a2e7cde81de854a6575f0fb055925,"@@ -1,20 +1,20 @@
 public char[] getMainTypeName() {
 	if (this.mainTypeName == null) {
 		int start = CharOperation.lastIndexOf('/', this.fileName) + 1;
 		if (start == 0 || start < CharOperation.lastIndexOf('\\', this.fileName))
 			start = CharOperation.lastIndexOf('\\', this.fileName) + 1;
-		int separator = CharOperation.indexOf('|', this.fileName) + 1;
+		int separator = CharOperation.lastIndexOf('|', this.fileName) + 1;
 		if (separator > start) // case of a .class file in a default package in a jar
 			start = separator;
 
 		int end = CharOperation.lastIndexOf('$', this.fileName);
 		if (end == -1 || !Util.isClassFileName(this.fileName)) {
 			end = CharOperation.lastIndexOf('.', this.fileName);
 			if (end == -1)
 				end = this.fileName.length;
 		}
 
 		this.mainTypeName = CharOperation.subarray(this.fileName, start, end);
 	}
 	return this.mainTypeName;
 }",Buggy,"Fix for bug 515484 [1.9] [dom ast] ast not generated for
module-info.class  ",Buggy,"The commit message suggests a fix for a specific bug related to the AST (Abstract Syntax Tree) generation for a `module-info.class` file. The change in the code involves modifying how the `separator` index is determined. Originally, the code used `CharOperation.indexOf('|', this.fileName) + 1`, which finds the first occurrence of the '|' character. The update changes this to `CharOperation.lastIndexOf('|', this.fileName) + 1`, which finds the last occurrence of '|' and adjusts the index calculation accordingly.

This adjustment is critical when dealing with files that might contain multiple '|' characters. By using the last index, the code is likely resolving an issue where incorrect indices could cause the system to incorrectly identify the main type name, particularly impacting `.class` files in jar packages. This aligns well with the problem of generating AST for module-related class files, which might include complex path names in build configurations.

Given that the commit message specifies a bug fix and the change directly addresses a potential flaw in file name parsing, it indicates a logical correction in the implementation. Thus, this change can be associated with a bug fix.

The conclusion is **Buggy**."
eclipseJdt,19098.json,302894c7bc98fd880776f5e317788c7b3da88ebe,"@@ -1,23 +1,23 @@
 protected void recordParticipantResult(CompilationParticipantResult result) {
 	// any added/changed/deleted generated files have already been taken care
 	// just record the problems and dependencies - do not expect there to be many
 	// must be called after we're finished with the compilation unit results but before incremental loop adds affected files
 	CategorizedProblem[] problems = result.problems;
 	if (problems != null && problems.length > 0) {
 		// existing problems have already been removed so just add these as new problems
 		this.notifier.updateProblemCounts(problems);
 		try {
 			storeProblemsFor(result.sourceFile, problems);
 		} catch (CoreException e) {
 			// must continue with compile loop so just log the CoreException
 			Util.log(e, ""JavaBuilder logging CompilationParticipant's CoreException to help debugging""); //$NON-NLS-1$
 		}
 	}
 
 	String[] dependencies = result.dependencies;
 	if (dependencies != null) {
-		ReferenceCollection refs = (ReferenceCollection) this.newState.references.get(result.sourceFile.typeLocator());
+		ReferenceCollection refs = this.newState.references.get(result.sourceFile.typeLocator());
 		if (refs != null)
 			refs.addDependencies(dependencies);
 	}
 }",Buggy,"Bug 563030 - builder.State: SimpleLookupTable performance does not
scale

- resolve new warning

Change-Id: I20cd500a7872eaa5cbda532f8f316d1a4168a3d9
",NotBuggy,"The commit message indicates a performance issue associated with the `SimpleLookupTable` not scaling well, with an additional note about resolving a ""new warning."" The code change removes an unnecessary cast from `ReferenceCollection`, which, while a refactor that may resolve type-related warnings, does not inherently align with the performance issue scaling implication. 

The resolution of a potential warning by removing an unnecessary cast does not directly suggest a bug fix, particularly since no change in behavior or logic is addressed that aligns with the performance scaling issue indicated in the commit message.

Thus, the changes in the diff do not appear to address the bug as described in the commit message, focusing instead on code cleanup or refactoring unrelated to bug resolution. Therefore, the answer is **NotBuggy**."
eclipseJdt,25968.json,15950deb058261ec048eb3ddbd1c2ef13df604e6,"@@ -1,29 +1,30 @@
 private CodeSnippetToCuMapper getMapper() {
 	if (this.mapper == null) {
 		char[] varClassName = null;
 		VariablesInfo installedVars = this.context.installedVars;
 		if (installedVars != null) {
 			char[] superPackageName = installedVars.packageName;
 			if (superPackageName != null && superPackageName.length != 0) {
 				varClassName = CharOperation.concat(superPackageName, installedVars.className, '.');
 			} else {
 				varClassName = installedVars.className;
 			}
 
 		}
 		this.mapper = new CodeSnippetToCuMapper(
 			this.codeSnippet,
 			this.context.packageName,
 			this.context.imports,
 			getClassName(),
 			varClassName,
 			this.context.localVariableNames,
 			this.context.localVariableTypeNames,
 			this.context.localVariableModifiers,
 			this.context.declaringTypeName,
-			this.context.lineSeparator
+			this.context.lineSeparator,
+			CompilerOptions.versionToJdkLevel(this.options.get(JavaCore.COMPILER_COMPLIANCE))
 		);
 
 	}
 	return this.mapper;
 }",Buggy,"HEAD - Fixed bug 345334: CodeSnippet's run method is missing @Override annotation
",NotBuggy,"The commit message indicates a fix for a missing `@Override` annotation on the `run` method, but the provided diff does not relate to any method or annotation updates. Instead, it adds an argument to the `CodeSnippetToCuMapper` constructor, specifically a JDK level retrieved from the compiler options. This change doesn't address any missing override annotation but seems to handle internal configuration more effectively or ensure compatibility.

The discrepancy between the commit message and the changes in the diff suggests that this particular modification was not directly related to the `@Override` annotation specified. Therefore, this change does not align with the specified bug description of the commit message.

Given that the actual code modification addresses a different aspect than the one described in the commit message, and without specific evidence in the code diff that suggests a direct bug related to the message's description, the relevance to a bug fix as described in the commit message is questionable.

**NotBuggy**"
eclipseJdt,4105.json,35e2019a40c179f5dd02638e5bff541c41227721,"@@ -1,6 +1,9 @@
 	void recordResolution(LookupEnvironment env, TypeBinding typeFound) {
-		if (typeFound != null && typeFound.isValidBinding())
-			for (int i = 0; i < env.resolutionListeners.length; i++) {
-				env.resolutionListeners[i].recordResolution(this, typeFound);
+		if (typeFound != null && typeFound.isValidBinding()) {
+			synchronized (env.root) {
+				for (int i = 0; i < env.root.resolutionListeners.length; i++) {
+					env.root.resolutionListeners[i].recordResolution(this, typeFound);
+				}
 			}
+		}
 	}",Buggy,"Fix for bug 519980 [9][search] search for a type does not return the
matches in provides 

Change-Id: I0059fa02a1040f10a8f4a949c07806018dee73b5",Buggy,"The commit message indicates a fix for a specific bug related to search functionality in a particular system. Analyzing the diff, the modification involves introducing synchronization when iterating over `env.root.resolutionListeners`. This change improves thread safety, likely addressing concurrent access issues that could lead to missing or incomplete data handling, such as not returning matches as expected. The synchronization suggests fixing a bug related to how types were being resolved and recorded under concurrent scenarios.

The changes align with the commit message indicating a resolution to a bug (in particular, bug 519980), making it a relevant update to ensure proper functionality during simultaneous operations.

Therefore, the analysis of the diff suggests a bug fix. The conclusion is **Buggy**."
eclipseJdt,11564.json,266fb0d2bc58e2f4f179d2c1279dee095ed53e45,"@@ -1,6 +1,6 @@
 public void generateSyntheticBodyForArrayConstructor(SyntheticMethodBinding methodBinding) {
 	initializeMaxLocals(methodBinding);
 	iload_0();
-	anewarray(((ArrayBinding) methodBinding.returnType).elementsType());
+	newArray(null, null, (ArrayBinding) methodBinding.returnType);
 	areturn();
 }",Buggy,"Fixed Bug 424444 - [1.8] VerifyError when constructor reference used
with primitive array",Buggy,"Analyzing the changes in the code diff:

1. **Functionality**: The method `generateSyntheticBodyForArrayConstructor` is updated. Originally, it used `anewarray` with a parameter extracted from the method binding's return type. The modification replaces this with `newArray(null, null, (ArrayBinding) methodBinding.returnType)`.

2. **Context of the Change**: The commit message refers to a bug (Bug 424444) related to a `VerifyError` encountered when using a constructor reference with a primitive array. A `VerifyError` typically indicates a verification failure in JVM bytecode, usually due to improper bytecode structure or operations that violate JVM rules.

3. **Code Modification**: The change in the method call suggests a different approach for constructing an array. This alteration might address issues with how arrays (particularly primitive arrays) are constructed, preventing a `VerifyError`.

4. **Verification Error Cause**: The bug could have been caused by incorrect handling of primitive array types within the original `anewarray` call, possibly due to not accommodating all necessary parameters or configurations specific to primitive types.

Given these points, the commit message and the code change align well, indicating a bug fix intended to resolve a `VerifyError` related to array construction with references. There is a clear indication that this change aims to resolve an identified error in the code’s logic or its execution environment.

The conclusion is that the change is a bug fix. Therefore, the assessment is **Buggy**."
eclipseJdt,6346.json,409121a5eb3d3ef99ff5c31121bd10011631e82f,"@@ -1,61 +1,65 @@
 	private void checkAndSetModifiersForField(FieldBinding fieldBinding, FieldDeclaration fieldDecl) {
 		int modifiers = fieldBinding.modifiers;
 		final ReferenceBinding declaringClass = fieldBinding.declaringClass;
 		if ((modifiers & ExtraCompilerModifiers.AccAlternateModifierProblem) != 0)
 			problemReporter().duplicateModifierForField(declaringClass, fieldDecl);
 
 		if (declaringClass.isInterface()) {
 			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal;
 			// set the modifiers
 			modifiers |= IMPLICIT_MODIFIERS;
 
 			// and then check that they are the only ones
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != IMPLICIT_MODIFIERS) {
 				if ((declaringClass.modifiers  & ClassFileConstants.AccAnnotation) != 0)
 					problemReporter().illegalModifierForAnnotationField(fieldDecl);
 				else
 					problemReporter().illegalModifierForInterfaceField(fieldDecl);
 			}
 			fieldBinding.modifiers = modifiers;
 			return;
 		} else if (fieldDecl.getKind() == AbstractVariableDeclaration.ENUM_CONSTANT) {
 			// check that they are not modifiers in source
 			if ((modifiers & ExtraCompilerModifiers.AccJustFlag) != 0)
 				problemReporter().illegalModifierForEnumConstant(declaringClass, fieldDecl);
 
 			// set the modifiers
-			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum;
+			// https://bugs.eclipse.org/bugs/show_bug.cgi?id=267670. Force all enumerators to be marked
+			// as used locally. We are unable to track the usage of these reliably as they could be used
+			// in non obvious ways via the synthesized methods values() and valueOf(String) or by using 
+			// Enum.valueOf(Class<T>, String).
+			final int IMPLICIT_MODIFIERS = ClassFileConstants.AccPublic | ClassFileConstants.AccStatic | ClassFileConstants.AccFinal | ClassFileConstants.AccEnum | ExtraCompilerModifiers.AccLocallyUsed;
 			fieldBinding.modifiers|= IMPLICIT_MODIFIERS;
 			return;
 		}
 
 		// after this point, tests on the 16 bits reserved.
 		int realModifiers = modifiers & ExtraCompilerModifiers.AccJustFlag;
 		final int UNEXPECTED_MODIFIERS = ~(ClassFileConstants.AccPublic | ClassFileConstants.AccPrivate | ClassFileConstants.AccProtected | ClassFileConstants.AccFinal | ClassFileConstants.AccStatic | ClassFileConstants.AccTransient | ClassFileConstants.AccVolatile);
 		if ((realModifiers & UNEXPECTED_MODIFIERS) != 0) {
 			problemReporter().illegalModifierForField(declaringClass, fieldDecl);
 			modifiers &= ~ExtraCompilerModifiers.AccJustFlag | ~UNEXPECTED_MODIFIERS;
 		}
 
 		int accessorBits = realModifiers & (ClassFileConstants.AccPublic | ClassFileConstants.AccProtected | ClassFileConstants.AccPrivate);
 		if ((accessorBits & (accessorBits - 1)) > 1) {
 			problemReporter().illegalVisibilityModifierCombinationForField(declaringClass, fieldDecl);
 
 			// need to keep the less restrictive so disable Protected/Private as necessary
 			if ((accessorBits & ClassFileConstants.AccPublic) != 0) {
 				if ((accessorBits & ClassFileConstants.AccProtected) != 0)
 					modifiers &= ~ClassFileConstants.AccProtected;
 				if ((accessorBits & ClassFileConstants.AccPrivate) != 0)
 					modifiers &= ~ClassFileConstants.AccPrivate;
 			} else if ((accessorBits & ClassFileConstants.AccProtected) != 0 && (accessorBits & ClassFileConstants.AccPrivate) != 0) {
 				modifiers &= ~ClassFileConstants.AccPrivate;
 			}
 		}
 
 		if ((realModifiers & (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile)) == (ClassFileConstants.AccFinal | ClassFileConstants.AccVolatile))
 			problemReporter().illegalModifierCombinationFinalVolatileForField(declaringClass, fieldDecl);
 
 		if (fieldDecl.initialization == null && (modifiers & ClassFileConstants.AccFinal) != 0)
 			modifiers |= ExtraCompilerModifiers.AccBlankFinal;
 		fieldBinding.modifiers = modifiers;
 	}",Buggy,"Fix for bug #267670
",Buggy,"The commit message refers to a ""Fix for bug #267670,"" and the diff provides context by addressing an issue with enum constants in a Java compiler's handling of field modifiers.

Here's the detailed breakdown of the diff:

1. **Original Code:** The `IMPLICIT_MODIFIERS` for an enum constant initially included `AccPublic`, `AccStatic`, `AccFinal`, and `AccEnum`.

2. **Modified Code:** The diff has added `ExtraCompilerModifiers.AccLocallyUsed` to the modifiers for an enum constant.

3. **Commit Comments:** A comment added in the code points to an issue tracked by a bug ID and explains the reasoning for the change. Specifically, it notes that all enumerator fields are forced to be marked as ""used locally."" This is to address the inability to track the usage of enum constants reliably because they could be used indirectly through synthesized methods like `values()`, `valueOf(String)`, or `Enum.valueOf(Class<T>, String)`.

This change indicates a bug fix because it addresses a known issue (bug #267670) related to enum constant handling. It ensures correct behavior in marking enum constants' usage, which was previously not handled properly or completely.

Given the focus on correcting a specific behavioral issue with enum usage and marking, the answer is **Buggy**."
eclipseJdt,4552.json,cc7009a12280dbb7a101bdbfbf1048948dc0093c,"@@ -1,66 +1,72 @@
 protected void verifyDuplicationAndOrder(int length, TypeBinding[] argumentTypes, boolean containsUnionTypes) {
 	// Verify that the catch clause are ordered in the right way:
 	// more specialized first.
 	if (containsUnionTypes) {
 		int totalCount = 0;
 		ReferenceBinding[][] allExceptionTypes = new ReferenceBinding[length][];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			ReferenceBinding currentExceptionType = (ReferenceBinding) argumentTypes[i];
 			TypeReference catchArgumentType = this.catchArguments[i].type;
 			if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 				TypeReference[] typeReferences = ((UnionTypeReference) catchArgumentType).typeReferences;
 				int typeReferencesLength = typeReferences.length;
 				ReferenceBinding[] unionExceptionTypes = new ReferenceBinding[typeReferencesLength];
 				for (int j = 0; j < typeReferencesLength; j++) {
 					unionExceptionTypes[j] = (ReferenceBinding) typeReferences[j].resolvedType;
 				}
 				totalCount += typeReferencesLength;
 				allExceptionTypes[i] = unionExceptionTypes;
 			} else {
 				allExceptionTypes[i] = new ReferenceBinding[] { currentExceptionType };
 				totalCount++;
 			}
 		}
 		this.caughtExceptionTypes = new ReferenceBinding[totalCount];
 		this.caughtExceptionsCatchBlocks  = new int[totalCount];
 		for (int i = 0, l = 0; i < length; i++) {
 			ReferenceBinding[] currentExceptions = allExceptionTypes[i];
+			if (currentExceptions == null) continue;
 			loop: for (int j = 0, max = currentExceptions.length; j < max; j++) {
 				ReferenceBinding exception = currentExceptions[j];
 				this.caughtExceptionTypes[l] = exception;
 				this.caughtExceptionsCatchBlocks[l++] = i;
 				// now iterate over all previous exceptions
 				for (int k = 0; k < i; k++) {
 					ReferenceBinding[] exceptions = allExceptionTypes[k];
+					if (exceptions == null) continue;
 					for (int n = 0, max2 = exceptions.length; n < max2; n++) {
 						ReferenceBinding currentException = exceptions[n];
 						if (exception.isCompatibleWith(currentException)) {
 							TypeReference catchArgumentType = this.catchArguments[i].type;
 							if ((catchArgumentType.bits & ASTNode.IsUnionType) != 0) {
 								catchArgumentType = ((UnionTypeReference) catchArgumentType).typeReferences[j];
 							}
 							this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 								catchArgumentType,
 								exception,
 								currentException);
 							break loop;
 						}
 					}
 				}
 			}
 		}
 	} else {
 		this.caughtExceptionTypes = new ReferenceBinding[length];
 		for (int i = 0; i < length; i++) {
+			if (argumentTypes[i] instanceof ArrayBinding)
+				continue;
 			this.caughtExceptionTypes[i] = (ReferenceBinding) argumentTypes[i];
 			for (int j = 0; j < i; j++) {
 				if (this.caughtExceptionTypes[i].isCompatibleWith(argumentTypes[j])) {
 					this.scope.problemReporter().wrongSequenceOfExceptionTypesError(
 						this.catchArguments[i].type,
 						this.caughtExceptionTypes[i],
 						argumentTypes[j]);
 				}
 			}
 		}
 	}
 }",Buggy,"Fixed bug 433879 - ArrayBinding cannot be cast to ReferenceBinding

Signed-off-by: Shankha Banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a fix for a specific bug where an `ArrayBinding` was being incorrectly cast to a `ReferenceBinding`. Let's analyze the diff to see if the changes align with this description:

1. The code now includes checks to skip processing for types that are instances of `ArrayBinding`:
   - `if (argumentTypes[i] instanceof ArrayBinding) continue;`
   This change is present in two places within the method `verifyDuplicationAndOrder`.

2. The inclusion of these checks prevents the code from attempting to cast an `ArrayBinding` to a `ReferenceBinding`. This is crucial where `currentExceptionType` and `this.caughtExceptionTypes[i]` are cast directly from `argumentTypes[i]`, which would have previously thrown a `ClassCastException`.

3. These conditional checks effectively resolve the issue described in the commit message by preventing the erroneous cast from occurring.

Overall, the changes directly address the bug described in the commit message by ensuring that `ArrayBinding` instances are not processed in a way that leads to invalid type casting, which was likely resulting in exceptions being thrown during runtime.

Given this analysis, the modifications clearly aim to fix a bug related to class casting in the `verifyDuplicationAndOrder` method. Therefore, this commit can be concluded as **Buggy**."
eclipseJdt,6311.json,c36a6a2b662267e56067d121b7f34ae48cbcb692,"@@ -1,16 +1,18 @@
 void checkInheritedMethods(MethodBinding inheritedMethod, MethodBinding otherInheritedMethod) {
 
 	// the 2 inherited methods clash because of a parameterized type overrides a raw type
 	//		interface I { void foo(A a); }
 	//		class Y { void foo(A<String> a) {} }
 	//		abstract class X extends Y implements I { }
 	//		class A<T> {}
 	// in this case the 2 inherited methods clash because of type variables
 	//		interface I { <T, S> void foo(T t); }
 	//		class Y { <T> void foo(T t) {} }
 	//		abstract class X extends Y implements I {}
 
-	if (inheritedMethod.declaringClass.isInterface() || inheritedMethod.isStatic()) return;
+	if (inheritedMethod.isStatic()) return;
+	if (this.environment.globalOptions.complianceLevel < ClassFileConstants.JDK1_7 && inheritedMethod.declaringClass.isInterface())
+		return;  // JDK7 checks for name clashes in interface inheritance, while JDK6 and below don't. See https://bugs.eclipse.org/bugs/show_bug.cgi?id=354229
 
 	detectInheritedNameClash(inheritedMethod.original(), otherInheritedMethod.original());
 }",Buggy,"Fixed bug 354229: [compiler][1.7] Name clash error not being reported by
ecj.",Buggy,"The commit message indicates a fix for a name clash error in the Eclipse Compiler for Java (ECJ) that was not being reported correctly when dealing with interface inheritance under JDK 1.7 compliance rules. The code changes introduce a specific condition to handle this scenario by checking the compiler's compliance level.

Here's a breakdown of the changes:

1. **Original Condition:** The code initially returned early from the function if the `inheritedMethod` belongs to an interface or if it's static (`if (inheritedMethod.declaringClass.isInterface() || inheritedMethod.isStatic())`).

2. **Updated Condition:**
   - The check for whether the method is static remains unchanged (`if (inheritedMethod.isStatic()) return;`).
   - The interface check is modified by adding a condition that compares the compliance level of the environment (`if (this.environment.globalOptions.complianceLevel < ClassFileConstants.JDK1_7 && inheritedMethod.declaringClass.isInterface())`). This stops the early return if the compliance level is JDK 1.7 or higher, allowing the check for method name clashes in interfaces to proceed.

3. **Why This Indicates a Bug Fix:**
   - **Name Clash Detection:** The change specifically addresses a failure to detect name clashes when the code is compiled with compliance equal to or greater than JDK 1.7. The error lies in earlier conditions that did not account for this when interface inheritance was involved.
   - **Behavior Alignment:** The comment in the code clearly reflects that JDK 7 compliance introduced new rules for detecting name clashes in interface inheritance, which were not appropriately handled before this fix.
   - **Commit Message Reference:** The commit message directly points to bug 354229, which involves correcting the lack of error reporting in the compiler under specific conditions.

Therefore, the modifications explicitly address a real-world bug related to the compiler's behavior and name clash detection. The changes ensure that under appropriate compliance levels, the check for inherited method name clashes is correctly executed.

The conclusion from this analysis is **Buggy**."
eclipseJdt,4397.json,7089fcfd703b096f125599a900905c00e15535fa,"@@ -1,30 +1,36 @@
 	public void getAllAnnotationContexts(int targetType, int typeParameterIndex, List allAnnotationContexts) {
 		AnnotationCollector collector = new AnnotationCollector(this, targetType, typeParameterIndex, allAnnotationContexts);
 		if (this.annotations != null) {
 			int annotationsLength = this.annotations.length;
 			for (int i = 0; i < annotationsLength; i++)
 				this.annotations[i].traverse(collector, (BlockScope) null);
 		}
 		switch(collector.targetType) {
 			case AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.CLASS_TYPE_PARAMETER_BOUND;
 				break;
 			case AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER :
 				collector.targetType = AnnotationTargetTypeConstants.METHOD_TYPE_PARAMETER_BOUND;
 		}
-		if (this.type != null && ((this.type.bits & ASTNode.HasTypeAnnotations) != 0)) {
-			collector.info2 = 0;
-			this.type.traverse(collector, (BlockScope) null);
+		int boundIndex = 0;
+		if (this.type != null) {
+			// boundIndex 0 is always a class
+			if (this.type.resolvedType.isInterface())
+				boundIndex = 1;
+			if ((this.type.bits & ASTNode.HasTypeAnnotations) != 0) {
+				collector.info2 = boundIndex;
+				this.type.traverse(collector, (BlockScope) null);
+			}
 		}
 		if (this.bounds != null) {
 			int boundsLength = this.bounds.length;
 			for (int i = 0; i < boundsLength; i++) {
 				TypeReference bound = this.bounds[i];
 				if ((bound.bits & ASTNode.HasTypeAnnotations) == 0) {
 					continue;
 				}
-				collector.info2 = i + 1;
+				collector.info2 = ++boundIndex;
 				bound.traverse(collector, (BlockScope) null);
 			}
 		}
 	}",Buggy,"Fixed Bug 415543 - [1.8][compiler] Incorrect bound index in
RuntimeInvisibleTypeAnnotations attrribute

Signed-off-by: Andrew Clement <aclement@gopivotal.com>",Buggy,"The commit message specifies a fix for an incorrect bound index in the RuntimeInvisibleTypeAnnotations attribute, which is aligned with the changes made in the code.

Here's a detailed analysis of the diff:

1. **Introduction of `boundIndex`**: 
   - The code introduces a new `boundIndex` variable to keep track of the current index of bounds. This variable is necessary for correctly identifying the index for type annotations.

2. **Conditional Logic for Interface Type**:
   - The new logic checks if `this.type.resolvedType.isInterface()` and sets `boundIndex` to 1 if true. This adjustment aligns with the need to handle class and interface types differently when computing bound indices.

3. **Use of `boundIndex`**:
   - For the type, `collector.info2` is set to `boundIndex`, which accounts for whether the type is an interface.
   - For bounds, `collector.info2 = ++boundIndex` ensures the indexing is correct by incrementally updating `boundIndex` before use.

4. **Alignment with the Commit Message**:
   - The logical adjustments and introduction of `boundIndex` directly address the issue of incorrect indices for type annotations within bounds. 

Based on these observations, the changes correct the way bound indices are calculated and reported for annotations, which is precisely what the commit message describes. Therefore, this change does indeed constitute a bug fix.

Thus, the conclusion for this analysis is **Buggy**."
eclipseJdt,5447.json,3c8db8654fc8e2927c75863ec1232c9bc3800c9b,"@@ -1,31 +1,34 @@
 	public TypeBinding resolveType(BlockScope scope) {
 
 		if (this.compilerAnnotation != null)
 			return this.resolvedType;
 
 		this.constant = Constant.NotAConstant;
 
 		ReferenceBinding containerAnnotationType = (ReferenceBinding) this.resolvedType;
 		if (!containerAnnotationType.isValidBinding())
 			containerAnnotationType = (ReferenceBinding) containerAnnotationType.closestMatch();
 		Annotation repeatingAnnotation = this.containees[0];
 		ReferenceBinding repeatingAnnotationType = (ReferenceBinding) repeatingAnnotation.resolvedType;
+		if (!repeatingAnnotationType.isDeprecated() && isTypeUseDeprecated(containerAnnotationType, scope)) {
+			scope.problemReporter().deprecatedType(containerAnnotationType, repeatingAnnotation);
+		}
 		checkContainerAnnotationType(repeatingAnnotation, scope, containerAnnotationType, repeatingAnnotationType, true); // true => repeated *use* site error reporting requested.
 		this.resolvedType = containerAnnotationType = repeatingAnnotationType.containerAnnotationType();
 		if (!this.resolvedType.isValidBinding())
 			return this.resolvedType;
 		
 		// OK, the declaration site of the repeating annotation type as well as the use site where the annotations actually repeat pass muster. 
 		MethodBinding[] methods = containerAnnotationType.methods();
 		MemberValuePair pair = memberValuePairs()[0];
 		
 		for (int i = 0, length = methods.length; i < length; i++) {
 			MethodBinding method = methods[i];
 			if (CharOperation.equals(method.selector, TypeConstants.VALUE)) {
 				pair.binding = method;
 				pair.resolveTypeExpecting(scope, method.returnType);
 			}
 		}
 		this.compilerAnnotation = scope.environment().createAnnotation((ReferenceBinding) this.resolvedType, computeElementValuePairs());
 		return this.resolvedType;
 	}",Buggy,"Follow up fixes for Bug 412153 - [1.8][compiler] Check validity of
annotations which may be repeatable",Buggy,"The commit message refers to follow-up fixes related to the validity of annotations which may be repeatable, specifically for bug 412153. In the provided diff, the added lines introduce additional checks for deprecated types:

1. It checks whether the `repeatingAnnotationType` is deprecated using `repeatingAnnotationType.isDeprecated()`.
2. If the type is not deprecated, it further checks if the `containerAnnotationType` is deprecated in the context of type use with `isTypeUseDeprecated(containerAnnotationType, scope)`.
3. If the type use is deprecated, it reports a problem using `scope.problemReporter().deprecatedType(containerAnnotationType, repeatingAnnotation);`.

These changes are targeted towards improving the correctness of the type resolution by adding extra validation to ensure that deprecated annotations are properly reported. This aligns with checking the validity of annotations, particularly considering repeatable annotations, which is consistent with the stated purpose in the commit message.

Considering these updates focus on error-checking, ensuring compatibility with deprecated types, and aligning with mentioned bug-tracking efforts, it indicates a bug fix related to the annotation checker. Therefore, the conclusion is **Buggy**."
eclipseJdt,3852.json,d15e79b3439bbaf42d22da90dbcfabf7ceac0248,"@@ -1,3 +1,3 @@
 public boolean isCompatibleWith(TypeBinding left, Scope scope) {
-	throw new UnsupportedOperationException(""Unexpected control flow, should not have reached Expression.isCompatibleWith""); //$NON-NLS-1$
+	return this.resolvedType != null && this.resolvedType.isCompatibleWith(left,  scope);
 }",Buggy,"Fixed Bug 426315 - [1.8][compiler] UnsupportedOperationException with
conditional expression ",Buggy,"The commit message states a specific bug fix regarding an `UnsupportedOperationException` in a conditional expression. The diff replaces throwing an exception with a logical operation that checks if `this.resolvedType` is not null and compatible with the `left` type within the given scope. This change directly addresses the potential for an `UnsupportedOperationException` by providing a meaningful return value instead of abruptly stopping execution, which indicates a bug was fixed in the code's control flow.

This modification aligns with the commit message by resolving the specific error scenario described. Therefore, the changes represent a correction to the handling of type compatibility checks in certain conditional expressions, consistent with fixing a bug. 

The appropriate conclusion for this analysis is **Buggy**."
eclipseJdt,22376.json,883d74a76cfedfa8b3e490bff988420af3184ead,"@@ -1,23 +1,24 @@
 	private void pushOnCommentsStack(int start, int end) {
 
 		for (int i=start; i<=end; i++) {
+			if (this.scanner.commentPtr < i) break;
 			// First see if comment hasn't been already stored
 			int scannerStart = this.scanner.commentStarts[i]<0 ? -this.scanner.commentStarts[i] : this.scanner.commentStarts[i];
 			int commentStart = this.commentPtr == -1 ? -1 : (this.commentStarts[this.commentPtr]<0 ? -this.commentStarts[this.commentPtr] : this.commentStarts[this.commentPtr]);
 			if (commentStart == -1 ||  scannerStart > commentStart) {
 				int stackLength = this.commentStarts.length;
 				if (++this.commentPtr >= stackLength) {
 					System.arraycopy(
 						this.commentStarts, 0,
 						this.commentStarts = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 					System.arraycopy(
 						this.commentStops, 0,
 						this.commentStops = new int[stackLength + CommentIncrement], 0,
 						stackLength);
 				}
 				this.commentStarts[this.commentPtr] = this.scanner.commentStarts[i];
 				this.commentStops[this.commentPtr] = this.scanner.commentStops[i];
 			}
 		}
 	}",Buggy,"Fix for Bug 526996: CompilationUnit.setCommentTable(...) throws
IllegalStateException",Buggy,"The commit message addresses a bug occurring when an `IllegalStateException` is thrown in `CompilationUnit.setCommentTable(...)`. The code modification adds a new condition that checks if `this.scanner.commentPtr < i` and breaks the loop if true. This prevents accessing scanner comment data beyond the point initialized, which could lead to an illegal state. The change acts as a safeguard against incorrect array access, a common cause of exceptions such as `IllegalStateException`.

The inclusion of this check suggests that there was a scenario where the code attempted to access invalid indices in the comment array, hence causing the exception, thus confirming it as a bug fix.

Therefore, the code changes align with the intent stated in the commit message, and this is indeed a fix for the bug described. 

The conclusion is **Buggy**."
eclipseJdt,27085.json,1793b84adc002b2d6c63dad898df1777f10b28a0,"@@ -1,89 +1,89 @@
 	public boolean containsAnnotations() throws IOException {
 		State state = NORMAL;
 		
 		// for escaping quotes -- need to ignore the next single character
 		// Since this applies to all states it's handled separately
 		boolean seenBackslash = false;
 		
 		int c = getNext();
 		while (c != -1) {
 			
 			if (seenBackslash) {
 				// Skip one character
 				seenBackslash = false;
 			}
 			else if (c == '\\') {
 				// Skip the next character
 				seenBackslash = true;
 			}
 			else {
 				// Handle the character based on state
 				switch (state) {
 				
-				case (NORMAL) :
+				case NORMAL :
 					if (c == '@')
 						return true;
 					if (c == '/') {
 						state = SEEN_SLASH;
 					}
 					else if (c == '\'') {
 						state = IN_SINGLE_QUOTE;
 					}
 					else if (c == '\""') {
 						state = IN_DOUBLE_QUOTE;
 					}
 					break;
 					
-				case (SEEN_SLASH) :
+				case SEEN_SLASH :
 					if (c == '*') {
 						state = IN_COMMENT;
 					}
 					else if (c == '/') {
 						state = IN_SINGLE_LINE_COMMENT;
 					}
 					else {
 						state = NORMAL;
 					}
 					break;
 				
-				case (IN_COMMENT) :
+				case IN_COMMENT :
 					if (c == '*') {
 						state = IN_COMMENT_SEEN_STAR;
 					}
 					break;
 				
-				case (IN_COMMENT_SEEN_STAR) :
+				case IN_COMMENT_SEEN_STAR :
 					if (c == '/') {
 						state = NORMAL;
 					}
 					else {
 						state = IN_COMMENT;
 					}
 					break;
 					
-				case (IN_SINGLE_LINE_COMMENT) :
+				case IN_SINGLE_LINE_COMMENT :
 					if (c == '\n' || c == '\r') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_SINGLE_QUOTE) :
+				case IN_SINGLE_QUOTE :
 					if (c == '\'') {
 						state = NORMAL;
 					}
 					break;
 					
-				case (IN_DOUBLE_QUOTE) :
+				case IN_DOUBLE_QUOTE :
 					if (c == '\""') {
 						state = NORMAL;
 					}
 					break;
 					
 				default :
 					throw new IllegalStateException(""Unhandled state: "" + state);  //$NON-NLS-1$
 				}
 			}
 			c = getNext();
 		}
 		return false;
 	}",Buggy,"jgarms: fix syntax error in switch/case usage that is not caught by eclipse. This allows compiling by javac.
",Buggy,"The commit message states that it fixes a syntax error in switch/case usage that was not caught by Eclipse but needs to be corrected for compilation with javac. The code diff reflects this by removing parentheses around the case labels, which are incorrect syntax in Java and could prevent compilation with certain compilers like javac, even if some IDEs would allow them.

The change involves adjusting how switch case labels are written to match Java's syntax requirements. This type of error could indeed cause compilation issues, which aligns with the commit message claiming it resolves a syntax error problem.

Since this change corrects a syntax issue that could affect compilation, it indicates a bug fix. Therefore, the answer is **Buggy**."
eclipseJdt,6097.json,9a8f94b0b98db0b5d6b8eb4d27d568dc18fc1309,"@@ -1,18 +1,20 @@
 public ReferenceBinding anonymousOriginalSuperType() {
 	if (!isPrototype())
 		return ((LocalTypeBinding) this.prototype).anonymousOriginalSuperType();
+	if (this.superclass == null && this.scope != null)
+		return this.scope.getJavaLangObject();
 	
 	if (this.superInterfaces != Binding.NO_SUPERINTERFACES) {
 		return this.superInterfaces[0];
 	}
 	if ((this.tagBits & TagBits.HierarchyHasProblems) == 0) {
 		return this.superclass;
 	}
 	if (this.scope != null) {
 		TypeReference typeReference = this.scope.referenceContext.allocation.type;
 		if (typeReference != null) {
 			return (ReferenceBinding) typeReference.resolvedType;
 		}
 	}
 	return this.superclass; // default answer
 }",Buggy,"Fixed Bug 435348 - [1.8][compiler] NPE in JDT Core during AST creation

Signed-off-by: shankha banerjee <shankhba@in.ibm.com>",Buggy,"The commit message indicates a bug fix for a NullPointerException (NPE) in JDT Core during AST creation. The diff adds a check for `this.superclass` being `null` and subsequently ensures that `this.scope` is not `null` before calling `this.scope.getJavaLangObject()`. This change prevents a potential NPE when `this.superclass` is `null` while providing a fallback to `java.lang.Object`, which is a common base type. 

This modification aligns well with the tagged bug fix related to NPE, as it introduces a safeguard against a situation that would otherwise potentially lead to a null dereference.

Given these adjustments that directly address a null pointer issue, the change represents a bug fix. The answer is **Buggy**."
eclipseJdt,20759.json,c71a2dd3276b1c054fbb64a586af2db8d142bd3f,"@@ -1,61 +1,66 @@
 	protected void seekTypesInSourcePackage(
 			String name,
 			IPackageFragment pkg,
 			int firstDot,
 			boolean partialMatch,
 			String topLevelTypeName,
 			int acceptFlags,
 			IJavaElementRequestor requestor) {
 
 		long start = -1;
 		if (VERBOSE)
 			start = System.currentTimeMillis();
 		try {
 			if (!partialMatch) {
 				try {
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						String cuName = cu.getElementName();
 						int lastDot = cuName.lastIndexOf('.');
 						if (lastDot != topLevelTypeName.length() || !topLevelTypeName.regionMatches(0, cuName, 0, lastDot))
 							continue;
+
+						// https://bugs.eclipse.org/bugs/show_bug.cgi?id=351697
+						// If we are looking at source location, just ignore binary types
+						if (!(cu instanceof ICompilationUnit))
+							continue;
 						IType type = ((ICompilationUnit) cu).getType(topLevelTypeName);
 						type = getMemberType(type, name, firstDot);
 						if (acceptType(type, acceptFlags, true/*a source type*/)) { // accept type checks for existence
 							requestor.acceptType(type);
 							break;  // since an exact match was requested, no other matching type can exist
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			} else {
 				try {
 					String cuPrefix = firstDot == -1 ? name : name.substring(0, firstDot);
 					IJavaElement[] compilationUnits = pkg.getChildren();
 					for (int i = 0, length = compilationUnits.length; i < length; i++) {
 						if (requestor.isCanceled())
 							return;
 						IJavaElement cu = compilationUnits[i];
 						if (!cu.getElementName().toLowerCase().startsWith(cuPrefix))
 							continue;
 						try {
 							IType[] types = ((ICompilationUnit) cu).getTypes();
 							for (int j = 0, typeLength = types.length; j < typeLength; j++)
 								seekTypesInTopLevelType(name, firstDot, types[j], requestor, acceptFlags);
 						} catch (JavaModelException e) {
 							// cu doesn't exist -> ignore
 						}
 					}
 				} catch (JavaModelException e) {
 					// package doesn't exist -> ignore
 				}
 			}
 		} finally {
 			if (VERBOSE)
 				this.timeSpentInSeekTypesInSourcePackage += System.currentTimeMillis()-start;
 		}
 	}",Buggy,"master - Fix for bug 351697: ClassCastException while copying a .class file
to wrong source package
",Buggy,"The commit message indicates that the patch addresses a `ClassCastException` caused by copying a `.class` file to the wrong source package. The code change adds a check to verify that `cu` is an instance of `ICompilationUnit` before casting it. This prevents the `ClassCastException` when `cu` is not an `ICompilationUnit`, aligning with the commit message's intention to fix the bug. 

The change involves checking the type of the element and continues the loop if it is not an instance of `ICompilationUnit`. This effectively prevents any invalid type casting and handles the scenario with care, addressing the bug as described in the commit message.

Thus, after analyzing both the diff and the commit message, the changes do indicate a bug fix related to the prevention of a `ClassCastException`. The conclusion is **Buggy**."
eclipseJdt,17605.json,0aa34db9e617bf74b6393bf5508b95dcdaed80af,"@@ -1,10 +1,11 @@
 protected void consumeMethodHeaderName(boolean isAnnotationMethod) {
 	long selectorSourcePositions = this.identifierPositionStack[this.identifierPtr];
 	int selectorSourceEnd = (int) selectorSourcePositions;
 	int currentAstPtr = this.astPtr;
 	super.consumeMethodHeaderName(isAnnotationMethod);
 	if (this.astPtr > currentAstPtr) { // if ast node was pushed on the ast stack
 		this.sourceEnds.put(this.astStack[this.astPtr], selectorSourceEnd);
 		rememberCategories();
 	}
+	flushCommentsDefinedPriorTo(this.scanner.currentPosition);
 }",Buggy,"Fix for Bug 443942 Reconciler reports AST with wrong node range (with
comment after return type)",Buggy,"The commit message indicates a fix for a bug related to reconciliation reporting an AST with an incorrect node range. 

**Analyzing the code diff:**
1. A new line of code, `flushCommentsDefinedPriorTo(this.scanner.currentPosition);`, has been added at the end of the method `consumeMethodHeaderName`.
2. This new line addresses how comments are managed when constructing the Abstract Syntax Tree (AST) for a method header.
3. The added line ensures that comments positioned before the scanner's current position are flushed, potentially correcting how the AST is built and how node ranges are determined when comments are present after the return type.

**Assessing the relevance to the commit message:**
- The change directly relates to managing AST node positions, particularly when comments are involved, aligning with the commit message which mentions that the bug is about the AST having the wrong node range.
- This addition improves the handling of comments within the parsing process, which is likely key to ensuring the AST node ranges are accurate, particularly when reconciling the AST.

Given the evidence, the changes made in the diff are aimed at fixing the reported bug. Therefore, the conclusion is **Buggy**."
eclipseJdt,6321.json,93b912e3753e060958c1b01615e164e0662ca645,"@@ -1,10 +1,10 @@
 MethodBinding findReplacedMethod(MethodBinding specific, MethodBinding general) {
 	MethodBinding generalSubstitute = computeSubstituteMethod(general, specific);
 	if (generalSubstitute != null 
-			&& (!specific.isAbstract() || general.isAbstract())	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
+			&& (!specific.isAbstract() || general.isAbstract() || (general.isDefaultMethod() && specific.declaringClass.isClass()))	// if (abstract(specific) => abstract(general)) check if 'specific' overrides 'general' 
 			&& isSubstituteParameterSubsignature(specific, generalSubstitute)) 
 	{
 		return generalSubstitute;
 	} 
 	return null;
 }",Buggy,"Fixed Bug 426318 - [1.8][compiler] Bogus name clash error in the
presence of default methods and varargs",Buggy,"The commit message indicates fixing a bug related to a name clash error with default methods and varargs in a compiler. 

Let's analyze the changes:
1. The method `findReplacedMethod` checks if one method substitutes another.
2. A condition has been modified by adding: `|| (general.isDefaultMethod() && specific.declaringClass.isClass())`.
3. The original comment remains relevant: ""check if 'specific' overrides 'general'"".
4. The added condition checks for default methods when the 'specific' method's declaring class is a regular class (not an interface).

The bug fix seems to be addressing a scenario where default methods in interfaces may not have been correctly acknowledged, leading to erroneous name clashes in Java 1.8 environments. The message about a ""Bogus name clash error"" suggests that the logic determining method substitution was incomplete or incorrect when involving interface default methods.

Given that this adjustment addresses precisely the kind of bug described in the commit message—specifically, conditions that improperly generate clash errors by extending the substitution logic to consider Java 1.8's interface defaults—it's clear this is a correction to a faulty logic. Thus, the changes are rightly indicative of fixing a bug.

Therefore, the conclusion is **Buggy**."
commons-lang,3021.json,e582456625cc8a7056cc9354d2a75913f4ceb393,"@@ -1,46 +1,48 @@
     private void init() {
         thisYear= Calendar.getInstance(timeZone, locale).get(Calendar.YEAR);
         
         nameValues= new ConcurrentHashMap<Integer, KeyValue[]>();
         
         StringBuilder regex= new StringBuilder();
         List<Strategy> collector = new ArrayList<Strategy>();
         
         Matcher patternMatcher= formatPattern.matcher(pattern);
         if(!patternMatcher.lookingAt()) {
             throw new IllegalArgumentException(""Invalid pattern"");
         }
 
-        String localeName = locale.toString();
         // These locales don't use the Gregorian calendar
         // See http://docs.oracle.com/javase/6/docs/technotes/guides/intl/calendar.doc.html
-        if (localeName.equals(""ja_JP_JP"") || localeName.startsWith(""th_TH"")) {
+        // Also, the getEras() methods don't return the correct era names.
+        // N.B. Not safe to use toString() comparison because that changes between Java versions
+        if (locale.equals(JAPANESE_IMPERIAL)
+        || (locale.getLanguage().equals(""th"") && locale.getCountry().equals(""TH""))) {
             collector.add(new SimpleDateFormatStrategy());
             strategies= collector.toArray(new Strategy[collector.size()]);
             parsePattern= Pattern.compile(""(.*+)"");
             return;
         }
 
         currentFormatField= patternMatcher.group();
         Strategy currentStrategy= getStrategy(currentFormatField);
         for(;;) {
             patternMatcher.region(patternMatcher.end(), patternMatcher.regionEnd());
             if(!patternMatcher.lookingAt()) {
                 nextStrategy = null;
                 break;
             }
             String nextFormatField= patternMatcher.group();
             nextStrategy = getStrategy(nextFormatField);
             if(currentStrategy.addRegex(this, regex)) {
                 collector.add(currentStrategy);                
             }
             currentFormatField= nextFormatField;
             currentStrategy= nextStrategy;
         }
         if(currentStrategy.addRegex(this, regex)) {
             collector.add(currentStrategy);                
         }
         currentFormatField= null;
         strategies= collector.toArray(new Strategy[collector.size()]);
         parsePattern= Pattern.compile(regex.toString());
     }",Buggy,"LANG-828 FastDateParser does not handle non-Gregorian calendars properly
Fix bug in Java 7 (Locale.toString() format has changed)

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1390189 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for handling non-Gregorian calendars and an issue with Java 7's `Locale.toString()` format. The code change reflects this by replacing the comparison of `locale.toString()` with direct comparison of `Locale` objects using equals() for more consistency across Java versions. This aligns with the bug description about `Locale.toString()` changes leading to potential misbehavior in pattern recognition for non-Gregorian locales.

Here’s a step-by-step breakdown:

1. **Locale Handling Change**: The locale comparison is changed from `localeName.equals(...)` to `locale.equals(...)`. The reason behind this is explained in the comments: `Locale.toString()` can change across Java versions. By using `Locale` object equality directly, the code remains robust against such changes.

2. **Non-Gregorian Calendar Handling**: The condition checks for specific locales ('ja_JP_JP' and 'th_TH') that do not use the Gregorian calendar. These checks ensure the appropriate date strategies are used for these locales.

3. **Logic and Bug Fix**: The updated code maintains the intent of the original logic but ensures the robustness and correctness of handling patterns for non-Gregorian calendars across Java versions. This change, therefore, addresses the potential issues with `Locale.toString()` formatting and its impact on calendar logic, fulfilling the commit message’s claim.

Given these points, the changes do indeed represent a bug fix as indicated by dealing with pattern matching based on locales potentially affected by changes in Java's `Locale.toString()`. The commit message and code modification are aligned, indicating the changes are intended to correct misbehavior regarding calendar handling.

The conclusion is **Buggy**."
commons-lang,734.json,fbb0f7f88c84001e0a92dae6a71b7e43bda65a56,"@@ -1,54 +1,58 @@
     public static String wrap(final String str, int wrapLength, String newLineStr, final boolean wrapLongWords) {
         if (str == null) {
             return null;
         }
         if (newLineStr == null) {
             newLineStr = SystemUtils.LINE_SEPARATOR;
         }
         if (wrapLength < 1) {
             wrapLength = 1;
         }
         final int inputLineLength = str.length();
         int offset = 0;
         final StringBuilder wrappedLine = new StringBuilder(inputLineLength + 32);
         
-        while (inputLineLength - offset > wrapLength) {
+        while (offset < inputLineLength) {
             if (str.charAt(offset) == ' ') {
                 offset++;
                 continue;
             }
+            // only last line without leading spaces is left
+            if(inputLineLength - offset <= wrapLength) {
+                break;
+            }
             int spaceToWrapAt = str.lastIndexOf(' ', wrapLength + offset);
 
             if (spaceToWrapAt >= offset) {
                 // normal case
                 wrappedLine.append(str.substring(offset, spaceToWrapAt));
                 wrappedLine.append(newLineStr);
                 offset = spaceToWrapAt + 1;
                 
             } else {
                 // really long word or URL
                 if (wrapLongWords) {
                     // wrap really long word one line at a time
                     wrappedLine.append(str.substring(offset, wrapLength + offset));
                     wrappedLine.append(newLineStr);
                     offset += wrapLength;
                 } else {
                     // do not wrap really long word, just extend beyond limit
                     spaceToWrapAt = str.indexOf(' ', wrapLength + offset);
                     if (spaceToWrapAt >= 0) {
                         wrappedLine.append(str.substring(offset, spaceToWrapAt));
                         wrappedLine.append(newLineStr);
                         offset = spaceToWrapAt + 1;
                     } else {
                         wrappedLine.append(str.substring(offset));
                         offset = inputLineLength;
                     }
                 }
             }
         }
 
         // Whatever is left in line is short enough to just pass through
         wrappedLine.append(str.substring(offset));
 
         return wrappedLine.toString();
     }",Buggy,"LANG-995: Fix bug with stripping spaces on last line in WordUtils.wrap(). This fixes #18 from github. Thanks to Andrey Khobnya

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1586649 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to how spaces are stripped on the last line of text when using `WordUtils.wrap()`. The modifications in the diff appear to address this issue:

1. **While Loop Condition Change:**
   - The original condition for the loop `inputLineLength - offset > wrapLength` ensures processing only if there are more characters than `wrapLength`. The updated `while (offset < inputLineLength)` ensures all characters are processed, including trailing ones.

2. **Handling Trailing Spaces:**
   - The added check `if(inputLineLength - offset <= wrapLength)` ensures that if the remaining characters are short enough (equal to or less than `wrapLength`), it breaks the loop, preventing unnecessary new lines from being added simply because spaces were initially present.

These changes specifically aim to ensure that trailing spaces don't affect the application of the new line (`newLineStr`) at the end of the text wrapping, thus addressing the issue described in the commit message. Given this context, the changes align well with fixing the specific bug referenced.

Based on these observations, the modifications correct the behavior concerning the handling of trailing spaces, as indicated in the commit message. Therefore, the conclusion is **Buggy**."
commons-lang,1056.json,cd9922ef8b2c57ed6aa1390f89f94c1c40cdc67c,"@@ -1,6 +1,6 @@
     protected void setArrayEnd(String arrayEnd) {
-        if (arrayStart == null) {
-            arrayStart = """";
+        if (arrayEnd == null) {
+            arrayEnd = """";
         }
         this.arrayEnd = arrayEnd;
     }",Buggy,"ToStringStyle setArrayEnd handled null incorrectly
bug 31933, fix from Masato Tezuka


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@138011 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message highlights a bug concerning how `setArrayEnd` handles `null` inputs. The code previously incorrectly checked and initialized `arrayStart`. The fix corrects this logic by verifying and initializing `arrayEnd` accurately. The change directly addresses the described bug in the handling of `null` values for `arrayEnd`. Thus, the modifications are consistent with the commit message. The answer is **Buggy**."
commons-lang,816.json,2d17989e282e67c9c37153adc4074082d3a38b1c,"@@ -1,7 +1,7 @@
     public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
-        comparison = superHashCode;
+        comparison = superCompareTo;
         return this;
     }",Buggy,"Fix stupid compile error....


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137489 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change corrects an assignment error, replacing `superHashCode` with `superCompareTo` in the `appendSuper` method. This aligns with the commit message's intent to fix a compile error. Likely, the original code caused a compile-time error because `superHashCode` was not defined or was the wrong variable. By assigning a method parameter `superCompareTo` to `comparison`, the change likely corrects the compilation issue. Therefore, this indicates a bug fix, and the conclusion is **Buggy**."
commons-lang,816.json,dbe97c09cad3ba2c7473c15778773fa4d071e7de,"@@ -1,7 +1,7 @@
-    public CompareToBuilder appendSuper(int superHashCode) {
+    public CompareToBuilder appendSuper(int superCompareTo) {
         if (comparison != 0) {
             return this;
         }
         comparison = superHashCode;
         return this;
     }",Buggy,"Javadoc fixes
bug 21758, from Pete Gieser


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137487 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests this update is to fix Javadocs, but the code change replaces a parameter name in the method signature. This implies an intention to clarify method logic or parameters due to misunderstanding in usage rather than fixing a bug in functionality. No error-handling, logical corrections, or exception-handling improvements are visible. Instead, this change appears to address a naming inconsistency or clarify intent, likely motivated by the associated bug report.

Additionally, the commit is titled with ""Javadoc fixes"", indicating an emphasis on documentation rather than functionality changes. The bug ID reference (bug 21758) suggests it addresses a specific issue, but the change itself does not involve correcting an error in logic or behavior; it is more of a code readability or API misunderstanding correction.

Therefore, based on the provided information and diff, the change does not signify fixing a functional bug in the code logic, and it more closely relates to improvement in naming consistency or potential documentation alignment. Thus, the conclusion is **NotBuggy**."
commons-lang,183.json,8cafd87c83f4197f7e8d14de8cba453c844266c0,"@@ -1,20 +1,20 @@
     public static List<Method> getMethodsListWithAnnotation(final Class<?> cls,
                                                             final Class<? extends Annotation> annotationCls,
                                                             boolean searchSupers, boolean ignoreAccess) {
 
         Validate.isTrue(cls != null, ""The class must not be null"");
         Validate.isTrue(annotationCls != null, ""The annotation class must not be null"");
         List<Class<?>> classes = (searchSupers ? getAllSuperclassesAndInterfaces(cls)
                 : new ArrayList<Class<?>>());
-        classes.add(cls);
+        classes.add(0, cls);
         final List<Method> annotatedMethods = new ArrayList<>();
         for (Class<?> acls : classes) {
             final Method[] methods = (ignoreAccess ? acls.getDeclaredMethods() : acls.getMethods());
             for (final Method method : methods) {
                 if (method.getAnnotation(annotationCls) != null) {
                     annotatedMethods.add(method);
                 }
             }
         }
         return annotatedMethods;
     }",Buggy,"LANG-1317: Add MethodUtils#findAnnotation and extend MethodUtils#getMethodsWithAnnotation for non-public, super-class and interface methods

fix bug introduced by last commit
",Buggy,"The commit message states a bug fix was made in the `MethodUtils#getMethodsWithAnnotation` method. The diff shows a change in how the `cls` is added to the `classes` list. Originally, `cls` was added at the end of the list, but now it is added at the beginning using `classes.add(0, cls);`. This change affects the order in which classes are searched for methods with the specified annotation.

Previously, the superclass and interface methods may have been considered before the methods in the class itself due to the order of the list. This could lead to unexpected behavior if method annotations needed to be found in `cls` before checking its superclasses and interfaces. By adjusting the order, the method ensures that the annotations on `cls` are checked first, which is a relevant fix aligning with the stated bug fix in the commit message.

This change is related to handling method searches correctly by addressing the traversal order in the search logic, which is crucial for user expectations when searching for annotated methods in a class and its hierarchy. Because this adjustment precisely matches the commit message's intent to fix a bug, the answer is **Buggy**."
commons-lang,2681.json,82c5dada6f7b369b69c383aa995f45578325e139,"@@ -1,15 +1,15 @@
-    private static void removeCommonFrames(List causeFrames, List wrapperFrames) {
+    public static void removeCommonFrames(List causeFrames, List wrapperFrames) {
         int causeFrameIndex = causeFrames.size() - 1;
         int wrapperFrameIndex = wrapperFrames.size() - 1;
         while (causeFrameIndex >= 0 && wrapperFrameIndex >= 0) {
             // Remove the frame from the cause trace if it is the same
             // as in the wrapper trace
             String causeFrame = (String) causeFrames.get(causeFrameIndex);
             String wrapperFrame = (String) wrapperFrames.get(wrapperFrameIndex);
             if (causeFrame.equals(wrapperFrame)) {
                 causeFrames.remove(causeFrameIndex);
             }
             causeFrameIndex--;
             wrapperFrameIndex--;
         }
     }",Buggy,"Bug #14357 fixed. Mohan's patch makes removeCommonFrames public, and adds an
isThrowableNested to ExceptionUtils.

It adds static attributes to decide if the stack trace should be topDown
and if the stack traces should be trimmed on repeat. If running 1.4 or higher,
it uses the default stack trace, and the functionality of NestableError,
NestableException and NestableRuntimeException getMessage()s all change.
Accompanying these changes are numerous tests.

Submitted by:	Mohan Kishore


git-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/proper/lang/trunk@137314 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"This commit describes changes and features unrelated to error handling, like making `removeCommonFrames` public and setting static attributes affecting stack trace behavior. While bug #14357 is mentioned in the message, the code change shown simply modifies access level, not addressing error correction. Though it indicates broader Error/Exception handling adjustments, this specific change doesn't fix a bug. Therefore, it's **NotBuggy**."
commons-lang,2064.json,f1ba9c252d99b3adf2bb3750339b895b2c097bd2,"@@ -1,16 +1,17 @@
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
             String name = desc.getName();
             try {
                 return Class.forName(name, false, classLoader);
             } catch (ClassNotFoundException ex) {
                 try {
                     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
                 } catch (ClassNotFoundException cnfe) {
                     Class<?> cls = primitiveTypes.get(name);
-                    if (cls != null)
+                    if (cls != null) {
                         return cls;
-                    else
+                    } else {
                         throw cnfe;
+                    }
                 }
             }
         }",Buggy,"Applying Benedikt Ritter's patch to fix the Checkstyle error in SerializationUtils - LANG-793

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1301321 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that the changes address a Checkstyle error. The changes are purely stylistic, such as adding braces to if-else statements that previously lacked them. This ensures better readability and adherence to coding standards but does not correct any program logic, behavior, or bug.

The code modifications do not go beyond these stylistic improvements to address any functional issues. Specifically, they do not alter how exceptions are handled, fix any kind of error handling logic, or correct any logical flaws. Therefore, these changes primarily focus on compliance with coding style guidelines and do not represent a bug fix in the logic or functionality of the code.

Given this, the changes do not align with a bug fix but rather with a style correction. The appropriate conclusion for the nature of this commit is **NotBuggy**."
elasticsearch,60742.json,f2411253026503954da172ddd0b1bf77789dd476,"@@ -1,3 +1,3 @@
     public boolean snapshot() {
-        return snapshot != null && snapshot;
+        return snapshot;
     }",Buggy,"Internal: Change snapshot state for unreleased versions and add
validation tests for constants

Currently the snapshot flag for Version constants is only set to true
for CURRENT.  However, this means that the snapshot state changes from
branch to branch.  Instead, snapshot should be ""is this version
released?"".  This change also adds a validation test checking that
ID -> constant and vice versa are correct, and fixes one bug found there
(for an unreleased version).
",Buggy,"The commit message describes changing how snapshot status is managed for version constants, aiming to reflect whether a version has been released. It mentions adding validation tests to check the correctness of ID-constant mapping and fixing a bug found during testing.

The code change simplifies the `snapshot()` method by returning the `snapshot` variable directly, instead of checking if it is not null. This implies that `snapshot` is now expected to be either `true` or `false` by default, with no need to handle a `null` case, which might have indicated an uninitialized state leading to inconsistent behavior.

Moreover, the message mentions that this change should address a bug found in an unreleased version, implying that the alteration is part of correcting a logic flaw regarding the `snapshot` management.

Given the provided context and the description in the commit message, which indicates that the modification ensures a more consistent and correct snapshot management, it aligns with the notion of fixing an issue with how `snapshot` was previously handled.

Thus, the change can be concluded as a part of a bug fix. Therefore, the answer is **Buggy**."
elasticsearch,52357.json,1a915446037ab83ae292df8733e7ba7d63bb667e,"@@ -1,15 +1,16 @@
     public RestStatus status() {
         if (shardFailures.length == 0) {
-            // if no successful shards, it means no active shards, so just return SERVICE_UNAVAILABLE
-            return RestStatus.SERVICE_UNAVAILABLE;
+            // if no successful shards, the failure can be due to EsRejectedExecutionException during fetch phase
+            // on coordinator node. so get the status from cause instead of returning SERVICE_UNAVAILABLE blindly
+            return getCause() == null ? RestStatus.SERVICE_UNAVAILABLE : ExceptionsHelper.status(getCause());
         }
         RestStatus status = shardFailures[0].status();
         if (shardFailures.length > 1) {
             for (int i = 1; i < shardFailures.length; i++) {
                 if (shardFailures[i].status().getStatus() >= 500) {
                     status = shardFailures[i].status();
                 }
             }
         }
         return status;
     }",Buggy,"Fixing 503 Service Unavailable errors during fetch phase  (#39086)

When ESRejectedExecutionException gets thrown on the coordinating node while trying to fetch hits, the resulting exception will hold no shard failures, hence `503` is used as the response status code. In that case, `429` should be returned instead. Also, the status code should be taken from the cause if available whenever there are no shard failures instead of blindly returning `503` like we currently do.

Closes #38586",Buggy,"The changes in the provided diff align well with the commit message, which mentions fixing 503 Service Unavailable errors by returning a more appropriate status code of 429 (Too Many Requests) when certain conditions are met. The commit focuses on improving the error-handling logic concerning status codes returned during the fetch phase in Elasticsearch.

The modifications made in the code involve:

1. **Error-Handling Update**: The logic checks if there are no successful shards. Initially, the code returned a generic 503 (SERVICE_UNAVAILABLE) status code. The updated logic now attempts to retrieve the status code from the cause of the exception (`getCause`) before defaulting to 503. This directly addresses the incorrect status code being returned as described in the commit message.

2. **Use of `ExceptionsHelper.status(getCause())`**: This suggests an intention to provide a more accurate response based on what might have caused the failure, specifically when requests are rejected due to execution limits, which aligns with the description of returning a 429 status.

These changes improve the precision of the API's error reporting, fulfilling the commit's promise by providing correct HTTP status codes based on the underlying issue.

Thus, the changes indicate a fix of a logical issue with status code handling, directly addressing the bug related to improperly returned HTTP 503 errors. Hence, this analysis leads to the conclusion **Buggy**."
elasticsearch,47411.json,f0f2b261595e2f5ea48736eac7a4e4121dd20b2c,"@@ -1,21 +1,21 @@
     public static int getRoutingFactor(int sourceNumberOfShards, int targetNumberOfShards) {
         final int factor;
         if (sourceNumberOfShards < targetNumberOfShards) { // split
             factor = targetNumberOfShards / sourceNumberOfShards;
             if (factor * sourceNumberOfShards != targetNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""factor of [""
                     + targetNumberOfShards + ""]"");
             }
         } else if (sourceNumberOfShards > targetNumberOfShards) { // shrink
             factor = sourceNumberOfShards / targetNumberOfShards;
             if (factor * targetNumberOfShards != sourceNumberOfShards || factor <= 1) {
-                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a must be a "" +
+                throw new IllegalArgumentException(""the number of source shards ["" + sourceNumberOfShards + ""] must be a "" +
                     ""multiple of [""
                     + targetNumberOfShards + ""]"");
             }
         } else {
             factor = 1;
         }
         return factor;
     }",Buggy,"Fix duplicate phrase in shrink/split error message (#36734)

This commit removes a duplicate ""must be a"" from the shrink/split error
messages.",NotBuggy,"The commit message and code changes indicate that the modification involves fixing duplicate words in error messages related to shard operations. The change removes the redundant ""must be a"" from the error messages, improving clarity but not altering functionality or logic.

Here's the reasoning:

1. **Commit Message Analysis**: The message indicates a correction of a duplicate phrase in error messages, making it clear that the change is cosmetic and focused on readability.

2. **Code Diff Analysis**:
   - The existing error messages ""must be a must be a"" are corrected to ""must be a"".
   - The logic for the error condition checks and the exception throwing itself remains unchanged.

3. **Conclusion**: The change is aligned with the commit message and pertains purely to correcting a textual mistake in error messages, not modifying any algorithmic logic or bug-prone code behavior. This does not constitute a bug fix in functional terms but rather an improvement in error message clarity.

Thus, based on the nature of the modification, the conclusion is **NotBuggy**."
elasticsearch,8045.json,92f24c967ae2ad22a728ff43de231cf351b92fd2,"@@ -1,39 +1,39 @@
         protected MultiSearchResponse shardOperation(Request request, ShardId shardId) throws IOException {
             final IndexService indexService = indicesService.indexService(shardId.getIndex());
             final IndexShard indexShard = indicesService.getShardOrNull(shardId);
             try (Engine.Searcher searcher = indexShard.acquireSearcher(""enrich_msearch"")) {
                 final FieldsVisitor visitor = new FieldsVisitor(true);
                 final QueryShardContext context = indexService.newQueryShardContext(shardId.id(),
-                    searcher.getIndexReader(), () -> {throw new UnsupportedOperationException();}, null);
+                    searcher, () -> {throw new UnsupportedOperationException();}, null);
                 final MapperService mapperService = context.getMapperService();
                 final Text typeText = mapperService.documentMapper().typeText();
 
                 final MultiSearchResponse.Item[] items = new MultiSearchResponse.Item[request.multiSearchRequest.requests().size()];
                 for (int i = 0; i < request.multiSearchRequest.requests().size(); i++) {
                     final SearchSourceBuilder searchSourceBuilder = request.multiSearchRequest.requests().get(i).source();
 
                     final QueryBuilder queryBuilder = searchSourceBuilder.query();
                     final int from = searchSourceBuilder.from();
                     final int size = searchSourceBuilder.size();
                     final FetchSourceContext fetchSourceContext = searchSourceBuilder.fetchSource();
 
                     final Query luceneQuery = queryBuilder.rewrite(context).toQuery(context);
                     final int n = from + size;
                     final TopDocs topDocs = searcher.search(luceneQuery, n, new Sort(SortField.FIELD_DOC));
 
                     final SearchHit[] hits = new SearchHit[topDocs.scoreDocs.length];
                     for (int j = 0; j < topDocs.scoreDocs.length; j++) {
                         final ScoreDoc scoreDoc = topDocs.scoreDocs[j];
 
                         visitor.reset();
                         searcher.doc(scoreDoc.doc, visitor);
                         visitor.postProcess(mapperService);
                         final SearchHit hit = new SearchHit(scoreDoc.doc, visitor.uid().id(), typeText, Map.of());
                         hit.sourceRef(filterSource(fetchSourceContext, visitor.source()));
                         hits[j] = hit;
                     }
                     items[i] = new MultiSearchResponse.Item(createSearchResponse(topDocs, hits), null);
                 }
                 return new MultiSearchResponse(items, 1L);
             }
         }",Buggy,"fixed compile errors after merging
",Buggy,"The commit message indicates that the update aims to fix compile errors following a merge. The code change involves modifying an argument passed to a method: the `searcher.getIndexReader()` is replaced with `searcher` as an argument to `indexService.newQueryShardContext`. This suggests that the merging process might have resulted in a signature mismatch or incorrect type for the `newQueryShardContext` method.

Reasons indicating this is a bug fix:
1. The error was likely due to type expectations mismatch after a merge, which `searcher.getIndexReader()` (likely expecting a different type) caused.
2. Using `searcher` directly might align with updated method expectations, resulting in successful compilation.
3. The change is localized and addresses a specific issue (compilation), aligning with the commit message context.

These aspects imply the change indeed targets compile errors, a common post-merge complication, categorizing it as a bug fix. Given these considerations, the assessment is **Buggy**."
elasticsearch,32648.json,fa3d365ee83a089d461eacf66f2118f68dc1a8ed,"@@ -1,19 +1,25 @@
     public BytesReference slice(int from, int length) {
+        FutureObjects.checkFromIndexSize(from, length, this.length);
+
+        if (length == 0) {
+            return BytesArray.EMPTY;
+        }
+
         // for slices we only need to find the start and the end reference
         // adjust them and pass on the references in between as they are fully contained
         final int to = from + length;
-        final int limit = getOffsetIndex(from + length);
+        final int limit = getOffsetIndex(to - 1);
         final int start = getOffsetIndex(from);
         final BytesReference[] inSlice = new BytesReference[1 + (limit - start)];
         for (int i = 0, j = start; i < inSlice.length; i++) {
             inSlice[i] = references[j++];
         }
         int inSliceOffset = from - offsets[start];
         if (inSlice.length == 1) {
             return inSlice[0].slice(inSliceOffset, length);
         }
         // now adjust slices in front and at the end
         inSlice[0] = inSlice[0].slice(inSliceOffset, inSlice[0].length() - inSliceOffset);
         inSlice[inSlice.length-1] = inSlice[inSlice.length-1].slice(0, to - offsets[limit]);
         return new CompositeBytesReference(inSlice);
     }",Buggy,"Fix CompositeBytesReference#slice to not throw AIOOBE with legal offsets. (#35955)

CompositeBytesReference#slice has two bugs:
 - One that makes it fail if the reference is empty and an empty slice is
   created, this is #35950 and is fixed by special-casing empty-slices.
 - One performance bug that makes it always create a composite slice when
   creating a slice that ends on a boundary, this is fixed by computing `limit`
   as the index of the sub reference that holds the last element rather than
   the next element after the slice.

Closes #35950
",Buggy,"The commit message outlines two specific bugs in the `CompositeBytesReference#slice` method: 

1. **Array Index Out of Bounds Exception (AIOOBE) with Empty Slices:**
   - The diff shows the addition of a check for slices with a length of zero. If the length is zero, the method now returns `BytesArray.EMPTY`, which addresses the bug where slicing an empty reference could lead to an exception. This modification directly targets the first mentioned bug in the commit message.

2. **Improved Slice Handling on Boundaries:**
   - The change in calculation of the `limit` from `getOffsetIndex(from + length)` to `getOffsetIndex(to - 1)` ensures the correct sub-reference is used, addressing the performance issue for slices ending on boundaries. This aligns with the second bug fix mentioned in the commit message.

Both code changes (the empty slice handling and the adjustment for boundaries) correspond directly to the issues outlined in the commit message, suggesting that these are genuine bug fixes.

Given these observations, it's clear that the changes align well with the commit message's bug fixes. Therefore, the changes should be classified as **Buggy**."
elasticsearch,53519.json,68c82cd113e0a79916ea9fce6546139de7753731,"@@ -1,15 +1,14 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
-        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"Revert ""fix bug in bulk replication for noop update operation""

This reverts commit 42bc2d15bedb0f3b457bbfe06247311b9667b7c9.
",Buggy,"The commit message states this change is a revert of a previous bug fix. The diff shows the removal of an assertion meant to ensure that a request is not null. This assertion might have been added in a commit aimed at fixing a bug relating to null request handling, and its removal indicates a rollback to an earlier state of the code, potentially reintroducing the original bug. Since the commit message explicitly refers to reverting a bug fix, the proper conclusion for this change would be **Buggy**, acknowledging the presence of a bug fix that is being undone."
elasticsearch,53519.json,42bc2d15bedb0f3b457bbfe06247311b9667b7c9,"@@ -1,14 +1,15 @@
     public static void writeDocumentRequest(StreamOutput out, DocumentWriteRequest request)  throws IOException {
+        assert request != null : ""request must not be null"";
         if (request instanceof IndexRequest) {
             out.writeByte((byte) 0);
         } else if (request instanceof DeleteRequest) {
             out.writeByte((byte) 1);
         } else if (request instanceof UpdateRequest) {
             out.writeByte((byte) 2);
         } else if (request instanceof UpdateReplicaRequest) {
             out.writeByte((byte) 3);
         } else {
             throw new IllegalStateException(""invalid request ["" + request.getClass().getSimpleName() + "" ]"");
         }
         request.writeTo(out);
     }",Buggy,"fix bug in bulk replication for noop update operation
",Buggy,"The commit message indicates a bug fix related to ""bulk replication for noop update operation."" The code change introduces an assertion to ensure the request is not null before proceeding with further operations. While assertions are not executed by default in Java runtime, they are used during testing and debugging to catch issues. The check aligns with preventing null-related errors during document request write operations. The context of a bug fix fits in this case because not checking for null could lead to unexpected states or crashes.

Overall, the change shows an attempt to enforce non-null requests early in the method, providing a safeguard against a potential issue. Considering the message context and the assertion, it's likely a preventive measure addressing a bug.

Thus, I conclude with **Buggy**."
elasticsearch,9986.json,7e12d5a7958eb9f62fdb073863110dbd09b79747,"@@ -1,48 +1,48 @@
     private void buildUserFromClaims(JWTClaimsSet claims, ActionListener<AuthenticationResult> authResultListener) {
         final String principal = principalAttribute.getClaimValue(claims);
         if (Strings.isNullOrEmpty(principal)) {
             authResultListener.onResponse(AuthenticationResult.unsuccessful(
                 principalAttribute + ""not found in "" + claims.toJSONObject(), null));
             return;
         }
 
         final Map<String, Object> tokenMetadata = new HashMap<>();
         tokenMetadata.put(""id_token_hint"", claims.getClaim(""id_token_hint""));
         ActionListener<AuthenticationResult> wrappedAuthResultListener = ActionListener.wrap(auth -> {
             if (auth.isAuthenticated()) {
                 // Add the ID Token as metadata on the authentication, so that it can be used for logout requests
                 Map<String, Object> metadata = new HashMap<>(auth.getMetadata());
                 metadata.put(CONTEXT_TOKEN_DATA, tokenMetadata);
                 auth = AuthenticationResult.success(auth.getUser(), metadata);
             }
             authResultListener.onResponse(auth);
         }, authResultListener::onFailure);
 
         if (delegatedRealms.hasDelegation()) {
             delegatedRealms.resolve(principal, wrappedAuthResultListener);
             return;
         }
 
         final Map<String, Object> userMetadata;
         if (populateUserMetadata) {
             userMetadata = claims.getClaims().entrySet().stream().filter(entry -> {
                 /*
                  * We whitelist the Types that we want to parse as metadata from the Claims, explicitly filtering out {@link Date}s
                  */
                 Object v = entry.getValue();
-                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collections);
+                return (v instanceof String || v instanceof Boolean || v instanceof Number || v instanceof Collection);
             }).collect(Collectors.toUnmodifiableMap(entry -> ""oidc("" + entry.getKey() + "")"", Map.Entry::getValue));
         } else {
             userMetadata = Map.of();
         }
         final List<String> groups = groupsAttribute.getClaimValues(claims);
         final String dn = dnAttribute.getClaimValue(claims);
         final String mail = mailAttribute.getClaimValue(claims);
         final String name = nameAttribute.getClaimValue(claims);
         UserRoleMapper.UserData userData = new UserRoleMapper.UserData(principal, dn, groups, userMetadata, config);
         roleMapper.resolveRoles(userData, ActionListener.wrap(roles -> {
             final User user = new User(principal, roles.toArray(Strings.EMPTY_ARRAY), name, mail, userMetadata, true);
             wrappedAuthResultListener.onResponse(AuthenticationResult.success(user));
         }, wrappedAuthResultListener::onFailure));
 
     }",Buggy,"Populate OpenIDConnect metadata collections (#50521)

The OpenIdConnectRealm had a bug which would cause it not to populate
User metadata for collections contained in the user JWT claims.

This commit fixes that bug.

Resolves: #50250",Buggy,"The commit message indicates a bug fix related to populating user metadata from JWT claims in the OpenIdConnectRealm. 

The code changes reflect this by adjusting the filtering logic that determines what types of claim entries are considered for metadata. In particular, the change corrects the type check from `Collections` to `Collection`, which is an interface implemented by Java collection classes (lists, sets, etc.). The original code used `Collections`, likely intended to check for collection memberships, but incorrectly referenced `Collections`, which refers to a utility class and not an interface for handling collections like lists or sets.

The correction ensures that collection types are appropriately recognized for inclusion in user metadata, addressing the bug where collections contained in user JWT claims were not being populated. This aligns perfectly with the commit message that describes fixing a bug affecting user metadata handling.

Therefore, the changes accurately reflect the resolution of the bug as described in the commit message. The assessment is **Buggy**."
elasticsearch,31859.json,cea2d21c50c5a680cbbaefba254866601b5b4608,"@@ -1,20 +1,20 @@
     private int findPlaceholderEndIndex(CharSequence buf, int startIndex) {
         int index = startIndex + this.placeholderPrefix.length();
         int withinNestedPlaceholder = 0;
         while (index < buf.length()) {
             if (Strings.substringMatch(buf, index, this.placeholderSuffix)) {
                 if (withinNestedPlaceholder > 0) {
                     withinNestedPlaceholder--;
-                    index = index + this.placeholderPrefix.length() - 1;
+                    index = index + this.placeholderSuffix.length();
                 } else {
                     return index;
                 }
             } else if (Strings.substringMatch(buf, index, this.placeholderPrefix)) {
                 withinNestedPlaceholder++;
                 index = index + this.placeholderPrefix.length();
             } else {
                 index++;
             }
         }
         return -1;
     }",Buggy,"Fix bug in PropertyPlaceholder and add unit tests.

Close #6034
",Buggy,"The code change addresses a logical flaw in handling nested placeholders, where indices were incorrectly adjusted after finding a suffix within another placeholder. Originally, the index was decremented by the length of the prefix, leading to logically flawed index traversal. The fix correctly uses the suffix length to move the index forward after decrementing the nested count. This correction directly aligns with fixing a bug in the placeholder management logic.

The commit message indicates that this is part of a bug fix for the `PropertyPlaceholder` class. The changes made in the code reflect logical correction to ensure proper navigation within the text buffer for placeholders. Based on these observations:

**Buggy**"
elasticsearch,52107.json,267cd65506ab22ceb7606243ef58979bde5dbfe5,"@@ -1,28 +1,28 @@
         public void start() {
             if (shardIt == null) {
                 // just execute it on the local node
                 transportService.sendRequest(clusterService.localNode(), transportShardAction, internalRequest.request(), new BaseTransportResponseHandler<Response>() {
                     @Override
                     public Response newInstance() {
                         return newResponse();
                     }
 
                     @Override
                     public String executor() {
                         return ThreadPool.Names.SAME;
                     }
 
                     @Override
                     public void handleResponse(final Response response) {
                         listener.onResponse(response);
                     }
 
                     @Override
                     public void handleException(TransportException exp) {
-                        perform(exp);
+                        listener.onFailure(exp);
                     }
                 });
             } else {
                 perform(null);
             }
         }",Buggy,"Analysis : Fix no response from Analyze API without specified index

Fix error handling in TransportSingleShardAction without shardIt

Closes #15148
",Buggy,"The commit message indicates a bug fix related to handling errors in the `TransportSingleShardAction` method when `shardIt` is null. Looking at the diff, the `handleException` method's behavior is altered to call `listener.onFailure(exp)` instead of `perform(exp)`. This change directly addresses error-handling by ensuring the failure is communicated appropriately to the listener when there is no shard iteration (`shardIt` is null).

Such a modification aligns with the commit message's intent to fix the problem when the `Analyze API` receives no response due to incorrect handling when no shard is specified. By correcting the error-handling logic, the system can now properly notify about failures and potentially handle or log the issue upstream, preventing silent failures.

Therefore, the change in the code appropriately aligns with the bug fix descriptor in the commit message, indicating a relevant error-handling update. The answer is **Buggy**."
elasticsearch,11277.json,3b739b9fd5da2323a84b242708b64c16660113cc,"@@ -1,6 +1,6 @@
         protected ShardsIterator shards(ClusterState state, InternalRequest request) {
-            return state.routingTable()
-                    .index(request.concreteIndex())
-                    .shard(request.request().getShard().id())
+            return state
+                    .routingTable()
+                    .shardRoutingTable(request.concreteIndex(), request.request().getShard().id())
                     .activeInitializingShardsRandomIt();
         }",Buggy,"Avoid NPE on shard changes action (#32630)

If a leader index is deleted while there is an active follower, the
follower will send shard changes requests bound for the leader
index. Today this will result in a null pointer exception because there
will not be an index routing table for the index. A null pointer
exception looks like a bug to a user so this commit addresses this by
throwing an index not found exception instead.",Buggy,"The commit message describes a scenario where a NullPointerException (NPE) occurs due to missing index routing tables when a leader index is deleted but a follower continues to send shard changes requests. The code change addresses this issue by:

1. Originally, the code accessed the index routing table directly with `state.routingTable().index(request.concreteIndex())` and then accessed a specific shard. This could lead to a NPE if the index was not found in the routing table.

2. The updated code uses `state.routingTable().shardRoutingTable(request.concreteIndex(), request.request().getShard().id())` instead. This suggests a direct attempt to handle the absence of the index routing table more gracefully, possibly by providing a clearer pathway to recognize and recover from the issue.

3. By making this change, the code no longer assumes that the index will always be present and is poised to handle the scenario where it is not. While the diff itself does not show the explicit throwing of an ""index not found exception"", aligning it with the message implies that the new approach ensures a more proper error handling, avoiding an uninformative NPE.

Based on this analysis, the changes align well with the commit message objective of avoiding a NPE and thereby handling the error case more gracefully by potentially leading to an ""index not found"" exception instead.

Therefore, the commit effectively addresses the described bug, and the conclusion is **Buggy**."
elasticsearch,50555.json,03e8734b04d841e31724b10cf0d9c41801699411,"@@ -1,24 +1,29 @@
     private void getMultipleReposSnapshotInfo(List<RepositoryMetaData> repos, String[] snapshots, boolean ignoreUnavailable,
                                               boolean verbose, ActionListener<GetSnapshotsResponse> listener) {
+        // short-circuit if there are no repos, because we can not create GroupedActionListener of size 0
+        if (repos.isEmpty()) {
+            listener.onResponse(new GetSnapshotsResponse(Collections.emptyList()));
+            return;
+        }
         final GroupedActionListener<GetSnapshotsResponse.Response> groupedActionListener =
                 new GroupedActionListener<>(
                         ActionListener.map(listener, responses -> {
                             assert repos.size() == responses.size();
                             return new GetSnapshotsResponse(responses);
                         }), repos.size());
 
         // run concurrently for all repos on GENERIC thread pool
         for (final RepositoryMetaData repo : repos) {
             threadPool.executor(ThreadPool.Names.GENERIC).execute(new ActionRunnable<>(groupedActionListener) {
                 @Override
                 protected void doRun() {
                     try {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.snapshots(
                                 repo.name(), getSingleRepoSnapshotInfo(repo.name(), snapshots, ignoreUnavailable, verbose)));
                     } catch (ElasticsearchException e) {
                         groupedActionListener.onResponse(GetSnapshotsResponse.Response.error(repo.name(), e));
                     }
                 }
             });
         }
     }",Buggy,"Fix GET /_snapshot/_all/_all if there are no repos (#43558)

When there are no repositories, a request to GET /_snapshot/_all/_all
returns a 504 timeout error.
This happens because try to create GroupedActionListener with the
size of zero, which leads to an exception.
This commit short-circuits if there are no repos and adds a test to
verify the fix.

Closes #43547",Buggy,"The commit message explicitly describes a bug fix where a request to GET /_snapshot/_all/_all results in a 504 timeout error when there are no repositories due to an attempt to create a `GroupedActionListener` with a size of zero. 

The code changes reflect this fix by adding a conditional check at the start of the `getMultipleReposSnapshotInfo` method. If the `repos` list is empty, it immediately responds with an empty `GetSnapshotsResponse` and returns, preventing any further execution that would lead to creating a `GroupedActionListener` with zero size.

This change directly addresses the issue mentioned in the commit message by avoiding the erroneous situation and ensuring a graceful handling of the no-repositories case. Additionally, the commit mentions the addition of a test to verify this fix, although the diff provided does not include the test changes.

Considering the alignment between the commit message and the code changes, this is a clear **Buggy** change, as it fixes a specific bug related to error handling when no repositories are present."
elasticsearch,35892.json,ba8ad9c2b727f9609e16aac6ed2df17f44c6b683,"@@ -1,3 +1,3 @@
     public long getAgeInMillis() {
-        return Math.max(0, (System.nanoTime() - creationDate) / 1000);
+        return TimeUnit.MILLISECONDS.convert(relativeTimeProvider.getAsLong() - creationDate, TimeUnit.NANOSECONDS);
     }",Buggy,"Fix calculation of age of pending tasks

This commit addresses a time unit conversion bug in calculating the age
of a PrioritizedRunnable. The issue was an incorrect conversion from
nanoseconds to milliseconds as instead the conversion was to
microseconds. This leads to the timeInQueue metric for pending tasks to
be off by three orders of magnitude.
",Buggy,"The commit message clearly states that it fixes a bug related to the incorrect conversion of time units, leading to errors in calculating the age of pending tasks. The change in the code replacement of manual division with `TimeUnit.MILLISECONDS.convert` correctly handles nanoseconds to milliseconds conversion, resolving the issue described in the commit message.

1. **Original Code**: The original code used `(System.nanoTime() - creationDate) / 1000`, which incorrectly converts nanoseconds to microseconds instead of milliseconds.
   
2. **New Code**: The updated code uses `TimeUnit.MILLISECONDS.convert()` to handle the conversion, ensuring precision and correctness in the unit conversion from nanoseconds to milliseconds.
   
3. **Commit Message and Code Alignment**: The commit message specifically describes fixing a time unit conversion bug. The updated implementation directly addresses and fixes this bug as described.

Given this analysis, the changes align with the commit message and clearly indicate a bug fix. Therefore, the evaluation of this commit is **Buggy**."
elasticsearch,55497.json,53f6dcfd375169c9847a6059253a1990fb7c99f5,"@@ -1,11 +1,11 @@
         private SpanQuery newSpanQuery(Term[] terms, boolean isPrefix) {
             if (terms.length == 1) {
                 return isPrefix ? fieldType.spanPrefixQuery(terms[0].text(), spanRewriteMethod, context) : new SpanTermQuery(terms[0]);
             }
             SpanQuery[] spanQueries = new SpanQuery[terms.length];
             for (int i = 0; i < terms.length; i++) {
-                spanQueries[i] = isPrefix ? new SpanTermQuery(terms[i]) :
-                    fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context);
+                spanQueries[i] = isPrefix ? fieldType.spanPrefixQuery(terms[i].text(), spanRewriteMethod, context) :
+                    new SpanTermQuery(terms[i]);
             }
             return new SpanOrQuery(spanQueries);
         }",Buggy,"Fix wrong logic in `match_phrase` query with multi-word synonyms (#43941)

Disjunction over two individual terms in a phrase query with multi-word synonyms
wrongly applies a prefix query to each of these terms. This change fixes this bug
by inversing the logic to use prefixes on `phrase_prefix` queries only.

Closes #43308",Buggy,"The commit message describes a fix for incorrect logic in handling queries with multi-word synonyms. The provided code diff aligns with this message by correcting the logic used to determine whether a `SpanTermQuery` or `spanPrefixQuery` should be applied. The change inverts the logic for applying prefix queries, fixing the bug in the `match_phrase` query. The structured nature of this fix alongside an issue reference in the commit message strongly indicates it addresses a specific bug. Therefore, the assessment is **Buggy**."
elasticsearch,58217.json,a46d2f21c6504c8ff8cb98a2180293994b857a03,"@@ -1,10 +1,11 @@
     private static int expandCommonMappers(List<ObjectMapper> parentMappers, String[] nameParts, int i) {
         ObjectMapper last = parentMappers.get(parentMappers.size() - 1);
         while (i < nameParts.length - 1 && last.getMapper(nameParts[i]) != null) {
             Mapper newLast = last.getMapper(nameParts[i]);
             assert newLast instanceof ObjectMapper;
-            parentMappers.add((ObjectMapper)newLast);
+            last = (ObjectMapper) newLast;
+            parentMappers.add(last);
             ++i;
         }
         return i;
     }",Buggy,"Fix dynamic mapper bug with deeply nested fields.
",Buggy,"The commit message indicates a fix for a bug related to handling deeply nested fields in a dynamic mapper. Examining the code diff:

1. **Previous Logic**: The loop fetched a mapper, asserted its type, and added it to the `parentMappers` list, but it didn't update `last` after the first addition.

2. **Modified Logic**: The fix updates the `last` variable within the loop to the newly retrieved mapper (`newLast`) before adding it to the `parentMappers`. This change ensures that subsequent iterations operate on the correct current `ObjectMapper`.

This modification addresses a potential issue where the loop would repeatedly add the same initial mapper without checking deeper nested mappers properly, which aligns with fixing a bug related to deeply nested fields.

Thus, given the nature of the change and the alignment with the commit message, the changes indeed correct a logical error concerning nested field mapping.

Therefore, the assessment concludes with **Buggy**."
elasticsearch,63178.json,c33f894846989694f6d6fdc250ae396a88545c55,"@@ -1,19 +1,4 @@
         public Set<Entry<String, List<String>>> entrySet() {
-            return httpHeaders.names().stream().map(k -> new Entry<String, List<String>>() {
-
-                @Override
-                public String getKey() {
-                    return k;
-                }
-
-                @Override
-                public List<String> getValue() {
-                    return httpHeaders.getAll(k);
-                }
-
-                @Override
-                public List<String> setValue(List<String> value) {
-                    throw new UnsupportedOperationException(""modifications are not supported"");
-                }
-            }).collect(Collectors.toSet());
+            return httpHeaders.names().stream().map(k -> new AbstractMap.SimpleImmutableEntry<>(k, httpHeaders.getAll(k)))
+                    .collect(Collectors.toSet());
         }",Buggy,"Fixing compilation problem in Eclipse (#22956)

",NotBuggy,"The commit message states that the change fixes a compilation problem in Eclipse. The code diff replaces an anonymous inner class implementation of the `Entry` interface with `AbstractMap.SimpleImmutableEntry`, simplifying the code. The change likely resolves a compatibility or class-related issue that Eclipse was having with the original anonymous class implementation. The modification appears to directly address the stated compilation problem without altering the core logic, rather it updates the way entries are created in the set to be compliant with the target environment (Eclipse in this case). 

This does not indicate a functional bug in terms of logical errors or exceptions during runtime operation, but rather a build-related issue. Since the focus is on compilation rather than fixing logical errors or runtime bugs, it aligns more with addressing integration or environment compatibility issues. 

Therefore, the final assessment is **NotBuggy**."
elasticsearch,10846.json,70d524c979f90759901fd81f70c56172c311bd86,"@@ -1,7 +1,7 @@
     protected boolean shouldCollect() {
-        if (licensee.collectionEnabled()) {
+        if (!licensee.collectionEnabled()) {
             logger.trace(""collector [{}] can not collect data due to invalid license"", name());
             return false;
         }
         return true;
     }",Buggy,"Marvel: Fix bug that prevent collectors to collect when license is enabled

Original commit: elastic/x-pack-elasticsearch@9f22baa2ee8f873e4123681d36bcbe759da619f1
",Buggy,"The code modification involves changing the conditional check in the method shouldCollect. Originally, the method returned false if licensee.collectionEnabled() was true. The change makes it return false if licensee.collectionEnabled() is false instead.

This change corrects a logical error where data collection was prevented when the license collection was enabled, contrary to the intended behavior. The modification fixes this logic, ensuring the collector can only collect data when collection is enabled, aligning with the stated purpose in the commit message.

The adjustment is directly related to a bug in the code, specifically a logical error in license validation, and the commit message explicitly states the intention to fix a bug regarding collectors not being able to collect data when the license is enabled.

Thus, the changes are indeed indicative of a bug fix. The conclusion is **Buggy**."
elasticsearch,15635.json,00cecac86e03195a28f4bec92ccd8cc182a16132,"@@ -1,13 +1,15 @@
     public List<ExecutorBuilder<?>> getExecutorBuilders(final Settings settings) {
         if (enabled) {
-            final FixedExecutorBuilder builder =
-                    new FixedExecutorBuilder(
-                            settings,
+            final ScalingExecutorBuilder builder =
+                    new ScalingExecutorBuilder(
                             InternalWatchExecutor.THREAD_POOL_NAME,
+                            0,
+                            // watcher threads can block on I/O for a long time, so we let this
+                            // pool be large so that execution of unblocked watches can proceed
                             5 * EsExecutors.boundedNumberOfProcessors(settings),
-                            1000,
+                            TimeValue.timeValueMinutes(5),
                             ""xpack.watcher.thread_pool"");
             return Collections.singletonList(builder);
         }
         return Collections.emptyList();
     }",Buggy,"Change Watcher thread pool to be scaling

Watcher uses a custom thread pool. This is because executing watches can
be long-running tasks that often block on I/O and it is best to not
consume the core thread pools with these tasks. Today this thread pool
is fixed, and sized at five times the bounded number of cores (so 160 on
a 32-core box). It makes sense for there to possibly be so many threads,
again because these tasks can block on I/O and having excess capacity
lets unblocked watches execute. It's the fixed size that can cause
problem, all these threads are always consuming resources even when
there are no or not that many watches running. This commit changes this
thread pool to be a scaling thread pool.

Relates elastic/elasticsearch#3660

Original commit: elastic/x-pack-elasticsearch@3cafab6e8360b17ecaf99d37bc1e024a9c7ff256
",NotBuggy,"The commit message describes changing the Watcher thread pool from a fixed to a scaling model, aiming to resolve issues with resource consumption when threads aren't actively executing tasks. 

Analyzing the diff:

1. The `FixedExecutorBuilder` is replaced with `ScalingExecutorBuilder`, indicating a shift from a fixed-size thread pool to one that can dynamically scale.
2. The parameters to the constructor:
   - `0`: The minimum number of threads, allowing the pool to shrink when inactive.
   - `TimeValue.timeValueMinutes(5)`: Specifies the maximum time that excess idle threads will wait for new tasks before terminating.

This change addresses the problem of unnecessarily consumed resources by threads in a fixed pool that may not all be active. By implementing a scaling thread pool, the system can more efficiently allocate resources, only consuming them when necessary.

Despite the change being performance-focused, no direct link to a bug is implied here. The description highlights resource optimization rather than fixing erroneous behavior.

Thus, the changes align with the commit message, focusing on improving behavior regarding resource management. However, there is no specific bug correction, as the current system functions correctly but inefficiently in certain scenarios.

The conclusion is **NotBuggy** because this adjustment is an improvement for performance, not a fix for a malfunction."
elasticsearch,11653.json,bca4edcd56fa984b9e712f9298e6f91a19983710,"@@ -1,76 +1,77 @@
     static Tuple<String, String> overrideFormatToGrokAndRegex(String overrideFormat) {
 
         if (overrideFormat.indexOf('\n') >= 0 || overrideFormat.indexOf('\r') >= 0) {
             throw new IllegalArgumentException(""Multi-line timestamp formats ["" + overrideFormat + ""] not supported"");
         }
 
         if (overrideFormat.indexOf(INDETERMINATE_FIELD_PLACEHOLDER) >= 0) {
             throw new IllegalArgumentException(""Timestamp format ["" + overrideFormat + ""] not supported because it contains [""
                 + INDETERMINATE_FIELD_PLACEHOLDER + ""]"");
         }
 
         StringBuilder grokPatternBuilder = new StringBuilder();
         StringBuilder regexBuilder = new StringBuilder();
 
         boolean notQuoted = true;
         char prevChar = '\0';
         String prevLetterGroup = null;
         int pos = 0;
         while (pos < overrideFormat.length()) {
             char curChar = overrideFormat.charAt(pos);
 
             if (curChar == '\'') {
                 notQuoted = !notQuoted;
             } else if (notQuoted && Character.isLetter(curChar)) {
                 int startPos = pos;
                 int endPos = startPos + 1;
                 while (endPos < overrideFormat.length() && overrideFormat.charAt(endPos) == curChar) {
                     ++endPos;
                     ++pos;
                 }
                 String letterGroup = overrideFormat.substring(startPos, endPos);
                 Tuple<String, String> grokPatternAndRegexForGroup = VALID_LETTER_GROUPS.get(letterGroup);
                 if (grokPatternAndRegexForGroup == null) {
                     // Special case of fractional seconds
                     if (curChar != 'S' || FRACTIONAL_SECOND_SEPARATORS.indexOf(prevChar) == -1 ||
                         ""ss"".equals(prevLetterGroup) == false || endPos - startPos > 9) {
                         String msg = ""Letter group ["" + letterGroup + ""] in ["" + overrideFormat + ""] is not supported"";
                         if (curChar == 'S') {
                             msg += "" because it is not preceded by [ss] and a separator from ["" + FRACTIONAL_SECOND_SEPARATORS + ""]"";
                         }
                         throw new IllegalArgumentException(msg);
                     }
-                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional
-                    // fraction, but we need to remove the separator that's included in %{SECOND}
-                    grokPatternBuilder.deleteCharAt(grokPatternBuilder.length() - 1);
+                    // No need to append to the Grok pattern as %{SECOND} already allows for an optional fraction,
+                    // but we need to remove the separator that's included in %{SECOND} (and that might be escaped)
+                    int numCharsToDelete = (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(prevChar) >= 0) ? 2 : 1;
+                    grokPatternBuilder.delete(grokPatternBuilder.length() - numCharsToDelete, grokPatternBuilder.length());
                     regexBuilder.append(""\\d{"").append(endPos - startPos).append('}');
                 } else {
                     grokPatternBuilder.append(grokPatternAndRegexForGroup.v1());
                     if (regexBuilder.length() == 0) {
                         regexBuilder.append(""\\b"");
                     }
                     regexBuilder.append(grokPatternAndRegexForGroup.v2());
                 }
                 if (pos + 1 == overrideFormat.length()) {
                     regexBuilder.append(""\\b"");
                 }
                 prevLetterGroup = letterGroup;
             } else {
                 if (PUNCTUATION_THAT_NEEDS_ESCAPING_IN_REGEX.indexOf(curChar) >= 0) {
                     grokPatternBuilder.append('\\');
                     regexBuilder.append('\\');
                 }
                 grokPatternBuilder.append(curChar);
                 regexBuilder.append(curChar);
             }
 
             prevChar = curChar;
             ++pos;
         }
 
         if (prevLetterGroup == null) {
             throw new IllegalArgumentException(""No time format letter groups in override format ["" + overrideFormat + ""]"");
         }
 
         return new Tuple<>(grokPatternBuilder.toString(), regexBuilder.toString());
     }",Buggy,"[ML] Fix custom timestamp override with dot-separated fractional seconds (#44127)

Custom timestamp overrides provided to the find_file_structure
endpoint produced an invalid Grok pattern if the fractional
seconds separator was a dot rather than a comma or colon.
This commit fixes that problem and adds tests for this sort
of timestamp override.

Fixes #44110",Buggy,"The changes in this commit fix an issue with custom timestamp overrides when the fractional seconds separator is a dot. Here's a breakdown of the relevant changes:

1. **Handling of Escaped Characters**: The code now checks for punctuation that needs escaping in regex and deletes an additional character if the previous character requires escaping. This addresses the issue where the Grok pattern might be incorrect if the separator for fractional seconds is a dot (which needs escaping).

2. **Error Handling for Unsupported Formats**: There's emphasis on managing unsupported letter groups, particularly dealing with fractional seconds preceded by incorrect separators.

3. **Validation Logic**: The logic now correctly ensures the appropriate removal of separators in building the Grok pattern, particularly for fractional seconds.

These changes directly target a specific bug related to timestamp parsing, especially handling edge cases for separators, which matches the description in the commit message about the custom timestamp override issue.

Considering these factors, this commit addresses a specific issue related explicitly to parsing and format validation, which indeed fixes a bug. Therefore, I conclude with **Buggy**."
elasticsearch,26537.json,61f5c188e0a577555db382dd0fe9a4222da9df1a,"@@ -1,12 +1,12 @@
-    static Request openJob(OpenJobRequest openJobRequest) {
+    static Request openJob(OpenJobRequest openJobRequest) throws IOException {
         String endpoint = new EndpointBuilder()
                 .addPathPartAsIs(""_xpack"")
                 .addPathPartAsIs(""ml"")
                 .addPathPartAsIs(""anomaly_detectors"")
                 .addPathPart(openJobRequest.getJobId())
                 .addPathPartAsIs(""_open"")
                 .build();
         Request request = new Request(HttpPost.METHOD_NAME, endpoint);
         request.setEntity(createEntity(openJobRequest, REQUEST_BODY_CONTENT_TYPE));
         return request;
     }",Buggy,"HLRC: Fix Compile Error From Missing Throws (#33083)

* 50441f97ae745814db96c262e99d0f465aca5b2c#diff-53a95fe7ded21313483f1b2f15977395L72 removed the throws breaking compilation here",Buggy,"The commit message explicitly states that the change fixes a compile error due to a missing `throws` declaration. The diff adds `throws IOException` to the method signature, which aligns with resolving a compilation issue when executing code that might throw an `IOException`. This change is directly related to fixing a bug in the code, namely a compilation error.

Thus, the modification in the code is consistent with the commit message’s intent to fix a compile error, which is indeed a type of bug fix. Therefore, the answer is **Buggy**."
elasticsearch,54800.json,2637e499ac88ce0aa14db62b13be052e7ca78f98,"@@ -1,8 +1,8 @@
     protected final DirectoryReader wrapReader(DirectoryReader reader,
                                                     Function<DirectoryReader, DirectoryReader> readerWrapperFunction) throws IOException {
-        reader = ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
         if (engineConfig.getIndexSettings().isSoftDeleteEnabled()) {
             reader = new SoftDeletesDirectoryReaderWrapper(reader, Lucene.SOFT_DELETES_FIELD);
         }
-        return readerWrapperFunction.apply(reader);
+        reader = readerWrapperFunction.apply(reader);
+        return ElasticsearchDirectoryReader.wrap(reader, engineConfig.getShardId());
     }",Buggy,"Fix assertion error when caching the result of a search in a read-only index (#41900)

The ReadOnlyEngine wraps its reader with a SoftDeletesDirectoryReaderWrapper if soft deletes
are enabled. However the wrapping is done on top of the ElasticsearchDirectoryReader and that
trips assertion later on since the cache key of these directories are different. This commit
changes the order of the wrapping to put the ElasticsearchDirectoryReader first in order to
ensure that it is always retrieved first when we unwrap the directory.

Closes #41795
",Buggy,"The commit message describes an issue related to an assertion error caused by the incorrect order of wrapping the `DirectoryReader`. The commit changes the order of wrapping so that `ElasticsearchDirectoryReader.wrap` is applied last, which is crucial for ensuring the cache key is consistent and prevents the assertion error mentioned in the commit message.

Let's break down the changes:
- Originally, `ElasticsearchDirectoryReader.wrap` was called first, followed by potentially wrapping it in a `SoftDeletesDirectoryReaderWrapper`.
- After the fix, `ElasticsearchDirectoryReader.wrap` is applied after the `readerWrapperFunction`, ensuring that this is the outermost wrap on the reader.

This change makes sure that when the directory is unwrapped, `ElasticsearchDirectoryReader` is the first wrapper to be removed. This directly addresses the described bug in the commit message (`Fix assertion error when caching the result of a search in a read-only index`) and corrects the order for consistency and correctness as described.

Given the details in the commit message and the nature of the code changes, there is a clear indication of a bug fix being applied to prevent assertion errors caused by incorrect wrapping order.

Thus, I conclude that this change is **Buggy**."
elasticsearch,31501.json,0f93b7abdf65425344f5cedda3e6b04b31e910d4,"@@ -1,3 +1,3 @@
-    static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
+    public static boolean needsReassignment(final Assignment assignment, final DiscoveryNodes nodes) {
         return (assignment.isAssigned() == false || nodes.nodeExists(assignment.getExecutorNode()) == false);
     }",Buggy,"Fix compilation errors in ML integration tests

After elastic/elasticsearch#29109, the `needsReassignment` method has
been moved to the PersistentTasksClusterService. This commit fixes
some compilation in tests I introduced.
",NotBuggy,"The commit message states it is fixing compilation errors in ML integration tests after a method relocation in another service. The code diff changes the visibility of the `needsReassignment` method from package-private (default) to public. This adjustment aligns with the scenario where the method needs to be accessed from other packages, which likely arose because of the relocation referred to in the commit message.

This change directly addresses an issue related to code compilation due to method access, suggesting that the previous compilation state would result in errors due to access restrictions. The `public` modifier resolves the accessibility issue and allows other parts of the codebase, possibly the tests, to compile successfully and access the method.

Given the commit message and the nature of this change, it correlates precisely with fixing a specific compilation issue post-refactoring or a codebase update. The change is not about correcting faulty logic or handling, but rather ensuring the code compiles without errors given recent changes in method location and access requirements.

The change doesn't indicate a bug in terms of a logical error or fault during runtime after deployment but rather a development-time error related to access that prevents compilation. Compilation errors, while they indicate problems in code that must be resolved, don't typically qualify as ""bugs"" in the operational sense unless they permit runtime faults undetected during development.

Thus, although it fixes a problem in code accessibility, it doesn't fix a ""bug"" in the system's operational logic or runtime, leading us to conclude **NotBuggy** for this specific context."
elasticsearch,63560.json,63f33e0f1e045f5c345b80bdbc4b6f367e72aaad,"@@ -1,16 +1,18 @@
     public void writeTo(StreamOutput out) throws IOException {
+        // marshall doc count
+        out.writeGenericValue(docCount);
         // marshall fieldSum
         out.writeGenericValue(fieldSum);
         // counts
         out.writeGenericValue(counts);
         // mean
         out.writeGenericValue(means);
         // variances
         out.writeGenericValue(variances);
         // skewness
         out.writeGenericValue(skewness);
         // kurtosis
         out.writeGenericValue(kurtosis);
         // covariances
         out.writeGenericValue(covariances);
     }",Buggy,"Serialize doc counts in Matrix-Stats module

This fixes a bug in the RunningStats class for the matrix stats aggregation module. doc counts were not being searlized which means they were only computed the first time the aggregation was computed. This was causing incorrect results when the aggregation was pulled from cache.
",Buggy,"The commit message indicates a fix for a bug where document counts were not serialized in the RunningStats class, causing incorrect results when using cached aggregation results.

Here’s the step-by-step reasoning:

1. **Commit Message Analysis**: The message explains that there was an issue with doc counts not being serialized, leading to incorrect aggregation results when cached values were used. This suggests that doc counts are essential for accurate calculations when retrieving data from the cache.

2. **Code Analysis**: The added line of code in the `writeTo` method writes the `docCount` to the output stream. This change ensures that `docCount` is included in the serialized data, aligning with the commit message explanation.

3. **Functionality**: The `writeTo` method is part of a serialization process. The serialization of `docCount` ensures that all necessary data is preserved across sessions, which would address the bug described (stale or incorrect aggregation results).

4. **Contextual Relevance**: The code modification directly addresses the issue described in the commit message. It specifically targets the bug related to missing serialization, which is crucial for data consistency when using cached aggregations.

Since the change involves fixing the serialization of an essential component (`docCount`) that impacts the correctness of cached results, the code modification is consistent with the described bug fix.

Therefore, the conclusion is **Buggy**."
elasticsearch,62818.json,e6fb3a5d950c52ccba8cf1f0b0d6e819e70c243c,"@@ -1,15 +1,21 @@
     public void execute(IngestDocument document) {
         String oldVal = document.getFieldValue(field, String.class, ignoreMissing);
 
         if (oldVal == null && ignoreMissing) {
             return;
         } else if (oldVal == null) {
             throw new IllegalArgumentException(""field ["" + field + ""] is null, cannot extract key-value pairs."");
         }
 
         String fieldPathPrefix = (targetField == null) ? """" : targetField + ""."";
         Arrays.stream(oldVal.split(fieldSplit))
-            .map((f) -> f.split(valueSplit, 2))
+            .map((f) -> {
+                String[] kv = f.split(valueSplit, 2);
+                if (kv.length != 2) {
+                    throw new IllegalArgumentException(""field ["" + field + ""] does not contain value_split ["" + valueSplit + ""]"");
+                }
+                return kv;
+            })
             .filter((p) -> includeKeys == null || includeKeys.contains(p[0]))
             .forEach((p) -> append(document, fieldPathPrefix + p[0], p[1]));
     }",Buggy,"fix index out of bounds error in KV Processor (#22288)

- checks for index-out-of-bounds
- added unit tests for failed `field_split` and `value_split` scenarios

missed this test in #22272.",Buggy,"The commit message specifies a fix for an ""index out of bounds error"" in a processor. Let's evaluate the code changes:

1. **Error Handling Added**: The new code introduces a check after splitting a string into key-value pairs based on `valueSplit`. 

2. **New Check**: It verifies the length of the resulting array `kv`. If the `kv` array does not have exactly two elements (indicating a missing value), it throws an `IllegalArgumentException`.

3. **Original Problem**: Without this check, the code assumes the array has exactly two parts, which could lead to an `IndexOutOfBoundsException` if the `split(valueSplit, 2)` method does not divide the `f` string into the expected two parts.

4. **Commit Message Alignment**: The commit message explicitly mentions fixing an index out of bounds error, which aligns with the type of improvement seen in the code change. The added error handling prevents such exceptions by proactively checking the length of arrays before accessing their elements.

The change directly prevents a specific runtime error by adding necessary checks, aligning well with the commit message indicating it is a bug fix. Therefore, the conclusion is **Buggy**."
elasticsearch,31383.json,76cd7b1eb2cf7f87081c24ae6362ae1d9dec6100,"@@ -1,3 +1,3 @@
-        public Params getParams() {
+        public P getParams() {
             return params;
         }",Buggy,"Fixes compile errors in Eclipse due to generics

PersistentTasksCustomMetadata was using a generic param named `Params`. This conflicted with the imported interface `ToXContent.Params`. The java compiler was preferring the generic param over the interface so everything was fine but Eclipse apparently prefers the interface int his case which was screwing up the Hierarchy and causing compile errors in Eclipse. This changes fixes it by renaming the Generic param to `P`
",Buggy,"The commit message indicates a fix for compile errors due to a naming conflict involving generics within an Eclipse environment. The code change reflects the described solution by renaming the generic parameter from `Params` to `P` to alleviate this conflict. This change directly addresses the problem outlined in the commit message and resolves the compilation error by avoiding ambiguity between similarly named items. Although this is not a traditional runtime bug fix, it resolves an important development bug: the inability to compile within a particular IDE. Therefore, I conclude this change is **Buggy** based on the context of fixing a known issue preventing compilation."
elasticsearch,18621.json,6236b3aee4c4ccf9a06ca94af4c4082b7fcf5cde,"@@ -1,32 +1,45 @@
     public ResourcePrivilegesMap checkResourcePrivileges(Set<String> checkForIndexPatterns, boolean allowRestrictedIndices,
                                                          Set<String> checkForPrivileges) {
         final ResourcePrivilegesMap.Builder resourcePrivilegesMapBuilder = ResourcePrivilegesMap.builder();
         final Map<IndicesPermission.Group, Automaton> predicateCache = new HashMap<>();
         for (String forIndexPattern : checkForIndexPatterns) {
-            final Automaton checkIndexAutomaton = IndicesPermission.Group.buildIndexMatcherAutomaton(allowRestrictedIndices,
-                    forIndexPattern);
-            Automaton allowedIndexPrivilegesAutomaton = null;
-            for (Group group : groups) {
-                final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
-                        g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
-                if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
-                    if (allowedIndexPrivilegesAutomaton != null) {
-                        allowedIndexPrivilegesAutomaton = Automatons
-                                .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
-                    } else {
-                        allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+            Automaton checkIndexAutomaton = Automatons.patterns(forIndexPattern);
+            if (false == allowRestrictedIndices && false == RestrictedIndicesNames.RESTRICTED_NAMES.contains(forIndexPattern)) {
+                checkIndexAutomaton = Automatons.minusAndMinimize(checkIndexAutomaton, RestrictedIndicesNames.NAMES_AUTOMATON);
+            }
+            if (false == Operations.isEmpty(checkIndexAutomaton)) {
+                Automaton allowedIndexPrivilegesAutomaton = null;
+                for (Group group : groups) {
+                    final Automaton groupIndexAutomaton = predicateCache.computeIfAbsent(group,
+                            g -> IndicesPermission.Group.buildIndexMatcherAutomaton(g.allowRestrictedIndices(), g.indices()));
+                    if (Operations.subsetOf(checkIndexAutomaton, groupIndexAutomaton)) {
+                        if (allowedIndexPrivilegesAutomaton != null) {
+                            allowedIndexPrivilegesAutomaton = Automatons
+                                    .unionAndMinimize(Arrays.asList(allowedIndexPrivilegesAutomaton, group.privilege().getAutomaton()));
+                        } else {
+                            allowedIndexPrivilegesAutomaton = group.privilege().getAutomaton();
+                        }
                     }
                 }
-            }
-            for (String privilege : checkForPrivileges) {
-                IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
-                if (allowedIndexPrivilegesAutomaton != null
-                        && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
-                    resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
-                } else {
+                for (String privilege : checkForPrivileges) {
+                    IndexPrivilege indexPrivilege = IndexPrivilege.get(Collections.singleton(privilege));
+                    if (allowedIndexPrivilegesAutomaton != null
+                            && Operations.subsetOf(indexPrivilege.getAutomaton(), allowedIndexPrivilegesAutomaton)) {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.TRUE);
+                    } else {
+                        resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
+                    }
+                }
+            } else {
+                // the index pattern produced the empty automaton, presumably because the requested pattern expands exclusively inside the
+                // restricted indices namespace - a namespace of indices that are normally hidden when granting/checking privileges - and
+                // the pattern was not marked as `allowRestrictedIndices`. We try to anticipate this by considering _explicit_ restricted
+                // indices even if `allowRestrictedIndices` is false.
+                // TODO The `false` result is a _safe_ default but this is actually an error. Make it an error.
+                for (String privilege : checkForPrivileges) {
                     resourcePrivilegesMapBuilder.addResourcePrivilege(forIndexPattern, privilege, Boolean.FALSE);
                 }
             }
         }
         return resourcePrivilegesMapBuilder.build();
     }",Buggy,"Fix Has Privilege API check on restricted indices (#41226)

The Has Privileges API allows to tap into the authorization process, to validate
privileges without actually running the operations to be authorized. This commit
fixes a bug, in which the Has Privilege API returned spurious results when checking
for index privileges over restricted indices (currently .security, .security-6,
.security-7). The actual authorization process is not affected by the bug.",Buggy,"The changes in this diff address a bug related to the `Has Privileges` API and how it handles restricted indices. The commit message clearly states that the bug causes spurious results when checking index privileges over restricted indices such as `.security`, `.security-6`, and `.security-7`. The diff itself incorporates several notable changes to fix this issue:

1. **Automaton Construction**: The logic for constructing the `checkIndexAutomaton` has been adjusted. The new code ensures that restricted indices are properly accounted for based on the `allowRestrictedIndices` flag and the presence of such indices in the `RestrictedIndicesNames.RESTRICTED_NAMES`.

2. **Automaton Minimization**: The introduction of `Automatons.minusAndMinimize` is used to handle patterns associated with restricted indices, ensuring that the automaton doesn't match restricted indices inadvertently unless explicitly allowed.

3. **Check Empty Automaton**: There's an added check for whether `checkIndexAutomaton` is empty. If it is, the code handles it specifically by setting all privileges to `FALSE`, indicating that privileges are not granted. The comment explains that the empty automaton's result, likely due to indices being only in the restricted namespace without permission, is a pragmatic failure prevention measure.

4. **Resource Privilege Handling**: The logic also ensures that privilege checks are accurate, only returning `TRUE` if the privilege automaton is a subset of the allowed automaton. Otherwise, a `FALSE` is added, correcting any previous logic that might have misjudged permissions.

These modifications directly align with the bug fix described in the commit message. The bug pertains to incorrect privilege results over restricted indices, which the changes address by refining how such indices and their associated privileges are processed.

Therefore, the changes, the commit message, and the identified fixes suggest this commit is indeed related to a bug fix. The conclusion is **Buggy**."
elasticsearch,52742.json,a36543531b4f547bbb7be8156d4c1e55f0d53cf9,"@@ -1,8 +1,11 @@
     private void onFailedFreedContext(Throwable e, DiscoveryNode node) {
         logger.warn(() -> new ParameterizedMessage(""Clear SC failed on node[{}]"", node), e);
+        /*
+         * We have to set the failure marker before we count down otherwise we can expose the failure marker before we have set it to a
+         * racing thread successfully freeing a context. This would lead to that thread responding that the clear scroll succeeded.
+         */
+        hasFailed.set(true);
         if (expectedOps.countDown()) {
             listener.onResponse(new ClearScrollResponse(false, freedSearchContexts.get()));
-        } else {
-            hasFailed.set(true);
         }
     }",Buggy,"Fix race in clear scroll (#31259)

Here is the problem: if two threads are racing and one hits a failure
freeing a context and the other succeeded, we can expose the value of
the has failure marker to the succeeding thread before the failing
thread has had a chance to set the failure marker. This is a problem if
the failing thread counted down the expected number of operations, then
be put to sleep by a gentle lullaby from the OS, and then the other
thread could count down to zero. Since the failing thread did not get to
set the failure marker, the succeeding thread would respond that the
clear scroll succeeded and that makes that thread a liar. This commit
addresses by first setting the failure marker before we potentially
expose its value to another thread.",Buggy,"The commit message describes a race condition bug where a thread might prematurely expose incorrect status due to the sequence of operations. The race condition involves multiple threads interacting with shared resource states (failure marker and countdown operations). The code change addresses this by setting the failure marker (`hasFailed.set(true)`) before the countdown operation. This ensures that other threads evaluating the state will see an accurate error status even if the failing thread has not yet completed its operations.

The fix clearly aligns with the described issue. It corrects the order of operations to eliminate the race condition, thus preventing the scenario where a false success status might be reported.

The code modification and the commit message align well with a classic bug fix related to concurrency, where the inappropriate order of operations was leading to incorrect behavior.

Therefore, the conclusion is **Buggy**."
elasticsearch,9348.json,308ae98988fb9d7c5374a5b16dac810a7bd9715c,"@@ -1,20 +1,22 @@
         private List<Realm> getRealmList(String principal) {
             final List<Realm> orderedRealmList = this.defaultOrderedRealmList;
             if (lastSuccessfulAuthCache != null) {
                 final Realm lastSuccess = lastSuccessfulAuthCache.get(principal);
                 if (lastSuccess != null) {
                     final int index = orderedRealmList.indexOf(lastSuccess);
                     if (index > 0) {
                         final List<Realm> smartOrder = new ArrayList<>(orderedRealmList.size());
                         smartOrder.add(lastSuccess);
-                        for (int i = 1; i < orderedRealmList.size(); i++) {
+                        for (int i = 0; i < orderedRealmList.size(); i++) {
                             if (i != index) {
                                 smartOrder.add(orderedRealmList.get(i));
                             }
                         }
+                        assert smartOrder.size() == orderedRealmList.size() && smartOrder.containsAll(orderedRealmList)
+                            : ""Element mismatch between SmartOrder="" + smartOrder + "" and DefaultOrder="" + orderedRealmList;
                         return Collections.unmodifiableList(smartOrder);
                     }
                 }
             }
             return orderedRealmList;
         }",Buggy,"Fix iterate-from-1 bug in smart realm order (#49473)

The AuthenticationService has a feature to ""smart order"" the realm
chain so that whicherver realm was the last one to successfully
authenticate a given user will be tried first when that user tries to
authenticate again.

There was a bug where the building of this realm order would
incorrectly drop the first realm from the default chain unless that
realm was the ""last successful"" realm.

In most cases this didn't cause problems because the first realm is
the reserved realm and so it is unusual for a user that authenticated
against a different realm to later need to authenticate against the
resevered realm.

This commit fixes that bug and adds relevant asserts and tests.",Buggy,"The commit message and the code changes align well, indicating a fix for an off-by-one iteration bug. Originally, the loop started from `i = 1`, which mistakenly skipped the first realm in the list unless it was the last successful one. Changing the loop to start from `i = 0` ensures that all realms are considered, fixing the bug. Additionally, an assertion is added to validate that the reordered list contains all elements from the original order, maintaining integrity in realm processing. This change effectively resolves the described bug in smart realm ordering. The analysis concludes with **Buggy**."
elasticsearch,38299.json,38085cf90af953cfcaf7807c2dfccc55b742cdff,"@@ -1,5 +1,4 @@
     public AggregationPath subPath(int offset, int length) {
-        PathElement[] subTokens = new PathElement[length];
-        System.arraycopy(pathElements, offset, subTokens, 0, length);
-        return new AggregationPath(pathElements);
+        List<PathElement> subTokens = new ArrayList<>(pathElements.subList(offset, offset + length));
+        return new AggregationPath(subTokens);
     }",Buggy,"Aggregation: Fix AggregationPath.subPath() to not throw ArrayStoreException

Aggregation.subPath() always threw an ArrayStoreException because we were trying to pass a List into System.arraycopy(). This change fixes that bug and adds a test to prevent regression
",Buggy,"The provided commit message explains that there was an `ArrayStoreException` being thrown due to an incorrect attempt to pass a `List` into `System.arraycopy()`. This change replaces the attempt to use `System.arraycopy()` with a `subList()` operation on a `List`, storing the result directly in an `ArrayList`.

The original code attempted to create an array and use `System.arraycopy()` on what seems to be elements intended originally for a list, causing the `ArrayStoreException`. The fix corrects this logic by directly using `List` operations, thus preventing `ArrayStoreException`.

Additionally, the commit message notes that a test was added to prevent regression, although the test code is not shown in the diff. 

The updated code aligns precisely with the commit message's description of the bug and its fix.

Thus, the changes indicate a bug fix as explained in the commit message. The assessment is **Buggy**."
elasticsearch,10639.json,8b201e64ffeffa8dfe8c5849a81decb2a62dbe06,"@@ -1,19 +1,20 @@
     public static String loadWatch(final ClusterService clusterService, final String watchId) {
         final String resource = String.format(Locale.ROOT, WATCH_FILE, watchId);
 
         try {
             final String clusterUuid = clusterService.state().metaData().clusterUUID();
             final String uniqueWatchId = createUniqueWatchId(clusterUuid, watchId);
 
             // load the resource as-is
             String source = loadResource(resource).utf8ToString();
 
             source = CLUSTER_UUID_PROPERTY.matcher(source).replaceAll(clusterUuid);
             source = WATCH_ID_PROPERTY.matcher(source).replaceAll(watchId);
             source = UNIQUE_WATCH_ID_PROPERTY.matcher(source).replaceAll(uniqueWatchId);
+            source = VERSION_CREATED_PROPERTY.matcher(source).replaceAll(Integer.toString(LAST_UPDATED_VERSION));
 
             return source;
         } catch (final IOException e) {
             throw new RuntimeException(""Unable to load Watch ["" + watchId + ""]"", e);
         }
     }",Buggy,"Fix cluster alert for watcher/monitoring IndexOutOfBoundsExcep… (#45308)

If a cluster sending monitoring data is unhealthy and triggers an
alert, then stops sending data the following exception [1] can occur.

This exception stops the current Watch and the behavior is actually
correct in part due to the exception. Simply fixing the exception
introduces some incorrect behavior. Now that the Watch does not
error in the this case, it will result in an incorrectly ""resolved""
alert.  The fix here is two parts a) fix the exception b) fix the
following incorrect behavior.

a) fixing the exception is as easy as checking the size of the
array before accessing it.

b) fixing the following incorrect behavior is a bit more intrusive

- Note - the UI depends on the success/met state for each condition
to determine an ""OK"" or ""FIRING""

In this scenario, where an unhealthy cluster triggers an alert and
then goes silent, it should keep ""FIRING"" until it hears back that
the cluster is green. To keep the Watch ""FIRING"" either the index
action or the email action needs to fire. Since the Watch is neither
a ""new"" alert or a ""resolved"" alert, we do not want to keep sending
an email (that would be non-passive too). Without completely changing
the logic of how an alert is resolved allowing the index action to
take place would result in the alert being resolved. Since we can
not keep ""FIRING"" either the email or index action (since we don't
want to resolve the alert nor re-write the logic for alert resolution),
we will introduce a 3rd action. A logging action that WILL fire when
the cluster is unhealthy. Specifically will fire when there is an
unresolved alert and it can not find the cluster state.
This logging action is logged at debug, so it should be noticed much.
This logging action serves as an 'anchor' for the UI to keep the state
in an a ""FIRING"" status until the alert is resolved.

This presents a possible scenario where a cluster starts firing,
then goes completely silent forever, the Watch will be ""FIRING""
forever. This is an edge case that already exists in some scenarios
and requires manual intervention to remove that Watch.

This changes changes to use a template-like method to populate the 
version_created for the default monitoring watches. The version is 
set to 7.5 since that is where this is first introduced.

Fixes #43184




",Buggy,"The commit message describes a bug fix for a cluster alert system related to an `IndexOutOfBoundsException`. The problem occurs when a cluster stops sending data, which leads to an exception that stops the current Watch. The fix aims to achieve two main objectives:

1. **Fix the Exception**: It states that simply fixing the exception by checking the size before accessing the array would resolve the exception itself. However, the provided diff does not display this specific part of the change, such as checking array sizes before access.

2. **Fix the Incorrect Behavior**: The message outlines the introduction of a logging action to maintain the alert in a ""FIRING"" status if the cluster remains unhealthy, preventing premature resolution of an alert due to the lack of new data.

Looking at the diff provided:

- The core change appears inside the `loadWatch` method, where the `source` string template for alert monitoring watches is modified. A new line is added to replace the `VERSION_CREATED_PROPERTY` with a version identifier, indicating this is when the modification is first introduced. It uses a `LAST_UPDATED_VERSION` constant to populate this field, which aligns with ensuring that the correct version metadata is included when processing watches.

The combined context of the commit message with its detailed explanation and the modifications made in the diff suggests a structured approach to preventing errors and mitigating subsequent incorrect behavior due to an unhealthy cluster. The newly introduced logging action, though not part of this exact diff, shows an effort to handle existing alerts appropriately regardless of whether further cluster data is received.

Therefore, considering the objectives explained and the nature of the changes contributing to both error prevention and behavioral correction, the conclusion is **Buggy**. This is because there is intent to both fix an error condition (avoiding `IndexOutOfBoundsException`) and handle resulting behavioral implications within the alerting system."
elasticsearch,61886.json,aed30d6cc7761d12b929696774591ac0ccb989d0,"@@ -1,32 +1,32 @@
     public void checkInvalidPatterns() throws IOException {
         Pattern allPatterns = Pattern.compile(""("" + String.join("")|("", getPatterns().values()) + "")"");
         List<String> failures = new ArrayList<>();
         for (File f : files()) {
             List<String> lines;
             try(Stream<String> stream = Files.lines(f.toPath(), StandardCharsets.UTF_8)) {
                     lines = stream.collect(Collectors.toList());
             } catch (UncheckedIOException e) {
                 throw new IllegalArgumentException(""Failed to read "" + f + "" as UTF_8"", e);
             }
             List<Integer> invalidLines = IntStream.range(0, lines.size())
                 .filter(i -> allPatterns.matcher(lines.get(i)).find())
                 .boxed()
                 .collect(Collectors.toList());
 
             String path = getProject().getRootProject().getProjectDir().toURI().relativize(f.toURI()).toString();
-            failures = invalidLines.stream()
+            failures.addAll(invalidLines.stream()
                 .map(l -> new AbstractMap.SimpleEntry<>(l+1, lines.get(l)))
                 .flatMap(kv -> patterns.entrySet().stream()
                     .filter(p -> Pattern.compile(p.getValue()).matcher(kv.getValue()).find())
                     .map(p -> ""- "" + p.getKey() + "" on line "" + kv.getKey() + "" of "" + path)
                 )
-                .collect(Collectors.toList());
+                .collect(Collectors.toList()));
         }
         if (failures.isEmpty() == false) {
             throw new GradleException(""Found invalid patterns:\n"" + String.join(""\n"", failures));
         }
 
         File outputMarker = getOutputMarker();
         outputMarker.getParentFile().mkdirs();
         Files.write(outputMarker.toPath(), ""done"".getBytes(StandardCharsets.UTF_8));
     }",Buggy,"Don't replace forbidden pattern failures when found (#40710)

This commit fixes a bug in forbidden patterns where the failures for a
file replace the failures from the previous files instead of extending
them.
",Buggy,"The commit message indicates a bug fix regarding how failures are managed across files. In the original code, the `failures` list was replaced for each file, so errors detected in one file would overwrite those found earlier. The change involves using `addAll` to extend the `failures` list instead of replacing it, allowing the code to accumulate all detected failures across multiple files. This aligns with the commit message describing a bug in handling forbidden pattern failures across files. The change clearly addresses this issue, confirming it as a bug fix.

Therefore, the answer is **Buggy**."
elasticsearch,39934.json,8260138e5975ebcb588933d792d22374168c48cf,"@@ -1,21 +1,21 @@
         void toXContent(XContentBuilder builder, Params params, boolean keyed, @Nullable ValueFormatter formatter) throws IOException {
-            if (formatter != null) {
+            if (formatter != null && formatter != ValueFormatter.RAW) {
                 Text keyTxt = new StringText(formatter.format(key));
                 if (keyed) {
                     builder.startObject(keyTxt.string());
                 } else {
                     builder.startObject();
                 }
                 builder.field(CommonFields.KEY_AS_STRING, keyTxt);
             } else {
                 if (keyed) {
                     builder.startObject(String.valueOf(getKeyAsNumber()));
                 } else {
                     builder.startObject();
                 }
             }
             builder.field(CommonFields.KEY, key);
             builder.field(CommonFields.DOC_COUNT, docCount);
             aggregations.toXContentInternal(builder, params);
             builder.endObject();
         }",Buggy,"Aggregations: Fixed Histogram key_as_string bug

The key as string field in the response for the histogram aggregation will now only show if format is specified on the request.

Closes #6655
",Buggy,"The commit message indicates a fix for a bug related to displaying ""key_as_string"" only when a specific format is provided in a histogram aggregation response.

Upon analyzing the diff:

1. The code now includes a condition to check if `formatter` is not equal to `ValueFormatter.RAW` in addition to checking if `formatter` is not null.
2. This ensures that the ""key_as_string"" will only be included in the output when a format is explicitly specified and is not the RAW format, aligning with the commit message.

The change addresses the problem described in the commit message by controlling the output based on the provided formatter, thus fixing the bug as intended. Therefore, the commit is a clear bug fix.

The conclusion is **Buggy**."
elasticsearch,8075.json,ae4bfe99ecd6e298c56236c7abf9a53c6b490ab2,"@@ -1,30 +1,31 @@
     protected void masterOperation(Task task, DeleteEnrichPolicyAction.Request request, ClusterState state,
                                    ActionListener<AcknowledgedResponse> listener) throws Exception {
         List<PipelineConfiguration> pipelines = IngestService.getPipelines(state);
         EnrichPolicy policy = EnrichStore.getPolicy(request.getName(), state);
         List<String> pipelinesWithProcessors = new ArrayList<>();
 
         for (PipelineConfiguration pipelineConfiguration : pipelines) {
             List<AbstractEnrichProcessor> enrichProcessors =
                 ingestService.getProcessorsInPipeline(pipelineConfiguration.getId(), AbstractEnrichProcessor.class);
             for (AbstractEnrichProcessor processor: enrichProcessors) {
                 if (processor.getPolicyName().equals(request.getName())) {
                     pipelinesWithProcessors.add(pipelineConfiguration.getId());
                 }
             }
         }
 
         if (pipelinesWithProcessors.isEmpty() == false) {
             listener.onFailure(
                 new ElasticsearchStatusException(""Could not delete policy [{}] because a pipeline is referencing it {}"",
                     RestStatus.CONFLICT, request.getName(), pipelinesWithProcessors));
+            return;
         }
 
         EnrichStore.deletePolicy(request.getName(), clusterService, e -> {
            if (e == null) {
                listener.onResponse(new AcknowledgedResponse(true));
            } else {
                listener.onFailure(e);
            }
         });
     }",Buggy,"Fix policy removal bug in delete policy (#45573)

The delete policy had a subtle bug in that it would still delete the
policy if pipelines were accessing it, after giving the client back an
error. This commit fixes that and ensures it does not happen by adding
verification in the test.",Buggy,"The commit message explains a bug in the delete policy operation where a policy could be deleted despite being referenced by pipelines, although an error was returned. The code change introduces a `return` statement after notifying the listener about the referencing pipelines and their IDs, effectively preventing the deletion from being carried out if the policy is in use.

This modification directly addresses the mentioned bug: it stops execution of the policy deletion code when pipelines still depend on the policy. Without this return, the code would continue to execute `EnrichStore.deletePolicy`, inadvertently allowing the policy's deletion despite active references.

Additionally, the commit message mentions that tests have been added to verify this behavior, reinforcing that the code will not remove policies still in use by pipelines.

Given the explanation above, this change aligns with the commit message and clearly resolves a bug related to policy deletion logic. Therefore, the assessment is **Buggy**."
elasticsearch,51449.json,63fe3c6ed604d2aee3ae5266b35e87f35ff09bee,"@@ -1,40 +1,44 @@
     private void verifyThenSubmitUpdate(ClusterRerouteRequest request, ActionListener<ClusterRerouteResponse> listener,
         Map<String, List<AbstractAllocateAllocationCommand>> stalePrimaryAllocations) {
         transportService.sendRequest(transportService.getLocalNode(), IndicesShardStoresAction.NAME,
             new IndicesShardStoresRequest().indices(stalePrimaryAllocations.keySet().toArray(Strings.EMPTY_ARRAY)),
             new ActionListenerResponseHandler<>(
                 ActionListener.wrap(
                     response -> {
                         ImmutableOpenMap<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> status =
                             response.getStoreStatuses();
                         Exception e = null;
                         for (Map.Entry<String, List<AbstractAllocateAllocationCommand>> entry : stalePrimaryAllocations.entrySet()) {
                             final String index = entry.getKey();
                             final ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>> indexStatus = status.get(index);
-                            assert indexStatus != null;
+                            if (indexStatus == null) {
+                                // The index in the stale primary allocation request was green and hence filtered out by the store status
+                                // request. We ignore it here since the relevant exception will be thrown by the reroute action later on.
+                                continue;
+                            }
                             for (AbstractAllocateAllocationCommand command : entry.getValue()) {
                                 final List<IndicesShardStoresResponse.StoreStatus> shardStatus =
                                     indexStatus.get(command.shardId());
                                 if (shardStatus == null || shardStatus.isEmpty()) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on any node"")
                                     );
                                 } else if (shardStatus.stream().noneMatch(storeStatus -> {
                                     final DiscoveryNode node = storeStatus.getNode();
                                     final String nodeInCommand = command.node();
                                     return nodeInCommand.equals(node.getName()) || nodeInCommand.equals(node.getId());
                                 })) {
                                     e = ExceptionsHelper.useOrSuppress(e, new IllegalArgumentException(
                                         ""No data for shard ["" + command.shardId() + ""] of index ["" + index + ""] found on node [""
                                             + command.node() + ']'));
                                 }
                             }
                         }
                         if (e == null) {
                             submitStateUpdate(request, listener);
                         } else {
                             listener.onFailure(e);
                         }
                     }, listener::onFailure
                 ), IndicesShardStoresResponse::new));
     }",Buggy,"Fix PrimaryAllocationIT Race Condition (#37355)

* Fix PrimaryAllocationIT Race Condition

* Forcing a stale primary allocation on a green index was tripping the assertion that was removed
   * Added a test that this case still errors out correctly
* Made the ability to wipe stopped datanode's data public on the internal test cluster and used it to ensure correct behaviour on the fixed test
   * Previously it simply passed because the test finished before the index went green and would NPE when the index was green at the time of the shard store status request, that would then come up empty
* Closes #37345
",Buggy,"The provided git diff modifies a method involved in handling shard store statuses and addressing race conditions. Let's analyze the changes step-by-step:

1. **Context and Purpose**: The method `verifyThenSubmitUpdate` checks and processes cluster reroute requests, focusing on primary shard allocations. This is crucial in ensuring data integrity and proper recovery mechanisms in distributed systems.

2. **Original Setup**: 
   - The original code asserts that `indexStatus` is not null. This indicates a strong assumption that the index in the request should always have a corresponding, non-null status.

3. **Changes**:
   - The assertion `assert indexStatus != null;` is replaced with an `if` check that filters out null `indexStatus` by continuing the loop when encountered.
   - A comment explains the reasoning: if `indexStatus` is null, it means the index was green (healthy) and thus excluded from the store status request.

4. **Error Handling**: 
   - By omitting the assertion and adding the conditional check, the method gracefully handles cases where the index may already be healthy and might not need further actions. It continues processing without raising an exception.
   - The commit message highlights that an exception will be thrown later in the reroute action if necessary, indicating that error handling related to such conditions is deferred appropriately.

5. **Race Condition Fix**:
   - The modification addresses a race condition where stale primary allocations for green indices previously led to premature assertions.
   - By altering this behavior, the change ensures that the system operates predictably, even when indices transition to a green state unexpectedly mid-operation.

6. **Supporting Changes**: 
   - The commit message notes the addition of a test to verify that errors occur correctly when relevant.
   - There are additional modifications mentioned (though not appearing in this diff) to improve testing and ensure correct behavior.

7. **Conclusion**:
   - The changes specifically address a bug related to race conditions in `PrimaryAllocationIT`. This makes the changes highly relevant to the described bug fix.
   - It enhances fault tolerance and system reliability by refining how exceptions and special cases are managed.

The changes in the git diff and the commit message indicate a clear fix for a bug related to a race condition. Therefore, the conclusion is **Buggy**."
elasticsearch,47709.json,049e122e8f653fb54f12e0f0aa1eb86c5a1a629d,"@@ -1,18 +1,18 @@
     public static Map<Integer, List<String>> getAutoExpandReplicaChanges(MetaData metaData, DiscoveryNodes discoveryNodes) {
         // used for translating ""all"" to a number
         final int dataNodeCount = discoveryNodes.getDataNodes().size();
 
         Map<Integer, List<String>> nrReplicasChanged = new HashMap<>();
 
         for (final IndexMetaData indexMetaData : metaData) {
-            if (indexMetaData.getState() != IndexMetaData.State.CLOSE) {
+            if (indexMetaData.getState() == IndexMetaData.State.OPEN || isIndexVerifiedBeforeClosed(indexMetaData)) {
                 AutoExpandReplicas autoExpandReplicas = SETTING.get(indexMetaData.getSettings());
                 autoExpandReplicas.getDesiredNumberOfReplicas(dataNodeCount).ifPresent(numberOfReplicas -> {
                     if (numberOfReplicas != indexMetaData.getNumberOfReplicas()) {
                         nrReplicasChanged.computeIfAbsent(numberOfReplicas, ArrayList::new).add(indexMetaData.getIndex().getName());
                     }
                 });
             }
         }
         return nrReplicasChanged;
     }",Buggy,"Auto-expand replicated closed indices (#48973)

Fixes a bug where replicated closed indices were not being auto-expanded.",Buggy,"The commit message mentions a bug fix related to auto-expanding replicated closed indices. Let's analyze the code:

1. **Understanding the Change**: Originally, indices in a closed state (`IndexMetaData.State.CLOSE`) were excluded from the logic by checking `if (indexMetaData.getState() != IndexMetaData.State.CLOSE)`. The change alters this logic by allowing indices to pass through if they are open (`IndexMetaData.State.OPEN`) or meet a new condition: `isIndexVerifiedBeforeClosed(indexMetaData)`.

2. **Identifying the Bug Fix**: The core of the bug appears to be that indices, even if intended for replication, were skipped entirely when they were closed. This change corrects that oversight by allowing certain closed (but verified) indices to also be considered for replica adjustments.

3. **Contextual Understanding**: The function aims to adjust the number of replicas for indices based on auto-expansion settings. By improperly excluding certain indices, the code wasn't fully functioning as required.

4. **Evaluation Against the Commit Message**: The commit message explicitly states a bug fix regarding the auto-expansion of replicated closed indices, which aligns perfectly with the change in the code logic.

Given these points, the changes fulfill the commit message by addressing the described bug related to closed indices and their replication settings. The conclusion is **Buggy** because the changes reflect a genuine bug fix."
elasticsearch,46897.json,cdb482eaae0517f202efdfaf445e8847917182f1,"@@ -1,9 +1,13 @@
     boolean cancelCommittedPublication() {
         synchronized (mutex) {
-            if (currentPublication.isPresent() && currentPublication.get().isCommitted()) {
-                currentPublication.get().cancel(""cancelCommittedPublication"");
-                return true;
+            if (currentPublication.isPresent()) {
+                final CoordinatorPublication publication = currentPublication.get();
+                if (publication.isCommitted()) {
+                    publication.cancel(""cancelCommittedPublication"");
+                    logger.debug(""Cancelled publication of [{}]."", publication);
+                    return true;
+                }
             }
             return false;
         }
     }",Buggy,"Fix RareClusterStateIT (#42430)

* It looks like we might be cancelling a previous publication instead of
the one triggered by the given request with a very low likelihood.
   * Fixed by adding a wait for no in-progress publications
   * Also added debug logging that would've identified this problem
* Closes #36813",Buggy,"The commit message indicates a bug fix related to cancelling a publication under specific conditions and adding debug logging to aid future diagnosis.

Analyzing the code changes:
1. **Logic Adjustment**: The original code cancelled the current publication if it was both present and committed, then returned `true`. The revised version extends this by separating the check for `isPresent()` from `isCommitted()`, using a local variable `publication` to store the current publication retrieved from `currentPublication.get()`. There’s no change in the cancellation condition, but the code is slightly clearer and more readable. This might help reduce the chance of misleading results due to potential concurrent modifications or side effects that aren't explicit in this snippet.

2. **Debug Logging**: Added a debug log statement to give visibility when a publication is canceled. This logging would have been useful for diagnosing issues related to publication management, as stated in the commit message.

3. **Concurrency Handling**: The statement ""adding a wait for no in-progress publications"" is mentioned in the commit message but not reflected in this code diff. It might refer to changes outside this snippet, or ""wait"" might be implicitly handled by surrounding code, like additional synchronization logic not present here. 

4. **Consistency with Message**: The commit message focuses on ensuring the correct operation of publication cancellation when conditions meet, aligning with the purpose of the revised logic and introducing logging as a measure to catch any misalignment of expectations during execution.

The improvements aim to enhance reliability and troubleshootability of the publication process, and the commit message reflects these changes accurately.

Thus, given these updates serve to fix logical and debugging issues with publication cancellations, this change aligns with the commit message's indication of a bug fix, supporting the decision of fixing a potentially rare bug or race condition.

Therefore, the answer is **Buggy**."
elasticsearch,19038.json,7597b7ce2bd280401fcbfbeb281dfbb205830d75,"@@ -1,34 +1,38 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        for (ApplicationPrivilegeDescriptor privilege : privileges) {
-            try {
-                ApplicationPrivilege.validateApplicationName(privilege.getApplication());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            try {
-                ApplicationPrivilege.validatePrivilegeName(privilege.getName());
-            } catch (IllegalArgumentException e) {
-                validationException = addValidationError(e.getMessage(), validationException);
-            }
-            if (privilege.getActions().isEmpty()) {
-                validationException = addValidationError(""Application privileges must have at least one action"", validationException);
-            }
-            for (String action : privilege.getActions()) {
-                if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
-                    validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
-                        validationException);
-                }
+        if (privileges.isEmpty()) {
+            validationException = addValidationError(""At least one application privilege must be provided"", validationException);
+        } else {
+            for (ApplicationPrivilegeDescriptor privilege : privileges) {
                 try {
-                    ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    ApplicationPrivilege.validateApplicationName(privilege.getApplication());
                 } catch (IllegalArgumentException e) {
                     validationException = addValidationError(e.getMessage(), validationException);
                 }
-            }
-            if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
-                validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
-                    + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                try {
+                    ApplicationPrivilege.validatePrivilegeName(privilege.getName());
+                } catch (IllegalArgumentException e) {
+                    validationException = addValidationError(e.getMessage(), validationException);
+                }
+                if (privilege.getActions().isEmpty()) {
+                    validationException = addValidationError(""Application privileges must have at least one action"", validationException);
+                }
+                for (String action : privilege.getActions()) {
+                    if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) {
+                        validationException = addValidationError(""action ["" + action + ""] must contain one of [ '/' , '*' , ':' ]"",
+                            validationException);
+                    }
+                    try {
+                        ApplicationPrivilege.validatePrivilegeOrActionName(action);
+                    } catch (IllegalArgumentException e) {
+                        validationException = addValidationError(e.getMessage(), validationException);
+                    }
+                }
+                if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) {
+                    validationException = addValidationError(""metadata keys may not start with ["" + MetadataUtils.RESERVED_PREFIX
+                        + ""] (in privilege "" + privilege.getApplication() + ' ' + privilege.getName() + "")"", validationException);
+                }
             }
         }
         return validationException;
     }",Buggy,"Add validation for empty PutPrivilegeRequest (#37569)

Return an error to the user if the put privilege api is called with
an empty body (no privileges)

Resolves: #37561",Buggy,"The commit message indicates that the change adds validation to ensure that a PutPrivilegeRequest is not empty. The provided diff aligns with this message as it introduces a check to see if the `privileges` list is empty. If it is, a validation error is added stating, ""At least one application privilege must be provided."" This addition directly addresses the situation where the request body is empty (i.e., no privileges are included in the request), which is explicitly mentioned in the commit message.

This modification clearly improves the validation process by preventing an empty request from being processed, which would have otherwise led to an unintended behavior or error downstream. As such, this change aligns with the commit message describing it as a fix to ensure accurate application input handling.

The modifications also include re-arranging the existing validation loop within the conditional block that checks for non-empty privileges. This ensures that further validation occurs only when there are privileges to validate, adding to the robustness of the system.

Overall, this change is accurately described by the commit message and indicates a fix to incorrect behavior by handling an empty input scenario. Thus, the correct conclusion is **Buggy**."
elasticsearch,49943.json,adc195e30c4e237372f168086f7a57b0c3b5e7f1,"@@ -1,10 +1,10 @@
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
         if (name == null) {
             validationException = addValidationError(""name is missing"", validationException);
         }
         if (indexPatterns == null || indexPatterns.size() == 0) {
-            validationException = addValidationError(""pattern is missing"", validationException);
+            validationException = addValidationError(""index patterns are missing"", validationException);
         }
         return validationException;
     }",Buggy,"Fix error message for a put index template request without index_patterns (#27102)

Just correct the error message from ""Validation Failed: 1: pattern is
missing;"" to ""Validation Failed: 1: index_patterns is missing;"".

Closes #27100",NotBuggy,"The commit message indicates that the purpose of the change is to correct an error message for a put index template request. The code change aligns with this objective by modifying the error message text from ""pattern is missing"" to ""index patterns are missing."" This change addresses the discrepancy in the error message, ensuring that it accurately reflects the condition being validated.

Since the change is purely related to modifying an error message and does not affect the program's logical flow or error-handling behavior, it does not fix a code bug but rather corrects a user-facing message for clarity.

Therefore, the code change is accurately described by the commit message, but it does not indicate a bug fix in the functionality of the code. It is a correction to improve message accuracy.

The conclusion is **NotBuggy**."
elasticsearch,53233.json,a8bfa466b2f2a68c2384bc730a0b8c7c9ce7ea87,"@@ -1,12 +1,12 @@
     public void markAsCompleted(BulkItemResponse translatedResponse) {
         assertInvariants(ItemProcessingState.EXECUTED);
-        assert executionResult == null || translatedResponse.getItemId() == executionResult.getItemId();
+        assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();
         assert translatedResponse.getItemId() == getCurrentItem().id();
 
-        if (translatedResponse.isFailed() == false && requestToExecute != getCurrent())  {
+        if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent())  {
             request.items()[currentIndex] = new BulkItemRequest(request.items()[currentIndex].id(), requestToExecute);
         }
         getCurrentItem().setPrimaryResponse(translatedResponse);
         currentItemState = ItemProcessingState.COMPLETED;
         advance();
     }",Buggy,"Fix NOOP bulk updates (#32819)

#31821 introduced an unreleased bug where NOOP updates were incorrectly mutating the bulk
shard request, inserting null item to be replicated, which would result in NullPointerExceptions when
serializing the request to be shipped to the replicas.

Closes #32808",Buggy,"The commit message describes a bug where NOOP updates incorrectly mutated a bulk shard request, causing potential `NullPointerException`s. Let's analyze the changes in the code:

1. An assertion previously allowed `executionResult` to be `null`, but now it requires `executionResult` to be non-null, ensuring that there is always a valid execution result when marking an item as completed. This avoids potential logic errors or null pointer exceptions due to missing execution details.

2. In the conditional check for setting a `BulkItemRequest`, a null check `requestToExecute != null` is now added. This prevents the assignment of a `null` request execution, which could lead to `NullPointerExceptions` when serializing a request with `null` items to be replicated to shards.

These adjustments directly prevent the possibility of `NullPointerExceptions` which the commit message described as the underlying bug when serializing requests. Thus, the code changes are consistent with the commit message and address the specified bug by adding necessary null checks and assertions.

Therefore, the changes align with the bug fix described in the commit message. The conclusion is **Buggy**."
elasticsearch,265.json,c9dc55c3322b631f708867fdfd650e5ca27a30d0,"@@ -1,18 +1,20 @@
     static Tuple<Set<String>, Set<String>> findTasksWithoutConfig(ClusterState state, String transformId) {
         PersistentTasksCustomMetaData tasks = state.metaData().custom(PersistentTasksCustomMetaData.TYPE);
 
         Set<String> taskIds = new HashSet<>();
         Set<String> executorNodes = new HashSet<>();
 
-        Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
-            TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
-            return Regex.simpleMatch(transformId, transformParams.getId());
-        };
+        if (tasks != null) {
+            Predicate<PersistentTask<?>> taskMatcher = Strings.isAllOrWildcard(new String[] { transformId }) ? t -> true : t -> {
+                TransformTaskParams transformParams = (TransformTaskParams) t.getParams();
+                return Regex.simpleMatch(transformId, transformParams.getId());
+            };
 
-        for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
-            executorNodes.add(pTask.getExecutorNode());
-            taskIds.add(pTask.getId());
+            for (PersistentTasksCustomMetaData.PersistentTask<?> pTask : tasks.findTasks(TransformField.TASK_NAME, taskMatcher)) {
+                executorNodes.add(pTask.getExecutorNode());
+                taskIds.add(pTask.getId());
+            }
         }
 
         return new Tuple<>(taskIds, executorNodes);
     }",Buggy,"check custom meta data to avoid NPE (#51163)

check custom meta data to avoid NPE, fixes a problem introduced in #51072

fixes #51153",Buggy,"The commit message indicates a fix is being implemented to prevent NullPointerException (NPE). The diff wraps the code block with a null-check on `tasks`, which avoids dereferencing a null reference. Without this check, invoking methods on a null `tasks` object would cause an NPE. This modification aligns with the commit message, directly addressing a potential bug related to null metadata in the program logic. The change prevents execution when `tasks` is null, which avoids crashes and makes the code more robust.

Hence, this commit indeed represents a bug fix. Consequently, the assessment of this diff and commit message is **Buggy**."
elasticsearch,7551.json,341006e9913e831408f5bbc7f8ad8c453a7f630e,"@@ -1,91 +1,91 @@
     private static List<EsIndex> buildIndices(String[] indexNames, String javaRegex, Map<String, Map<String, FieldCapabilities>> fieldCaps,
             Function<String, String> indexNameProcessor,
             BiFunction<String, Map<String, FieldCapabilities>, InvalidMappedField> validityVerifier) {
 
         if (indexNames == null || indexNames.length == 0) {
             return emptyList();
         }
 
         final List<String> resolvedIndices = asList(indexNames);
         Map<String, Fields> indices = new LinkedHashMap<>(resolvedIndices.size());
         Pattern pattern = javaRegex != null ? Pattern.compile(javaRegex) : null;
 
         // sort fields in reverse order to build the field hierarchy
         Set<Entry<String, Map<String, FieldCapabilities>>> sortedFields = new TreeSet<>(
                 Collections.reverseOrder(Comparator.comparing(Entry::getKey)));
 
         sortedFields.addAll(fieldCaps.entrySet());
 
         for (Entry<String, Map<String, FieldCapabilities>> entry : sortedFields) {
             String fieldName = entry.getKey();
             Map<String, FieldCapabilities> types = entry.getValue();
 
             // ignore size added by the mapper plugin
             if (FIELD_NAMES_BLACKLIST.contains(fieldName)) {
                 continue;
             }
 
             // apply verification
             final InvalidMappedField invalidField = validityVerifier.apply(fieldName, types);
 
             // filter meta fields and unmapped
             FieldCapabilities unmapped = types.get(UNMAPPED);
             Set<String> unmappedIndices = unmapped != null ? new HashSet<>(asList(unmapped.indices())) : emptySet();
 
             // check each type
             for (Entry<String, FieldCapabilities> typeEntry : types.entrySet()) {
                 FieldCapabilities typeCap = typeEntry.getValue();
                 String[] capIndices = typeCap.indices();
 
                 // Skip internal fields (name starting with underscore and its type reported by field_caps starts
                 // with underscore as well). A meta field named ""_version"", for example, has the type named ""_version"".
                 if (typeEntry.getKey().startsWith(""_"") && typeCap.getType().startsWith(""_"")) {
                     continue;
                 }
 
                 // compute the actual indices - if any are specified, take into account the unmapped indices
                 List<String> concreteIndices = null;
                 if (capIndices != null) {
                     if (unmappedIndices.isEmpty() == true) {
                         concreteIndices = asList(capIndices);
                     } else {
-                        concreteIndices = new ArrayList<>(capIndices.length - unmappedIndices.size() + 1);
+                        concreteIndices = new ArrayList<>(capIndices.length);
                         for (String capIndex : capIndices) {
                             // add only indices that have a mapping
                             if (unmappedIndices.contains(capIndex) == false) {
                                 concreteIndices.add(capIndex);
                             }
                         }
                     }
                 } else {
                     concreteIndices = resolvedIndices;
                 }
 
                 // put the field in their respective mappings
                 for (String index : concreteIndices) {
                     if (pattern == null || pattern.matcher(index).matches()) {
                         String indexName = indexNameProcessor.apply(index);
                         Fields indexFields = indices.get(indexName);
                         if (indexFields == null) {
                             indexFields = new Fields();
                             indices.put(indexName, indexFields);
                         }
                         EsField field = indexFields.flattedMapping.get(fieldName);
                         if (field == null || (invalidField != null && (field instanceof InvalidMappedField) == false)) {
                             createField(fieldName, fieldCaps, indexFields.hierarchicalMapping, indexFields.flattedMapping,
                                     s -> invalidField != null ? invalidField : createField(s, typeCap.getType(), emptyMap(),
                                             typeCap.isAggregatable()));
                         }
                     }
                 }
             }
         }
 
         // return indices in ascending order
         List<EsIndex> foundIndices = new ArrayList<>(indices.size());
         for (Entry<String, Fields> entry : indices.entrySet()) {
             foundIndices.add(new EsIndex(entry.getKey(), entry.getValue().hierarchicalMapping));
         }
         foundIndices.sort(Comparator.comparing(EsIndex::name));
         return foundIndices;
     }",Buggy,"SQL: concrete indices array size bug fix (#43878)

* The created array didn't have the correct initial size while attempting to resolve multiple indices
",Buggy,"The commit message indicates a bug fix related to the creation of an array with the incorrect initial size when resolving multiple indices.

Upon analyzing the diff, we can see the following change:

- **Original Code**: The `ArrayList` was initialized with size `capIndices.length - unmappedIndices.size() + 1`.
- **Modified Code**: The `ArrayList` is now initialized with size `capIndices.length`.

Here's why this change is significant:

1. **Correct Size Initialization**: 
   - The original initialization seemed to attempt calculating a smaller initial size by subtracting the size of `unmappedIndices` and adding 1.
   - However, this calculation could result in an incorrect size for the list, possibly leading to performance inefficiencies or an array resizing operation.
   - By changing to `capIndices.length`, it ensures that the list has an initial capacity that can handle all elements in `capIndices` if none are ""unmapped.""

2. **Logical Correction**:
   - The array should be capable of accommodating all potentially valid indices as per `capIndices`, and the initial capacity should reflect that.
   - This adjustment suggests that the previous initialization logic might have led to situations where the size was underestimated, causing logic errors during runtime when adding elements, especially when considering indices that were not unmapped.

The change relates directly to correcting the behavior in dealing with indices array creation, suggesting it addresses an error in processing. Hence, the code change is consistent with the commit message mentioning a bug fix. Consequently, this commit should be classified as **Buggy**."
elasticsearch,41046.json,0354825914ba79d7e3ec500aa9eab0532e43850f,"@@ -1,24 +1,22 @@
     private FieldLookup loadFieldData(String name) {
         FieldLookup data = cachedFieldData.get(name);
         if (data == null) {
             FieldMapper mapper = mapperService.smartNameFieldMapper(name, types);
             if (mapper == null) {
                 throw new ElasticSearchIllegalArgumentException(""No field found for ["" + name + ""] in mapping with types "" + Arrays.toString(types) + """");
             }
             data = new FieldLookup(mapper);
             cachedFieldData.put(name, data);
         }
         if (data.doc() == null) {
             fieldVisitor.name(data.mapper().names().indexName());
             try {
                 reader.document(docId, fieldVisitor);
                 // LUCENE 4 UPGRADE: Only one field we don't need document
                 data.doc(fieldVisitor.createDocument());
             } catch (IOException e) {
                 throw new ElasticSearchParseException(""failed to load field ["" + name + ""]"", e);
-            } finally {
-                fieldVisitor.reset();
             }
         }
         return data;
     }",Buggy,"lucene 4: Fixed compile error
",NotBuggy,"The commit message indicates that a compile error was fixed. Observing the code diff, the `finally` block containing `fieldVisitor.reset();` is removed. While this might avoid a compile-time issue caused by misusing or misplacing `reset()`, it doesn't inherently reflect a bug fix directly tied to runtime issues but rather resolves a potential misuse or non-compatibility with Lucene 4 updates.

Given this analysis, the change more accurately represents a resolution to a compile error rather than a bug during execution. The removal doesn't involve logical corrections, exception improvements, or clear error-handling updates (beyond potential compatibility or API usage adjustments).

Therefore, the commit is more about adjustment and removal of an unnecessary or incorrect statement for compilation purposes rather than a logical error fix. The conclusion is **NotBuggy**."
elasticsearch,11747.json,22415fa2de1d7d07cea7dd5e7263eb1ed4270503,"@@ -1,82 +1,88 @@
     CharsetMatch findCharset(List<String> explanation, InputStream inputStream) throws Exception {
 
         // We need an input stream that supports mark and reset, so wrap the argument
         // in a BufferedInputStream if it doesn't already support this feature
         if (inputStream.markSupported() == false) {
             inputStream = new BufferedInputStream(inputStream, BUFFER_SIZE);
         }
 
         // This is from ICU4J
         CharsetDetector charsetDetector = new CharsetDetector().setText(inputStream);
         CharsetMatch[] charsetMatches = charsetDetector.detectAll();
 
         // Determine some extra characteristics of the input to compensate for some deficiencies of ICU4J
         boolean pureAscii = true;
         boolean containsZeroBytes = false;
         inputStream.mark(BUFFER_SIZE);
         byte[] workspace = new byte[BUFFER_SIZE];
         int remainingLength = BUFFER_SIZE;
         do {
             int bytesRead = inputStream.read(workspace, 0, remainingLength);
             if (bytesRead <= 0) {
                 break;
             }
             for (int i = 0; i < bytesRead && containsZeroBytes == false; ++i) {
                 if (workspace[i] == 0) {
                     containsZeroBytes = true;
                     pureAscii = false;
                 } else {
                     pureAscii = pureAscii && workspace[i] > 0 && workspace[i] < 128;
                 }
             }
             remainingLength -= bytesRead;
         } while (containsZeroBytes == false && remainingLength > 0);
         inputStream.reset();
 
         if (pureAscii) {
             // If the input is pure ASCII then many single byte character sets will match.  We want to favour
             // UTF-8 in this case, as it avoids putting a bold declaration of a dubious character set choice
             // in the config files.
             Optional<CharsetMatch> utf8CharsetMatch = Arrays.stream(charsetMatches)
                 .filter(charsetMatch -> StandardCharsets.UTF_8.name().equals(charsetMatch.getName())).findFirst();
             if (utf8CharsetMatch.isPresent()) {
                 explanation.add(""Using character encoding ["" + StandardCharsets.UTF_8.name() +
                     ""], which matched the input with ["" + utf8CharsetMatch.get().getConfidence() + ""%] confidence - first ["" +
                     (BUFFER_SIZE / 1024) + ""kB] of input was pure ASCII"");
                 return utf8CharsetMatch.get();
             }
         }
 
         // Input wasn't pure ASCII, so use the best matching character set that's supported by both Java and Go.
         // Additionally, if the input contains zero bytes then avoid single byte character sets, as ICU4J will
         // suggest these for binary files but then
         for (CharsetMatch charsetMatch : charsetMatches) {
             String name = charsetMatch.getName();
             if (Charset.isSupported(name) && FILEBEAT_SUPPORTED_ENCODINGS.contains(name.toLowerCase(Locale.ROOT))) {
 
                 // This extra test is to avoid trying to read binary files as text.  Running the log config
                 // deduction algorithms on binary files is very slow as the binary files generally appear to
                 // have very long lines.
                 boolean spaceEncodingContainsZeroByte = false;
-                byte[] spaceBytes = "" "".getBytes(name);
-                for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
-                    spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                Charset charset = Charset.forName(name);
+                // Some character sets cannot be encoded.  These are extremely rare so it's likely that
+                // they've been chosen based on incorrectly provided binary data.  Therefore, err on
+                // the side of rejecting binary data.
+                if (charset.canEncode()) {
+                    byte[] spaceBytes = "" "".getBytes(charset);
+                    for (int i = 0; i < spaceBytes.length && spaceEncodingContainsZeroByte == false; ++i) {
+                        spaceEncodingContainsZeroByte = (spaceBytes[i] == 0);
+                    }
                 }
                 if (containsZeroBytes && spaceEncodingContainsZeroByte == false) {
                     explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                         ""%] confidence but was rejected as the input contains zero bytes and the ["" + name + ""] encoding does not"");
                 } else {
                     explanation.add(""Using character encoding ["" + name + ""], which matched the input with ["" +
                         charsetMatch.getConfidence() + ""%] confidence"");
                     return charsetMatch;
                 }
             } else {
                 explanation.add(""Character encoding ["" + name + ""] matched the input with ["" + charsetMatch.getConfidence() +
                     ""%] confidence but was rejected as it is not supported by ["" +
                     (Charset.isSupported(name) ? ""Filebeat"" : ""the JVM"") + ""]"");
             }
         }
 
         throw new IllegalArgumentException(""Could not determine a usable character encoding for the input"" +
             (containsZeroBytes ? "" - could it be binary data?"" : """"));
     }",Buggy,"[ML] Fix character set finder bug with unencodable charsets (#33234)

Some character sets cannot be encoded and this was tripping
up the binary data check in the ML log structure character
set finder.

The fix is to assume that if ICU4J identifies that some bytes
correspond to a character set that cannot be encoded and those
bytes contain zeroes then the data is binary rather than text.

Fixes #33227",Buggy,"The commit message indicates a fix for handling character sets that cannot be encoded, which complicates binary data detection in the character set finder. The changes in the diff directly address this issue:

1. **Character Encoding Check**: The updated code includes a condition to check if a character set can be encoded using `charset.canEncode()`. This check prevents attempting to work with character sets that ICU4J suggests, but are based on unencodable data indicative of binary, not textual, data.

2. **Binary Data Handling**: It adds logic erring on the side of detecting binary data if unencodable character sets are involved and the input contains zero bytes. This aligns with the message outlining the bug fix intent to handle cases where binary data masquerades as text due to incorrect charset suggestions.

These updates fix a specific bug related to character set detection and handling binary data correctly. Therefore, this commit and its changes are indeed a bug fix. 

Thus, the conclusion is **Buggy**."
elasticsearch,54093.json,b3341da0779673c106db526ec58c4449bda59dc2,"@@ -1,9 +1,13 @@
     private static boolean usingBundledJdk() {
         /*
          * We are using the bundled JDK if java.home is the jdk sub-directory of our working directory. This is because we always set
          * the working directory of Elasticsearch to home, and the bundled JDK is in the jdk sub-directory there.
          */
         final String javaHome = System.getProperty(""java.home"");
         final String userDir = System.getProperty(""user.dir"");
-        return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        if (Constants.MAC_OS_X) {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk/Contents/Home"").toAbsolutePath());
+        } else {
+            return PathUtils.get(javaHome).equals(PathUtils.get(userDir).resolve(""jdk"").toAbsolutePath());
+        }
     }",Buggy,"Fix bug in detecting use of bundled JDK on macOS

This commit fixes a bug in detecting the use of the bundled JDK on
macOS. This bug arose because the path of Java home is different on
macOS.
",Buggy,"The commit message states there's a bug related to detecting the bundled JDK on macOS. The code modification introduces a conditional check specifically for macOS using `Constants.MAC_OS_X`. On macOS, the path resolves differently (`jdk/Contents/Home`), while other systems use `jdk`. This change aligns the code with platform-specific configurations, effectively fixing the detection issue outlined in the commit message.

Thus, the code change addresses the described bug. Concluding, this change is indeed a bug fix. The answer is **Buggy**."
elasticsearch,12667.json,7ae57d6e226bfc314ce31acc1a622fb0d111fa46,"@@ -1,36 +1,47 @@
     void refresh(PersistentTasksCustomMetaData persistentTasks, ActionListener<Void> onCompletion) {
 
         synchronized (fullRefreshCompletionListeners) {
             fullRefreshCompletionListeners.add(onCompletion);
             if (fullRefreshCompletionListeners.size() > 1) {
                 // A refresh is already in progress, so don't do another
                 return;
             }
         }
 
         ActionListener<Void> refreshComplete = ActionListener.wrap(aVoid -> {
             lastUpdateTime = Instant.now();
             synchronized (fullRefreshCompletionListeners) {
                 assert fullRefreshCompletionListeners.isEmpty() == false;
                 for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
                     listener.onResponse(null);
                 }
                 fullRefreshCompletionListeners.clear();
             }
-        }, onCompletion::onFailure);
+        },
+        e -> {
+            synchronized (fullRefreshCompletionListeners) {
+                assert fullRefreshCompletionListeners.isEmpty() == false;
+                for (ActionListener<Void> listener : fullRefreshCompletionListeners) {
+                    listener.onFailure(e);
+                }
+                // It's critical that we empty out the current listener list on
+                // error otherwise subsequent retries to refresh will be ignored
+                fullRefreshCompletionListeners.clear();
+            }
+        });
 
         // persistentTasks will be null if there's never been a persistent task created in this cluster
         if (persistentTasks == null) {
             refreshComplete.onResponse(null);
         } else {
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlDataFrameAnalyticsJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             ActionListener<Void> refreshDataFrameAnalyticsJobs =
                 ActionListener.wrap(aVoid -> refreshAllDataFrameAnalyticsJobTasks(mlDataFrameAnalyticsJobTasks, refreshComplete),
                     refreshComplete::onFailure);
 
             List<PersistentTasksCustomMetaData.PersistentTask<?>> mlAnomalyDetectorJobTasks = persistentTasks.tasks().stream()
                 .filter(task -> MlTasks.JOB_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList());
             iterateAnomalyDetectorJobTasks(mlAnomalyDetectorJobTasks.iterator(), refreshDataFrameAnalyticsJobs);
         }
     }",Buggy,"[ML] Fix ML memory tracker lockup when inner step fails (#44158)

When the ML memory tracker is refreshed and a refresh is
already in progress the idea is that the second and
subsequent refresh requests receive the same response as
the currently in progress refresh.

There was a bug that if a refresh failed then the ML
memory tracker's view of whether a refresh was in progress
was not reset, leading to every subsequent request being
registered to receive a response that would never come.

This change makes the ML memory tracker pass on failures
as well as successes to all interested parties and reset
the list of interested parties so that further refresh
attempts are possible after either a success or failure.

This fixes problem 1 of #44156",Buggy,"The commit message describes a bug where the ML memory tracker could become unresponsive if a refresh operation failed. The provided code diff shows the addition of proper failure handling to the refresh method:

1. **Handling onFailure:** The original code only invoked onFailure on the completion listener provided as an argument if the refresh failed. The updated code ensures that all completion listeners in `fullRefreshCompletionListeners` are notified of a failure.

2. **Clearing Listeners on Failure:** The change also clears the `fullRefreshCompletionListeners` list upon failure. This prevents the lockup situation described in the commit message, where subsequent refresh attempts would be ignored because the memory tracker's state incorrectly indicated a refresh was still in progress.

These changes correctly reflect the issue and solution described in the commit message. The locking issue would indeed cause the problem outlined, and ensuring that all listeners are informed of both successes and failures would resolve it. The fix directly handles both the logical error and state management issue outlined.

Thus, these changes definitely indicate a bug fix, and the appropriate conclusion is **Buggy**."
elasticsearch,39911.json,4735e0a9d3c7c89f34100e4fd47f3de25e56e1ed,"@@ -1,11 +1,14 @@
     public HistogramAggregationBuilder extendedBounds(double minBound, double maxBound) {
-        if (minBound == Double.NEGATIVE_INFINITY) {
-            throw new IllegalArgumentException(""minBound must not be -Infinity, got: "" + minBound);
+        if (Double.isFinite(minBound) == false) {
+            throw new IllegalArgumentException(""minBound must be finite, got: "" + minBound);
         }
-        if (maxBound == Double.POSITIVE_INFINITY) {
-            throw new IllegalArgumentException(""maxBound must not be +Infinity, got: "" + maxBound);
+        if (Double.isFinite(maxBound) == false) {
+            throw new IllegalArgumentException(""maxBound must be finite, got: "" + maxBound);
+        }
+        if (maxBound < minBound) {
+            throw new IllegalArgumentException(""maxBound ["" + maxBound + ""] must be greater than minBound ["" + minBound + ""]"");
         }
         this.minBound = minBound;
         this.maxBound = maxBound;
         return this;
     }",Buggy,"Throw exception when maxBounds greater than minBounds

The recent changes to the Histogram Aggregator introduced a bug where
an exception would not be thrown if the maxBound of the extended bounds
is less that the minBound. This change fixes that bug.

Closes #19833
",Buggy,"The commit message states an intention to address a logic error where an exception should be thrown if `maxBound` is less than `minBound`. The code diff reflects this by adding a check `if (maxBound < minBound)` that throws an `IllegalArgumentException` if the condition is met. This change directly addresses the described bug, enforcing the requirement that `maxBound` must be greater than or equal to `minBound`.

Additionally, the changes also improve existing validations by replacing checks for infinity to ensure both bounds are finite using `Double.isFinite()`, which further aligns with ensuring logical correctness.

The modifications correctly align with the commit message by addressing both the described bug and improving validation logic.

Given these observations, this commit is correctly classified as **Buggy** because it addresses a previously introduced bug related to bounds validation."
elasticsearch,45276.json,c5b6f52eccb2a663770e0eb9fee496432e1a8d5c,"@@ -1,21 +1,22 @@
-        private ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
+        ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
             for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> entry : shards) {
                 if (entry.value.state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.key.getIndex());
+                    final String indexName = entry.key.getIndexName();
+                    List<ShardId> waitingShards = waitingIndicesMap.get(indexName);
                     if (waitingShards == null) {
                         waitingShards = new ArrayList<>();
-                        waitingIndicesMap.put(entry.key.getIndexName(), waitingShards);
+                        waitingIndicesMap.put(indexName, waitingShards);
                     }
                     waitingShards.add(entry.key);
                 }
             }
             if (waitingIndicesMap.isEmpty()) {
                 return ImmutableOpenMap.of();
             }
             ImmutableOpenMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableOpenMap.builder();
             for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
                 waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
             }
             return waitingIndicesBuilder.build();
         }",Buggy,"Fixes maintaining the shards a snapshot is waiting on (#24289)

There was a bug in the calculation of the shards that a snapshot must
wait on, due to their relocating or initializing, before the snapshot
can proceed safely to snapshot the shard data.  In this bug, an
incorrect key was used to look up the index of the waiting shards,
resulting in the fact that each index would have at most one shard in
the waiting state causing the snapshot to pause.  This could be
problematic if there are more than one shard in the relocating or
initializing state, which would result in a snapshot prematurely
starting because it thinks its only waiting on one relocating or
initializing shard (when in fact there could be more than one).  While
not a common case and likely rare in practice, it is still problematic.

This commit fixes the issue by ensuring the correct key is used to look
up the waiting indices map as it is being built up, so the list of
waiting shards for each index (those shards that are relocating or
initializing) are aggregated for a given index instead of overwritten.
",Buggy,"The commit message describes an issue in tracking shards a snapshot waits for, which could lead to snapshots prematurely starting. The code fix involves using the correct index name when updating the `waitingIndicesMap`, ensuring the list of waiting shards for each index is aggregated and not overwritten. The change from `entry.key.getIndex()` to `entry.key.getIndexName()` directly addresses the described bug by correctly using the index name as the key.

Given the changes in the code and the detailed explanation in the commit message, the modifications align perfectly with a bug fix scenario. Therefore, the final assessment of this commit is **Buggy**."
elasticsearch,60460.json,953a99c75cde29a18db58abde3fdee720fcddc4f,"@@ -1,6 +1,6 @@
     public boolean isSame(StoreFileMetaData other) {
-        if (checksum != null && other.checksum != null) {
-            return checksum.equals(other.checksum);
+        if (checksum == null || other.checksum == null) {
+            return false;
         }
-        return length == other.length;
+        return length == other.length && checksum.equals(other.checksum);
     }",Buggy,"fix a bug in new checksum mechanism that caused for replicas not to retain the _checksums file. Also, now that checksums are widely used, consider files without checksums as ones that need to be recovered
",Buggy,"The commit message discusses a bug fix in the checksum mechanism regarding file retention. The code diff updates the isSame method, which compares file metadata based on checksum and file length.

### Analysis of Changes:
- **Previous Logic**: 
  - If both files have non-null checksums, only the checksums were compared.
  - If either checksum was null, the files were compared by length.

- **New Logic**:
  - If either checksum is null, the method immediately returns false.
  - If both checksums are non-null, it checks both the checksums and the file lengths for equality.

### Alignment with Commit Message:
- **Bug Fix**: The previous logic allowed files with missing checksums to be considered the same if their lengths were equal, potentially leading to data integrity issues. This aligns with the commit message regarding fixes around how checksum files are handled.
- **Checksum Consideration**: The update ensures that all files are checked for both length and checksum equality, enhancing the reliability of the checksum mechanism as outlined in the commit message.

The changes directly address the bug described and improve the checksum verification mechanism as intended. Therefore, this commit is a bug fix.

The answer is **Buggy**."
elasticsearch,52327.json,fe5af6f34d1204b6238c46c3aff7ea2a056eeb7a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""]"";
+        return ""shard ["" + (shardTarget == null ? ""_na"" : shardTarget) + ""], reason ["" + reason + ""], cause ["" + (cause == null ? ""_na"" : ExceptionsHelper.stackTrace(cause)) + ""]"";
     }",Buggy,"Tests: fix test bug (foo -> bar)
",NotBuggy,"The commit message states that a test bug is fixed, although this change is in a `toString()` method. The addition of cause details using ExceptionsHelper.stackTrace(cause) improves logging. Though helpful for debugging, it's unrelated to test cases or fixing bugs in tests. The change enhances information but not fix a test bug, misaligned with the commit message. Conclusion: **NotBuggy**."
elasticsearch,15357.json,368e5a1194c2d2f762c117dd84397bb65f835dec,"@@ -1,5 +1,6 @@
     public static boolean validate(ClusterState state) {
-        return state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) &&
+        return (state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME) ||
+            state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.HISTORY_TEMPLATE_NAME_NO_ILM)) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.TRIGGERED_TEMPLATE_NAME) &&
             state.getMetaData().getTemplates().containsKey(WatcherIndexTemplateRegistryField.WATCHES_TEMPLATE_NAME);
     }",Buggy,"fix unlikely bug that can prevent Watcher from restarting (#42030)

The bug fixed here is unlikely to happen. It requires ES to be started with
ILM disabled, Watcher enabled, and Watcher explicitly stopped and restarted.
Due to template validation Watcher does not fully start and can result in a
partially started state. This is an unlikely scenerio outside of the testing
framework.

Note - this bug was introduced while the test that would have caught it was
muted. The test remains muted since the underlying cuase of the random failures
has not been identified. When this test is un-muted it will now work.",Buggy,"The commit message describes a specific scenario that leads to a bug when Elasticsearch (ES) is started with certain configurations regarding Index Lifecycle Management (ILM) and the Watcher feature. The provided diff indicates a change in the validation logic within the `validate` method.

In the original code, the validation required the presence of specific templates, including `HISTORY_TEMPLATE_NAME`. The update allows for either `HISTORY_TEMPLATE_NAME` or `HISTORY_TEMPLATE_NAME_NO_ILM` to be present. This change addresses the bug where the absence of one specific template (`HISTORY_TEMPLATE_NAME`) would lead to validation failure and prevent the Watcher component from restarting correctly.

The condition now accommodates configurations where the ILM feature is disabled by also checking for `HISTORY_TEMPLATE_NAME_NO_ILM`, thereby preventing the described issue when ES is configured this way. The commit message indicates this is a rare bug but significant for specific configurations, particularly in a testing framework.

Therefore, this change directly correlates with the described bug fix. The answer is **Buggy**."
elasticsearch,53154.json,b7314c87211d99c45569ae0cdd8797636220b980,"@@ -1,11 +1,11 @@
     public String toString() {
         StringBuilder builder = new StringBuilder();
         builder.append(""IndexResponse["");
         builder.append(""index="").append(getIndex());
         builder.append("",type="").append(getType());
         builder.append("",id="").append(getId());
         builder.append("",version="").append(getVersion());
         builder.append("",result="").append(getResult().getLowercase());
-        builder.append("",shards="").append(getShardInfo());
+        builder.append("",shards="").append(Strings.toString(getShardInfo(), true));
         return builder.append(""]"").toString();
     }",Buggy,"fix IndexResponse#toString to print out shards info (#20562)

IndexResponse#toString method outputs an error caused by the shards object needing to be wrapped into another object. It is fixed by calling a different variant of Strings.toString(XContent) which accepts a second boolean argument that makes sure that a new object is created before outputting ShardInfo. I didn't change ShardInfo#toString directly as whether it needs a new object or not very much depends on where it is printed out. IndexResponse seemed a specific case as the rest of the info were not json, hence the shards object was the first one, but it is usually not the case.",Buggy,"The commit message addresses a specific issue in the `IndexResponse#toString` method regarding how shard information is output. The problem lies in the incorrect handling of the `ShardInfo` object, which needed to be wrapped or represented correctly in JSON format.

The diff changes the way the `ShardInfo` object is appended to the `StringBuilder`. Instead of using `getShardInfo()` directly, which could lead to formatting issues, the modified code uses `Strings.toString(getShardInfo(), true)`. This approach ensures that a new JSON object is created when converting `ShardInfo` to a string, which was the root of the error as described. This change directly addresses the issue mentioned in the commit message.

Additionally, the commit message provides reasoning for why `ShardInfo#toString` was not changed directly, noting that the behavior may vary depending on the context in which it is used, making `IndexResponse` a special case for this fix.

The changes in the code align with the commit message and fix the output issue by improving how `ShardInfo` is converted and displayed as part of the `IndexResponse#toString` method.

The assessment confirms that the changes are indeed a bug fix. Therefore, the appropriate conclusion is **Buggy**."
elasticsearch,45221.json,5c59b1206762df8461b7f827ce3922231250c49e,"@@ -1,33 +1,35 @@
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVInt(deletes.size());
             for (K delete : deletes) {
                 keySerializer.writeKey(delete, out);
             }
             Version version = out.getVersion();
             // filter out custom states not supported by the other node
             int diffCount = 0;
             for (Diff<T> diff : diffs.values()) {
                 if(valueSerializer.supportsVersion(diff, version)) {
                     diffCount++;
                 }
             }
             out.writeVInt(diffCount);
             for (Map.Entry<K, Diff<T>> entry : diffs.entrySet()) {
                 if(valueSerializer.supportsVersion(entry.getValue(), version)) {
                     keySerializer.writeKey(entry.getKey(), out);
                     valueSerializer.writeDiff(entry.getValue(), out);
                 }
             }
             // filter out custom states not supported by the other node
             int upsertsCount = 0;
             for (T upsert : upserts.values()) {
                 if(valueSerializer.supportsVersion(upsert, version)) {
                     upsertsCount++;
                 }
             }
             out.writeVInt(upsertsCount);
             for (Map.Entry<K, T> entry : upserts.entrySet()) {
-                keySerializer.writeKey(entry.getKey(), out);
-                valueSerializer.write(entry.getValue(), out);
+                if(valueSerializer.supportsVersion(entry.getValue(), version)) {
+                    keySerializer.writeKey(entry.getKey(), out);
+                    valueSerializer.write(entry.getValue(), out);
+                }
             }
         }",Buggy,"Fixing the custom object serialization bug in diffable utils. (#39544)

While serializing custom objects, the length of the list is computed after
filtering out the unsupported objects but while writing objects the filter
is not applied thus resulting in writing unsupported objects which will fail
to deserialize by the receiever. Adding the condition to filter out unsupported
custom objects.",Buggy,"The commit message indicates a fix for a serialization bug, where unsupported custom objects were being serialized and causing deserialization failures. Let's analyze the code changes:

1. The existing code writes a VInt for `deletes.size()` without any filtering, which seems unrelated to the given issue.
2. The `diffCount` is calculated and written after filtering with `valueSerializer.supportsVersion()`, which correctly applies only supported versions.
3. The key change occurs in the section handling `upserts`:
   - Originally, all entries in `upserts` were written without filtering.
   - The updated code now includes a conditional check `if(valueSerializer.supportsVersion(entry.getValue(), version))` before writing each entry, ensuring only supported objects are serialized.

This modification aligns with the commit message's focus on filtering unsupported objects during serialization to prevent deserialization issues. The conditions now consistently apply to both counting and writing operations.

Conclusively, the commit message and the code reflect a clear bug fix. The changes aim to correct serialization logic flaws by adding necessary checks to prevent unsupported objects from being serialized, thereby aligning with the described bug fix purpose.

Therefore, the assessment is **Buggy**."
elasticsearch,56477.json,1792bd6b160993aae2d925ad4b7c0663d14a9e82,"@@ -1,14 +1,6 @@
     public static IndexShardState fromId(byte id) throws ElasticSearchIllegalArgumentException {
-        if (id == 0) {
-            return CREATED;
-        } else if (id == 1) {
-            return RECOVERING;
-        } else if (id == 2) {
-            return STARTED;
-        } else if (id == 3) {
-            return RELOCATED;
-        } else if (id == 4) {
-            return CLOSED;
+        if (id < ORDS[0].id && id > ORDS[ORDS.length - 1].id) {
+            throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
         }
-        throw new ElasticSearchIllegalArgumentException(""No mapping for id ["" + id + ""]"");
+        return ORDS[id];
     }",Buggy,"Fixed serialization error. POST_RECOVERY is now also serialized
",Buggy,"The commit message states a fix for a serialization error and highlights the inclusion of POST_RECOVERY in serialization. In the diff, the provided code modifies the method that presumably returns an index state based on an ID value. Previously, the method explicitly checked ranges of IDs, returning specific constants. The updated code uses an array, ORDS, to map IDs directly to their corresponding states. This change simplifies the logic and ensures all states, including potentially POST_RECOVERY, are considered by utilizing the array. The method now also correctly throws an exception if the ID is out of range, unlike the prior implementation, which included a redundant exception throw.

This change is relevant to fixing serialization issues because it ensures that all states within the expected range can be accurately obtained based on IDs. While the diff does not overtly highlight serialization or the specific inclusion of POST_RECOVERY, the indirect handling and mapping of IDs are clues suggesting that the fix might address serialization problems, especially if specific IDs weren't previously handled.

Therefore, based on the connection between handling IDs and serialization, and the enhanced robustness of the changes, I conclude that the commit addresses a bug related to serialization. The answer is **Buggy**."
checkstyle,1499.json,ca8fcd42047b2637e0027ff6aec79edd3656ba56,"@@ -1,3 +1,4 @@
-    public void setIgnoreEqualsIgnoreCase(boolean newValue) {
-        mIgnoreEqualsIgnoreCase = newValue;
+    public void setIgnoreEqualsIgnoreCase(boolean aNewValue)
+    {
+        mIgnoreEqualsIgnoreCase = aNewValue;
     }",Buggy,"fix up checkstyle errors.
",NotBuggy,"The commit message indicates a fix for checkstyle errors, i.e., it addresses code formatting and style issues rather than functionality. The changes in the diff adjust the placement of the curly brace and rename a method parameter for style consistency, which aligns with the commit message about addressing checkstyle violations. There is no indication of a logical or functional bug fix in the code changes; they solely focus on style.

Based on this analysis, the conclusion is **NotBuggy**."
checkstyle,1058.json,9d41bddb46e5fbe291be4cdb347dee18ea4424fe,"@@ -1,4 +1,8 @@
     public int[] getRequiredTokens()
     {
-        return getDefaultTokens();
+        return new int[] {
+            TokenTypes.CTOR_DEF,
+            TokenTypes.METHOD_DEF,
+            TokenTypes.EXPR,
+        };
     }",Buggy,"Fixed bug #1579227: Fixed docs, added unit tests and changed the check implementation to allow turning off the | and & operators
",NotBuggy,"The commit message indicates fixing a bug related to documentation, unit tests, and checks that control `|` and `&` operators. However, the code change in the diff shows a modification to the `getRequiredTokens()` method, replacing a call to `getDefaultTokens()` with a specific array of token types. This change doesn't seem related to documentation or operators, and there's no context provided within this specific diff regarding a bug related to these tokens. Without additional context showing a connection between these tokens and the bug or operators mentioned, this appears to be an unrelated change.

The difference in the code doesn't directly address or imply a fix connected to the bug described in the commit message—it seems more like a change to the required tokens functionality, which isn't clearly tied to the specified bug in the commit message.

Thus, with the given information, this change seems more like an enhancement or refactoring rather than a bug fix as described in the commit message. Therefore, the conclusion is **NotBuggy**."
lombok,2739.json,aa40323ae8dde889f4bcf21304e5cffdc0003b4c,"@@ -1,21 +1,26 @@
 	public static TypeParameter[] copyTypeParams(TypeParameter[] params) {
 		if ( params == null ) return null;
 		TypeParameter[] out = new TypeParameter[params.length];
 		int idx = 0;
 		for ( TypeParameter param : params ) {
 			TypeParameter o = new TypeParameter();
 			o.annotations = param.annotations;
 			o.bits = param.bits;
 			o.modifiers = param.modifiers;
 			o.name = param.name;
 			o.type = copyType(param.type);
+			o.sourceStart = param.sourceStart;
+			o.sourceEnd = param.sourceEnd;
+			o.declarationEnd = param.declarationEnd;
+			o.declarationSourceStart = param.declarationSourceStart;
+			o.declarationSourceEnd = param.declarationSourceEnd;
 			if ( param.bounds != null ) {
 				TypeReference[] b = new TypeReference[param.bounds.length];
 				int idx2 = 0;
 				for ( TypeReference ref : param.bounds ) b[idx2++] = copyType(ref);
 				o.bounds = b;
 			}
 			out[idx++] = o;
 		}
 		return out;
 	}",Buggy,"Fixed a problem where @Data with a static constructor and generics params on the class would generate errors regarding IllegalArgumentException in setSourcePosition in ASTNode.
",Buggy,"The commit message references an issue with `@Data` using static constructors and generics, causing errors with `IllegalArgumentException` related to `setSourcePosition` in `ASTNode`. The code changes address potential issues in copying source position data for type parameters. By copying properties like `sourceStart`, `sourceEnd`, `declarationEnd`, `declarationSourceStart`, and `declarationSourceEnd`, the code ensures that all relevant pieces of information are retained in the `TypeParameter` objects.

These modifications suggest fixing the issue described in the commit message by ensuring that source positioning data is preserved correctly when copying the type parameters. This alignment indicates a correction of particular runtime errors associated with source positions, likely solving the reported problem regarding IllegalArgumentException.

Given these observations, the code changes align with the intent described in the commit message and indicate a bug fix. Hence, the conclusion is **Buggy**."
lombok,3095.json,6fce13a8db45629a76e31c2a1c676ab9dac021dc,"@@ -1,7 +1,7 @@
-	private static int getReplacementOffset(IJavaCompletionProposal proposal) {
+	private static int getReplacementOffset(Object proposal) {
 		try {
 			return Reflection.replacementOffsetField.getInt(proposal);
 		} catch (Exception ignore) {
 			return 0;
 		}
 	}",Buggy,"Somehow in eclipse mars wanted us to pull in more ecj deps, fixed that problem with a trivial code tweak.
",NotBuggy,"The commit message talks about resolving an issue with dependencies in Eclipse Mars through a trivial code tweak. However, the actual code change involves altering the parameter type of a method from `IJavaCompletionProposal` to a more generic `Object`. 

This adjustment is likely to avoid type-checking issues or reflect broader compatibility within the Eclipse environment, possibly aligning with the dependency management changes. The method relies on reflection to access a field, which means the exact type is not necessary during compile time if it can be determined at runtime. The change does not directly suggest a fix for a functional bug in the logic of the code related to runtime execution—more so, it handles a type-related compatibility or operational issue experienced in that development environment.

Given the context, it's primarily about improving flexibility in the face of a development setup challenge rather than directly addressing a bug in application logic or exception handling that affects runtime behavior. Therefore, while the change allows for greater adaptability, it doesn't directly imply a bug fix in the typical execution path of the application logic.

Thus, the change indicates a tweak for compatibility or setup rather than directly addressing a runtime bug, resulting in the conclusion of **NotBuggy**."
lombok,1236.json,e4b61e1263eb0eb832eb6cfbd97ad92e869ca27e,"@@ -1,11 +1,11 @@
 	public static boolean typeMatches(String type, JavacNode node, JCTree typeNode) {
 		String typeName = typeNode == null ? null : typeNode.toString();
 		if (typeName == null || typeName.length() == 0) return false;
 		int lastIndexA = typeName.lastIndexOf('.') + 1;
-		int lastIndexB = type.lastIndexOf('.') + 1;
+		int lastIndexB = Math.max(type.lastIndexOf('.'), type.lastIndexOf('$')) + 1;
 		int len = typeName.length() - lastIndexA;
 		if (len != type.length() - lastIndexB) return false;
 		for (int i = 0; i < len; i++) if (typeName.charAt(i + lastIndexA) != type.charAt(i + lastIndexB)) return false;
 		TypeResolver resolver = node.getImportListAsTypeResolver();
 		return resolver.typeMatches(node, type, typeName);
 	}",Buggy,[performance] fixing bug in earlier performance commit on optimizing lookups of typeMatches.,Buggy,"The commit message indicates a bug fix related to a previous performance optimization. The code change affects how the method calculates the start position of the simple class name within the fully-qualified class name by considering both '.' and '$' as potential separators, instead of only '.'.

This change suggests a bug fix, as it corrects the handling of inner class names (which often use '$' in their representation) in the type matching logic. By adding the logic to find the maximum index between '.' and '$', the updated code handles both top-level and inner class names more reliably.

Thus, the modification aligns with the commit message, signifying a correction of an earlier oversight or incorrect optimization.

Therefore, the answer is **Buggy**."
lombok,314.json,9ac86c8a234f2ecd280f27f0dbda15440b1bf155,"@@ -1,9 +1,12 @@
 	public static LombokOptions getDelombokOptions(Context context) {
+		Options rawOptions = Options.instance(context);
+		if (rawOptions instanceof LombokOptions) return (LombokOptions) rawOptions;
+		
 		LombokOptions options;
 		if (Javac.getJavaCompilerVersion() < 8) {
 			options = LombokOptionCompilerVersion.JDK7_AND_LOWER.createAndRegisterOptions(context);
 		} else {
 			options = LombokOptionCompilerVersion.JDK8.createAndRegisterOptions(context);
 		}
 		return options;
 	}",Buggy,"fixed a bug where called LombokOptionsFactory.getLombokOptions() would actually cause the old options to be wrapped continuously, thus wrapping LombokOptions into itself infinitely. Now it just wraps if needed, other wise returns what's already been done.
",Buggy,"The commit message describes a bug where LombokOptionsFactory.getLombokOptions() recursively wrapped options, leading to excessive wrapping and potential memory issues. The code change is designed to check if rawOptions is already an instance of LombokOptions by using an `instanceof` check. If it is, it returns the existing LombokOptions directly rather than creating and registering a new set of options. This prevents unnecessary wrapping and addresses the outlined bug.

The changes align with the commit message by ensuring that the function only wraps the options if necessary. The `if` statement ensures that the function now returns the already wrapped options when appropriate, addressing the infinite wrapping concern.

Thus, the answer is **Buggy**."
hibernate-search,6397.json,58ae33f9813b56aa60ffb2d301bc1aca8f023f6c,"@@ -1,27 +1,41 @@
 	private void indexAllQueue(Session session) {
 		final InstanceInitializer sessionInitializer = new HibernateSessionLoadingInitializer(
-				(SessionImplementor) session );
+				(SessionImplementor) session
+		);
 		try {
 			ConversionContext contextualBridge = new ContextualExceptionBridgeHelper();
 			while ( true ) {
 				List<?> takeList = source.take();
 				if ( takeList == null ) {
 					break;
 				}
 				else {
 					log.tracef( ""received a list of objects to index: %s"", takeList );
-					for ( Object take : takeList ) {
+					for ( Object object : takeList ) {
 						//trick to attach the objects to session:
-						session.buildLockRequest( LockOptions.NONE ).lock( take );
-						index( take, session, sessionInitializer, contextualBridge );
-						monitor.documentsBuilt( 1 );
+						session.buildLockRequest( LockOptions.NONE ).lock( object );
+						try {
+							index( object, session, sessionInitializer, contextualBridge );
+							monitor.documentsBuilt( 1 );
+						}
+						catch (InterruptedException ie) {
+							// rethrowing the interrupted exception
+							throw ie;
+						}
+						catch (RuntimeException e) {
+							String errorMsg = log.massIndexerUnableToIndexInstance(
+									object.getClass().getName(),
+									object.toString()
+							);
+							errorHandler.handleException( errorMsg, e );
+						}
 						session.clear();
 					}
 				}
 			}
 		}
 		catch (InterruptedException e) {
 			// just quit
 			Thread.currentThread().interrupt();
 		}
 	}",Buggy,"HSEARCH-1354 Fixing error handling during indexing for mass indexer and adding test case
",Buggy,"The commit message indicates that the purpose of the change is to fix error handling during indexing for a mass indexer and to add a test case. 

Let's analyze the changes in the diff:
1. **Refactoring for Clarity**: 
   - Variable `take` is renamed to `object` for better readability, which is not directly related to bug fixes but improves the clarity of the code.

2. **Exception Handling Update**:
   - A `try-catch` block is introduced around the `index` method call. This is a significant change as it enhances how exceptions are handled during the indexing process.
   - Specific handling is added for `InterruptedException`, where the exception is rethrown. Proper handling of this exception is crucial to avoid leaving threads in a blocked state, which points to fixing an error-handling flaw.
   - A more generalized `RuntimeException` catch clause has been added, which logs errors specific to the object being indexed and delegates error handling to an `errorHandler`. This suggests that previously, such errors might have been ignored or not properly logged and handled, indicating a correction in the error-handling mechanism.

3. **Logging Improvement**: 
   - When a `RuntimeException` occurs, the changes include logging the class name and the string representation of the object, which is new. This improves transparency and traceability of what occurs during indexing errors.

The nature and context of these changes align strongly with the commit message's claim of ""fixing error handling during indexing."" The implementation improves robustness and reliability by ensuring exceptions are properly managed, logged, and acted upon rather than being potentially ignored or mishandled.

Therefore, considering the improvements to error handling and how exceptions are actively managed now where they were not before, it is clear that these changes are addressing an existing bug related to error handling in the mass indexer.

The conclusion is **Buggy**."
hibernate-search,1888.json,835d01200cf073c05220f435d7ec47dc2593dddb,"@@ -1,13 +1,13 @@
-	public Query filterOrPassthrough(Query filteredQuery) {
+	public Query filterOrPassthrough(Query queryToFilter) {
 		if ( isEmpty() ) {
-			return filteredQuery;
+			return queryToFilter;
 		}
 		else {
 			BooleanQuery.Builder boolQueryBuilder = new BooleanQuery.Builder();
-			boolQueryBuilder.add( filteredQuery, Occur.MUST );
+			boolQueryBuilder.add( queryToFilter, Occur.MUST );
 			for ( Query bc : filterQueries ) {
 				boolQueryBuilder.add( bc, BooleanClause.Occur.FILTER );
 			}
 			return boolQueryBuilder.build();
 		}
 	}",Buggy,"HSEARCH-2698 Fix incomplete javadoc for some methods

This does not address all the problems, I intentionally didn't address
the more controversial ones (like the useless @return in fluent APIs).
",NotBuggy,"The commit message mentions fixing incomplete Javadoc, but this diff only involves a renaming of a method parameter from `filteredQuery` to `queryToFilter`. The change clarifies the variable's role but doesn't directly relate to fixing Javadoc or resolving a bug in the code. No logical, error-handling, or exception-related adjustments are evident in the provided diff. As such, the code modifications do not align with the message of addressing Javadoc issues, nor do they indicate a bug fix. 

Thus, the conclusion is **NotBuggy**."
wildfly,13142.json,77807f20fe0a38e6b867210ebf1b305edc14cf6a,"@@ -1,55 +1,59 @@
     private Method getTimeoutMethod(TimeoutMethod timeoutMethodInfo) {
 
         String declaringClass = timeoutMethodInfo.getDeclaringClass();
         Class<?> timeoutMethodDeclaringClass = null;
         try {
             timeoutMethodDeclaringClass = Class.forName(declaringClass, false, timedObjectInvoker.getClassLoader());
         } catch (ClassNotFoundException cnfe) {
-            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"");
+            throw new RuntimeException(""Could not load declaring class: "" + declaringClass + "" of timeout method"", cnfe);
         }
 
         String timeoutMethodName = timeoutMethodInfo.getMethodName();
         String[] timeoutMethodParams = timeoutMethodInfo.getMethodParams();
         // load the method param classes
         Class<?>[] timeoutMethodParamTypes = new Class<?>[]
                 {};
         if (timeoutMethodParams != null) {
             timeoutMethodParamTypes = new Class<?>[timeoutMethodParams.length];
             int i = 0;
             for (String paramClassName : timeoutMethodParams) {
                 Class<?> methodParamClass = null;
                 try {
                     methodParamClass = Class.forName(paramClassName, false, timedObjectInvoker.getClassLoader());
                 } catch (ClassNotFoundException cnfe) {
-                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"");
+                    throw new RuntimeException(""Could not load method param class: "" + paramClassName + "" of timeout method"", cnfe);
                 }
                 timeoutMethodParamTypes[i++] = methodParamClass;
             }
         }
         // now start looking for the method
         Class<?> klass = timeoutMethodDeclaringClass;
         while (klass != null) {
             Method[] methods = klass.getDeclaredMethods();
             for (Method method : methods) {
                 if (method.getName().equals(timeoutMethodName)) {
                     Class<?>[] methodParamTypes = method.getParameterTypes();
                     // param length doesn't match
                     if (timeoutMethodParamTypes.length != methodParamTypes.length) {
                         continue;
                     }
+                    boolean match = true;
                     for (int i = 0; i < methodParamTypes.length; i++) {
                         // param type doesn't match
                         if (!timeoutMethodParamTypes[i].equals(methodParamTypes[i])) {
-                            continue;
+                            match = false;
+                            break;
                         }
                     }
-                    // match found
-                    return method;
+                    if (match) {
+                        // match found
+                        return method;
+                    }
                 }
             }
             klass = klass.getSuperclass();
 
         }
         // no match found
         return null;
     }",Buggy,"Fix minor timer service bug
",Buggy,"The commit message indicates a fix for a ""minor timer service bug."" The code changes support this:

1. **Exception Handling Improvement**:
    - The `RuntimeException` constructors are updated to include the `ClassNotFoundException` `cnfe` as a cause, using the form `new RuntimeException(message, cnfe)`. This adjustment provides more detailed context about exceptions when they occur, which is crucial for debugging. Although this modification is more about improving error traceability than directly fixing a bug, it aligns with addressing robustness issues.

2. **Logical Correction for Method Matching**:
    - The original loop checking for parameter type matches incorrectly used `continue`, which caused it to prematurely and unintentionally exit instead of proceeding to the next iteration for matching further parameters. The new logic introduces a boolean flag `match` to correctly track if all parameters match, using `break` to exit the loop early if a mismatch is detected. If any parameter does not match, it breaks the loop and sets `match` to `false`, ensuring every parameter comparison occurs.
    - This modification properly evaluates method parameter matches against the expected parameters, addressing potential incorrect method selections due to premature loop exits.

These changes reflect a clear improvement in exception handling and logical correctness, specifically related to the method matching process, potentially preventing incorrect behavior when identifying methods. This update aligns with the commit message indicating a bug fix, as it resolves an error in the method parameter comparison logic and improves diagnostics.

Thus, the changes represent a bug fix. The conclusion is **Buggy**."
wildfly,15035.json,162a90c311abfe554f09dc4b9c080098bd27ad89,"@@ -1,18 +1,18 @@
         public final T visit(final ContextNode contextNode) throws NamingException {
             if (isEmpty(currentName)) {
                 return found(contextNode);
             }
             final String childName = currentName.get(0);
             traversedName.add(childName);
             currentName = currentName.getSuffix(1);
             final TreeNode node = contextNode.children.get(childName);
             if (node == null) {
                 if (createIfMissing) {
-                    final NamingContext subContext = new NamingContext(traversedName, InMemoryNamingStore.this, new Hashtable<String, Object>());
-                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, traversedName, subContext)).accept(this);
+                    final NamingContext subContext = new NamingContext((Name)traversedName.clone(), InMemoryNamingStore.this, new Hashtable<String, Object>());
+                    return contextNode.addOrGetChild(childName, new ContextNode(contextNode, childName, (Name)traversedName.clone(), subContext)).accept(this);
                 } else {
                     throw nameNotFoundException(childName, contextNode.fullName);
                 }
             }
             return node.accept(this);
         }",Buggy,"AS7-1407 - Fix bug in nested context binding
",Buggy,"The commit message indicates a fix related to a bug in nested context binding. The provided diff suggests changes in how the `traversedName` is handled within the method.

1. **Context of the Change**:
   - The code snippet is part of a class dealing with context nodes and name resolution within a presumably hierarchical structure—often relevant in frameworks involving naming and directory services.
   - The alteration occurs in a method that visits `ContextNode` objects, checking the current name's state and traversing context nodes accordingly.

2. **Analysis of the Diff**:
   - Before the change, the code passed `traversedName` directly to `NamingContext` and `ContextNode` constructors.
   - After the change, `(Name)traversedName.clone()` is passed instead.

3. **Reasoning for the Change**:
   - Cloning `traversedName` before passing it ensures that the original `traversedName` is not modified or affected by subsequent changes in the newly created `NamingContext` or `ContextNode`.
   - This aligns with the notion of fixing a ""nested context binding"", as shared reference issues could disrupt proper context binding in nested structures by unintentionally altering the name path during traversal or context manipulations.

4. **Conclusion**:
   - The commit message is a good match for the changes seen in the diff. The cloning of `traversedName` resolves potential issues related to incorrect bindings within nested contexts by preventing unintended side effects from shared references.
   - This change effectively represents a correction of logical or contextual behavior pertinent to binding, characteristic of a bug fix.

Thus, the change is consistent with the commit message asserting a bug fix in nested context bindings. The result is **Buggy**."
wildfly,11562.json,1af2b154ec4daa8436ba6f946ef54fe201062c6a,"@@ -1,6 +1,8 @@
     private synchronized void destroySingletonInstance() {
         if (this.singletonComponentInstance != null) {
-            this.destroyInstance(this.singletonComponentInstance);
+            // TODO: Implement destroying an instance
+            logger.warn(""Destorying of singleton instance not yet implemented"");
+            //this.destroyInstance(this.singletonComponentInstance);
             this.singletonComponentInstance = null;
         }
     }",Buggy,"Fix compilation errors in EJB3 module to start refactoring the EJB3 component
",NotBuggy,"The commit message mentions fixing compilation errors, suggesting code modifications to resolve issues preventing the code from compiling. However, the provided diff highlights two main changes: 

1. Adding a warning log statement.
2. Commenting out an existing method call (`this.destroyInstance(this.singletonComponentInstance);`).

These changes do not directly address compilation errors. Instead, they imply temporarily disabling functionality by commenting out a method call and acknowledging the lack of functionality with a logging statement.

While this might be a part of a broader refactoring or placeholder for future implementation, it does not align well with the stated goal of fixing compilation errors. There's no indication of actual compilation-related changes like resolving mismatched types, fixing import issues, or correcting method signatures.

Given the divergence between the commit message and the actual code change, and since there is no clear indication of a bug fix—such as resolving a logical error or incorrect behavior—the conclusion is **NotBuggy**."
wildfly,9528.json,3b47b9b23bf3f10d124f26f37dff3245fe09ec56,"@@ -1,52 +1,52 @@
         public void execute(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final ModelNode model = context.readResource(PathAddress.EMPTY_ADDRESS).getModel();
 
             final ModelNode coreThreads;
             final ModelNode maxThreads;
             final ModelNode queueLength;
 
             if (isRuntimeStage) {
                 coreThreads = CORE_THREADS_AD.resolveModelAttribute(context, model);
                 maxThreads = MAX_THREADS_AD.resolveModelAttribute(context, model);
                 queueLength = QUEUE_LENGTH_AD.resolveModelAttribute(context, model);
             } else {
                 coreThreads = model.get(CORE_THREADS);
                 maxThreads = model.get(MAX_THREADS);
                 queueLength = model.get(QUEUE_LENGTH);
             }
 
             if (coreThreads.getType() == ModelType.EXPRESSION || maxThreads.getType() == ModelType.EXPRESSION ||
                     queueLength.getType() == ModelType.EXPRESSION) {
                 context.addStep(new ExecutorQueueValidationStepHandler(true), OperationContext.Stage.RUNTIME, true);
                 return;
             }
 
             // Validate an unbounded queue
             if (!queueLength.isDefined() || queueLength.asInt() == Integer.MAX_VALUE) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
 
             }
 
             // Validate a hand-off queue
             if (queueLength.isDefined() && queueLength.asInt() == 0) {
                 if (coreThreads.isDefined() && coreThreads.asInt() <= 0) {
-                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(coreThreads.asString());
+                    throw EeLogger.ROOT_LOGGER.invalidCoreThreadsSize(queueLength.asString());
                 }
             }
 
             // max-threads must be defined and greater than 0 if core-threads is 0
             if (coreThreads.isDefined() && coreThreads.asInt() == 0) {
                 if (!maxThreads.isDefined() || maxThreads.asInt() <= 0) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.isDefined() ? maxThreads.asInt() : 0, coreThreads.asInt());
                 }
             }
 
             // max-threads must be greater than or equal to core-threads
             if (coreThreads.isDefined() && maxThreads.isDefined()) {
                 if (maxThreads.asInt() < coreThreads.asInt()) {
                     throw EeLogger.ROOT_LOGGER.invalidMaxThreads(maxThreads.asInt(), coreThreads.asInt());
                 }
             }
         }",Buggy,"[WFLY-8516] Update schema documentation and fix error messages for the managed-executor-service.
",Buggy,"The commit message indicates updates to schema documentation and error messages. The code diff shows changes specifically in error message handling within the execute method, where the error message for invalid core thread size now uses `queueLength.asString()` instead of `coreThreads.asString()`. This change suggests a correction in the parameter used for logging the error, as `queueLength` is likely the contextually correct value to report when an invalid core threads size error is thrown in these specific cases (unbounded and hand-off queue validation).

The alignment between the commit message and diff suggests a clear focus on improving error messages, which is a typical indicator of addressing a bug or miscommunication issue within log outputs or exception messages. These changes would likely help in providing clearer and more accurate error diagnostics, especially in production environments.

Considering these points, the purpose of this change is to fix an issue related to error message specificity. Since it involves correcting what values are displayed in those error messages, it aligns well with a bug fix.

Therefore, the evaluation of this commit results in the conclusion: **Buggy**."
wildfly,10266.json,2bb205f62a00e2d69781f4b468c299316f855764,"@@ -1,5 +1,5 @@
     public void start(StartContext context) throws StartException {
         this.value.setDefaultBootstrapContext(defaultBootstrapContext.getValue());
-        ROOT_LOGGER.startingSubsystem(""JCS"", Version.FULL_VERSION);
+        ROOT_LOGGER.startingSubsystem(""JCA"", Version.FULL_VERSION);
         ROOT_LOGGER.tracef(""config=%s"", value);
     }",Buggy,"Fix spelling error in a JCA subsystem log message.
",NotBuggy,"The change corrects a spelling error in a log message, changing ""JCS"" to ""JCA"". The commit message directly describes this modification, and it pertains to logging output rather than the system’s functional behavior or bug fixes in logic. Therefore, this commit does not address a bug fix in the code's behavior or functionality. The answer is **NotBuggy**."
wildfly,18502.json,a10bd882e880725b709e6745e140931720629a95,"@@ -1,17 +1,20 @@
     public InputStream sanitize(InputStream in) throws Exception {
+        byte [] content = IOUtils.toByteArray(in);
         try {
-            Document doc = builder.parse(in);
+            // storing the entire file in memory in case we need to bail.
+            Document doc = builder.parse(new ByteArrayInputStream(content));
             Object result = expression.evaluate(doc, XPathConstants.NODESET);
             NodeList nodes = (NodeList) result;
             for (int i = 0; i < nodes.getLength(); i++) {
                 nodes.item(i).setTextContent("""");
             }
             DOMSource source = new DOMSource(doc);
             ByteArrayOutputStream output = new ByteArrayOutputStream();
             StreamResult outStream = new StreamResult(output);
             transformer.transform(source, outStream);
             return new ByteArrayInputStream(output.toByteArray());
         } catch (Exception e) {
-            return in;
+            ROOT_LOGGER.debug(""Error while sanitizing an xml document"", e);
+            return new ByteArrayInputStream(content);
         }
     }",Buggy,"Addressing AS7-6731 [bz920113]

Module dependency on xalan now uses services=import
Adding javadoc to various bits of the code
Fixes an issue where an error during sanitation causes
0 length files to be stored in the archive.
",Buggy,"The commit message claims to fix an issue where sanitation errors create zero-length files in archives. The code change aligns; it stores input in a `byte[]`, reusing it if parsing fails, preventing the issue. Previously, it returned `in`, potentially leading to zero-length files. Logging the error enhances debugging without affecting functionality. The changes reduce the impact of an exception, ensuring the original content passes through even if sanitized parsing fails, which is consistent with the commit message's bug fix intent. The answer is **Buggy**."
wildfly,6365.json,7f64913774a8043da0b0cc545f4e1b2a8af1a082,"@@ -1,7 +1,13 @@
     public boolean isTransactionActive() {
         try {
-            return (injectedTransaction.getValue().getStatus() & Status.STATUS_ACTIVE) != 0;
+            final int status = injectedTransaction.getValue().getStatus();
+            return status == Status.STATUS_ACTIVE ||
+                    status == Status.STATUS_COMMITTING ||
+                    status == Status.STATUS_MARKED_ROLLBACK ||
+                    status == Status.STATUS_PREPARED ||
+                    status == Status.STATUS_PREPARING ||
+                    status == Status.STATUS_ROLLING_BACK;
         } catch (SystemException e) {
             throw new RuntimeException(""SystemException while getting transaction status"", e);
         }
     }",Buggy,"Fix bug in WeldTransactionServices
",Buggy,"The commit message indicates a bug fix in WeldTransactionServices, which is supported by the code changes. The original code only checked if the transaction status was active but did not account for other valid transaction states indicating activity: committing, marked rollback, prepared, preparing, and rolling back. The update broadens the conditional logic to correctly identify an active transaction considering different statuses, which likely addresses a bug where other active status codes were not properly handled. This aligns well with the commit message, reflecting a logical correction regarding transaction handling.

The changes suggest a bug fix related to transaction state management, so the conclusion is **Buggy**."
wildfly,14704.json,59232bbc1b3a2808f2cbc125e1aca2647fa5abd5,"@@ -1,16 +1,11 @@
     public static void initializeNamingManager() {
         // Setup naming environment
         System.setProperty(Context.URL_PKG_PREFIXES, PACKAGE_PREFIXES);
         try {
             //If we are reusing the JVM. e.g. in tests we should not set this again
             if (!NamingManager.hasInitialContextFactoryBuilder())
                 NamingManager.setInitialContextFactoryBuilder(new InitialContextFactoryBuilder());
         } catch (NamingException e) {
             log.warn(""Failed to set InitialContextFactoryBuilder"", e);
         }
-        try {
-            NamingManager.setObjectFactoryBuilder(ObjectFactoryBuilder.INSTANCE);
-        } catch(Throwable t) {
-            log.warn(""Failed to set ObjectFactoryBuilder"", t);
-        }
     }",Buggy,"Fix problem where object factory builder can only be set once
",Buggy,"The commit message describes a problem with an object factory builder that can only be set once, implying efforts to prevent exceptions on attempts to reset the builder. The diff aligns with this by removing potential re-setting and resultantly problematic lines. The changes seem to mitigate or eliminate the bug mentioned in the commit message by preventing the attempt to set the object factory builder, which thereby adhering to the intended limitation within the `initializeNamingManager` method.

Overall, this code modification is aligned with fixing a bug related to setting an object factory builder that the commit message refers to. By eliminating the attempt to set the `ObjectFactoryBuilder` again, the code is in line with preventing the issue noted in the commit message.

Therefore, this change indicates a bug fix, concluding the assessment with **Buggy**."
wildfly,14735.json,603f475d6af9bc7abc7bcee74a897d4577977d27,"@@ -1,3 +1,3 @@
     public void close() throws NamingException {
-        namingStore.close();
+        // NO-OP
     }",Buggy,"Fix bug in naming context with Context.close removing entries from the naming store
",Buggy,"The commit message states a bug fix related to `Context.close` removing entries from the naming store. The code change alters the `close()` method to become a no-op (no operation), potentially resolving an issue where calling `close()` on the context unintentionally closed or cleared the naming store entries. By making `close()` perform no action, the potential bug, where important data was lost or incorrectly managed upon calling `close`, is mitigated.

The alignment between the commit message and the code change suggests that a problematic behavior was addressed. Rather than deleting or altering entries, `close()` now deliberately does nothing, which implies that the unintended side effect of clearing the naming store is avoided.

Hence, the changes are consistent with the commit message's implication of a bug fix. Therefore, the conclusion is **Buggy**."
wildfly,5832.json,8c3680a316585b462638de3f031affc527f5c723,"@@ -1,7 +1,7 @@
     public static <T> Class<T> loadClass(String className, ClassLoader classLoader) {
         try {
             return cast(classLoader.loadClass(className));
-        } catch (Exception e) {
+        } catch (Throwable e) {
             return null;
         }
     }",Buggy,"Capture RuntimeExceptions in dinamic class loading

Since 9.x, if you deploy an war with two (or more wars) with different libraries, deployment crash since it's trying to load classes from one WAR with the classloader from the other WAR. This cross load is intended (see BeanDeploymentArchiveImpl, method isAccesible, line 246, WFLY-4250), but it's waiting for a null load, not a RuntimeException.

With this fix, a warn is registered in log but application can be loaded without major problems.
",Buggy,"The diff changes the exception handling from catching `Exception` to catching `Throwable`. `Throwable` includes all subtypes, such as `Exception` and `Error`, including `RuntimeException`. The commit message explains that the deployment crashes due to an unhandled cross-load `RuntimeException`. This fix aligns the code with the intention of capturing unexpected `RuntimeException`, preventing application crashes due to class loading issues across different WAR files. This change adds robustness to error handling during dynamic class loading, catching exceptions that would have otherwise been unhandled, causing the application to fail. Therefore, it is indeed addressing a bug related to class loading. 

The answer is **Buggy**."
wildfly,10236.json,efb61994477cc0e8f0f1eca6da690a78c9c33699,"@@ -1,21 +1,21 @@
         private void writeWorkManagers(XMLExtendedStreamWriter writer, ModelNode parentNode) throws XMLStreamException {
             if (parentNode.hasDefined(WORKMANAGER) && parentNode.get(WORKMANAGER).asList().size() != 0) {
                 for (Property property : parentNode.get(WORKMANAGER).asPropertyList()) {
                     if (""default"".equals(property.getValue().get(NAME).asString())) {
                         writer.writeStartElement(Element.DEFAULT_WORKMANAGER.getLocalName());
                     } else {
                         writer.writeStartElement(Element.WORKMANAGER.getLocalName());
                         WorkManagerAdd.WmParameters.NAME.getAttribute().marshallAsAttribute(property.getValue(), writer);
                     }
-                    for (Property prop : property.getValue().asPropertyList()) {
+                    for (Property prop : property.getValue().asPropertyList() ) {
                         if (WORKMANAGER_LONG_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.LONG_RUNNING_THREADS.getLocalName(), false);
                         }
-                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName())) {
+                        if (WORKMANAGER_SHORT_RUNNING.equals(prop.getName()) && prop.getValue().isDefined() && prop.getValue().asPropertyList().size() != 0) {
                             ThreadsParser.getInstance().writeBoundedQueueThreadPool(writer, prop.getValue().asProperty().getValue(), Element.SHORT_RUNNING_THREADS.getLocalName(), false);
                         }
                     }
                     writer.writeEndElement();
                 }
             }
         }",Buggy,"Fixing persistemce problem when all thread executor config has been removed
",Buggy,"The diff introduces additional checks for property definitions and sizes before processing. Previously, there was no check for whether `prop.getValue()` was defined and had a non-empty property list for `WORKMANAGER_SHORT_RUNNING`. This would lead to problems if thread executors were missing or removed, resulting in either unexpected behavior or exceptions during persistence operations.

The new conditions:

- `prop.getValue().isDefined()`
- `prop.getValue().asPropertyList().size() != 0`

These conditions ensure that the properties for both `WORKMANAGER_LONG_RUNNING` and `WORKMANAGER_SHORT_RUNNING` are only processed when they are properly defined and non-empty, which aligns with addressing persistence issues when all thread executor configurations have been inadvertently removed.

Therefore, this change does indeed address a bug related to handling the absence of thread executor configurations during persistence processes, as described in the commit message.

The answer is **Buggy**."
wildfly,10893.json,e8dd2b36ea6a8dd25b8cffc37791e862fb08aa45,"@@ -1,12 +1,12 @@
         public ResourceAdapter doDeploy(URL url, String deploymentName, File root, ClassLoader cl, Connector cmd,
                 IronJacamar ijmd) throws Throwable {
             // TODO
             this.setConfiguration(getConfig().getValue());
 
             this.start();
 
-            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, null, cl, cmd, ijmd, null);
+            CommonDeployment dep = this.createObjectsAndInjectValue(url, deploymentName, root, cl, cmd, ijmd);
 
             return dep.getResourceAdapter();
 
         }",Buggy,"fixing problem with latests ironjacamar snapshots
",Buggy,"The change in the diff removed a `null` argument from the `createObjectsAndInjectValue` method call. This suggests an adjustment to match the latest ironjacamar API or an update that removes unnecessary parameters or corrects them, which could be problematic if kept as is. The commit message mentions fixing a problem with the latest ironjacamar snapshots. This aligns with adjusting the method call to resolve a compatibility or functionality issue.

Removing a `null` argument might fix a bug if the expected behavior needed a more optimized or compatible method signature, potentially preventing issues like method signature mismatch or unexpected null handling.

Given the mention of ""fixing problem"" and the adjustment of the method signature without a `null` parameter, the change is likely addressing a bug related to recent updates in the ironjacamar library.

Therefore, this diff aligns with the commit message in fixing a problem, and given these points, it can be concluded as **Buggy**."
javaparser,3383.json,85b5cf5a98bc7fc59b3481d802bdff3b736ae8a1,"@@ -1,62 +1,65 @@
     public void accept(VarType node, ProblemReporter reporter) {
         // All allowed locations are within a VariableDeclaration inside a VariableDeclarationExpr inside something else.
         Optional<VariableDeclarator> variableDeclarator = node.findAncestor(VariableDeclarator.class);
         if (!variableDeclarator.isPresent()) {
             // Java 11's var in lambda's
             if (varAllowedInLambdaParameters) {
                 boolean valid = node
                         .findAncestor(Parameter.class)
                         .flatMap(Node::getParentNode)
                         .map((Node p) -> p instanceof LambdaExpr).orElse(false);
                 if (valid) {
                     return;
                 }
             }
             reportIllegalPosition(node, reporter);
             return;
         }
         variableDeclarator.ifPresent(vd -> {
+            if(vd.getType().isArrayType()){
+                reporter.report(vd, ""\""var\"" cannot have extra array brackets."");
+            }
             Optional<Node> variableDeclarationExpr = vd.getParentNode();
             if (!variableDeclarationExpr.isPresent()) {
                 reportIllegalPosition(node, reporter);
                 return;
             }
             variableDeclarationExpr.ifPresent(vdeNode -> {
                 if (!(vdeNode instanceof VariableDeclarationExpr)) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 VariableDeclarationExpr vde = (VariableDeclarationExpr) vdeNode;
                 if (vde.getVariables().size() > 1) {
                     reporter.report(vde, ""\""var\"" only takes a single variable."");
                 }
                 Optional<Node> container = vdeNode.getParentNode();
                 if (!container.isPresent()) {
                     reportIllegalPosition(node, reporter);
                     return;
                 }
                 container.ifPresent(c -> {
                     boolean positionIsFine = c instanceof ForStmt || c instanceof ForeachStmt || c instanceof ExpressionStmt;
                     if (!positionIsFine) {
                         reportIllegalPosition(node, reporter);
                     }
                     // A local variable declaration ends up inside an ExpressionStmt.
                     if (c instanceof ExpressionStmt) {
                         if (!vd.getInitializer().isPresent()) {
                             reporter.report(node, ""\""var\"" needs an initializer."");
                         }
                         vd.getInitializer().ifPresent(initializer -> {
                             if (initializer instanceof NullLiteralExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer type from just null."");
                             }
-                            if (initializer instanceof ArrayCreationExpr) {
+                            if (initializer instanceof ArrayInitializerExpr) {
                                 reporter.report(node, ""\""var\"" cannot infer array types."");
                             }
                         });
 
                     }
                 });
             });
         });
 
     }",Buggy,"Fix mistake with var and array initializers.
",Buggy,"The commit message indicates a fix related to the use of `var` and array initializers. Let's analyze the changes:

1. **Array Type Check**:
   - The addition checks if `vd.getType().isArrayType()` and reports an error if true. This addresses a potential misuse of `var` by adding extra array brackets, which may not behave as intended. This addition seems directly relevant to the message since it targets a specific ""mistake"" related to `var`.

2. **Array Initializer Change**:
   - The code replaces a check `if (initializer instanceof ArrayCreationExpr)` with `if (initializer instanceof ArrayInitializerExpr)`. This modification suggests a more precise detection of the scenario where a `var` variable is initialized with an array. If the original condition was too broad or incorrect, this change fixes that mistake.

Both changes are consistent with the commit message, which calls for addressing mistakes involving `var` and clarifying usage with array initializers. 

Overall, the modifications adjust the logic to correctly capture edge cases for the use of `var`, improving the accuracy of error reporting. These changes imply corrections to potential logical errors in type inference or user guidance when using `var`.

Therefore, the changes align well with the description of fixing mistakes, indicating a bug fix. The answer is **Buggy**."
javaparser,6722.json,c620101966295e302b9ff94a73a0e2ab29c7ec6d,"@@ -1,3 +1,3 @@
     public boolean isWildcard() {
-        return getType().isArray();
+        return getType().isWildcard();
     }",Buggy,"Fix bug in LazyType
",Buggy,"The commit message states a bug fix in `LazyType`, and the code change reflects this intention. Originally, the `isWildcard` method incorrectly returned `getType().isArray()`, which suggests a logical error where the array type check was mistakenly used instead of a wildcard type check. The update to `return getType().isWildcard();` corrects this logic, thus aligning with the commit message concerning a bug fix.

This modification corrects the implementation to return the actual wildcard status of the type, clearly resolving a bug related to incorrect method behavior.

Therefore, the conclusion is **Buggy**."
javaparser,6689.json,a795de5cf63b37147e9efcd9ed89e486a613fd3c,"@@ -1,3 +1,3 @@
     public String getName() {
-        return constructor.getName();
+        return constructor.getDeclaringClass().getSimpleName();
     }",Buggy,"Fixed a bug in ReflectionConstructorDeclaration that caused the constructor's signature to be calculated incorrectly.
",Buggy,"The commit message states that a bug related to the incorrect calculation of a constructor's signature was fixed. The diff shows a change in how the name of a constructor is retrieved:

- The original code calls `constructor.getName()`, which returns the name of the constructor method itself.
- The updated code calls `constructor.getDeclaringClass().getSimpleName()`, which returns the simple name of the class that declares the constructor.

In Java, constructors do not have separate names like methods; instead, their names match the class they belong to. In signature calculations, using the class's simple name instead of the constructor's name is typically correct for accurate representation. This change aligns well with the commit message, indicating that the implementation was corrected to reflect the constructor signature accurately.

Thus, the code change resolves a bug where the signature was arguably incorrect due to the misuse of `constructor.getName()`. In conclusion, the changes are indeed related to the described bug fix, and therefore the verdict is **Buggy**."
javaparser,6349.json,dc99432270600b2bb2a445c86b329a77601005e6,"@@ -1,10 +1,10 @@
     public SymbolReference<ResolvedAnnotationDeclaration> solve(AnnotationExpr annotationExpr) {
         Context context = JavaParserFactory.getContext(annotationExpr, typeSolver);
         SymbolReference<ResolvedTypeDeclaration> typeDeclarationSymbolReference = context.solveType(annotationExpr.getNameAsString(), typeSolver);
-        ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
         if (typeDeclarationSymbolReference.isSolved()) {
+            ResolvedAnnotationDeclaration annotationDeclaration = (ResolvedAnnotationDeclaration) typeDeclarationSymbolReference.getCorrespondingDeclaration();
             return SymbolReference.solved(annotationDeclaration);
         } else {
             return SymbolReference.unsolved(ResolvedAnnotationDeclaration.class);
         }
     }",Buggy,"Fixed minor bug: a corresponding declaration of a SymbolReference is only available if the SymbolReference is solved, and one should not try to retrieve it otherwise, lest a NullPointerException be thrown.
",Buggy,"The commit message indicates a fix for an issue that could lead to a `NullPointerException`. The code modification shows that the retrieval of the `CorrespondingDeclaration` from a `SymbolReference` is only performed after checking if the `SymbolReference` is solved. Originally, the code attempted to retrieve the `CorrespondingDeclaration` without such a guard, leading to potential `NullPointerException` if it was unsolved.

This adjustment ensures the program safely retrieves the declaration only when it is available, aligning perfectly with the commit message's description of preventing a `NullPointerException`. Therefore, the implementation of this check is indeed a bug fix.

Based on this analysis, the conclusion is **Buggy**."
intellij-community,15168.json,8983e75f816afa5ca0d149e488a252e54d16f50f,"@@ -1,22 +1,22 @@
   private static void showWhatsNewNotification(@NotNull Project project) {
     PropertiesComponent properties = PropertiesComponent.getInstance();
     String updateHtmlMessage = properties.getValue(UPDATE_WHATS_NEW_MESSAGE);
     if (updateHtmlMessage == null) {
       LOG.warn(""Cannot show what's new notification: no content found."");
       return;
     }
 
     String title = IdeBundle.message(""update.whats.new.notification.title"", ApplicationNamesInfo.getInstance().getFullProductName());
     UpdateChecker.getNotificationGroup().createNotification(title, null, null, NotificationType.INFORMATION, null)
       .addAction(new NotificationAction(IdeBundle.message(""update.whats.new.notification.action"")) {
         @Override
         public void actionPerformed(@NotNull AnActionEvent e, @NotNull Notification notification) {
           LightVirtualFile file = new LightVirtualFile(IdeBundle.message(""update.whats.new.file.name"", ApplicationInfo.getInstance().getFullVersion()), updateHtmlMessage);
           file.putUserData(HTMLEditorProvider.Companion.getHTML_CONTENT_TYPE(), true);
           FileEditorManager.getInstance(project).openFile(file, true);
           IdeUpdateUsageTriggerCollector.trigger(""update.whats.new"");
           notification.expire();
         }
-      });
+      }).notify(project);
     properties.setValue(UPDATE_WHATS_NEW_MESSAGE, null);
   }",Buggy,"Fix stupid mistake: lost call notify()

GitOrigin-RevId: 954900084aa113c881590d721b66974c560d9d9f",Buggy,"The commit message specifies a ""stupid mistake"" of missing a call to `notify()`. The code diff adds `.notify(project)` to the end of a method chain that creates and configures a notification. Without this call, the notification would not appear, which matches the description of the mistake as an oversight or forgotten operation.

This change involves adding a notification, which is essential for informing users about updates, and corrects a logical mistake where the notification was not being triggered due to the missing call. The change aligns with the commit message indicating a fix of a previously lost function call.

Therefore, this indicates a correction of erroneous behavior, pointing towards a bug fix as described in the commit message. The conclusion is **Buggy**."
intellij-community,24434.json,358dddc2d98464d660a801e43e3671cbd5b340ed,"@@ -1,5 +1,5 @@
   public static Icon loadApplicationIconImage(@NotNull ScaleContext ctx, int size) {
     String url = ApplicationInfoImpl.getShadowInstance().getApplicationSvgIconUrl();
     Image image = loadApplicationIconImage(url, ctx, size, null);
-    return image != null ? new JBImageIcon(ImageUtil.ensureHiDPI(image, ctx)) : null;
+    return image != null ? new JBImageIcon(image) : null;
   }",Buggy,"IDEA-217767 License dialog polishing

* App icon problems fixed

GitOrigin-RevId: babcb9bce8c091934088cc24bc7ce335170096a6",NotBuggy,"The commit message indicates that the change fixes app icon problems in a licensing dialog. The code modification focuses on returning a `JBImageIcon` constructed directly from an `image`, dropping `ImageUtil.ensureHiDPI`. This suggests improvements in handling application icon display, possibly addressing icon quality or scalability issues, not a functional bug. However, this doesn't align with typical ""bug fixes"" in logic or error handling; instead, it's more of a UI polish or visual adjustment.

Considering the focus on polishing, and the specificity of the commit message to ""polishing"" rather than bug fixing, the primary intent of this change appears to be enhancing the visual quality rather than rectifying a functional error directly related to what we'd traditionally classify as a ""bug.""

Thus, the conclusion is **NotBuggy**."
intellij-community,8042.json,08de853c05b9eb1944f79c1ea7df05ae1f103aa7,"@@ -1,3 +1,3 @@
   protected static String getUserHome() {
-    return SystemProperties.getUserHome();
+    return StringUtil.trimEnd(SystemProperties.getUserHome(), ""/"");
   }",Buggy,"save all paths in .iml relatively except for the ones under USER_HOME (IDEA-60906)
reason: fixes the issue and I don't see any problems it could cause
",NotBuggy,"The commit message indicates a modification to save paths relatively, except for user home paths. The code change trims a trailing slash from the user home path string using `StringUtil.trimEnd`. This change aligns with the commit message intention by potentially modifying how user home paths are handled without altering non-user home paths. Given that trimming the slash could affect path calculations, this is likely part of addressing the specified issue. However, the message and the change do not explicitly mention a bug fix. Instead, it seems to be more of a consistency or formatting fix applied to facilitate relative path handling.

The change indicates an enhancement or correction in handling paths but does not necessarily pinpoint a direct bug. This appears more like refining path handling rather than resolving an identifiable defect. 

Thus, the conclusion is **NotBuggy**."
intellij-community,32648.json,0922a97944f2ca7d388ba2807e0e64afb02fe427,"@@ -1,3 +1,3 @@
   public void load(Element element) {
-    setVagrantFolder(element.getAttributeValue(VAGRANT_FOLDER));
+    setVagrantFolder(StringUtil.notNullize(element.getAttributeValue(VAGRANT_FOLDER)));
   }",Buggy,"Vagrant error messaging fixed.
",Buggy,"The commit message states that Vagrant error messaging was fixed. The code change replaces `setVagrantFolder(element.getAttributeValue(VAGRANT_FOLDER))` with `setVagrantFolder(StringUtil.notNullize(element.getAttributeValue(VAGRANT_FOLDER)))`. This modification handles cases where the `getAttributeValue` method might return `null` by defaulting to an empty string (or another default value, depending on the implementation of `StringUtil.notNullize`). This prevents potential `null` values from causing errors or exceptions.

This change aligns with the commit message since it improves robustness by avoiding `null` values, which can prevent errors related to message processing or storage. 
The use of `notNullize` suggests defensive programming to ensure error-free operation and aligns with the commit message about fixing error messaging.

Hence, the resolution of a potential error scenario can reasonably be categorized as a fix. Therefore, the determination is **Buggy**."
intellij-community,23329.json,bcb3520ffa862b37e8147a7dac6cddb7d67c82b8,"@@ -1,29 +1,29 @@
     public void doExecute(Editor editor, @Nullable Caret caret, DataContext dataContext) {
       int endOffset = editor.getDocument().getTextLength();
       List<Caret> carets = editor.getCaretModel().getAllCarets();
       if (editor.isColumnMode() && editor.getCaretModel().supportsMultipleCarets()) {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0) == editor.getCaretModel().getPrimaryCaret() ? carets.get(carets.size() - 1) : carets.get(0);
         }
         LogicalPosition leadSelectionPosition = editor.visualToLogicalPosition(caret.getLeadSelectionPosition());
         LogicalPosition targetPosition = editor.offsetToLogicalPosition(endOffset).leanForward(true);
         editor.getSelectionModel().setBlockSelection(leadSelectionPosition, targetPosition);
       }
       else {
         if (caret == null) { // normally we're always called with null caret
           caret = carets.get(0);
         }
         int selectionStart = caret.getLeadSelectionOffset();
         if (editor instanceof EditorImpl && ((EditorImpl)editor).myUseNewRendering) {
-          editor.getCaretModel().moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
+          caret.moveToLogicalPosition(editor.offsetToLogicalPosition(endOffset).leanForward(true));
         }
         else {
           caret.moveToOffset(endOffset);
         }
         caret.setSelection(selectionStart, endOffset);
       }
       ScrollingModel scrollingModel = editor.getScrollingModel();
       scrollingModel.disableAnimation();
       scrollingModel.scrollToCaret(ScrollType.CENTER);
       scrollingModel.enableAnimation();
     }",Buggy,"editor.new.rendering: fix a mistake in TextEndWithSelectionAction
",Buggy,"The code diff reflects a change in the method used to move the caret. The original code moved the caret using `editor.getCaretModel().moveToLogicalPosition(...)`, which would affect the primary caret. The change switches this to `caret.moveToLogicalPosition(...)`. This specifically updates the position for the current caret instance being handled, rather than the primary caret. This adjustment would prevent incorrect behavior when multiple carets are used, ensuring the correct caret position is updated.

Given the context in the commit message – ""fix a mistake in TextEndWithSelectionAction"" – this change aligns with correcting a logical error in handling editor actions. The original implementation could have caused a bug by moving the wrong caret when multiple carets were in use, particularly in a 'new rendering' mode scenario.

The change in the code fixes an identified mistake in the logic of caret movement specific to the 'new rendering' requirement, which corresponds well with the described commit intent.

Therefore, the changes effectively address a bug, making the conclusion **Buggy**."
intellij-community,11452.json,8a97673aeb2e23f7a0bafdb1907b513cd3514810,"@@ -1,3 +1,3 @@
-  protected boolean atToken(final IElementType tokenType) {
+  protected boolean atToken(@Nullable final IElementType tokenType) {
     return myBuilder.getTokenType() == tokenType;
   }",Buggy,"better recovery for syntax errors in parameter list (PY-3635); fix old bug with tuple parameters parsing
",Buggy,"The commit message refers to improving syntax error recovery and fixing a tuple parameter parsing bug, but the code change involves adding a `@Nullable` annotation to a method parameter. This change suggests that `tokenType` can now be null, potentially addressing edge cases or errors when `tokenType` was unexpectedly null. While this might contribute to more robust error management, it does not directly fix a bug related to tuple parameters parsing or broader syntax error recovery.

The change does improve error-handling or potential null-related bugs but is not directly connected to the specific issues mentioned in the commit message about tuple parameters and syntax errors in a parameter list. Let's analyze it:

1. **Nullability Annotation**: Adding `@Nullable` is often done to prevent `NullPointerException` by indicating that a parameter can accept `null`. This helps in preventing unintended null dereference issues, which is generally a bug-related improvement.

2. **No Tuple-specific Changes**: The change does not directly relate to tuple parameters or parsing advancements, as it only adjusts how a parameter is annotated for possible null values.

3. **Better Syntax Recovery**: The annotation could contribute to better handling of `null` cases, which might result in improved defensive coding against syntax errors. However, the diff doesn't show any logic or structural changes related to syntax recovery directly.

In conclusion, the change seems to address a potential null-related bug, indirectly contributing to improved robustness, but it doesn't straightforwardly match the syntax recovery or tuple parsing improvements emphasized in the commit message, though it might be part of the broader effort. Despite that, considering the nullability handling suggests addressing a bug aspect:

The correct conclusion is **Buggy**, focused on smaller bug-related fixes."
intellij-community,28989.json,ac607f3f6a68a2dbccca5581d55264e5ecad1181,"@@ -1,4 +1,4 @@
     public void addNotify() {
       super.addNotify();
-      addMouseListenerToHierarchy(this, myEntranceListener);
+      ListenerUtil.addMouseListener(this, myEntranceListener);
     }",Buggy,"links in IDE Fatal errors fix
",NotBuggy,"The commit message mentions a fix related to ""links in IDE Fatal errors,"" but the code diff shows a change in the method used to add a mouse listener. The original code uses `addMouseListenerToHierarchy`, switched to `ListenerUtil.addMouseListener`. This change suggests a refactor or improvement in how listeners are managed, possibly related to handling user interactions more robustly.

Without additional context, such as `ListenerUtil` addressing specific errors or improved handling tied to fatal errors, we cannot definitively say this change addresses a bug stated in the commit message. It appears as a refactor to use a potentially more appropriate or robust method, but it doesn't necessarily fix a ""link"" or ""fatal error"" as the message indicates.

Therefore, given the information from the diff alone and its limited linkage to the commit message's described fix, the conclusion is **NotBuggy**."
intellij-community,10790.json,e3fc0ee66ac0e1b3cff41e6cf7cf90b5c16318f5,"@@ -1,26 +1,29 @@
   private static int getExpressionPriority(PyElement expr) {
     int priority = 0;
-    if (expr instanceof PySubscriptionExpression || expr instanceof PySliceExpression || expr instanceof PyCallExpression) priority = 1;
+    if (expr instanceof PyReferenceExpression ||
+        expr instanceof PySubscriptionExpression ||
+        expr instanceof PySliceExpression ||
+        expr instanceof PyCallExpression) priority = 1;
     else if (expr instanceof PyPrefixExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == PLUS || opType == MINUS || opType == TILDE) priority = 2;
       if (opType == NOT_KEYWORD) priority = 11;
     }
     else if (expr instanceof PyBinaryExpression) {
       final IElementType opType = getOperationType(expr);
       if (opType == EXP) priority =  3;
       if (opType == MULT || opType == DIV || opType == PERC || opType == FLOORDIV) priority =  4;
       if (opType == PLUS || opType == MINUS) priority =  5;
       if (opType == LTLT || opType == GTGT) priority = 6;
       if (opType == AND) priority = 7;
       if (opType == XOR) priority = 8;
       if (opType == OR) priority = 9;
       if (COMPARISON_OPERATIONS.contains(opType)) priority = 10;
       if (opType == AND_KEYWORD) priority = 12;
       if (opType == OR_KEYWORD) priority = 13;
     }
     else if (expr instanceof PyConditionalExpression) priority = 14;
     else if (expr instanceof PyLambdaExpression) priority = 15;
 
     return -priority;
   }",Buggy,"PY-13114 Inlining of method call does not add extra parenthesis

Fix prioritization problem: reference expressions should have the same
priority as indexing, slicing and function calls.
",Buggy,"The commit message indicates that there's an issue with prioritization that could affect how expressions are inlined, specifically regarding adding parentheses. The code change adds `PyReferenceExpression` to the condition that assigns a priority of 1. This adjustment addresses the misprioritization in expressions involving reference expressions compared to indexing, slicing, and function calls, ensuring they are treated equivalently in terms of priority.

The change directly aligns with the commit message by correcting a logical prioritization error, which could affect the correct function of expression handling, particularly in certain Python expression tree manipulations. This is indicative of a bug fix.

Therefore, the answer is **Buggy**."
intellij-community,11366.json,aa7434f6b1f2853d50f940681697db30743e0f2e,"@@ -1,12 +1,13 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
-        removeSlash(pyImportStatement, next);
+        final ASTNode prev = next.getTreePrev();
         pyImportStatement.deleteChildInternal(next);
+        removeSlash(pyImportStatement, prev);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"The commit message indicates a fix related to syntax errors in multi-line imports during a refactoring or move operation.

Let's analyze the code change step by step:

1. **Original Issue Context**: The original method involved deleting a comma adjacent to a given ASTNode (`child`) within a multi-line import statement represented by `pyImportStatement`.

2. **Initial Code Logic**:
   - If the `child` node is found in the specified `elements`, the code seeks the next or previous comma related to `child` using `getNextComma` or `getPrevComma`.
   - If a comma is found, it uses `removeSlash` before deleting the comma node (`next`).

3. **Change Implemented**:
   - The original code only called `removeSlash` on the `next` node.
   - The modification includes capturing the previous node of the `next` node (`prev`) and using `removeSlash` on this `prev` node before calling `deleteChildInternal(next)`.

4. **Bug Fix Confirmation**: 
   - The modification addresses a potential scenario where removing a comma could lead to improper syntax, especially with multi-line imports.
   - By ensuring `removeSlash` is called on the `prev` node of `next`, the code likely aims to clean up any trailing artifacts or line continuation issues (often denoted by a backslash) caused by the deletion of the adjacent comma.
   - This adjustment improves handling in multi-line contexts and addresses a specific bug (as cited in the commit message as PY-5489).

The change directly aligns with the commit message by addressing a syntax error issue involving the manipulation of multi-line import statements. Therefore, the code modification indicates a bug fix. Thus, the conclusion is **Buggy**."
intellij-community,11366.json,319662174c861f6c1472a6d644db4513907f4c74,"@@ -1,11 +1,12 @@
   static void deleteAdjacentComma(ASTDelegatePsiElement pyImportStatement, ASTNode child, final PyElement[] elements) {
     if (ArrayUtil.contains(child.getPsi(), elements)) {
       ASTNode next = getNextComma(child);
       if (next == null) {
         next = getPrevComma(child);
       }
       if (next != null) {
+        removeSlash(pyImportStatement, next);
         pyImportStatement.deleteChildInternal(next);
       }
     }
   }",Buggy,"fixed PY-5489 Refactor/Move: leads to syntax error with multi-line imports
",Buggy,"The commit message mentions a fix for a syntax error caused by refactoring or moving multi-line imports. The code change adds a call to `removeSlash(pyImportStatement, next)` before `pyImportStatement.deleteChildInternal(next)`. The addition of `removeSlash` likely addresses the syntax error described, potentially related to issues with the slash character in multi-line imports. Given that this addition seems directly aimed at solving the issue mentioned in the commit message, the change aligns with addressing a bug.

Hence, these changes indicate a bug fix. The conclusion is **Buggy**."
intellij-community,36036.json,84dd250e37af6f1a98d160eaae6a59022d24c7dd,"@@ -1,8 +1,11 @@
   public void revalidate(@NotNull PositionTracker<Balloon> tracker) {
+    if (ApplicationManager.getApplication().isDisposeInProgress()) {
+      return;
+    }
     RelativePoint newPosition = tracker.recalculateLocation(this);
 
     if (newPosition != null) {
       myTargetPoint = myPosition.getShiftedPoint(newPosition.getPoint(myLayeredPane), myCalloutShift);
       myPosition.updateBounds(this);
     }
   }",Buggy,"Fix for ""Application being disposed"" error
",Buggy,"The diff introduces a conditional check to verify if the application is in the process of being disposed, using `ApplicationManager.getApplication().isDisposeInProgress()`. If true, the method returns early, likely preventing further actions that could result in errors since the application is being disposed of. This type of change addresses a specific error condition, the ""Application being disposed"" error, mentioned in the commit message.

Here's the reasoning:
- The `revalidate` method interacts with position tracking for a visual component (`Balloon`), which could cause issues if these operations occur while the application is disposing resources.
- The newly added check ensures safety by preventing position recalculation and updates if the application dispose is in progress.
- The guard clause effectively avoids executing code that could fail or lead to inconsistent states during app disposal, which matches the ""application being disposed"" error in the commit message.

Given these considerations, the change aligns perfectly with the described bug fix in the commit message. Therefore, the evaluation of these changes concludes with **Buggy**."
intellij-community,29938.json,d5022fa9bef92fcb2e21a87b4a03cb0938a1f961,"@@ -1,12 +1,20 @@
   private void doCloseCommunication() {
     if (myPydevConsoleCommunication != null) {
-      try {
-        myPydevConsoleCommunication.close();
-        // waiting for REPL communication before destroying process handler
-        Thread.sleep(300);
-      }
-      catch (Exception e1) {
-        // Ignore
-      }
+
+      UIUtil.invokeAndWaitIfNeeded(new Runnable() {
+        @Override
+        public void run() {
+          try {
+            myPydevConsoleCommunication.close();
+            Thread.sleep(300);
+          }
+          catch (Exception e1) {
+            // Ignore
+          }
+        }
+      });
+
+      // waiting for REPL communication before destroying process handler
+
     }
   }",Buggy,"Fixed EDT bug (PY-3434).
",Buggy,"The commit message indicates a fix for an ""EDT bug,"" which typically involves ensuring that certain operations on user interface components occur on the Event Dispatch Thread (EDT). 

The changes in the code introduce the use of `UIUtil.invokeAndWaitIfNeeded`, wrapping the logic inside it. This modifies the execution of `myPydevConsoleCommunication.close()` and the subsequent `Thread.sleep(300)` call to run on the EDT if it's not already on it.

Here's the step-by-step analysis:
- **UI Thread Requirement:** Swing and similar UI frameworks require interactions with UI elements to occur on the EDT. The `UIUtil.invokeAndWaitIfNeeded` construct ensures that the enclosed code is executed on the EDT, which is a common requirement for avoiding issues like UI freeze or inconsistency.
- **Potential Bug Fix:** Originally, `myPydevConsoleCommunication.close()` and the sleep call execute possibly on a non-EDT thread, which can cause problems if they need to interact with UI components or ensure a UI-consistent state.
- **Handling with Runnable:** Wrapping the code in a `Runnable` and using `invokeAndWaitIfNeeded` ensures execution on the correct thread, aligning with the fix for an EDT-related issue.
- **Error Handling:** The error-handling logic remains the same, ignoring exceptions, which suggests the primary change is related to threading concerns rather than error handling itself.

Given these points, the code change directly addresses the potential threading issue hinted at in the commit message. Therefore, the change aligns with the commit message's intent to fix an EDT bug.

The conclusion is **Buggy**, as the changes indeed address a bug related to proper threading in UI operations."
intellij-community,12571.json,d9a69b15763b0da7a03c07d0f9c8e5a8f7e3d7f7,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
+                if (colon != null && colon.getStartOffset() + 1 < colon.getTextRange().getEndOffset()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when indentation is wrong. (This whole thing begs to be rewritten.)
",Buggy,"The commit message indicates a fix for an assertion error related to incorrect indentation. The code change corrects a condition involved in calculating the end offset of a text range. Previously, the conditional logic used `node.getStartOffset() + node.getTextLength()` to determine where to end the range after a colon, which could mistakenly extend the range beyond the actual text. By utilizing `colon.getTextRange().getEndOffset()`, it ensures the calculated end is correct based on the actual text range of the colon. This change likely avoids an assertion failure under certain conditions where indentation is incorrect.

Given the commit message and this logic adjustment, it aligns with correcting faulty behavior (the assertion error) when handling text ranges, which strongly indicates a bug fix regarding the code's logical correctness in handling AST nodes related to indentation.

Therefore, I conclude the relevance between the commit message and the code change indicates a bug fix, marking the review as **Buggy**."
intellij-community,12571.json,df6834a4b25a87ff95c28760a761cae01c53874f,"@@ -1,25 +1,25 @@
     private static void appendDescriptors(ASTNode node,
                                           List<FoldingDescriptor> descriptors) {
         if (node.getElementType() == PyElementTypes.STATEMENT_LIST) {
             IElementType elType = node.getTreeParent().getElementType();
             if (elType == PyElementTypes.FUNCTION_DECLARATION || elType == PyElementTypes.CLASS_DECLARATION) {
                 ASTNode colon = node.getTreeParent().findChildByType(PyTokenTypes.COLON);
-                if (colon != null) {
+                if (colon != null && colon.getStartOffset() + 1 < node.getStartOffset() + node.getTextLength()) {
                     descriptors.add(new FoldingDescriptor(node,
                             new TextRange(colon.getStartOffset() + 1, node.getStartOffset() + node.getTextLength())));
                 }
                 else {
                     TextRange range = node.getTextRange();
                     if (range.getStartOffset() < range.getEndOffset()-1) { // only for ranges at leas 1 char wide
                         descriptors.add(new FoldingDescriptor(node, range));
                     }
                 }
             }
         }
 
         ASTNode child = node.getFirstChildNode();
         while (child != null) {
             appendDescriptors(child, descriptors);
             child = child.getTreeNext();
         }
     }",Buggy,"Fixed an assertion error when a function is being defined and a colon is not yet typed in.
",Buggy,"The commit message mentions fixing an assertion error related to defining a function without a colon. The code modification checks if the colon's position makes sense before adding a folding descriptor. This prevents errors from incomplete syntax, aligning with the message. The answer is **Buggy**."
intellij-community,679.json,5dc1df3c0d442dd294ec203a212d97f07a9fb175,"@@ -1,80 +1,80 @@
   private static void compileCythonExtension(@NotNull Project project) {
     try {
       final RunManager runManager = RunManager.getInstance(project);
       final RunnerAndConfigurationSettings selectedConfiguration = runManager.getSelectedConfiguration();
       if (selectedConfiguration == null) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       final RunConfiguration configuration = selectedConfiguration.getConfiguration();
       if (!(configuration instanceof AbstractPythonRunConfiguration)) {
         throw new ExecutionException(""Python Run Configuration should be selected"");
       }
       AbstractPythonRunConfiguration runConfiguration = (AbstractPythonRunConfiguration)configuration;
-      final String sdkPath = runConfiguration.getSdkHome();
+      final String interpreterPath = runConfiguration.getInterpreterPath();
       final String helpersPath = PythonHelpersLocator.getHelpersRoot().getPath();
 
       final String cythonExtensionsDir = PyDebugRunner.CYTHON_EXTENSIONS_DIR;
       final String[] cythonArgs =
         {""build_ext"", ""--build-lib"", cythonExtensionsDir, ""--build-temp"", String.format(""%s%sbuild"", cythonExtensionsDir, File.separator)};
 
       final List<String> cmdline = new ArrayList<>();
-      cmdline.add(sdkPath);
+      cmdline.add(interpreterPath);
       cmdline.add(FileUtil.join(helpersPath, FileUtil.toSystemDependentName(SETUP_CYTHON_PATH)));
       cmdline.addAll(Arrays.asList(cythonArgs));
       LOG.info(""Compile Cython Extensions "" + StringUtil.join(cmdline, "" ""));
 
       final Map<String, String> environment = new HashMap<>(System.getenv());
       PythonEnvUtil.addToPythonPath(environment, cythonExtensionsDir);
       PythonEnvUtil.setPythonUnbuffered(environment);
       PythonEnvUtil.setPythonDontWriteBytecode(environment);
-      if (sdkPath != null) {
-        PythonEnvUtil.resetHomePathChanges(sdkPath, environment);
+      if (interpreterPath != null) {
+        PythonEnvUtil.resetHomePathChanges(interpreterPath, environment);
       }
       GeneralCommandLine commandLine = new GeneralCommandLine(cmdline).withEnvironment(environment);
 
       final boolean canCreate = FileUtil.ensureCanCreateFile(new File(helpersPath));
       final boolean useSudo = !canCreate && !SystemInfo.isWindows;
       Process process;
       if (useSudo) {
         process = ExecUtil.sudo(commandLine, ""Please enter your password to compile cython extensions: "");
       }
       else {
         process = commandLine.createProcess();
       }
 
       ProgressManager.getInstance().run(new Task.Backgroundable(project, ""Compile Cython Extensions"") {
         @Override
         public void run(@NotNull ProgressIndicator indicator) {
           final CapturingProcessHandler handler =
             new CapturingProcessHandler(process, commandLine.getCharset(), commandLine.getCommandLineString());
           handler.addProcessListener(new ProcessAdapter() {
             @Override
             public void onTextAvailable(@NotNull ProcessEvent event, @NotNull Key outputType) {
               if (outputType == ProcessOutputTypes.STDOUT || outputType == ProcessOutputTypes.STDERR) {
                 for (String line : StringUtil.splitByLines(event.getText())) {
                   if (isSignificantOutput(line)) {
                     indicator.setText2(line.trim());
                   }
                 }
               }
             }
 
             private boolean isSignificantOutput(String line) {
               return line.trim().length() > 3;
             }
           });
           final ProcessOutput result = handler.runProcessWithProgressIndicator(indicator);
           final int exitCode = result.getExitCode();
           if (exitCode != 0) {
             final String message = StringUtil.isEmptyOrSpaces(result.getStdout()) && StringUtil.isEmptyOrSpaces(result.getStderr())
                                    ? ""Permission denied""
                                    : ""Non-zero exit code ("" + exitCode + ""): \n"" + result.getStderr();
             UIUtil.invokeLaterIfNeeded(() -> showErrorDialog(project, message));
           }
         }
       });
     }
     catch (IOException | ExecutionException e) {
       showErrorDialog(project, e.getMessage());
     }
   }",Buggy,"Fixes after review (PY-26101)

Fix stepping after error jump, fix tests, move code modification to an appropriate function and make it more general
",Buggy,"The commit message indicates that it addresses multiple issues including fixing a ""stepping after error jump,"" fixing tests, reorganizing code, and making it more general.

Key changes in this diff:
1. **Variable Renaming**:
   - `sdkPath` is renamed to `interpreterPath`. This change does not have any direct functional impact as it is solely about renaming for clarity and correctness, assuming the new name better reflects the purpose of the variable.

2. **Logical Check Update**:
   - The logical check and its subsequent function call change from `sdkPath` to `interpreterPath`.

These updates improve the code's clarity and correctness without altering its underlying logic. If the previous variable name (`sdkPath`) was indeed misleading and could cause confusion or misbehavior by assuming it points to the SDK home versus the Python interpreter path, then the renaming resolves that ambiguity. This potentially indicates a fix of erroneous assumptions leading to runtime issues, aligning with fixing bugs, especially in environments or scripts depending on these paths.

Considering the message about addressing bugs (`stepping after error jump`), which typically implies logical or state-related problems, even though it's not exactly clear from the provided code itself, the renaming could be related to addressing these issues. 

Thus, the commit message and the nature of the changes suggest that this was part of a bug fix along with other revisions not fully shown here. Therefore, I conclude that this change contributes to fixing bugs as stated in the commit.

**Buggy**"
intellij-community,6419.json,39a18a8cfd9b96c783c9a408ad36fee10452c418,"@@ -1,24 +1,30 @@
-  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, char hostStringQuote) {
+  private static String quoteItemsInFragments(@NotNull PyNewStyleStringFormatParser.Field field, @NotNull StringNodeInfo hostStringInfo) {
     List<String> escaped = new ArrayList<>();
     for (String part : field.getAttributesAndLookups()) {
       if (part.startsWith(""."")) {
         escaped.add(part);
       }
       else if (part.startsWith(""["")) {
         if (part.contains(""\\"")) {
           return null;
         }
         final String indexText = part.substring(1, part.length() - 1);
         if (indexText.matches(""\\d+"")) {
           escaped.add(part);
           continue;
         }
-        final char quote = flipQuote(hostStringQuote);
-        if (indexText.indexOf(hostStringQuote) >= 0 || indexText.indexOf(quote) >= 0) {
-          return null;
+        final char originalQuote = hostStringInfo.getSingleQuote();
+        char targetQuote = flipQuote(originalQuote);
+        // there are no escapes inside the fragment, so the lookup key cannot contain 
+        // the host string quote unless it's a multiline string literal
+        if (indexText.indexOf(targetQuote) >= 0) {
+          if (!hostStringInfo.isTripleQuoted() || indexText.indexOf(originalQuote) >= 0) {
+            return null;
+          }
+          targetQuote = originalQuote;
         }
-        escaped.add(""["" + quote + indexText + quote + ""]"");
+        escaped.add(""["" + targetQuote + indexText + targetQuote + ""]"");
       }
     }
     return StringUtil.join(escaped, """");
   }",Buggy,"PY-21245 Handle quotes in lookup items inside multiline string literals better

Namely, allow lookup items that contain quotes in some cases if the
containing string literal is multiline, hence there is still a suitable
type of quotes to wrap injected string literal into.

The orignal error described in the issue has been fixed earlier as
part of other changes for PY-21161.
",NotBuggy,"This commit message references handling lookup items with quotes within multiline string literals. The code changes involve a more nuanced approach to quote management, where it adjusts logic to take into account whether the host string literal is multiline. The diff demonstrates changes in the function `quoteItemsInFragments`. It now takes `StringNodeInfo` as a parameter, reflecting the necessity to handle situations where quotes can appear meaningfully within multiline strings.

- Originally, if a part contained a quote (from either potential opening character), the method returned null due to potential quoting issues.
- The new implementation considers if the string is triple-quoted, allowing certain quote types within lookup items that are inappropriate otherwise.

This expanded logic aligns with the purpose described in the commit message: handling quotes better in multiline literal contexts. However, the commit message states that the specific issue was previously resolved in a different patch, suggesting this change may be more about enhancing capabilities or behavior rather than fixing a specific bug.

While it's indicated that changes intend an improvement, they don’t reflect a classic bug fix, which often involves resolving incorrect behavior directly identifiable from the prior implementation. Rather, this expands functionality regarding multiline strings.

Given the context in both the code and message, the improvements aim at feature enhancement rather than fixing a notable defect or bug.

The conclusion is **NotBuggy**."
intellij-community,3353.json,234eea4ef406b48f114d35e7385b2ebfeece9e6d,"@@ -1,10 +1,3 @@
-  private static AndroidFacet getFacet(@NotNull Module module) {
-    AndroidFacet androidFacet = null;
-
-    for (Facet facet : module.getFacets().values()) {
-      if (facet instanceof AndroidFacet) {
-        androidFacet = (AndroidFacet)facet;
-      }
-    }
-    return androidFacet;
-  }

+    public AndroidFacet getFacet() {
+      return myFacet;
+    }",Buggy,"android-jps: fix some bugs, support aapt and dex compilation
",Buggy,"The commit message mentions fixing bugs and adding support for aapt and dex compilation. The code change removes a static method, replacing it with a direct instance method using a pre-existing `myFacet` property.

### Analysis:
1. **Removed Method**: The original method iterated over a collection to find and return an `AndroidFacet`. During this iteration, if multiple facets existed, the last match would override the previous ones, potentially causing bugs.
2. **New Method**: The new method simply returns `myFacet`, assuming it is already set correctly elsewhere, reducing complexity and removing potential iteration issues.
3. **Bug Fix Indication**: The changes imply a simplification, likely fixing a bug by avoiding the risk of incorrect facet selection during iteration. This suggests the refactor directly addresses an issue with the previous approach.

The change aligns with bug-related improvements, addressing unspecified issues with facet retrieval. Thus, the answer is **Buggy**."
intellij-community,23124.json,295d3d1dd1470e62a6748a0baeded3ad4ca1d508,"@@ -1,13 +1,14 @@
-  public void beginObject() {
+  public JsonReaderEx beginObject() {
     int p = peeked;
     if (p == PEEKED_NONE) {
       p = doPeek();
     }
     if (p == PEEKED_BEGIN_OBJECT) {
       push(JsonScope.EMPTY_OBJECT);
       peeked = PEEKED_NONE;
     }
     else {
       throw createParseError(""Expected BEGIN_OBJECT but was "" + peek());
     }
+    return this;
   }",Buggy,"cleanup, fix WEB-14597 Breakpoints being ignored if functions called from live console

AsynPromise — don't ignore error if no rejected handler
",NotBuggy,"The commit message indicates fixing an issue (WEB-14597) related to breakpoints and mentions a change in error handling for promises. However, the shown diff modifies a `beginObject` method by adding a return type, changing it from `void` to returning `JsonReaderEx`. The diff itself does not demonstrate a direct link to breakpoints being ignored or promise error handling.

Here's the analysis of the code changes:

1. **Return Change**: Changing the return type of the `beginObject` method from `void` to `JsonReaderEx` and returning `this` suggests a design improvement to allow method chaining but doesn't specifically relate to fixing breakpoints or promise error handling.

2. **Relevance to Commit Message**: The specific issue mentioned in the commit message (WEB-14597) isn't directly addressed in the diff. Additionally, the commit refers to promise error handling, which doesn't appear in the shown changes.

Since there's no indication from the diff that it directly resolves the specified bug related to breakpoints, nor does it address promise error handling changes, this part of the commit does not align with the described bug fix nature in the message.

Thus, with respect to the provided diff, the conclusion is **NotBuggy**."
intellij-community,35717.json,ac323a030f4b82bc3a41f4bb970f770de5a10462,"@@ -1,10 +1,10 @@
     private void updateFile(final VirtualFile file) {
-      myQueue.queue(new Update(""ProblemUpdate"") {
+      myQueue.queue(new Update(file) {
         public void run() {
           if (isFileOpen(file)) {
             updateFileIcon(file);
             updateFileColor(file);
           }
         }
       });
     }",Buggy,"fixed bug with file staying red underwaved in editor tab
",Buggy,"The commit message refers to a bug concerning a file remaining flagged with a red underline in an editor tab, which suggests an error indication issue. The code change replaces a hardcoded string, ""ProblemUpdate"", with the dynamic `file` object as the argument for the `Update` constructor. 

This change likely affects how updates are queued for the file in question. By passing the `file` object instead of a static string identifier, the system might now correctly manage updates specific to each individual file, addressing possible issues with the status or representation of files in the editor. This implies fixing specific file association issues in the update mechanism, which aligns with resolving the issue described in the commit message.

Given this reasoning, the changes do suggest a bug fix related to file updates and their visual representation in the editor. Therefore, the conclusion is **Buggy**."
intellij-community,26650.json,aca5c269d7a12badc8892d6a204c4496583cd882,"@@ -1,4 +1,4 @@
   protected void buildPayload(Payload payload) {
     super.buildPayload(payload);
-    payload.add(myThreadId).add(myFrameId).add(""FRAME"").add(myExpression);
+    payload.add(""FRAME"").add(myExpression);
   }",Buggy,"fixed bug after refactoring
",Buggy,"The commit message suggests fixing a bug that emerged after refactoring. The diff shows a method modification in `buildPayload`. Previously, `myThreadId` and `myFrameId` were added to the `payload`. The new code removes these, directly adding ""FRAME"" and `myExpression`.

This change likely corrects a logic issue related to how the payload is constructed. Removing `myThreadId` and `myFrameId` might imply they were either incorrectly included or unnecessary for the intended functionality after the refactor. The simplification implies a possible alignment with a new structure or logic, aligning with the commit message about fixing a post-refactoring bug.

Considering that the commit aims to address a bug and that the changes correspond to a fundamental adjustment in method logic (which typically reflects a bug fix), the conclusion is **Buggy**."
intellij-community,15806.json,1a1325592a39001a011e5c0ece3664a582fc1fff,"@@ -1,9 +1,12 @@
-  public EditorWithProviderComposite findFileComposite(final VirtualFile file) {
+  public EditorWithProviderComposite findFileComposite(VirtualFile file) {
+    if (file instanceof BackedVirtualFile)
+      file = ((BackedVirtualFile)file).getOriginFile();
+
     for (int i = 0; i != getTabCount(); ++i) {
       final EditorWithProviderComposite editor = getEditorAt(i);
       if (editor.getFile().equals(file)) {
         return editor;
       }
     }
     return null;
   }",Buggy,"Add check of BackedVirtualFile in findFileComposite in EditorWindow.java
to fix problem with openedEditorWindow resolving.

Now if you try go to Usage of variables in Jupyter, Zeppelin Notebooks (BackedVirtualFile)
new window will be opened. We need to get original file for correct search of File Composite

GitOrigin-RevId: 5a3577cb3964fc0a7fa0c7fa91a4fed137fe2ebf",Buggy,"The commit message indicates a fix concerning `BackedVirtualFile` in the `findFileComposite` method of `EditorWindow.java`. The change in the code implements a check for whether the `file` parameter is an instance of `BackedVirtualFile`. If it is, the code gets the original file using `getOriginFile()`.

This adjustment aligns with the commit message, which describes an issue with resolving opened editor windows correctly when dealing with `BackedVirtualFile` types. By retrieving the original file, the updated method ensures that the search for the file correctly identifies the intended `EditorWithProviderComposite`.

Indicators of a bug fix here include:
- A specific check for `BackedVirtualFile` that adjusts the redundant or incorrect handling of such files.
- The goal of solving a problem related to file resolution, which was potentially causing incorrect behavior in the application's functionality.

Overall, the changes specifically address the problem outlined in the commit message and make logical corrections to ensure accurate file resolutions, preventing potential misbehavior with opened editors. Therefore, the label is **Buggy**."
intellij-community,6447.json,f96c4ad9fa786eb5990bebe12f45dbbe4f698642,"@@ -1,16 +1,17 @@
   public void beforeCharDeleted(char c, PsiFile file, Editor editor) {
     isTripleQuote = false;
     if (c == '""' || c == '\'' && CodeInsightSettings.getInstance().AUTOINSERT_PAIR_QUOTE) {
       final QuoteHandler quoteHandler = TypedHandler.getQuoteHandler(file, editor);
       if (quoteHandler == null || !(quoteHandler instanceof BaseQuoteHandler)) return;
 
       final int offset = editor.getCaretModel().getCurrentCaret().getOffset();
       String text = editor.getDocument().getText();
       boolean mayBeTripleQuote = offset >= 3 && offset + 2 < text.length();
       if (mayBeTripleQuote) {
         HighlighterIterator iterator = ((EditorEx)editor).getHighlighter().createIterator(offset);
-        boolean hasTripleQuoteAfter = text.charAt(offset - 1) == c && text.charAt(offset - 2) == c && text.charAt(offset - 3) == c;
+        boolean hasTripleQuoteAfter = offset + 2 < text.length() &&
+                                      text.charAt(offset) == c && text.charAt(offset + 1) == c && text.charAt(offset + 2) == c;
         isTripleQuote = quoteHandler.isOpeningQuote(iterator, offset - 1) && hasTripleQuoteAfter;
       }
     }
   }",Buggy,"PY-1779 Fix problem with removing pair of triple quotes

Check if there is triple quote to the right of the cursor
",Buggy,"The commit message discusses a problem with removing a pair of triple quotes. The provided code diff examines the presence of triple quotes to the right of the cursor, which aligns with the commit message, aiming to ensure accurate detection and handling of triple quotes.

Here are the changes and their rationale:

1. **Original Code Bug**: The original logic checked for triple quotes before the cursor, which doesn't align with the intent mentioned in the message. It was faulty for verifying the presence of triple quotes following the cursor.

2. **Modified Code Fix**: The updated logic correctly checks for the presence of triple quotes right after the current cursor position. It validates if the next three characters are identical to the character being deleted, thus accurately detecting the triple quote sequence.

3. **Commit Message Relevance**: The changes reflect the commit message wherein it identifies a bug related to identifying triple quotes. The logic fix assures the accurate detection of triple quotes post the cursor position, aligning with the described problem and solution in the message.

The updates correct the logic to ensure that the feature of triple quote deletion operates as intended, aligning the code behavior with its intended function as described in the commit message.

Therefore, these changes indicate a bug fix related to the handling of triple quotes, leading to the conclusion of **Buggy**."
intellij-community,11190.json,bf7f424b6599b6b662a69c63d5028263dcbee88b,"@@ -1,10 +1,10 @@
   public static String constructPythonPathCommand(Collection<String> pythonPath) {
     final String path = Joiner.on("", "").join(Collections2.transform(pythonPath, new Function<String, String>() {
       @Override
       public String apply(String input) {
-        return ""'"" + input.replace(""\\"", ""\\\\"") + ""'"";
+        return ""'"" + input.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"") + ""'"";
       }
     }));
 
     return ""sys.path.extend(["" + path + ""])"";
   }",Buggy,"fix problem with single quote in project path
http://youtrack.jetbrains.com/issue/PY-10179
",Buggy,"The commit message clearly describes the intent to fix a problem related to single quotes in project paths. The diff implements this by adding a `.replace(""'"", ""\\'"")` to handle single quotes within the path. This ensures that single quotes are correctly escaped, preventing potential issues when this path is processed. The change directly aligns with the commit message, indicating a bug fix for handling single quotes in path strings. Therefore, the correct conclusion is **Buggy**."
intellij-community,7728.json,9421b80596d7526f1637a84fb4bf457977dd635e,"@@ -1,16 +1,16 @@
     public boolean accept(File file) {
       final JavaSourceRootDescriptor rd = myBuildRootIndex.findJavaRootDescriptor(myContext, file);
       if (rd == null) {
         return true;
       }
-      final JpsModule moduleOfFile = rd.target.getModule();
-      if (myChunkModules.contains(moduleOfFile)) {
+      final ModuleBuildTarget targetOfFile = rd.target;
+      if (myChunkTargets.contains(targetOfFile)) {
         return true;
       }
-      Set<JpsModule> moduleOfFileWithDependencies = myCache.get(moduleOfFile);
-      if (moduleOfFileWithDependencies == null) {
-        moduleOfFileWithDependencies = ProjectPaths.getModulesWithDependentsRecursively(moduleOfFile, true);
-        myCache.put(moduleOfFile, moduleOfFileWithDependencies);
+      Set<BuildTarget<?>> targetOfFileWithDependencies = myCache.get(targetOfFile);
+      if (targetOfFileWithDependencies == null) {
+        targetOfFileWithDependencies = myBuildTargetIndex.getDependenciesRecursively(targetOfFile, myContext);
+        myCache.put(targetOfFile, targetOfFileWithDependencies);
       }
-      return Utils.intersects(moduleOfFileWithDependencies, myChunkModules);
+      return ContainerUtil.intersects(targetOfFileWithDependencies, myChunkTargets);
     }",Buggy,"external build: fixed problem with files remaining dirty after 'Make Project'
",Buggy,"The commit message indicates a fix for a build system issue where files remained ""dirty,"" meaning not properly marked as up to date after a build. The code diff provided suggests changes in how dependencies are determined for build targets as opposed to modules. 

These changes include several aspects:

1. **Target-Based Build Process**:  
   The diff changes from a module-based to a target-based approach:
   - `JpsModule` is replaced with `ModuleBuildTarget`, indicating a switch in focus from module dependencies to more granular build targets.

2. **Dependency Management**:
   - Dependency calculation changes from using `ProjectPaths.getModulesWithDependentsRecursively` to `myBuildTargetIndex.getDependenciesRecursively`. This switch suggests the new method may more accurately reflect target dependencies, crucial for determining which files need rebuilding.

3. **Performance and Accuracy**:
   - The introduced `ContainerUtil.intersects` for checking intersections implies a broader check may now be applied, possibly addressing issues with stale or incorrect state management.

The changes ensure that dependency tracking is aligned with the actual targets being built, likely solving issues where some files were not being updated or marked correctly due to outdated dependency resolution logic.

Given this context and analyzing the nature of changes in the code, these modifications align well with fixing the ""dirty files"" issue mentioned in the commit message.

The changes do signal an actual bug fix in the build process regarding dependencies and cache management. The conclusion is **Buggy**."
intellij-community,10628.json,cfedbcc1be3a6aec0e05d57995ce6945e112168c,"@@ -1,15 +1,21 @@
   protected JPanel createAdvancedSettings() {
     JComponent advancedSettings = null;
     if (myProjectGenerator instanceof PythonProjectGenerator)
       advancedSettings = ((PythonProjectGenerator)myProjectGenerator).getSettingsPanel(myProjectDirectory);
     else if (myProjectGenerator instanceof WebProjectTemplate) {
       advancedSettings = ((WebProjectTemplate)myProjectGenerator).getPeer().getComponent();
     }
     if (advancedSettings != null) {
       final JPanel jPanel = new JPanel(new VerticalFlowLayout());
       final HideableDecorator deco = new HideableDecorator(jPanel, ""Mor&e Settings"", false);
+      boolean isValid = checkValid();
+      deco.setOn(!isValid);
+      if (myProjectGenerator instanceof PythonProjectGenerator && !deco.isExpanded()) {
+        final ValidationResult result = ((PythonProjectGenerator)myProjectGenerator).warningValidation(getSdk());
+        deco.setOn(!result.isOk());
+      }
       deco.setContentComponent(advancedSettings);
       return jPanel;
     }
     return null;
   }",Buggy,"fixed PY-13503 Create New Project: Expand more setting by default if there is an error in configuration
",Buggy,"The commit message indicates a fix for an issue related to expanding settings when an error is present in project configuration. The diff adds logic to determine whether to expand the ""More Settings"" section by default when there is an error in the configuration:

1. The code introduces a `boolean isValid = checkValid();` statement to assess validity.
2. The `HideableDecorator` object `deco` is instructed to expand (`deco.setOn(!isValid);`) based on the validity result.
3. For `PythonProjectGenerator`, additional validation is conducted via `warningValidation(getSdk())`, and the decorator is set to expand based on this result (`deco.setOn(!result.isOk());`).

These changes align with the commit message, addressing a specific issue with the settings panel behavior when configuration errors are present. This makes it a targeted bug fix related to user interface behavior.

Thus, the answer is **Buggy**."
intellij-community,35455.json,04eb55fb65bb931432ac5b51786825882d53027a,"@@ -1,21 +1,26 @@
     private boolean runNextEvent() {
       long startedAt = System.currentTimeMillis();
       final RunnableInfo lastInfo = getNextEvent(true);
       myLastInfo = lastInfo;
 
       if (lastInfo != null) {
         try {
           doRun(lastInfo);
           lastInfo.markDone();
         }
-        catch (ProcessCanceledException ignored) { }
+        catch (ProcessCanceledException ignored) {
+
+        }
         catch (Throwable t) {
+          if (ApplicationManager.getApplication().isUnitTestMode()) {
+            ExceptionUtil.rethrow(t);
+          }
           LOG.error(t);
         }
         finally {
           if (!DEBUG) myLastInfo = null;
           TransactionGuardImpl.logTimeMillis(startedAt, lastInfo.runnable);
         }
       }
       return lastInfo != null;
     }",Buggy,"make exceptions thrown in invokeLater() and friends visible in tests to help fix errors in abandoned futures

GitOrigin-RevId: aec5c2221220e3567f67c9a35d050a61f06a3a2e",NotBuggy,"The commit message describes making exceptions visible in tests to address errors related to abandoned futures. The changes in the code fit this description, particularly the added logic for rethrowing the exception if in unit test mode. By rethrowing exceptions during testing, developers can catch errors that might otherwise go unnoticed during test execution.

This update does not directly fix a bug in the production code but improves debug visibility in test scenarios, which aids in diagnosing and fixing bugs. It enhances error-handling capabilities without altering existing behavior outside of tests.

Given the context and the nature of the change, it aligns with the commit message that aims at helping fix underlying issues by making exceptions more visible during testing. However, the changes themselves do not constitute a direct bug fix to production code but rather an improvement in the testing/debugging process.

Thus, while this change supports identifying and fixing bugs during testing, the diff does not indicate a bug fix within the production code itself. The classification heavily depends on how such improvements are considered relative to testing environments. For this analysis, the conclusion will be:

**NotBuggy**"
intellij-community,18837.json,7e6fe7d6c4d89fc902a471879c0244f8b19597db,"@@ -1,6 +1,8 @@
   public boolean isDumb() {
-    if (!ApplicationManager.getApplication().isReadAccessAllowed() && REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
+    if (!ApplicationManager.getApplication().isReadAccessAllowed() &&
+        Registry.is(""ide.check.is.dumb.contract"") &&
+        REPORTED_EXECUTIONS.add(ExceptionUtil.currentStackTrace())) {
       LOG.error(""To avoid race conditions isDumb method should be used only under read action or in EDT thread."");
     }
     return myState.get() != State.SMART;
   }",Buggy,"IDEA-CR-47405: IDEA-214525 Disable isDumb check by default

Need to fix found frequent problems and switch on it back

GitOrigin-RevId: 2a916fbfe43bb077042adf0e0515253c3aa7ce55
",NotBuggy,"The commit diff modifies the isDumb method by adding a condition to check the registry key ""ide.check.is.dumb.contract"" before logging an error about race conditions. This change indicates that the error logging, which was previously always checked if read access wasn't allowed, now also requires this registry condition to be true.

The commit message mentions disabling the isDumb check by default, which aligns with the code change, as it adds a layer of configuration control (through the registry key) over when this check should be enforced. However, this doesn't directly indicate a bug fix. Instead, it configures the code to enable or disable certain checks based on configuration.

There are no signs of error-handling updates, logical corrections, or exception-handling improvements directly related to a specific bug fix. The commit is more about changing when and how checks are performed rather than correcting an existing fault in the logic.

Thus, the conclusion is **NotBuggy**."
intellij-community,2087.json,90a381b2f133397b7cbbfac2a4966e528b3e91cf,"@@ -1,10 +1,10 @@
   public boolean isAsyncAllowed() {
     final LanguageLevel languageLevel = LanguageLevel.forElement(this);
     final String functionName = getName();
 
     return languageLevel.isAtLeast(LanguageLevel.PYTHON35) && (
       functionName == null ||
-      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT) ||
+      ArrayUtil.contains(functionName, PyNames.AITER, PyNames.ANEXT, PyNames.AENTER, PyNames.AEXIT, PyNames.CALL) ||
       !PyNames.getBuiltinMethods(languageLevel).containsKey(functionName)
     );
   }",Buggy,"PY-20662 Fixed: Making any magic method async displays an error, even though it is sometimes legal

Mark __call__ builtin as allowed to be async
",Buggy,"The commit message indicates a bug fix related to enabling async for certain magic methods in Python, specifically mentioning `__call__`. The diff aligns with this message by adding `PyNames.CALL` to the list of function names allowed to be async.

Here's how these changes indicate a bug fix:

1. **Purpose**: The function `isAsyncAllowed()` checks if async is allowed for certain Python magic methods starting from Python 3.5.

2. **Modification**: The diff adds `PyNames.CALL` to the list of names checked by `ArrayUtil.contains()`, which already includes methods like `__aiter__`, `__anext__`, etc.

3. **Context**: The commit message addresses a specific issue (PY-20662) whereby making any magic method async would raise an error. This change fixes that by marking `__call__` as allowed for async usage. This is relevant for Python versions 3.5 and above, as async functionalities were introduced then.

Given these observations, the change directly addresses a bug by modifying logic to prevent incorrect error generation when `__call__` is implemented as an async method. Therefore, the code changes align well with the commit message.

**Buggy**"
intellij-community,10885.json,2b57b19667f12b0967107bbb6dd4052825fed484,"@@ -1,4 +1,7 @@
   public String getValue() {
+    if (myListPopUp == null || !myListPopUp.isVisible()) {
+      return null; // Nothing is selected if list is invisible
+    }
     final Object value = myList.getSelectedValue();
     return ((value == null) ? """" : getElement(value).mySuggestion.getText());
   }",Buggy,"PY-11855 Run manage.py task improvements

Tests enabled, completion bug fixed
",Buggy,"The commit message describes improvements to a task and fixes a completion bug. The diff adds a check to return `null` if a list popup is not visible, ensuring no selection is assumed when the list isn't present. This prevents potential null dereferences or incorrect values in unseen states. Given the message and the diff, it aligns with a bug fix by addressing a logical error regarding visibility and selection. Therefore, this indicates a **Buggy** change."
intellij-community,38709.json,1e53ffa3e0e7fe08387b61f9aba2b48275685b41,"@@ -1,6 +1,8 @@
   public void setCancelButtonText(String text){
-    if (myDialog != null)
+    if (myDialog != null) {
       myDialog.changeCancelButtonText(text);
-    else
+    }
+    else {
       myCancelText = text;
+    }
   }",Buggy,"Fixing problem of progress window appearing over modal dialogs shown during runProcessWithProgressSynchronously().
",NotBuggy,"The commit message addresses a problem with the progress window appearing over modal dialogs, whereas the code change relates to adding braces for better structure and readability in the `setCancelButtonText` method. The change does not appear to address the behavior of modal dialogs or window layering.

The modifications do not resolve the issue specified in the commit message. Hence, they do not indicate a bug fix as described. 

The answer is **NotBuggy**."
intellij-community,18404.json,87fa91dd700f6b1cfc0885f0857cf501df46649f,"@@ -1,6 +1,7 @@
     private JComponent createErrorsLink() {
       IdeMessagePanel panel = new IdeMessagePanel(null, MessagePool.getInstance());
       panel.setBorder(JBUI.Borders.emptyRight(13));
+      panel.setOpaque(false);
       Disposer.register(this, panel);
       return panel;
     }",Buggy,"Fix error icon background on welcome screen

GitOrigin-RevId: 6412c1adb4627ebdd5895f3319a3878f7a68a44d
",Buggy,"The code change involves setting the panel's opacity to false by calling `panel.setOpaque(false)`. This modification likely addresses a visual issue on the welcome screen related to how the background of the error icon is rendered. When a component is opaque, it is responsible for painting all its pixels within its bounds. By setting `setOpaque(false)`, it instructs the component to let its ancestor's background show through, which can fix issues where the component's background could obscure or incorrectly display behind certain UI elements like icons.

Given the commit message's focus on fixing an error icon background, this change directly relates to it. The modification aligns with the commit message by aiming to resolve a UI presentation bug regarding the error icon on the welcome screen.

Thus, based on the commit message and the nature of the code change, the conclusion is **Buggy**."
intellij-community,17737.json,29ea6cca1f95c474adfc2715fe7d091e26d043d7,"@@ -1,104 +1,109 @@
     private void processModules(final DiffState state, File fileName) {
       final Difference.Specifier<ModuleRepr, ModuleRepr.Diff> modulesDiff = state.myModulesDiff;
       if (modulesDiff.unchanged()) {
         return;
       }
+
+      for (ModuleRepr moduleRepr : modulesDiff.added()) {
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
+      }
+      
       for (ModuleRepr removedModule : modulesDiff.removed()) {
-        myDelta.addDeletedClass(removedModule, fileName);
+        myDelta.addDeletedClass(removedModule, fileName); // need this for integrate
         myPresent.affectDependentModules(state, removedModule.name, null, true);
       }
 
       for (Pair<ModuleRepr, ModuleRepr.Diff> pair : modulesDiff.changed()) {
         final ModuleRepr moduleRepr = pair.first;
         final ModuleRepr.Diff d = pair.second;
         boolean affectSelf = false;
         boolean affectDeps = false;
         UsageConstraint constraint = null;
 
-        myDelta.addChangedClass(moduleRepr.name);
+        myDelta.addChangedClass(moduleRepr.name); // need this for integrate
 
         if (d.versionChanged()) {
           final int version = moduleRepr.getVersion();
           myPresent.affectDependentModules(state, moduleRepr.name, new UsageConstraint() {
             public boolean checkResidence(int dep) {
               final ModuleRepr depModule = myPresent.moduleReprByName(dep);
               if (depModule != null) {
                 for (ModuleRequiresRepr requires : depModule.getRequires()) {
                   if (requires.name == moduleRepr.name && requires.getVersion() == version) {
                     return true;
                   }
                 }
               }
               return false;
             }
           }, false);
         }
 
         final Difference.Specifier<ModuleRequiresRepr, ModuleRequiresRepr.Diff> requiresDiff = d.requires();
         for (ModuleRequiresRepr removed : requiresDiff.removed()) {
           affectSelf = true;
           if (removed.isTransitive()) {
             affectDeps = true;
             constraint = UsageConstraint.ANY;
             break;
           }
         }
         for (Pair<ModuleRequiresRepr, ModuleRequiresRepr.Diff> changed : requiresDiff.changed()) {
           affectSelf |= changed.second.versionChanged();
           if (changed.second.becameNonTransitive()) {
             affectDeps = true;
             // we could have created more precise constraint here: analyze if required module (recursively)
             // has only qualified exports that include given module's name. But this seems to be excessive since
             // in most cases module's exports are unqualified, so that any other module can access the exported API.
             constraint = UsageConstraint.ANY;
           }
         }
 
         final Difference.Specifier<ModulePackageRepr, ModulePackageRepr.Diff> exportsDiff = d.exports();
 
         if (!affectDeps) {
           for (ModulePackageRepr removedPackage : exportsDiff.removed()) {
             affectDeps = true;
             if (!removedPackage.isQualified()) {
               constraint = UsageConstraint.ANY;
               break;
             }
             for (Integer name : removedPackage.getModuleNames()) {
               final UsageConstraint matchName = UsageConstraint.exactMatch(name);
               if (constraint == null) {
                 constraint = matchName;
               }
               else {
                 constraint = constraint.or(matchName);
               }
             }
           }
         }
 
         if (!affectDeps || constraint != UsageConstraint.ANY) {
           for (Pair<ModulePackageRepr, ModulePackageRepr.Diff> p : exportsDiff.changed()) {
             final Collection<Integer> removedModuleNames = p.second.targetModules().removed();
             affectDeps |= !removedModuleNames.isEmpty();
             if (!removedModuleNames.isEmpty()) {
               affectDeps = true;
               for (Integer name : removedModuleNames) {
                 final UsageConstraint matchName = UsageConstraint.exactMatch(name);
                 if (constraint == null) {
                   constraint = matchName;
                 }
                 else {
                   constraint = constraint.or(matchName);
                 }
               }
             }
           }
         }
 
         if (affectSelf) {
           myPresent.affectModule(moduleRepr, myAffectedFiles);
         }
         if (affectDeps) {
           myPresent.affectDependentModules(state, moduleRepr.name, constraint, true);
         }
       }
     }",Buggy,"fix module-info integrate after errors (IDEA-181208)
",Buggy,"The commit message states, ""fix module-info integrate after errors."" This suggests the commit addresses integration issues related to modules. Examining the code changes:

1. **Added Handling for Added Modules**: 
   - The code now includes a loop for processing added modules with `modulesDiff.added()`, which calls `myDelta.addChangedClass(moduleRepr.name)`. This indicates that previously, added modules may not have been integrated correctly, which could lead to integration errors being handled here.

2. **Comments for Clarity**:
   - Comments ""need this for integrate"" have been added next to certain method calls, suggesting emphasis on ensuring these calls are now correctly impacting the integration of changes.

3. **Logic for Affected Modules**:
   - Adjustments were made to consistently invoke `myDelta.addChangedClass` within different conditions to handle all module state changes properly.

Considering these points, the changes suggest an adjustment to the module handling logic to address specific integration errors and ensure all conditions affecting module states are processed correctly. These corrections align with resolving errors in module integration.

Thus, the changes align with the commit message and indicate a bug fix related to integration errors in module processing. The conclusion is **Buggy**."
intellij-community,29881.json,89451925c29655bb8771859c792c18d6b28c9e55,"@@ -1,19 +1,19 @@
     public void visitPyFunction(final PyFunction node) {
       PyDecoratorList decolist = node.getDecoratorList();
       if (decolist != null) {
         PyDecorator[] decos = decolist.getDecorators();
         if (decos.length > 1) {
           for (int i = decos.length - 1; i >= 1; i -= 1) {
             PyDecorator deco = decos[i];
             String deconame = deco.getName();
             if ((PyNames.CLASSMETHOD.equals(deconame) || PyNames.STATICMETHOD.equals(deconame)) && deco.isBuiltin()) {
               registerProblem(
                 decos[i-1],
                 PyBundle.message(""INSP.decorator.receives.unexpected.builtin""),
-                ProblemHighlightType.GENERIC_ERROR_OR_WARNING
+                ProblemHighlightType.GENERIC_ERROR_OR_WARNING, null, new RemoveDecoratorQuickFix()
               );
             }
           }
         }
       }
     }",Buggy,"added Remove decorator quick fix to the Problematic nesting of decorators inspection
",NotBuggy,"The commit message states that a ""Remove decorator quick fix"" was added to the inspection for ""Problematic nesting of decorators."" The git diff shows that the call to `registerProblem` within a decorator inspection method is updated to include a new `RemoveDecoratorQuickFix`. This addition suggests an enhancement to the inspection tool by providing a suggested correction, or ""quick fix,"" for identified issues related to decorator nesting.

The change aligns perfectly with the commit message, which indicates adding a quick fix feature. This modification doesn't correct a bug in the existing functionality—it's an enhancement providing better tooling for users encountering specific decorator issues.

Therefore, although the change introduces an improvement to the inspection tool and is directly relevant to the commit message, it does not indicate a bug fix in code logic or functionality errors. The conclusion is **NotBuggy**."
intellij-community,27903.json,0b9404bccbc274086537d9c49bf8a31a26ba4d7a,"@@ -1,9 +1,11 @@
   private static boolean distinctTokens(@Nullable IElementType token1, @Nullable IElementType token2) {
     if (token1 == token2) return false;
     if (token1 == null || token2 == null) return true;
     if (StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token1) ||
         StringEscapesTokenTypes.STRING_LITERAL_ESCAPES.contains(token2)) return false;
-    if (!token1.getLanguage().is(token2.getLanguage())) return true;
-    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(token1.getLanguage());
+    if (token1 != TokenType.WHITE_SPACE && token2 != TokenType.WHITE_SPACE && !token1.getLanguage().is(token2.getLanguage())) return true;
+    Language language = token1.getLanguage();
+    if (language == Language.ANY) language = token2.getLanguage();
+    BidiRegionsSeparator separator = LanguageBidiRegionsSeparator.INSTANCE.forLanguage(language);
     return separator.createBorderBetweenTokens(token1, token2);
   }",Buggy,"IDEA-169157 Right-to-left bug - fix for JSX files
",Buggy,"The commit message mentions a ""Right-to-left bug"" fix for JSX files. The diff modifies the `distinctTokens` method, where it addresses the handling of tokens with different languages by adding extra checks:

1. It enhances the condition checking whether the languages of two tokens differ, explicitly excluding white space tokens from causing a distinction.
2. It adds logic to handle cases where `token1.getLanguage()` might be `Language.ANY`, in which case it defaults to `token2.getLanguage()`.
3. Both changes are likely related to improving how tokens are separated in a right-to-left context, which aligns with handling language-specific token differences in bidirectional text scenarios, such as those found in JSX files.

These updates suggest the alterations target a right-to-left language processing problem, specifically concerning token language identification and separation. This aligns well with the concept of fixing a right-to-left bug.

Therefore, the changes in the code are consistent with the commit message referring to a ""Right-to-left bug - fix for JSX files"". Hence, the determination is **Buggy**."
intellij-community,35807.json,6555125d8e28088da575e0b10ca50780c50642d0,"@@ -1,22 +1,23 @@
   public static ScriptOutput executeScriptInConsoleWithFullOutput(String exePathString,
                                                                   @Nullable VirtualFile scriptFile,
                                                                   @Nullable String workingDirectory,
                                                                   long timeout,
                                                                   ScriptOutputType scriptOutputType,
                                                                   @NonNls String... parameters)
     throws ExecutionException {
     final OSProcessHandler processHandler = execute(exePathString, workingDirectory, scriptFile, parameters);
 
     final StringBuilder standardOutput = scriptOutputType.readStandardOutput() ? new StringBuilder() : null;
     final StringBuilder errorOutput = scriptOutputType.readErrorOutput() ? new StringBuilder() : null;
     final StringBuilder mergedOutput =
       (scriptOutputType.readStandardOutput() && scriptOutputType.readErrorOutput()) ? new StringBuilder() : null;
     addReadingProcessListener(scriptOutputType, processHandler, standardOutput, errorOutput, mergedOutput);
+    processHandler.startNotify();
 
     if (!processHandler.waitFor(timeout)) {
       LOG.warn(""Process did not complete in "" + timeout / 1000 + ""s"");
       throw new ExecutionException(ExecutionBundle.message(""script.execution.timeout"", String.valueOf(timeout / 1000)));
     }
     LOG.debug(""script output: "" + standardOutput);
     return new ScriptOutput(scriptOutputType, standardOutput, errorOutput, mergedOutput);
   }",Buggy,"WI-6566 Empty output in command line toolwindow:
fixed bug with too early processHandler.startNotify() in ScriptRunnerUtil; removed command execution ""cmd \c"" wrapping.
",Buggy,"The commit message indicates a bug fix regarding an early invocation of `processHandler.startNotify()` in `ScriptRunnerUtil`. The diff showcases a significant change: moving `processHandler.startNotify()` after the process listener has been set up. This likely resolves an issue where output wasn't captured properly if the process started before listeners were ready.

This adjustment corresponds well with the described issue in the commit message. By executing `startNotify()` after setting up the listener, the output would get captured fully, resolving any previously encountered issues with empty output.

Given the specificity of the commit message and the nature of the change, it is indeed a bug fix. Therefore, the changes are:

**Buggy**"
intellij-community,18229.json,6ee5fbcfe50c4e017eff321c5ebe208dd89d6945,"@@ -1,9 +1,10 @@
   public ExternalSystemTaskExecutionSettings clone() {
     ExternalSystemTaskExecutionSettings result = new ExternalSystemTaskExecutionSettings();
     result.setExternalSystemIdString(getExternalSystemIdString());
     result.setExternalProjectPath(getExternalProjectPath());
     result.setVmOptions(getVmOptions());
+    result.setScriptParameters(getScriptParameters());
     result.setTaskNames(ContainerUtilRt.newArrayList(getTaskNames()));
     result.setTaskDescriptions(ContainerUtilRt.newArrayList(getTaskDescriptions()));
     return result;
   }",Buggy,"IDEA-117792 Gradle Run/Debug Configuration: support adding Gradle specific parameters. Fix after review.
+ fix bug - http://youtrack.jetbrains.com/issue/IDEA-118470
",Buggy,"The commit is linked to a bug fix, IDEA-118470. The code diff adds a line to copy script parameters in the clone method of ExternalSystemTaskExecutionSettings. This ensures all configurations are correctly cloned, suggesting previous oversight (missing script parameters). The answer is **Buggy**."
hector,2566.json,4e038b0c1c911e81c2ce68123f3ecdb588c0e090,"@@ -1,3 +1,3 @@
   public K getKey() {
-    return keySerializer.fromByteBuffer(entry.getKey()); 
+    return keySerializer.fromByteBuffer(entry.getKey().duplicate());
   }",Buggy,"#345: fix bug with multiple bytebuffer readings
",Buggy,"The commit message specifies a bug fix related to issues with multiple ByteBuffer readings. The change in the code reflects this statement by modifying the behavior of how the ByteBuffer is passed to the `fromByteBuffer` method. By using `entry.getKey().duplicate()`, the code now duplicates the ByteBuffer before reading, which prevents modifications to the original buffer's position, limit, or mark by subsequent read operations. This change addresses potential issues when the ByteBuffer might be reused or shared among different reads, thereby ensuring the correctness of read operations.

The changed code directly correlates with the described bug about multiple ByteBuffer readings. The use of `duplicate()` suggests that the original position and mark of the ByteBuffer remain unchanged while allowing safe reads.

Thus, this is indeed a bug fix addressing the issue highlighted in the commit message. Therefore, the analysis concludes with **Buggy**."
hector,1056.json,77b7e5e1215b64a40f87488849cead5aa67e6ac4,"@@ -1,27 +1,30 @@
   public static void main(String[] args) throws HectorException {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
+    Keyspace keyspace = null;
     try {
-      Keyspace keyspace = client.getKeyspace(""Keyspace1"");
+      keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+    } finally {
       // This line makes sure that even if the client had failures and recovered, a correct
       // releaseClient is called, on the up to date client.
-      client = keyspace.getClient();
-    } finally {
-      // return client to pool. do it in a finally block to make sure it's executed
-      pool.releaseClient(client);
+      if (keyspace != null) {
+        client = keyspace.getClient();
+        // return client to pool. do it in a finally block to make sure it's executed
+        pool.releaseClient(client);
+      }
     }
   }",Buggy,"Fix a bug when releasing a client in the ExampleClient
",Buggy,"The commit message says it fixed a bug when releasing a client, which is consistent with the changes in the code.

### Analysis:

1. **Original Code Structure:**
   - The client retrieval using `keyspace.getClient()` and the release of the client were inside the `finally` block without checking if the `keyspace` was initialized. This could cause a NullPointerException if an exception happens before `keyspace` is initialized.

2. **Modification in the Code:**
   - The code changes moved the `client = keyspace.getClient();` statement inside an `if (keyspace != null)` check within the `finally` block. This ensures that the client is only released if the `keyspace` has been successfully initialized.
   - The modification correctly prevents errors when attempting to access methods on a null `keyspace` object.

### Conclusion:

The code changes ensure safe client release by addressing a potential `NullPointerException`. Given these improvements align with the stated bug fix need in the commit message, the changes are indicative of a bug fix.

The answer is **Buggy**."
hector,1056.json,7e7fd435503f29b0bfbc6ec8a997e28ecf323a22,"@@ -1,25 +1,28 @@
   public static void main(String[] args) throws IllegalStateException, PoolExhaustedException,
       Exception {
     CassandraClientPool pool = CassandraClientPoolFactory.INSTANCE.get();
     CassandraClient client = pool.borrowClient(""tush"", 9160);
     // A load balanced version would look like this:
     // CassandraClient client = pool.borrowClient(new String[] {""cas1:9160"", ""cas2:9160"", ""cas3:9160""});
 
     try {
       Keyspace keyspace = client.getKeyspace(""Keyspace1"");
       ColumnPath columnPath = new ColumnPath(""Standard1"");
       columnPath.setColumn(bytes(""column-name""));
 
       // insert
       keyspace.insert(""key"", columnPath, bytes(""value""));
 
       // read
       Column col = keyspace.getColumn(""key"", columnPath);
 
       System.out.println(""Read from cassandra: "" + string(col.getValue()));
 
+      // This line makes sure that even if the client had failures and recovered, a correct
+      // releaseClient is called, on the up to date client.
+      client = keyspace.getClient();
     } finally {
       // return client to pool. do it in a finally block to make sure it's executed
       pool.releaseClient(client);
     }
   }",Buggy,"Fix ExampleClient to release the correct client even when there's possible errors and failover
(cherry picked from commit 184b6460812c21e7fde77821245848f9ed2f3e6e)
",Buggy,"This commit message indicates a fix related to properly releasing a client back to the pool, even in cases of errors or failover. Analyzing the code changes:

1. An assignment `client = keyspace.getClient();` is added before releasing the client back to the pool.
2. This additional line ensures that the client object being released is current, possibly after failover or recovery by reassigning `client` with the up-to-date client reference from `keyspace`.

The change addresses situations where the client in use may have changed due to internal handling, like failover, which could potentially lead to incorrect client objects being released back into the pool or even leaving some clients unreleased. In the context of resource management, especially with connection pooling, this change is important to prevent resource leaks and ensure that the pool's state stays consistent.

The intent matches the commit message as it involves error handling and failover scenarios to fix logical issues in resource management. Therefore, the modifications signify a bug fix aimed at improving the correctness of the client release process.

Conclusion: **Buggy**."
hector,1991.json,4be28e7d287b02b6f1036b415c5e732ec804065f,"@@ -1,14 +1,17 @@
   private String getContextPath() {
-    URL url = getClass().getClassLoader().getResource(""/"");
+    ClassLoader loader = getClass().getClassLoader();
+    if(loader == null)
+     return null;
+    URL url = loader.getResource(""/"");
     if (url != null) {
       String[] elements = url.toString().split(""/"");
       for (int i = elements.length - 1; i > 0; --i) {
         // URLs look like this: file:/.../ImageServer/WEB-INF/classes/
         // And we want that part that's just before WEB-INF
         if (""WEB-INF"".equals(elements[i])) {
           return elements[i - 1];
         }
       }
     }
     return null;
   }",Buggy,"fixed a bug which causes NullPointerException in JmxMonitor on some platforms.
",Buggy,"The commit message states that it fixes a NullPointerException in `JmxMonitor`. The changes in the diff address potential null values by adding a check to ensure the `ClassLoader` is not null before it is used to get a resource. The original code could result in a `NullPointerException` if `getClass().getClassLoader()` returned null, which is a situation that could occur on some platforms. The change involves adding a null check before proceeding with operations on the `loader` object.

The modification aligns with the commit message's description of fixing a bug related to a `NullPointerException`. The code ensures that the `ClassLoader` is valid before attempting to use it, thereby preventing the exception. 

Therefore, the answer is **Buggy**."
hector,115.json,872da019f266973c0bf570d927371e33d838b4e8,"@@ -1,28 +1,28 @@
   public boolean addColumnToCollection(CFMappingDef<?> cfMapDef, Object obj, String colName,
       byte[] colValue) {
     // if can parse, then at least adheres to formatting
     CollectionItemColName collColumnName;
     try {
       collColumnName = parseCollectionItemColName(colName);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     // get property from mapping def - if not there, then isn't a collection
     // (but probably a problem elsewhere)
-    PropertyMappingDefinition md = cfMapDef.getPropMapByPropName(collColumnName.getPropertyName());
+    PropertyMappingDefinition md = cfMapDef.getPropMapByColumnName(collColumnName.getPropertyName());
     if (null == md) {
       return false;
     }
 
     Collection<Object> coll;
     try {
       coll = (Collection<Object>) reflectionHelper.invokeGetter(obj, md);
     } catch (HectorObjectMapperException e) {
       return false;
     }
 
     Object value = deserializeCollectionValue(colValue);
     coll.add(value);
     return true;
   }",Buggy,"fix bug: when mapping java collections, if property name different from column name, would not map correctly
",Buggy,"The commit message indicates a bug fix regarding the mapping of Java collections when the property name differs from the column name. In the code diff, the method `cfMapDef.getPropMapByPropName()` is replaced with `cfMapDef.getPropMapByColumnName()`. This change likely addresses the bug described: previously, the mapping retrieved was based on property names but has now switched to column names. This alteration ensures that the correct mapping is used even when property names and column names differ, aligning with the commit message. Therefore, the commit fixes a bug related to collection property mappings. The answer is **Buggy**."
hector,2740.json,bf6efaeda600325fdeed53aad52e1700a0556cfd,"@@ -1,28 +1,28 @@
   public long createClock() {
     switch (this) {
 	    case MICROSECONDS:
 	    	return System.currentTimeMillis() * ONE_THOUSAND;
 	    case MICROSECONDS_SYNC:
 	      // The following simulates a microseconds resolution by advancing a static counter
 	      // every time a client calls the createClock method, simulating a tick.
 	      long us = System.currentTimeMillis() * ONE_THOUSAND;
 	      // Synchronized to guarantee unique time within and across threads.
 	      synchronized (ClockResolution.class) {
 	         if (us > lastTime) {
 	        	 lastTime = us;
 	         } else {
 	        	 // the time i got from the system is equals or less
 	        	 // (hope not - clock going backwards)
 	        	 // One more ""microsecond""
-	        	 us = lastTime++;
+	        	 us = ++lastTime;
 	         }
 	      }
 	      return us;
 	    case MILLISECONDS:
 	      return System.currentTimeMillis();
 	    case SECONDS:
 	      return System.currentTimeMillis() / 1000;
 	}
 
     return System.currentTimeMillis();
   }",Buggy,"Fix bug. Increment lastTime first and later assign

The lastTime was getting incremented right after assigning it to us.
",Buggy,"The commit message states a bug fix related to incrementing `lastTime` and its assignment. The diff changes `us = lastTime++;` to `us = ++lastTime;`. This modification corrects the behavior by ensuring `lastTime` is incremented before being assigned to `us`. The original code assigned `lastTime` to `us`, then incremented `lastTime`, resulting in `us` being one less than `lastTime`. The change makes sure `us` and `lastTime` are equal after the increment, fitting the described bug fix. Thus, the changes in the diff align with the commit message, indicating a bug fix was indeed performed.

The answer is **Buggy**."
hector,52.json,d0401ba77a823d118c98a74db02678c948cf33bc,"@@ -1,16 +1,26 @@
   private byte[] generateColumnFamilyKeyFromPkObj(CFMappingDef<?> cfMapDef, Object pkObj) {
     List<byte[]> segmentList = new ArrayList<byte[]>(cfMapDef.getKeyDef().getIdPropertyMap().size());
-
+    
+    List<String> rm1 = new ArrayList<String>();
+    List<String> rm2 = new ArrayList<String>();
+    
     if (cfMapDef.getKeyDef().isComplexKey()) {
-      for (PropertyDescriptor pd : cfMapDef.getKeyDef().getPropertyDescriptorMap().values()) {
-        segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
-            new DefaultConverter()));
+    	
+      Map<String, PropertyDescriptor> propertyDescriptorMap = cfMapDef.getKeyDef().getPropertyDescriptorMap(); 	
+      Map<String, PropertyMappingDefinition> idPropertyMap =  cfMapDef.getKeyDef().getIdPropertyMap();
+    	
+      for (String key : cfMapDef.getKeyDef().getIdPropertyMap().keySet()) {
+    	  PropertyDescriptor pd = propertyDescriptorMap.get(key);
+    	  segmentList.add(callMethodAndConvertToCassandraType(pkObj, pd.getReadMethod(),
+    	            new DefaultConverter()));
       }
+     
+    	 
     } else {
       PropertyMappingDefinition md = cfMapDef.getKeyDef().getIdPropertyMap().values().iterator()
                                              .next();
       segmentList.add(md.getConverter().convertObjTypeToCassType(pkObj));
     }
 
     return keyConcatStrategy.concat(segmentList);
   }",Buggy,"Fix for unordered Composite Key when finding values by key

A problem occurred when you attempted to find an object by composite
key.  The composite would be persisted in order by the order of the
composite fields in the class.  When attempting to find the object the
objectMapper would use the natural ordering of the field names instead.
",Buggy,"The commit message refers to a bug fix involving the ordering of composite keys when finding objects. The diff shows changes to the method generateColumnFamilyKeyFromPkObj where the order of properties used to form the key is changed. Originally, it iterated the properties using the values method of a map, which does not guarantee order. The updated code explicitly iterates over the keySet of the idPropertyMap map. This suggests that the composite key is now constructed following the explicit order of keys from the idPropertyMap map, aligning with the message about fixing unordered composite keys. This change addresses a potential issue with inconsistent ordering, which could lead to finding incorrect objects.

The alignment between the commit message and the code changes indicates that this is indeed a bug fix. Therefore, the conclusion is **Buggy**."
hector,2393.json,befd460c79a1d16c64a1a9857678621106e4a111,"@@ -1,13 +1,16 @@
-        public boolean hasNext()
-        {
-            boolean retval = false;
-            if (isStart)
-            {
-                retval = res.hasResults();
-            }
-            else
-            {
-                retval = res.hasNext();
-            }
-            return retval;
-        }

+		public boolean hasNext() 
+		{
+			boolean retval = false;
+			if (isStart) 
+			{
+				if(res.hasResults() || res.hasNext()) 
+				{
+					retval = true; 
+				}
+			} 
+			else 
+			{
+				retval = res.hasNext();
+			}
+			return retval;
+		}",Buggy,"Fix bug in hasNext when no result for a key is returned
",Buggy,"The commit message specifies a fix to the `hasNext` method when no result for a key is returned. Examining the code changes:

1. **Original Implementation:**
   - The method checks `res.hasResults()` if `isStart` is true, otherwise checks `res.hasNext()`.

2. **Updated Implementation:**
   - The method now checks if either `res.hasResults()` or `res.hasNext()` is true if `isStart` is true. 
   - If either condition is true, `retval` is set to true.

**Analysis:**
- The original implementation only returned true if `res.hasResults()` was true when `isStart` is true. 
- The updated logic introduces an additional condition with `res.hasNext()`. This ensures `retval` is true if there are ongoing results after the start, catering to a scenario the previous implementation might have missed.
  
This change corrects a logical flaw allowing `hasNext` to reflect the existence of subsequent elements more accurately, especially when starting. This aligns with the commit message stating a fix for when no result for a key is returned, solidifying the assurance of element existence at start. Therefore, this change represents a bug fix.

**Conclusion:** **Buggy**"
cucumber-jvm,1443.json,5f8f31b3d2b34050a93e27990ed25ce9274f49ac,"@@ -1,22 +1,23 @@
     public void newWorld() {
+        stepDefinitions.clear();
         MutablePicoContainer pico = new DefaultPicoContainer();
         for(Class stepsClass : stepsClasses) {
             pico.addComponent(stepsClass);
         }
 
         for(Object stepObject : pico.getComponents()) {
             for (Method method : stepObject.getClass().getMethods()) {
                 String regexpString = null;
                 if (method.isAnnotationPresent(Given.class)) {
                     regexpString = method.getAnnotation(Given.class).value();
                 } else if (method.isAnnotationPresent(When.class)) {
                     regexpString = method.getAnnotation(When.class).value();
                 } else if (method.isAnnotationPresent(Then.class)) {
                     regexpString = method.getAnnotation(Then.class).value();
                 }
                 if(regexpString != null) {
                     stepDefinitions.add(new StepDefinition(regexpString, stepObject, method));
                 }
             }
         }
     }",Buggy,"Fix Ambiguous problem for Cucumber Java
",Buggy,"Analyzing the provided Java code diff and commit message, the following observations can be made:

1. **Commit Message**: The message states ""Fix Ambiguous problem for Cucumber Java"", indicating a fix regarding ambiguity in step definitions commonly encountered in Cucumber for Java.

2. **Code Changes**: The change introduces a call to `stepDefinitions.clear();` at the beginning of the `newWorld()` method.

3. **Context and Purpose**:
   - The purpose of the `newWorld()` method appears to be to set up a new context or environment for Cucumber step definitions.
   - Without clearing `stepDefinitions` at the start, previously added definitions may persist across contexts, potentially leading to ambiguity since multiple step definitions could match the same step, which is a known issue in Cucumber when not handled correctly.

4. **Resolving Ambiguity**:
   - By clearing `stepDefinitions`, any previously registered step definitions are removed before starting to register new ones within the current context.
   - This helps avoid the scenario where multiple step definitions could potentially match the same Gherkin step due to stale entries.

Given that the change directly addresses a fundamental issue in Cucumber (step definition ambiguity) by ensuring `stepDefinitions` does not retain prior values, it aligns well with the commit message indicating a bug fix for an ""ambiguous problem"".

Therefore, the code change reflects a genuine bug fix concerning the management of step definitions in Cucumber, and the verdict is **Buggy**."
cucumber-jvm,1018.json,d8406dd1cbd14b82a79ccf5205f4367c9883a768,"@@ -1,10 +1,10 @@
         private String calculateElementName(cucumber.api.TestCase testCase) {
             String testCaseName = testCase.getName();
             if (testCaseName.equals(previousTestCaseName)) {
-                return testCaseName + (includesBlank(testCaseName) ? "" "" : ""_"") + ++exampleNumber;
+                return Utils.getUniqueTestNameForScenarioExample(testCaseName, ++exampleNumber);
             } else {
                 previousTestCaseName = testCase.getName();
                 exampleNumber = 1;
                 return testCaseName;
             }
         }",Buggy,"Android: Fix Cucumber execution on Gradle (#1094)

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

* Fix AndroidInstrumentationReporter for Gradle builds

* the connected check tasks of the Android/Gradle build system do not like non-unique test names
* we add a unique index to the test names if they are non-unique (e.g. on scenario outlines with multiple examples)
* for this bug fix, we provide a unit test

This is a re-integration of PR-1094. The original pull request code was performed on 1.2.6-SNAPSHOT base. This code bases on version 2.3.2-SNAPSHOT.

* Update Cukeulator example project to work with newest Android build tools

- Update to Android Studio 3.0.1, SDK 26+ and Gradle 4.1
- Replace Instrumentation class by CucumberRunner
- CucumberRunner uses AndroidJUnitRunner (JUnit 4+)
- CalculationSteps class uses ActivityTestRule instead of deprecated ActivityInstrumentationTestCase2
- Fix permissions to write reports on internal storage

* Improve Cukeulator example project

* Update README.md
* Enable local Maven dependencies for better development experience
* Describe, how to use Cukeulator example project with locally built Cucumber-JVM

* Rename duplicated test case names in AndroidInstrumentationReporter like JUnitFormatter in cucumber-core

* Fix typo

* Improve readability of AndroidInstrumentationReporterTest

* Share common logic for test case name between JUnitFormatter and AndroidInstrumentationReporter

* Improve code quality

- Create merged method calculateUniqueTestName from getUniqueTestName and ensureUniqueTestName and make it private
- Use better test method name: test_case_names_are_unique_on_equal_scenario_names (instead of scenario_outline_all_test_names_unique)
- Refactor test code: now it should be readable

* Change misleading variable names

* [Android] Split up test of making test names unique.

To provide better documentation of the functionality, that is:
* test names within feature are made unique by appending blank and
  number
* test names within are made unique by appending underscore and number
  when no blank in name
* test names in different features can be the same
* test names are made unique also when not consecutive
",Buggy,"The commit message describes multiple improvements and bug fixes, specifically addressing an issue with duplicate test names in the Android/Gradle build system. The diff provided is a small part of these changes, altering how unique test names are calculated when duplications occur. 

Initially, a test name is appended with either a space or an underscore followed by a number if it matches a previous test case name. The code change replaces this logic with a call to `Utils.getUniqueTestNameForScenarioExample`, which likely centralizes and possibly improves the logic for generating unique test names.

The focus on ensuring unique test names aligns with the message describing a fix for the AndroidInstrumentationReporter where non-unique test names caused issues in Gradle builds. The change to use a utility method for handling unique names indicates consolidation, likely addressing and fixing any inconsistencies or errors in test name generation.

Considering all this and the additional context provided in the commit, it can be concluded that the change is a bug fix. Therefore, the answer is **Buggy**."
hazelcast,11513.json,7a17eba9e37e1ce2953f5320c56c13c92620953d,"@@ -1,12 +1,11 @@
     public boolean shouldWait() {
         WriteBehindQueue<DelayedEntry> writeBehindQueue = store.getWriteBehindQueue();
-        int size = writeBehindQueue.size();
-        if (size == 0) {
+        DelayedEntry entry = writeBehindQueue.peek();
+        if (entry == null) {
             return false;
         }
 
-        DelayedEntry entry = writeBehindQueue.peek();
         long currentSequence = entry.getSequence();
-        return entry != null && currentSequence <= this.sequence
-                && size + currentSequence - 1 >= this.sequence;
+        return currentSequence <= this.sequence
+                && writeBehindQueue.size() + currentSequence - 1 >= this.sequence;
     }",Buggy,"Fixes findbugs problem
",Buggy,"The commit message mentions fixing a ""findbugs problem,"" indicating potential issues such as null dereferencing or inefficient code. 

Analyzing the diff:
1. The check for `entry == null` is moved before accessing `entry.getSequence()`. The original code did the reverse, risking a `NullPointerException` if the queue is empty.
2. The null check directly after `peek()` ensures safe access to `entry.getSequence()`.

The change improves code safety by preventing possible null dereference and aligns with typical static analysis tool recommendations like those from FindBugs.

These modifications focus on stability and correctness. Therefore, the changes reflect an error-handling improvement, aligning with the message about fixing a ""findbugs problem.""

**Buggy**"
hazelcast,27666.json,515671c5314975716054e45c0866ffd8be64f4c2,"@@ -1,17 +1,17 @@
     private void updateConfig(AliasedDiscoveryConfig config, Node node) {
         NamedNodeMap attributes = node.getAttributes();
         for (int a = 0; a < attributes.getLength(); a++) {
             Node att = attributes.item(a);
             String value = getTextContent(att).trim();
             if (""enabled"".equals(lowerCaseInternal(att.getNodeName()))) {
                 config.setEnabled(getBooleanValue(value));
             } else if (att.getNodeName().equals(""connection-timeout-seconds"")) {
                 config.setProperty(""connection-timeout-seconds"", value);
             }
         }
         for (Node n : childElements(node)) {            
-            String key = cleanNodeName(n, ""eureka"".equals(n.getParentNode().getLocalName()));
+            String key = cleanNodeName(n, !""eureka"".equals(n.getParentNode().getLocalName()));
             String value = getTextContent(n).trim();
             config.setProperty(key, value);
         }
     }",Buggy,eureka lowercase bug fix,Buggy,"The diff changes a condition that impacts how node names are cleaned. Previously, `cleanNodeName` expected `true` for nodes under ""eureka"" to perform specific cleaning. The change to `!""eureka"".equals(...)` implies the logic was incorrect, possibly causing errors in configuration if an ""eureka"" parent node required special handling. This adjustment suggests an intended fix related to processing nodes under ""eureka,"" consistent with the ""eureka lowercase bug fix"" in the commit message. This indicates a bug fix. Therefore, the answer is **Buggy**."
hazelcast,16541.json,6ce43d4090fe4924dfc7dc45583a328e25bb1e9f,"@@ -1,14 +1,15 @@
     public void run() throws Exception {
         CollectionWrapper wrapper = getCollectionWrapper();
-        if (wrapper == null || wrapper.getVersion() == version){
+        if (wrapper == null || wrapper.getVersion() != version){
             notify = false;
             return;
         }
+        wrapper.incrementAndGetVersion();
         for (Operation op: opList){
             op.setNodeEngine(getNodeEngine()).setServiceName(getServiceName()).setPartitionId(getPartitionId());
             op.beforeRun();
             op.run();
             op.afterRun();
         }
         getOrCreateContainer().unlock(dataKey, getCallerUuid(), threadId);
     }",Buggy,"Multimap commit txn version bug fixed
",Buggy,"The commit message indicates a fix related to a transaction version bug for a Multimap component. Let's review the changes:

1. **Condition Change**:
   - The conditional check was altered from `wrapper.getVersion() == version` to `wrapper.getVersion() != version`. This change suggests that the previous logic might have allowed operations to proceed only when versions were the same, which could have been incorrect behavior. 
   - The revised condition intends to stop the process if the versions do not match, indicating a correction to ensure proper synchronization or consistency model.

2. **Version Increment**:
   - The addition of `wrapper.incrementAndGetVersion()` updates the version if the condition allows the continuation of the method. This change implies that once an operation is successfully executed, the version must be incremented, which is standard in transaction/version management to track changes and ensure data consistency.

Both modifications collectively suggest addressing a synchronization or transaction issue where the intended version control logic was not correctly applied, potentially leading to erroneous transactions or application states.

Given these observations, the changes indeed align with the commit message's indication of a ""txn version bug"" and constitute a bug fix related to version management in multi-threaded operations.

Thus, the evaluation concludes with **Buggy**."
hazelcast,28391.json,8a5e458780154dfd44ef41dd02770456c19c7dbf,"@@ -1,4 +1,13 @@
     public final void sendResponse(Object value) {
         OperationResponseHandler responseHandler = getOperationResponseHandler();
-        responseHandler.sendResponse(this, value);
+        if (responseHandler == null) {
+            if (value instanceof Throwable) {
+                // in case of a throwable, we want the stacktrace.
+                getLogger().warning(""Missing responseHandler for "" + toString(), (Throwable) value);
+            } else {
+                getLogger().warning(""Missing responseHandler for "" + toString() + "" value["" + value + ""]"");
+            }
+        } else {
+            responseHandler.sendResponse(this, value);
+        }
     }",Buggy,"NPE on operation sendResponse if no response handler is set

The problem is fixed by adding a check if the response handler is set.

If it isn't set a warning is printed so we don't loose the exception. It is better
that a response handler is always set to prevent loosing any track of problematic flows.
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) that occurs in the `sendResponse` method when no response handler is set. The provided code diff aligns with the commit message as it introduces a null check for the `responseHandler` variable before attempting to use it.

1. **Null Check Before Method Call**: The updated code introduces a check for `responseHandler == null`. If the response handler is not set, instead of proceeding with the method call `responseHandler.sendResponse(this, value)`, the code logs a warning. This prevents the NPE that would occur when trying to invoke a method on a null reference.

2. **Logging**: When the response handler is missing, the code logs a warning message. If the value is an instance of `Throwable`, the stack trace is also included in the log, which aids in debugging by preserving information about the error.

3. **Alignment with Commit Message**: The code change directly addresses the issue described in the commit message by adding a check to prevent an NPE, ensuring that a message is logged when a response handler is not set, thereby eliminating the silent failure.

Given these detailed updates to address a specific bug (NPE) and the alignment with the commit message, the assessment of these changes concludes with **Buggy**."
hazelcast,21845.json,7ab1db9edc8409f599bf01054636a1643e3cf2d3,"@@ -1,3 +1,11 @@
     public double getRatio() {
-        return ((double) hits / misses) * 100.0;
+        if (misses == 0) {
+            if (hits == 0) {
+                return Double.NaN;
+            } else {
+                return Double.POSITIVE_INFINITY;
+            }
+        } else {
+            return ((double) hits / misses) * PERCENTAGE;
+        }
     }",Buggy,"Fixed checkstyle issue and divide by zero problem for ratio in ""NearCacheStatsImpl""",Buggy,"This code change addresses a divide-by-zero problem in the getRatio method, which calculates a ratio of hits to misses. By adding checks for when misses equals zero, it prevents division by zero, returning Double.NaN if both hits and misses are zero, and Double.POSITIVE_INFINITY if only misses is zero. This solves a potential runtime exception (ArithmeticException) when misses is zero, aligning perfectly with the commit message, which states a fix for the divide-by-zero problem in the ""NearCacheStatsImpl"" class. There is no specific indication of what the ""checkstyle issue"" refers to in the diff, so that part of the commit message isn't directly related to the shown changes. Nonetheless, the showcased change deals with a logical error fix.

The analysis concludes with **Buggy** since it involves a logical correction to prevent a divide-by-zero problem."
hazelcast,22872.json,0fd3a71c944b64d8b3a3ad75361015fb289be227,"@@ -1,4 +1,4 @@
     protected DiagnosticsLogWriter write(String s) {
-        printWriter.write(s);
+        printWriter.write(s == null ? ""null"" : s);
         return this;
     }",Buggy,"Fixed NPE in Diagnostics

Fix #9085

The problem was that the printWriter was called with a print(null). So the fix is to do a simple null check and write
""null"" in case of null.
",Buggy,"The commit message precisely details a fix for a NullPointerException (NPE) within the `DiagnosticsLogWriter` class. The diff shows a change in the `write` method, where `printWriter.write(s);` is replaced with `printWriter.write(s == null ? ""null"" : s);`. 

This update performs a null check on the input `s`. If `s` is null, it writes the string ""null"" instead of attempting to write a null object, which would lead to an NPE. This change directly addresses the described issue, preventing a bug caused by passing a null value to the `printWriter`.

Given the nature of the change and the explicit mention of fixing a NullPointerException within the commit message, this commit is a clear indication of a bug fix.

Therefore, the conclusion is **Buggy**."
hazelcast,10446.json,cd1d3e928d20e4ec4cc22d0cc91d93c2b6600c9e,"@@ -1,3 +1,3 @@
     public Object getKey() {
-        return key;
+        return new MapKey(name, key);
     }",Buggy,"map txn bug fix: same key overrides txn op with same key but different map name
",Buggy,"The commit message suggests a bug fix in transaction handling for maps with the same key but different map names. The code change aligns with this message by modifying the `getKey` method to return a `MapKey` object that includes both `name` and `key`. This likely resolves an issue where transactions might incorrectly treat keys as the same when they belong to different maps. By incorporating `name`, keys are differentiated based on their map, addressing the described bug.

The changes show clear signs of logically correcting transaction behavior with respect to keys in different maps. Therefore, the code change is consistent with the commit message's intent to fix a bug.

The conclusion is **Buggy**."
hazelcast,6968.json,0ae63554413e19520e136b5a35fec3c05d801866,"@@ -1,4 +1,6 @@
     private void createCachesOnCluster() {
         ClientCacheProxyFactory proxyFactory = (ClientCacheProxyFactory) getClientProxyFactory(ICacheService.SERVICE_NAME);
-        proxyFactory.recreateCachesOnCluster();
+        if (proxyFactory != null) {
+            proxyFactory.recreateCachesOnCluster();
+        }
     }",Buggy,"Fix NPE when jcache not in classpath

fixing a bug recently introduced in #13810

fixes #13851
",Buggy,"The commit message clearly states the intention to fix a NullPointerException (NPE) when jcache is not in the classpath. The code change aligns with this by adding a null check for the `proxyFactory` object before calling the `recreateCachesOnCluster()` method. Without this check, invoking the method on a null `proxyFactory` would result in an NPE. The change prevents this and therefore addresses the reported bug. The connection between the commit message and the code change indicates a bug fix. The answer is **Buggy**."
hazelcast,29434.json,f701a369208b2acf3066ba1b306d68688a765a56,"@@ -1,7 +1,9 @@
     public void populate(LiveOperations liveOperations) {
         for (Queue<ParkedOperation> parkQueue : parkQueueMap.values()) {
-            for (ParkedOperation op : parkQueue) {
-                liveOperations.add(op.getCallerAddress(), op.getCallId());
+            for (ParkedOperation parkedOperation : parkQueue) {
+                // we need to read out the data from the BlockedOperation; not from the ParkerOperation-container.
+                Operation operation = parkedOperation.getOperation();
+                liveOperations.add(operation.getCallerAddress(), operation.getCallId());
             }
         }
     }",Buggy,"Fix heartbeat problem for BlockingOperations

The problem is that the ParkedOperation; the container around the BlockingOperation; is
asked for callid/calleraddress. But this container object doesn't have any sensible
information, so the heartbeat is not constructed correctly for a blocked Operation.
",Buggy,"The commit message indicates a bug fix related to a heartbeat problem involving `BlockingOperations` and `ParkedOperation`. The changes in the diff confirm this by switching the source of the call ID and caller address from `ParkedOperation` to its contained `Operation`. Given the issue was the lack of correct information in `ParkedOperation`, this change directly addresses that by obtaining information from `BlockingOperation`, ensuring that the heartbeat is correctly constructed.

The commit message is consistent with the code changes, and these changes reflect a meaningful correction related to error handling or behavior improvement:

- The original code adds a caller address and call ID from the `ParkedOperation`, which supposedly lacks meaningful information.
- The new code retrieves these details from the `Operation` inside the `ParkedOperation`, presumably providing the correct data.

Thus, the changes effectively resolve an incorrect behavior (a bug) as described. Therefore, the correct classification is **Buggy**."
hazelcast,17124.json,fc8ec0a85ea7e388794baf5c7710ec824b6cb10c,"@@ -1,20 +1,27 @@
     public void close() throws IOException {
         if (!CLOSED.compareAndSet(this, FALSE, TRUE)) {
             return;
         }
 
+        // we execute this in its own try/catch block because we don't want to skip closing the socketChannel in case
+        // of problems.
+        try {
+            onClose();
+        } catch (Exception e) {
+            getLogger().severe(format(""Failed to call 'onClose' on channel [%s]"", this), e);
+        }
+
         try {
             socketChannel.close();
         } finally {
             for (ChannelCloseListener closeListener : closeListeners) {
                 // it is important we catch exceptions so that other listeners aren't obstructed when
                 // one of the listeners is throwing an exception.
                 try {
                     closeListener.onClose(this);
                 } catch (Exception e) {
-                    ILogger logger = Logger.getLogger(getClass());
-                    logger.severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
+                    getLogger().severe(format(""Failed to process closeListener [%s] on channel [%s]"", closeListener, this), e);
                 }
             }
         }
     }",Buggy,"Fix AbstractChannel.close forwarding to reader/writer

A bug was introduced by me in the 3.9 io changes where the forwarding of
the close to the underlying channel reader/writer is skipped.

This pr restores these closes by adding a template method to the AbstractChannel
which is implemented by the NioChannel.
",Buggy,"The commit message indicates a bug fix related to the `close` method in `AbstractChannel`, emphasizing restoring forwarding the `close` operation to the channel's reader/writer. The changes in the code indeed reflect this intent. Here's why:

1. **Adding Error Handling for `onClose`:**
   - The added `try-catch` block around `onClose()` ensures that if `onClose` throws an exception, it won't prevent the subsequent closing of `socketChannel`. This behavior aligns with the commit message's intention to restore the close functionality and ensure all necessary operations are completed during the close process.

2. **Error Logging Improvement:**
   - The update replaces how exceptions are logged by using `getLogger()` directly instead of creating a new `ILogger` instance, which is a minor but sensible improvement. While not directly mentioned in the commit message, it helps maintain cleaner and more consistent logging.

Overall, these code changes demonstrate a careful approach to ensure that all close-related operations are executed while safeguarding against errors interrupting the process. This aligns well with the commit message's focus on fixing the forwarding of the close operation, indicating a bug fix.

Therefore, the conclusion is **Buggy**."
hazelcast,12126.json,8bbe3fbfaa5e85b1e504100aa7d4c425970dee3d,"@@ -1,3 +1,5 @@
     public boolean eval(Object arg) {
-        return (key == null || key.equals(arg)) && predicate.apply((Map.Entry)arg);
+        final QueryEntry entry = (QueryEntry) arg;
+        final Data keyData = entry.getKeyData();
+        return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry)arg);
     }",Buggy,"MapAddListenerRequest serialization bug fix and related to that  QueryEventFilter bug fix
",Buggy,"This commit modifies the eval method to address how it processes its argument, specifically focusing on the key comparison. Let's break down the changes:

1. The original code checks if `key` is `null` or `key.equals(arg)`, and then applies a predicate to `arg` cast as a `Map.Entry`.

2. The updated code casts `arg` to a `QueryEntry`, which implies `arg` was expected to be or is better handled as a `QueryEntry`, aligning with the `QueryEventFilter` named in the commit message.

3. It retrieves the `keyData` from this `QueryEntry` and performs the equality check against `key`. This change likely fixes a potential bug where the original argument might not have the necessary structure (as assumed) to perform the correct comparison or could have produced incorrect outcomes if `arg` was not an instance that supports `key.equals(arg)` directly.

4. The change from using the generic `arg` to explicitly working with `QueryEntry` and its method `getKeyData()` suggests that the previous logic could improperly handle `arg` if it wasn't of the expected type or structure, thus resulting in a bug related to key evaluation or comparison during query filtering.

The commit message indicates this is a serialization-related bug fix for `MapAddListenerRequest` and a related `QueryEventFilter` bug fix. The change directly targets the logic where the argument is evaluated and used, supporting the claim in the commit message that a bug is being addressed.

Thus, the modifications align well with the commit message indicating a bug fix for both serialization and query filtering processes. The careful adjustment of how `arg` is handled and used suggests the elimination of incorrect behavior or outcomes, including potential serialization issues related to incorrect argument handling. This supports the conclusion that it is indeed addressing a bug.

The analysis concludes the changes are addressing a bug fix as described in the commit message, thus the result is **Buggy**."
hazelcast,12000.json,777bd5c28ac36f1b948f94a68b21328e3743e877,"@@ -1,11 +1,10 @@
     public static int getMaxSizePerNode(MaxSizeConfig maxSizeConfig) {
-
-        int maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
+        double maxSizePerNode = getApproximateMaxSize(maxSizeConfig, MaxSizePolicy.PER_NODE);
 
         if (maxSizePerNode == MaxSizeConfig.DEFAULT_MAX_SIZE) {
             // unlimited
             return -1;
         }
 
-        return maxSizePerNode;
+        return (int) maxSizePerNode;
     }",Buggy,"Fixed getApproximateMaxSize calculation bug
",Buggy,"The commit message indicates a bug fixed in the `getApproximateMaxSize` calculation. The diff changes the return type of `maxSizePerNode` from `int` to `double` when retrieved from `getApproximateMaxSize`, and then casts it back to `int` just before returning it.

Analyzing this change:

1. **Data Type Adjustment**: Converting `maxSizePerNode` to `double` and casting it back to `int` before returning it suggests that the original calculation required support for a fractional value, which an `int` cannot accommodate. By using a `double`, you capture more precision during intermediate calculations, perhaps expecting that the final conversion to `int` is enough as the final step.

2. **Bug Fix Indication**: This suggests that the original `int` based approach was insufficient somewhere in the calculation where intermediate precision could affect the outcome (like prematurely flooring results before final processing).

3. **Conversion Rationale**: Returning as `int` indicates that the ultimate consumer of this value is expected to handle it as an integer, hence verifying that the fractional precision was necessary mid-calculation but wasn't expected at the consumer level.

4. **Commit Message Alignment**: The message indicates a calculation bug was fixed, aligning with the diff snippet that enhances precision temporarily to potentially correct a logic flaw in size approximation.

Overall, the change aligns well with the commit message indicating an error fix by improving calculation accuracy. Thus, the answer is **Buggy**."
jetty,11444.json,93b6877d724cbec52cc427e168144d539ea8e38b,"@@ -1,9 +1,9 @@
     protected void doStart() throws Exception
     {
         Objects.requireNonNull(httpClient, ""Provided HttpClient is null"");
 
+        super.doStart();
+
         if (!httpClient.isRunning())
             throw new IllegalStateException(""HttpClient is not running (did you forget to start it?): "" + httpClient);
-
-        super.doStart();
     }",Buggy,"Issue #2210 - fixing flaw in safety checks with JSR356 ClientContainer
",Buggy,"The diff indicates a change in the order of operations in the `doStart` method. Originally, `super.doStart()` was called after checking if `httpClient` is running, but now it is called before this check. This alteration is likely to address an issue where initializing the superclass has to precede certain operations to ensure the system is in a valid state before further checks or actions are made.

This reordering could solve issues where the original sequence allowed for certain operations (like using a not fully initialized object) that could cause the application to misbehave or fail.

The commit message mentions fixing a flaw in safety checks, which aligns with this type of change. The new order ensures proper initialization and validates system readiness, which indicates that it is indeed addressing a bug related to system state safety checks.

Therefore, the solution aligns with the described purpose of the commit message. This change can be categorized as a bug fix, so the conclusion is **Buggy**."
jetty,3568.json,6c81941142b9efe2b5b80198268ae75687dc6374,"@@ -1,61 +1,66 @@
     public void validate(Certificate[] certChain) throws CertificateException
     {
         try
         {
             ArrayList<X509Certificate> certList = new ArrayList<X509Certificate>();
             for (Certificate item : certChain)
             {
                 if (item == null)
                     continue;
                 
                 if (!(item instanceof X509Certificate))
                 {
                     throw new IllegalStateException(""Invalid certificate type in chain"");
                 }
                 
                 certList.add((X509Certificate)item);
             }
     
             if (certList.isEmpty())
             {
                 throw new IllegalStateException(""Invalid certificate chain"");
                 
             }
     
             X509CertSelector certSelect = new X509CertSelector();
             certSelect.setCertificate(certList.get(0));
             
             // Configure certification path builder parameters
             PKIXBuilderParameters pbParams = new PKIXBuilderParameters(_trustStore, certSelect);
             pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(certList)));
     
             // Set maximum certification path length
             pbParams.setMaxPathLength(_maxCertPathLength);
     
             // Enable revocation checking
             pbParams.setRevocationEnabled(true);
     
             // Set static Certificate Revocation List
             if (_crls != null && !_crls.isEmpty())
             {
                 pbParams.addCertStore(CertStore.getInstance(""Collection"", new CollectionCertStoreParameters(_crls)));
             }
     
             // Enable On-Line Certificate Status Protocol (OCSP) support
-            Security.setProperty(""ocsp.enable"",""true"");
-    
+            if (_enableOCSP)
+            {
+                Security.setProperty(""ocsp.enable"",""true"");
+            }
             // Enable Certificate Revocation List Distribution Points (CRLDP) support
-            System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            if (_enableCRLDP)
+            {
+                System.setProperty(""com.sun.security.enableCRLDP"",""true"");
+            }
     
             // Build certification path
             CertPathBuilderResult buildResult = CertPathBuilder.getInstance(""PKIX"").build(pbParams);               
             
             // Validate certification path
             CertPathValidator.getInstance(""PKIX"").validate(buildResult.getCertPath(),pbParams);
         }
         catch (GeneralSecurityException gse)
         {
             LOG.debug(gse);
             throw new CertificateException(""Unable to validate certificate: "" + gse.getMessage(), gse);
         }
     }",Buggy,"[Bug 373567] cert validation issue with ocsp and crldp always being enabled when validating turned on fixed
",Buggy,"The commit message indicates a bug fix related to certificate validation, specifically the unintended default enabling of OCSP and CRLDP. The code changes reflect this by introducing conditional logic to enable OCSP and CRLDP only when specified by the `_enableOCSP` and `_enableCRLDP` flags. Before the change, these features were always enabled when validation was initiated, regardless of intended configuration, which aligns with the reported issue.

Key indicators:
1. **Presence of conditional checks**: The addition of checks (`if (_enableOCSP)` and `if (_enableCRLDP)`) ensures that OCSP and CRLDP are enabled based on specific conditions, resolving the issue where they were always being enabled unintentionally.
2. **Contextual relevance**: The changes directly address revocation checking features (OCSP and CRLDP) mentioned in the commit message, clearly aligning with the bug described.
3. **Logical corrections**: By modifying the logic around setting system properties related to OCSP and CRLDP, the update aligns the program's behavior with the intended configuration.

Since the modified code resolves the bug described, the conclusion is **Buggy**."
jetty,12794.json,c6d86122dbd737f2247b01653da0af938075370d,"@@ -1,45 +1,46 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
+        String authScheme = getAuthSchemeFromHeader(header);
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
         // The client has responded to the challenge we sent previously
-        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
+        if (header != null && isAuthSchemeNegotiate(authScheme))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
         // A challenge should be sent if any of the following cases are true:
         //   1. There was no Authorization header provided
         //   2. There was an Authorization header for a type other than Negotiate
         try
         {
              if (DeferredAuthentication.isDeferred(res))
              {
                  return Authentication.UNAUTHENTICATED;
              }
 
             LOG.debug(""SpengoAuthenticator: sending challenge"");
             res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
             res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
             return Authentication.SEND_CONTINUE;
         }
         catch (IOException ioe)
         {
             throw new ServerAuthException(ioe);
         }
     }",Buggy,"Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAut… (#1710)

* Fix a bug around handling ""Negotiate"" case-insensitively in SpnegoAuthenticator

Closes #1709

Signed-off-by: Josh Elser <elserj@apache.org>

* Clean up isAuthSchemeNegotiate(String) since we don't need to use startsWith()

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The commit message clearly states that it addresses a bug related to handling the ""Negotiate"" authentication scheme in a case-insensitive manner. The code changes reflect this by introducing a utility method `isAuthSchemeNegotiate(authScheme)` instead of directly using `startsWith()` on the header string in a case-sensitive manner. This modification implies that the method `isAuthSchemeNegotiate` performs case-insensitive checks, which aligns with the commit message stating the need for case-insensitivity in handling the ""Negotiate"" scheme.

The code change enhances the flexibility and correctness of the authentication process by ensuring that it correctly recognizes variations in the casing of ""Negotiate"" tokens in HTTP headers, which can be a common issue in interoperability scenarios when dealing with different clients or libraries that may deviate from expected casing. This is a clear indication of a logical correction and improvement in handling the authentication header.

Thus, the change precisely addresses a specific bug as described in the commit message. Therefore, the answer is **Buggy**."
jetty,12794.json,1fd3e4ad1b8903bc9033dd24c86a0207996097b1,"@@ -1,46 +1,45 @@
     public Authentication validateRequest(ServletRequest request, ServletResponse response, boolean mandatory) throws ServerAuthException
     {
         HttpServletRequest req = (HttpServletRequest)request;
         HttpServletResponse res = (HttpServletResponse)response;
 
         String header = req.getHeader(HttpHeader.AUTHORIZATION.asString());
 
         if (!mandatory)
         {
             return new DeferredAuthentication(this);
         }
 
-        // check to see if we have authorization headers required to continue
-        if ( header == null )
-        {
-            try
-            {
-                 if (DeferredAuthentication.isDeferred(res))
-                 {
-                     return Authentication.UNAUTHENTICATED;
-                 }
-
-                LOG.debug(""SpengoAuthenticator: sending challenge"");
-                res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
-                res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
-                return Authentication.SEND_CONTINUE;
-            }
-            catch (IOException ioe)
-            {
-                throw new ServerAuthException(ioe);
-            }
-        }
-        else if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString()))
+        // The client has responded to the challenge we sent previously
+        if (header != null && header.startsWith(HttpHeader.NEGOTIATE.asString().toLowerCase()))
         {
             String spnegoToken = header.substring(10);
 
             UserIdentity user = login(null,spnegoToken, request);
 
             if ( user != null )
             {
                 return new UserAuthentication(getAuthMethod(),user);
             }
         }
 
-        return Authentication.UNAUTHENTICATED;
+        // A challenge should be sent if any of the following cases are true:
+        //   1. There was no Authorization header provided
+        //   2. There was an Authorization header for a type other than Negotiate
+        try
+        {
+             if (DeferredAuthentication.isDeferred(res))
+             {
+                 return Authentication.UNAUTHENTICATED;
+             }
+
+            LOG.debug(""SpengoAuthenticator: sending challenge"");
+            res.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), HttpHeader.NEGOTIATE.asString());
+            res.sendError(HttpServletResponse.SC_UNAUTHORIZED);
+            return Authentication.SEND_CONTINUE;
+        }
+        catch (IOException ioe)
+        {
+            throw new ServerAuthException(ioe);
+        }
     }",Buggy,"Sends the WWW-Authenticate header if a non-Negotiate authorization he… (#1700)

* Sends the WWW-Authenticate header if a non-Negotiate authorization header was given

Fixes #1698

Signed-off-by: Josh Elser <elserj@apache.org>

* Dumb compilation error

Signed-off-by: Josh Elser <elserj@apache.org>

* Adds a test to show the challenge is sent.

Signed-off-by: Josh Elser <elserj@apache.org>

* Refactor the conditionals per Greg's suggestion

Signed-off-by: Josh Elser <elserj@apache.org>

* Add the expected license header

Signed-off-by: Josh Elser <elserj@apache.org>
",Buggy,"The provided commit message indicates that the change deals with sending the WWW-Authenticate header when a non-Negotiate authorization header is detected, which is intended to fix an issue in the system (Fixes #1698).

The key changes in the code include:
1. Adjusting the logic to handle cases where the `Authorization` header is absent or differs from the `Negotiate` scheme.
2. Removing the initial guard clause that checks for a `null` header separately.
3. Consolidating logic to handle sending the `WWW-Authenticate` challenge to the client, even when a non-Negotiate authorization header is present or absent, thereby ensuring the correct challenge-response behavior.

This indicates the developer is addressing robustness of authentication handling, ensuring the correct HTTP response headers are used based on the specified authorization scheme. By moving and consolidating the code that sends the challenge, it handles both cases of missing and alternative authorization headers effectively.

The provided changes align with the commit message, reflecting corrective action for handling authentication headers correctly based on client requests, which aligns with a bug fix in the authentication flow.

Therefore, the changes in the diff are consistent with the commit message, indicating a **Buggy** nature as it addresses a correctness issue in the code."
jetty,6485.json,c170801600f37d831007e6f496e7fc7d86e71dd1,"@@ -1,27 +1,27 @@
     protected void doStop() throws Exception
     {
         super.doStop();
 
         try
         {
             for (int i=_configurations.length;i-->0;)
                 _configurations[i].deconfigure(this);
             
             _configurations=null;
             
             // restore security handler
-            if (_securityHandler.getHandler()==null)
+            if (_securityHandler != null && _securityHandler.getHandler()==null)
             {
                 _sessionHandler.setHandler(_securityHandler);
                 _securityHandler.setHandler(_servletHandler);
             }
         }
         finally
         {
             if (_ownClassLoader)
                 setClassLoader(null);
 
             setAvailable(true);
             _unavailableException=null;
         }
     }",Buggy,"Bug 277027 Fix NPE if no security handler installed

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@239 7e9141cc-0065-0410-87d8-b60c137991c4
",Buggy,"The commit message indicates a fix for a NullPointerException (NPE) related to a missing security handler. The diff adds a null check for `_securityHandler` before accessing its method `getHandler()`. This prevents the NPE if `_securityHandler` is null, aligning perfectly with the described bug fix. The change is clearly related to error-handling improvement by safeguarding against accessing a method on a potentially null object.

Therefore, the modifications in the code and the explanation in the commit message are consistent with a bug fix intended to prevent a specific runtime error, namely the NullPointerException.

The answer is **Buggy**."
jetty,1562.json,4236f14955e828946c33f447fed3e65dfa8bfc1e,"@@ -1,15 +1,15 @@
         public void run()
         {
             try
             {
                 if (handle)
                     handleWithContext();
-                else
+                else if (getHttpConfiguration().isNotifyRemoteAsyncErrors())
                     getState().asyncError(failure);
                 callback.succeeded();
             }
             catch (Throwable x)
             {
                 callback.failed(x);
             }
         }",Buggy,"Fixes #1891 - Make HTTP/2 async error notifications configurable.

Introduced HttpConfiguration.notifyRemoteAsyncErrors, true by default.
",Buggy,"The commit introduces a configurability option for handling HTTP/2 asynchronous errors based on the new `HttpConfiguration.notifyRemoteAsyncErrors` setting, with a default value of `true`. The updated code checks this configuration before calling `getState().asyncError(failure)`, allowing users to determine whether they want to be notified of remote asynchronous errors. This aligns with the commit message about making error notifications configurable.

In terms of identifying a bug or an enhancement:

1. **Error-Handling Update**: The update allows controlling when async errors should be notified, solving potential issues for users who may not want default error notifications. This addresses specific use cases related to bug #1891.

2. **Logical Correction**: It seems the original logic always attempted to notify about async errors unconditionally. The new conditional checks whether error notification settings are enabled.

3. **Exception Handling Improvement**: Improved configurability implies better handling of asynchronous errors, which aids stability and robustness. 

Given these assessments, the code changes effectively align with fixing a configuration bug related to error notification in HTTP/2.

Therefore, the changes should be categorized as **Buggy**."
jetty,6487.json,0b13e6cddd8b8056311176389024fdb187305749,"@@ -1,12 +1,12 @@
     private void dumpUrl()
     {
         Connector[] connectors = getServer().getConnectors();
         for (int i=0;i<connectors.length;i++)
         {
             String displayName = getDisplayName();
             if (displayName == null)
-                displayName = ""WebApp@""+connectors.hashCode();
+                displayName = ""WebApp@""+Arrays.hashCode(connectors);
 
             LOG.info(displayName + "" at http://"" + connectors[i].toString() + getContextPath());
         }
     }",Buggy,"fix error prone error

Signed-off-by: olivier lamy <olamy@webtide.com>
",Buggy,"The commit message states that it fixes an ""error prone error."" The change in the code replaces the use of `hashCode()` on the `connectors` array with `Arrays.hashCode(connectors)`. The `hashCode()` method on an array object returns the identity hash code of the array, rather than a hash of its contents, which can lead to inconsistent and potentially erroneous behavior when a hash of the array content is needed.

Here's the step-by-step reasoning:

1. **Original Code Analysis**: The original code uses `connectors.hashCode()`, which returns the identity hash code of the array object. This is generally not useful for understanding array contents analytically, which might lead to errors in debugging/logging given that it may not reflect changes in the array's actual contents.

2. **Modified Code Analysis**: By using `Arrays.hashCode(connectors)`, the code now computes a hash code based on the array's contents. This is intended behavior when the content-based identity is needed, such as logging or comparing arrays.

3. **Error Explanation**: The previous implementation was technically correct but not semantically intended most probably, which could mislead developers when debugging via logs, as the hash would seem unrelated to the array contents.

4. **Commit Relevance**: The change addressed the issue of content representation via hash by making the hash code reflect actual array contents, resolving the potential misunderstanding or incorrect logging.

Considering these points, the change aligns with securing correct and meaningful functionality (as interpreted due to the scope of ""error prone error"") by appropriately addressing the use case with an accurate hash method. It indicates a fix, primarily in how the function's diagnostic behavior is effective in reflecting actual logic processing.

Thus, the outcome of the analysis is **Buggy**."
jetty,5128.json,8ec4e56681231a2370715cde72d46262aba65294,"@@ -1,17 +1,17 @@
     private static MultiMap<String> parseQueryString(String url)
     {
         MultiMap<String> res = new MultiMap<String>();
         int questionMarkIndex = url.indexOf('?');
         if (questionMarkIndex == -1)
         {
             return res;
         }
         int poundIndex = url.indexOf('#');
         if (poundIndex == -1)
         {
             poundIndex = url.length();
         }
-        UrlEncoded.decodeUtf8To(url.getBytes(), questionMarkIndex+1,
+        UrlEncoded.decodeUtf8To(url, questionMarkIndex+1,
                     poundIndex - questionMarkIndex - 1, res);
         return res;
     }",Buggy,"Fixing compile error
",Buggy,"In the provided Git diff, there is a change in the `UrlEncoded.decodeUtf8To` method call. The parameter type has been altered:

- The original implementation uses `url.getBytes()`, indicating that it is passing a byte array of the URL substring to the method.
- The updated line changes this to pass the `url` string directly instead of its byte array.

This change from `url.getBytes()` to `url` likely addresses a compile error if the `decodeUtf8To` method expects a `String` parameter rather than a `byte[]`. If the method signature had changed or was mistakenly implemented to accept a `byte[]`, this would indeed be a compile-time type mismatch.

Hence, this change aligns with the commit message ""Fixing compile error,"" as correcting parameter types in a method call is a common source of compile errors. The reasoning aligns with a logical necessity to match expected types in method signatures to resolve such errors.

Therefore, the answer is **Buggy** because the change addresses a compile error, resolving a bug in the code compilation process."
jclouds,26844.json,c0e6a2c51cd131bcef2ee051cfe200b41afda528,"@@ -1,57 +1,57 @@
       public T read(JsonReader in) throws IOException {
          if (in.peek() == JsonToken.NULL) {
             in.nextNull();
             return null;
          }
 
          List<Parameter> params = parameterizedCtor.getParameters();
          Object[] values = new Object[params.size()];
          boolean empty = true;
 
          // Set all primitive constructor params to defaults
          for (Parameter param : params) {
             if (param.getType().getRawType() == boolean.class) {
                values[param.hashCode()] = Boolean.FALSE;
             } else if (param.getType().getRawType().isPrimitive()) {
                values[param.hashCode()] = 0;
             }
          }
 
          try {
             in.beginObject();
             while (in.hasNext()) {
                empty = false;
                String name = in.nextName();
                ParameterReader<?> parameter = parameterReaders.get(name);
-               if (parameter == null) {
+               if (parameter == null || in.peek() == JsonToken.NULL) {
                   in.skipValue();
                } else {
                   Object value = parameter.read(in);
                   if (value != null)
                      values[parameter.position] = value;
                }
             }
          } catch (IllegalStateException e) {
             throw new JsonSyntaxException(e);
          }
 
          for (Parameter param : params) {
             if (param.getType().getRawType().isPrimitive()) {
                checkArgument(values[param.hashCode()] != null,
                   ""Primitive param[%s] in constructor %s cannot be absent!"", param.hashCode(), parameterizedCtor);
             } else if (param.getType().getRawType() == Optional.class && values[param.hashCode()] == null) {
                values[param.hashCode()] = Optional.absent();
             }
          }
          in.endObject();
 
          try {
             return newInstance(values);
          } catch (NullPointerException ex) {
             // If {} was found and constructor threw NPE, we treat the field as null
             if (empty && values.length > 0) {
                return null;
             }
             throw ex;
          }
       }",Buggy,"Fixed bug found in docker provider where deserialization constructors don't handle json null.
",Buggy,"The commit message suggests a bug fix related to handling JSON null values during deserialization in a Docker provider. The changes in the code:

1. Add a check to skip JSON tokens that are `NULL` in addition to missing parameters. This change is essential because it prevents attempting to read NULL values into non-nullable fields, which could lead to exceptions or incorrect data handling.

2. Handling for cases where the JSON might have `NULL` values by using `in.peek() == JsonToken.NULL`. This update improves robustness and ensures that deserialization can handle JSON inputs where fields might explicitly be set to null.

These modifications are directly related to the bug described in the commit message. By ensuring null JSON tokens are adequately handled, the changes focus on correcting behavior that was previously flawed.

Thus, this change aligns with the commit message, addressing a specific bug regarding JSON null handling in deserialization constructors. Therefore, the conclusion is **Buggy**."
jclouds,27517.json,b59457a405f1741cebfa14775b397ae225dd714b,"@@ -1,4 +1,4 @@
    public <C extends Context> C unwrap(TypeToken<C> type) {
-      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable from %s"", backendType, type);
+      checkArgument(checkNotNull(type, ""type"").isAssignableFrom(backendType), ""backend type: %s not assignable to %s"", backendType, type);
       return (C) backend;
    }",Buggy,"Fixing an error message about an unassignable backend
",NotBuggy,"The commit fixes a typographical error in the error message. Specifically, it changes ""not assignable from"" to the correct ""not assignable to,"" which logically matches the condition being checked in the `checkArgument` call. This change increases clarity and presents the error message accurately, but doesn't fix a bug in execution or logical processes. This change focuses merely on the error message's content.

Despite this being an improvement in terms of clarity and correctness of the message, it doesn't change the behavior of the code or correct a defect in the algorithm or application logic.

Hence, while the alteration aligns well with the commit message since it addresses and corrects the error message format specifically related to assignment logic, it is not indicative of a bug fix in the functionality of the application itself.

Therefore, the correct assessment here would be **NotBuggy**."
jclouds,25412.json,d41101df5932043c3a8614552f25ab91194595ba,"@@ -1,19 +1,19 @@
    private ImmutableMap<String, String> getContentMetadataForManifest(ContentMetadata contentMetadata) {
       Builder<String, String> mapBuilder = ImmutableMap.builder();
       if (contentMetadata.getContentType() != null) {
          mapBuilder.put(""content-type"", contentMetadata.getContentType());
       }
       /**
-       * Do not set content-length. Set automatically to manifest json string length by BindManifestToJsonPayload
+       * Do not set content-length. Set automatically to manifest json string length by BindToJsonPayload
        */
       if (contentMetadata.getContentDisposition() != null) {
          mapBuilder.put(""content-disposition"", contentMetadata.getContentDisposition());
       }
       if (contentMetadata.getContentEncoding() != null) {
          mapBuilder.put(""content-encoding"", contentMetadata.getContentEncoding());
       }
       if (contentMetadata.getContentLanguage() != null) {
          mapBuilder.put(""content-language"", contentMetadata.getContentLanguage());
       }
       return mapBuilder.build();
    }",Buggy,"JCLOUDS-1264: Swift Unicode multipart manifests

This fixes a bug where previously BindManifestToJsonPayload used the
character length as the ContentLength, instead of the byte length,
which caused issues if the JSON contained multi-byte Unicode
characters.
",Buggy,"The commit message states that this change addresses a bug related to incorrect content length calculation due to character length being used instead of byte length for JSON manifests containing multi-byte Unicode characters. The diff itself makes a minor correction in a comment, updating the name from ""BindManifestToJsonPayload"" to ""BindToJsonPayload"", but does not appear to directly address the character vs. byte length issue discussed. This suggests that the actual logic fix addressing the issue is elsewhere, and this diff is part of that change, updating the associated documentation.

However, considering the provided commit message explains a specific bug fix related to content length calculation, and this change is part of the same commit, it can be reasonably inferred that this line might relate to documenting that no explicit content-length is needed as it's handled elsewhere correctly, in line with the fix.

Ultimately, the commit message suggests a bug fix, albeit this specific diff may not directly contain the core logic change. Therefore, the conclusion is **Buggy** for the overall commit, with this diff being a part of mostly related documentation cleanup."
jclouds,18000.json,9965fbcadb0913889f19d4f29e9f66f20e3f4f08,"@@ -1,24 +1,24 @@
    public Instance getWorkingInstance(String zone, String name, String flavorId, int size) {
       InstanceApi instanceApi = api.getInstanceApiForZone(zone);
       for (int retries = 0; retries < 10; retries++) {
          Instance instance = null;
          try {
             instance = instanceApi.create(flavorId, size, name);
          } catch (Exception e) {
 
             Uninterruptibles.sleepUninterruptibly(15, TimeUnit.SECONDS);
 
-            logger.error(e.getStackTrace().toString());
+            logger.error(Arrays.toString(e.getStackTrace()));
             continue;
          }
 
          Instance updatedInstance = awaitAvailable(instance, instanceApi);
          if (updatedInstance != null) {
             return updatedInstance;
          }
          instanceApi.delete(instance.getId());
          InstancePredicates.awaitDeleted(instanceApi).apply(instance);
          
       }
       return null;
    }",Buggy,"Address error-prone warning
",NotBuggy,"The commit message indicates an ""error-prone warning"" was addressed, and the diff contains a change in how the stack trace of an exception is logged. The original code calls `e.getStackTrace().toString()`, which does not provide a human-readable representation of the stack trace. The updated code uses `Arrays.toString(e.getStackTrace())`, which correctly formats the array.

This change improves logging accuracy but doesn't directly fix a bug. Instead, it addresses a warning related to best practices in error handling and logging. Since the commit focuses on improving code quality by eliminating potential errors or misunderstandings in log outputs, it aligns with enhancements rather than corrections of faulty logic or behavior.

Therefore, the correct conclusion for this analysis is **NotBuggy**."
jclouds,1113.json,347926543ee72a14f8587bc83f72e1064cef8784,"@@ -1,4 +1,8 @@
    @Override protected void configure() {
       super.configure();
       bindHttpApi(binder(), UseApiToResolveProjectName.GetProject.class);
+      bind(OAuthScopes.class).toInstance(ReadOrWriteScopes.create( //
+            ""https://www.googleapis.com/auth/compute.readonly"", //
+            ""https://www.googleapis.com/auth/compute"" //
+      ));
    }",Buggy," * Change OAuthScopes into an interface as opposed to boilerplating annotations.
 * Fixed errors because of boilerplating annotations.
",NotBuggy,"The commit message indicates a change in how OAuthScopes is implemented, shifting from boilerplate annotations to an interface. The diff shows code configuring bindings for OAuthScopes specifying OAuth URIs, removing redundant annotations. However, without more context, this likely enhances maintainability and clarity rather than fixing bugs. The answer is **NotBuggy**."
jclouds,25772.json,f851271ae3d56d1549e62cd3723ad6207ced97f1,"@@ -1,3 +1,3 @@
-      public static NodeAttributes condition(String condition) {
-         return new NodeAttributes().condition(condition);
+      public static NodeAttributes condition(Condition condition) {
+         return new NodeAttributes().condition(condition.name());
       }",Buggy,"minor bug fixes plus wiring up
",Buggy,"This commit changes the method parameter type from `String` to `Condition`, then calls `condition.name()` as the argument to `NodeAttributes().condition()`. This suggests a bug fix where using an enum `Condition` instead of a raw `String` reduces errors, provides better type safety, and ensures consistent `Condition` values.

The original code likely allowed any string as input, leading to potential invalid values or runtime errors. By using an enum, this issue is mitigated.

Considering these improvements align with the commit message of ""minor bug fixes,"" it's reasonable to conclude that this change resolves a small bug related to type safety or consistency.

Thus, the conclusion is **Buggy**."
jclouds,14356.json,6d657b1c90d590d38a6110f18f4683592932336d,"@@ -1,3 +1,3 @@
    public float getCpuUsed() {
-      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(9, cpuUsed.length() - 1)) : 0.0f;
+      return cpuUsed != null ? Float.parseFloat(cpuUsed.substring(0, cpuUsed.length() - 1)) : 0.0f;
    }",Buggy,"Fix bug in parsing VirtualMachine.cpuUsed
",Buggy,"The commit message indicates a bug fix in parsing VirtualMachine.cpuUsed. The modification in the code changes the `substring` method call, altering the start index from `9` to `0`. This implies the previous logic assumed the CPU usage string had a specific prefix of length 9 that should not be included in the float parsing. 

The change suggests the prefix assumption was incorrect, as the entire string, minus the last character, should be parsed. This rectifies potential parsing issues—misreading CPU values due to an incorrect substring operation, leading to misinterpretation of the CPU usage data.

Hence, the code change aligns well with the commit message indicating a bug fix in the parsing logic, providing evidence of a bug fix regarding correct data interpretation.

Therefore, the outcome is **Buggy**."
okhttp,1134.json,aed222454743ebe5724d6ad438fafed37956521e,"@@ -1,15 +1,25 @@
   @Override public void connectSocket(Socket socket, InetSocketAddress address,
       int connectTimeout) throws IOException {
     try {
       socket.connect(address, connectTimeout);
     } catch (AssertionError e) {
       if (Util.isAndroidGetsocknameError(e)) throw new IOException(e);
       throw e;
     } catch (SecurityException e) {
       // Before android 4.3, socket.connect could throw a SecurityException
       // if opening a socket resulted in an EACCES error.
       IOException ioException = new IOException(""Exception in connect"");
       ioException.initCause(e);
       throw ioException;
+    } catch (ClassCastException e) {
+      // On android 8.0, socket.connect throws a ClassCastException due to a bug
+      // see https://issuetracker.google.com/issues/63649622
+      if (Build.VERSION.SDK_INT == 26) {
+        IOException ioException = new IOException(""Exception in connect"");
+        ioException.initCause(e);
+        throw ioException;
+      } else {
+        throw e;
+      }
     }
   }",Buggy,"Add workaround for https://issuetracker.google.com/issues/63649622 (#3624)

* Add workaround for https://issuetracker.google.com/issues/63649622

This fixes #3438

* Rethrowing the Android O bug CCE as IOException
",Buggy,"The commit message and the code diff suggest that a workaround has been implemented for dealing with a specific bug related to Android 8.0 (Oreo), where a `ClassCastException` is thrown during a socket connection. The code has added exception handling for `ClassCastException` specifically for Android 8.0 (SDK 26), which wraps the exception in an `IOException`. This indicates handling of a known bug, as referenced in the Android issue tracker link and the fixed GitHub issue numbers provided in the commit message.

Key points:
1. **ClassCastException Handling:** The new catch block for `ClassCastException` addresses the bug identified in the provided issue tracker link.
2. **Condition on SDK Version:** The handling specifically checks if the SDK version is 26, which corresponds to Android 8.0, showing that the fix is targeted and not arbitrary.
3. **Rethrowing as IOException:** By wrapping the `ClassCastException` in an `IOException`, the code provides a consistent exception type for callers, simplifying error handling for developers using this API.

This change is clearly aligned with the commit message and indicates a fix for a bug. Therefore, the assessment is **Buggy**."
okhttp,55.json,8bce6897f9f7e0b16508c462665e4f9d26d92e39,"@@ -1,34 +1,34 @@
   private static Request transformRequest(HttpRequest request) {
     Request.Builder builder = new Request.Builder();
 
     RequestLine requestLine = request.getRequestLine();
     String method = requestLine.getMethod();
     builder.url(requestLine.getUri());
 
     String contentType = null;
     for (Header header : request.getAllHeaders()) {
       String name = header.getName();
-      if (""Content-Type"".equals(name)) {
+      if (""Content-Type"".equalsIgnoreCase(name)) {
         contentType = header.getValue();
       } else {
         builder.header(name, header.getValue());
       }
     }
 
     RequestBody body = null;
     if (request instanceof HttpEntityEnclosingRequest) {
       HttpEntity entity = ((HttpEntityEnclosingRequest) request).getEntity();
       if (entity != null) {
         // Wrap the entity in a custom Body which takes care of the content, length, and type.
         body = new HttpEntityBody(entity, contentType);
 
         Header encoding = entity.getContentEncoding();
         if (encoding != null) {
           builder.header(encoding.getName(), encoding.getValue());
         }
       }
     }
     builder.method(method, body);
 
     return builder.build();
   }",Buggy,"fixed a bug that content type is compared by case sensitive
",Buggy,"The commit message indicates a change to fix a bug related to case-sensitive comparison of content types. The code change reflects this by updating the comparison from `equals` to `equalsIgnoreCase` for the ""Content-Type"" header. This update ensures that content type headers will be correctly recognized regardless of the case used in the header name. Such adjustments are essential for avoiding potential errors that occur when headers are treated in a case-sensitive manner, which is contrary to how HTTP headers are generally intended to be handled. Therefore, this code modification aligns perfectly with the commit message and signifies a fix for a previously existing bug in the method's behavior.

Hence, the changes indicate a bug fix. The answer is **Buggy**."
okhttp,1015.json,8b17ab4e65b21c6480a38c383a16899f9ffabbcc,"@@ -1,39 +1,42 @@
   public void streamFailed(IOException e) {
     Socket socket;
     Connection releasedConnection;
     boolean noNewStreams = false;
 
     synchronized (connectionPool) {
       if (e instanceof StreamResetException) {
-        StreamResetException streamResetException = (StreamResetException) e;
-        if (streamResetException.errorCode == ErrorCode.REFUSED_STREAM) {
+        ErrorCode errorCode = ((StreamResetException) e).errorCode;
+        if (errorCode == ErrorCode.REFUSED_STREAM) {
+          // Retry REFUSED_STREAM errors once on the same connection.
           refusedStreamCount++;
-        }
-        // On HTTP/2 stream errors, retry REFUSED_STREAM errors once on the same connection. All
-        // other errors must be retried on a new connection.
-        if (streamResetException.errorCode != ErrorCode.REFUSED_STREAM || refusedStreamCount > 1) {
+          if (refusedStreamCount > 1) {
+            noNewStreams = true;
+            route = null;
+          }
+        } else if (errorCode != ErrorCode.CANCEL) {
+          // Keep the connection for CANCEL errors. Everything else wants a fresh connection.
           noNewStreams = true;
           route = null;
         }
       } else if (connection != null
           && (!connection.isMultiplexed() || e instanceof ConnectionShutdownException)) {
         noNewStreams = true;
 
         // If this route hasn't completed a call, avoid it for new connections.
         if (connection.successCount == 0) {
           if (route != null && e != null) {
             routeSelector.connectFailed(route, e);
           }
           route = null;
         }
       }
       releasedConnection = connection;
       socket = deallocate(noNewStreams, false, true);
       if (connection != null || !reportedAcquired) releasedConnection = null;
     }
 
     closeQuietly(socket);
     if (releasedConnection != null) {
       eventListener.connectionReleased(call, releasedConnection);
     }
   }",Buggy,"Fix stream cancel error stopping reused connection
",Buggy,"The commit message claims to fix a stream cancellation error affecting reused connections. In the diff, there's a notable change regarding the handling of `StreamResetException` for certain error codes. Specifically:

1. The handling of the `ErrorCode.REFUSED_STREAM` is refined by encapsulating the error code logic, which attempts to reuse the connection once upon encountering `ErrorCode.REFUSED_STREAM`. If the refusal count exceeds one, it marks that no new streams should be initiated on the current connection.

2. The significant addition is the handling of `ErrorCode.CANCEL`, where the code now distinguishes this scenario and avoids setting `noNewStreams` to `true` for `CANCEL` errors. This is crucial because cancel errors often arise due to timing rather than actual connection issues, implying that the connection might still be valid for reuse.

These changes reflect a correction in how the streaming logic handles specific HTTP/2 stream reset errors, ensuring that the connection pool logic does not prematurely discard connections that are encountered because of a `CANCEL` error. Thus, the changes align with the commit message, indicating a fix for a bug related to connection reuse in the presence of stream cancellation errors.

Therefore, the answer is **Buggy**."
okhttp,2062.json,a87147e65424df05b7d7829555eef1220dbf4633,"@@ -1,18 +1,18 @@
     public Response response(Request request, DiskLruCache.Snapshot snapshot) {
       String contentType = responseHeaders.get(""Content-Type"");
       String contentLength = responseHeaders.get(""Content-Length"");
       Request cacheRequest = new Request.Builder()
           .url(url)
-          .method(message, null)
+          .method(requestMethod, null)
           .headers(varyHeaders)
           .build();
       return new Response.Builder()
           .request(cacheRequest)
           .protocol(protocol)
           .code(code)
           .message(message)
           .headers(responseHeaders)
           .body(new CacheResponseBody(snapshot, contentType, contentLength))
           .handshake(handshake)
           .build();
     }",Buggy,"Fix a bug where the cacheResponse's request method was wrong.

We were returning the message (like 'OK') rather than the method
(like 'GET'). Ugh.
",Buggy,"The commit message accurately describes a fix for a bug related to using the incorrect method when creating a Request. The code changed from using `message` to `requestMethod`. The original code mistakenly used the response message (e.g., 'OK') instead of the method (e.g., 'GET'). This change corrects that issue. The fix aligns with the commit message, indicating a bug fix. The answer is **Buggy**."
openmrs-core,30.json,35fbc63bd8566c1b7800fcfdd1e34a6640ef0bd4,"@@ -1,30 +1,36 @@
 	protected File getFile(HttpServletRequest request) {
 		
 		String path = request.getPathInfo();
 		
 		Module module = ModuleUtil.getModuleForPath(path);
 		if (module == null) {
 			log.warn(""No module handles the path: "" + path);
 			return null;
 		}
 		
 		String relativePath = ModuleUtil.getPathForResource(module, path);
 		String realPath = getServletContext().getRealPath("""") + MODULE_PATH + module.getModuleIdAsPath() + ""/resources""
 		        + relativePath;
 		
 		//if in dev mode, load resources from the development directory
 		File devDir = ModuleUtil.getDevelopmentDirectory(module.getModuleId());
 		if (devDir != null) {
 			realPath = devDir.getAbsolutePath() + ""/omod/target/classes/web/module/resources"" + relativePath;
 		}
 		
 		realPath = realPath.replace(""/"", File.separator);
 		
 		File f = new File(realPath);
 		if (!f.exists()) {
+			if (isJstlFile(path)) {
+				f =  new File(realPath + "".withjstl"");
+				if (f.exists()) {
+					return f;
+				}
+			}
 			log.warn(""No file with path '"" + realPath + ""' exists for module '"" + module.getModuleId() + ""'"");
 			return null;
 		}
 		
 		return f;
 	}",Buggy,Fixing jslt file load error,Buggy,"The commit message indicates fixing a `.jslt` file load error. The code change introduces logic specifically for handling `.jslt` files. If a file does not exist at the given `realPath`, it checks if the request was for a `.jslt` file using the new `isJstlFile(path)` method. If true, it attempts to find a file with the "".withjstl"" suffix.

This change clearly aligns with the commit message by adding specific logic to address `.jslt` file loading errors. The introduction of the file existence check for `.jslt` suggests that this problem was likely leading to incorrect or failed file loading, corroborating the idea of a bug fix.

Therefore, the commit appears to address a specific error, aligning the modifications with a reported issue in `.jslt` file handling.

The conclusion is **Buggy**."
openmrs-core,6620.json,6c5b1205526a53e2327f802adf31e1f4e340d93f,"@@ -1,20 +1,20 @@
 	public PatientIdentifier savePatientIdentifier(PatientIdentifier patientIdentifier) throws APIException {
 		//if the argument or the following required fields are not specified
 		PatientIdentifierType.LocationBehavior locationBehavior = null;
 		if (patientIdentifier != null) {
-			patientIdentifier.getIdentifierType().getLocationBehavior();
+			locationBehavior = patientIdentifier.getIdentifierType().getLocationBehavior();
 		}
 		
 		if (patientIdentifier == null
 		        || patientIdentifier.getPatient() == null
 		        || patientIdentifier.getIdentifierType() == null
 		        || StringUtils.isBlank(patientIdentifier.getIdentifier())
 		        || (locationBehavior == PatientIdentifierType.LocationBehavior.REQUIRED && patientIdentifier.getLocation() == null))
 			throw new APIException(""PatientIdentifier argument or one of its required fields is null or invalid"");
 		if (patientIdentifier.getPatientIdentifierId() == null) {
 			Context.requirePrivilege(PrivilegeConstants.ADD_PATIENT_IDENTIFIERS);
 		} else
 			Context.requirePrivilege(PrivilegeConstants.EDIT_PATIENT_IDENTIFIERS);
 		
 		return dao.savePatientIdentifier(patientIdentifier);
 	}",Buggy,"Fixing forgotten assignment bug for:
PatientService.savePatientIdentifier still requires location -
TRUNK-4056",Buggy,"The commit message states that there was a forgotten assignment bug. The code diff shows that the `locationBehavior` variable was supposed to be assigned the result of `patientIdentifier.getIdentifierType().getLocationBehavior()`, but this assignment was inadvertently omitted in the original code. Without this assignment, the subsequent logic checking `locationBehavior` could lead to a null pointer exception or incorrect validation behavior, especially if `locationBehavior` is required to be checked for certain conditions regarding location requirement.

This matches the described bug in the commit message, specifically related to fixing forgotten assignments and ensuring that `locationBehavior` is properly initialized before being used in conditional logic. Therefore, the changes are a direct bug fix as described in the commit message.

The assessment indicates **Buggy**."
openmrs-core,4909.json,837c70069a4bfca920ed92a7e73da0dbead6aff0,"@@ -1,6 +1,3 @@
 	public void purgeConceptClass(ConceptClass cc) throws DAOException  {
-		sessionFactory.getCurrentSession().createQuery(""delete from ConceptClass where concept_class_id = :c"")
-					.setInteger(""c"", cc.getConceptClassId())
-					.executeUpdate();			
 		sessionFactory.getCurrentSession().delete(cc);
 	}",Buggy,"Fixing conceptClass deletion error and fieldAnswer errors found by cmack

git-svn-id: http://svn.openmrs.org/openmrs/trunk@4553 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The code change in the diff removes an explicit query that deleted a ConceptClass by its ID and instead uses the `delete(cc)` method on the session. 

Here's the analysis:

1. **Direct Deletion by ID**: The original code deletes a `ConceptClass` record by executing a query with a condition on `concept_class_id`.

2. **Improved Deletion Method**: The updated code removes this direct query and switches to using the Hibernate `delete(cc)` method, which should also appropriately handle associated constraints and dependent deletions as configured in ORM mappings.

3. **Bug Fix Justification**: This change likely addresses issues related to integrity constraints, orphaned records, or other cascading deletes that were not handled by the manual query. Using `delete(cc)` ensures that Hibernate leverages its internal mechanisms to manage the deletion with respect to the object relationships defined in the ORM.

From this assessment, the modification aligns with the commit message by rectifying an error in the deletion process. Therefore, we conclude with **Buggy**, indicating a bug fix with this change."
openmrs-core,7646.json,f6e02ac804e140765a89e0f983c794be1b3ffa14,"@@ -1,127 +1,127 @@
 	public void execute(Database database) throws CustomChangeException {
 		JdbcConnection connection = (JdbcConnection) database.getConnection();
 		Map<String, HashSet<Integer>> duplicates = new HashMap<String, HashSet<Integer>>();
 		Statement stmt = null;
 		PreparedStatement pStmt = null;
 		ResultSet rs = null;
 		boolean autoCommit = true;
 		try {
 			// set auto commit mode to false for UPDATE action
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			stmt = connection.createStatement();
-			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type""
+			rs = stmt.executeQuery(""SELECT * FROM location_attribute_type ""
 			        + ""INNER JOIN (SELECT name FROM location_attribute_type GROUP BY name HAVING count(name) > 1) ""
 			        + ""dup ON location_attribute_type.name = dup.name"");
 			Integer id = null;
 			String name = null;
 			
 			while (rs.next()) {
 				id = rs.getInt(""location_attribute_type_id"");
 				name = rs.getString(""name"");
 				if (duplicates.get(name) == null) {
 					HashSet<Integer> results = new HashSet<Integer>();
 					results.add(id);
 					duplicates.put(name, results);
 				} else {
 					HashSet<Integer> results = duplicates.get(name);
 					results.add(id);
 				}
 			}
 			
 			Iterator it2 = duplicates.entrySet().iterator();
 			while (it2.hasNext()) {
 				Map.Entry pairs = (Map.Entry) it2.next();
 				HashSet values = (HashSet) pairs.getValue();
 				List<Integer> duplicateNames = new ArrayList<Integer>(values);
 				int duplicateNameId = 1;
 				for (int i = 1; i < duplicateNames.size(); i++) {
 					String newName = pairs.getKey() + ""_"" + duplicateNameId;
 					List<List<Object>> duplicateResult = null;
 					boolean duplicateName = false;
 					Connection con = DatabaseUpdater.getConnection();
 					do {
 						String sqlValidatorString = ""select * from location_attribute_type where name = '"" + newName + ""'"";
 						duplicateResult = DatabaseUtil.executeSQL(con, sqlValidatorString, true);
 						if (!duplicateResult.isEmpty()) {
 							duplicateNameId += 1;
 							newName = pairs.getKey() + ""_"" + duplicateNameId;
 							duplicateName = true;
 						} else {
 							duplicateName = false;
 						}
 					} while (duplicateName);
 					pStmt = connection
 					        .prepareStatement(""update location_attribute_type set name = ?, changed_by = ?, date_changed = ? where location_attribute_type_id = ?"");
 					if (!duplicateResult.isEmpty()) {
 						pStmt.setString(1, newName);
 					}
 					pStmt.setString(1, newName);
 					pStmt.setInt(2, DatabaseUpdater.getAuthenticatedUserId());
 					
 					Calendar cal = Calendar.getInstance();
 					Date date = new Date(cal.getTimeInMillis());
 					
 					pStmt.setDate(3, date);
 					pStmt.setInt(4, duplicateNames.get(i));
 					duplicateNameId += 1;
 					
 					pStmt.executeUpdate();
 				}
 			}
 		}
 		catch (BatchUpdateException e) {
 			log.warn(""Error generated while processsing batch insert"", e);
 			try {
 				log.debug(""Rolling back batch"", e);
 				connection.rollback();
 			}
 			catch (Exception rbe) {
 				log.warn(""Error generated while rolling back batch insert"", e);
 			}
 			// marks the changeset as a failed one
 			throw new CustomChangeException(""Failed to update one or more duplicate LocationAttributeType names"", e);
 		}
 		catch (Exception e) {
 			throw new CustomChangeException(""Error while updating duplicate LocationAttributeType object names"", e);
 		}
 		finally {
 			// reset to auto commit mode
 			try {
 				connection.commit();
 				connection.setAutoCommit(autoCommit);
 			}
 			catch (DatabaseException e) {
 				log.warn(""Failed to reset auto commit back to true"", e);
 			}
 			
 			if (rs != null) {
 				try {
 					rs.close();
 				}
 				catch (SQLException e) {
 					log.warn(""Failed to close the resultset object"");
 				}
 			}
 			
 			if (stmt != null) {
 				try {
 					stmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the select statement used to identify duplicate LocationAttributeType object names"");
 				}
 			}
 			
 			if (pStmt != null) {
 				try {
 					pStmt.close();
 				}
 				catch (SQLException e) {
 					log
 					        .warn(""Failed to close the prepared statement used to update duplicate LocationAttributeType object names"");
 				}
 			}
 		}
 	}",Buggy,Fixing liquibase upgrade error for: TRUNK-4334,Buggy,"The commit message indicates a fix for a ""liquibase upgrade error,"" and the provided code diff reflects changes likely intended to address this issue. Let's break down the modifications:

1. **Database Query Adjustment:**
   - **Whitespace Change**: The SQL query originally used `""SELECT * FROM location_attribute_type""`. A space was added after the table name. This is a trivial change and typically does not have functional implications, but it might indicate attention to query accuracy or readability.

2. **Logic Handling Duplicates:**
   - The code is identifying and renaming duplicate entries in the `location_attribute_type` table. This involves:
     - Fetching duplicates using a query and grouping them based on the `name`.
     - Iterating over duplicates and generating a new unique name by appending an incrementing number.
     - Ensuring the uniqueness of the new name in the database before updating.
     - Updating and applying changes with rollback logic in case of exceptions.

3. **Error Handling:**
   - The code has mechanisms to rollback changes in case of errors using `BatchUpdateException` and general exceptions, ensuring data integrity.
   - It logs troubleshooting information, making recovery easier if the update fails.

4. **Commit and Auto-commit Management:**
   - There's careful management of transaction settings, like disabling auto-commit to handle transactional integrity and re-enabling it afterward.

5. **Object Closure:**
   - The code closes SQL objects (`ResultSet`, `Statement`, `PreparedStatement`) in the `finally` block to prevent resource leaks.

The changes align with the commit message, as they address potential liquibase upgrade issues related to duplicate names in the `location_attribute_type` table by ensuring names are unique. The improved error handling and transaction management indicate correcting a potentially buggy behavior with SQL operations, which aligns with bug-fixing activity.

Thus, considering the context of the changes and the objective mentioned in the commit message, I conclude that this commit reflects a bug fix, so the analysis concludes with **Buggy**."
openmrs-core,6353.json,95b3ab3de2d8ec488d0ca0fcbcb1e8969d491313,"@@ -1,21 +1,28 @@
 	private Class datatypeClassHandled(Type t) {
 		if (t instanceof ParameterizedType) {
 			ParameterizedType pt = (ParameterizedType) t;
 			Type first = pt.getActualTypeArguments()[0];
 			if (first instanceof Class && CustomDatatype.class.isAssignableFrom((Class) first)) {
 				return (Class) first;
 			} else {
 				return datatypeClassHandled(pt.getRawType());
 			}
 			
 		} else if (t instanceof Class) {
+			Type genericSuperclass = ((Class) t).getGenericSuperclass();
+			if (genericSuperclass != null) {
+				Class ret = datatypeClassHandled(genericSuperclass);
+				if (ret != null) {
+					return ret;
+				}
+			}
 			for (Type candidate : ((Class) t).getGenericInterfaces()) {
 				Class ret = datatypeClassHandled(candidate);
 				if (ret != null) {
 					return ret;
 				}
 			}
 		}
 		
 		return null;
 	}",Buggy,"Fixed bug where a custom datatype handler that extends a generic abstract class can't be found  -TRUNK-3499
",Buggy,"The commit message describes a bug involving identifying a custom datatype handler that extends a generic abstract class. The code modification adds logic to check the generic superclass of a class type (`(Class) t`). The new code attempts to recursively determine if an implemented datatype is present at higher levels of inheritance through `getGenericSuperclass()`. If a datatype is found in the superclass, it returns it immediately, resolving the issue when searching through interfaces alone was insufficient.

These changes are directly related to the bug described since they enhance the function's capability to locate the correct datatype by accounting for inheritance structures that were previously ignored.

Given these improvements addressing the described problem, the answer is **Buggy**."
openmrs-core,7456.json,4fff45254e5332a1b608a88d6daa83e7a330daf7,"@@ -1,9 +1,9 @@
 	public static byte[] getSavedSecretKey() {
-		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_VECTOR_RUNTIME_PROPERTY,
-		    OpenmrsConstants.ENCRYPTION_VECTOR_DEFAULT);
+		String keyText = Context.getRuntimeProperties().getProperty(OpenmrsConstants.ENCRYPTION_KEY_RUNTIME_PROPERTY,
+		    OpenmrsConstants.ENCRYPTION_KEY_DEFAULT);
 		
 		if (StringUtils.hasText(keyText))
 			return Base64.decode(keyText);
 		
 		throw new APIException(""no encryption secret key found"");
 	}",Buggy,"TRUNK-1919 - fixed glaring flaw where I pulled in the encryption.vector runtime property instead of encryption.key

git-svn-id: http://svn.openmrs.org/openmrs/trunk@18684 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix for an error involving the incorrect runtime property being retrieved. The code change corrects this by updating the property key from `ENCRYPTION_VECTOR_RUNTIME_PROPERTY` to `ENCRYPTION_KEY_RUNTIME_PROPERTY`, aligning with the message's description. This fix is clearly addressing a bug where the wrong property was being accessed, potentially causing incorrect encryption key handling. Thus, the answer is **Buggy**."
openmrs-core,6446.json,119e3fa6338f5401f0a75873e5a6526b8b23cd80,"@@ -1,10 +1,10 @@
 	private void checkPrivileges(Role role) {
-		Collection<Privilege> privileges = role.getPrivileges();
-		
 		Optional.ofNullable(role.getPrivileges())
 		.map(p -> p.stream().filter(pr -> !Context.hasPrivilege(pr.getPrivilege())).map(Privilege::getPrivilege)
 			.distinct().collect(Collectors.joining("", "")))
 		.ifPresent(missing -> {
-			throw new APIException(""Role.you.must.have.privileges: "", new Object[] { missing });
+			if (StringUtils.isNotBlank(missing)) {
+				throw new APIException(""Role.you.must.have.privileges"", new Object[] { missing });
+			}
 		});
     }",Buggy,"Fix for TRUNK-5726

* Only throws exception if actually missing permissions are found
* Correct error string
* Adds unit tests
",Buggy,"The provided diff corresponds closely with the commit message, which states that the change involves throwing an exception only when genuinely missing permissions are identified. Here's how the diff accomplishes this:

1. **Exception Handling Logic**: 
   - The original code throws an exception unconditionally if any privileges were processed. 
   - The new code wraps the exception-throwing logic within a condition (`if (StringUtils.isNotBlank(missing))`). This ensures an exception is only thrown when there are actual missing privileges, aligning with the message's indication of only throwing when necessary.

2. **Error String Correction**:
   - The correction of the error string involves removing the colon in the message key, which the commit message implies with ""Correct error string"". The actual change from `""Role.you.must.have.privileges: ""` to `""Role.you.must.have.privileges""` aligns with correcting string formatting issues.

3. **Code Clarity**:
   - While the addition of unit tests is mentioned in the commit message, their related diff is not shown here. However, the presence of a check condition improves logical clarity and reliability.

Overall, the modifications demonstrated in the diff correctly align with the commit message objectives and target improvements in error-handling logic. Hence, classifying these changes as fixing a bug is accurate.

The conclusion is **Buggy**."
openmrs-core,2762.json,29eae1d038bc6036af4903c9b43f42a157b10a1a,"@@ -1,49 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
 		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
 			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
-					specificErrorMsg = ""with provider identifier:"" + id;
+					specificErrorMsg = ""with provider identifier"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 				specificErrorMsg = ""associated to a person with person id"";
 			}
 			
 			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
 		} else {
 			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
 		if (provider == null) {
 			throw new HL7Exception(errorMessage);
 		}
 		
 		return provider;
 	}",Buggy,"Follow up to ,Fixed the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26248 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates that error messages were improved for scenarios where the provider can't be resolved. The code changes align with this message by changing the way specific error messages are constructed. The modification involves removing the inclusion of the identifier value `id` from the `specificErrorMsg` string, while keeping the id in the general `errorMessage`. This suggests an improvement in how detailed information is conveyed when an exception is thrown, likely focusing on clarity when an error occurs.

However, it's important to note that the enhancements to error messages do not necessarily indicate a typical bug fix, but rather an improvement in error reporting or logging. In many cases, improvements to error messages are push for better diagnostics during troubleshooting and debugging rather than resolving a functional issue in the code logic.

Given the context provided by the commit message and the code changes, the focus here is more on refining the diagnostic output rather than fixing a direct bug in the code's functionality. Therefore, based on the above reasoning, we conclude with **NotBuggy**."
openmrs-core,2762.json,dc4c547246dcf46aa2ff0dcb5e43caee7cd5f1d7,"@@ -1,38 +1,49 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = hl7Provider.getAssigningAuthority().getUniversalID().getValue();
 		String type = hl7Provider.getAssigningAuthority().getUniversalIDType().getValue();
+		String errorMessage = """";
 		if (StringUtils.hasText(id)) {
+			String specificErrorMsg = """";
 			if (OpenmrsUtil.nullSafeEquals(""L"", type)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
+					specificErrorMsg = ""with provider Id"";
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
+					specificErrorMsg = ""with provider identifier:"" + id;
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
+					specificErrorMsg = ""with provider uuid"";
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
+				specificErrorMsg = ""associated to a person with person id"";
 			}
+			
+			errorMessage = ""Could not resolve provider "" + specificErrorMsg + "":"" + id;
+		} else {
+			errorMessage = ""No unique identifier was found for the provider"";
 		}
 		
-		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider"");
+		if (provider == null) {
+			throw new HL7Exception(errorMessage);
+		}
 		
 		return provider;
 	}",Buggy,"Improved the error messages to be more helpful when the provider can't be resolved - TRUNK-3108

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26246 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates an improvement in error messages when a provider cannot be resolved. The code change reflects this improvement by providing more context-specific error messages in the `getProvider` method. Instead of a generic message, it now specifies whether the failure was due to a missing provider ID, identifier, UUID, or association with a person.

The update does not suggest a fix for a coding bug, but rather an enhancement in error handling. The original functionality related to provider resolution remains the same; only error feedback to the user has changed. The commit improves the user experience by offering more informative error messages, but it does not resolve a logic error, exception handling issue, or another direct bug in the code.

In conclusion, while this change does improve the clarity and helpfulness of error messages, it does not indicate fixing a defect in the logic or operation of the code, but rather enhances its usability.

**NotBuggy**"
openmrs-core,2762.json,7932358da8194ef18be1ce23e0b72b0c4f51a63a,"@@ -1,39 +1,39 @@
 	private Provider getProvider(PV1 pv1) throws HL7Exception {
 		XCN hl7Provider = pv1.getAttendingDoctor(0);
 		Provider provider = null;
 		String id = hl7Provider.getIDNumber().getValue();
 		String assignAuth = ((HD) hl7Provider.getComponent(8)).getNamespaceID().getValue();
 		String nameTypeCode = ((ID) hl7Provider.getComponent(9)).getValue();
 		
 		if (StringUtils.hasText(id)) {
 			if (OpenmrsUtil.nullSafeEquals(""L"", nameTypeCode)) {
 				if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_ID.equalsIgnoreCase(assignAuth)) {
 					try {
 						provider = Context.getProviderService().getProvider(Integer.valueOf(id));
 					}
 					catch (NumberFormatException e) {
 						// ignore
 					}
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_IDENTIFIER.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByIdentifier(id);
 				} else if (HL7Constants.PROVIDER_ASSIGNING_AUTH_PROV_UUID.equalsIgnoreCase(assignAuth)) {
 					provider = Context.getProviderService().getProviderByUuid(id);
 				}
 			} else {
 				try {
 					Person person = Context.getPersonService().getPerson(Integer.valueOf(id));
 					Collection<Provider> providers = Context.getProviderService().getProvidersByPerson(person);
 					if (!providers.isEmpty())
 						provider = providers.iterator().next();
 				}
 				catch (NumberFormatException e) {
 					// ignore
 				}
 			}
 		}
 		
 		if (provider == null)
-			throw new HL7Exception(""Could not resolve provider with personId or identifier as '"" + id + ""'"");
+			throw new HL7Exception(""Could not resolve provider"");
 		
 		return provider;
 	}",Buggy,"Rephrased the error message when the provider can't be resolved when processing an HL7 message

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26240 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates a rephrasing of the error message related to unresolved providers when processing an HL7 message. The diff shows a change from a specific error message:

```java
""Could not resolve provider with personId or identifier as '"" + id + ""'""
```

to a more general one:

```java
""Could not resolve provider""
```

This update doesn't alter the logic or error-handling mechanism of the function but does change how the error situation is communicated. The specificity of the error is reduced, likely to generalize the error message or possibly to avoid leaking potentially sensitive information such as internal IDs.

Given that no logic correction or algorithmic update is made in the code that might fix an existing defect aside from message rephrasing, this change does not reflect a bug fix.

Therefore, the conclusion is **NotBuggy**."
openmrs-core,6970.json,6f7de13f5e4d0b9e4d8ea553bfd92842cd0c161a,"@@ -1,19 +1,24 @@
 	public FormResource saveFormResource(FormResource formResource) throws APIException {
-		if (formResource == null) {
+	    	if (formResource == null) {
 			return null;
 		}
-		
 		// If a form resource with same name exists, replace it with current value
 		FormResource toPersist = formResource;
 		FormResource original = Context.getFormService().getFormResource(formResource.getForm(), formResource.getName());
 		if (original != null) {
 			original.setName(formResource.getName());
 			original.setValue(formResource.getValue());
 			original.setDatatypeClassname(formResource.getDatatypeClassname());
 			original.setDatatypeConfig(formResource.getDatatypeConfig());
 			original.setPreferredHandlerClassname(formResource.getPreferredHandlerClassname());
 			toPersist = original;
 		}
-		CustomDatatypeUtil.saveIfDirty(toPersist);
+		try {
+		    CustomDatatypeUtil.saveIfDirty(toPersist);
+		}
+		catch (ConstraintViolationException ex) {
+		    throw new InvalidFileTypeException(ex.getMessage(), ex);
+		}
+		
 		return dao.saveFormResource(toPersist);
 	}",Buggy,"TRUNK-4473: Fixed error occuring when user tries to upload binary files with form resource
",Buggy,"The changes in the diff align with the commit message indicating a bug fix related to error handling when uploading binary files as form resources. Here's a step-by-step analysis:

1. **Initial Null Check**: The code already checks if `formResource` is null, returning null if true. This is standard practice and not directly related to the bug fix.

2. **Form Resource Replacement Logic**: The code performs a check to see if a form resource with the same name already exists. If it does, it updates the existing resource's properties with the new values.

3. **Try-Catch Block Introduction**: The notable change is the addition of the try-catch block around `CustomDatatypeUtil.saveIfDirty(toPersist);`. The catch block specifically deals with `ConstraintViolationException`.

4. **Custom Exception**: When a `ConstraintViolationException` is caught, the code now throws a new `InvalidFileTypeException`, passing the original exception message and the exception itself as arguments.

5. **Aligning with the Commit Message**: The change suggests that previously, uploading certain binary files would throw a `ConstraintViolationException`, likely due to constraint checks failing (e.g., incorrect file type). By catching this specific exception and rethrowing it as an `InvalidFileTypeException`, the code handles this scenario more gracefully and informatively, providing more context about the error to the user.

The commit message mentions fixing an error related to uploading binary files, and the code change directly addresses this by improving error handling specific to file type validation. The added exception handling helps the method to communicate what went wrong during the upload process, aligning with the commit message's intent to fix an error.

Thus, this diff indeed represents a bug fix related to the described problem. Therefore, the conclusion is **Buggy**."
openmrs-core,105.json,f2d3fdd76e95f7783e530e4c3e7b4f0b7d48a852,"@@ -1,9 +1,10 @@
 	public static String escapeQuotesAndNewlines(String s) {
 		if (s == null)
 			return """";
 		
 		s = s.replace(""\"""", ""\\\"""");
+		s = s.replace(""\r\n"", ""\\r\\n"");
 		s = s.replace(""\n"", ""\\n"");
 		
 		return s;
 	}",Buggy,"Fixed javascript error on form schema designer when field descriptions had multiple lines - #594

git-svn-id: http://svn.openmrs.org/openmrs/trunk@11807 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates a fix for a JavaScript error related to handling multi-line field descriptions in a form schema designer. The change in the diff modifies the escapeQuotesAndNewlines method to handle Windows-style newlines (\r\n) by replacing them with \r\n. This adjustment is crucial for ensuring that strings are properly escaped and do not break JavaScript code when rendered, thus addressing the problem of multi-line descriptions causing errors.

Steps to determine this:

1. **Commit Message Analysis**: The commit message clearly states the intent to fix a JavaScript error related to multi-line field descriptions, providing a context of a bug fix.

2. **Code Changes**:
   - The existing method already handles single newline characters (\n) by replacing them with \\n, which is necessary for proper string handling in JavaScript.
   - By adding a line to replace Windows-style newlines (\r\n) with \\r\\n, the method now comprehensively escapes newlines across different operating systems. This would prevent JS errors that occur when raw newline characters disrupt JavaScript string handling.

3. **Bug Fix Indicators**:
   - The problem addressed involves correcting behavior where unescaped newline characters cause errors, a typical bug in input handling or string manipulation for web environments.
   - The fix aligns with improving code robustness across platforms, which directly addresses the issue mentioned in the commit message.

Given the alignment between the problem described in the commit message and the code changes made to handle input correctly, this change indeed represents a bug fix. Thus, the result is **Buggy**."
hadoop,10762.json,d3949058b84c393413ffea11de5c81ab8ad2ae3c,"@@ -1,40 +1,40 @@
   void doRollback(StorageDirectory bpSd, NamespaceInfo nsInfo)
       throws IOException {
     File prevDir = bpSd.getPreviousDir();
     // regular startup if previous dir does not exist
     if (!prevDir.exists())
       return;
     // read attributes out of the VERSION file of previous directory
-    DataStorage prevInfo = new DataStorage();
+    BlockPoolSliceStorage prevInfo = new BlockPoolSliceStorage();
     prevInfo.readPreviousVersionProperties(bpSd);
 
     // We allow rollback to a state, which is either consistent with
     // the namespace state or can be further upgraded to it.
     // In another word, we can only roll back when ( storedLV >= software LV)
     // && ( DN.previousCTime <= NN.ctime)
     if (!(prevInfo.getLayoutVersion() >= HdfsConstants.LAYOUT_VERSION && 
         prevInfo.getCTime() <= nsInfo.getCTime())) { // cannot rollback
       throw new InconsistentFSStateException(bpSd.getRoot(),
           ""Cannot rollback to a newer state.\nDatanode previous state: LV = ""
               + prevInfo.getLayoutVersion() + "" CTime = "" + prevInfo.getCTime()
               + "" is newer than the namespace state: LV = ""
               + nsInfo.getLayoutVersion() + "" CTime = "" + nsInfo.getCTime());
     }
     
     LOG.info(""Rolling back storage directory "" + bpSd.getRoot()
         + "".\n   target LV = "" + nsInfo.getLayoutVersion()
         + ""; target CTime = "" + nsInfo.getCTime());
     File tmpDir = bpSd.getRemovedTmp();
     assert !tmpDir.exists() : ""removed.tmp directory must not exist."";
     // 1. rename current to tmp
     File curDir = bpSd.getCurrentDir();
     assert curDir.exists() : ""Current directory must exist."";
     rename(curDir, tmpDir);
     
     // 2. rename previous to current
     rename(prevDir, curDir);
     
     // 3. delete removed.tmp dir
     deleteDir(tmpDir);
     LOG.info(""Rollback of "" + bpSd.getRoot() + "" is complete"");
   }",Buggy,"HDFS-3970. Fix bug causing rollback of HDFS upgrade to result in bad VERSION file. Contributed by Vinay and Andrew Wang.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430037 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message specifies a bug fix related to the rollback of HDFS upgrades resulting in an incorrect VERSION file. The diff shows a key change made to address this issue:

- The class used to read previous version properties has been changed from `DataStorage` to `BlockPoolSliceStorage`.

Given the context, it's likely that `BlockPoolSliceStorage` is the appropriate class to handle the specific storage details of block pool directories during rollback, which `DataStorage` might not have correctly addressed. This misallocation could lead to incorrect data in the VERSION file during a rollback operation.

The bug description aligns well with the code change, particularly focusing on ensuring that the correct object is used in the rollback process to prevent mismanagement of version information.

Hence, this code change directly addresses the bug mentioned in the commit message, demonstrating a correction in the logic that should result in the proper management of the VERSION file during rollbacks.

Thus, the answer to this evaluation based on the analysis is **Buggy**."
hadoop,78812.json,64f537da0a216055d5b6eae49a2a9129189cacbf,"@@ -1,18 +1,26 @@
-    public FileStatus[] globStatus(Path pathPattern, PathFilter filter)
+    public FileStatus[] globStatus(final Path pathPattern, final PathFilter filter)
       throws IOException {
+      
       String filename = pathPattern.toUri().getPath();
+      
       List<String> filePatterns = GlobExpander.expand(filename);
       if (filePatterns.size() == 1) {
-        return globStatusInternal(pathPattern, filter);
+        Path p = fixRelativePart(pathPattern);
+        FileSystem fs = getFSofPath(p);
+        URI uri = fs.getUri();
+        return globStatusInternal(uri, p, filter);
       } else {
         List<FileStatus> results = new ArrayList<FileStatus>();
         for (String filePattern : filePatterns) {
-          FileStatus[] files = 
-                      globStatusInternal(new Path(filePattern), filter);
+          Path p = new Path(filePattern);
+          p = fixRelativePart(p);
+          FileSystem fs = getFSofPath(p);
+          URI uri = fs.getUri();
+          FileStatus[] files = globStatusInternal(uri, p, filter);
           for (FileStatus file : files) {
             results.add(file);
           }
         }
         return results.toArray(new FileStatus[results.size()]);
       }
     }",Buggy,"HADOOP-6286. Fix bugs in related to URI handling in glob methods in FileContext. Contributed by Boris Shkolnik.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@822805 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that it addresses bugs related to URI handling in glob methods of FileContext. The code changes reflect adjustments in how file system URIs are managed during pattern expansion and status retrieval.

Here's a breakdown of the changes:

1. **Parameter Changes**: The `globStatus` method receives `Path pathPattern` and `PathFilter filter` as final parameters. This indicates an intention to fix issues with parameter modification or reliance safety.

2. **Path Correction**: The `fixRelativePart` method call ensures the path is correctly absolute or normalized before further processing. This correct handling can prevent errors in file system operations when paths are not appropriately set.

3. **File System and URI Adjustments**: The code retrieves the file system's URI using `fs.getUri()` for the path. By passing the URI to `globStatusInternal`, it ensures that the method operates with full URI awareness, addressing any previous oversight in URI handling when working with file patterns.

4. **Expansion and File System Handling**: For multiple file patterns (after expansion), each pattern undergoes URI correction, making operations consistent and potentially resolving issues where patterns were treated without considering their respective file systems.

Considering these points, the changes aim to resolve issues in URI and path handling, consistent with the commit message.

Thus, the changes demonstrate corrections in URI handling, reflecting a bug fix. The conclusion is **Buggy**."
hadoop,37278.json,3a154f75ed85d864b3ffd35818992418f2b6aa59,"@@ -1,21 +1,25 @@
   public RLESparseResourceAllocation getRangeOverlapping(long start, long end) {
     readLock.lock();
     try {
       NavigableMap<Long, Resource> a = this.getCumulative();
 
       if (a != null && !a.isEmpty()) {
         // include the portion of previous entry that overlaps start
         if (start > a.firstKey()) {
           long previous = a.floorKey(start);
           a = a.tailMap(previous, true);
         }
-        a = a.headMap(end, true);
+
+        if (end < a.lastKey()) {
+          a = a.headMap(end, true);
+        }
+
       }
       RLESparseResourceAllocation ret =
           new RLESparseResourceAllocation(a, resourceCalculator);
       return ret;
     } finally {
       readLock.unlock();
     }
 
   }",Buggy,"YARN-4525. Fix bug in RLESparseResourceAllocation.getRangeOverlapping(). (Ishai Menache and Carlo Curino via asuresh)
",Buggy,"The commit message clearly states that the change is meant to fix a bug in the method `getRangeOverlapping()` of `RLESparseResourceAllocation`. Let's analyze the code changes:

1. **Locking**: No change here; the method starts with acquiring a read lock and releases it in the `finally` block.

2. **Initial Setup**: The method starts by obtaining a `NavigableMap<Long, Resource>` which represents cumulative resource allocations.

3. **Checking Map for Entries**: 
   - Before the change, the code checked if the map `a` is not null and not empty, then attempted to adjust the range of interest by looking for keys overlapping the specified `start` and `end`.

4. **Logic Prior to Change**:
   - If the start is greater than the first key, the map is adjusted to start from the closest key below or equal to `start`.
   - The map is then truncated to only include keys up to `end` with `a.headMap(end, true)`.

5. **Code Changes**:
   - The previous logic forcibly truncated the map up to and including `end` without any condition.
   - The updated logic first checks if `end` is less than the last key. Only in this case does it modify the map to include only keys up to `end`.

6. **Implication of Change**: The change prevents unnecessary truncation of the map beyond the last available key. It ensures correct boundary handling and prevents potential out-of-bound errors where the requested range end exceeds available data.

Considering the nature of the changes, the decision to conditionally truncate the map to `end` addresses a logical flaw in the initial implementation. It corrects the handling of the edge case where `end` could be outside the bounds of existing keys, potentially causing incorrect behavior or resource allocation representations.

Given that this change resolves what is described as a ""bug"" in the method, the diff aligns with the commit message's assertion of a bug fix. Therefore, the conclusion is **Buggy**."
hadoop,3337.json,46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3,"@@ -1,18 +1,18 @@
   private static LocatedBlock toLocatedBlock(final Map<?, ?> m) throws IOException {
     if (m == null) {
       return null;
     }
 
     final ExtendedBlock b = toExtendedBlock((Map<?, ?>)m.get(""block""));
     final DatanodeInfo[] locations = toDatanodeInfoArray(
         (Object[])m.get(""locations""));
     final long startOffset = (Long)m.get(""startOffset"");
     final boolean isCorrupt = (Boolean)m.get(""isCorrupt"");
     final DatanodeInfo[] cachedLocations = toDatanodeInfoArray(
         (Object[])m.get(""cachedLocations""));
 
     final LocatedBlock locatedblock = new LocatedBlock(b, locations,
-        startOffset, isCorrupt, cachedLocations);
+        null, null, startOffset, isCorrupt, cachedLocations);
     locatedblock.setBlockToken(toBlockToken((Map<?, ?>)m.get(""blockToken"")));
     return locatedblock;
   }",Buggy,"HDFS-5508. Fix compilation error after merge. (Contributed by szetszwo)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1541352 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message mentions fixing a compilation error after a merge, which suggests the changes should address a coding issue preventing successful compilation. Analyzing the diff:

1. The constructor for `LocatedBlock` has changed the parameters it accepts, evident from the addition of two `null` values passed as arguments. 
2. This likely corresponds to changes in `LocatedBlock`'s constructor parameters that occurred after the merge, causing a mismatch.

- By adding `null, null`, the code aligns with the new constructor expectation, resolving the compilation issue.

The diff indicates that the code modification is addressing a compilation problem due to updated API requirements—exactly what the commit message specified. 

This change focuses on resolving a compile-time error, not a runtime bug, given the direct message in the commit and no indication of runtime issues such as exception handling or logic flaws.

Conclusion: **NotBuggy**."
hadoop,33132.json,c6ea28c480940d1f95cabe3af452dce197c6669d,"@@ -1,23 +1,24 @@
   public List<PrivilegedOperation> bootstrap(Configuration configuration)
       throws ResourceHandlerException {
     Set<Device> availableDevices = null;
     try {
       availableDevices = devicePlugin.getDevices();
     } catch (Exception e) {
       throw new ResourceHandlerException(""Exception thrown from""
           + "" plugin's \""getDevices\"""" + e.getMessage());
     }
     /**
      * We won't fail the NM if plugin returns invalid value here.
      * */
     if (availableDevices == null) {
       LOG.error(""Bootstrap "" + resourceName
           + "" failed. Null value got from plugin's getDevices method"");
       return null;
     }
     // Add device set. Here we trust the plugin's return value
     deviceMappingManager.addDeviceSet(resourceName, availableDevices);
-    // TODO: Init cgroups
-
+    // Init cgroups
+    this.cGroupsHandler.initializeCGroupController(
+        CGroupsHandler.CGroupController.DEVICES);
     return null;
   }",Buggy,"YARN-9331. [YARN-8851] Fix a bug that lacking cgroup initialization when bootstrap DeviceResourceHandlerImpl. Contributed by Zhankun Tang.
",Buggy,"The commit message describes a bug fix related to the initialization of cgroups in the `DeviceResourceHandlerImpl`. Examining the code diff:

1. The `bootstrap` method is responsible for initializing certain components in a resource handler.
2. The original comment `// TODO: Init cgroups` indicates an acknowledgment of missing cgroup initialization, which hints at incomplete functionality.
3. The updated code includes a call to `this.cGroupsHandler.initializeCGroupController(CGroupsHandler.CGroupController.DEVICES);` which initializes the cgroup controller for devices, fulfilling the missing functionality mentioned in the comment.

The change directly addresses a potential issue where cgroups may not have been initialized, which could lead to incorrect or incomplete resource handling. This aligns with the commit message stating that there was a ""bug that lacking cgroup initialization"".

Given these details, the changes in the code accurately reflect a bug fix as described in the commit message. Therefore, the conclusion is **Buggy**."
hadoop,22809.json,b524501d4f4b48edeb02901114087f3b5f57691f,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return reduceContext.getCacheArchives();
+      return reduceContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5685. Fixed a bug with JobContext getCacheFiles API inside the WrappedReducer class. Contributed by Yi Song.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554320 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the `getCacheFiles` API inside the `WrappedReducer` class. The code change modifies the method called from `getCacheArchives` to `getCacheFiles` within the `getCacheFiles` method, which seems more appropriate given the method's name. This likely corrects a logical error wherein the `getCacheFiles` method mistakenly returned archive URIs instead of file URIs, aligning with the expected functionality of the `JobContext` API. Therefore, the change indeed addresses a bug as described in the commit message.

The commit fixes the bug by ensuring the correct method (`getCacheFiles`) retrieves the cache files, consistent with the function's responsibility and the commit message intent.

The answer is **Buggy**."
hadoop,12133.json,68d5dfdc78d121e89eeae4e577d670028a14a955,"@@ -1,90 +1,90 @@
   void startDataNode(List<StorageLocation> dataDirectories,
                      SecureResources resources
                      ) throws IOException {
 
     // settings global for all BPs in the Data Node
     this.secureResources = resources;
     synchronized (this) {
       this.dataDirs = dataDirectories;
     }
     this.dnConf = new DNConf(this);
     checkSecureConfig(dnConf, getConf(), resources);
 
     if (dnConf.maxLockedMemory > 0) {
       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {
         throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) is greater than zero and native code is not available."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));
       }
       if (Path.WINDOWS) {
         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);
       } else {
         long ulimit = NativeIO.POSIX.getCacheManipulator().getMemlockLimit();
         if (dnConf.maxLockedMemory > ulimit) {
           throw new RuntimeException(String.format(
             ""Cannot start datanode because the configured max locked memory"" +
             "" size (%s) of %d bytes is more than the datanode's available"" +
             "" RLIMIT_MEMLOCK ulimit of %d bytes."",
             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,
             dnConf.maxLockedMemory,
             ulimit));
         }
       }
     }
     LOG.info(""Starting DataNode with maxLockedMemory = {}"",
         dnConf.maxLockedMemory);
 
     int volFailuresTolerated = dnConf.getVolFailuresTolerated();
     int volsConfigured = dnConf.getVolsConfigured();
     if (volFailuresTolerated < MAX_VOLUME_FAILURE_TOLERATED_LIMIT
         || volFailuresTolerated >= volsConfigured) {
       throw new DiskErrorException(""Invalid value configured for ""
           + ""dfs.datanode.failed.volumes.tolerated - "" + volFailuresTolerated
-          + "". Value configured is either greater than -1 or >= ""
+          + "". Value configured is either less than -1 or >= ""
           + ""to the number of configured volumes ("" + volsConfigured + "")."");
     }
 
     storage = new DataStorage();
     
     // global DN settings
     registerMXBean();
     initDataXceiver();
     startInfoServer();
     pauseMonitor = new JvmPauseMonitor();
     pauseMonitor.init(getConf());
     pauseMonitor.start();
   
     // BlockPoolTokenSecretManager is required to create ipc server.
     this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();
 
     // Login is done by now. Set the DN user name.
     dnUserName = UserGroupInformation.getCurrentUser().getUserName();
     LOG.info(""dnUserName = {}"", dnUserName);
     LOG.info(""supergroup = {}"", supergroup);
     initIpcServer();
 
     metrics = DataNodeMetrics.create(getConf(), getDisplayName());
     peerMetrics = dnConf.peerStatsEnabled ?
         DataNodePeerMetrics.create(getDisplayName()) : null;
     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);
 
     ecWorker = new ErasureCodingWorker(getConf(), this);
     blockRecoveryWorker = new BlockRecoveryWorker(this);
 
     blockPoolManager = new BlockPoolManager(this);
     blockPoolManager.refreshNamenodes(getConf());
 
     // Create the ReadaheadPool from the DataNode context so we can
     // exit without having to explicitly shutdown its thread pool.
     readaheadPool = ReadaheadPool.getInstance();
     saslClient = new SaslDataTransferClient(dnConf.getConf(),
         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);
     saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);
     startMetricsLogger();
 
     if (dnConf.diskStatsEnabled) {
       diskMetrics = new DataNodeDiskMetrics(this,
           dnConf.outliersReportIntervalMs);
     }
   }",Buggy,"HDFS-14056. Fix error messages in HDFS-12716. Contributed by Ayush Saxena.
",Buggy,"The commit message states that it aims to fix error messages in a specific ticket, HDFS-12716. Analyzing the changes made in the code, we can observe that the error message logic in the if condition with the variable `volFailuresTolerated` has been corrected.

1. **Error Message Logic Correction**: 
   - The original code stated: ""Value configured is either greater than -1 or >= to the number of configured volumes."" This has been changed to ""Value configured is either less than -1 or >= to the number of configured volumes.""
   - This change corrects the logic of the error message to properly reflect the condition checked in the if statement: `volFailuresTolerated < MAX_VOLUME_FAILURE_TOLERATED_LIMIT || volFailuresTolerated >= volsConfigured`.

The updated code now provides an error message that accurately describes the invalid configuration condition being checked, which enhances clarity and usability when users encounter this error. The discrepancy in the error message could lead to misunderstanding or difficulty in troubleshooting configuration issues.

Since the commit's changes align with the commit message stating a fix in error messages, and considering the purpose of fixing message clarity in error handling, the alterations suggest a resolution to a previously incorrect or unclear implementation. Therefore, the conclusion is **Buggy**."
hadoop,33173.json,a457a8951a1b35f06811c40443ca44bb9c698c30,"@@ -1,46 +1,48 @@
   public boolean initPlugin(Configuration conf) {
     this.aliasMap = new HashMap<>();
     if (this.initialized) {
       return true;
     }
     // Find the proper toolchain, mainly aocl
     String pluginDefaultBinaryName = getDefaultBinaryName();
     String pathToExecutable = conf.get(YarnConfiguration.NM_FPGA_PATH_TO_EXEC,
         """");
     if (pathToExecutable.isEmpty()) {
       pathToExecutable = pluginDefaultBinaryName;
     }
     // Validate file existence
     File binaryPath = new File(pathToExecutable);
     if (!binaryPath.exists()) {
       // When binary not exist, fail
       LOG.warn(""Failed to find FPGA discoverer executable configured in "" +
           YarnConfiguration.NM_FPGA_PATH_TO_EXEC +
           "", please check! Try default path"");
       pathToExecutable = pluginDefaultBinaryName;
       // Try to find in plugin's preferred path
       String pluginDefaultPreferredPath = getDefaultPathToExecutable();
       if (null == pluginDefaultPreferredPath) {
         LOG.warn(""Failed to find FPGA discoverer executable from system environment "" +
             getDefaultPathEnvName()+
             "", please check your environment!"");
       } else {
         binaryPath = new File(pluginDefaultPreferredPath + ""/bin"", pluginDefaultBinaryName);
         if (binaryPath.exists()) {
-          pathToExecutable = pluginDefaultPreferredPath;
+          pathToExecutable = binaryPath.getAbsolutePath();
+          LOG.info(""Succeed in finding FPGA discoverer executable: "" +
+              pathToExecutable);
         } else {
           pathToExecutable = pluginDefaultBinaryName;
           LOG.warn(""Failed to find FPGA discoverer executable in "" +
               pluginDefaultPreferredPath + "", file doesn't exists! Use default binary"" + pathToExecutable);
         }
       }
     }
     setPathToExecutable(pathToExecutable);
     if (!diagnose(10*1000)) {
       LOG.warn(""Intel FPGA for OpenCL diagnose failed!"");
       this.initialized = false;
     } else {
       this.initialized = true;
     }
     return this.initialized;
   }",Buggy,"YARN-8456. Fix a configuration handling bug when user leave FPGA discover executable path configuration default but set OpenCL SDK path environment variable. (Zhankun Tang via wangda)

Change-Id: Iff150ea98ba0c60d448474fd940eb121afce6965
",Buggy,"The commit message indicates a configuration handling bug fix related to the FPGA discover executable path. The code changes indeed address this issue: 

1. **Path Handling Logic**: Previously, if the specified path to the executable did not exist, the code attempted to use a default path without properly setting the `pathToExecutable` to an actual file path unless found in the plugin's preferred path. The update fixes this by setting `pathToExecutable` to the absolute path of the found executable file (`binaryPath.getAbsolutePath()`), ensuring a valid path is set.

2. **Logging Enhancement**: The code now logs success when it locates the executable, enhancing transparency and aiding future debugging.

These modifications effectively address potential misconfigurations where the executable path might not be directly set but is intended to default sensibly, which aligns with the bug fix description in the commit message. Therefore, the changes represent a bug fix related to proper path configuration handling.

The conclusion here is **Buggy**."
hadoop,16528.json,c3ca348b81bdf6aa0857a1d4d140c6c9d64be490,"@@ -1,29 +1,29 @@
   public void checkSuperuserPrivilege() throws  AccessControlException {
 
     // Try to get the ugi in the RPC call.
     UserGroupInformation ugi = null;
     try {
       ugi = NameNode.getRemoteUser();
     } catch (IOException e) {
       // Ignore as we catch it afterwards
     }
     if (ugi == null) {
       LOG.error(""Cannot get the remote user name"");
       throw new AccessControlException(""Cannot get the remote user name"");
     }
 
     // Is this by the Router user itself?
-    if (ugi.getUserName().equals(superUser)) {
+    if (ugi.getShortUserName().equals(superUser)) {
       return;
     }
 
     // Is the user a member of the super group?
     List<String> groups = Arrays.asList(ugi.getGroupNames());
     if (groups.contains(superGroup)) {
       return;
     }
 
     // Not a superuser
     throw new AccessControlException(
         ugi.getUserName() + "" is not a super user"");
   }",Buggy,"HDFS-14620. RBF: Fix 'not a super user' error when disabling a namespace in kerberos with superuser principal. Contributed by luhuachao.
",Buggy,"The commit message indicates a bug fix related to a ""not a super user"" error in a specific HDFS operation when using Kerberos with a superuser principal. 

Let's analyze the code changes to see if they are aligned with this message:

1. **Original Logic**: The original condition checks `ugi.getUserName().equals(superUser)`. This means it checks the full user name against the superuser name.

2. **Modified Logic**: The change made is to use `ugi.getShortUserName().equals(superUser)`. This logic now compares the short user name instead of the full user name to the superuser.

3. **Implication of Change**: The use of `getShortUserName()` instead of `getUserName()` suggests that the check is adjusted to accommodate cases where only the short username should be considered equivalent to the superuser. In Kerberos contexts, where principal names can include realm data which might not be relevant for superuser checks, this logic change is designed to prevent false negatives (the ""not a super user"" error mentioned in the commit message).

4. **Commit Message Relevance**: The commit message's reference to fixing the ""not a super user"" error specifically ties into this username-checking logic. The change does indeed align with addressing bugs related to super user privilege verification.

The code change seems to specifically address an identified discrepancy in recognizing superuser status, aligning with the commit message about fixing an error.

Therefore, based on both the context provided by the commit message and the changes in the code, this change is indeed a bug fix. The conclusion is **Buggy**."
hadoop,75002.json,c8abf5f20a7ca802e3e7c93c8c5d260902cb4052,"@@ -1,37 +1,44 @@
-  private int init(String[] args) throws IOException {
+  protected int init(String[] args) throws IOException {
+    // no args should print the help message
+    if (0 == args.length) {
+      printCredShellUsage();
+      ToolRunner.printGenericCommandUsage(System.err);
+      return 1;
+    }
+
     for (int i = 0; i < args.length; i++) { // parse command line
       if (args[i].equals(""create"")) {
         String alias = args[++i];
         command = new CreateCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""delete"")) {
         String alias = args[++i];
         command = new DeleteCommand(alias);
         if (alias.equals(""-help"")) {
           printCredShellUsage();
-          return -1;
+          return 0;
         }
       } else if (args[i].equals(""list"")) {
         command = new ListCommand();
       } else if (args[i].equals(""-provider"")) {
         userSuppliedProvider = true;
         getConf().set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, 
             args[++i]);
       } else if (args[i].equals(""-i"") || (args[i].equals(""-interactive""))) {
         interactive = true;
       } else if (args[i].equals(""-v"") || (args[i].equals(""-value""))) {
         value = args[++i];
       } else if (args[i].equals(""-help"")) {
         printCredShellUsage();
-        return -1;
+        return 0;
       } else {
         printCredShellUsage();
         ToolRunner.printGenericCommandUsage(System.err);
-        return -1;
+        return 1;
       }
     }
     return 0;
   }",Buggy,"HADOOP-10927. Fix CredentialShell help behavior and error codes. Contributed by Josh Elser.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615827 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that the change fixes the help behavior and error codes in CredentialShell. Let's analyze the changes in the code:

1. **Help Message with No Arguments**: 
   - The code now checks if no arguments are passed (`args.length == 0`). If true, it prints the help message and returns `1`.
   - Returning `1` likely indicates an error state or an informational exit code, aligning with the intent to fix help behavior.

2. **Help Command Behavior**:
   - For `create`, `delete`, and general `-help` options, the return code is changed from `-1` to `0`.
   - Returning `0` usually indicates a successful or expected outcome, which aligns with typical behavior upon requesting help messages, correcting the earlier behavior where a `-1` might have been misinterpreted as an error.

3. **Unexpected Arguments**:
   - For unrecognized arguments, the method now returns `1` after printing help, indicating an error state resulting from improper usage, fixing the inconsistent prior error code of `-1`.

Overall, these changes address incorrect behavior regarding error handling and help messages by improving consistency and setting expected exit codes. The update ensures that the method returns appropriate success or error codes based on user input variations.

The changes and the commit message both indicate the resolution of an issue with the program’s error-signaling mechanism when invoking help or encountering unexpected input. 

Concluding, the modifications in the code align with the commit message indicating a bug fix for error codes and help behavior. Thus, the changes qualify as **Buggy**."
hadoop,22351.json,1d915238a6a06d09e1789532994f00f496bd969c,"@@ -1,3 +1,3 @@
     public URI[] getCacheFiles() throws IOException {
-      return mapContext.getCacheArchives();
+      return mapContext.getCacheFiles();
     }",Buggy,"MAPREDUCE-5385. Fixed a bug with JobContext getCacheFiles API. Contributed by Omkar Vinit Joshi.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508595 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message explicitly states that a bug is being fixed in the `JobContext getCacheFiles API`. The diff indicates a change in the method call within the `getCacheFiles` method, switching from `mapContext.getCacheArchives()` to `mapContext.getCacheFiles()`. This change aligns with the commit message, suggesting that the previous implementation incorrectly fetched cache archives instead of cache files, which is a logical error in the context of the method's purpose. The update corrects this by calling the appropriate method, thereby fixing the bug.

Thus, the changes clearly address a bug in the method implementation as described in the commit message.

The answer is **Buggy**."
hadoop,8777.json,3b5ea8750202ad9ed0e297d92a90d6dc772ce12a,"@@ -1,45 +1,45 @@
   FSImageStorageInspector readAndInspectDirs()
       throws IOException {
     Integer layoutVersion = null;
     boolean multipleLV = false;
     StringBuilder layoutVersions = new StringBuilder();
 
     // First determine what range of layout versions we're going to inspect
     for (Iterator<StorageDirectory> it = dirIterator();
          it.hasNext();) {
       StorageDirectory sd = it.next();
       if (!sd.getVersionFile().exists()) {
         FSImage.LOG.warn(""Storage directory "" + sd + "" contains no VERSION file. Skipping..."");
         continue;
       }
       readProperties(sd); // sets layoutVersion
       int lv = getLayoutVersion();
       if (layoutVersion == null) {
         layoutVersion = Integer.valueOf(lv);
       } else if (!layoutVersion.equals(lv)) {
         multipleLV = true;
       }
       layoutVersions.append(""("").append(sd.getRoot()).append("", "").append(lv).append("") "");
     }
     
     if (layoutVersion == null) {
       throw new IOException(""No storage directories contained VERSION information"");
     }
     if (multipleLV) {            
       throw new IOException(
-          ""Storage directories containe multiple layout versions: ""
+          ""Storage directories contain multiple layout versions: ""
               + layoutVersions);
     }
     // If the storage directories are with the new layout version
     // (ie edits_<txnid>) then use the new inspector, which will ignore
     // the old format dirs.
     FSImageStorageInspector inspector;
     if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {
       inspector = new FSImageTransactionalStorageInspector();
     } else {
       inspector = new FSImagePreTransactionalStorageInspector();
     }
     
     inspectStorageDirs(inspector);
     return inspector;
   }",Buggy,"HDFS-3629. Fix the typo in the error message about inconsistent storage layout version. Contributed by Brandon Li. (harsh)


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1359905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that the change involves fixing a typo in the error message about inconsistent storage layout versions. The diff confirms this by modifying the word ""containe"" to ""contain,"" correcting the spelling error.

Here's the reasoning breaking down the elements:

1. **Contextual Review of Code Changes**:
   - The function `readAndInspectDirs()` checks storage directories for their layout versions.
   - The code constructs error messages about these versions if inconsistencies are found.

2. **Analysis of the Code Diff**:
   - The code change substitutes ""containe"" with ""contain"" in the error message that is thrown when multiple layout versions are detected.

3. **Relation to the Commit Message**:
   - The commit message specifies the correction of a typo in an error message, which matches the provided diff where ""containe"" is corrected to ""contain"".

4. **Determining if the Change Fixes a Bug**:
   - While the correction improves the error message's clarity by fixing a typo, it doesn't address a functional bug in the program's logic or execution.
   - This is a text correction, not a logical fix, and therefore does not relate to a functional defect or buggy behavior.

Given the nature of the change (correcting a typo), it doesn't constitute a traditional bug fix in terms of program logic or behavior from a user's or system's perspective. It's more of a maintenance or usability improvement to avoid confusion.

Thus, the conclusion is **NotBuggy**."
hadoop,35650.json,c4d7bbda5cb3ceaf54a022f3cf97109e8e190d71,"@@ -1,8 +1,8 @@
-  protected void serviceStart() throws Exception {
-    super.serviceStart();
+  protected void serviceInit(Configuration conf) throws Exception {
+    super.serviceInit(conf);
+    getDispatcher().register(SystemMetricsEventType.class,
+        new TimelineV2EventHandler());
     publishContainerMetrics = getConfig().getBoolean(
         YarnConfiguration.RM_PUBLISH_CONTAINER_METRICS_ENABLED,
         YarnConfiguration.DEFAULT_RM_PUBLISH_CONTAINER_METRICS_ENABLED);
-    getDispatcher().register(SystemMetricsEventType.class,
-        new TimelineV2EventHandler());
   }",Buggy,"YARN-4460. [Bug fix] RM fails to start when SMP is enabled. (Li Lu via Varun Saxena)
",Buggy,"The commit message indicates a bug fix related to the Resource Manager (RM) failure during startup when Service Metrics Publishing (SMP) is enabled. The changes in the diff include:

1. Method Change: The code changes the method `serviceStart()` to `serviceInit(Configuration conf)`, which implies modification in the startup sequence of this service. This method is called earlier in the lifecycle of a service than `serviceStart()`, which is significant for initialization processes.

2. Metric Registration Order: The registration of `TimelineV2EventHandler` is moved to come immediately after the invocation of `super.serviceInit(conf);`, indicating that the registration needs to happen during the initialization phase rather than the start phase. This change can solve issues where certain services need to be available by the time the service starts.

These code changes address the component's initialization sequence, which is directly relevant to correcting failures during startup processes such as those described in the commit message. The bug fix is likely related to ensuring that components are registered and initialized in the correct order to prevent runtime exceptions or logical errors with startup, especially when SMP is enabled.

Thus, given the nature of the changes and the provided commit message, these modifications indeed represent a bug fix. Therefore, the conclusion is **Buggy**."
hadoop,34713.json,3d00c8f3942da931150de79f42cd4913bf751123,"@@ -1,3 +1,3 @@
   public Service[] getServices() {
-    return nodeManagerServices;
+    return NODE_MANAGER_SERVICES;
   }",Buggy,"Made fixes for whitespace errors and checstyle warnings before merge.
",NotBuggy,"The commit message mentions fixes for whitespace errors and checkstyle warnings, which are non-functional code quality improvements. The code change here alters the return statement from returning `nodeManagerServices` to `NODE_MANAGER_SERVICES`, which implies a change in the data being returned from the method `getServices()`. This is a change in functionality rather than a simple fix for whitespace or style warnings.

The change potentially impacts how the `getServices()` method behaves if `NODE_MANAGER_SERVICES` is different from `nodeManagerServices`, even though this change isn't related to fixing a bug explicitly mentioned in the commit message.

Given the discrepancy between the commit message and the code change, and since we can't conclusively determine if this fixes a bug or merely alters functionality without additional context or documentation on `nodeManagerServices` and `NODE_MANAGER_SERVICES`, I would conclude this change is **NotBuggy**. If the actual intent was related to fixing a functional bug, it should have been documented in the commit message."
hadoop,71639.json,9591765040b85927ac69179ab46383eef9560a28,"@@ -1,25 +1,44 @@
   private byte remoteLookup(Message response, Name name, int type,
       int iterations) {
+    // If retrieving the root zone, query for NS record type
+    if (name.toString().equals(""."")) {
+      type = Type.NS;
+    }
+
+    // Always add any CNAMEs to the response first
+    if (type != Type.CNAME) {
+      Record[] cnameAnswers = getRecords(name, Type.CNAME);
+      if (cnameAnswers != null) {
+        for (Record cnameR : cnameAnswers) {
+          if (!response.findRecord(cnameR)) {
+            response.addRecord(cnameR, Section.ANSWER);
+          }
+        }
+      }
+    }
+
     // Forward lookup to primary DNS servers
     Record[] answers = getRecords(name, type);
     try {
       for (Record r : answers) {
-        if (r.getType() == Type.SOA) {
-          response.addRecord(r, Section.AUTHORITY);
-        } else {
-          response.addRecord(r, Section.ANSWER);
+        if (!response.findRecord(r)) {
+          if (r.getType() == Type.SOA) {
+            response.addRecord(r, Section.AUTHORITY);
+          } else {
+            response.addRecord(r, Section.ANSWER);
+          }
         }
         if (r.getType() == Type.CNAME) {
           Name cname = ((CNAMERecord) r).getAlias();
           if (iterations < 6) {
-            remoteLookup(response, cname, Type.CNAME, iterations + 1);
+            remoteLookup(response, cname, type, iterations + 1);
           }
         }
       }
     } catch (NullPointerException e) {
       return Rcode.NXDOMAIN;
     } catch (Throwable e) {
       return Rcode.SERVFAIL;
     }
     return Rcode.NOERROR;
   }",Buggy,"YARN-8410.  Fixed a bug in A record lookup by CNAME record.
            Contributed by Shane Kumpf
",Buggy,"The changes in this diff address the handling of CNAME records in DNS lookups, which is consistent with the commit message indicating a fix for ""A record lookup by CNAME record"". Let's examine the key modifications step by step:

1. **Root Zone Query Adjustment**: 
   - The code introduces a condition to change the query type to NS if querying the root zone (`"".""`). This is likely refining the lookup to handle a special case more accurately.

2. **CNAME Handling Enhancement**:
   - Prior to performing the main lookup, it now retrieves and adds any relevant CNAME records to the response. This step ensures that CNAMEs are accounted for even if the current lookup type isn't directly CNAME. It supports the correctness of DNS resolution by respecting intermediary CNAME records.

3. **Recursion Logic Correction**:
   - Inside the loop over DNS records, the recursive call to `remoteLookup` was corrected. Previously, in case of a CNAME record, the recursive call incorrectly passed `Type.CNAME` as an argument, restricting recursive lookups to only search for CNAME records.
   - The updated logic uses `type` for recursion, which reflects the original intended query type (e.g., A record), ensuring that after resolving through potential CNAME chains, the original type is still pursued. This is crucial for correctly chasing and resolving CNAME chains to actual target records intended by clients.

4. **Error Handling Improvements**:
   - The additional check for `response.findRecord(r)` avoids duplicate DNS records, optimizing the DNS response generation and ensuring no redundancy in the answers provided.

The commit is addressing a logical bug in the handling of CNAME records during DNS lookups, ensuring that intermediary CNAMEs are properly resolved and other types such as A records are correctly retrieved after resolving CNAME chains. These adjustments ensure that DNS responses are accurate and complete, which is essential for network operations depending on name resolution.

Thus, according to the evaluation of the changes related to improving DNS lookup logic and handling CNAME records appropriately, this commit is indeed fixing a bug related to DNS record lookups.

Therefore, the conclusion is **Buggy**."
hadoop,4984.json,076ecf79ca38cbb908f54ea58d985d24486ceefc,"@@ -1,4 +1,4 @@
   public String toString() {
-    return getClass().getSimpleName() + "": "" + snapshotId + "" (post=""
-        + (posteriorDiff == null? null: posteriorDiff.snapshotId) + "")"";
+    return getClass().getSimpleName() + "": "" + this.getSnapshotId() + "" (post=""
+        + (posteriorDiff == null? null: posteriorDiff.getSnapshotId()) + "")"";
   }",Buggy,"HDFS-5726. Fix compilation error in AbstractINodeDiff for JDK7. Contributed by Jing Zhao.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556433 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates that the change is made to fix a compilation error for JDK7 in the `AbstractINodeDiff` class. The diff shows a change in the `toString` method where `snapshotId` is replaced by `this.getSnapshotId()`, and `posteriorDiff.snapshotId` is replaced by `posteriorDiff.getSnapshotId()`. 

Key Points:
1. Use of accessors: The modification replaces direct field access with method calls. This is often done to maintain encapsulation, especially if fields are intended to be private or if there are issues accessing fields directly across different JDK versions or configurations.
2. Compilation error: The direct access to a field `snapshotId` might have led to a compilation issue with JDK7, potentially due to changes in how fields are resolved or accessed in that Java version.

The change is aligned with the commit message ""Fix compilation error in AbstractINodeDiff for JDK7,"" as replacing field access with accessor methods would likely resolve such issues, especially if the fields were made private or the class structure necessitated the use of getters.

Conclusion: The changes indicate a fix for a compilation issue, which can be considered a bug fix. Thus, the answer is **Buggy**."
hadoop,43166.json,23f394240e1568a38025e63e9dc0842e8c5235f7,"@@ -1,61 +1,61 @@
   public int initiateUpgrade(Service service) throws YarnException,
       IOException {
     boolean upgradeEnabled = getConfig().getBoolean(
         YARN_SERVICE_UPGRADE_ENABLED,
         YARN_SERVICE_UPGRADE_ENABLED_DEFAULT);
     if (!upgradeEnabled) {
       throw new YarnException(ErrorStrings.SERVICE_UPGRADE_DISABLED);
     }
     Service persistedService =
         ServiceApiUtil.loadService(fs, service.getName());
     if (!StringUtils.isEmpty(persistedService.getId())) {
       cachedAppInfo.put(persistedService.getName(), new AppInfo(
           ApplicationId.fromString(persistedService.getId()),
           persistedService.getKerberosPrincipal().getPrincipalName()));
     }
 
     if (persistedService.getVersion().equals(service.getVersion())) {
       String message =
           service.getName() + "" is already at version "" + service.getVersion()
               + "". There is nothing to upgrade."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Service liveService = getStatus(service.getName());
     if (!liveService.getState().equals(ServiceState.STABLE)) {
       String message = service.getName() + "" is at "" +
           liveService.getState()
-          + "" state, upgrade can not be invoked when service is STABLE."";
+          + "" state and upgrade can only be initiated when service is STABLE."";
       LOG.error(message);
       throw new YarnException(message);
     }
 
     Path serviceUpgradeDir = checkAppNotExistOnHdfs(service, true);
     ServiceApiUtil.validateAndResolveService(service, fs, getConfig());
     ServiceApiUtil.createDirAndPersistApp(fs, serviceUpgradeDir, service);
 
     ApplicationReport appReport =
         yarnClient.getApplicationReport(getAppId(service.getName()));
     if (StringUtils.isEmpty(appReport.getHost())) {
       throw new YarnException(service.getName() + "" AM hostname is empty"");
     }
     ClientAMProtocol proxy = createAMProxy(service.getName(), appReport);
 
     UpgradeServiceRequestProto.Builder requestBuilder =
         UpgradeServiceRequestProto.newBuilder();
     requestBuilder.setVersion(service.getVersion());
     if (service.getState().equals(ServiceState.UPGRADING_AUTO_FINALIZE)) {
       requestBuilder.setAutoFinalize(true);
     }
     UpgradeServiceResponseProto responseProto = proxy.upgrade(
         requestBuilder.build());
     if (responseProto.hasError()) {
       LOG.error(""Service {} upgrade to version {} failed because {}"",
           service.getName(), service.getVersion(), responseProto.getError());
       throw new YarnException(""Failed to upgrade service "" + service.getName()
           + "" to version "" + service.getVersion() + "" because "" +
           responseProto.getError());
     }
     return EXIT_SUCCESS;
   }",Buggy,"YARN-8610.  Fixed initiate upgrade error message.
            Contributed by Chandni Singh
",NotBuggy,"The commit message states that the change fixes an error message during the initiation of an upgrade process. The code diff reveals a change in the error message related to the service state needed for an upgrade. Originally, the message incorrectly suggested that an upgrade cannot occur if a service is in a STABLE state. The revised message clarifies that an upgrade can only be initiated when the service is in a STABLE state. This adjustment clarifies a logical condition by improving the error message, aligning it with the expected state for an upgrade to begin.

While this change corrects the error message for better clarity, it is not directly fixing a programmatic bug but instead resolving a user-facing message issue. Therefore, it might resolve confusion or miscommunication without affecting the underlying functionality of the codebase.

Given the commit's context of refining the error-handling aspect in messaging without altering functionality or fixing a code logic error, the conclusion is **NotBuggy**."
hadoop,81000.json,1ddb48872f6a4985f4d0baadbb183899226cff68,"@@ -1,9 +1,9 @@
   public long getTimeDuration(String name, long defaultValue,
       TimeUnit defaultUnit, TimeUnit returnUnit) {
     String vStr = get(name);
     if (null == vStr) {
-      return defaultValue;
+      return returnUnit.convert(defaultValue, defaultUnit);
     } else {
       return getTimeDurationHelper(name, vStr, defaultUnit, returnUnit);
     }
   }",Buggy,"HADOOP-16265. Fix bug causing Configuration#getTimeDuration to use incorrect units when the default value is used. Contributed by starphin.
",Buggy,"The commit message indicates a bug fix related to the `Configuration#getTimeDuration` method. The diff demonstrates that the method, when the configuration value is absent (i.e., `vStr` is `null`), previously returned the `defaultValue` directly without converting it into the `returnUnit`. The updated code converts `defaultValue` from `defaultUnit` to `returnUnit` using `returnUnit.convert(defaultValue, defaultUnit)`.

This change addresses a specific logical error: returning a time duration in the incorrect unit when the default value is used. The fix ensures consistency between the units of the default value and what is expected.

Given the provided diff and commit message, the changes align accurately with the described issue, identifying it as a bug fix.

The answer is **Buggy**."
hadoop,10220.json,3335e502446b1542fc99c0c831e0542e53eac6f1,"@@ -1,11 +1,11 @@
-  void shutDownAll() throws InterruptedException {
-    BPOfferService[] bposArray = this.getAllNamenodeThreads();
-    
-    for (BPOfferService bpos : bposArray) {
-      bpos.stop(); //interrupts the threads
-    }
-    //now join
-    for (BPOfferService bpos : bposArray) {
-      bpos.join();
+  void shutDownAll(BPOfferService[] bposArray) throws InterruptedException {
+    if (bposArray != null) {
+      for (BPOfferService bpos : bposArray) {
+        bpos.stop(); //interrupts the threads
+      }
+      //now join
+      for (BPOfferService bpos : bposArray) {
+        bpos.join();
+      }
     }
   }",Buggy,"HDFS-3616. Fix a ConcurrentModificationException bug that BP actor threads may not be shutdown properly in DataNode.  Contributed by Jing Zhao 


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402608 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a fix for a `ConcurrentModificationException` relating to shutting down `BP actor threads`. The change involves modifying the `shutDownAll` method to accept an external `BPOfferService[]` array instead of calling `this.getAllNamenodeThreads()` internally. Previously, the method could potentially operate on a collection that was modified by other threads, leading to a `ConcurrentModificationException`.

By passing the `BPOfferService[]` array as an argument, it reduces the chance of modifying a shared collection while iterating over it. The `if (bposArray != null)` check ensures that the method doesn't attempt to operate on a null array, adding a layer of safety.

These changes align with the commit message, as they address the potential for concurrent modification by removing any implicit handling of shared data within the method. This update is definitely related to fixing a concurrency bug. The correct conclusion here is **Buggy**."
hadoop,26655.json,977d7cc5b947682478ad7b38bd442f0efa1cd204,"@@ -1,45 +1,38 @@
     protected void setup(JobImpl job) throws IOException {
 
       String oldJobIDString = job.oldJobId.toString();
       String user = 
         UserGroupInformation.getCurrentUser().getShortUserName();
       Path path = MRApps.getStagingAreaDir(job.conf, user);
       if(LOG.isDebugEnabled()) {
         LOG.debug(""startJobs: parent="" + path + "" child="" + oldJobIDString);
       }
 
       job.remoteJobSubmitDir =
           FileSystem.get(job.conf).makeQualified(
               new Path(path, oldJobIDString));
       job.remoteJobConfFile =
           new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);
 
       // Prepare the TaskAttemptListener server for authentication of Containers
       // TaskAttemptListener gets the information via jobTokenSecretManager.
       JobTokenIdentifier identifier =
           new JobTokenIdentifier(new Text(oldJobIDString));
       job.jobToken =
           new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);
       job.jobToken.setService(identifier.getJobId());
       // Add it to the jobTokenSecretManager so that TaskAttemptListener server
       // can authenticate containers(tasks)
       job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);
       LOG.info(""Adding job token for "" + oldJobIDString
           + "" to jobTokenSecretManager"");
 
       // Upload the jobTokens onto the remote FS so that ContainerManager can
       // localize it to be used by the Containers(tasks)
       Credentials tokenStorage = new Credentials();
       TokenCache.setJobToken(job.jobToken, tokenStorage);
 
       if (UserGroupInformation.isSecurityEnabled()) {
         tokenStorage.addAll(job.fsTokens);
       }
-
-      Path remoteJobTokenFile =
-          new Path(job.remoteJobSubmitDir,
-              MRJobConfig.APPLICATION_TOKENS_FILE);
-      tokenStorage.writeTokenStorageFile(remoteJobTokenFile, job.conf);
-      LOG.info(""Writing back the job-token file on the remote file system:""
-          + remoteJobTokenFile.toString());
     }",Buggy,"MAPREDUCE-3233. Fixed a bug in MR Job so as to be able to restart the application on AM crash. Contributed by Mahadev Konar.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187669 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message describes a fix for restarting an MR job when the application master (AM) crashes. The code changes remove lines handling the writing of job-token files to the remote file system: 

- Removed: Writing back of the job-token file on the remote file system, which possibly avoids issues during AM restarts.

Since tokens might have been erroneously managed during restarts, removing problematic write actions can stabilize fuctionality. The log message and file writing removal are likely part of the error that interfered with AM crash recovery.

Hence, this diff aligns with the commit message's focus on improving restart logic and fixes a bug related to job token management. The answer is **Buggy**."
hadoop,10300.json,eb7fe1d588de903be2ff6e20384c25c184881532,"@@ -1,24 +1,26 @@
   private void breakHardlinks(File file, Block b) throws IOException {
     final FileIoProvider fileIoProvider = getFileIoProvider();
     final File tmpFile = DatanodeUtil.createFileWithExistsCheck(
         getVolume(), b, DatanodeUtil.getUnlinkTmpFile(file), fileIoProvider);
-    try (FileInputStream in = fileIoProvider.getFileInputStream(
-        getVolume(), file)) {
-      try (FileOutputStream out = fileIoProvider.getFileOutputStream(
-          getVolume(), tmpFile)) {
-        IOUtils.copyBytes(in, out, 16 * 1024);
+    try {
+      try (FileInputStream in = fileIoProvider.getFileInputStream(
+          getVolume(), file)) {
+        try (FileOutputStream out = fileIoProvider.getFileOutputStream(
+            getVolume(), tmpFile)) {
+          IOUtils.copyBytes(in, out, 16 * 1024);
+        }
       }
       if (file.length() != tmpFile.length()) {
-        throw new IOException(""Copy of file "" + file + "" size "" + file.length()+
-                              "" into file "" + tmpFile +
-                              "" resulted in a size of "" + tmpFile.length());
+        throw new IOException(""Copy of file "" + file + "" size "" + file.length()
+            + "" into file "" + tmpFile + "" resulted in a size of ""
+            + tmpFile.length());
       }
       fileIoProvider.replaceFile(getVolume(), tmpFile, file);
     } catch (IOException e) {
       if (!fileIoProvider.delete(getVolume(), tmpFile)) {
         DataNode.LOG.info(""detachFile failed to delete temporary file "" +
                           tmpFile);
       }
       throw e;
     }
   }",Buggy,"HDFS-13509. Bug fix for breakHardlinks() of ReplicaInfo/LocalReplica, and fix TestFileAppend failures on Windows. Contributed by Xiao Liang.
",Buggy,"The commit message indicates a bug fix related to the `breakHardlinks()` method of `ReplicaInfo/LocalReplica`. The changes in the diff primarily focus on restructuring the `try-with-resources` block. 

Originally, the method had two nested `try-with-resources` blocks for file input and output streams, and upon closing the streams, it would check whether the file lengths differ to raise an exception if necessary. The restructuring consolidates these nested blocks into a single outer try-catch with inner `try-with-resources` blocks for each stream. Here's a breakdown:

1. **Code Restructuring**: This refactored structure improves readability and reliability by ensuring that each resource is closed properly, even if an exception occurs.

2. **Error Handling**: The presence of an exception handling mechanism that checks for discrepancies in file size after copying addresses potential issues with file copy operations. If the file lengths differ, it throws an `IOException` to signal an error, which aligns with typical bug-fixing practices.

3. **Bug Fix Indication**: The commit message clearly states a bug fix for the `breakHardlinks()` functionality. The revised code handles copying failures more gracefully, particularly on Windows, where filesystem semantics can differ, potentially leading to the ""TestFileAppend failures"" mentioned.

Overall, the restructuring likely aims to address a subtle bug related to the reliability and correctness of the hardlink breaking operation, particularly under conditions encountered on Windows platforms.

Based on these observations, the changes align with the commit message indicating a bug fix. Therefore, I conclude that these changes are **Buggy**."
hadoop,62895.json,2e61ed306f1d525096a800f28546601ef585a832,"@@ -1,14 +1,14 @@
   public boolean equals(Object obj) {
     if (this == obj)
       return true;
-    if (!super.equals(obj))
+    if (obj == null)
       return false;
     if (getClass() != obj.getClass())
       return false;
     NodeId other = (NodeId) obj;
     if (!this.getHost().equals(other.getHost()))
       return false;
     if (this.getPort() != other.getPort())
       return false;
     return true;
   }",Buggy,"MAPREDUCE-3030. Fixed a bug in NodeId.equals() that was causing RM to reject all NMs. Contributed by Devaraj K.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172638 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message clearly states a bug fix in the `NodeId.equals()` method that was impacting resource manager (RM) operations. The provided diff shows a change in the `equals` method's logic:

1. Originally, the `equals` method was calling `super.equals(obj)` to perform part of its null and type checks. This might have been incorrect for this class, as it depended on the behavior of the superclass's `equals` implementation.
  
2. The modified code replaces this with a direct `obj == null` check, simplifying and making the logic clearer and more explicit. Additionally, this change prevents an unnecessary call to `super.equals(obj)`, which might have been incorrectly returning a `true` or `false` based on unrelated superclass logic.

3. The remaining part of the method checks if `obj` is of the same class, and then compares the `host` and `port` fields, ensuring that the objects are indeed the same.

This update rectifies errors related to the equality determination that might have caused the resource manager to wrongly reject node managers (NMs). Therefore, these changes correspond directly to the commit message's bug fix assertion.

Thus, the changes indeed signal a bug fix, and the conclusion is **Buggy**."
facebook-android-sdk,2616.json,e3a111ea10f0646633777dd8cf5ed06838bf2a62,"@@ -1,13 +1,14 @@
     static float[] maxPool1D(float[] x, int rows, int cols, int pool_size) {
         int len = rows - pool_size + 1;
         float[] res = new float[len * cols];
 
         for (int c = 0; c < cols; c++) {
             for (int i = 0; i < len; i++) {
-                for (int r = i; r < i + pool_size; r++) {
+                res[i * cols + c] = x[i * cols + c];
+                for (int r = i + 1; r < i + pool_size; r++) {
                     res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);
                 }
             }
         }
         return res;
     }",Buggy,"Fix the bug of MaxPool1D

Summary:
Previously, for code:
```
for (int r = i; r < i + pool_size; r++) {
    res[i * cols + c] = Math.max(res[i * cols + c], x[r * cols + c]);​
```
will get 0 if all the number in x is smaller than 0, which would make the prediction not accurate if there is no relu() after maxpooling.

Reviewed By: ninanina0815

Differential Revision: D20881716

fbshipit-source-id: a8a50f1fb584a35780683377a10548f19ab4fede
",Buggy,"The commit message describes a bug in a max pooling function where the result would default to zero if all elements are negative because the initial value of the maximum is zero. The fix addresses this by initializing the result `res[i * cols + c]` with the first element of the pool (`x[i * cols + c]`) instead of zero. This ensures the correct maximum value is selected, even if all numbers are negative.

The code change aligns with the commit message as it directly addresses the problem described, and the modification ensures that the max pooling function behaves correctly without relying on a post-processing step like `relu()`.

Therefore, this change clearly indicates a bug fix in the max pooling logic, making the final conclusion: **Buggy**."
facebook-android-sdk,2859.json,6b275f68c69db30100c542919db3325837938c25,"@@ -1,5 +1,6 @@
     public void onReceive(Context context, Intent intent) {
-        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())) {
+        if (AccessTokenManager.ACTION_CURRENT_ACCESS_TOKEN_CHANGED.equals(intent.getAction())
+                && FacebookSdk.isInitialized()) {
             AccessTokenManager.getInstance().currentAccessTokenChanged();
         }
     }",Buggy,"Fixes bug with SDK Initialize on broadcast

Summary: [android sdk] Fixes bug with SDK Initialize on broadcast

Reviewed By: ct2mak

Differential Revision: D15450566

fbshipit-source-id: 7ec5a7f8f55a658959a5ac7143bd8577c57a2a74
",Buggy,"The commit message indicates the fix is related to SDK initialization during a broadcast. The code change adds a check to ensure FacebookSdk.isInitialized() before proceeding with currentAccessTokenChanged(). This additional condition prevents attempts to handle the broadcast if the SDK is not yet initialized, addressing potential situations where the method might be called prematurely, causing errors. This update improves error-handling and logical correctness, aligning with the commit message's description of fixing a bug concerning SDK initialization.

The code modification is consistent with the commit message, which mentions a bug with SDK initialization. The introduction of the initialization check represents a logical correction in response to a specific bug scenario.

The conclusion is **Buggy**."
facebook-android-sdk,1309.json,39a0d134e7f137fc0b7d727eb0d0f229d43f1db0,"@@ -1,20 +1,20 @@
     private String getChromePackage() {
         if (currentPackage != null) {
             return currentPackage;
         }
         Context context = loginClient.getActivity();
-        Intent serviceIntent = new Intent(CUSTOM_TABS_SERVICE_ACTION);
+        Intent serviceIntent = new Intent(CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION);
         List<ResolveInfo> resolveInfos =
                 context.getPackageManager().queryIntentServices(serviceIntent, 0);
         if (resolveInfos != null) {
             Set<String> chromePackages = new HashSet<>(Arrays.asList(CHROME_PACKAGES));
             for (ResolveInfo resolveInfo : resolveInfos) {
                 ServiceInfo serviceInfo = resolveInfo.serviceInfo;
                 if (serviceInfo != null && chromePackages.contains(serviceInfo.packageName)) {
                     currentPackage = serviceInfo.packageName;
                     return currentPackage;
                 }
             }
         }
         return null;
     }",Buggy,"AndroidX Custom Tab Issue Fix (#670)

Summary:
Thanks for proposing a pull request!

To help us review the request, please complete the following:

- [ ] sign [contributor license agreement](https://developers.facebook.com/opensource/cla)
- [ ] I've ensured that all existing tests pass and added tests (when/where necessary)
- [ ] I've updated the documentation (when/where necessary) and [Changelog](CHANGELOG.md) (when/where necessary)
- [ ] I've added the proper label to this pull request (e.g. `bug` for bug fixes)

## Pull Request Details
Custom Tab has to be working for AndroidX enabled projects

## Expected Results
What do you expect to happen?
If the Facebook app is not installed, then on click of the Facebook login button in App, Custom Chrome Tab has to be open.

## Actual Results
What actually happened? Can you provide a stack trace?
If the Facebook app is not installed, then on click of Facebook login button in App, by default WebView is opening instead of Custom Chrome Tab even I have enabled the Custom Tab for my project.

## Steps to Reproduce
What are the steps necessary to reproduce this issue?
Integrate Facebook Login Button in App
Uninstall Facebook App from the Device
Make sure your project is migrated to AndroidX.
Click on the ""Login With Facebook"" button, WebView is opening instead of Custom Chrome Tab

Please merge the Pull Requests, so that we can use the Custom Chrome Tab in Facebook SDK for AndroidX Projects
Pull Request resolved: https://github.com/facebook/facebook-android-sdk/pull/670

Test Plan:
Built Hackbook with Gradle and buck and chrome custom tabs works for both
Buck: buck install hb4a. This uses Android X and is not working without this fix
Gradle: On Android studio

Went to Login > Login with custom tab to test the feature

Reviewed By: Mxiim

Differential Revision: D19603133

Pulled By: ct2mak

fbshipit-source-id: 44d2f463d2fbd3a50646dc8caf3e4f7cb02c026b
",Buggy,"The commit message indicates a fix for an issue with AndroidX Custom Tabs. The code change in the diff updates an Intent's action from a generic `CUSTOM_TABS_SERVICE_ACTION` string to `CustomTabsService.ACTION_CUSTOM_TABS_CONNECTION`, which is the standardized Android constant for connecting with custom tabs. This change is crucial as it ensures compatibility with AndroidX-enabled projects by accurately resolving the services required for Custom Tabs.

Given the issue described in the commit message (""WebView is opening instead of Custom Chrome Tab""), this code change directly addresses the problem by correcting the action used to resolve the service, ensuring the Custom Tab is used instead of a WebView.

This fix is a clear example of correcting a logical flaw and enhancing the compatibility of the application with the intended component (Custom Chrome Tab) for projects migrated to AndroidX. Thus, this change aligns perfectly with the commit message and represents a bug fix.

The conclusion is **Buggy**."
facebook-android-sdk,2911.json,4c16e0ff5234b07ad26111a74cade26f1fecfcb0,"@@ -1,78 +1,80 @@
     public static void setAppEventExtendedDeviceInfoParameters(
             JSONObject params,
             Context appContext
     ) throws JSONException {
         JSONArray extraInfoArray = new JSONArray();
         extraInfoArray.put(EXTRA_APP_EVENTS_INFO_FORMAT_VERSION);
 
         Utility.refreshPeriodicExtendedDeviceInfo(appContext);
 
         // Application Manifest info:
         String pkgName = appContext.getPackageName();
         int versionCode = -1;
         String versionName = """";
 
         try {
             PackageInfo pi = appContext.getPackageManager().getPackageInfo(pkgName, 0);
             versionCode = pi.versionCode;
             versionName = pi.versionName;
         } catch (PackageManager.NameNotFoundException e) {
             // Swallow
         }
 
         // Application Manifest info:
         extraInfoArray.put(pkgName);
         extraInfoArray.put(versionCode);
         extraInfoArray.put(versionName);
 
         // OS/Device info
         extraInfoArray.put(Build.VERSION.RELEASE);
         extraInfoArray.put(Build.MODEL);
 
         // Locale
         Locale locale;
         try {
             locale = appContext.getResources().getConfiguration().locale;
         } catch (Exception e) {
             locale = Locale.getDefault();
         }
         extraInfoArray.put(locale.getLanguage() + ""_"" + locale.getCountry());
 
         // Time zone
         extraInfoArray.put(deviceTimezoneAbbreviation);
 
         // Carrier
         extraInfoArray.put(carrierName);
 
         // Screen dimensions
         int width = 0;
         int height = 0;
         double density = 0;
         try {
             WindowManager wm = (WindowManager) appContext.getSystemService(Context.WINDOW_SERVICE);
             if (wm != null) {
                 Display display = wm.getDefaultDisplay();
                 DisplayMetrics displayMetrics = new DisplayMetrics();
                 display.getMetrics(displayMetrics);
                 width = displayMetrics.widthPixels;
                 height = displayMetrics.heightPixels;
                 density = displayMetrics.density;
             }
         } catch (Exception e) {
             // Swallow
         }
         extraInfoArray.put(width);
         extraInfoArray.put(height);
-        extraInfoArray.put(String.format(""%.2f"", density));
+
+        final DecimalFormat df = new DecimalFormat(""#.##"");
+        extraInfoArray.put(df.format(density));
 
         // CPU Cores
         extraInfoArray.put(refreshBestGuessNumberOfCPUCores());
 
         // External Storage
         extraInfoArray.put(totalExternalStorageGB);
         extraInfoArray.put(availableExternalStorageGB);
 
         extraInfoArray.put(deviceTimeZoneName);
 
         params.put(""extinfo"", extraInfoArray.toString());
     }",Buggy,"Resolve Issue with String Format

Summary:
Should fix a problem where `String.format` crashes with:

```
Fatal Exception: java.lang.NullPointerException
Attempt to get length of null array
```

For more context, view the [bug report](https://developers.internmc.facebook.com/bugs/406495719925086/)

Reviewed By: KylinChang

Differential Revision: D14219986

fbshipit-source-id: 7d2a4ec0720deb48661fabcbcb39d861ff5b70a7
",Buggy,"The commit message indicates that it resolves an issue with `String.format` that leads to a `NullPointerException` due to a potentially null array. The diff shows that the use of `String.format` to convert a density float to a string with two decimal places has been replaced with `DecimalFormat`. Using `DecimalFormat` improves error handling as it does not require the use of a potentially null array for formatting, unlike `String.format`, which could encounter null-related issues if not properly handled.

Here’s how this change aligns with the commit message:
1. **Error Handling Update**: By switching to `DecimalFormat`, the likelihood of encountering a `NullPointerException` due to a null array being passed to `String.format` is eliminated, addressing the crash described in the commit message.

2. **Logical Correction**: The change corrects how density values are formatted, ensuring robustness against potential null-related issues that were not being handled when `String.format` was used previously.

3. **Commit Message Relevance**: The adjustments noted in the diff are directly aligned with the problem described in the commit message, and they fulfill the purpose of resolving the issue.

Considering the points above, the changes made in this commit suggest the resolution of a bug related to improper handling of array inputs in the `String.format()` method. Therefore, the evaluation concludes with **Buggy** as the updated code aligns with fixing the crash outlined in the commit message."
facebook-android-sdk,3863.json,1583b645976fdcdbe41d34020b13fb6939c5bf8a,"@@ -1,5 +1,5 @@
     public final String getAccessToken() {
         synchronized (this.lock) {
-            return this.tokenInfo.getToken();
+            return (this.tokenInfo == null) ? null : this.tokenInfo.getToken();
         }
     }",Buggy,"[android-sdk] Don't add access_token parameters from a closed Session.

Summary:
Request would try to append an access_token if it could get one from a supplied Session, even if the Session
was closed. This is incorrect; it should act as if no Session was provided and let the server return an error if
appropriate.

Test Plan:
- Modified HelloFacebook to always enable the ""Pick Some Friends"" button
- Logged in
- Picked friends
- Logged out
- Picked friends, observed an error message indicating no access token was provided, as expected
- Added unit tests (and fixed one broken one), verified they ran

Revert Plan:

Reviewers: mmarucheck, mingfli, karthiks, gregschechte

Reviewed By: mmarucheck

Differential Revision: https://phabricator.fb.com/D616959

Task ID: 1852996
",Buggy,"The commit message and code change are aligned as both describe fixing incorrect behavior regarding session access tokens. Specifically, the change addresses a situation where the token was retrieved even from closed sessions. By checking if `tokenInfo` is `null`, the code now correctly avoids using an invalid access token.

The modification ensures that if `tokenInfo` is `null`, which would likely happen when a session is closed, the method returns `null` instead of trying to access a token. This change prevents appending an invalid or non-existent access token to the request, aligning with the commit message's intention to treat closed sessions as if no session was provided.

The test plan in the commit message also supports the notion of bug fixing, mentioning observing an error when attempting actions without a valid access token, which indicates the fix was successful.

Given these observations, the change is indeed a bug fix. Therefore, the conclusion is **Buggy**."
titan,920.json,74166f9174459bcbd8b7b8d7812d6b14c90faba5,"@@ -1,15 +1,34 @@
-    public void unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
+    public boolean unlock(KeyColumn kc, ConsistentKeyLockTransaction requestor) {
 
-        assert locks.containsKey(kc);
-
-        AuditRecord audit = new AuditRecord(requestor, 0);
-
-        assert locks.get(kc).equals(audit);
-
-        locks.remove(kc, audit);
-
-        if (log.isTraceEnabled()) {
-            log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
-                    new Object[]{kc, name, requestor});
+        if (!locks.containsKey(kc)) {
+            log.error(""Local unlock failed: no locks found for {}"", kc);
+            return false;
         }
+
+        AuditRecord unlocker = new AuditRecord(requestor, 0);
+
+        AuditRecord holder = locks.get(kc);
+
+        if (!holder.equals(unlocker)) {
+            log.error(""Local unlock of {} by {} failed: it is held by {}"",
+                    new Object[] { kc, unlocker, holder });
+            return false;
+        }
+
+        boolean removed = locks.remove(kc, unlocker);
+
+        if (removed) {
+            if (log.isTraceEnabled()) {
+                log.trace(""Local unlock succeeded: {} namespace={} txn={}"",
+                        new Object[] { kc, name, requestor });
+            }
+        } else {
+            log.warn(""Local unlock warning: lock record for {} disappeared ""
+                    + ""during removal; this suggests the lock either expired ""
+                    + ""while we were removing it, or that it was erroneously ""
+                    + ""unlocked multiple times."", kc);
+        }
+
+        // Even if !removed, we're finished unlocking, so return true
+        return true;
     }",Buggy,"Locking logging tweaks and renewal fix

* Added new logging statements and reworded existing statements in
  ConsistentKeyLockTransaction and LocalLockMediator

* LocalLockMediator#unlock() now logs errors and returns false instead
  of generating assertion failures when called on an expired or
  nonexistent lock; call sites updated accordingly

* Replaced broken unit test in LockKeyColumnValueStoreTest named
  relockExtendsLocalExpiration with a new test method called
  repeatLockingDoesNotExtendExpiration (closes #264)
",Buggy,"The changes align well with the commit message, explicitly mentioning the update in the `unlock` method. Here's a breakdown of the changes and how they relate to a bug fix:

1. **Return Type Change**: The `unlock` method has changed from `void` to `boolean`, suggesting a change in its behavior by now returning a boolean to indicate success or failure.

2. **Error Logging**: 
   - Previously, the method relied on assertions, which could fail silently in production (if assertions are disabled). The new implementation logs an error when there's no lock for the given key (`kc`), and when the `unlocker` doesn't match the current lock holder.
   - This enhances error detection and makes issues visible in logs, which aligns with the commit message that mentions tweaking logging.

3. **Error Handling with Return Value**:
   - The method now returns `false` if unlocking conditions aren't met, such as when no locks are found for the given key or when the lock holder doesn't match the unlocker.
   - The return value provides better handling for calling code, which can act on it appropriately instead of encountering an assertion failure as before.

4. **Final Return**: 
   - Even if the lock removal fails, the method returns `true` as documented in the code comments, indicating completion of the unlocking attempt (either way).

5. **Commit Message Clarity**: The commit message clearly explains these changes, indicating an improvement to handling expired or nonexistent locks by returning `false` instead of generating assertion failures.

This update reflects a typical bug fix pattern, where error-prone behavior (assertions) is replaced with robust error handling that logs issues and communicates status via return values. Thus, the changes indeed address what could be considered a bug and improve the resilience of the code.

In conclusion, the changes indicate fixing a bug as described in the commit message, so the answer is **Buggy**."
titan,316.json,9b77325b51d00dd5c81ddce7b8cf312b04d7057c,"@@ -1,4 +1,9 @@
     public <O> void set(String key, O value) {
-        if (value==null) config.clearProperty(key);
-        else config.setProperty(key,value);
+        if (value==null) {
+            config.clearProperty(key);
+        } else if (Duration.class.isAssignableFrom(value.getClass())) {
+            config.setProperty(key,((Duration)value).getLength(TimeUnit.MILLISECONDS));
+        } else {
+            config.setProperty(key,value);
+        }
     }",Buggy,"Make CommonsConfiguration write Durations as ms

CommonsConfiguration read Durations as unitless positive milliseconds
expressed as a bare integer, but would serialize through the toString
behavior used for the general case.  The toString was something like
""Duration [60 ms]"", so it was asymmetrical.  Special-casing Duration
on the write path to make it symmetrical.  This fixes an error when
copying the graph's local configuration into a Hadoop configuration
object prior to launching a MapReduce-based index management job.
",Buggy,"The commit message addresses an error due to the asymmetric handling of `Duration` objects during serialization with CommonsConfiguration. The code change modifies the `set` method to check if the `value` is a `Duration` object. If it is, the method stores the `Duration` as milliseconds using `getLength(TimeUnit.MILLISECONDS)`. This adjustment ensures that `Duration` is handled symmetrically by representing it as plain integers (milliseconds), solving the described serialization discrepancy.

This change directly addresses the error mentioned in the commit message and aligns with the bug fix description. Therefore, the changes indicate a bug fix in handling `Duration` objects for serialization, and the conclusion is **Buggy**."
titan,3867.json,a457c6bc3a440d72e045436f8be7a93948568f2e,"@@ -1,21 +1,21 @@
     public List<SliceQuery> getQueries() {
         if (isGlobalGraphIndex()) {
             //Everything
-            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(128), BufferUtil.oneBuffer(128)));
+            return ImmutableList.of(new SliceQuery(BufferUtil.zeroBuffer(1), BufferUtil.oneBuffer(128)));
         } else {
             RelationTypeIndexWrapper wrapper = (RelationTypeIndexWrapper)index;
             InternalRelationType wrappedType = wrapper.getWrappedType();
             Direction direction=null;
             for (Direction dir : Direction.values()) if (wrappedType.isUnidirected(dir)) direction=dir;
             assert direction!=null;
 
             StandardTitanTx tx = (StandardTitanTx)graph.get().buildTransaction().readOnly().start();
             try {
                 QueryContainer qc = new QueryContainer(tx);
                 qc.addQuery().type(wrappedType).direction(direction).relations();
                 return qc.getSliceQueries();
             } finally {
                 tx.rollback();
             }
         }
     }",Buggy,"Fixed bug in IndexRemoveJob. The issue was that a slicequery starting with 128 0s was used as the start slice in an attempt to get all columns. However, if the contents of the buffer are identical it compares the lengths. Hence, a staticbuffer with just a single 0 in it will be considered smaller and hence such index records werent' returned.
",Buggy,"The commit message highlights a specific bug in the `IndexRemoveJob` where using a `SliceQuery` with `BufferUtil.zeroBuffer(128)` as the start could lead to incorrect behavior due to the way comparisons are made. The buffer with a single `0` is considered smaller than a buffer of the same contents but longer length. The code changes reflect this understanding:

- **Change Details**: The diff updates the creation of a `SliceQuery` in the `getQueries()` method.
  - **Old Code**: It used `BufferUtil.zeroBuffer(128)` as the starting buffer.
  - **New Code**: It changes to `BufferUtil.zeroBuffer(1)` as the starting buffer while keeping `BufferUtil.oneBuffer(128)` as the ending buffer.

- **Reasoning**: This change addresses the commit message explanation regarding the mismatch in comparison logic. By initializing the `SliceQuery` with a shorter buffer length, the bug where such index records weren't returned is fixed.

The change directly correlates with the described bug and provides a logical fix by altering the length of the initial buffer to avoid incorrect assumptions based on buffer length comparison.

The modification aligns with the commit message's claim of fixing a bug in index retrieval logic.

Conclusion: **Buggy**"
spring-boot,2459.json,ccb1eaf8eda94ae41db2a432b1781efb421c86c8,"@@ -1,3 +1,3 @@
-		public RedisFlushMode getFlushMode() {
-			return this.flushMode;
-		}

+	public HazelcastFlushMode getFlushMode() {
+		return this.flushMode;
+	}",NotBuggy,"Ensure compatibility with Spring Session module split

This commit updates Spring Session auto-configuration to ensure
compatibility with extraction of `SessionRepository` implementations into
separate Spring Session modules.

See gh-9554
",NotBuggy,"The commit message suggests the changes are about ensuring compatibility with changes in Spring Session modules. The diff changes the type of `getFlushMode()` from `RedisFlushMode` to `HazelcastFlushMode`. This change seems unrelated to a bug fix; instead, it reflects a transition from one session repository configuration to another, possibly due to modularization or an architectural change.

There is no clear indication of error-handling improvements, logical corrections, or exception handling in the provided code diffs. These changes align with the commit message's intent of a compatibility update rather than fixing a bug.

Thus, the assessment is **NotBuggy**."
spring-boot,686.json,d6977ea89f526f45319fb1f57f8eb679e0f5132a,"@@ -1,13 +1,13 @@
 	public void validate() {
-		if (this.getOpaqueToken().getIntrospectionUri() != null) {
+		if (this.getOpaquetoken().getIntrospectionUri() != null) {
 			if (this.getJwt().getJwkSetUri() != null) {
 				handleError(""jwt.jwk-set-uri"");
 			}
 			if (this.getJwt().getIssuerUri() != null) {
 				handleError(""jwt.issuer-uri"");
 			}
 			if (this.getJwt().getPublicKeyLocation() != null) {
 				handleError(""jwt.public-key-location"");
 			}
 		}
 	}",NotBuggy,"Remove hyphenation of opaquetoken in OAuth2 property names

See gh-17290
",NotBuggy,"The commit message states that the change involves removing the hyphenation of ""opaquetoken"" in OAuth2 property names. The diff shows that a method call from `getOpaqueToken()` was changed to `getOpaquetoken()`. This is purely a naming convention update from camel case to all lowercase without any hyphen. There is no indication of a bug fix in logic or functionality within this change; it solely reflects a naming adjustment to align with property naming conventions.

There is no change to the logic of the code or any correction to a previously faulty behavior. Therefore, this change does not indicate a bug fix.

The appropriate conclusion is **NotBuggy**."
spring-boot,8615.json,a6a19292745d2e70aee12feed6660ef0a14840d2,"@@ -1,12 +1,12 @@
-	private void addAstTransformations(final CompilationUnit compilationUnit) {
+	private LinkedList[] getPhaseOperations(final CompilationUnit compilationUnit) {
 		try {
 			Field field = CompilationUnit.class.getDeclaredField(""phaseOperations"");
 			field.setAccessible(true);
 			LinkedList[] phaseOperations = (LinkedList[]) field.get(compilationUnit);
-			processConversionOperations(phaseOperations[Phases.CONVERSION]);
+			return phaseOperations;
 		}
-		catch (Exception npe) {
+		catch (Exception ex) {
 			throw new IllegalStateException(
 					""Phase operations not available from compilation unit"");
 		}
 	}",NotBuggy,"Extract inner-classes from GroovyCompiler

Extract and refactor inner-classes from the GroovyCompiler to improve
code readability.
",NotBuggy,"The commit message describes a refactor aiming to improve code readability by extracting inner classes from `GroovyCompiler`. The changes in the diff, however, show a method transformation and exception variable renaming, unrelated to inner classes or readability enhancements. The method now returns a value rather than having a side effect, which may help reorganization, but it's not extractive refactoring. There's no indicator of bug fixes either, as handling of the exception remains unchanged, focusing on code structure instead.

These modifications align with the commit message's intent to refactor, but there's no evidence of bug fixing or addressing actual bugs in the code. Therefore, my conclusion is **NotBuggy**."
spring-boot,4024.json,2dfd916c96b8fa16c21a9dfcd1a4b9a06893fff4,"@@ -1,12 +1,10 @@
 	public CloudFoundryInfoEndpointWebExtension cloudFoundryInfoEndpointWebExtension(
 			GitProperties properties, ObjectProvider<InfoContributor> infoContributors) {
 		List<InfoContributor> contributors = infoContributors.orderedStream()
-				.map((infoContributor) -> {
-					if (infoContributor instanceof GitInfoContributor) {
-						return new GitInfoContributor(properties,
-								InfoPropertiesInfoContributor.Mode.FULL);
-					}
-					return infoContributor;
-				}).collect(Collectors.toList());
+				.map((infoContributor) -> (infoContributor instanceof GitInfoContributor)
+						? new GitInfoContributor(properties,
+								InfoPropertiesInfoContributor.Mode.FULL)
+						: infoContributor)
+				.collect(Collectors.toList());
 		return new CloudFoundryInfoEndpointWebExtension(new InfoEndpoint(contributors));
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message says ""Polish,"" signaling readability or style improvements. The code change replaces if-else with a ternary operator inside a lambda expression to make it more concise. This doesn't alter functionality or address errors. The answer is **NotBuggy**."
spring-boot,13196.json,8a6e254465a7a11001ba0b3a6234b1a2cf9ee501,"@@ -1,10 +1,10 @@
 	private void writeDefaultValue(AsciidocBuilder builder) {
 		String defaultValue = (this.defaultValue != null) ? this.defaultValue : """";
-		defaultValue = defaultValue.replace(""\\"", ""\\\\"").replace(""|"", ""{vbar}"" + System.lineSeparator());
+		defaultValue = defaultValue.replace(""\\"", ""\\\\"");
 		if (defaultValue.isEmpty()) {
-			builder.appendln(""|"");
+			builder.appendln(""ￂﾦ"");
 		}
 		else {
-			builder.appendln(""|`+"", defaultValue, ""+`"");
+			builder.appendln(""ￂﾦ`+"", defaultValue, ""+`"");
 		}
 	}",NotBuggy,"Avoid need to escape pipe character in reference tables
",NotBuggy,"The commit message describes eliminating the need to escape the pipe character in reference tables. The code changes reflect this by replacing the pipe character ""|"" with what seems to be an alternative representation or placeholder, ""ￂﾦ"". This avoids issues with pipes being interpreted as table delimiters in Asciidoc formatting. These changes align with the commit message and focus on improving the handling of specific characters rather than fixing a logical or runtime error within the code.

Since the modifications involve changing how a character is handled for formatting purposes and do not indicate a resolution of an existing bug (logical or error correction), this commit is best categorized as a formatting or usability adjustment rather than a bug fix.

Thus, the judgment is **NotBuggy**."
spring-boot,11072.json,9ebe15232e20e61edc91089dd0a88d3bcf5f7524,"@@ -1,4 +1,4 @@
-	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(InputStream in)
-			throws IOException {
-		return withJsonResource(in, defaultCharset);
+	public ConfigurationMetadataRepositoryJsonBuilder withJsonResource(
+			InputStream inputStream) throws IOException {
+		return withJsonResource(inputStream, this.defaultCharset);
 	}",NotBuggy,"Polish
",NotBuggy,"The changes in the given diff show minor refactoring for code clarity and consistency. The method signature is slightly updated to use `inputStream` instead of `in`, and `this.defaultCharset` clarifies the reference scope. These are stylistic improvements for readability and do not indicate any bug fixes. The commit message ""Polish"" suggests these changes are for polishing the code rather than fixing bugs.

Since there are no logical changes or fixes that directly address any errors, the conclusion is **NotBuggy**."
spring-boot,2926.json,f7032bd8d3c8bb6a721bd5860bba9a43e611561e,"@@ -1,16 +1,16 @@
 		public void process(AnnotationMetadata annotationMetadata,
 				DeferredImportSelector deferredImportSelector) {
 			Assert.state(
 					deferredImportSelector instanceof AutoConfigurationImportSelector,
 					() -> String.format(
-							""AutoConfigurationImportSelector only supports %s implementations, got %s"",
+							""Only %s implementations are supported, got %s"",
 							AutoConfigurationImportSelector.class.getSimpleName(),
 							deferredImportSelector.getClass().getName()));
 			AutoConfigurationEntry autoConfigurationEntry = ((AutoConfigurationImportSelector) deferredImportSelector)
 					.getAutoConfigurationEntry(getAutoConfigurationMetadata(),
 							annotationMetadata);
 			this.autoConfigurationEntries.add(autoConfigurationEntry);
 			for (String importClassName : autoConfigurationEntry.getConfigurations()) {
 				this.entries.putIfAbsent(importClassName, annotationMetadata);
 			}
 		}",NotBuggy,"Polish contribution

Closes gh-13963
",NotBuggy,"The change in the code involves a text modification in the error message string for clarity and conciseness. The functional logic of the code remains unchanged. The commit message ""Polish contribution"" suggests that this update is for code refinement or aesthetic purposes, such as improving readability or phrasing, rather than fixing bugs.

Since there are no indications of a bug being addressed, such as error-handling corrections, logical corrections, or exception-handling improvements in this particular commit, the conclusion is **NotBuggy**."
spring-boot,5928.json,9bdad06c10ad40c7ef02971cb923d52fd84c8f53,"@@ -1,3 +1,3 @@
 		public String chooseServerAlias(String keyType, Principal[] issuers, Socket socket) {
-			return this.keyManager.chooseServerAlias(keyType, issuers, socket);
+			return this.delegate.chooseServerAlias(keyType, issuers, socket);
 		}",NotBuggy,"Polish Netty SSL key alias support

See gh-19197
",NotBuggy,"The commit message states ""Polish Netty SSL key alias support,"" which implies a refinement or optimization rather than a bug fix. The code change modifies the method to use `this.delegate` instead of `this.keyManager`. This could be an internal improvement to make the implementation consistent or leverage a different object design, but it's not explicitly fixing a bug based on the information provided. This aligns more with code polishing or refactoring, as the commit message suggests, rather than resolving a bug.

Therefore, based on the available information and the context, this set of changes is classified as **NotBuggy**."
spring-boot,4528.json,90949669be923b3b9b7692b663b49e9dee6702ff,"@@ -1,7 +1,4 @@
-	Map<String, Object> parseMap(String json,
+	protected final Map<String, Object> parseMap(String json,
 			Function<String, Map<String, Object>> parser) {
-		assert parser != null;
-
-		return trimIfStartsWith(json, START_MAP).map(parser::apply)
-				.orElseThrow(AbstractJsonParser::cannotParseJson);
+		return trimParse(json, ""{"", parser);
 	}",NotBuggy,"Polish ""refactor spring-boot JSON parser""

Polish JSON parser refactoring and remove some more duplication.

Closes gh-12428
",NotBuggy,"The commit message indicates a refactoring that focuses on polishing code and removing duplication. The changes in the diff:

1. Modify method signature by making `parseMap` protected and final.
2. Remove an assertion for a non-null parser, implying increased confidence in the contract.
3. Replace the existing return logic with a call to `trimParse` which appears to consolidate previously inline logic.

The changes are consistent with the message as they indicate a restructuring aimed at code simplification and consolidation. The indication here is refactoring, not a bug fix. Therefore, the conclusion is **NotBuggy**."
spring-boot,8354.json,76ad1975844550609ff58caa0ce6255be15a9cb7,"@@ -1,11 +1,12 @@
 		private void initializeServlet(Context context, String name) {
 			Container child = context.findChild(name);
 			if (child instanceof StandardWrapper) {
 				try {
-					((StandardWrapper) child).allocate();
+					StandardWrapper wrapper = (StandardWrapper) child;
+					wrapper.deallocate(wrapper.allocate());
 				}
 				catch (ServletException ex) {
 					// Continue
 				}
 			}
 		}",NotBuggy,"Deallocate servlet after forcing initialization in mappings endpoint

Previously, when using Tomcat, a call to mappings endpoint would force
the initialization of any DispatcherServlets in the context. This was
done by calling allocate on Tomcat's StandardWrapper. This left the
wrapper in a state that would cause it to block for two seconds during
shutdown as the wrapper has an outstanding allocation.

This commit immediately deallocates the servlet after it has been
allocated. This ensures that the DispatcherServlet has been initialized
while also leaving the wrapper in a state that it can shut down
immediately when asked to do so.

Closes gh-14898
",Buggy,"This commit message talks about a bug related to servlet allocation in Tomcat's StandardWrapper, specifically about blocking during shutdown due to outstanding allocations. The change in the code deallocates the servlet immediately after allocation, which addresses the problem described. The change ensures that after initialization (through `allocate()`), the wrapper does not retain outstanding allocations, preventing the described blocking issue.

Here's the step-by-step analysis of the changes:

1. **Code Change**: The code initially called only `allocate()` on StandardWrapper. Now, it calls `allocate()` and immediately `deallocate()` on the same object.

2. **Bug Context**: The commit message clearly states the bug: calling `allocate()` without deallocation leaves Tomcat's wrapper with an outstanding allocation, causing a delay during shutdown.

3. **Purpose**: The combination of `allocate()` and `deallocate()` ensures that the servlet initializes correctly but then does not maintain any allocations that would cause blocking.

4. **Exception Handling**: The try-catch block remains unchanged, continuing to handle `ServletException`, meaning exceptions during allocation are still appropriately managed.

The changes are in clear alignment with the commit message, addressing the specific bug concerning Tomcat's servlet lifecycle and shutdown behavior.

Therefore, the changes correspond to a bug fix, and the conclusion is **Buggy**."
spring-boot,1398.json,685babc8295d48710b0c93861ca6b1c3e1a90d8d,"@@ -1,19 +1,20 @@
 	public ConditionEvaluationReport getDelta(ConditionEvaluationReport previousReport) {
 		ConditionEvaluationReport delta = new ConditionEvaluationReport();
-		this.outcomes.forEach((key, value) -> {
-			ConditionAndOutcomes previous = previousReport.outcomes.get(key);
+		this.outcomes.forEach((source, sourceOutcomes) -> {
+			ConditionAndOutcomes previous = previousReport.outcomes.get(source);
 			if (previous == null
-					|| previous.isFullMatch() != value.isFullMatch()) {
-				value.forEach((conditionAndOutcome) -> delta.recordConditionEvaluation(
-								key, conditionAndOutcome.getCondition(),
+					|| previous.isFullMatch() != sourceOutcomes.isFullMatch()) {
+				sourceOutcomes.forEach(
+						(conditionAndOutcome) -> delta.recordConditionEvaluation(source,
+								conditionAndOutcome.getCondition(),
 								conditionAndOutcome.getOutcome()));
 			}
 		});
 		List<String> newExclusions = new ArrayList<>(this.exclusions);
 		newExclusions.removeAll(previousReport.getExclusions());
 		delta.recordExclusions(newExclusions);
 		List<String> newUnconditionalClasses = new ArrayList<>(this.unconditionalClasses);
 		newUnconditionalClasses.removeAll(previousReport.unconditionalClasses);
 		delta.unconditionalClasses.addAll(newUnconditionalClasses);
 		return delta;
 	}",NotBuggy,"Polish ""Use lambdas for map entry iteration where possible""

Closes gh-12626
",NotBuggy,"The commit message indicates a code refactoring to use lambda expressions for map entry iteration. Analyzing the diff, the key change is a straightforward refactoring to use more descriptive variable names (`key` to `source`, `value` to `sourceOutcomes`), enhancing readability. Lambdas are already used before and after the changes, and the logic remains unchanged. No error handling or logical correction signifies a bug fix.

Hence, the changes align with the commit message's intent to polish the code without indicating a bug fix.

The conclusion is **NotBuggy**."
spring-boot,1012.json,99e72664d9eda43c170d339ca7b5c5986f6b8a33,"@@ -1,11 +1,5 @@
 	protected PlatformTransactionManager createTransactionManager() {
-		PlatformTransactionManager txManager;
-		if (this.entityManagerFactory != null) {
-			txManager = new JpaTransactionManager(this.entityManagerFactory);
-		}
-		else {
-			txManager = new DataSourceTransactionManager(this.dataSource);
-		}
-		this.transactionProperties.applyTo(txManager);
-		return txManager;
+		AbstractPlatformTransactionManager transactionManager = createAppropriateTransactionManager();
+		this.properties.getTransaction().applyTo(transactionManager);
+		return transactionManager;
 	}",NotBuggy,"Polish spring transaction manager properties

Polish and update contribution so that TransactionManager properties can
be defined per technology, rather than globally.

Closes gh-7561
",NotBuggy,"The commit diff shows a refactor in how the transaction manager is created and configured. Originally, the type of `PlatformTransactionManager` was determined based on the presence of `entityManagerFactory`. The code is updated to use a method `createAppropriateTransactionManager()` without showing its implementation. This method presumably abstracts and determines the correct transaction manager. Additionally, `this.transactionProperties.applyTo(txManager)` is replaced with `this.properties.getTransaction().applyTo(transactionManager)`, suggesting changes in how properties are applied, likely to support per-technology configurations rather than a global one.

The commit message mentions polishing and updating for flexibility in configuration. There is no indication that the changes are directly related to a bug fix. It seems to focus on improving/configuring code rather than correcting errors in functionality.

Based on the information provided and the nature of the changes, the conclusion is **NotBuggy**."
spring-boot,4009.json,3f00ba3cad8c708eaa06ef599be104e077736132,"@@ -1,9 +1,10 @@
-	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel, Map<String, Link> links) {
+	private Map<String, Link> getAccessibleLinks(AccessLevel accessLevel,
+			Map<String, Link> links) {
 		if (accessLevel == null) {
 			return new LinkedHashMap<>();
 		}
 		return links.entrySet().stream()
-				.filter((e) -> e.getKey().equals(""self"")
-						|| accessLevel.isAccessAllowed(e.getKey()))
+				.filter((entry) -> entry.getKey().equals(""self"")
+						|| accessLevel.isAccessAllowed(entry.getKey()))
 				.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 	}",NotBuggy,"Polish
",NotBuggy,"The commit message ""Polish"" usually indicates improvements or refinements without functional changes. The diff reflects this as it shows only minor stylistic changes: reformatting method parameters for readability and renaming a lambda expression variable from `e` to `entry` for clarity. These modifications don't change the functionality or fix a bug. Therefore, the code change is consistent with the commit message, and there are no signs of bug-related changes here. The conclusion is **NotBuggy**."
spring-boot,3679.json,598e9bb842d02aba092e290045bd53d280bddbe0,"@@ -1,4 +1,4 @@
-	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig config,
+	public WavefrontMeterRegistry wavefrontMeterRegistry(WavefrontConfig wavefrontConfig,
 			Clock clock) {
-		return new WavefrontMeterRegistry(config, clock);
+		return new WavefrontMeterRegistry(wavefrontConfig, clock);
 	}",NotBuggy,"Polish
",NotBuggy,"The diff shows a simple renaming of the parameter from `config` to `wavefrontConfig` in the method `wavefrontMeterRegistry`. This change improves clarity by using a more descriptive parameter name but does not affect functionality or fix any bugs. It aligns with the commit message ""Polish,"" indicating minor code improvements or cleanup.

There are no logical corrections, exception-handling improvements, or signs of bug fixes present in this change. Therefore, this change is not indicative of a bug fix.

The answer is **NotBuggy**."
guava,14599.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,11 +1,11 @@
     public ImmutableTable<R, C, V> build() {
       int size = cells.size();
       switch (size) {
         case 0:
           return of();
         case 1:
-          return new SingletonImmutableTable<R, C, V>(Iterables.getOnlyElement(cells));
+          return new SingletonImmutableTable<>(Iterables.getOnlyElement(cells));
         default:
           return RegularImmutableTable.forCells(cells, rowComparator, columnComparator);
       }
     }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The commit message indicates the use of the diamond operator to simplify code. Indeed, this change replaces explicit generic type parameters with the diamond operator <> in the constructor of SingletonImmutableTable. This update enhances code readability and maintainability, but it doesn't fix any bugs. The code functionality remains unchanged. 

Therefore, the change does not relate to a bug fix as it is merely a syntactic improvement. The conclusion is **NotBuggy**."
guava,22386.json,8894dba44923a6e06842b6696c6eb2bc08c3c7e0,"@@ -1,13 +1,13 @@
     ImmutableList<E> createAsList() {
       return new ImmutableAsList<E>() {
         @Override
         public E get(int index) {
-          return entries[index].getElement();
+          return Indexed.this.get(index);
         }
 
         @Override
-        ImmutableCollection<E> delegateCollection() {
-          return ElementSet.this;
+        Indexed<E> delegateCollection() {
+          return Indexed.this;
         }
       };
     }",NotBuggy,"Pull out a subclass of ImmutableSet for indexed implementations, which are common enough that it saves code overall.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=90549526
",NotBuggy,"This commit focuses on refactoring by refining the subclass structure rather than fixing any particular issue. The change of `entries[index].getElement()` to `Indexed.this.get(index)` and `ElementSet.this` to `Indexed.this` likely reflects the organizational change to support indexed implementations without indicating a bug fix. Thus, the change aligns with the commit message about restructuring for code reuse, not a bug fix. Therefore, the assessment is **NotBuggy**."
guava,16888.json,afe6ff88023693d9c825b8d619dab56276fa8885,"@@ -1,3 +1,3 @@
-    Entry<E> get(int index) {
-      return getEntry(index);
+    E get(int index) {
+      return entries.get(index).getElement();
     }",NotBuggy,"Add hash flooding protection to ImmutableMultiset, falling back to a JDK HashMap backed implementation when probable flooding is detected.

While we're at it, rearchitect ImmutableMultiset on GWT to use that implementation rather than the handrolled hash table in RegularImmutableMultiset.

RELNOTES=Add hash flooding protection to ImmutableMultiset, and significantly improve its GWT impl while we're at it.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=185173745
",NotBuggy,"The commit message describes adding hash flooding protection and improving the implementation of ImmutableMultiset, specifically for GWT (Google Web Toolkit). However, the provided diff doesn't indicate any changes directly related to hash flooding protection or a move to a JDK-based HashMap implementation. Instead, it shows a change in return type and method implementation:

- The previous method returned an `Entry<E>`, utilizing `getEntry(index)`.
- It is replaced by returning an `E`, using `entries.get(index).getElement()`.

This change appears to be a simplification or refactoring of how elements are accessed from some internal data structure (probably `entries`), focusing on retrieving the element directly rather than the entire entry.

Based on the given diff alone, there is no evidence of the changes described in the commit message concerning hash flooding protection or significant architectural overhauls to the GWT implementation. Instead, it seems focused on a direct retrieval change method rather than addressing issues related to flooding or performance.

Thus, considering the specific diff provided, there are no specific signs of a bug fix as part of these changes. As a result, I conclude with **NotBuggy**."
guava,9796.json,4a6ce819542d5fe440bd5d13956bb4e46084325c,"@@ -1,3 +1,3 @@
     public Set<N> predecessors(N node) {
-      return network.successors(node); // transpose
+      return delegate().successors(node); // transpose
     }",NotBuggy,"common.graph: create (package-protected) ForwardingValueGraph and ForwardingNetwork classes, and use those (and ForwardingGraph) to simplify the implementation of the Transposed* inner classes in Graphs.

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=161856111
",NotBuggy,"The commit message specifies a structural refactor to simplify inner class implementations with forwarding classes. This code change switches from directly calling network to using delegate(), likely adhering to a new forwarding pattern. This aligns with simplification and code restructuring rather than a bug fix. Therefore, the conclusion is **NotBuggy**."
guava,12300.json,78b54f8250067837828e9b7235d7db6f8d728a24,"@@ -1,6 +1,11 @@
   public void forEach(BiConsumer<? super K, ? super V> action) {
     checkNotNull(action);
-    for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
-      action.accept((K) keys[i], (V) values[i]);
+    @Nullable Map<K, V> delegate = delegateOrNull();
+    if (delegate != null) {
+      delegate.forEach(action);
+    } else {
+      for (int i = firstEntryIndex(); i >= 0; i = getSuccessor(i)) {
+        action.accept((K) keys[i], (V) values[i]);
+      }
     }
   }",NotBuggy,"Make Compact(Linked)HashMap fall back to a java.util.LinkedHashMap if hash flooding is detected, just as the immutable collections do.

RELNOTES=n/a

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=286604248
",Buggy,"The commit message mentions a fallback to `java.util.LinkedHashMap` in case of hash flooding, similar to behavior in immutable collections. The code diff demonstrates a change in the `forEach` method: before iterating over internal keys and values, it checks if a delegate `Map` is present. If a delegate exists, it uses this map's `forEach` method instead, indicating a fallback mechanism.

This modification aligns well with the commit message, as it exhibits a potential performance enhancement or protection against hash flooding by delegating to a more robust `LinkedHashMap`.

While this change enhances performance or reliability rather than an explicit bug fix, it addresses a flood-related handling issue, which could be perceived as correcting a potential defect in handling edge cases or extreme conditions. Thus, the conclusion is **Buggy**."
guava,17771.json,0c81386a239370c073572212079efc727c911d0b,"@@ -1,3 +1,3 @@
   ImmutableCollection<V> createValues() {
-    return new Values<K, V>(this);
+    return new Values<>(this);
   }",NotBuggy,"Use diamond operator in collect
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165450412
",NotBuggy,"The provided change replaces explicit type arguments in the constructor call with the diamond operator (<>), which is a feature for cleaner and more concise code in Java from version 7 onwards. This change does not fix a bug but rather improves code readability and conciseness by utilizing language features for better type inference. 

The commit message states it is about using the diamond operator in a method related to collecting, and the actual change reflects exactly that by using the diamond operator in the creation of a new `Values` object. There is no indication of error-handling updates, logical corrections, exception-handling improvements, or other signs of bug-related changes.

Therefore, the conclusion is **NotBuggy**."
guava,3767.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-  protected boolean standardEquals(@Nullable Object object) {
+  protected boolean standardEquals(@NullableDecl Object object) {
     return Multisets.equalsImpl(this, object);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message describes a migration from jsr305's `@Nullable` to the Checker Framework's `@NullableDecl`, which is a change in the type annotation used for nullability purposes. The commit message aligns with the code changes, as it shows a direct substitution of these annotations. This migration relates to potential improvements in static analysis or compatibility with the type-checking tools but does not imply a bug fix.

There is no indication that the previous nullability annotation was incorrect or led to a bug, nor does the change suggest logical corrections, error-handling updates, or improvements in exception handling that one would typically associate with a bug fix.

Therefore, this change is more about adopting a different tool or library for nullability checks rather than correcting an error in the code functionality itself. Consequently, the evaluation is **NotBuggy**."
guava,15700.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,4 +1,4 @@
-  public V remove(@Nullable Object rowKey, @Nullable Object columnKey) {
+  public V remove(@NullableDecl Object rowKey, @NullableDecl Object columnKey) {
     Map<C, V> row = Maps.safeGet(rowMap(), rowKey);
     return (row == null) ? null : Maps.safeRemove(row, columnKey);
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message mentions a migration from `jsr305 @Nullable` to `Checker Framework @NullableDecl`, which is focused on updating annotations for static analysis purposes, not runtime behavior. The change from `@Nullable` to `@NullableDecl` merely switches the annotations for the fields, which is a non-functional change aimed at improving code quality or future-proofing against annotation usage rather than fixing a runtime bug.

These annotations primarily help with null analysis during static code checks, so the change itself does not address any bug or issue with the actual logic or functionality of the code. There is no indication of enhancements to error handling or logic that would typically accompany a bug fix.

Therefore, even though these types of changes are important for maintaining code quality and preventing future issues, they are not indicative of a bug fix in terms of existing runtime behavior.

Hence, I conclude with **NotBuggy**."
guava,22187.json,109421efed0515d0921da5895cd67e8c102bed0e,"@@ -1,4 +1,5 @@
   public static PairwiseEquivalence<?> instantiate(SerializationStreamReader reader)
       throws SerializationException {
+    checkGwtRpcEnabled();
     return create((Equivalence<?>) reader.readObject());
   }",NotBuggy,"Prepare for removing GWT-RPC support for Guava types by adding a system property that disables it.

When the time comes, we'll flip the default value, and users who didn't see our earlier warnings can use a system property to restore support until they can stop serializing our types.

[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=265490471
",NotBuggy,"The code change introduces a call to the `checkGwtRpcEnabled()` method at the beginning of the `instantiate` method that handles deserialization using GWT-RPC. The commit message refers to adding a system property to disable GWT-RPC support for Guava types. The method `checkGwtRpcEnabled()` is likely used to check this property and throw an exception or halt the process if the support is disabled.

While this change is related to the commit message, it does not indicate a bug fix. Rather, it introduces a mechanism for deprecating a feature by adding a safeguard for future deprecation in line with the described plan.

Therefore, the changes do not reflect a bug fix but an enhancement for future compatibility and feature management. Consequently, the conclusion for this change is **NotBuggy**."
guava,271.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,7 +1,7 @@
-    public boolean equals(@Nullable Object obj) {
+    public boolean equals(@NullableDecl Object obj) {
       if (obj instanceof SupplierFunction) {
         SupplierFunction<?> that = (SupplierFunction<?>) obj;
         return this.supplier.equals(that.supplier);
       }
       return false;
     }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message explains that the change involves migrating from the `@Nullable` annotation of JSR-305 to the `@NullableDecl` annotation from the Checker Framework. The diff reflects exactly this change in the `equals` method parameter. This modification is not about fixing a bug but rather transitioning to a different nullability annotation system possibly for compatibility, policy, or standards reasons. Such a migration ensures that code adheres to new or updated library annotations but does not directly address a logical or runtime error.

Given that this change is purely about annotation migration and does not relate to any bug symptoms, error handling, or logic correction, the appropriate conclusion is **NotBuggy**."
guava,20365.json,de8eeba94bd6a12a83a05786cf2ddd00d8547b48,"@@ -1,9 +1,9 @@
     HashCode makeHash(Hasher[] hashers) {
       // TODO(user): Get rid of the ByteBuffer here?
       byte[] bytes = new byte[bits / 8];
       ByteBuffer buffer = ByteBuffer.wrap(bytes);
       for (Hasher hasher : hashers) {
         buffer.put(hasher.hash().asBytes());
       }
-      return HashCodes.fromBytes(bytes);
+      return HashCodes.fromBytesNoCopy(bytes);
     }",NotBuggy,"Expose c.g.common.hash.HashCodes and its static factory methods.
-------------
Created by MOE: http://code.google.com/p/moe-java
MOE_MIGRATED_REVID=27671928
",NotBuggy,"The commit message indicates a change related to exposing `HashCodes` and its static factory methods, which seems to be more about code restructuring or API adjustments rather than fixing a bug. 

Analyzing the code, the change replaces `HashCodes.fromBytes(bytes)` with `HashCodes.fromBytesNoCopy(bytes)`. This suggests a modification in how byte arrays are handled, likely aiming at optimizing performance by avoiding unnecessary copying of the byte array. This change does not indicate a bug fix in the usual sense (i.e., correcting logical errors or handling exceptions), but rather an improvement or an optimization.

The commit message about exposing methods also doesn't align directly with the code diff, which is more focused on potentially improving method performance.

Given the lack of direct evidence of error handling, logical correction, or other traditional bug-fixing indicators, the change does not appear to be a bug fix. 

Thus, I conclude the changes are **NotBuggy**."
guava,17383.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,3 +1,3 @@
-      public V2 apply(@Nullable V1 v1) {
+      public V2 apply(@NullableDecl V1 v1) {
         return transformer.transformEntry(key, v1);
       }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message states that this change involves migrating annotations from `@Nullable` to `@NullableDecl`. The diff reflects this migration, where the `@Nullable` annotation is replaced by `@NullableDecl`. This change is purely annotation-related and does not indicate any direct bug-fixing activity, as it deals with enhancing or modifying the code's nullability annotations for better compatibility or understanding about nullability. There is no indication of bug fixing in terms of logic correction, error handling, or exception handling.

Therefore, the change in the diff aligns with the commit message but doesn't indicate any bug fix. The correct classification is **NotBuggy**."
guava,20839.json,8cdd8c718cceb67bd94c83fe60544ff46f9f4460,"@@ -1,3 +1,3 @@
   public static NetworkBuilder<Object, Object> undirected() {
-    return new NetworkBuilder<Object, Object>(false);
+    return new NetworkBuilder<>(false);
   }",NotBuggy,"Use diamond operator for rest.
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=165581940
",NotBuggy,"The commit message indicates a refactoring using the diamond operator, which simplifies generic types by reducing verbosity. The diff reflects this by replacing explicit generic type arguments with <>. There is no indication of a bug being fixed here. This change is a code improvement for readability and simplicity, not a bug fix.

The conclusion is **NotBuggy**."
guava,1897.json,6f22af40e1526b8c194e9e36d457bcd37680c6a3,"@@ -1,24 +1,24 @@
-  public List<V> replaceValues(@Nullable K key, Iterable<? extends V> values) {
+  public List<V> replaceValues(@NullableDecl K key, Iterable<? extends V> values) {
     List<V> oldValues = getCopy(key);
     ListIterator<V> keyValues = new ValueForKeyIterator(key);
     Iterator<? extends V> newValues = values.iterator();
 
     // Replace existing values, if any.
     while (keyValues.hasNext() && newValues.hasNext()) {
       keyValues.next();
       keyValues.set(newValues.next());
     }
 
     // Remove remaining old values, if any.
     while (keyValues.hasNext()) {
       keyValues.next();
       keyValues.remove();
     }
 
     // Add remaining new values, if any.
     while (newValues.hasNext()) {
       keyValues.add(newValues.next());
     }
 
     return oldValues;
   }",NotBuggy,"Migrate from jsr305 @Nullable to Checker Framework @NullableDecl.
[]

-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=178274875
",NotBuggy,"The commit message indicates a migration from `@Nullable` to `@NullableDecl`, which is a change in the way nullability is annotated, but it doesn't necessarily imply a bug fix unless the previous annotation was incorrect or caused issues with null-safety checks and this change resolves it. The code change itself merely updates the annotation for nullability without altering the logic or behavior of the method.

Given that the main focus is on migrating annotations, and the diff does not demonstrate a bug fix (such as a logical or functional error), this change aligns more with code maintenance and migration for consistency in the codebase rather than a direct bug fix.

Therefore, this commit should be classified as **NotBuggy**."
pmd,1864.json,e1b12da1747de470397d4e02e01caaf4e2c81eed,"@@ -1,4 +1,12 @@
-    protected static String[] packageNamesIn(Map<String, String> params) {
-        // TODO
-        return null;
+    protected static String[] packageNamesIn(Map<PropertyDescriptorField, String> params) {
+        String[] packageNames = StringUtil.substringsOf(params.get(LEGAL_PACKAGES),
+                                                        PACKAGE_NAME_DELIMITER);
+
+        for (String name : packageNames) {
+            if (!packageNamePattern.matcher(name).matches()) {
+                throw new IllegalArgumentException(""One name is not a package: '"" + name + ""'"");
+            }
+        }
+
+        return packageNames;
     }",NotBuggy,"Some documentation
",NotBuggy,"The commit message states ""Some documentation,"" but the code changes show significant modifications to a method, not documentation updates.

Examining the code changes:

1. **Method Signature Change:**
   - The method's parameter type is altered from `Map<String, String>` to `Map<PropertyDescriptorField, String>`. This suggests an update in the way parameters are handled, requiring the method to accept different inputs.

2. **Functionality Addition:**
   - The previous method returned `null`, indicating it wasn't fully implemented (`// TODO` comment).
   - The new implementation includes:
     - Extracting package names by splitting a parameter value using `StringUtil.substringsOf`.
     - Acquiring the specific parameter using `params.get(LEGAL_PACKAGES)`, suggesting this could be a valid key from the new `Map` type.
     - A loop that checks each package name against a pattern `packageNamePattern` to verify its correctness.
     - Throwing an `IllegalArgumentException` if a name doesn't conform to the expected package format.

3. **Error Handling:**
   - Newly added error handling with the `IllegalArgumentException` ensures the method only deals with correctly formatted package names, preventing potential runtime issues if invalid data were processed.

Despite the commit message referring to ""Some documentation,"" this code modification represents a significant addition of business logic to check and process package names, along with improved error handling.

Hence, due to the implementation of the actual method logic and the addition of appropriate error handling, this commit addresses a previously unimplemented feature rather than a bug fix. Therefore, the conclusion is **NotBuggy**."
pmd,4829.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,13 +1,13 @@
     private boolean insideLoop(AbstractNode node) {
-        Node n = node.jjtGetParent();
+        Node n = node.getParent();
 
         while (n != null) {
             if (n instanceof ASTDoLoopStatement || n instanceof ASTWhileLoopStatement
                     || n instanceof ASTForLoopStatement || n instanceof ASTForEachStatement) {
                 return true;
             }
-            n = n.jjtGetParent();
+            n = n.getParent();
         }
 
         return false;
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message suggests a refactoring by changing ""node"" method usages. The diff updates from `jjtGetParent()` to `getParent()`, implying a change in method names or access methods. The goal seems to be standardizing or modernizing the API, rather than fixing incorrect behavior or addressing bugs. This update appears to maintain existing functionality without evident bug-related corrections.

Given this, the change aligns with the message but doesn't indicate a bug fix. Therefore, the evaluation is **NotBuggy**."
pmd,3801.json,320339240fc1d34844b1f6eb123265898a0fed4f,"@@ -1,3 +1,3 @@
     public Object visit(ASTColonSubsript node, Object data) {
-        return visit((AbstractModelicaNode) node, data);
+        return visit((ModelicaNode) node, data);
     }",NotBuggy,"[modelica] Small cleanups

* Move AbstractModelicaRule into the `rule` package
* Make the visitor interface mention only published types (ie not
AbstractModelicaNode)
* Other cleanups(eg make interfaces extend ModelicaNode)
",NotBuggy,"The commit message describes a cleanup that involves refactoring and interface adjustments, but does not indicate fixing bugs. The diff modifies the visit method to work with ModelicaNode instead of AbstractModelicaNode, aligning with the commit message. Since there's no indication of a bug fix, just a structural change, the answer is **NotBuggy**."
pmd,8296.json,4175460d3559266c2046a632e40e6c878ade355f,"@@ -1,38 +1,35 @@
     private boolean isMemberVisibleFromClass(Class<?> classWithMember, int modifiers, Class<?> accessingClass) {
         if (accessingClass == null) {
             return false;
         }
 
         // public members
         if (Modifier.isPublic(modifiers)) {
             return true;
         }
 
-        Package accessingPackage = accessingClass.getPackage();
         boolean areInTheSamePackage;
-        if (accessingPackage != null) {
-            areInTheSamePackage = accessingPackage.getName().startsWith(
+        if (accessingClass.getPackage() != null) {
+            areInTheSamePackage = accessingClass.getPackage().getName().startsWith(
                     classWithMember.getPackage().getName());
         } else {
-            return false;
+            return false; // if the package information is null, we can't do nothin'
         }
 
         // protected members
-        if (Modifier.isProtected(modifiers)
-                && (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass))) {
-            return true;
-        }
-
-        // package private
-        if (!(Modifier.isPrivate(modifiers) || Modifier.isPublic(modifiers) || Modifier.isProtected(modifiers))
-                && areInTheSamePackage) {
-            return true;
-        }
-
-        // private members
-        if (Modifier.isPrivate(modifiers) && classWithMember.equals(accessingClass)) {
+        if (Modifier.isProtected(modifiers)) {
+            if (areInTheSamePackage || classWithMember.isAssignableFrom(accessingClass)) {
+                return true;
+            }
+            // private members
+        } else if (Modifier.isPrivate(modifiers)) {
+            if (classWithMember.equals(accessingClass)) {
+                return true;
+            }
+            // package private members
+        } else if (areInTheSamePackage) {
             return true;
         }
 
         return false;
     }",NotBuggy,"Java, typeres: clean up code, add comments
",NotBuggy,"The commit message denotes code cleanup and comment additions, and the diff reflects these intentions. The changes include:

1. Logical Condition reorganization for clarity: The restructuring of the conditions for modifier visibility aims to clarify the logic relative to member access rights, but does not change functionality.
2. Comment Enhancement: There's a small addition of a comment for null package handling, meant to enhance understanding without modifying behavior.
3. Removing Ineffective Code: Accessing the package was slightly optimized by removing unnecessary variable `accessingPackage`. It doesn't alter functionality but simplifies the code.

Overall, these alterations serve to improve code readability without impacting functionality or addressing a specific bug. The changes align well with the commit message focusing on code cleanup and comment additions. Therefore, these modifications are categorized as **NotBuggy**."
pmd,2554.json,9ade75a621f58c22e6aa2eeb71419c7901d1076d,"@@ -1,13 +1,11 @@
      private Language getLanguageByExtension(String extension) {
          Language result = null;
-         File dir = new File(""."");
-         String filename = ""file."" + extension;
 
          for (Language language : languages.values()) {
-             if (language.getFileFilter().accept(dir, filename)) {
+             if (language.getExtensions().contains(extension)) {
                  result = language;
                  break;
              }
          }
          return result;
      }",NotBuggy,"Refactor CPDGui to detect available languages dynamically
",NotBuggy,"The commit message states a refactoring change, not a bug fix, aimed at improving how languages are detected using extensions. The code is modified to use a more straightforward approach, checking if the list of extensions for a language contains the given extension, instead of simulating file filtering. This change simplifies the logic but doesn't inherently indicate a bug fix. It improves maintainability and perhaps performance but is not explicitly tied to resolving a defect in the logic. Therefore, the analysis is consistent with the commit message stating it as a refactor. 

The answer is **NotBuggy**."
pmd,6893.json,40ce9313aef9adcbefb4769d9639751286c14a92,"@@ -1,8 +1,3 @@
     public String getOperator() {
-        if (getImage() != null) {
-            return getImage(); // <<
-        }
-
-        ASTRSIGNEDSHIFT rsshift = (ASTRSIGNEDSHIFT) jjtGetChild(1);
-        return rsshift != null ? "">>"" : "">>>"";
+        return getImage();
     }",NotBuggy,"Deprecate R[UN]SIGNEDSHIFT

No usages to report
",NotBuggy,"The commit message indicates a deprecation of `R[UN]SIGNEDSHIFT`, suggesting a reduction of reliance on certain shift operations, which might have been unnecessary or legacy elements of the code.

Examining the diff:

1. The method `getOperator` was previously using an `if` condition to determine what value to return.
2. The logic involved checking `getImage` for a non-null return, otherwise inspecting `ASTRSIGNEDSHIFT`. Depending on its presence, it returned either `>>` or `>>>`.
3. The updated code simplifies this by only returning `getImage()`, removing the `ASTRSIGNEDSHIFT` logic.

Since the primary change involves code simplification and removing specific checks and usages of `ASTRSIGNEDSHIFT`, it aligns with the deprecation of shift operations hinted at by the commit message. However, the change doesn't directly address a bug or improve error-handling, exception management, or similar bug-related corrections. Instead, it appears to be a straightforward refactor aiming at clean-up in light of deprecated code paths rather than a bug fix.

Thus, the conclusion for this change is **NotBuggy**."
pmd,1967.json,6b50104cb57d40e5057c169fd817b7468f187c48,"@@ -1,3 +1,3 @@
     public static <E> EnumPBuilder<E> builder(String name, Class<E> type) {
-        return new EnumPBuilder<>(type).name(name);
+        return new EnumPBuilder<>(name, type);
     }",NotBuggy,"Corrections for PR #669
",Buggy,"The change modifies the constructor used when creating a new `EnumPBuilder` object. Previously, the method called the constructor with one argument and then set the name via a method call. Now it directly uses a constructor with two arguments, presumably where the name can be set directly during construction.

This change hints that the original way might have been incorrect, possibly leading to incomplete initialization or misuse of the builder pattern, which can be common oversight bugs. Passing all necessary parameters in the constructor can improve correctness by ensuring that an object is fully initialized upon creation without needing subsequent method calls.

Since this modification corrects the way the `EnumPBuilder` is initialized, aligning the fix with the likely bug where the name was not being set as intended or creating initialization errors, this change accurately aligns with a bug fix.

The answer is **Buggy**."
pmd,8796.json,88ff3f0628f4ff6c1228771c4d9297422d7bb28a,"@@ -1,4 +1,4 @@
     public Object visit(ASTClassOrInterfaceDeclaration decl, Object data) {
-        checkCommentMeetsRequirement(data, decl, HEADER_CMT_REQUIREMENT_DESCRIPTOR);
+        checkCommentMeetsRequirement(data, decl, CLASS_CMT_REQUIREMENT_DESCRIPTOR);
         return super.visit(decl, data);
     }",NotBuggy,"[java] CommentRequired: Separate header comment from class comment and header top-level comment

Refs #1683
",Buggy,"The commit message talks about separating header comments from class comments, indicating a correction in how comments are processed or validated. The code change modifies the descriptor used in checkCommentMeetsRequirement. This suggests the method now distinguishes between header and class comments, which aligns with the commit message's intent to separate these comment types.

It's likely this change fixes an issue where class comments were previously evaluated against the wrong requirement descriptor, possibly causing incorrect comment validation behavior. This would be a logic error correction, which aligns with fixing a bug related to comment handling.

Thus, this change reflects a bug fix as it improves the logic for comment verification, matching the intent stated in the commit message. Therefore, the answer is **Buggy**."
pmd,1304.json,32b7cba79a9a8350fb3b13cbedce804ec3d24157,"@@ -1,4 +1,4 @@
         public void syntaxError(Recognizer<?, ?> recognizer, Object offendingSymbol, int line, int charPositionInLine,
-                String msg, RecognitionException ex) {
+                                String msg, RecognitionException ex) {
             throw new ANTLRSyntaxError(msg, line, charPositionInLine, ex);
         }",NotBuggy,"abstraction for cpd with antlr
",NotBuggy,"The commit message describes an abstraction change related to ""cpd with antlr."" However, the diff only shows whitespace change in the `syntaxError` method's parameters, which improves formatting without affecting functionality. There are no indications of bug fixes, logical corrections, or error handling improvements in this code snippet. Therefore, the commit change does not align with the commit message as describing a bug fix or abstraction change for ""cpd with antlr.""

**NotBuggy**"
pmd,1359.json,af5130ab0037fc9b0c6a13305d0037bf7afd88d3,"@@ -1,3 +1,3 @@
     public boolean isDeprecated() {
-        return deprecated != null && deprecated.booleanValue();
+        return deprecated != null && deprecated;
     }",NotBuggy,"Improvements and planned updates around PropertySource
",NotBuggy,"The change in the code simplifies the return statement of the `isDeprecated` method. The modification replaces `deprecated.booleanValue()` with just `deprecated` which is a more concise way to return a `Boolean` value in Java when `deprecated` is a `Boolean` object. Both statements are logically equivalent when `deprecated` is non-null. 

This change is a code simplification (refactoring) rather than a fix for a bug, given that it does not alter the functionality or fix any incorrect behavior. The commit message indicates ""Improvements and planned updates"", aligning with the type of refactoring seen here. There is no mistake correction evident in this specific modification.

Thus, the nature of the change aligns with the commit message as an improvement rather than a bug fix. Therefore, the conclusion is **NotBuggy**."
pmd,2224.json,785d663553072e5aeca8d8448347087e9d6c8aa3,"@@ -1,3 +1,3 @@
-    protected Object createFrom(String value) {
-        return Integer.valueOf(value);
+    protected Integer createFrom(String toParse) {
+        return Integer.valueOf(toParse);
     }",NotBuggy,"Finished refactoring properties
",NotBuggy,"The commit message indicates a refactoring of properties, and the code diff shows a small change related to method signature and parameter naming. 

Here's the analysis of the changes in the diff:

1. **Method Signature Change**:
   - The method's return type is explicitly changed from `Object` to `Integer`.
   - This is a refactoring change that clarifies and restricts the method's usage. An `Integer` is indeed a subclass of `Object`, so the functionality remains the same, but the method is now more specifically typed.

2. **Parameter Renaming**:
   - The parameter name is changed from `value` to `toParse`.
   - This is a refactor for clarity, making the purpose of the parameter more understandable.

3. **Core Functionality**:
   - The core functionality of returning `Integer.valueOf(...)` remains the same, with no changes to the logic or behavior.

The modifications enhance code clarity and type safety without altering logic or fixing a bug. 

Therefore, the changes align with the commit message about refactoring and do not indicate a bug fix. The final assessment is **NotBuggy**."
pmd,7422.json,d9ad819dabcbf8eb6923cc334af48182ad27852e,"@@ -1,3 +1,3 @@
     public ASTStatement getBody() {
-        return (ASTStatement) jjtGetChild(jjtGetNumChildren() - 1);
+        return (ASTStatement) getChild(getNumChildren() - 1);
     }",NotBuggy,"Replace usages of node methods everywhere
",NotBuggy,"The commit message indicates a refactor, changing method names from `jjtGetChild` and `jjtGetNumChildren` to `getChild` and `getNumChildren`. This suggests a standardized API update or simplification, but it doesn't indicate a bug fix, as the core functionality appears unchanged. The answer is **NotBuggy**."
pmd,4286.json,89fcd45f75f79f5f223e4f572ef332f75869dcef,"@@ -1,8 +1,8 @@
-    public boolean visit(AstNode node) {
-	if (parents.peek() == node) {
-	    return true;
-	} else {
-	    buildInternal(node);
-	    return false;
-	}
-    }

+	public boolean visit(AstNode node) {
+		if (parents.peek() == node) {
+			return true;
+		} else {
+			build(node);
+			return false;
+		}
+	}",NotBuggy,Next step towards compile ;-),NotBuggy,"The commit message ""Next step towards compile ;-)"" suggests incremental progress toward compiling the code, but doesn't mention a bug fix. The changes in the diff include:

1. The function `visit` has been reformatted with consistent indentation.
2. The method call `buildInternal(node)` has been changed to `build(node)`.

There is no clear indication from this diff that the change from `buildInternal(node)` to `build(node)` addresses a specific bug. It may be an adjustment to align with another method name or code structure that is evolving, possibly as part of a refactor or modernization effort.

The commit message doesn't specify a bug fix, and the changes don't inherently indicate resolution of a bug—like a logic error or exception handling improvement—within the provided diff. The description is playful and non-informative regarding functionality improvement, further suggesting this isn't a bug fix.

Thus, the conclusion is **NotBuggy**."
pmd,1620.json,0eaac2fbdb4fe4207b3f329fec2fe0b0e19bbc32,"@@ -1,8 +1,8 @@
         public Status step(int type) {
             for (int i = 0; i < this.nextSteps.size(); i++) {
-                if (type == ((Status) nextSteps.get(i)).type) {
-                    return (Status) nextSteps.get(i);
+                if (type == nextSteps.get(i).type) {
+                    return nextSteps.get(i);
                 }
             }
             return null;
         }",NotBuggy,"Checking in some Java 5 changes


git-svn-id: https://pmd.svn.sourceforge.net/svnroot/pmd/trunk@4993 51baf565-9d33-0410-a72c-fc3788e3496d
",NotBuggy,"The change simplifies the code by removing unnecessary casts, as Java 5's generics allow type safety at compile time, eliminating the need for explicit casting. This update is consistent with the commit message, which indicates Java 5 changes. However, these changes don't imply a bug fix, but rather an enhancement or modernization of code. There are no error-handling updates, logical corrections, or exception-handling improvements that indicate a bug-related change. 

Thus, the conclusion is **NotBuggy**."
pmd,3945.json,eb7738c3416f467477c81b805c5f28313527b610,"@@ -1,3 +1,3 @@
     public Object visit(ASTExpression node, Object data) {
-        return visit((AbstractApexNodeBase) node, data);
+        return visit((ApexNode<?>) node, data);
     }",NotBuggy,"Make FormalComment extend ApexNode
",NotBuggy,"The commit message and code change indicate a type hierarchy modification. `FormalComment` could be related to documentation, and the tweak from `AbstractApexNodeBase` to `ApexNode<?>` might be aligning code with hierarchy changes. No bug fix is evident in the code. The answer is **NotBuggy**."
hbase,11147.json,d822ee3a7ccc4959ed5a4b85bb54ff6142aa7d6e,"@@ -1,5 +1,5 @@
   protected void resetActive() {
     // Reset heap to not include any keys
-    this.active = SegmentFactory.instance().createMutableSegment(conf, comparator);
-    this.timeOfOldestEdit = Long.MAX_VALUE;
+    active = SegmentFactory.instance().createMutableSegment(conf, comparator);
+    timeOfOldestEdit = Long.MAX_VALUE;
   }",NotBuggy,"HBASE-20542: Better heap utilization for IMC with MSLABs
",NotBuggy,"The commit message mentions better heap utilization, implying improvements in memory handling. However, the code diff shows stylistic changes by removing `this.` from `this.active` and `this.timeOfOldestEdit`. These changes do not impact functionality or memory efficiency; they merely simplify the code syntax.

There is no evidence in the diff to indicate a bug fix related to heap utilization or any other issue suggested by the commit message.

Therefore, the answer is **NotBuggy**."
hbase,10378.json,d1147eeb7e1d5f41161c7cf5bc5ddb4744ca5b57,"@@ -1,8 +1,7 @@
   public double getFlushPressure() {
     if (getRegionServerAccounting() == null || cacheFlusher == null) {
       // return 0 during RS initialization
       return 0.0;
     }
-    return getRegionServerAccounting().getGlobalMemstoreSize() * 1.0
-        / cacheFlusher.globalMemStoreLimitLowMark;
+    return getRegionServerAccounting().getFlushPressure();
   }",NotBuggy,"HBASE-15787 Change the flush related heuristics to work with offheap size
configured (Ram)
",NotBuggy,"The commit message describes a change in heuristics for flush operations to consider off-heap memory size configurations. The code diff reflects this by replacing the calculation that uses `getGlobalMemstoreSize()` and `globalMemStoreLimitLowMark` with a call to `getFlushPressure()`. This suggests an update to more accurately reflect flush pressure possibly considering off-heap memory, aligning with the commit message purpose. However, there's no indication that it addresses an existing defect or incorrect behavior; rather, it updates the logic based on new configurations.

Therefore, this change is more about improvements or updates in functionality than fixing a specific bug. The conclusion is **NotBuggy**."
hbase,5011.json,4c6a98bd767f3bec4d2e6107016267a09d58dd12,"@@ -1,6 +1,6 @@
-  public void toStringClassDetails(StringBuilder sb) {
+  public void toStringClassDetails(final StringBuilder sb) {
     sb.append(getClass().getSimpleName());
     sb.append("" (table="");
     sb.append(getTableName());
     sb.append("")"");
   }",NotBuggy,"HBASE-16618 Procedure v2 - Add base class for table and ns procedures
",NotBuggy,"The commit message indicates an addition of a base class related to table and namespace (ns) procedures, which is part of a larger feature or refactor, not a bug fix. The change made here is a minor stylistic or convention update—adding the `final` keyword to a method parameter—likely for better code stability or style adherence, but unrelated to fixing a bug. The diff does not suggest any error-handling improvements, logical corrections, or any other classical signs of bug fixes. Hence, this change aligns more with code enhancement practices.

Therefore, the analysis concludes with **NotBuggy**."
hbase,12378.json,30424ec73f7f100b233e27196c0a6a90abd62ad2,"@@ -1,3 +1,3 @@
-  public long getModificationTime() {
-    return this.fileStatus.getModificationTime();
+  public long getModificationTime() throws IOException {
+    return getFileStatus().getModificationTime();
   }",NotBuggy,"HBASE-12749 Tighten HFileLink api to enable non-snapshot uses
",NotBuggy,"The diff changes the `getModificationTime` method to throw an `IOException`. This indicates an update in error-handling to reflect scenarios where accessing the file status might fail, which wasn't accounted for previously. However, the commit message focuses on API changes to enable non-snapshot uses, rather than directly addressing a bug. 

The change might indirectly address a bug by ensuring exceptions are properly propagated, but given the primary focus in the commit message and the lack of specific reference to a bug, this commit primarily refines the API.

Therefore, this change does not directly correspond to a bug fix, and it aligns more with API tightening as described in the commit message. The conclusion is **NotBuggy**."
hbase,35546.json,f3f17fa111f37233ddc42ddb9c38594e35d8d501,"@@ -1,8 +1,8 @@
-  private void releaseLock(final Procedure proc, final boolean force) {
-    final TEnvironment env = getEnvironment();
+  private void releaseLock(Procedure<TEnvironment> proc, boolean force) {
+    TEnvironment env = getEnvironment();
     // For how the framework works, we know that we will always have the lock
     // when we call releaseLock(), so we can avoid calling proc.hasLock()
-    if (force || !proc.holdLock(env)) {
-      proc.doReleaseLock(env);
+    if (force || !proc.holdLock(env) || proc.isFinished()) {
+      proc.doReleaseLock(env, store);
     }
   }",NotBuggy,"HBASE-20846 Restore procedure locks when master restarts
",Buggy,"The commit message indicates restoring procedure locks upon a master restart, part of correcting behavior issues. This diff modifies releaseLock to ensure proc.isFinished() is considered before releasing a lock, adding robustness in lock handling and addressing potential issues during master restarts. Enforcing this check suggests preventing premature lock releases, signifying a bug fix. Restating changed method signatures is benign and unrelated. Overall, the code change is consistent with fixing a described bug. The answer is **Buggy**."
hbase,6414.json,6c22333599b9910314f57d0b6a580fb69eb7aa2b,"@@ -1,14 +1,14 @@
   public void whoAmI(RpcController controller, AuthenticationProtos.WhoAmIRequest request,
                      RpcCallback<AuthenticationProtos.WhoAmIResponse> done) {
-    User requestUser = RequestContext.getRequestUser();
+    User requestUser = RpcServer.getRequestUser();
     AuthenticationProtos.WhoAmIResponse.Builder response =
         AuthenticationProtos.WhoAmIResponse.newBuilder();
     if (requestUser != null) {
       response.setUsername(requestUser.getShortName());
       AuthenticationMethod method = requestUser.getUGI().getAuthenticationMethod();
       if (method != null) {
         response.setAuthMethod(method.name());
       }
     }
     done.run(response.build());
   }",NotBuggy,"HBASE-13397 Purge duplicate rpc request thread local
",Buggy,"The commit message indicates removing duplicate RPC request thread locals. The change replaces RequestContext.getRequestUser() with RpcServer.getRequestUser(). This likely ensures the user retrieval method aligns with the intended architecture, removing potential duplication issues. Assuming this change solves unintended behavior without functional repercussions, the modification aligns with a bug fix concerning resource management. The answer is **Buggy**."
hbase,35116.json,17703f03614e0803f46eadb70a2242060d04125c,"@@ -1,23 +1,23 @@
   public String toString() {
     StringBuilder sb = new StringBuilder();
     toStringClassDetails(sb);
 
     if (procId != null) {
       sb.append("" id="");
       sb.append(getProcId());
     }
 
     if (hasParent()) {
       sb.append("" parent="");
       sb.append(getParentProcId());
     }
 
     if (hasOwner()) {
       sb.append("" owner="");
       sb.append(getOwner());
     }
 
     sb.append("" state="");
-    sb.append(getState());
+    toStringState(sb);
     return sb.toString();
   }",NotBuggy,"HBASE-14015 Allow setting a richer state value when toString a pv2
",NotBuggy,"The commit message indicates an enhancement for toString in ""pv2,"" allowing a ""richer state value."" This diff aligns with that message. The original concatenation of getState() was replaced with toStringState(sb), presumably providing more detail. The changes improve information completeness but don't hint at a bug fix. The conclusion is **NotBuggy**."
hbase,7109.json,a9b671b31f07ade8968b42956aa60c722032dcc8,"@@ -1,48 +1,48 @@
   private byte[] createModifiedVisExpression(final List<Tag> tags)
       throws IOException {
     StringBuilder visibilityString = new StringBuilder();
     for (Tag tag : tags) {
       if (tag.getType() == TagType.VISIBILITY_TAG_TYPE) {
         if (visibilityString.length() != 0) {
           visibilityString.append(VisibilityConstants.CLOSED_PARAN).append(
               VisibilityConstants.OR_OPERATOR);
         }
-        int offset = tag.getTagOffset();
-        int endOffset = offset + tag.getTagLength();
+        int offset = tag.getValueOffset();
+        int endOffset = offset + tag.getValueLength();
         boolean expressionStart = true;
         while (offset < endOffset) {
-          Pair<Integer, Integer> result = StreamUtils.readRawVarint32(tag.getBuffer(), offset);
+          Pair<Integer, Integer> result = TagUtil.readVIntValuePart(tag, offset);
           int currLabelOrdinal = result.getFirst();
           if (currLabelOrdinal < 0) {
             int temp = -currLabelOrdinal;
             String label = this.labelsCache.getLabel(temp);
             if (expressionStart) {
               // Quote every label in case of unicode characters if present
               visibilityString.append(VisibilityConstants.OPEN_PARAN)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR)
                   .append(VisibilityConstants.NOT_OPERATOR).append(CellVisibility.quote(label));
             }
           } else {
             String label = this.labelsCache.getLabel(currLabelOrdinal);
             if (expressionStart) {
               visibilityString.append(VisibilityConstants.OPEN_PARAN).append(
                   CellVisibility.quote(label));
             } else {
               visibilityString.append(VisibilityConstants.AND_OPERATOR).append(
                   CellVisibility.quote(label));
             }
           }
           expressionStart = false;
           offset += result.getSecond();
         }
       }
     }
     if (visibilityString.length() != 0) {
       visibilityString.append(VisibilityConstants.CLOSED_PARAN);
       // Return the string formed as byte[]
       return Bytes.toBytes(visibilityString.toString());
     }
     return null;
   }",NotBuggy,"HBASE-12593 Tags to work with ByteBuffer.
",Buggy,"The commit message ""Tags to work with ByteBuffer"" hints at an effort to update how tags interact with ByteBuffer or improve tag handling. Examining the diff:

1. **Method Changes**: The key changes involve:
   - Replacing `tag.getTagOffset()` and `tag.getTagLength()` with `tag.getValueOffset()` and `tag.getValueLength()`. This potentially adjusts the starting point and length of the tag value, likely correcting how tags are parsed or processed.
   - Changing `StreamUtils.readRawVarint32(tag.getBuffer(), offset)` to `TagUtil.readVIntValuePart(tag, offset)`. This indicates an internal method change, which might be an optimization or bug fix related to how tag values are read and interpreted.

The variable `offset` is essential for correctly processing tags, usually representing their parsed position in a buffer. Any miscalculation might result in incorrect data processing or application errors. Such changes often relate to bug fixes, especially when it involves reading or interpreting data.

Considering these changes alongside the commit message stating an update to work with ByteBuffer, it seems to address underlying issues of how tags are handled, likely fixing a problem related to tag parsing or tag buffer reading.

Thus, the changes in this commit seem to align with the message and indicate a fix addressing issues with tag handling, which could manifest as a bug fix. Therefore, the assessment is **Buggy**."
hbase,20259.json,d82c1a6c2b534c63bb347cf1d21c36af400b0f19,"@@ -1,9 +1,9 @@
       public void read(org.apache.thrift.protocol.TProtocol prot, addColumnFamily_result struct) throws org.apache.thrift.TException {
-        TTupleProtocol iprot = (TTupleProtocol) prot;
-        BitSet incoming = iprot.readBitSet(1);
+        org.apache.thrift.protocol.TTupleProtocol iprot = (org.apache.thrift.protocol.TTupleProtocol) prot;
+        java.util.BitSet incoming = iprot.readBitSet(1);
         if (incoming.get(0)) {
           struct.io = new TIOError();
           struct.io.read(iprot);
           struct.setIoIsSet(true);
         }
       }",NotBuggy,"HBASE-21791 Upgrade thrift dependency to 0.12.0
",NotBuggy,"The commit message indicates an upgrade of the Thrift dependency to version 0.12.0. The code changes show modifications mainly involving the specification of fully qualified names for certain classes like `TTupleProtocol` and `BitSet`. These changes are typically necessary to resolve issues that arise from API changes or updates in library dependencies, such as class name conflicts or package restructuring, which often occur during version upgrades.

However, the changes themselves do not specifically point to a bug fix in the functionality. Instead, they aim to ensure compatibility with the new version of the Thrift library. Without evidence of logical corrections, error-handling updates, or other definitive signs of fixing functional problems within the application beyond addressing compatibility, these changes align more with maintaining compatibility due to a library upgrade rather than rectifying a code bug.

Thus, the code change is more about ensuring continuous integration with the updated dependency rather than directly fixing a software bug. Therefore, it is appropriate to conclude that the nature of this commit is **NotBuggy**."
hbase,5138.json,826b9436fbba39ce78650680c71425bc8547cd39,"@@ -1,9 +1,9 @@
   private TableQueue getTableQueue(TableName tableName) {
     TableQueue node = AvlTree.get(tableMap, tableName, TABLE_QUEUE_KEY_COMPARATOR);
     if (node != null) return node;
 
-    NamespaceQueue nsQueue = getNamespaceQueue(tableName.getNamespaceAsString());
-    node = new TableQueue(tableName, nsQueue, getTablePriority(tableName));
+    node = new TableQueue(tableName, tablePriorities.getPriority(tableName),
+        locking.getTableLock(tableName), locking.getNamespaceLock(tableName.getNamespaceAsString()));
     tableMap = AvlTree.insert(tableMap, node);
     return node;
   }",NotBuggy,"HBASE-17605 Changes
- Moved locks out of MasterProcedureScheduler#Queue. One Queue object is used for each namespace/table, which aren't more than 100. So we don't need complexity arising from all functionalities being in one place. SchemaLocking now owns locks and locking implementaion has been moved to procedure2 package.
- Removed NamespaceQueue because it wasn't being used as Queue (add,peek,poll,etc functions threw UnsupportedOperationException). It's was only used for locks on namespaces. Now that locks have been moved out of Queue class, it's not needed anymore.
- Remoed RegionEvent which was there only for locking on regions. Tables/namespaces used locking from Queue class and regions couldn't (there are no separate proc queue at region level), hence the redundance. Now that locking is separate, we can use the same for regions too.
- Removed QueueInterface class. No declarations, except one implementaion, which makes the point of having an interface moot.
- Removed QueueImpl, which was the only concrete implementation of abstract Queue class. Moved functions to Queue class itself to avoid unnecessary level in inheritance hierarchy.
- Removed ProcedureEventQueue class which was just a wrapper around ArrayDeque class. But we now have ProcedureWaitQueue as 'Type class'.
- Encapsulated table priority related stuff in a single class.
- Removed some unused functions.
Change-Id: I6a60424cb41e280bc111703053aa179d9071ba17
",NotBuggy,"The commit message describes a significant refactoring of locking mechanisms and class design simplification within the HBASE codebase. The changes involve removing obsolete and redundant classes associated with queue and locking management, in addition to restructuring how locks are managed for tables and namespaces.

Analyzing the diff:
- The `getTableQueue` method has been updated to account for the new locking system. The outdated `NamespaceQueue` reference is removed, and instead, it uses a more direct lock retrieval mechanism from the `locking` object for both table and namespace locks.
- This change aligns with the commit message by focusing on refactoring the lock management system and removing unnecessary complexities in the queue system.

Although this specific excerpt does not highlight a bug fix but a structural refactoring, the overarching goal in the commit seems to deal with improving the system's robustness and maintainability by removing unused and complex components. However, without explicit mention of a bug in this part of the code and based on the provided code change:

1. It enhances performance and maintains locking consistency.
2. Aligns with the commit message which primarily describes restructuring and cleanup.

For determination of a ""bug fix"":
- The diff does not show evidence of a direct bug fix, such as logic corrections or error handling improvements.
- The changes are more oriented towards code refactoring and architectural adjustments.

Thus, the code changes primarily represent refactoring rather than addressing a specific bug in this section. Therefore, the conclusion for this diff is **NotBuggy**."
hbase,28109.json,dec0ec229070465ce5a1f7381366a82278149f06,"@@ -1,7 +1,4 @@
   public HBaseProtos.TableState convert() {
     return HBaseProtos.TableState.newBuilder()
-        .setState(this.state.convert())
-        .setTable(ProtobufUtil.toProtoTableName(this.tableName)) // set for backward compatibility
-        .setTimestamp(this.timestamp)
-            .build();
+        .setState(this.state.convert()).build();
   }",NotBuggy,"HBASE-13016 Clean up remnants of table states stored in table descriptors (Andrey Stepachev)
",NotBuggy,"The commit message suggests a cleanup related to removing table states stored in table descriptors. The diff shows that entries such as `setTable` and `setTimestamp` are being removed from the `HBaseProtos.TableState` building process, leaving only the `setState` call.

This change aligns with the message, which implies removing unnecessary or obsolete information that no longer needs to be stored as part of the table state. However, this cleanup doesn't indicate a bug fix. It addresses technical debt or simplifies the existing code by removing outdated components but doesn't directly correct any incorrect behavior.

Therefore, the commit message and the code change are consistent with each other in terms of removing remnants, but they don't indicate a bug fix. Concluding based on this analysis, the result is **NotBuggy**."
hbase,3595.json,eddc746457d0d25db1b51bef953db684131d4dd0,"@@ -1,3 +1,3 @@
-      protected void chore() {
-        master.balance();
-      }

+  protected void chore() {
+    master.balance();
+  }",NotBuggy,"HBASE-6730 Enable rolling averages in StochasticLoadBalancer

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1387865 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message references enabling rolling averages in the StochasticLoadBalancer, suggesting a functional enhancement related to load balancing. However, the code change simply adjusts the indentation of the `chore` method and does not introduce logic changes or new functionality. It appears that the diff might be incorrectly aligned with the commit message provided, as there's no implementation of rolling averages nor any substantial change visible here.

This change does not indicate a bug fix. There are no error-handling updates, logical corrections, or exception-handling improvements that align with the message's context. This modification seems unrelated to bug resolution or the mentioned enhancement. 

Hence, the conclusion is **NotBuggy**."
hbase,14572.json,33bedf8d4d7ec320c5cc01c1c031035a1523f973,"@@ -1,14 +1,14 @@
   public void put(E e) throws InterruptedException {
-    if (e == null) throw new NullPointerException();
+    Objects.requireNonNull(e);
 
     lock.lock();
     try {
       while (queue.remainingCapacity() == 0) {
         notFull.await();
       }
       this.queue.add(e);
       notEmpty.signal();
     } finally {
       lock.unlock();
     }
   }",NotBuggy,"HBASE-23308: Review of NullPointerExceptions (#836)

Signed-off-by: stack <stack@apache.org>",NotBuggy,"The commit message indicates a review for potential `NullPointerExceptions`. The code change replaces a manual check for `null` with the use of `Objects.requireNonNull(e)`. This change still serves the same purpose: preventing the addition of a `null` element to the queue by throwing a `NullPointerException` if `e` is `null`.

This adjustment aligns with the stated intent in the commit message to review and possibly revise `NullPointerException` handling. However, this change specifically does not fix a bug but rather refines the way a potential null value is handled using a standard utility, thereby making the code cleaner and more consistent.

Given that this change alone only refines existing behavior rather than fixing a bug, the approach resulting from the potential for clearer exception handling is valued as more idiomatic Java. Since there is no change to the error handling logic beyond employing a standard library method, it doesn't clearly reflect a specific bug fix, just an enhancement.

Thus, the final assessment is **NotBuggy** since it aligns more with code refinement and consistency improvements for null handling."
hbase,883.json,4b91a6425eadc3578ec291496bbf7fde9742f96a,"@@ -1,31 +1,32 @@
-  public static void copySnapshotForScanner(Configuration conf, FileSystem fs, Path rootDir,
-      Path restoreDir, String snapshotName) throws IOException {
+  public static RestoreMetaChanges copySnapshotForScanner(Configuration conf, FileSystem fs,
+      Path rootDir, Path restoreDir, String snapshotName) throws IOException {
     // ensure that restore dir is not under root dir
     if (!restoreDir.getFileSystem(conf).getUri().equals(rootDir.getFileSystem(conf).getUri())) {
       throw new IllegalArgumentException(""Filesystems for restore directory and HBase root "" +
           ""directory should be the same"");
     }
     if (restoreDir.toUri().getPath().startsWith(rootDir.toUri().getPath())) {
       throw new IllegalArgumentException(""Restore directory cannot be a sub directory of HBase "" +
           ""root directory. RootDir: "" + rootDir + "", restoreDir: "" + restoreDir);
     }
 
     Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
     SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
     SnapshotManifest manifest = SnapshotManifest.open(conf, fs, snapshotDir, snapshotDesc);
 
     MonitoredTask status = TaskMonitor.get().createStatus(
         ""Restoring  snapshot '"" + snapshotName + ""' to directory "" + restoreDir);
     ForeignExceptionDispatcher monitor = new ForeignExceptionDispatcher();
 
     // we send createBackRefs=false so that restored hfiles do not create back reference links
     // in the base hbase root dir.
     RestoreSnapshotHelper helper = new RestoreSnapshotHelper(conf, fs,
       manifest, manifest.getTableDescriptor(), restoreDir, monitor, status, false);
-    helper.restoreHdfsRegions(); // TODO: parallelize.
+    RestoreMetaChanges metaChanges = helper.restoreHdfsRegions(); // TODO: parallelize.
 
     if (LOG.isDebugEnabled()) {
       LOG.debug(""Restored table dir:"" + restoreDir);
       FSUtils.logFileSystemState(fs, restoreDir, LOG);
     }
+    return metaChanges;
   }",NotBuggy,"HBASE-14714 some cleanup to snapshot code
",NotBuggy,"The commit message refers to cleanup in snapshot code. Let's analyze the changes:

1. **Return Type Change**: The function `copySnapshotForScanner` had its return type changed from `void` to `RestoreMetaChanges`. This indicates a change in design, likely to return additional information about the restoration process.

2. **Variable Introduction**: The variable `metaChanges` is introduced to capture the output from the method `helper.restoreHdfsRegions()`, which did not return a value previously.

3. **Return Statement Addition**: The code now ends with `return metaChanges;`, suggesting that the function now provides feedback on the process or results of the snapshot restoration.

These modifications align with the commit message of cleaning up the snapshot code but don't directly reflect fixing a specific bug. There are no logical corrections, error-handling updates, or exception-handling improvements typically associated with bug fixes.

Thus, the changes seem to focus on code design and possibly enhancing functionality rather than fixing a defect.

Therefore, the evaluation is **NotBuggy**."
hbase,40253.json,6af4292630daca370de7fce3a2b2b3401cd1abfa,"@@ -1,4 +1,5 @@
-  public String toString() {
-    return ""id: "" + transactionId + "", particpants: ""
-        + participatingRegions.size();
+  public static String toString(final byte [] b1,
+                                String sep,
+                                final byte [] b2) {
+    return toString(b1, 0, b1.length) + sep + toString(b2, 0, b2.length);
   }",NotBuggy,"HBASE-1304 - New client server implementation of how gets and puts are handled. -- Thanks to jgray,holstad,stack,rawson



git-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@782178 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests a new client-server implementation, possibly related to improving how ""gets and puts"" operations are handled in HBase. However, the provided diff shows a change in a `toString` method, altering its signature and functionality. This change replaces a non-static method that returned transaction details with a static method for converting two byte arrays to Strings, separated by a separator.

Assessment of the relevance between the commit message and the code modifications:

1. **Commit Message Context**: The message indicates a major change or enhancement related to how certain operations are handled in HBase. It doesn't mention specific bug fixes, but rather a broad implementation update.

2. **Code Modifications**: 
   - The original `toString()` method was an instance method, presumably used to describe a particular object's state (including transaction ID and participating regions count).
   - The new `toString()` method is static and utility-like, focusing on converting byte arrays into a String. It serves a completely different purpose compared to the original method.

3. **Bug Fix Indicators**: The modifications do not explicitly point to a bug or its fix. There's no indication of error correction or exception handling related to ""gets and puts"" in the HBase context.

Considering the new method's purpose and the nature of the commit message, the change seems to be part of a larger refactoring or feature addition rather than addressing a specific bug. The specific change in the diff isn't directly tied to fixing previously faulty behavior; instead, it seems auxiliary to broader implementation work not detailed in the diff.

Therefore, I conclude that this change is **NotBuggy**."
ant,1414.json,020f9a52c4a54560f617aea642a180e96294e9cc,"@@ -1,3 +1,3 @@
-        public String getFile() {
-            return m_filename;
-        }

+    public String getFile() {
+        return m_filename;
+    }",NotBuggy,"Make tagEntry a top level class.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@272453 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a structural change by making `tagEntry` a top-level class, but the diff shows only an indentation change without altering functionality or indicating a bug fix. There's no evidence in the changes that relate to a bug fix. The answer is **NotBuggy**."
ant,9732.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,7 +1,7 @@
-    public Object setProperty(String key, String value)
+	public Object setProperty(final String key, final String value)
         throws NullPointerException {
-        Object obj = super.setProperty(key, value);
+        final Object obj = super.setProperty(key, value);
         // the above call will have failed if key or value are null
         innerSetProperty(key, value);
         return obj;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message ""checkstyle"" suggests changes to improve code style rather than functionality or bug fixes. The diff reflects this: parameters and variables were made `final`, aligning with style rather than altering operation. The original logic is unchanged, and there are no indications of error corrections or bug fixes within this diff. The modifications focus on immutability and likely align with style guidelines, as indicated by the commit message. Therefore, the changes do not indicate a bug-related update.

The conclusion is **NotBuggy**."
ant,632.json,761f82ae3158f6a4b131f0f4f3efff152c72bd95,"@@ -1,3 +1,3 @@
     public void setExecutor(Executor e) {
-        addReference(""ant.executor"", e);
+        addReference(MagicNames.ANT_EXECUTOR_REFERENCE, e);
     }",NotBuggy,"moving some magic names into place, using them where needed.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278417 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the change involves moving ""magic names"" to a centralized location and using them where needed. The code modification reflects this by replacing a string literal (""ant.executor"") with a static reference (MagicNames.ANT_EXECUTOR_REFERENCE). This update enhances maintainability by centralizing configuration values and reducing the possibility of errors due to typos or inconsistencies.

There is no evidence in the commit message or code change that suggests the resolution of a bug. The alteration is more of a refactoring aimed at code organization and maintainability.

Therefore, the answer is **NotBuggy**."
ant,8540.json,a738000acab66d8d7ce5cf563f6dd96d9c7024fb,"@@ -1,3 +1,7 @@
     public Reference getLoader() {
+        if (isReference()) {
+            return ((AbstractClasspathResource) getCheckedRef()).getLoader();
+        }
+        dieOnCircularReference();
         return loader;
     }",NotBuggy,"refid-audit of indirect subclasses of DataType

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@722437 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change introduces a check for references with `isReference()`, which then retrieves a loader from a checked reference instead of the current instance. This suggests the original code potentially mishandled objects when they were references, which could cause incorrect loader retrieval and subsequent errors if misused.

The addition of this logic indicates a correction focusing on ensuring the referenced object loads its own configuration, potentially fixing errors related to misconfiguration or wrong instances being utilized in complex object graphs.

Given these observations, the changes align with the commit message reference to auditing indirect subclasses and addressing issues found. This change primarily addresses logical errors in handling indirect subclass instances represented as `Reference`.

Thus, the changes reflect a bug fix, and I conclude with **Buggy**."
ant,7011.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    protected void addExtdirsToClasspath(Path classpath) {
+	protected void addExtdirsToClasspath(final Path classpath) {
         classpath.addExtdirs(extdirs);
     }",NotBuggy,checkstyle,NotBuggy,"The diff shows a minor change in the code: the method signature's access modifier is changed from `protected` to include a `final` keyword for the parameter and alignment adjustments. The commit message ""checkstyle"" suggests this change is related to code style or formatting adherence rather than fixing a bug. There are no indications of bug fixes such as logic corrections, error handling improvements, or exception handling updates.

The alterations made here, modifying the parameter to be final and adjusting indentation, do not imply any logical or functional change that would address a bug. It's consistent with typical checkstyle adjustments meant for improving code consistency, readability, or enforcing coding standards.

Therefore, the code change does not align with a bug fix as per the commit message, concluding with **NotBuggy**."
ant,9308.json,048caece4061746150290b0264128118a4be3870,"@@ -1,30 +1,4 @@
     protected String getElementName(Project project, Object element)
     {
-        Hashtable elements = project.getTaskDefinitions();
-        String typeName = ""task"";
-        if (!elements.contains( element.getClass() ))
-        {
-            elements = project.getDataTypeDefinitions();
-            typeName = ""data type"";
-            if (!elements.contains( element.getClass() ))
-            {
-                elements = null;
-            }
-        }
-
-        if (elements != null)
-        {
-            Enumeration e = elements.keys();
-            while (e.hasMoreElements())
-            {
-                String elementName = (String) e.nextElement();
-                Class elementClass = (Class) elements.get( elementName );
-                if ( element.getClass().equals( elementClass ) )
-                {
-                    return ""The <"" + elementName + ""> "" + typeName;
-                }
-            }
-        }
-
-        return ""Class "" + element.getClass().getName();
+        return project.getElementName(element);
     }",NotBuggy,"move method refactoring - applied in a way that keeps
IntrospectionHelper's API backwards compatible.

Inspired by:	Jon Skeet <jon.skeet@peramon.com>


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@271451 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the change is a move method refactoring intended to maintain backward compatibility in `IntrospectionHelper`. The code changes show that the implementation of `getElementName(Project project, Object element)` has been replaced by a call to a presumably equivalent method, `project.getElementName(element)`.

Here's the breakdown:

1. **Refactoring Indicator**: The change replaces the method's logic with a single call to another method, suggesting that the functionality has been moved to another location, in this case, `Project`. This is typical of a ""move method"" refactoring pattern.

2. **Backward Compatibility**: The old functionality is retained but rerouted through `Project`. This ensures that code using `IntrospectionHelper` is still operational, aligning with the commit message's emphasis on maintaining backward compatibility.

3. **No Indication of Bug Fix**: There is no indication that this diff addresses any logical errors, exceptions, or unintended behavior in the previous implementation of `getElementName`. The change is strictly structural, focusing on moving functionality to a more appropriate location.

Based on this analysis, the diff aligns with the commit message as a structural refactoring focused on code organization and maintaining backward compatibility, rather than on correcting a bug. Therefore, the appropriate conclusion is **NotBuggy**."
ant,3462.json,789422e131b6c2c003d94f394169a64297e986c6,"@@ -1,3 +1,3 @@
-    public void setTimeout(long timeout) {
+    public void setTimeout(final long timeout) {
         maxwait = timeout;
     }",NotBuggy,oops - make it compile under Java5 again,NotBuggy,"The commit message indicates a change to ensure compatibility with Java5, typically involving the removal of syntax or features not supported in that version. The diff shows that the change is adding the `final` modifier to the `timeout` parameter in the `setTimeout` method. Java5 supports the use of the `final` keyword for method parameters, and this does not look like a change necessary for compilation under Java5. This looks more like a minor refactor for code safety or style, rather than a fix related to Java5 compatibility, a bug fix, or handling an error.

Therefore, the change described in the diff does not match the commit message or indicate a bug fix. The introduction of `final` doesn't solve any bug or syntactic issue related to Java5. 

Thus, the assessment is **NotBuggy**."
ant,7372.json,ff9b546d81da21c253e71319dc8a756380bac15c,"@@ -1,3 +1,3 @@
     public void setString(String str) {
-        str_text = str;
+        strText = str;
     }",NotBuggy,"checkstyle - mostly removal of _


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278353 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message conveys that the change mainly involves modifying code style, specifically by removing underscores from variable names. The provided code diff aligns with this intention, as it shows the replacement of a variable named `str_text` with `strText`, indicating a shift from an underscore-separated styling to camel case.

There is no indication here that this change addresses a bug. It's purely a stylistic change, as evidenced by both the commit message mentioning ""checkstyle"" and the actual code change presented.

Following this analysis, the changes do not signify a bug fix. Therefore, the appropriate conclusion is **NotBuggy**."
ant,8584.json,f67efecf7b71abb13cde1c89fcb56e4e8b7c5c9e,"@@ -1,4 +1,4 @@
-    public void setBase(File b) {
+    public void setBaseDir(File b) {
         checkAttributesAllowed();
-        base = b;
+        baseDir = b;
     }",NotBuggy,"Renamed <file> resource's ""base"" attribute to ""basedir"",
for consistency / familiarity.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@278388 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message refers to a change in naming from ""base"" to ""basedir"" for consistency and familiarity. The provided git diff reflects this change in a method's name and the associated variable name from `base` to `baseDir`. There is no indication of a bug fix here; instead, this appears to be a refactoring for naming consistency.

No error handling, logical corrections, or exception-handling improvements are evident in this diff that suggest a bug is being fixed. The update is purely a renaming effort to improve code readability or maintainability by adhering to more familiar or consistent naming conventions.

Therefore, this commit does not align with a bug fix, hence the conclusion is **NotBuggy**."
ant,7270.json,9caeed5081071d6a2de28844d0a724918f8c6d86,"@@ -1,18 +1,18 @@
-    protected synchronized void dieOnCircularReference(Stack stk, Project p)
+    protected synchronized void dieOnCircularReference(Stack<Object> stk, Project p)
         throws BuildException {
         if (isChecked()) {
             return;
         }
         if (isReference()) {
             super.dieOnCircularReference(stk, p);
         } else {
             if (mapper != null) {
                 pushAndInvokeCircularReferenceCheck(mapper, stk, p);
             }
-            for (Iterator i = setRefs.iterator(); i.hasNext(); ) {
-                pushAndInvokeCircularReferenceCheck((PropertySet) i.next(), stk,
+            for (PropertySet propertySet : setRefs) {
+                pushAndInvokeCircularReferenceCheck(propertySet, stk,
                                                     p);
             }
             setChecked(true);
         }
     }",NotBuggy,"Java 1.5, continued

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@1376015 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Java 1.5, continued"" suggests a continuation of updating code to conform to Java 1.5 standards. Analyzing the diff:

1. The method `dieOnCircularReference` has been updated:
   - The `Stack` parameter is now typed with generics, changing from `Stack` to `Stack<Object>`.
   - The `for` loop has been updated from using an `Iterator` to an enhanced `for-each` loop, improving readability and modernizing syntax.

These changes are part of modernizing the code to use Java 1.5 features such as generics and enhanced for-loops. They help with type safety and concise syntax but do not address any specific bugs related to functionality.

Since the commit message and changes align with enhancing or modernizing the codebase rather than fixing a functional bug, the conclusion is **NotBuggy**."
ant,9623.json,ffea0a9f7459b7c64148ae244b22e847bad9128b,"@@ -1,3 +1,3 @@
     public String getPattern() {
-        return pattern.pattern();
+        return pattern;
     }",NotBuggy,"Rewritten part of the regexp stuff, adding substitution and options.
New optional replaceregexp tasks that takes advantage of it.

Submitted by:	Matthew Inger <matti@sedonacorp.com>

This doesn't really work (but doesn't break anything either), but I
wanted to get the stuff committed now, I will put work into it
tomorrow.  See mail to the list for details.


git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@269820 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message suggests that there is ongoing work on regular expression functionality, mentioning the addition of substitution and options. However, it also admits that this is not yet fully operational (""This doesn't really work"").

The diff shows a change in the `getPattern` method. Previously, it used `pattern.pattern()` to obtain the string representation of a pattern. The update simplifies this by just returning `pattern`. Given the context around regular expression changes, this suggests that a transition from using the `Pattern` class to a simpler string-based approach might be underway.

The commit message indicates that this change doesn't cause malfunction but also doesn't resolve any existing issues yet. The phrase ""adding substitution and options"" implies enhancements, not specific bug corrections. Additionally, the fact it ""doesn't really work"" suggests that this commit is more about laying groundwork than fixing something broken.

Given all this, even though the change relates to regular expressions (often a source of bugs), the message and diff do not explicitly describe or implement a bug fix, but rather an intermediate step in ongoing work.

The conclusion is **NotBuggy**."
ant,7050.json,1b76f1b6dae88a7a158e50222bb19f6f5ca26d70,"@@ -1,3 +1,3 @@
-    public void setAddproperty (String addproperty) {
+    public void setAddproperty (final String addproperty) {
         this.addproperty = addproperty;
     }",NotBuggy,checkstyle,NotBuggy,"The commit message indicates a change related to ""checkstyle,"" which typically involves enforcing coding standards and consistency, such as modifying variable visibility, adding final modifiers, correcting formatting, etc. These adjustments are generally for code quality and maintainability, rather than addressing any functional bugs or errors. 

In the provided diff, the only change is the addition of the `final` keyword to a method parameter. This kind of change does not alter the functionality or fix a bug; it merely enforces immutability for the parameter within the method, aligning with certain coding style guidelines that might be recommended or enforced via checkstyle configurations.

Given this information, there is no indication of a bug being fixed here. Therefore, the appropriate conclusion is **NotBuggy**."
ant,1185.json,dccbf1fcec0fc4e0812e8906494b15a1301ac32a,"@@ -1,7 +1,3 @@
     public boolean hasMoreTokens() {
-        if (lookahead != null) {
-            return true;
-        }
-
-        return tokenizer.hasMoreTokens();
+        return lookahead != null || tokenizer.hasMoreTokens();
     }",NotBuggy,"Checkstyle and simplify, next iteration",NotBuggy,"The commit message suggests stylistic changes and simplification, without mentioning a bug fix. The code refactor combines conditionals with a logical OR, maintaining behavior while making it concise. There are no indications of error fixes or logic corrections. The answer is **NotBuggy**."
ant,1488.json,70f35e75416468e31f537f33b248caaae29ee587,"@@ -1,19 +1,19 @@
-    private static String removePackageName(String line, List packageNames) {
+    private static String removePackageName(String line,
+                                            String[] packagePrefixes,
+                                            int[] prefixLengths) {
+        if (line.length() < FILE_STRING_LENGTH) {
+            return null;
+        }
         boolean matched = false;
-        for (Iterator iter = packageNames.iterator(); iter.hasNext(); ) {
-            String toBeRemoved = FILE_STRING + iter.next() + ""/"";
-            int len = toBeRemoved.length();
-            if (line.length() > len) {
-                if (line.startsWith(toBeRemoved)) {
-                    matched = true;
-                    line = line.substring(len);
-                    break;
-                }
+        for (int i = 0; i < packagePrefixes.length; i++) {
+            if (line.startsWith(packagePrefixes[i])) {
+                matched = true;
+                line = line.substring(prefixLengths[i]);
+                break;
             }
         }
-        if (!matched && line.length() > FILE_STRING.length()) {
-            line = line.substring(FILE_STRING.length());
-            matched = true;
+        if (!matched) {
+            line = line.substring(FILE_STRING_LENGTH);
         }
-        return !matched ? null : line;
+        return line;
     }",NotBuggy,"optimize handling of package names

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@705452 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an optimization in handling package names. Analyzing the diff:

1. **Parameter Change**: The method now accepts arrays `String[] packagePrefixes` and `int[] prefixLengths` instead of a `List packageNames`. This suggests the caller is expected to construct these arrays directly, which is likely optimized for frequent invocation performance due to reduced object overhead and faster access patterns.

2. **Iteration and Check Change**: The method changes from iterating over an `Iterator` to a simple indexed loop, which is often more performant due to reduced overhead.

3. **String Operations**: Instead of constructing a new string each loop iteration (`FILE_STRING + iter.next() + ""/""`) and checking the line's length against this, it directly operates with existing substrings indicated by `packagePrefixes` and `prefixLengths`. This reduces repeat reference to FILE_STRING within the loop, cutting redundant operations.

4. **Logic Simplification**: The original code checked whether the line length was greater than the `toBeRemoved` length every iteration. The refactored code removes this redundant check.

5. **Return Change**: If no match is found, `line` is trimmed by `FILE_STRING_LENGTH` unconditionally, suggesting the method always intends to remove the file string portion when no specific prefix matches. 

These changes focus on performance optimization, specifically by reducing object creation and using static arrays instead of dynamic lists and iterators. There's no indication of a bug being directly fixed; rather, it's about optimization.

Thus, based on the commit message and the nature of code changes, I conclude the analysis with **NotBuggy**."
ant,393.json,8741512d8c2e4cad30f0c6e6134d9a793f00d4b7,"@@ -1,3 +1,3 @@
     public void setName(String name) {
-        this.name = new StringBuffer(normalizeFileName(name));
+        this.name = new StringBuffer(normalizeFileName(name, false));
     }",NotBuggy,"Make preserveLeadingSlashes in tar work again, was broken by revision 755231

git-svn-id: https://svn.apache.org/repos/asf/ant/core/trunk@807513 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a bug fix related to the preservation of leading slashes in tar operations. The code change modifies the `setName` method to call `normalizeFileName` with an additional boolean argument set to `false`. This likely adjusts how file names are normalized, possibly affecting whether leading slashes are preserved.

Here's a step-by-step breakdown:

1. **Functionality Context**: The `setName` method sets a file name using a normalized version of the input string.

2. **Code Change**: The change adds a boolean parameter `false` to the `normalizeFileName` method call.

3. **Potential Impact**: Without examining the implementation of `normalizeFileName`, it is reasonable to deduce from the commit message that this additional parameter directly influences how leading slashes are managed in file names. The previous implementation (potentially modified by revision 755231) does not preserve leading slashes, and the change aims to correct this behavior.

4. **Alignment with Commit Message**: The commit message explicitly mentions fixing a broken feature related to preserving leading slashes. The code change seems to directly address this issue by adjusting normalization logic.

Given this analysis, the change suggests a restoration of intended behavior that was previously broken, aligning with the commit message indicating a bug fix. Therefore, the conclusion is **Buggy**."
lucene-solr,41126.json,e167e9124757b3f3597db8149c49b7f388c48627,"@@ -1,23 +1,22 @@
   static MatchesIterator fromTermsEnum(LeafReaderContext context, int doc, String field, BytesRefIterator terms) throws IOException {
     Objects.requireNonNull(field);
     List<MatchesIterator> mis = new ArrayList<>();
     Terms t = context.reader().terms(field);
     if (t == null)
       return null;
     TermsEnum te = t.iterator();
     PostingsEnum reuse = null;
     for (BytesRef term = terms.next(); term != null; term = terms.next()) {
       if (te.seekExact(term)) {
         PostingsEnum pe = te.postings(reuse, PostingsEnum.OFFSETS);
         if (pe.advance(doc) == doc) {
-          // TODO do we want to use the copied term here, or instead create a label that associates all of the TMIs with a single term?
-          mis.add(new TermMatchesIterator(BytesRef.deepCopyOf(term), pe));
+          mis.add(new TermMatchesIterator(pe));
           reuse = null;
         }
         else {
           reuse = pe;
         }
       }
     }
     return fromSubIterators(mis);
   }",NotBuggy,"LUCENE-8270: Remove MatchesIterator.term()
",NotBuggy,"The commit message indicates that `MatchesIterator.term()` was removed, which reflects a method or attribute related to term handling being eliminated from use. 

In the code diff, the `TermMatchesIterator` constructor no longer takes `BytesRef.deepCopyOf(term)` as an argument. This implies that the `term` attribute or method possibly used in `TermMatchesIterator` has been removed or refactored. The elimination of passing the term to `TermMatchesIterator` aligns with the commit message's intent to remove `MatchesIterator.term()`. 

However, the change does not indicate a correction of faulty logic, error handling improvement, or bug-related functionality adjustment—rather, it appears to be a refactoring or cleanup process.

Thus, the diff aligns with the commit message's intention but doesn't signal a bug fix. The conclusion is **NotBuggy**."
lucene-solr,27518.json,72f99b47b4673d858e3ffd42afdca76e0a5bb860,"@@ -1,3 +1,7 @@
     public int startOffset() {
-      return startOffsets[nextPos-1];
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[nextPos-1];
+      }
     }",NotBuggy,"LUCENE-4230: allow consumer to specify that payloads are not require when pulling D&PEnum, so codec can optimize if possible

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1367186 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The given diff indicates a defensive check added to handle cases where `startOffsets` might be `null`. Originally, the method `startOffset()` blindly accessed an array `startOffsets`. If this array were `null`, attempting to access its elements would have resulted in a `NullPointerException`. By checking if `startOffsets` is `null` and returning `-1` in such cases, the code avoids the exception and handles unexpected input more gracefully.

This change is indicative of a bug fix related to error-handling improvements. The commit itself doesn't explicitly mention a bug fix in this part of the code, focusing more on optimizations related to payload handling. However, the change in the diff directly addresses a potential runtime error (a bug related to null handling), thus aligning with a bug fix behavior.

Therefore, considering this evidence, the change should be classified as **Buggy**."
lucene-solr,22902.json,aa1d5feba01a50d4bc002e346e54ff26f6abc664,"@@ -1,14 +1,14 @@
-    private void rehash() {
-      long[] oldVals = vals;
-      int newCapacity = vals.length << 1;
-      vals = new long[newCapacity];
-      mask = newCapacity - 1;
-      threshold = (int) (newCapacity * LOAD_FACTOR);
-      cardinality = 0;
+  private void rehash() {
+    long[] oldVals = vals;
+    int newCapacity = vals.length << 1;
+    vals = new long[newCapacity];
+    mask = newCapacity - 1;
+    threshold = (int) (newCapacity * LOAD_FACTOR);
+    cardinality = 0;
 
-      for (long val : oldVals) {
-        if (val != 0) {
-          add(val);
-        }
+    for (long val : oldVals) {
+      if (val != 0) {
+        add(val);
       }
-    }

+    }
+  }",NotBuggy,"SOLR-11093: add Points to GraphQuery
",NotBuggy,"The given commit message refers to adding ""Points to GraphQuery"" related to a JIRA issue (SOLR-11093), which suggests functionality enhancement, not a bug fix. 

The code changes consist of adjusting the indentation without altering the functionality of the `rehash` method. This refactor typically enhances code readability and alignment but does not involve any error-handling, logical corrections, or improvements suggesting a bug fix.

Given the content of the diff and its alignment with the commit message, which implies a focus on new functionality rather than addressing any bugs, the conclusion is **NotBuggy**."
lucene-solr,49335.json,831c0648f992faea7024e36167749cd210cddc14,"@@ -1,6 +1,6 @@
   public long get(int index) {
     final int o = blockOffset(index);
     final int b = offsetInBlock(index);
 
-    return (blocks[o] >> shifts[b]) & readMask;
+    return (blocks[o] >>> shifts[b]) & readMask;
   }",NotBuggy,"LUCENE-4062: PackedInts improvements.

 * Direct64 now uses System.arraycopy for its bulk operations,
 * Packed64SingleBlock unnecessary optimizations have been removed.


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1351682 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The change in the diff involves swapping the right shift operator (>>) with the unsigned right shift operator (>>>). This modification is crucial because when dealing with bit manipulation and long integer arrays, using >>> ensures that zero-fill is used from the left, which can correct potential errors when handling large negative numbers. Such a change can prevent incorrect retrieval of values due to sign extension errors inherent in using >>> over >> for non-negative logic operations on binary data.

The commit message suggests various improvements, which typically imply performance optimizations or cleanup. However, this specific change is more aligned with a bug fix concerning proper data handling and retrieval. Though ""improvements"" is part of the commit message and this might be categorized as improving correctness, it strongly addresses handling numeric data accurately.

Thus, the change can indeed be considered a bug fix as it corrects the logic for retrieving shifted data from the blocks array and prevents potential erroneous value interpretation. Therefore, this change is **Buggy**."
lucene-solr,2051.json,f7aa200d406dbd05a35d6116198302d90b92cb29,"@@ -1,3 +1,3 @@
-      public boolean exists(int doc) {
+      public boolean exists(int doc) throws IOException {
         return vals.exists(doc);
       }",NotBuggy,"LUCENE-7407: switch doc values usage to an iterator API, based on DocIdSetIterator, instead of random acces, freeing codecs for future improvements
",NotBuggy,"The commit message indicates a switch to using an iterator API instead of random access for doc values, aligning with changes for future codec improvements. The diff shows modification of the method signature to include `throws IOException`. This suggests that the method could be interacting with components that might throw I/O-related exceptions, which aligns more with handling potential errors due to the new API usage approach. Although this change is preparatory for future improvements and better error handling or API structure, it does not indicate a direct bug being fixed.

Thus, even though it implies better structural design, there is no explicit evidence of a bug fix present in this diff. The conclusion is **NotBuggy**."
lucene-solr,7428.json,ce172acb8fec6c3bbb18837a4d640da6c5aad649,"@@ -1,7 +1,6 @@
   public static void close(HttpClient httpClient) { 
-    if (httpClient instanceof CloseableHttpClient) {
-      org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
-    } else {
-      httpClient.getConnectionManager().shutdown();
-    }
+
+    org.apache.solr.common.util.IOUtils.closeQuietly((CloseableHttpClient) httpClient);
+
+    assert ObjectReleaseTracker.release(httpClient);
   }",NotBuggy,"SOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client.
",NotBuggy,"The commit message references moving to non-deprecated HttpClient classes and eliminating the stale connection check by adjusting connection management. The diff shows the removal of type checks and conditionals specific to CloseableHttpClient, suggesting reliance on newer HttpClient lifecycle methods. The removal of conditional branches implies trust in HttpClient to handle connections, signaling a move towards the client for lifecycle management. An `assert` is added to check if the httpClient is released properly with `ObjectReleaseTracker.release(httpClient)`. While this aligns with modernizing and maintaining code, there's no indication of a bug fix, but more about improving practices and using updated library features. Thus, I conclude **NotBuggy**."
lucene-solr,2821.json,f01b3e97d19c4b96ca9eb7d4e2a8647f68fbb969,"@@ -1,13 +1,16 @@
   public void finishStage(ResponseBuilder rb) {
     if (rb.isAnalytics() && rb.stage == ResponseBuilder.STAGE_GET_FIELDS) {
       AnalyticsRequestManager reqManager = getAnalyticsRequestManager(rb);
       // Generate responses from the merged shard data
       if (rb.isOlapAnalytics()) {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_OLD_HEADER, reqManager.createOldResponse());
       } else {
         rb.rsp.add(AnalyticsResponseHeadings.COMPLETED_HEADER, reqManager.createResponse());
       }
+      if (reqManager.isPartialResults()) {
+        rb.rsp.getResponseHeader().asShallowMap().put(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY,true);
+      }
     }
 
     super.finishStage(rb);
   }",NotBuggy,"SOLR-13904: Make Analytics component sensitive to timeAllowed.
",Buggy,"The commit message states that the Analytics component is being made sensitive to `timeAllowed`. The code change implements a check for partial results in the `AnalyticsRequestManager`. If partial results are detected, it adds an entry indicating partial results to the response header. 

This is likely related to a condition where the analysis operation might not complete in the allowed time, thus resulting in a partial outcome. By flagging the request as having partial results, the component can handle or inform about incomplete execution, aligning perfectly with the goal outlined in the commit message.

This indicates a bug fix addressing the issue of incomplete results due to time constraints, which were not previously flagged as such in the response. Therefore, this change is functionally a bug fix as per the commit message's specified intention.

The conclusion is **Buggy**."
lucene-solr,42269.json,765403b2af47c2aef71dd824f57c1d90d3a21cef,"@@ -1,17 +1,17 @@
-  public Iterable<? extends Accountable> getChildResources() {
+  public Iterable<Accountable> getChildResources() {
     List<Accountable> resources = new ArrayList<>();
     
     long docBaseDeltaBytes = RamUsageEstimator.shallowSizeOf(docBasesDeltas);
     for (PackedInts.Reader r : docBasesDeltas) {
       docBaseDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""doc base deltas"", docBaseDeltaBytes));
     
     long startPointerDeltaBytes = RamUsageEstimator.shallowSizeOf(startPointersDeltas);
     for (PackedInts.Reader r : startPointersDeltas) {
       startPointerDeltaBytes += r.ramBytesUsed();
     }
     resources.add(Accountables.namedAccountable(""start pointer deltas"", startPointerDeltaBytes));
     
-    return resources;
+    return Collections.unmodifiableList(resources);
   }",NotBuggy,"LUCENE-6104: simplify internals of Lucene50NormsProducer

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1645711 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a simplification of the internals of `Lucene50NormsProducer`. The diff shows a couple of changes:

1. The method return type is changed from `Iterable<? extends Accountable>` to `Iterable<Accountable>`. This change simplifies type variance without altering logic—modernizing code style instead of fixing a bug.

2. Returning `Collections.unmodifiableList(resources);`: This change ensures that the list cannot be modified after being returned. While this change can lead to more robust code by preventing accidental modifications, it is more about improving code safety and maintainability rather than fixing an inherent bug in functionality.

There is no clear indication of a bug fix related to incorrect logic or runtime errors within the provided change.

The answer is **NotBuggy**."
lucene-solr,48313.json,93fa72f77bd024aa09eef043c65c64a6524613dc,"@@ -1,3 +1,13 @@
   final int getCharClass(int c) {
-    return Operations.findIndex(c, points);
+
+    // binary search
+    int a = 0;
+    int b = points.length;
+    while (b - a > 1) {
+      int d = (a + b) >>> 1;
+      if (points[d] > c) b = d;
+      else if (points[d] < c) a = d;
+      else return d;
+    }
+    return a;
   }",NotBuggy,"LUCENE-7465: add SimplePatternTokenizer and SimpleSplitPatternTokenizer, for tokenization using Lucene's regexp/automaton implementation
",NotBuggy,"The commit message discusses the addition of SimplePatternTokenizer and SimpleSplitPatternTokenizer, which are new features. The provided diff modifies a method to improve its efficiency by replacing a linear search (`Operations.findIndex`) with a binary search algorithm within an array `points`. 

The change does not directly relate to bug fixing, but rather performance optimization. The message does not indicate any error to be fixed with this specific change; rather, it suggests an addition of new tokenization functionalities. Since the change involves improving the performance rather than fixing an erroneous behavior, this reflects an enhancement.

Thus, based on the diff and the commit message, the conclusion is **NotBuggy**."
lucene-solr,32009.json,af680af77f3f80c779e038a0ad8a136c9dcb9f5d,"@@ -1,3 +1,3 @@
-  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, SimScorer scorer, int flags) throws IOException {
+  public ImpactsEnum impacts(FieldInfo fieldInfo, BlockTermState state, int flags) throws IOException {
     throw new UnsupportedOperationException(""Should never be called, IDVersionSegmentTermsEnum implements impacts directly"");
   }",NotBuggy,"LUCENE-8142: Make postings APIs expose raw impacts rather than scores.
",NotBuggy,"The commit message indicates a change in the API to expose raw impacts rather than scores, which means removing dependencies on scoring components. The diff shows the `impacts` method signature being changed to remove the `SimScorer scorer` parameter. This aligns with the message by focusing on exposing raw data rather than computed scores. However, removing the scorer parameter isn't directly fixing a bug, but aligning the method signature with new requirements.

The change is about altering API behavior rather than correcting an error or a flaw in the logic. There are no direct indications of bug fixes, such as error handling or correcting faulty logic. Thus, this change is more about functionality adjustment than bug resolution.

The answer is **NotBuggy**."
lucene-solr,22923.json,8007ac0cb0c88838ba6e58e56e2bc23374c15dc4,"@@ -1,19 +1,19 @@
   private void markAndSweepByRamSize() {
     assert markAndSweepLock.isHeldByCurrentThread() : ""markAndSweepLock held by another thread"";
     List<CacheEntry<K, V>> entriesInAccessOrder = new ArrayList<>(map.size());
     map.forEach((o, kvCacheEntry) -> {
       kvCacheEntry.lastAccessedCopy = kvCacheEntry.lastAccessed; // important because we want to avoid volatile read during comparisons
       entriesInAccessOrder.add(kvCacheEntry);
     });
 
     Collections.sort(entriesInAccessOrder); // newer access is smaller, older access is bigger
 
     // iterate in oldest to newest order
     for (int i = entriesInAccessOrder.size() - 1; i >= 0; i--) {
       CacheEntry<K, V> kvCacheEntry = entriesInAccessOrder.get(i);
       evictEntry(kvCacheEntry.key);
-      if (ramBytes.get() <= ramLowerWatermark)  {
+      if (ramBytes.sum() <= ramLowerWatermark)  {
         break; // we are done!
       }
     }
   }",NotBuggy,"SOLR-8241: Add CaffeineCache, an efficient implementation of SolrCache.
",NotBuggy,"The commit mentions adding a new cache type, CaffeineCache, to Solr. The code diff shows a modification in the cache eviction method, specifically changing `ramBytes.get()` to `ramBytes.sum()`. This change likely addresses situations where ramBytes.get() did not reflect all contributions to RAM usage, while sum() presumably accounts for a more comprehensive view, ensuring entries are evicted correctly according to the desired RAM limits.

However, since the commit message focuses on adding a new cache implementation rather than fixing an issue, the relationship between the code change and the commit message is indirect. The change implies a possible pre-existing problem in tracking RAM usage accurately, but since the primary focus of the commit message is on adding functionality rather than directly addressing a bug, the evidence of it being a bug fix is insufficient.

The answer is **NotBuggy**."
lucene-solr,16275.json,f177a660f5745350207dc61b46396b49404fd383,"@@ -1,29 +1,30 @@
   public ZkWriteCommand updateShardState(ClusterState clusterState, ZkNodeProps message) {
     String collectionName = message.getStr(ZkStateReader.COLLECTION_PROP);
     if (!checkCollectionKeyExistence(message)) return ZkStateWriter.NO_OP;
     log.info(""Update shard state invoked for collection: "" + collectionName + "" with message: "" + message);
 
     DocCollection collection = clusterState.getCollection(collectionName);
     Map<String, Slice> slicesCopy = new LinkedHashMap<>(collection.getSlicesMap());
     for (String key : message.keySet()) {
       if (ZkStateReader.COLLECTION_PROP.equals(key)) continue;
       if (Overseer.QUEUE_OPERATION.equals(key)) continue;
 
       Slice slice = collection.getSlice(key);
       if (slice == null) {
         throw new RuntimeException(""Overseer.updateShardState unknown collection: "" + collectionName + "" slice: "" + key);
       }
       log.info(""Update shard state "" + key + "" to "" + message.getStr(key));
       Map<String, Object> props = slice.shallowCopy();
       
-      if (Slice.State.getState((String) props.get(ZkStateReader.STATE_PROP)) == Slice.State.RECOVERY
-          && Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
+      if (Slice.State.getState(message.getStr(key)) == Slice.State.ACTIVE) {
         props.remove(Slice.PARENT);
+        props.remove(""shard_parent_node"");
+        props.remove(""shard_parent_zk_session"");
       }
       props.put(ZkStateReader.STATE_PROP, message.getStr(key));
       Slice newSlice = new Slice(slice.getName(), slice.getReplicasCopy(), props);
       slicesCopy.put(slice.getName(), newSlice);
     }
 
     return new ZkWriteCommand(collectionName, collection.copyWithSlices(slicesCopy));
   }",NotBuggy,"SOLR-9438: Shard split can be marked successful and sub-shard states switched to 'active' even when one or more sub-shards replicas do not recover due to the leader crashing or restarting between the time the replicas are created and before they can recover
",Buggy,"The commit message indicates a bug fix related to SOLR-9438, where a shard split can be erroneously marked as successful even if sub-shard replicas fail to recover due to a leader crash or restart.

Upon examining the code diff:

1. **Context**: The method `updateShardState` is responsible for updating the state of shards within a Solr cluster. It fetches the collection state, iterates over shards, and updates their states based on a provided message.

2. **Code Change**:
   - The original condition checked if the current state was `RECOVERY` and the new state was `ACTIVE`. 
   - The modified condition now only checks if the new state is `ACTIVE`.
   - Several properties related to the shard's parent (`Slice.PARENT`, `shard_parent_node`, `shard_parent_zk_session`) are removed when transitioning a shard to the `ACTIVE` state.

3. **Implication**:
   - The code seems to rectify an issue where a shard could potentially skip necessary checks or transitions if it was transitioning from a specific state (`RECOVERY`). By simplifying the condition and ensuring that certain parent-tracking properties are removed when marking a shard as `ACTIVE`, it is likely addressing a state transition bug.
   - Removing these properties is critical as they relate to the tracking of shard lineage or session details, which should not persist once the shard becomes fully `ACTIVE`.

4. **Relation to Commit Message**: 
   - The code aligns with the commit message's objective: ensuring that shard state transitions to `ACTIVE` do not occur under inappropriate conditions that may arise due to leader issues (like a crash or restart).

Given these observations, it is clear that the changes are correcting behavior related to shard state transitions, and the modifications align with the description in the commit message. Therefore, this change is indeed a bug fix.

**Buggy**"
lucene-solr,27550.json,54f256503867c4265a994b94af2917e1375c2181,"@@ -1,14 +1,14 @@
   protected int split(byte[] minPackedValue, byte[] maxPackedValue) {
     // Find which dim has the largest span so we can split on it:
     int splitDim = -1;
     for(int dim=0;dim<numDims;dim++) {
       NumericUtils.subtract(bytesPerDim, dim, maxPackedValue, minPackedValue, scratchDiff);
-      if (splitDim == -1 || StringHelper.compare(bytesPerDim, scratchDiff, 0, scratch1, 0) > 0) {
+      if (splitDim == -1 || FutureArrays.compareUnsigned(scratchDiff, 0, bytesPerDim, scratch1, 0, bytesPerDim) > 0) {
         System.arraycopy(scratchDiff, 0, scratch1, 0, bytesPerDim);
         splitDim = dim;
       }
     }
 
     //System.out.println(""SPLIT: "" + splitDim);
     return splitDim;
   }",NotBuggy,"LUCENE-8469: Inline calls to the deprecated StringHelper.compare, removed StringHelper.compare from master.
",NotBuggy,"The commit message indicates that the deprecated method `StringHelper.compare` was removed and replaced with a more current method, `FutureArrays.compareUnsigned`. Although this change could impact system behavior—since `FutureArrays.compareUnsigned` is more reliable for unsigned lexicographic byte comparison—it does not explicitly point to fixing a bug but rather to updating the codebase to eliminate deprecated methods and potentially future-proof the code.

Therefore, while the change may lead to more consistent behavior, it is primarily a refactoring effort to remove deprecated calls. The commit does not explicitly account for fixing a buggy situation, except in the broader scope of aligning with updated practices.

Hence, the conclusion is **NotBuggy**."
lucene-solr,47988.json,f1fda6880b784c8554439efcce4e6d464d97763b,"@@ -1,56 +1,54 @@
-  public static void decompress(DataInput compressed, int decompressedLen, BytesRef destBytes) throws IOException {
-    final byte[] dest = destBytes.bytes;
+  public static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {
     final int destEnd = dest.length;
-    int dOff = 0;
 
-    while (dOff < decompressedLen) {
+    do {
       // literals
       final int token = compressed.readByte() & 0xFF;
       int literalLen = token >>> 4;
 
       if (literalLen != 0) {
         if (literalLen == 0x0F) {
           byte len;
           while ((len = compressed.readByte()) == (byte) 0xFF) {
             literalLen += 0xFF;
           }
           literalLen += len & 0xFF;
         }
         compressed.readBytes(dest, dOff, literalLen);
         dOff += literalLen;
       }
 
       if (dOff >= decompressedLen) {
         break;
       }
 
       // matchs
       final int matchDec = (compressed.readByte() & 0xFF) | ((compressed.readByte() & 0xFF) << 8);
       assert matchDec > 0;
 
       int matchLen = token & 0x0F;
       if (matchLen == 0x0F) {
         int len;
         while ((len = compressed.readByte()) == (byte) 0xFF) {
           matchLen += 0xFF;
         }
         matchLen += len & 0xFF;
       }
       matchLen += MIN_MATCH;
 
       // copying a multiple of 8 bytes can make decompression from 5% to 10% faster
       final int fastLen = ((matchLen - 1) & 0xFFFFFFF8) + 8;
       if (matchDec < matchLen || dOff + fastLen > destEnd) {
         // overlap -> naive incremental copy
         for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {
           dest[dOff] = dest[ref];
         }
       } else {
         // no overlap -> arraycopy
         System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);
         dOff += matchLen;
       }
-    }
-    destBytes.offset = 0;
-    destBytes.length = dOff;
+    } while (dOff < decompressedLen);
+
+    return dOff;
   }",NotBuggy,"LUCENE-4554: CompressingStoredFieldsFormat: don't write the original length at the beginning of the chunk with LZ4 compressors


git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1408476 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message indicates a change related to not writing the original length at the beginning of a chunk with LZ4 compressors, but the provided code diff doesn't explicitly show that. Instead, there are several changes to the decompress method. Let's assess these changes:

1. **Parameter Changes**: The method signature changes from using `BytesRef` to directly dealing with a `byte[]` and an offset `dOff`. The new version returns `dOff`.

2. **While Loop to Do-While Loop**: The loop structure has changed from a `while` loop to a `do-while` loop. This ensures that the loop runs at least once, potentially indicating a need to process at least some input data - likely for cases where the check condition might previously skip necessary operations on initial data.

3. **Return Value**: The method now returns the final offset `dOff`, which may be used for further operations outside this method, enhancing utility and clarity about where writing ends.

4. **Removal of `BytesRef` Handling**: The approach directly manipulates the `byte[]` from the start, removing the need to set offsets on `BytesRef`, which simplifies handling within the method. This change may reduce overhead with `BytesRef` structures and align with the performance-related part of the commit message.

5. **Break Condition Changed**: Exiting the loop now relies on the `do-while` condition, although the internal `break` for when `dOff` exceeds or equals `decompressedLen` remains. The adjustment helps to ensure at least one iteration occurs as intended.

The commit message suggests improvements in functionality (specifically not writing lengths) and performance. The code reflects a restructuring potentially aligned with performance improvements, while the focus on the removed writing action isn’t explicitly represented in the snippet. However, these changes could easily align with indirectly preventing unnecessary length management by handling decompression differently or based on different processing flow.

Based on these observations, and considering the context of a performance improvement and possibly a structural change preventing unwanted operations pertaining to data length handling, it seems the changes are indeed addressing a functional issue within the context described. Albeit indirectly visible in this snippet, there are implications of meaningful changes.

Thus, the answer is **Buggy** since the broader context indicates addressing structural alignment with LZ4 that might involve bugs or inefficiencies."
lucene-solr,4912.json,23c73d8ff85f9b6eea30e2c03930d9e8ba116da0,"@@ -1,13 +1,10 @@
-  private void loadParams(String params, Properties props) throws SQLException {
-    try {
-      String[] pairs = params.split(""&"");
-      for (String pair : pairs) {
-        String[] keyValue = pair.split(""="");
-        String key = URLDecoder.decode(keyValue[0], ""UTF-8"");
-        String value = URLDecoder.decode(keyValue[1], ""UTF-8"");
-        props.put(key, value);
+  private void loadParams(URI uri, Properties props) throws SQLException {
+    List<NameValuePair> parsedParams = URLEncodedUtils.parse(uri, ""UTF-8"");
+    for (NameValuePair pair : parsedParams) {
+      if (pair.getValue() != null) {
+        props.put(pair.getName(), pair.getValue());
+      } else {
+        props.put(pair.getName(), """");
       }
-    } catch(Exception e) {
-      throw new SQLException(e);
     }
   }",NotBuggy,"SOLR-8179: SQL JDBC - DriverImpl loadParams doesn't support keys with no values in the connection string

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1716198 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message states that the update addresses issue SOLR-8179 by modifying the `DriverImpl loadParams` function to handle connection string keys without values. The original code split parameters using `split(""&"")` and `split(""="")`, which would fail if a parameter had no value (resulting in an `ArrayIndexOutOfBoundsException`).

The revised code uses `URLEncodedUtils.parse` to handle URI parameters. It distinguishes between keys with values and those without, inserting empty strings for keys lacking values. This change directly tackles the problem outlined in the commit message, thus aligning with a bug fix.

Overall, the modifications improve the method's robustness by handling edge scenarios more gracefully, indicative of resolving a bug.

The conclusion is **Buggy**."
jna,3166.json,81908ed5b40ce3cdab32aabd29db56e2d0611e1f,"@@ -1,18 +1,18 @@
         public void fromXModifierKeymap(X11.XModifierKeymapRef xModifierKeymapRef) {
             int count = xModifierKeymapRef.max_keypermod;
             byte[] keys = xModifierKeymapRef.modifiermap.getByteArray(0, 8*count);
 
             ArrayList<Byte>[] allModifiers = getAllModifiers();
 
             for (int modNr = 0; modNr < 8; modNr++) {
                 ArrayList<Byte> modifier = allModifiers[modNr];
                 modifier.clear();
 
                 for (int keyNr = 0; keyNr < count; keyNr++) {
                     byte key = keys[modNr*count + keyNr];
                     if (key != 0) {
-                        modifier.add(new Byte(key));
+                        modifier.add(Byte.valueOf(key));
                     }
                 }
             }
         }",NotBuggy,"Apply generic definitions wherever applicable
",NotBuggy,"The commit message indicates a general update to apply generic definitions, but the diff shows a specific change in how a `Byte` object is created: switching from `new Byte(key)` to `Byte.valueOf(key)`. This change is actually about optimizing object creation by utilizing a method that potentially uses a cache for frequently-used values, which could improve performance and reduce unnecessary object creation overhead. 

However, this change is not a bug fix but rather an improvement or a refactor to use best practices in Java. It does not directly align with the commit message, which speaks to applying generics; rather, it is aimed at optimizing code in terms of performance and memory use.

Thus, this particular change is not indicative of a bug fix. The correct conclusion here is **NotBuggy**."
jna,2910.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,9 +1,7 @@
     public static void lSetXAttr(String path, String name, byte[] value) throws IOException {
-        Memory valueMem = bytesToMemory(value);
-        final int retval = XAttr.INSTANCE.lsetxattr(path, name, valueMem,
-            new size_t(valueMem.size()), 0);
+        final int retval = XAttr.INSTANCE.lsetxattr(path, name, value, new size_t(value.length), 0);
         if (retval != 0) {
             final int eno = Native.getLastError();
             throw new IOException(""errno: "" + eno);
         }
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",NotBuggy,"The commit message indicates that byte[] based bindings were added to the method `lSetXAttr`. In the diff, we observe that the `lSetXAttr` method's implementation has been simplified. The original implementation used a helper method `bytesToMemory(value)` to convert the byte array into a `Memory` object, which was then passed to the `lsetxattr` function. The updated code removes this conversion, directly passing the byte array `value` to the `lsetxattr` function.

Despite the change aligning with the commit message, there's no indication of a bug fix in the context of this operation. The change simplifies the function call, but nothing suggests a prior error or logic issue being corrected—rather, it's an efficiency or API usability improvement.

Therefore, the changes appear to be oriented towards a binding update or enhancement rather than a bug fix. Consequently, I conclude with **NotBuggy**."
jna,1326.json,3bba92c542dbb2ccd4daf6120ae914a43c297294,"@@ -1,3 +1,4 @@
     public static boolean IsWindowsXPSP3OrGreater() {
-        return IsWindowsVersionOrGreater(HIBYTE(WinNT.WIN32_WINNT_WINXP), LOBYTE(WinNT.WIN32_WINNT_WINXP), 3);
+        return IsWindowsVersionOrGreater((byte) (Kernel32.WIN32_WINNT_WINXP >>> 8), (byte) Kernel32.WIN32_WINNT_WINXP,
+                3);
     }",NotBuggy,"Javadocs, better typecasting",NotBuggy,"The commit message suggests improvements in documentation and typecasting. The code change alters how values are extracted from a Windows version constant, using shifts and typecasting instead of functions like `HIBYTE` and `LOBYTE`. This adjustment is considered a refactor for better code clarity or consistency, but it doesn’t expressly address a bug.

The changes involve better typecasting, aligning with the commit message. However, the conversion in the code does not appear to fix an error or bug; rather, it optimizes or clarifies how the same result is achieved in a potentially cleaner or more efficient manner.

Since the update does not address a clear bug, it aligns well with the ""better typecasting"" part of the commit message instead.

Consequently, the appropriate conclusion for these changes is **NotBuggy**."
jna,3850.json,d395318a8ba8744fb61805cde5644863daaf5f94,"@@ -1,3 +1,3 @@
     public String[] getWideStringArray(long offset, int length) {
-        return getStringArray(offset, -1, NativeString.WIDE_STRING);
+        return getStringArray(offset, length, NativeString.WIDE_STRING);
     }",NotBuggy,"Pointer.getWideStringArray respects the length parameter
",Buggy,"The commit message states that the `getWideStringArray` method should respect the length parameter. In the original code, the method called `getStringArray` with a static length of `-1`, which likely ignored the actual `length` argument. The modification changes this to use the provided `length` parameter, ensuring that the function respects the intended behavior described in the commit message.

This change clearly corrects a logic issue where the `length` argument was not utilized as expected, likely resulting in incorrect behavior when this method was called with a specific length. The change is aligned with the commit message, indicating it fixes this bug.

The conclusion is **Buggy**."
jna,324.json,e4a6950d98b96f258fa31576ae9fc606091e66b6,"@@ -1,38 +1,41 @@
-	public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
-		IntByReference pcbNeeded = new IntByReference();
-		IntByReference pcReturned = new IntByReference();
-		HANDLEByReference pHandle = new HANDLEByReference();
+    public static PRINTER_INFO_2 getPrinterInfo2(String printerName) {
+        IntByReference pcbNeeded = new IntByReference();
+        IntByReference pcReturned = new IntByReference();
+        HANDLEByReference pHandle = new HANDLEByReference();
 
-		if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null))
-			throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        if (!Winspool.INSTANCE.OpenPrinter(printerName, pHandle, null)) {
+            throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+        }
 
-		Win32Exception we = null;
-		PRINTER_INFO_2 pinfo2 = null;
+        Win32Exception we = null;
+        PRINTER_INFO_2 pinfo2 = null;
 
-		try {
-			Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
-			if (pcbNeeded.getValue() <= 0)
-				return new PRINTER_INFO_2();
+        try {
+            Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, null, 0, pcbNeeded);
+            if (pcbNeeded.getValue() <= 0) {
+                return new PRINTER_INFO_2();
+            }
 
-			pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
-			if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned))
-				throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            pinfo2 = new PRINTER_INFO_2(pcbNeeded.getValue());
+            if (!Winspool.INSTANCE.GetPrinter(pHandle.getValue(), 2, pinfo2.getPointer(), pcbNeeded.getValue(), pcReturned)) {
+                throw new Win32Exception(Kernel32.INSTANCE.GetLastError());
+            }
 
-			pinfo2.read();
-		} catch (Win32Exception e) {
-			we = e;
-		} finally {
-			if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
-				Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
-				if (we != null) {
-					ex.addSuppressedReflected(we);
-				}
-			}
-		}
+            pinfo2.read();
+        } catch (Win32Exception e) {
+            we = e;
+        } finally {
+            if (!Winspool.INSTANCE.ClosePrinter(pHandle.getValue())) {
+                Win32Exception ex = new Win32Exception(Kernel32.INSTANCE.GetLastError());
+                if (we != null) {
+                    ex.addSuppressedReflected(we);
+                }
+            }
+        }
 
-		if (we != null) {
-			throw we;
-		}
+        if (we != null) {
+            throw we;
+        }
 
-		return pinfo2;
-	}

+        return pinfo2;
+    }",NotBuggy,"Checkstyle FileTabCharacter, Trailing Whitespace, line endings to \n, indention
",NotBuggy,"The commit message indicates stylistic changes such as removing tabs, fixing trailing whitespace, normalizing line endings to `\n`, and adjusting indentation. The diff reflects these changes, replacing tabs with spaces, adding braces around single statements to follow a consistent coding style, and adjusting the indentation to enhance readability.

There are no logical corrections, exception-handling improvements, or other signs of bug-related changes in the actual code functionality. Instead, this is a refactor for style consistency and better adherence to code formatting standards.

Since the modifications were strictly formatting changes without impacting the logic or handling of the code, the changes do not indicate a bug fix.

Therefore, the correct conclusion is **NotBuggy**."
jna,3493.json,6705513b86030fd7d609d8e03539527d2a553651,"@@ -1,6 +1,9 @@
         protected void finalize() {
-            for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
-                Map.Entry e = (Map.Entry)i.next();
-                unregister((Class)e.getKey(), (long[])e.getValue());
+            synchronized(registeredClasses) {
+                for (Iterator i=registeredClasses.entrySet().iterator();i.hasNext();) {
+                    Map.Entry e = (Map.Entry)i.next();
+                    unregister((Class)e.getKey(), (long[])e.getValue());
+                    i.remove();
+                }
             }
         }",NotBuggy,"add structure, string, NIO Buffer and primitive array support to direct type mapping

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@841 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",NotBuggy,"The commit message discusses adding support for different data types to direct type mapping, which is unrelated to the code diff presented. The diff shows changes to the finalizer method, adding synchronization with `registeredClasses` to ensure thread safety and removing items from the iterator explicitly during traversal. This change is more about improving the safety and integrity of the registry process, ensuring no concurrent modification exceptions occur during iteration and removal.

The commit message's description does not align with the changes shown, as it refers to type mapping extensions rather than concurrency and resource management updates. 

The synchronization and iterator removal may address concurrent execution issues (potential bugs), but the lack of a direct link in the message to the modification means the improvement is related to robustness rather than a specific bug fix. 

Therefore, I will conclude that the answer is **NotBuggy**."
jna,3649.json,8e6cbda11092f95b4e00a0f650dfd3de52ac5bb8,"@@ -1,3 +1,3 @@
-        static boolean isBuffer(Class cls) {
+        static boolean isBuffer(Class<?> cls) {
             return Buffer.class.isAssignableFrom(cls);
         }",NotBuggy,"Use generic definition of Native#loadLibrary
",NotBuggy,"The commit message indicates a change to use a generic definition. The diff modifies the method signature isBuffer from `Class cls` to `Class<?> cls`, introducing generics. While this change improves type safety and addresses potential raw-type warnings, it doesn't suggest a bug fix related to functionality or logic errors. The message aligns with the changes, which focus on code improvement rather than correcting a bug.

Thus, this change is a refactoring for code quality and not a bug fix. The conclusion is **NotBuggy**."
jna,2373.json,596022a4f955a5decfd41e6c709a90a4680184f9,"@@ -1,43 +1,57 @@
 	public Object invoke(final Object proxy, final java.lang.reflect.Method method, final Object[] args)
 			throws Throwable {
                 boolean declaredAsInterface = 
                         (method.getAnnotation(ComMethod.class) != null)
                         ||(method.getAnnotation(ComProperty.class) != null);
             
 		if ((! declaredAsInterface) && (method.getDeclaringClass().equals(Object.class)
                         || method.getDeclaringClass().equals(IRawDispatchHandle.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IUnknown.class)
                         || method.getDeclaringClass().equals(com.sun.jna.platform.win32.COM.util.IDispatch.class)
                         || method.getDeclaringClass().equals(IConnectionPoint.class)
                         )) {
                         try {
                             return method.invoke(this, args);
                         } catch (InvocationTargetException ex) {
                             throw ex.getCause();
                         }
 		}
 
 		Class<?> returnType = method.getReturnType();
 		boolean isVoid = Void.TYPE.equals(returnType);
 
 		ComProperty prop = method.getAnnotation(ComProperty.class);
 		if (null != prop) {
+                        int dispId = prop.dispId();
 			if (isVoid) {
-				String propName = this.getMutatorName(method, prop);
-				this.setProperty(propName, args[0]);
-				return null;
+                                if(dispId != -1) {
+                                    this.setProperty(new DISPID(dispId), args[0]);
+                                    return null;
+                                } else {
+                                    String propName = this.getMutatorName(method, prop);
+                                    this.setProperty(propName, args[0]);
+                                    return null;
+                                }
 			} else {
-				String propName = this.getAccessorName(method, prop);
-				return this.getProperty(returnType, propName, args);
+                                if(dispId != -1) {
+                                    return this.getProperty(returnType, new DISPID(dispId), args);
+                                } else {
+                                    String propName = this.getAccessorName(method, prop);
+                                    return this.getProperty(returnType, propName, args);
+                                }
 			}
 		}
 
 		ComMethod meth = method.getAnnotation(ComMethod.class);
 		if (null != meth) {
-			String methName = this.getMethodName(method, meth);
-			Object res = this.invokeMethod(returnType, methName, args);
-			return res;
+                        int dispId = meth.dispId();
+                        if(dispId != -1) {
+                            return this.invokeMethod(returnType, new DISPID(dispId), args);
+                        } else {
+                            String methName = this.getMethodName(method, meth);
+                            return this.invokeMethod(returnType, methName, args);
+                        }
 		}
 
 		return null;
 	}",NotBuggy,"Add dispId to ComProperty and use value to simplify COM calls (skip GetIDsOfNames)

dispIDs can be queried via GetIDsOfNames at runtime or retrieved from the
typelibrary when bindings are generated.

Using the dispID eliminates one additional call into native environment.

It was also observed, that runtime reflection sometimes retrieves wrong
values (toString on IHTMLDocument2), so using a previously retrieved value
seems to be saver.
",NotBuggy,"The commit message indicates the addition of a `dispId` to `ComProperty` to optimize COM calls by skipping `GetIDsOfNames`. The changes show the implementation of this optimization by using `dispId` to directly access methods and properties, avoiding the runtime reflection issues mentioned.

Key points and how they relate to the message:
1. **Use of `dispId`:** The code now checks for a `dispId` when accessing properties or invoking methods. If a valid `dispId` is present (i.e., not -1), it uses it directly rather than computing the property or method name. This aligns perfectly with the commit message, which mentions eliminating an additional call into the native environment and reducing errors in runtime reflection.

2. **Direct Access:** By using `DISPID(dispId)`, the code attempts to directly interact with the COM component using the known dispatch identifier, which improves performance and reliability, as per the commit message rationale.

3. **Fallback Logic:** If `dispId` is not set, the original method of obtaining names is used. This preservation of the existing behavior ensures backward compatibility and stability.

This is an improvement and optimization rather than a bug fix, as it primarily aims to enhance performance and reliability based on retrieved `dispId` rather than address a malfunction in the existing logic.

Consequently, the changes outlined in the diff are consistent with the purpose described in the commit message, focusing on using `dispId` for efficiency without explicitly fixing a coding error. Therefore, the conclusion is **NotBuggy**."
jna,1645.json,429bed5aba63585863f9d734b7a1b0e8364610ff,"@@ -1,3 +1,3 @@
-		public SCODE getValue() {
-			return new SCODE(getPointer().getLong(0));
+		public CHAR getValue() {
+			return new CHAR(getPointer().getChar(0));
 		}",NotBuggy,"datatypes overworked 2
",NotBuggy,"The commit message ""datatypes overworked 2"" suggests a general change to data types, rather than a bug fix. The code is modified to change the return type from `SCODE` to `CHAR`, also updating the object construction to reflect this change (`getLong(0)` to `getChar(0)`). Without more context, this change appears to reflect a design decision rather than a bug fix. There is no explicit error handling, logical modifications, or any apparent correction related to a specific bug mentioned in the commit message. Given the lack of explicit context indicating a bug, it is more likely to be a type adjustment or refactor rather than a bug fix.

Thus, I conclude the changes are **NotBuggy**."
jna,602.json,2cea7182931b6f5c314d9d26afa8d1f73a0fd400,"@@ -1,3 +1,3 @@
         public String getPrintName() {
-            return String.copyValueOf(PathBuffer, this.PrintNameOffset.intValue() / 2, this.PrintNameLength.intValue() / 2);
+            return String.copyValueOf(PathBuffer, PrintNameOffset / 2, PrintNameLength / 2);
         }",NotBuggy,"Changes based on feedback
",NotBuggy,The code change simplifies the method getPrintName by removing unnecessary object dereferencing (`this.PrintNameOffset.intValue()` to `PrintNameOffset`) and arithmetic on Integers. This might be based on style or performance feedback but isn't a clear bug fix. The answer is **NotBuggy**.
jna,3448.json,fcf1f5c283ee83da50cfbf2b6d2900cc944eee18,"@@ -1,8 +1,3 @@
 	public ActiveWorkbook getActiveWorkbook() {
-		VARIANT.ByReference result = new VARIANT.ByReference();
-		HRESULT hr = oleMethod(OleAuto.DISPATCH_PROPERTYGET, result,
-				this.iDispatch, ""ActiveWorkbook"");
-
-		COMUtils.SUCCEEDED(hr);
-		return new ActiveWorkbook((IDispatch) result.getValue());
+		return new ActiveWorkbook(this.getAutomationProperty(""ActiveWorkbook""));
 	}",NotBuggy,"restored because of lost source code
",NotBuggy,"The commit message ""restored because of lost source code"" suggests that previous functionality is being reinstated due to lost code. The diff changes how the `getActiveWorkbook` method acquires an `ActiveWorkbook` object. 

The original method used COM interactions with `oleMethod`, `HRESULT`, and `COMUtils.SUCCEEDED(hr)` to retrieve and verify the `ActiveWorkbook`. The updated code simplifies this by calling `getAutomationProperty(""ActiveWorkbook"")` directly.

The code change might indicate:
- Simplification and possible fix: If the original code had issues in COM interaction or error handling, the new approach might resolve such problems.
- Restoration intent aligns with the message: Regardless of whether the original had bugs, this change effectively restores functionality by simplifying the logic without relying on potentially problematic COM methods.

However, without explicit error reports tied to this change, it's hard to definitively classify it as a bug fix based solely on the provided information.

Given the commit message's focus on restoration of lost code rather than explicitly fixing a reported bug, and the lack of clear indication of a bug fix in the diff, the answer would be **NotBuggy**."
jna,657.json,e69d7fa485d90d0ae2541f2adbb0f194035188f7,"@@ -1,3 +1,3 @@
 		public void setValue(String value) {
-			getPointer().setString(0, value, true);
+			getPointer().setString(0, value);
 		}",NotBuggy,"some changes
",NotBuggy,"The change in this diff involves removing the `true` parameter from the `setString` method call on a `Pointer` object. Here's a step-by-step analysis:

1. **Understanding the Change**: 
   - The original code called `setString(0, value, true)`.
   - The modified code calls `setString(0, value)`.

2. **Implications of the Change**:
   - The third parameter in the original method call (`true`) has been removed. 
   - The significance of this change depends on the context of the `setString` method. Typically, such boolean parameters are used to adjust how the string is set, potentially dealing with character encoding, string termination, or other flags.

3. **Potential Reasons for Removal**:
   - If `true` indicated a flag that is no longer needed or was incorrectly applied, removing it could rectify an unintended behavior.
   - It might correct a logic error, such as improper string handling or performance implications.

4. **Commit Message Context**:
   - The commit message ""some changes"" is too vague and provides no insight into the motivation behind this code modification.

5. **Conclusion**:
   - Without insight into the specifics of the `setString` method, deciding if this change specifically addresses a bug is challenging. However, the removal of a parameter suggests either a correction or optimization.
   - Given the lack of details and clarity, and without explicit evidence of a bug fix (such as a description of incorrect behavior in the commit message), it's not possible to definitively conclude that this change fixes a bug.

Given the information provided, the determination would be **NotBuggy** due to insufficient evidence and context to classify it as a bug fix."
jna,3553.json,ca95bfb424336336be4b3e2ef8d670281a554690,"@@ -1,18 +1,19 @@
     static void removeTemporaryFiles() throws IOException {
         File dir = getTempDir();
         FilenameFilter filter = new FilenameFilter() {
+            @Override
             public boolean accept(File dir, String name) {
                 return name.endsWith("".x"") && name.startsWith(JNA_TMPLIB_PREFIX);
             }
         };
         File[] files = dir.listFiles(filter);
         for (int i=0;files != null && i < files.length;i++) {
             File marker = files[i];
             String name = marker.getName();
             name = name.substring(0, name.length()-2);
             File target = new File(marker.getParentFile(), name);
             if (!target.exists() || target.delete()) {
                 marker.delete();
             }
         }
     }",NotBuggy,"Added volume management functions
",NotBuggy,"The commit message indicates the addition of volume management functions, but the code diff does not reflect any volume management additions. Instead, this diff adds an `@Override` annotation to an anonymous `FilenameFilter` class's `accept` method. This change primarily aids in code clarity and ensures the method correctly overrides a superclass method, which can prevent potential bugs related to method signature errors or misimplementation of interfaces.

However, the change itself doesn't directly indicate any new functionality related to volume management, nor does it clearly fix any bug related to the functionality described in the commit message.

Since the change doesn't address any specific bug or introduce the volume management functions as claimed in the commit message, I'll conclude this as **NotBuggy** based on the provided code changes."
jna,3910.json,5dc6bfb195a7e309feb17ec9a69b4540b03fc7da,"@@ -1,3 +1,3 @@
-        public void setChar(int bOff, char value) {
+        public String getString(long bOff, boolean wide) {
             throw new UnsupportedOperationException(MSG); 
         }",NotBuggy,"use long offsets and sizes for pointer arithmetic

git-svn-id: https://svn.java.net/svn/jna~svn/trunk@381 2f8a963e-d2e4-e7d0-97bf-ccb7fcea9d80
",NotBuggy,"The commit message states the purpose of switching to long offsets and sizes for pointer arithmetic. However, the provided code diff doesn't show arithmetic modifications. Instead, a method signature changes from `setChar` (which likely sets a character in data) to `getString` (likely returns a string). The change between methods isn't aligned with the message as no pointer arithmetic or use of long offsets is visible. Additionally, the implementation details of `getString` are missing, making it unclear if the method indeed addresses the switching to long offsets. Thus, this doesn't appear to be a bug fix but rather a change in method functionality. The answer is **NotBuggy**."
jna,2915.json,bd184d5b389666d7eeeda102c46914196e31da8d,"@@ -1,6 +1,4 @@
     public static String getXAttr(String path, String name, String encoding) throws IOException {
-        Memory valueMem = getXAttrAsMemory(path, name);
-        return Charset.forName(encoding)
-            .decode(valueMem.getByteBuffer(0, valueMem.size()))
-            .toString();
+        byte[] valueMem = getXAttrBytes(path, name);
+        return new String(valueMem, Charset.forName(encoding));
     }",NotBuggy,"Add byte[] based bindings to c.s.j.p.linux.XAttr
",NotBuggy,"The commit message indicates an addition of byte array-based bindings to a method. The diff shows a refactor of the `getXAttr` method to use a `byte[]` directly instead of converting using `Memory` and `ByteBuffer`. This change appears to simplify the code by directly using the byte array and converting it to a `String` with the specified encoding.

The commit message doesn't explicitly mention a bug fix but rather an addition or change in the method to use a different data type for processing. The modifications do not indicate a bug fix related to error handling, logical errors, or exception handling improvements. The new approach may be more efficient or serve another purpose, but without evidence pointing specifically to a bug being addressed, this change does not qualify as a bug fix.

Therefore, the classification of the commit should be **NotBuggy**."
spring-framework,19645.json,fe7e2a7f54c49efd99de2236dfdddf2639c2b354,"@@ -1,3 +1,3 @@
-	public int compareTo(MediaTypeExpression other) {
+	public int compareTo(AbstractMediaTypeExpression other) {
 		return MediaType.SPECIFICITY_COMPARATOR.compare(this.getMediaType(), other.getMediaType());
 	}",NotBuggy,"Improvements to the way the content of RequestConditions is exposed.

RequestCondition types keep individual expression types (e.g. the
discrete header or param expressions) package private. Although the 
implementation of these types should remain private, there is no 
reason not to provide access to the underlying expression data --
e.g. for creating a REST endpoint documentation tool, or if you 
want to know which of the ""consumes""/""produces"" media types
are negated.

This change ensures that all RequestCondition types have a public
getter that makes available the basic expression data. 



",NotBuggy,"The commit message talks about exposing the content of `RequestConditions` for easier access to expression data. However, the code diff shows a single-line change where the `compareTo` method parameter's type is changed from `MediaTypeExpression` to `AbstractMediaTypeExpression`.

Analyzing the change:

1. **Code Change:**
   - The `compareTo` method parameter's type is changed from `MediaTypeExpression` to `AbstractMediaTypeExpression`.
   - This change implies a type alteration which could be broadened or narrowed based on the class hierarchy.

2. **Commit Message Analysis:**
   - The message indicates that changes are about exposing expression data through public getters.
   - The present change does not introduce new public getters, nor does it expose additional data. 

3. **Relevance:**
   - The change in the diff does not directly relate to the content of the commit message.
   - It seems more like a refactoring or a correction ensuring type compatibility rather than exposing expression data.

Given that the diff does not align with the description in the commit message about exposing expression data and does not appear to fix any specific logical issues, this change is not indicative of fixing a bug.

Thus, the conclusion is **NotBuggy**."
spring-framework,1371.json,00ed8da5c5b5d82cf60c90de1fe41c687c822b2f,"@@ -1,3 +1,3 @@
-	public Session getSession(Class<? extends Session> sessionType) {
+	public <S extends Session> S getSession(Class<S> sessionType) {
 		return getSession(sessionType, null);
 	}",NotBuggy,"JmsTransactionManager supports lazy resource retrieval

Closes gh-22468
",NotBuggy,"The diff shows a change from a method returning a `Session` to a generic method returning `<S extends Session>`. This change enhances type safety and flexibility but does not imply a direct bug fix. The update supports more effective resource handling but lacks typical bug indicators like errors in logic or exception handling corrections mentioned in the commit message.

The commit message highlights support for lazy resource retrieval, likely relating to deferred operations to optimize resource use. The change alters the method signature for improved Java generics utilization but doesn't directly indicate a bug fix.

Given these observations and the lack of specific indicators for a bug-related correction, I conclude with **NotBuggy**."
spring-framework,5771.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,4 +1,4 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		return (this == other || (other instanceof ApplicationListenerDetector &&
 				this.applicationContext == ((ApplicationListenerDetector) other).applicationContext));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message indicates that the change involves annotating the parameter of an `equals` method with `@Nullable`. This annotation is used to convey that the `other` parameter can legally be `null`, which aligns with the contract of the `equals` method in Java.

In Java, it is permissible and common for `equals` methods to accept `null` parameters and return `false` when the parameter is `null`. Therefore, adding the `@Nullable` annotation is a way to document this expectation, making the codebase clearer and less error-prone for developers by highlighting that `null` is an acceptable argument.

The change does not alter any logic, error handling, or functionality of the method. Instead, it serves as documentation to assist developers using static analysis tools that respect annotations. It ensures that any potential static analysis or IDE warnings regarding nullability are appropriately suppressed.

As this change does not fix a bug but rather enhances code quality and clarity, the appropriate conclusion is **NotBuggy**."
spring-framework,29158.json,347f34c68a7a5bab46cf869e762fb3cee9a1fedc,"@@ -1,3 +1,3 @@
 	public String getParameter(String paramName) {
-		return (String) getExternalContext().getRequestParameterMap().get(paramName);
+		return getExternalContext().getRequestParameterMap().get(paramName);
 	}",NotBuggy,"EL container integration; support for contextual objects; removal of deprecated Spring 2.0 functionality; Java 5 code style
",NotBuggy,"In this git diff, the code change removes an unnecessary cast from `(String)` to simply the method call `getExternalContext().getRequestParameterMap().get(paramName)`. 

Let's break down the analysis:

1. **Commit Message Review**:
   - The commit message explains that changes involve EL container integration, support for contextual objects, removal of deprecated Spring 2.0 functionality, and adherence to Java 5 code style.
   - There's no mention of bug fixes or any issues related to parameter retrieval.

2. **Code Analysis**:
   - The modification changes the `getParameter` method, which previously cast the result of `get(paramName)` to a `String`.
   - Removing the cast aligns with Java 5's enhanced type inference and generics, making the code cleaner and more robust.
   - There's no indication that the type casting was causing a bug; rather, it's an enhancement towards cleaner code style.

3. **Relevance to Bug Fix**:
   - The absence of a cast improves type safety and clarity if `getRequestParameterMap()` properly infers the correct type itself, following Java 5's generic methodology.
   - However, this cast removal doesn't directly address a bug in functionality.

Given these observations, the change aligns more with code style improvements and may contribute to modernizing the codebase according to Java 5 standards rather than fixing a functionality bug. Thus, the change is best described as a refactor for code quality rather than solving a specific bug.

Overall, the alignment of the commit with the code changes focuses on style and integration improvements rather than bug resolution.

The answer is **NotBuggy**."
spring-framework,12706.json,a7c736915a4c23b3e3a5eb17d0c6aea904b53bd1,"@@ -1,16 +1,15 @@
 	public DataBuffer encodeValue(ByteBuf byteBuf, DataBufferFactory bufferFactory,
 			ResolvableType valueType, @Nullable MimeType mimeType, @Nullable Map<String, Object> hints) {
 
-		DataBuffer dataBuffer;
-		if (bufferFactory instanceof NettyDataBufferFactory) {
-			dataBuffer = ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
-		} else {
-			dataBuffer = bufferFactory.wrap(byteBuf.nioBuffer());
-		}
-
 		if (logger.isDebugEnabled() && !Hints.isLoggingSuppressed(hints)) {
 			String logPrefix = Hints.getLogPrefix(hints);
-			logger.debug(logPrefix + ""Writing "" + dataBuffer.readableByteCount() + "" bytes"");
+			logger.debug(logPrefix + ""Writing "" + byteBuf.readableBytes() + "" bytes"");
 		}
-		return dataBuffer;
+		if (bufferFactory instanceof NettyDataBufferFactory) {
+			return ((NettyDataBufferFactory) bufferFactory).wrap(byteBuf);
+		}
+		byte[] bytes = new byte[byteBuf.readableBytes()];
+		byteBuf.readBytes(bytes);
+		byteBuf.release();
+		return bufferFactory.wrap(bytes);
 	}",NotBuggy,"Polishing and completing contribution

See gh-24866
",NotBuggy,"The commit message states ""Polishing and completing contribution,"" suggesting minor improvements or refactoring rather than a bug fix. 

Let's analyze the changes:
- Initially, a `DataBuffer` was created based on the `bufferFactory` type and returned.
- The debug log now uses `byteBuf.readableBytes()` instead of `dataBuffer.readableByteCount()`, which doesn't inherently point to a bug.
- The refactored code no longer constructs a `DataBuffer` before the debug statement. Instead, the code returns directly from within the conditional.
- If `bufferFactory` is not a `NettyDataBufferFactory`, the new code reads bytes into an array, releases the `byteBuf`, and then wraps the bytes.

These changes seem more focused on improving code clarity and efficiency rather than fixing incorrect behavior. Importantly, no clearly incorrect behavior or exception handling is addressed.

Given this analysis, the changes are better categorized as refactoring or code polishing. Therefore, they do not indicate a bug fix. The conclusion is **NotBuggy**."
spring-framework,23230.json,9d6c38bd54fe8931ee73fbb7a076625deb934e28,"@@ -1,3 +1,3 @@
 	public boolean isCompilable() {
-		return this.exitTypeDescriptor != null;
+		return (this.exitTypeDescriptor != null);
 	}",NotBuggy,"Consistent bracket alignment
",NotBuggy,"The commit message refers to consistent bracket alignment, and the change indeed adds parentheses around a condition in the return statement. This is purely a stylistic adjustment and does not affect the logic or fix any bugs.

The commit does not indicate any bug fix or change in functionality. The code previously functioned correctly without the parentheses. This change does not address any error handling, logical corrections, or exceptions.

Therefore, the conclusion is **NotBuggy**."
spring-framework,25172.json,e881d4b1441b4ade35806b6f06d5b7ebe1828e92,"@@ -1,4 +1,3 @@
-	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include serializationInclusion) {
-		this.serializationInclusion = serializationInclusion;
-		return this;
+	public Jackson2ObjectMapperBuilder serializationInclusion(JsonInclude.Include inclusion) {
+		return serializationInclusion(JsonInclude.Value.construct(inclusion, inclusion));
 	}",NotBuggy,"Allow use of JsonInclude.Value

Closes gh-24824
",NotBuggy,"This commit changes the method serializationInclusion to use JsonInclude.Value instead of directly setting a JsonInclude.Include value. The change does not suggest a bug fix as it does not address any logical errors, exception handling, or error correction. Instead, it likely enhances functionality by supporting a new feature or API method, resulting in a more flexible configuration. Since the commit message also refers to allowing use rather than fixing anything, there is no indication of a bug fix.

The conclusion is **NotBuggy**."
spring-framework,470.json,b5e5e3307838359b42c7f4b1ed592b03b249eb4f,"@@ -1,3 +1,3 @@
 	public TransactionContext getParent() {
-		return parent;
+		return this.parent;
 	}",NotBuggy,"Remove legacy config options from AbstractReactiveTransactionManager

Includes general revision of reactive transaction sources.

See gh-22646
",NotBuggy,"The commit message indicates the removal of legacy configuration options and a revision of transaction sources, suggesting substantial changes. However, the diff only shows a minor code style change, prefixing `this.` to an existing field reference, which has no effect on functionality. This change doesn't align with the commit message and doesn't indicate a bug fix; therefore, the verdict is **NotBuggy**."
spring-framework,18397.json,b0790bf5e7b6091552ccf17a2e707ab438d129d7,"@@ -1,3 +1,5 @@
-	protected final void copyModelParameters(Map model, Transformer transformer) {
-		copyMapEntriesToTransformerParameters(model, transformer);
+	protected final void copyModelParameters(Map<String, Object> model, Transformer transformer) {
+		for (Map.Entry<String, Object> entry : model.entrySet()) {
+			transformer.setParameter(entry.getKey(), entry.getValue());
+		}
 	}",NotBuggy,"Java 5 code style
",NotBuggy,"The given commit message indicates a change to adopt Java 5 code style, which usually involves using generics and enhanced for-loops. The code changes align with this by introducing generics to the Map parameter, changing from a raw Map to Map<String, Object>, and implementing an enhanced for-loop for iterating over the map entries instead of using a method call.

The changes are refactoring adjustments for code readability and type safety introduced in Java 5 but do not indicate a bug fix such as error-handling updates, logic corrections, or other bug-related changes. The commit purely focuses on modernizing the code style.

Thus, the conclusion is **NotBuggy**."
spring-framework,27849.json,d0c8545d9bf4b97c47d257b445cfe0a2ed09db88,"@@ -1,3 +1,3 @@
 	public String getCookieName() {
-		return cookieName;
+		return this.cookieName;
 	}",NotBuggy,"updated setCookieMaxAge to match getCookieMaxAge return type (SPR-6070)
",NotBuggy,"The commit message describes an update to `setCookieMaxAge` for consistency with `getCookieMaxAge`'s return type, implying a type or interface-related correction. However, the actual code change is unrelated to `setCookieMaxAge` or `getCookieMaxAge`. The only change is using `this.cookieName` instead of just `cookieName`, which is a stylistic or clarity improvement, not indicative of a bug fix related to the return type of `getCookieMaxAge` or affecting `setCookieMaxAge`. Hence, there is a disconnect between the commit message and the code change.

The assessment is **NotBuggy**."
spring-framework,19387.json,f2b926467491098b09b38296b95d9543c97e22ad,"@@ -1,3 +1,3 @@
-		public int read(@NotNull byte[] b, int off, int len) throws IOException {
+		public int read(byte[] b, int off, int len) throws IOException {
 			return this.delegate.read(b, off, len);
 		}",NotBuggy,"Remove Jetbrains annotations
",NotBuggy,"The commit message indicates the removal of JetBrains annotations, specifically the `@NotNull` annotation. The code diff shows this removal. This change does not address any bug fixes, error handling, or logical corrections in the method's functionality itself. The method simply continues its operation without the annotation, which was likely used for static analysis or documentation purposes. Without evidence of a bug fix, especially since no functional code changes (such as logic correction or handling improvements) are part of this diff, we conclude that the commit is **NotBuggy**."
spring-framework,402.json,8dabb3e6269ab18bb2256266a70d4c285bbb4ff5,"@@ -1,8 +1,8 @@
-	private DefaultReactiveTransactionStatus newTransactionStatus(
-			ReactiveTransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
+	private GenericReactiveTransaction newReactiveTransaction(
+			TransactionSynchronizationManager synchronizationManager, TransactionDefinition definition,
 			@Nullable Object transaction, boolean newTransaction, boolean debug, @Nullable Object suspendedResources) {
 
-		return new DefaultReactiveTransactionStatus(transaction, newTransaction,
+		return new GenericReactiveTransaction(transaction, newTransaction,
 				!synchronizationManager.isSynchronizationActive(),
 				definition.isReadOnly(), debug, suspendedResources);
 	}",NotBuggy,"Shorter class names for common reactive transaction API types

Introduces TransactionExecution base interface for TransactionStatus as well as ReactiveTransaction. Renames getTransaction method to getReactiveTransaction, allowing for combined implementations of PlatformTransactionManager and ReactiveTransactionManager.

See gh-22646
",NotBuggy,"The commit message mentions the introduction of a ""TransactionExecution"" interface, a method rename, and shorter class names. The diff shows renaming methods and classes, aligning with the commit message focus on refactoring and improving naming conventions. However, there's no indication of a bug being fixed, only structural refactoring and renaming for clarity or consistency. Therefore, the answer is **NotBuggy**."
spring-framework,6430.json,d93303c0089d311f2b014f45f1b345ca7ab9cb1f,"@@ -1,20 +1,21 @@
 		public void processGroupImports() {
 			for (DeferredImportSelectorGrouping grouping : this.groupings.values()) {
+				Predicate<String> candidateFilter = grouping.getCandidateFilter();
 				grouping.getImports().forEach(entry -> {
-					ConfigurationClass configurationClass = this.configurationClasses.get(
-							entry.getMetadata());
+					ConfigurationClass configurationClass = this.configurationClasses.get(entry.getMetadata());
 					try {
-						processImports(configurationClass, asSourceClass(configurationClass),
-								asSourceClasses(entry.getImportClassName()), false);
+						processImports(configurationClass, asSourceClass(configurationClass, candidateFilter),
+								Collections.singleton(asSourceClass(entry.getImportClassName(), candidateFilter)),
+								candidateFilter, false);
 					}
 					catch (BeanDefinitionStoreException ex) {
 						throw ex;
 					}
 					catch (Throwable ex) {
 						throw new BeanDefinitionStoreException(
 								""Failed to process import candidates for configuration class ["" +
 										configurationClass.getMetadata().getClassName() + ""]"", ex);
 					}
 				});
 			}
 		}",NotBuggy,"ImportSelector.getCandidateFilter() for transitive filtering of classes

Closes gh-24175
",NotBuggy,"The commit message describes changes in `ImportSelector.getCandidateFilter()` for transitive filtering of classes, which indicates an enhancement rather than a bug fix. The code introduces a `Predicate<String> candidateFilter` to the import processing logic, potentially allowing more refined filtering of classes that can be imported. This is indicative of a design or functionality enhancement rather than a bug fix. 

In the context of bug fixes, you’d generally look for corrections to logical errors, improved exception handling, or other modifications addressing incorrect or unintended behavior, which are not explicitly present here.

Given that the code and the commit message align with an enhancement and not a bug fix, this change should conclude with **NotBuggy**."
spring-framework,42.json,a25e24f37ee46595ce30045c6fb9efdb692b7386,"@@ -1,3 +1,6 @@
-		public Object getResult() {
+		public T getResult() {
+			if (this.exception != null) {
+				ReflectionUtils.rethrowRuntimeException(this.exception);
+			}
 			return this.result;
 		}",NotBuggy,"generified TransactionCallback; WebSphereUowTransactionManager preserves original exception in case of rollback (SPR-5270)
",Buggy,"The commit message refers to changes in exception handling specifically for WebSphereUowTransactionManager. The diff modifies the `getResult` method by adding a check for `this.exception`. If an exception exists, it is rethrown using `ReflectionUtils.rethrowRuntimeException(this.exception)`, instead of returning `this.result`. 

This change suggests that the method now correctly handles exceptions by ensuring they are not silently ignored, aligning with the commit message about preserving the original exception during rollback situations.

These alterations correlate with the commit message and indicate a bug fix related to exception handling improvements.

Therefore, the conclusion is **Buggy**."
spring-framework,2798.json,098ac0bbb88cd178e85b7dc31642bed091560316,"@@ -1,10 +1,10 @@
-	public boolean equals(Object other) {
+	public boolean equals(@Nullable Object other) {
 		if (this == other) {
 			return true;
 		}
 		if (!(other instanceof DefaultIntroductionAdvisor)) {
 			return false;
 		}
 		DefaultIntroductionAdvisor otherAdvisor = (DefaultIntroductionAdvisor) other;
 		return (this.advice.equals(otherAdvisor.advice) && this.interfaces.equals(otherAdvisor.interfaces));
 	}",NotBuggy,"Annotate Object#equals parameter with @Nullable

Closes gh-23093
",NotBuggy,"The commit message mentions annotating the equals method's parameter with @Nullable, which doesn't affect the code's logical correctness. This change enhances code clarity regarding nullity but doesn't fix a runtime issue or logic error. The code change aligns with the commit message, focusing on clarification, not bug resolution. The answer is **NotBuggy**."
sonarqube,9746.json,1df148803610cd54f182b8636f01c0e6ece92b19,"@@ -1,3 +1,3 @@
-  public Date getUpdatedAt() {
+  public Long getUpdatedAt() {
     return updatedAt;
   }",NotBuggy,"Extract module sonar-db
",NotBuggy,"The commit message mentions extracting a module called ""sonar-db,"" implying changes related to module separation or refactoring. However, the provided code diff shows a change in the return type of the `getUpdatedAt` method from `Date` to `Long`. 

This change suggests a shift from using a `Date` object to potentially using a `Long` to represent a timestamp or epoch time. This is a significant modification as it changes the expected data type and how consumers of this method might process the returned value.

The change does not directly relate to the extraction of a module like ""sonar-db"" as described in the commit message. Instead, it introduces a type change which doesn't seem to address a bug fix directly. Typically, such a change could be part of a larger refactor or architectural change rather than a fix of a specific defect unless there was a clear issue with using `Date` previously unmentioned here.

Considering the focus on module extraction in the commit message and the nature of the change in the diff, this modification seems more aligned with refactoring or architectural concerns rather than fixing an existing bug.

Hence, the analysis indicates **NotBuggy**."
sonarqube,10338.json,85b940dd935f57860f26dff7f1bfaa9de1ba8af2,"@@ -1,4 +1,4 @@
   public List<String> selectQProfileUuidsByOrganizationAndGroups(DbSession dbSession, OrganizationDto organization, Collection<GroupDto> groups) {
-    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getId).collect(toList()),
+    return DatabaseUtils.executeLargeInputs(groups.stream().map(GroupDto::getUuid).collect(toList()),
       g -> mapper(dbSession).selectQProfileUuidsByOrganizationAndGroups(organization.getUuid(), g));
   }",NotBuggy,"SONAR-13221 change PK of GROUPS and update FKs
",NotBuggy,"The commit message mentions changing the primary key (PK) of the ""GROUPS"" table and updating foreign keys (FKs). The diff shows a change in how groups are identified when querying profile UUIDs, switching from using the group's ID to using its UUID. This aligns with the commit message, as switching from IDs to UUIDs in methods dealing with groups likely reflects a change in the primary key or the relevant foreign keys.

However, this diff alone doesn't directly indicate a bug fix since it doesn't address a specific defect in the logic but instead aligns with updating the schema or object references from IDs to UUIDs. 

Without evidence of a bug being corrected resulting in incorrect behavior and with the changes reflecting structural referential updates, this update addresses modification requirements rather than a direct logical error.

Therefore, the changes are **NotBuggy**."
sonarqube,18294.json,74c4ac9a4a5c9df13116862dd5dca982cc3f84bd,"@@ -1,13 +1,13 @@
   public boolean equals(Object obj) {
     if (!(obj instanceof Rule)) {
       return false;
     }
     if (this == obj) {
       return true;
     }
     Rule other = (Rule) obj;
     return new EqualsBuilder()
-        .append(pluginName, other.getPluginName())
+        .append(pluginName, other.getRepositoryKey())
         .append(key, other.getKey())
         .isEquals();
   }",NotBuggy,"Deprecate Rule.setPluginName() , Rule.getPluginName() and related stuff, because was replaced by repositoryKey
",NotBuggy,"The commit message talks about deprecating `Rule.setPluginName()`, `Rule.getPluginName()`, and related methods, with `repositoryKey` being the replacement. The code modification updates the `equals` method to compare `pluginName` with `repositoryKey`, aligning with the deprecation strategy.

While the change could affect functionality by altering what is compared in the `equals` method, the commit does not explicitly indicate fixing incorrect behavior due to a bug. Instead, it suggests adapting the code for a planned deprecation which is more of an enhancement or refactor associated with library changes.

Given the information, the code does not directly address a bug, nor does the commit message emphasize bug-related terminology or intent. Thus, the change seems more related to an internal API improvement and deprecation handling rather than resolving an existing error.

Therefore, this does not align with a bug fix, making the assessment **NotBuggy**."
sonarqube,14645.json,f6b2a269c44043f1c230b646dbb8662aed3fa5a4,"@@ -1,15 +1,15 @@
     private int computeValueForMetric(NewCoverageCounter counter, Metric metric) {
-      if (metric.getKey().equals(outputMetricKeys.getNewLinesToCover())) {
+      if (metric.getKey().equals(NEW_LINES_TO_COVER_KEY)) {
         return counter.getNewLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredLines())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_LINES_KEY)) {
         return counter.getNewLines() - counter.getNewCoveredLines();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewConditionsToCover())) {
+      if (metric.getKey().equals(NEW_CONDITIONS_TO_COVER_KEY)) {
         return counter.getNewConditions();
       }
-      if (metric.getKey().equals(outputMetricKeys.getNewUncoveredConditions())) {
+      if (metric.getKey().equals(NEW_UNCOVERED_CONDITIONS_KEY)) {
         return counter.getNewConditions() - counter.getNewCoveredConditions();
       }
       throw new IllegalArgumentException(""Unsupported metric "" + metric.getKey());
     }",NotBuggy,"SONAR-11795 Drop persistit and move coverage measures computation to CE side
",NotBuggy,"The commit message describes the removal of ""persistit"" and moving some computations to the ""CE side,"" neither of which is related to the code changes shown. The diff only replaces calls to methods from `outputMetricKeys` with direct constant keys for metric comparison. This refactor makes the code simpler and possibly improves performance but doesn't suggest any bug fix.

None of these changes address a bug, and the commit message does not align directly with the changes indicated in the diff. Thus, this commit seems more like a code refactor rather than a bug fix.

**NotBuggy**"
sonarqube,2497.json,184dbe792ce14f192f75b13507c30a4b697d5852,"@@ -1,3 +1,3 @@
   private static Predicate<QProfileDto> byName(SearchWsRequest request) {
-    return p -> request.getProfileName() == null || Objects.equals(p.getName(), request.getProfileName());
+    return p -> request.getQualityProfile() == null || Objects.equals(p.getName(), request.getQualityProfile());
   }",NotBuggy,"SONAR-9865 Sanitize WS api/qualityprofiles/*

- functional key of a quality profile is: name, language and organization
- quality profile name is named 'name'
- quality profile key is named 'key'
- quality profile key parameter is deprecated when appropriate
",NotBuggy,"The modification involves changing the comparison of profile names from using `getProfileName()` to `getQualityProfile()`. This suggests a naming correction or update to conform with a new or updated API method signature or naming convention.

The commit message hints at the sanitization of the WS API for quality profiles, potentially aligning with updated methods or parameters, such as renaming methods or deprecating parameters. This specific change directly affects how quality profiles are identified by their name within a predicate function, suggesting an update in the API that may have required this adjustment to continue functioning correctly.

While this change resolves the potential misalignment with the API's usage or naming conventions, it does not directly indicate a bug fix in terms of erroneous behavior, logic corrections, or exception handling improvements. The language of the commit message indicates a general enhancement in alignment with API updates, but there's insufficient evidence to conclude that this was a fix of incorrect behavior.

Therefore, although the change is closely related to functionality, it is more about adaptation to new standards or API conventions rather than fixing a bug.

The answer is **NotBuggy**."
sonarqube,4754.json,0f0c3b73cc423fe90d6f4b97d6e73fe7bcdc485d,"@@ -1,5 +1,5 @@
   private static <PARAM> PARAM getNonNullValue(LiveMeasureDto measure, Function<LiveMeasureDto, PARAM> function) {
     PARAM value = function.apply(measure);
-    checkState(value != null, ""Measure not found"");
+    checkState(value != null, ""Measure has not been found"");
     return value;
   }",NotBuggy,"SONAR-10713 Badges can only be used on projects, long living branches and applications
",NotBuggy,"The change in the code diff updates a string in an exception message from ""Measure not found"" to ""Measure has not been found"". This change is a minor wording update and does not indicate that a bug is being fixed. The functionality of the `getNonNullValue` method remains unchanged, and the change is not related to address a logical error, exception-handling improvement, or error-handling update. The commit message mentioning badges and their usage does not relate to the change in the code diff either, which is focused only on a message string update. The conclusion is **NotBuggy**."
sonarqube,21891.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,4 +1,4 @@
-  public SelectWsRequest setProjectKey(String projectKey) {
+  public RemoveGroupRequest setProjectKey(String projectKey) {
     this.projectKey = projectKey;
     return this;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",Buggy,"The provided commit message mentions the auto-generation of sonar-ws for integration tests. However, the code change consists of renaming the method's return type from `SelectWsRequest` to `RemoveGroupRequest`. This modification does not align with the commit message, which makes no reference to such a change.

Moreover, this change appears to be a correction to the return type of the method, indicating that the previous return type might have been incorrect. If `RemoveGroupRequest` is indeed the correct type to return (perhaps aligning with the intended functionality of the method being part of the `RemoveGroupRequest` class), then this could be seen as a bug fix.

Despite the misalignment with the commit message, which focuses on aspects unrelated to the code change, the correction of a potentially improper return type signifies an error correction in the code's logic.

Consequently, based on the nature of the code modification as a potential correction of a previous mistake, I conclude that this change is **Buggy**."
sonarqube,17860.json,a4f8bbcf6f4d44cd0b637eff93ee8a3e49906edb,"@@ -1,4 +1,4 @@
-    public NewRule setType(Type t) {
+    public NewRule setType(RuleType t) {
       this.type = t;
       return this;
     }",NotBuggy,"SONAR-7353 Move RuleType to API
",NotBuggy,"The commit message indicates the movement of `RuleType` to the API, and the code change reflects an update in the method's parameter type from `Type` to `RuleType`.

However, there's no indication here of a bug fix. Instead, this seems to be a refactoring or an update to adapt to API changes where `Type` has been renamed or refactored to `RuleType`. The change likely involves an internal or structural modification to the codebase to improve clarity, align with new API conventions, or prepare for further development.

Since there's no correction of a faulty logic, handling of an exception, or any error rectification, it does not indicate a bug fix.

Therefore, the conclusion is **NotBuggy**."
sonarqube,16129.json,3821d2d07b3d299f44a643d77aec3d48fadf6c74,"@@ -1,10 +1,10 @@
   public static SecurityStandards fromSecurityStandards(Set<String> securityStandards) {
     Set<String> standards = securityStandards.stream()
       .filter(Objects::nonNull)
       .collect(MoreCollectors.toSet());
     Set<String> owaspTop10 = toOwaspTop10(standards);
     Set<String> cwe = toCwe(standards);
     Set<String> sansTop25 = toSansTop25(cwe);
-    Set<String> sq = toSonarSourceSecurityCategories(cwe);
+    Set<SQCategory> sq = toSQCategories(cwe);
     return new SecurityStandards(standards, cwe, owaspTop10, sansTop25, sq);
   }",NotBuggy,"SONAR-12717 use enum for SQ categories in SecurityStandards
",NotBuggy,"The commit message indicates the introduction of an enum for SQ categories. The code change reflects this by altering the `sq` variable from a `Set<String>` (using `toSonarSourceSecurityCategories`) to a `Set<SQCategory>` (now using `toSQCategories`). This adjustment aligns with the commit message's goal of using an enum for categories, ensuring type safety and potentially reducing errors related to string handling.

The change does not inherently suggest a bug fix but rather an enhancement or refactor to use a more robust type system approach. There are no indications of corrections to functional logic or error handling typically associated with a bug fix.

Therefore, the conclusion is **NotBuggy**."
sonarqube,22005.json,bff9cf373e2ecbe66344d58c3423b873f1fb17c5,"@@ -1,3 +1,3 @@
-  public String getComponentId() {
-    return componentId;
+  public String getComponent() {
+    return component;
   }",NotBuggy,"Auto-generate sonar-ws for integration tests
",NotBuggy,"The commit message indicates an update related to auto-generating sonar-ws for integration tests, while the code change reflects a simple renaming of a method and its returned member. There is no indication of a bug fix based on the provided commit message and the diff itself. The refactor seems to align method naming with possibly newer standards or naming conventions but doesn't imply fixing erroneous behavior. Therefore, the conclusion is **NotBuggy**."
sonarqube,16517.json,34cdbe891e9b45f42d35ee6deb2776cdfe50b431,"@@ -1,3 +1,3 @@
-  public Optional<Long> getDurationInMs() {
+  public Optional<Integer> getDurationInMs() {
     return Optional.ofNullable(durationInMs);
   }",NotBuggy,"SONAR-8353 persist deliveries in Compute Engine

Deliveries older than 1 month are purged.
",NotBuggy,"The commit message doesn't directly indicate a bug fix; it merely mentions persistence changes in the Compute Engine and a policy for purging old deliveries. The code change modifies the return type of the method `getDurationInMs` from `Optional<Long>` to `Optional<Integer>`. This change could reflect an adjustment in how durations are stored or interpreted, but without more context, it's unclear if it resolves a bug.

However, considering conversion from `Long` to `Integer` could indicate dealing with a value range issue (e.g., values not exceeding `Integer.MAX_VALUE`), which might fix storage or compatibility concerns. Nevertheless, in isolation, this doesn't distinctly appear like an error-handling or logical correction typical in bug fixes.

Given the available information, the change doesn't seem to directly address an explicit bug, based on both the commit message's primary focus and the nature of the code alteration.

**NotBuggy**"
sonarqube,19146.json,6adb553ddc2711cb88632be9bd96b4e91c51082e,"@@ -1,3 +1,3 @@
   protected Logger newInstance(String name) {
-    return new ConsoleLogger(name);
+    return new ConsoleLogger();
   }",NotBuggy,"SONAR-5700 ability to change level of loggers of 3rd-party libraries
",NotBuggy,"The commit message suggests a feature-related change involving the ability to change the level of loggers for third-party libraries, likely related to configuration or control over logging levels. 

The provided diff shows a change in the `newInstance` method of a logging-related class. The parameter `name` is removed from the `ConsoleLogger` constructor call, but `name` is still part of the method's signature, suggesting it might still be used elsewhere in the code, or that the use of `name` in `ConsoleLogger` might be unnecessary for its instantiation.

However, there's no evidence in the diff or message that indicates a bug fix. The modification appears to focus on the method of instantiating `ConsoleLogger`. While it could potentially relate to the more advanced manipulation of loggers, such as allowing dynamic logging level configurations elsewhere, the diff does not explicitly address a bug fix.

Thus, the change does not directly involve fixing a bug in terms of error handling, logical correction, or exception-handling improvement. The focus seems to be on a functionality or feature adjustment related to logging.

Therefore, the conclusion based on the details provided is **NotBuggy**."
sonarqube,12862.json,1d8455866b14fc692bfebe2de64e4437438a5ce8,"@@ -1,4 +1,4 @@
-  public MetricDto setDomain(String domain) {
+  public MetricDto setDomain(@Nullable String domain) {
     this.domain = domain;
     return this;
   }",NotBuggy,"SONAR-8116 Return rating metrics in api/qualitygates/app
",NotBuggy,"The change involves annotating a parameter with `@Nullable`, suggesting the parameter may accept null values. This is not inherently a bug fix but rather an enhancement for code clarity or API usage flexibility. The commit message does not mention fixing a bug but instead references returning rating metrics, unrelated to this change. Thus, the changes do not directly align with the commit message in indicating a bug fix.

In conclusion, the changes do not indicate a bug fix; therefore, the assessment is **NotBuggy**."
sonarqube,3675.json,8de13aed2d978699d7f8d379f6af8610c173c29e,"@@ -1,4 +1,7 @@
   private Map<String, String> getKeysToDisplayMap(Set<String> keys) {
     return keys.stream()
-      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity()));
+      .collect(Collectors.toMap(propertyDefinitions::validKey, Function.identity(),
+        (u, v) -> {
+          throw new IllegalArgumentException(format(""'%s' and '%s' cannot be used at the same time as they refer to the same setting"", u, v));
+        }));
   }",NotBuggy,"SONAR-7969 Fail when using key and deprecated in values WS

Display specific message when key and deprecated key are used at the same time
",Buggy,"The commit message indicates that the code update aims to ensure a failure occurs when both a key and its deprecated counterpart are used simultaneously. The diff shows an update to the `getKeysToDisplayMap` method, which now includes a merge function in `Collectors.toMap`. This function throws an `IllegalArgumentException` if there is an attempt to map two keys that refer to the same setting, aligning with the commit's goal of failing in such a case.

This change introduces error handling that wasn't present before, in line with detecting and reacting to erroneous use of configuration keys (when both a current and deprecated key are used). It greatly enhances the code's robustness by correctly implementing a check that previously didn't exist.

The modification aligns well with the commit message, which describes introducing a specific error message when conflicting keys are used. The use of an exception with a descriptive error message is a clear sign of a bug fix related to input validation or logical correctness.

Therefore, based on the provided information, the changes indeed address a validation issue likely considered a bug given the context. Consequently, this update is considered a bug fix.

The conclusion is **Buggy**."
sonarqube,12954.json,e0ad2379d3e1f0e4c286570b5b86dd0a351589da,"@@ -1,4 +1,3 @@
   public void deleteGlobalProperty(String key, DbSession session) {
-    PropertiesMapper mapper = session.getMapper(PropertiesMapper.class);
-    mapper.deleteGlobalProperty(key);
+    getMapper(session).deleteGlobalProperty(key);
   }",NotBuggy,"SONAR-7676 some modernization of PropertiesDao and PropertiesDaoTest
",NotBuggy,"The commit message refers to modernizing the `PropertiesDao` class. The diff shows a change in the `deleteGlobalProperty` method, where `session.getMapper(PropertiesMapper.class)` is replaced with `getMapper(session)`. 

This modification suggests an internal refactor or improvement in how the mapper is retrieved, likely for simplifying code, adhering to new coding practices, or possibly utilizing a different mechanism for obtaining the mapper that is considered more efficient or appropriate. However, it does not directly address any logical errors, error handling, or exception handling that would signify a bug fix. Instead, it points to a modernization effort, as stated in the commit message.

Given that there's no evidence of error correction or bug resolution, this change aligns with code modernization rather than a bug fix.

The conclusion is **NotBuggy**."
voldemort,6120.json,45f14abf745371e4920d9c4d7ea0164ddaa2b9e5,"@@ -1,4 +1,9 @@
     public static voldemort.client.protocol.pb.VProto.VoldemortRequest parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeDelimitedFrom(input).buildParsed();
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
     }",NotBuggy,"Updated Protobuff to 2.3
",Buggy,"The commit message ""Updated Protobuff to 2.3"" does not match the changes in the code diff, which primarily focus on modifying the behavior of the `parseDelimitedFrom` method. The code change introduces a conditional check to determine if `mergeDelimitedFrom(input)` was successful before proceeding to `buildParsed()`. If the merge fails, the method now returns `null` instead of potentially returning an improperly built object.

Such a change indicates an improvement in error handling, where the method previously might have thrown an exception or returned an invalid object without performing this check. Introducing this conditional structure is a sign of improved reliability in handling input streams that might not be properly formatted or complete.

Therefore, the code modifications indicate a potential bug fix by preventing the use of improperly constructed `VoldemortRequest` objects from incomplete or invalid input streams. Despite the disconnect with the commit message focusing on a version update, the essence of the code modification strongly suggests a bug fix.

Hence, the evaluation here is **Buggy**."
voldemort,9648.json,b80351a00a6a6f3a0938aac86e57245badfe0daa,"@@ -1,8 +1,8 @@
     public void clearCount(int nodeId) {
         AtomicLong counter = values.get(nodeId);
         if(counter == null) {
             counter = new AtomicLong(0L);
-            values.putIfAbsent(nodeId, counter);
-        }
-        counter.set(0L);
+            values.put(nodeId, counter);
+        } else
+            counter.set(0L);
     }",NotBuggy,"Minor code cleanup
",NotBuggy,"The commit message describes the changes as ""Minor code cleanup."" Upon examining the provided diff, the following can be observed:

1. **Original Behavior**: 
   - The `AtomicLong` counter for a `nodeId` is fetched.
   - If it doesn't exist (i.e., if `counter == null`), a new `AtomicLong` is instantiated, and then `putIfAbsent` is used to store it in the `values` map if it's not already present.
   - If `counter` exists, its value is directly set to 0.

2. **Modified Behavior**:
   - The `putIfAbsent` method is replaced with `put`, which will simply insert the new `AtomicLong` into the map, replacing any existing value with the same key.
   - If `counter` isn't null, the method sets the value to 0, now occurring within an `else` block.

**Analysis**:
- The change ensures that the `AtomicLong` is always inserted, rather than conditionally, which aligns with the intended cleanup but subtly changes the functionality: the existing value is replaced.
- This tidy structure is clearer but doesn't address a bug per se, and no error handling, exception correction, or logical issue resolution typically signifying a bug fix is present.

Based on the analysis, the changes align with the commit message of ""Minor code cleanup"" and don't indicate a direct bug fix. Therefore, the conclusion is **NotBuggy**."
voldemort,938.json,2150275cb31c85a0a8d56744abd030c96f8ba804,"@@ -1,29 +1,28 @@
         public void pruneJob(int nodeId, String store) {
             logger.info(""Kicking off prune job on Node "" + nodeId + "" for store "" + store);
             VAdminProto.PruneJobRequest.Builder jobRequest = VAdminProto.PruneJobRequest.newBuilder()
                                                                                         .setStoreName(store);
 
             VAdminProto.VoldemortAdminRequest adminRequest = VAdminProto.VoldemortAdminRequest.newBuilder()
                                                                                               .setPruneJob(jobRequest)
                                                                                               .setType(VAdminProto.AdminRequestType.PRUNE_JOB)
                                                                                               .build();
-            // FIXME VC probably need a helper to do all this, at some point..
-            // All
+            // TODO probably need a helper to do all this, at some point.. all
             // of this file has repeated code
             Node node = AdminClient.this.getAdminClientCluster().getNodeById(nodeId);
             SocketDestination destination = new SocketDestination(node.getHost(),
                                                                   node.getAdminPort(),
                                                                   RequestFormatType.ADMIN_PROTOCOL_BUFFERS);
             SocketAndStreams sands = socketPool.checkout(destination);
 
             try {
                 DataOutputStream outputStream = sands.getOutputStream();
                 ProtoUtils.writeMessage(outputStream, adminRequest);
                 outputStream.flush();
             } catch(IOException e) {
                 helperOps.close(sands.getSocket());
                 throw new VoldemortException(e);
             } finally {
                 socketPool.checkin(destination, sands);
             }
         }",NotBuggy,"Adding test for Prune job
",NotBuggy,"The commit message states that a test for a prune job is being added. However, the provided diff shows comments being updated and code formatting changes, but no test cases or testing-related code has been introduced. The changes include the conversion of a FIXME comment to a TODO and some minor comment adjustments, which do not relate to bug fixes or adding tests. Since there is no indication of testing additions or bug-related modifications in the code, the changes do not align with the commit message.

Thus, the changes do not indicate a bug fix, and are unrelated to the commit message stating that a test is being added. Therefore, the answer is **NotBuggy**."
voldemort,4195.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public int getRoStoreVersionsCount() {
-        return result.getRoStoreVersionsCount();
+      public voldemort.client.protocol.pb.VAdminProto.ROMetadataMap getRoStoreMetadata(int index) {
+        return result.getRoStoreMetadata(index);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message indicates ongoing development related to rebalancing state changes, mentioning future rollback options, which suggests that the focus is not on fixing bugs but rather on feature development. The change in the diff alters a method, replacing `getRoStoreVersionsCount` with `getRoStoreMetadata`, which likely shifts functionality rather than fixing a specific defect.

Given the message's emphasis on an ""intermediate commit"" and the lack of any mention of bug fixes, combined with the fact that this change appears to be part of broader feature development, it aligns more with feature modifications rather than direct bug fixing efforts.

Therefore, the answer is **NotBuggy**."
voldemort,7783.json,94be1a5451f7c763e84401ce4eb6a9e58b27b91b,"@@ -1,11 +1,10 @@
-    private void writeResults(DataOutputStream outputStream, List<Versioned<byte[]>> values)
-            throws IOException {
-        outputStream.writeInt(values.size());
-        for(Versioned<byte[]> v: values) {
-            byte[] clock = ((VectorClock) v.getVersion()).toBytes();
-            byte[] value = v.getValue();
-            outputStream.writeInt(clock.length + value.length);
+    public void writeResponse(DataOutputStream outputStream) throws IOException {
+        outputStream.writeShort(0);
+        outputStream.writeInt(results.size());
+        for(Version v: results) {
+            byte[] clock = ((VectorClock) v).toBytes();
+
+            outputStream.writeInt(clock.length);
             outputStream.write(clock);
-            outputStream.write(value);
         }
     }",NotBuggy,"ShareBuffer Refactoring

Refactored the Shared Buffer code to eliminate the separate read and
write buffers. Now a common buffer is used and the code is refactored
into its own classes. running the unit test.
",NotBuggy,"The provided commit message describes a refactoring activity and indicates changes related to buffer management. Let's evaluate the changes in the diff:

1. **Method Signature Changes:** 
   - The method `writeResults` is modified to `writeResponse`, and its parameters have been altered. The new method does not take `List<Versioned<byte[]>> values` as a parameter, implying a class-level access to `results`, suggesting some class structure changes.

2. **Data Writing Logic:** 
   - The original logic writes the number of elements, followed by a combined length of clock and value byte arrays, and then both arrays themselves.
   - The updated logic only writes the clock's length and the clock byte array, without writing the value. This indicates a change in what data is being sent/written.

3. **Added Writing Logic:**
   - `outputStream.writeShort(0);` is added at the beginning, which wasn't part of the original method. The purpose of this line is not apparent from the code alone.

4. **Missing Components:**
   - The value portion previously written (`outputStream.write(value);`) is completely removed in the updated code.

The commit message emphasizes general refactoring and sharing of a buffer, with no specific mention of bug fixes. The changes primarily focus on modifying the data writing protocol, aligning with refactoring for perhaps new architectural or design goals rather than fixing existing bugs.

Given that this change might result in different data being written but does not necessarily fix an error or incorrect logic as described by the previous state, it aligns more with a structural code modification.

Thus, the changes are consistent with a refactor and do not explicitly target a bug fix. Therefore, the nature of the changes aligns with the commit message's indication of a refactor rather than addressing a bug.

**Conclusion: NotBuggy**"
voldemort,3071.json,a4f42bf9d1d1b83d91d8f562019eb1f9e225460d,"@@ -1,4 +1,5 @@
       public Builder clearStoreName() {
-        result.storeName_ = java.util.Collections.emptyList();
+        result.hasStoreName = false;
+        result.storeName_ = getDefaultInstance().getStoreName();
         return this;
       }",NotBuggy,"Removed the new cleaner way to get read-only metadata. Fall back to the old dirty multiple message format to maintain backwards compatibility...Sigh!
",NotBuggy,"The commit message indicates a reversion to a previous approach to maintain backward compatibility rather than addressing a specific bug. The code changes replace `java.util.Collections.emptyList()` with fetching a default instance's store name and resetting a flag (`hasStoreName`). These changes align with restoring old behavior for backward compatibility, rather than fixing a bug. The use of `getDefaultInstance().getStoreName()` suggests reverting to a pre-existing default setup.

Overall, this modification seems to focus on compatibility rather than a defect correction. Therefore, the classification is **NotBuggy**."
voldemort,978.json,0cc38b8ce750c9940eefe9b3274c4fb2f9e5437e,"@@ -1,28 +1,28 @@
     public static void validateReadOnlyStores(Cluster cluster,
                                               List<StoreDefinition> storeDefs,
                                               AdminClient adminClient) {
         List<StoreDefinition> readOnlyStores = filterStores(storeDefs, true);
 
         if(readOnlyStores.size() == 0) {
             // No read-only stores
             return;
         }
 
-        List<String> storeNames = RebalanceUtils.getStoreNames(readOnlyStores);
+        List<String> storeNames = getStoreNames(readOnlyStores);
         for(Node node: cluster.getNodes()) {
             if(node.getNumberOfPartitions() != 0) {
                 for(Entry<String, String> storeToStorageFormat: adminClient.getROStorageFormat(node.getId(),
                                                                                                storeNames)
                                                                            .entrySet()) {
                     if(storeToStorageFormat.getValue()
                                            .compareTo(ReadOnlyStorageFormat.READONLY_V2.getCode()) != 0) {
                         throw new VoldemortRebalancingException(""Cannot rebalance since node ""
                                                                 + node.getId() + "" has store ""
                                                                 + storeToStorageFormat.getKey()
                                                                 + "" not using format ""
                                                                 + ReadOnlyStorageFormat.READONLY_V2);
                     }
                 }
             }
         }
     }",NotBuggy,"a) Add metadata file for fetch files in case of new node
b) Moved down ""show-plan"" option to get better idea of what is happening
",NotBuggy,"Analyzing the given diff and commit message:

1. **Commit Message Analysis**:
   - The commit message mentions two distinct changes:
     a) Adding a metadata file for fetching files when dealing with new nodes.
     b) Adjusting the position of the ""show-plan"" option for clarity.

2. **Code Diff Analysis**:
   - The diff shows a single change: the static method `RebalanceUtils.getStoreNames(readOnlyStores)` is directly replaced with `getStoreNames(readOnlyStores)`. There's no evidence in the diff for the changes mentioned in the commit message.
   - This change appears to simplify the code by removing the class qualifier, potentially indicating a code organization change, like moving the method to the same class or context.

3. **Relevance to Commit Message**:
   - The change does not relate to file fetching or node management directly.
   - Similarly, it does not concern ""show-plan"" positioning or alterations of any plan display.

4. **Bug Indication**:
   - There's nothing in this code change specifically targeting a bug fix, such as correcting a logical flaw or handling exceptions.

Conclusion: The code change does not align with the commit message regarding the specific functionalities mentioned (metadata file for nodes and show-plan option). It does not indicate a bug fix; it is likely a code organization or refactor.

The conclusion is **NotBuggy**."
voldemort,8068.json,97d3f70279da869411b1d6c6344d5fe8b164c891,"@@ -1,9 +1,9 @@
     public int getAvailableNodeCount() {
         int available = 0;
 
         for(NodeStatus nodeStatus: nodeStatusMap.values())
-            if(nodeStatus.isAvailable())
+            if(isAvailable(nodeStatus))
                 available++;
 
         return available;
     }",NotBuggy,"Work on implementation-specific unit tests for FailureDetectors. Also reintroduced usage of Time to allow for easier unit testing.
",NotBuggy,"The commit message doesn't mention bug fixes, focusing on unit tests and reintroducing Time for testing. The code change alters node availability checks, suggesting a method replacement. However, no logical correction or bug fix is evident from this change alone. It's likely part of a refactor or testability improvement. Thus, the answer is **NotBuggy**."
voldemort,577.json,4c276b83ff229e20956e3f25803a31d633727318,"@@ -1,7 +1,7 @@
     public int taskCount() {
         int numTasks = 0;
         for(RebalanceBatchPlan batchPlan: batchPlans) {
-            numTasks += batchPlan.getBatchPlan().size();
+            numTasks += batchPlan.getTaskCount();
         }
         return numTasks;
     }",NotBuggy,"Added ""progress bar"" for rebalance batch plan

Added RebalanceBatchPlanProgressBar
- progress tracking object for each rebalancing batch
- integrated with RebalanceController, AsyncRebalanceTask, and
  (Donor|Stealer)BasedRebalanceTask

Did other general clean up of logging during rebalance to make it
appropriately verbose (more verbose in some places, less verbose in
others...)
",NotBuggy,"The provided commit message discusses the addition of a ""progress bar"" and other logging changes related to rebalancing tasks. However, the diff itself focuses on changing the method used to retrieve task counts from RebalanceBatchPlan objects. This alteration suggests a refactor to possibly improve encapsulation or accuracy of obtaining task counts, rather than addressing an error. 

There is no clear indication or implication of a bug fix based on the commit message and the diff provided. The commit message does not mention fixing bugs related to task count inaccuracies and primarily describes additional features and logging improvements.

Thus, the change in the method call is more aligned with cleanup or general code improvement than correcting a specific bug.

The conclusion is **NotBuggy**."
voldemort,42.json,e011531aa5e8504081b92044bd4b1ecf7baf791d,"@@ -1,8 +1,9 @@
             public boolean accept(Path input) {
-                if(input.getName().startsWith(Integer.toString(partitionId) + ""_""
-                                              + Integer.toString(replicaType))) {
+                if(input.getName().matches(""^"" + Integer.toString(partitionId) + ""_""
+                                           + Integer.toString(replicaType) + ""_""
+                                           + Integer.toString(chunkId) + ""\\.data"")) {
                     return true;
                 } else {
                     return false;
                 }
             }",NotBuggy,"a) Changed back RO to use one reducer per chunk - Why? Finding diff at one chunk level is simpler than at chunk set level
b) Changed collision iterator to return key as well
c) Loads of unit tests to test other wrapper data file chunk set code
",NotBuggy,"The commit message does not mention a bug fix. The diff changes how file paths are accepted, shifting from a startsWith check to a regex match for more specific file pattern validation. The adjustment isn't described in terms of bug resolution and aligns with the message's code functionality enhancement theme. Since no bug fix is evident, the answer is **NotBuggy**."
voldemort,3034.json,d06fcec37fb3f3d45ffbd3fe15eafce77ba64069,"@@ -1,3 +1,3 @@
-      public java.util.List<voldemort.client.protocol.pb.VAdminProto.ROStoreVersionDirMap> getRoStoreVersionsList() {
-        return java.util.Collections.unmodifiableList(result.roStoreVersions_);
+      public java.util.List<voldemort.client.protocol.pb.VAdminProto.RebalancePartitionInfoMap> getRebalancePartitionInfoListList() {
+        return java.util.Collections.unmodifiableList(result.rebalancePartitionInfoList_);
       }",NotBuggy,"Another intermediate commit which has the code for changing rebalancing state. Still need to add rollback options
",NotBuggy,"The commit message describes an intermediate commit related to altering rebalancing state, with a note that rollback features are not yet implemented. The code change observed involves renaming a method and replacing a list field from `roStoreVersions_` to `rebalancePartitionInfoList_`. This update aligns with the commit message's focus on changing the rebalancing state, suggesting work in progress rather than a bug fix. 

The diff suggests that the method now returns a different list (`RebalancePartitionInfoMap`) compared to the previous list (`ROStoreVersionDirMap`), potentially related to rebalancing rather than resolving a specific error or erroneous behavior. The change doesn't appear to address error-handling or correct previous logic but seems more aligned with feature development.

Based on this analysis, the modification reflects the ongoing development process rather than addressing a specific bug or error fix. Therefore, the conclusion is **NotBuggy**."
voldemort,10042.json,6f1cf577b021124cd3f377420c69299819ee3c40,"@@ -1,3 +1,5 @@
-        public List<Version> execute(Store<ByteArray, byte[]> store, ByteArray key) {
+        public List<Version> execute(Store<ByteArray, byte[], byte[]> store,
+                                     ByteArray key,
+                                     byte[] transforms) {
             return store.getVersions(key);
         }",NotBuggy,"Intermediate check-in for server-side transforms
Conflicts:

	contrib/mongodb/example/MongoDBClient.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageConfiguration.java
	contrib/mongodb/src/java/voldemort/store/mongodb/MongoDBStorageEngine.java
	contrib/mongodb/test/voldemort/store/mongodb/MongoDBStorageEngineTest.java
	src/java/voldemort/client/AbstractStoreClientFactory.java
	src/java/voldemort/client/SocketStoreClientFactory.java
	src/java/voldemort/server/StoreRepository.java
	src/java/voldemort/server/storage/StorageService.java
	src/java/voldemort/store/invalidmetadata/InvalidMetadataCheckingStore.java
	src/java/voldemort/store/readonly/ReadOnlyStorageEngine.java
	src/java/voldemort/store/rebalancing/RebootstrappingStore.java
	src/java/voldemort/store/rebalancing/RedirectingStore.java
	src/java/voldemort/store/routed/RoutedStore.java
	src/java/voldemort/store/socket/SocketStore.java
	test/integration/voldemort/performance/AbstractLoadTestHarness.java
	test/integration/voldemort/performance/LocalDirectLoadTest.java
	test/integration/voldemort/performance/LocalRoutedStoreLoadTest.java
	test/integration/voldemort/performance/ReadOnlyStorePerformanceTest.java
	test/integration/voldemort/performance/RemoteHttpStoreLoadTest.java
	test/integration/voldemort/performance/RemoteStoreComparisonTest.java
	test/integration/voldemort/performance/RemoteTest.java
	test/integration/voldemort/performance/SemiLocalHttpStoreLoadTest.java
	test/unit/voldemort/server/EndToEndTest.java
	test/unit/voldemort/store/routed/ReadRepairerTest.java
	test/unit/voldemort/store/routed/RoutedStoreTest.java
	test/unit/voldemort/store/socket/AbstractSocketStoreTest.java
",NotBuggy,"The commit message suggests an intermediate check-in for server-side transformations. The code diff shows a method signature change, adding a `transforms` parameter to the `execute` method. This suggests an ongoing feature enhancement rather than a direct bug fix. The commit also mentions various conflicts, indicating that this might be part of a broader integration or addition of features. There is no direct indication of error handling improvements, logical corrections, or exception-handling adjustments related to bug fixing in this specific code change.

Given the nature of the commit message and the code diff, the purpose appears more aligned with feature development rather than addressing any specific bug. Thus, the correct conclusion for this change is **NotBuggy**."
voldemort,7698.json,7871933f0f0f056e2eeac03a01db1e9cf81f8bda,"@@ -1,3 +1,29 @@
     public boolean isCompleteRequest(ByteBuffer buffer) {
-        throw new VoldemortException(""Non-blocking server not supported for AdminServiceRequestHandler"");
+        DataInputStream inputStream = new DataInputStream(new ByteBufferBackedInputStream(buffer));
+
+        try {
+            int dataSize = inputStream.readInt();
+
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, dataSize: "" + dataSize + "", buffer position: ""
+                             + buffer.position());
+
+            if(dataSize == -1)
+                return true;
+
+            // Here we skip over the data (without reading it in) and
+            // move our position to just past it.
+            buffer.position(buffer.position() + dataSize);
+
+            return true;
+        } catch(Exception e) {
+            // This could also occur if the various methods we call into
+            // re-throw a corrupted value error as some other type of exception.
+            // For example, updating the position on a buffer past its limit
+            // throws an InvalidArgumentException.
+            if(logger.isTraceEnabled())
+                logger.trace(""In isCompleteRequest, probable partial read occurred: "" + e);
+
+            return false;
+        }
     }",NotBuggy,"Changes to support streaming in NIO-based socket server. No longer using interrupts to stop the NioSocketServer as this can interfere with server-side routing. Also deprecated voldemort.server.socket.AdminService class.
",NotBuggy,"The commit message mentions changes for streaming support in an NIO-based socket server and avoiding interrupts to stop the NioSocketServer. The diff reveals a fundamental change in handling the completion of requests within a socket server by implementing logic to check if a request is complete based on data size.

Here are the important aspects in the diff:

1. **Exception Handling Improvement**: The method now uses an `InputStream` to read an integer and determine the size of data expected. If this process encounters any exception (e.g., end of buffer or invalid operation), it assumes a partial read has occurred and returns `false`. This improves robustness over the previous implementation which threw an exception unconditionally.

2. **Data Handling**: The method reads the size of the next data segment with `readInt()` and subsequently positions the buffer to skip over the data, indicating successful reading by returning `true`. If the data size is `-1`, it also returns `true`, likely signaling a permissible state or the end of the request.

3. **Logging**: There is trace-level logging to capture the buffer's state and any exceptions that occur, which could aid in diagnosing partial reads or errors but is not directly tied to a specific bug fix.

The transition from interrupt-based stopping mechanisms to an exception-handling approach and direct data size management in the server’s buffer indicates a restructuring to improve data stream management, aligning with the commit message on enhancing server-side routing operations without explicit mentions of specific bugs.

There is no explicit indication of a bug fix; rather, this appears to focus on refactoring and improving streaming capabilities. If the original throwing of an exception was problematic in production due to incomplete request detection, this change indirectly fixes such issues by implementing a more granular exception-based approach.

Due to the lack of explicit bug fix indications and a focus on handling requests rather than correcting faulty logic or errors, this commit is more about improvement than directly addressing a bug.

Therefore, I conclude that the changes are **NotBuggy**."
cassandra,18667.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,5 +1,5 @@
-    public void close() throws IOException
+    public void close()
     {
-        if (subIterator != null)
-            subIterator.close();
+        if (iterator != null)
+            iterator.close();
     }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",NotBuggy,"In this diff, the `close` method has undergone a change in whether it throws `IOException` and in the variable it operates on. Initially, the method closes `subIterator` if it's not null, while the updated version closes `iterator`. This change suggests a logical correction in the resource management: it seems the original method was closing the wrong resource. However, since the commit message describes a ""Storage engine refactor,"" it implies structural reorganizations rather than specific bug fixes. 

The suppression of the `throws IOException` might also indicate an architectural decision during the refactoring process, as it might be handled upstream in a different way or by another mechanism now. Still, the primary nature of this commit is to refactor and reorganize, not explicitly to fix bugs.

Based on the commit message and nature of the change focusing on refactor rather than explicitly solving an issue, the assessment concluded with **NotBuggy**."
cassandra,17698.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,4 +1,4 @@
-    public int hashCode()
+    public final int hashCode()
     {
-        return Objects.hash(timestamp(), ttl(), localDeletionTime());
+        return Objects.hashCode(partitionDeletion, ranges);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",NotBuggy,"The commit message suggests that implementations related to a certain component or functionality (referred to as ""8099's"") are being simplified. The code changes modify the `hashCode` method from using `Objects.hash()` to `Objects.hashCode()`, and change what attributes are used to compute the hash code, going from `timestamp(), ttl(), localDeletionTime()` to `partitionDeletion, ranges`.

Let's break down the changes:
1. **Method Declaration**: The `hashCode()` method was made `final`. This change prevents subclasses from overriding this method, which might help maintain consistent behavior in subclasses.

2. **Hash Code Calculation**: 
   - The original version used `Objects.hash(timestamp(), ttl(), localDeletionTime());` to compute hash code, possibly based on some dynamic property values.
   - The new version uses `Objects.hashCode(partitionDeletion, ranges);`. This suggests a shift in the properties considered important for object identity or equality checks.

3. **Evaluation**:
   - **Simplification**: The change appears to be a simplification if the new properties (`partitionDeletion, ranges`) are more directly representative of the object's identity or state changes that affect consistency.
   - **Potential Bug Aspect**: The commit does not specifically indicate a bug fix, but changes to the properties in `hashCode()` can resolve logical bugs related to object comparison or usage in data structures like `HashMap`.

Overall, the focus is on simplification, and there's no direct indication in the commit message or diff of fixing an existing error in behavior. Given the lack of clear bug-fixing evidence, the conclusion would be **NotBuggy**."
cassandra,2369.json,831bebdba86ac1956852bd216a4cc62d898c87d7,"@@ -1,4 +1,4 @@
     public boolean isEmpty()
     {
-        return getDelegate().isEmpty();
+        return this.restrictions.isEmpty();
     }",NotBuggy,"Refactor Restriction hierarchy

patch by Benjamin Lerer; reviewed by Tyler Hobbs for CASSANDRA-11354
",NotBuggy,"The commit message states a refactor of the Restriction hierarchy. The code change updates the method `isEmpty()` to return `this.restrictions.isEmpty()` instead of calling `getDelegate().isEmpty()`. This suggests a direct access to the `restrictions` object instead of going through a delegate, which is typical in refactoring for simplification and clarity. There's no indication of a bug being fixed, as the functionality of checking if something is empty is preserved, but the approach to obtain that value is streamlined. 

Given the information provided, the refactor aligns well with the commit message's purpose, and there are no signs of this specific change addressing a bug. This change appears to be about improving code readability or possibly performance by removing unnecessary delegation.

Therefore, I conclude **NotBuggy**."
cassandra,5527.json,2d991a7e4cc73b522f2c8adf14b5ff37a7947427,"@@ -1,4 +1,4 @@
     public ByteBuffer serialize(UUID value)
     {
-        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : ByteBuffer.wrap(UUIDGen.decompose(value));
+        return value == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : UUIDGen.toByteBuffer(value);
     }",NotBuggy,"Minor optimizations

patch by slebresne; reviewed by blambov for CASSANDRA-10410

The patch includes the 3 minor optimizations desribed below:
1) Faster copy in CompositeType.build(). We call CompositeType.build()
   relatively often when a table has a composite partition key. When
   copying source buffers into the composite result in that method, we
   can use our existing ByteBuffer.arrayCopy() method which is supposed
   to be faster since it uses FastByteOperations and saves the
   duplication of the buffer to copy
2) Faster UUID->ByteBuffer. We convert UUID to their ByteBuffer
    representation in a few places and do that by converting the UUID to
    a byte[] and then wrapping it. But our convertion of UUID to byte[]
    writes the 2 long the UUID is composed of byte by byte, while for a
    ByteBuffer we can easily just ""put"" both long. So the patch
    introduces a specific method to do that. That's arguably cleaner
    anyway.
3) Memoize hash value for DataResource. Every request does a hash map
   lookup on a DataResource (for validating access rights) which imply
   calling DataResource.hashCode(). Currently that uses
   Objects.hashCode(), which is a varargs methods and allocate an array
   (which, according to MissionControl seems to not be stack allocated).
   The patch does 2 things: it computes the hash only once in the ctor
   and it stores the DataResource for each table in the CFMetaData and
   uses that when checking access permissions for SelectStatement and
   ModificationStatement (the 2 statements where performance matters).
   The 2nd point also saves some unecessary test that the table exists
   when we actually know it does.
",NotBuggy,"The commit message outlines three optimizations rather than bug fixes, specifying changes intended to enhance performance. Analyzing the diff:

1. The change in `serialize(UUID value)` method replaces `UUIDGen.decompose(value)` with `UUIDGen.toByteBuffer(value)`.
2. This change is consistent with item 2 of the commit message, which aims to optimize UUID to ByteBuffer conversion.
3. `UUIDGen.decompose(value)` likely converts a UUID to a byte array before wrapping it into a ByteBuffer.
4. By switching to `UUIDGen.toByteBuffer(value)`, the conversion directly places UUID data into a ByteBuffer, likely circumventing unnecessary interim byte array creation, thus enhancing performance.

The code alteration is primarily a performance improvement without any indication of a prior error or bug. Both the diff and message consistently reference optimizing a serialization process rather than fixing incorrect behavior.

Therefore, the change should be classified as **NotBuggy**."
cassandra,12544.json,490c1c27c9b700f14212d9591a516ddb8d0865c7,"@@ -1,18 +1,18 @@
         public void merge(CombinedValue<Long> other)
         {
             if (!(other instanceof Token))
                 return;
 
             Token o = (Token) other;
             assert o.get().equals(token);
 
             if (o instanceof DKToken)
             {
                 keys.addAll(((DKToken) o).keys);
             }
             else
             {
-                for (RowKey key : o)
+                for (DecoratedKey key : o)
                     keys.add(key);
             }
         }",NotBuggy,"Revert ""Add row offset support to SASI""

This reverts commit 7d857b46fb070548bf5e5f6ff81db588f08ec22a.
",Buggy,"The commit message indicates a reversion of a previous commit related to adding row offset support to SASI. The diff shows a change from `RowKey` to `DecoratedKey`. This reversion implies that the original changes may have introduced issues or bugs, leading to the decision to revert them. Reverting a commit itself is typically an indication of resolving problems introduced by the prior changes.

Here, the original change intended to support a new feature (row offset support) that somehow affected the behavior consequently. Such reverts are generally used to restore previous, non-buggy behavior until a proper solution or fix is identified for the new feature.

Given that this involves reverting potentially buggy changes that introduced issues, the changes likely align with a bug resolution effort.

Therefore, the conclusion is **Buggy**."
cassandra,11667.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,10 +1,12 @@
-    protected CBuilder buildIndexClusteringPrefix(ByteBuffer rowKey, ClusteringPrefix prefix, CellPath path)
+    public CBuilder buildIndexClusteringPrefix(ByteBuffer partitionKey,
+                                               ClusteringPrefix prefix,
+                                               CellPath path)
     {
         CBuilder builder = CBuilder.create(getIndexComparator());
-        builder.add(rowKey);
-        for (int i = 0; i < Math.min(columnDef.position(), prefix.size()); i++)
+        builder.add(partitionKey);
+        for (int i = 0; i < Math.min(indexedColumn.position(), prefix.size()); i++)
             builder.add(prefix.get(i));
-        for (int i = columnDef.position() + 1; i < prefix.size(); i++)
+        for (int i = indexedColumn.position() + 1; i < prefix.size(); i++)
             builder.add(prefix.get(i));
         return builder;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",NotBuggy,"The commit message indicates new API features and implementations, focusing on built-in indexes, and does not explicitly mention bug fixes. 

The code changes indicate several modifications:

1. **Visibility Change**:
   - The method `buildIndexClusteringPrefix` visibility is changed from `protected` to `public`. This usually relates to API changes or broadening access for new functionalities.

2. **Parameter Renaming**:
   - `rowKey` was renamed to `partitionKey`. This change could simply align with internal conventions or reflect a broader update, likely part of the new API changes.

3. **Variable Renaming**:
   - `columnDef` is renamed to `indexedColumn`. This suggests a shift in focus or aligning to better semantics according to the new index API. 

None of these modifications explicitly indicate a bug fix. The renaming and access level change appear to fit with the message about a new API, aligning the method's interface and logic to be consistent with a new design. No apparent error-handling, exception-handling improvements, or logical corrections that simplify possible bug indications are present here.

Considering the alignment of the commit message and provided diff, with no explicit signs of bug correction, the conclusion is that these amendments accommodate new API features rather than fixing bugs.

The changes do not indicate a bug fix, so the answer is **NotBuggy**."
cassandra,17797.json,c055ab997ea66faccdb10eddd5241f909ff73408,"@@ -1,5 +1,5 @@
-    public static void writeDelTime(DeletionTime dt, SerializationHeader header, DataOutputPlus out) throws IOException
+    public void writeDeletionTime(DeletionTime dt, DataOutputPlus out) throws IOException
     {
-        out.writeVInt(header.encodeTimestamp(dt.markedForDeleteAt()));
-        out.writeVInt(header.encodeDeletionTime(dt.localDeletionTime()));
+        writeTimestamp(dt.markedForDeleteAt(), out);
+        writeLocalDeletionTime(dt.localDeletionTime(), out);
     }",NotBuggy,"Minor improvements to RowStats

patch by slebresne; reviewed by JoshuaMcKenzie for CASSANDRA-9828
",NotBuggy,"The changes in the diff to the method do not correspond to any bug fixes. The refactor involves renaming the method, moving from static context, and creating helper methods for better readability or maintainability, with no clear bug fix indication. The answer is **NotBuggy**."
cassandra,4841.json,17624248efc316de125d1bc3c6be4f0cb4e174a2,"@@ -1,14 +1,10 @@
                 protected Row computeNext()
                 {
-                    try {
-                        while (currentPage == null || !currentPage.hasNext())
-                        {
-                            if (pager.isExhausted())
-                                return endOfData();
-                            currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
-                        }
-                        return new Row(metadata, currentPage.next());
-                    } catch (RequestValidationException | RequestExecutionException e) {
-                        throw new RuntimeException(e);
+                    while (currentPage == null || !currentPage.hasNext())
+                    {
+                        if (pager.isExhausted())
+                            return endOfData();
+                        currentPage = select.process(pager.fetchPage(pageSize)).rows.iterator();
                     }
+                    return new Row(metadata, currentPage.next());
                 }",NotBuggy,"Make CassandraException unchecked, extend RuntimeException

Patch by Robert Stupp, reviewed by Sylvain Lebresne for CASSANDRA-8560
",NotBuggy,"The commit message mentions making `CassandraException` unchecked by extending `RuntimeException`. However, the diff shows removal of a try-catch block and related exception handling (conversion of checked exceptions to `RuntimeException`). This suggests the upstream logic (possibly around or within `select.process`) has changed to only throw unchecked exceptions, aligning it with the commit message's purpose of changing exception handling strategy.

While this change in exception handling could be related to making `CassandraException` unchecked, it doesn't directly demonstrate fixes of logical errors or bugs in this particular snippet. The focus is on refactoring the way exceptions are managed, eliminating the need for catching specific exceptions.

Based on this analysis, the changes align with improving code style or structure rather than directly targeting a bug fix. Therefore, I conclude **NotBuggy**."
cassandra,992.json,7dffa83a0619613a17deb3f0d08858006f320e03,"@@ -1,22 +1,8 @@
-    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
+    static void serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
     {
-        boolean bVal = true;
-        int size = gDigestList.size();                        
-        dos.writeInt(size);
-        
-        int estimate = 0;            
+        dos.writeInt(gDigestList.size());
         for ( GossipDigest gDigest : gDigestList )
         {
-            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-            {
-                logger_.info(""@@@@ Breaking out to respect the MTU size in GD @@@@"");
-                bVal = false;
-                break;
-            }
-            int pre = dos.size();               
             GossipDigest.serializer().serialize( gDigest, dos );
-            int post = dos.size();
-            estimate = post - pre;
         }
-        return bVal;
     }",NotBuggy,"remove obsolete gossip size limit.  patch by Anthony Molinaro and jbellis for CASSANDRA-1138

git-svn-id: https://svn.apache.org/repos/asf/cassandra/trunk@957593 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the removal of an obsolete gossip size limit. The code diff shows changes that align with this message:

1. The method `serialize` originally returned a boolean indicating whether the serialization respected a size (MTU) limit based on `Gossiper.MAX_GOSSIP_PACKET_SIZE`. This restriction has been removed.

2. The functionality checking the size (`estimate` and related conditions) and returning a boolean to signal if the size limit requirement was met have also been removed. Now, the method directly writes the integer size of the list and serializes each `GossipDigest` without checking or affecting serialization based on size constraints.

3. The removal of the MTU size constraint aligns with the commit message about eliminating an ""obsolete"" gossip size limit.

These changes support the commit message's claim as they clearly demonstrate the removal of a functionality that limits gossip message size based on an MTU requirement.

Thus, the changes match the commit message about removing an obsolete feature, which was not indicated as a bug but as a cleanup or update to align with current requirements or expectations. Hence, the changes do not inherently indicate a bug fix but rather a feature removal.

Given this analysis, the conclusion is **NotBuggy**."
cassandra,10965.json,59b5b6bef0fa76bf5740b688fcd4d9cf525760d0,"@@ -1,4 +1,4 @@
     public String getDatacenter(String host) throws UnknownHostException
     {
-        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddress.getByName(host));
+        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddressAndPort.getByName(host));
     }",NotBuggy,"Allow storage port to be configurable per node

Patch by Ariel Weisberg; Reviewed by Jason Brown for CASSANDRA-7544
",NotBuggy,"The commit message describes the change as making the storage port configurable per node. The code modification updates a method call from `InetAddress.getByName(host)` to `InetAddressAndPort.getByName(host)`. 

This change supports the commit message because it suggests that the address resolution now handles both the IP address and the port, thereby making the port part of the configuration. Although the message doesn't explicitly state a bug fix, the change could address limitations in how nodes' ports were handled, which aligns with making ports configurable per node.

However, since the commit is not explicitly fixing an incorrect behavior or logical error in the code but rather extending functionality for configuration purposes, it doesn't represent a typical bug fix.

Therefore, the answer is **NotBuggy**."
cassandra,11631.json,0626be8667aefdcf50a051471f83da90bbae9bcf,"@@ -1,4 +1,7 @@
-    protected ByteBuffer getIndexedValue(ByteBuffer rowKey, Clustering clustering, ByteBuffer cellValue, CellPath path)
+    public ByteBuffer getIndexedValue(ByteBuffer partitionKey,
+                                      Clustering clustering,
+                                      CellPath path,
+                                      ByteBuffer cellValue)
     {
         return cellValue;
     }",NotBuggy,"New 2i API and implementations for built in indexes

Patch by Sam Tunnicliffe; reviewed by Sergio Bossa, Sylvain Lebresne and
Aleksey Yeschenko for CASSANDRA-9459

Conflicts:
	src/java/org/apache/cassandra/db/ColumnFamilyStore.java
	src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
	src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
	src/java/org/apache/cassandra/db/index/AbstractSimplePerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/PerColumnSecondaryIndex.java
	src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndex.java
	src/java/org/apache/cassandra/db/index/composites/CompositesIndexOnClusteringKey.java
	src/java/org/apache/cassandra/db/partitions/AtomicBTreePartition.java
	src/java/org/apache/cassandra/db/partitions/PartitionUpdate.java
	src/java/org/apache/cassandra/index/internal/ColumnIndexSearcher.java
	src/java/org/apache/cassandra/index/internal/composites/CompositesSearcher.java
	src/java/org/apache/cassandra/index/internal/keys/KeysSearcher.java
	src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java
	test/unit/org/apache/cassandra/schema/DefsTest.java
	test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
",NotBuggy,"The commit message does not mention a bug fix; instead, it discusses adding a new 2i (secondary index) API and implementations for built-in indexes in Cassandra. The diff showcases a change in the method signature of `getIndexedValue`, altering the parameter order and scope from `protected` to `public`. This change seems aligned with providing broader access to this method, likely for integration into newly added features or APIs related to secondary indexes.

There is no indication that this change addresses a specific bug related to incorrect behavior, error-handling, or logical issues with data handling. Instead, it seems more related to enhancing or refactoring the code to fit new architectural requirements or designs needed for the new API functionalities.

Thus, since there's no clear fix for erroneous behavior in the code depicted in the diff, the changes do not indicate a bug fix. The commit should be classified as **NotBuggy**."
cassandra,20574.json,2457599427d361314dce4833abeb5cd4915d0b06,"@@ -1,5 +1,4 @@
     public Mutation makeMutation()
     {
-        assert update != null;
         return new Mutation(update);
     }",NotBuggy,"Simplify some 8099's implementations

patch by slebresne; reviewed by iamalesky for CASSANDRA-9705
",NotBuggy,"The change removes an assertion that checks if `update` is not `null` before creating a `Mutation` object. The removal of the assertion indicates a simplification of the code, possibly because the `update` being null-safe is handled elsewhere, or it is guaranteed by the program's logic. This change might affect error-handling by allowing a `null` value, but it does not directly address a logic or runtime defect. The commit message focuses on simplification rather than a bug fix. The removal of the assertion does not explicitly indicate a bug resolution, such as a fixed null pointer exception; it rather suggests a design or refactoring decision.

The changes appear to be aligned with the simplification stated in the commit message. There are no further indicators that the modification is related to fixing a bug.

The conclusion is **NotBuggy**."
cassandra,4540.json,35668435090eb47cf8c5e704243510b6cee35a7b,"@@ -1,14 +1,14 @@
     public boolean equals(Object o)
     {
         if (!(o instanceof UDAggregate))
             return false;
 
         UDAggregate that = (UDAggregate) o;
         return Objects.equal(name, that.name)
-            && Functions.typeEquals(argTypes, that.argTypes)
-            && Functions.typeEquals(returnType, that.returnType)
+            && Functions.typesMatch(argTypes, that.argTypes)
+            && Functions.typesMatch(returnType, that.returnType)
             && Objects.equal(stateFunction, that.stateFunction)
             && Objects.equal(finalFunction, that.finalFunction)
             && Objects.equal(stateType, that.stateType)
             && Objects.equal(initcond, that.initcond);
     }",NotBuggy,"Improve handling of UDA and UDF metadata

patch by Aleksey Yeschenko; reviewed by Robert Stupp for CASSANDRA-9665
",NotBuggy,"The commit message indicates improvements in handling UDA (User-Defined Aggregate) and UDF (User-Defined Function) metadata. The diff shows a modification in the equality check logic for UDAggregate objects by replacing `Functions.typeEquals` with `Functions.typesMatch`. This change suggests an improvement or correction in how function argument types and return types are compared. 

The context of the change indicates a refinement in the method used for type comparison, likely to enhance accuracy or consistency in equality checks. The improved handling of metadata could imply fixing potential inaccuracy or ensuring a more robust comparison mechanism. However, without additional context or documentation on the behavior difference between `typeEquals` and `typesMatch`, it's difficult to definitively classify this as a bug fix.

Based on the provided information, the change aims to improve function behavior but lacks explicit details on correcting erroneous behavior, a typical indicator of a bug fix. Therefore, this is more aligned with an enhancement rather than a direct bug fix.

Thus, the conclusion is **NotBuggy**."
cassandra,20466.json,af3fe39dcabd9ef77a00309ce6741268423206df,"@@ -1,5 +1,9 @@
         public Builder add(KeyspaceMetadata keyspace)
         {
             keyspaces.put(keyspace.name, keyspace);
+
+            keyspace.tables.forEach(t -> tables.put(t.id, t));
+            keyspace.views.forEach(v -> tables.put(v.metadata.id, v.metadata));
+
             return this;
         }",NotBuggy,"Make TableMetadata immutable, optimize Schema

patch by Aleksey Yeschenko; reviewed by Sylvain Lebresne for
CASSANDRA-9425
",NotBuggy,"The commit message indicates making `TableMetadata` immutable and optimizing `Schema`. The diff shows adding tables and views to a map within the `add` method of a builder, iterating over keyspace metadata. This change aligns with optimizing how tables and views are managed within a schema, potentially contributing to the immutability goal by encapsulating the tables and views data within a single process.

However, there's no clear correction of an error or improvement in exception handling in the diff to suggest that it addresses a bug in the code. There are no changes indicative of fixing a bug like handling improper data states or exceptions. Instead, these changes seem to be focused more on refactoring and optimizing the organization of data as stated in the commit message.

Therefore, given that the change pertains to organization and optimization rather than directly addressing a bug, the classification for this commit would be **NotBuggy**."
cassandra,16107.json,a991b64811f4d6adb6c7b31c0df52288eb06cf19,"@@ -1,4 +1,4 @@
-    public int remainingCount()
-    {
-        return ctype.size() - size;
-    }

+        public int remainingCount()
+        {
+            return values.length - size;
+        }",NotBuggy,"Storage engine refactor, a.k.a CASSANDRA-8099

Initial patch, see ticket for details
",Buggy,"The commit message describes this change as a refactor related to a storage engine, specifically linked to a ticket CASSANDRA-8099. The code change in the diff modifies the method `remainingCount()` to calculate the remaining count differently. Originally, it used `ctype.size() - size`, but it has been changed to `values.length - size`.

To determine if this is a bug fix, we should consider whether the change corrects a logical error in how the remaining count was previously calculated:

1. **Potential Bug Fix**:
   - **Original Calculation**: `ctype.size() - size` suggests that `ctype.size()` might have been incorrectly used if `ctype` does not accurately represent the length or the intended context for remaining items.
   - **Updated Calculation**: `values.length - size` indicates a change to use the length of an array `values`, which might be the correct source of truth for the number of items.

2. **Context Indication**: Without additional context (like what `ctype` and `values` exactly represent), the modification might aim to align the calculation with the true source of remaining counts, suggesting a correction to what could have been a logical error previously.

Since the commit message refers to a refactor, it might be addressing several improvements, not strictly bug fixes. However, the code suggests a correction to what data should be referenced in `remainingCount()`. Considering the possibility of an error in using `ctype.size()`, this change is likely a bug fix in logic.

Therefore, based on the above reasoning, the answer is **Buggy**."
jgit,6893.json,6e05d98cce318056f95700e562cec6b68fcf7475,"@@ -1,4 +1,5 @@
 	public CloneCommand setDirectory(File directory) {
+		validateDirs(directory, gitDir, bare);
 		this.directory = directory;
 		return this;
 	}",NotBuggy,"Allow explicit configuration of git directory in CloneCommand

This feature is needed to support the new submodule layout where the
.git folder of the submodules is under .git/modules/<submodule>.

Change-Id: If5f13426cfd09b7677e23478e9700c8c25a6dae5
",NotBuggy,"The commit message talks about introducing a feature to allow explicit configuration of the `.git` directory in `CloneCommand`, which supports a new submodule layout. The code change adds a call to `validateDirs()`, which likely ensures the directory's validity according to the new feature's requirements. There's no indication in the commit message or the code changes that this update is addressing a bug. Instead, the change appears related to new functionality. Hence, this is a feature addition rather than a bug fix. The answer is **NotBuggy**."
jgit,4585.json,683bd09092e90aef5b7cf963355995d76aefa439,"@@ -1,3 +1,3 @@
 		public CompressedBitmap andNot(Bitmap other) {
-			return new CompressedBitmap(bitmap.andNot(bitmapOf(other)));
+			return new CompressedBitmap(bitmap.andNot(ewahBitmap(other)), bitmapIndex);
 		}",NotBuggy,"Make BitmapIndexImpl.CompressedBitmap, CompressedBitmapBuilder static

A CompressedBitmap represents a pair (EWAH bit vector, PackIndex
assigning bit positions to git objects).  The bit vector is a member
field and the PackIndex is implicit via the 'this' reference to the
outer class.

Make this clearer by making CompressedBitmap a static class and
replacing the 'this' reference by an explicit field.

Likewise for CompressedBitmapBuilder.

Change-Id: Id85659fc4fc3ad82034db3370cce4cdbe0c5492c
Suggested-by: Terry Parker <tparker@google.com>
",NotBuggy,"The commit message focuses on making inner classes static and clarifying the relationship between components. It describes structural changes, not bug fixes. The diff shows an adjustment in method call arguments to use `ewahBitmap(other)` instead of `bitmapOf(other)` and adds `bitmapIndex` as a second argument in the `CompressedBitmap` constructor. 

While this change may correct some functionality, there's no explicit indication of handling errors or fixing incorrect logic based on the commit message. The change aligns more with the message's structural intent rather than an indication of fixing bugs.

Thus, the change should be categorized as **NotBuggy**."
jgit,319.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	protected boolean containsHelp(final String... args) {
+	protected boolean containsHelp(String... args) {
 		return TextBuiltin.containsHelp(args);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,The commit describes removing 'final' from method parameters. This change removes 'final' from the parameters of the `containsHelp` method. This modification doesn't affect functionality or fix any bugs; it adjusts code style. No error or logical correction is evident. The answer is **NotBuggy**.
jgit,1651.json,1f2022e3a7e9482caa4823e031504b6bbe8246af,"@@ -1,6 +1,6 @@
 		public void onSendPack(UploadPack up,
 				Collection<? extends ObjectId> wants,
 				Collection<? extends ObjectId> haves)
-				throws UploadPackMayNotContinueException {
+				throws ServiceMayNotContinueException {
 			// Do nothing.
 		}",NotBuggy,"Modify refs in UploadPack/ReceivePack using a hook interface

This is intended to replace the RefFilter interface (but does not yet,
for backwards compatibility). That interface required lots of extra
scanning and copying in filter cases such as only advertising a subtree
of the refs directory. Instead, provide a hook that can be executed
right before ref advertisement, using the public methods on
UploadPack/ReceivePack to explicitly set the map of advertised refs.

Change-Id: I0067019a191c8148af2cfb71a675f2258c5af0ca
",NotBuggy,"The commit focuses on modifying refs in UploadPack/ReceivePack through a hook, aiming to optimize ref filtering. The change involves replacing the exception type thrown by onSendPack from UploadPackMayNotContinueException to ServiceMayNotContinueException.

Here's the analysis:

1. **Commit Message Analysis**:
   - The commit aims to replace the `RefFilter` interface with a more efficient hook interface. There's no direct reference to fixing a bug, but rather it is about improving or changing the functionality.

2. **Code Change Analysis**:
   - The change in the exception type suggests that `onSendPack` could be part of a broader refactor or functionality update, possibly to harmonize exception handling across service methods (`ServiceMayNotContinueException` may be intended to be more generic or appropriate in this context).
   - There is no specific error handling or bug fix documented in this specific line of code. The change generalized the exception possibly for consistency or design purposes without mentioning a resolution to any incorrect behavior.

Given this analysis, the commit message aligns with the code change as it talks about modifying interfaces and does not specifically indicate a bug fix; thus, it doesn't directly imply a fix to incorrect behavior or a bug. The nature of the change appears to be more of a structure modification than a specific bug resolution.

Thus, this change does not indicate a code bug fix. The answer is **NotBuggy**."
jgit,3458.json,89b91ad40677c7048d31925986e48150e975bcce,"@@ -1,3 +1,3 @@
-			public InMemoryRepository build() throws IOException {
-				throw new UnsupportedOperationException();
-			}

+		public InMemoryRepository build() throws IOException {
+			return new InMemoryRepository(this);
+		}",NotBuggy,"InMemoryRepository: Use a real Builder class

Change-Id: I161b98a58503415955a21f2720395611f439ce98
",NotBuggy,"The commit message indicates the introduction of a real Builder class for `InMemoryRepository`. The diff shows that the `build()` method has been modified from throwing an `UnsupportedOperationException` to actually constructing and returning an `InMemoryRepository` instance using `this` as its argument.

### Analysis:
- **Original Code**: The method `build()` previously raised an `UnsupportedOperationException`, meaning it was not implemented before and using this method would always result in an exception.
- **Modified Code**: The method now constructs and returns a new instance of `InMemoryRepository`, indicating the builder pattern is now functioning correctly.

### Relevance to Commit Message:
- The change aligns with the commit message, which mentions using a ""real Builder class"". Implementing the `build()` method to return an instance of the class supports this.
- The commit does not directly state a bug fix, but the previous implementation prevented building which could be seen as a functional limitation or an incomplete feature rather than a bug per se.

### Conclusion:
Though this modification fixes previously non-functional code by implementing the builder, it's more about completing a feature rather than correcting an incorrect behavior (bug). Thus, it is an enhancement, not a bug fix. Therefore, the conclusion is **NotBuggy**."
jgit,1828.json,eadfcd3ec166c55c1ff3f3fe0b5e97dd94ff8d83,"@@ -1,8 +1,8 @@
-	private static boolean abort(Iterable<Command> cmdList) {
-		for (Command cmd : cmdList) {
-			if (cmd.getResult() == NOT_ATTEMPTED) {
-				reject(cmd, JGitText.get().transactionAborted);
+	public static void abort(Iterable<ReceiveCommand> commands) {
+		for (ReceiveCommand c : commands) {
+			if (c.getResult() == NOT_ATTEMPTED) {
+				c.setResult(REJECTED_OTHER_REASON,
+						JGitText.get().transactionAborted);
 			}
 		}
-		return false;
 	}",NotBuggy,"ReceiveCommand.abort(): Utility to mark batch of commands as failed

If one or more commands is failing the entire group usually has to
also fail with ""transaction aborted"". Pull this loop into a helper
so the idiom can be easily reused in several places throughout JGit.

Change-Id: I3b9399b7e26ce2b0dc5f7baa85d585a433b4eaed
",NotBuggy,"The commit message states the intent to provide a utility function for marking a batch of commands as failed with ""transaction aborted."" Analyzing the diff:

1. **Method Changes:** The method has been transformed from a private instance method returning a boolean to a public static void method named `abort`. The new version is intended to be more reusable.

2. **Type Consistency:** The parameter type has changed from `Iterable<Command>` to `Iterable<ReceiveCommand>`, indicating the change should align with the intended use of the specific `ReceiveCommand` type in JGit.

3. **Operation Change:** 
   - In the new implementation, instead of calling a local `reject` function, it directly sets the command's result using `c.setResult(REJECTED_OTHER_REASON, JGitText.get().transactionAborted)`.
   - The return type has been removed indicating no need for a value return, suggesting more emphasis on a side effect of modifying the state of `ReceiveCommand` objects directly.

4. **Implementation Reasoning**: 
   - The change aligns well with the commit message's intent, i.e., when any command in the loop is not attempted, the function marks it as rejected due to 'transaction aborted'. This is consistent with the need to mark a batch of operations collectively as failed if any single operation hasn't been attempted.
   - This does not fix an explicit bug but promotes code reuse by modularizing the failure mechanism.

Considering there is no explicit fix for a logic error, exception, or runtime defect, and instead focuses on providing a generalized utility to support aborting transactions, the logical indication of a fix isn't evident outside of these refactorings geared towards better utility.

Based on this assessment, the answer is **NotBuggy**."
jgit,5753.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,7 +1,7 @@
-	static RawCharSequence textFor(final RevCommit cmit) {
+	static RawCharSequence textFor(RevCommit cmit) {
 		final byte[] raw = cmit.getRawBuffer();
 		final int b = RawParseUtils.commitMessage(raw, 0);
 		if (b < 0)
 			return RawCharSequence.EMPTY;
 		return new RawCharSequence(raw, b, raw.length);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of unnecessary 'final' keywords from various code constructs, such as package-private functions, try blocks, and for loops. The provided diff shows the removal of the 'final' keyword from the parameter of the method `textFor`:

```java
-static RawCharSequence textFor(final RevCommit cmit) {
+static RawCharSequence textFor(RevCommit cmit) {
```

This change aligns with the commit message, as it is an example of removing a 'final' keyword from a method's parameter. The 'final' keyword in a method parameter is used to indicate that the parameter reference cannot be changed within the method. However, its necessity depends on the context and coding guidelines. The removal here does not affect the functionality or fix a specific bug related to the parameter's immutability; instead, it is likely a stylistic or readability choice.

Since the commit is described as a removal of unnecessary 'final' keywords, with no indication in the diff of fixing a bug or error, this change and the commit focus on code style rather than functional fixes.

Therefore, the classification of this commit is **NotBuggy**."
jgit,5925.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,20 +1,20 @@
-	private void insertChild(final int stIdx, final DirCacheTree st) {
+	private void insertChild(int stIdx, DirCacheTree st) {
 		final DirCacheTree[] c = children;
 		if (childCnt + 1 <= c.length) {
 			if (stIdx < childCnt)
 				System.arraycopy(c, stIdx, c, stIdx + 1, childCnt - stIdx);
 			c[stIdx] = st;
 			childCnt++;
 			return;
 		}
 
 		final int n = c.length;
 		final DirCacheTree[] a = new DirCacheTree[n + 1];
 		if (stIdx > 0)
 			System.arraycopy(c, 0, a, 0, stIdx);
 		a[stIdx] = st;
 		if (stIdx < n)
 			System.arraycopy(c, stIdx, a, stIdx + 1, n - stIdx);
 		children = a;
 		childCnt++;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of `final` in parameter lists, which aligns with the changes in the diff where `final` has been removed from the parameters of the method `insertChild`. This change doesn't imply a bug fix but rather indicates a cleanup or style preference adjustment. The code behavior remains the same, and there are no signs of error, exception, or logical corrections that would suggest a bug fix.

Therefore, the changes do not relate to bug fixing, and the assessment is **NotBuggy**."
jgit,1557.json,d4e7b7060610678e863fbf091746ae1fcc240b0d,"@@ -1,18 +1,18 @@
 		byte[] toArray() throws IOException {
 			try {
 				if (length >= 0) {
 					final byte[] r = new byte[(int) length];
-					NB.readFully(in, r, 0, r.length);
+					IO.readFully(in, r, 0, r.length);
 					return r;
 				}
 
 				final ByteArrayOutputStream r = new ByteArrayOutputStream();
 				final byte[] buf = new byte[2048];
 				int n;
 				while ((n = in.read(buf)) >= 0)
 					r.write(buf, 0, n);
 				return r.toByteArray();
 			} finally {
 				in.close();
 			}
 		}",NotBuggy,"Move pure IO utility functions to a utility class of its own.

According the javadoc, and implied by the name of the class, NB
is about network byte order. The purpose of moving the IO only,
and non-byte order related functions to another class is to
make it easier for new contributors to understand that they
can use these functions in general and it's also makes it easier
to understand where to put new IO related utility functions

Change-Id: I4a9f6b39d5564bc8a694b366e7ff3cc758c5181b
Signed-off-by: Robin Rosenberg <robin.rosenberg@dewire.com>
Signed-off-by: Shawn O. Pearce <spearce@spearce.org>
",NotBuggy,"The commit message explains that IO utility functions are being relocated to enhance code clarity and separation of concerns. The purpose is to make the network byte order aspect of the original class more focused by moving purely IO-related functions elsewhere. 

In this diff, the only change is replacing `NB.readFully` with `IO.readFully`. This change aligns with the commit message, which indicates a structural code improvement, not a bug fix or error correction.

The commit aims at organizing the code better without addressing any bugs. The focus is on making it easier for contributors to identify where IO functions can be found and used, and where to place new utility functions. There is no indication of a bug fix related to logic or error handling.

Given the context and the description provided in the commit message, the primary goal is code maintenance and organization, rather than solving a specific programming error.

Therefore, the conclusion is **NotBuggy**."
jgit,8136.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,17 +1,17 @@
-	public static Boolean toBooleanOrNull(final String stringValue) {
+	public static Boolean toBooleanOrNull(String stringValue) {
 		if (stringValue == null)
 			return null;
 
 		if (equalsIgnoreCase(""yes"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""true"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""1"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""on"", stringValue)) //$NON-NLS-1$
 			return Boolean.TRUE;
 		else if (equalsIgnoreCase(""no"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""false"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""0"", stringValue) //$NON-NLS-1$
 				|| equalsIgnoreCase(""off"", stringValue)) //$NON-NLS-1$
 			return Boolean.FALSE;
 		else
 			return null;
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the 'final' keyword from parameter lists, which aligns with the code change seen in the diff. The code modification does not include any adjustments to logic, error handling, or exception handling that would suggest a bug fix. It is a simple refactoring effort to remove unnecessary use of 'final' from method parameters, which might improve readability or comply with coding standards but does not impact functionality or correct a bug.

Therefore, this change does not indicate a bug fix, leading to the conclusion: **NotBuggy**."
jgit,1282.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,4 +1,4 @@
-	public void setPushRefSpecs(final List<RefSpec> specs) {
+	public void setPushRefSpecs(List<RefSpec> specs) {
 		push.clear();
 		push.addAll(specs);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of the `final` keyword from parameter lists, which is evident in the given code diff where `final` is removed from the `specs` parameter in the `setPushRefSpecs` method. This change is consistent with the commit message and is typically done for readability or style reasons rather than bug fixing.

There is no indication of a bug being fixed in the logic of the code. The method's functionality seems unchanged, focusing solely on the method parameter's immutability as it interacts with the passed-in list. Removing `final` doesn't impact the method's execution or address any specific functionality issue.

Therefore, the change does not reflect a bug fix but is rather a style change regarding method parameters. The correct conclusion is **NotBuggy**."
jgit,348.json,f3ec7cf3f0436a79e252251a31dbc62694555897,"@@ -1,3 +1,3 @@
-	void enableBoundary(final boolean on) {
+	void enableBoundary(boolean on) {
 		enableRevSort(RevSort.BOUNDARY, on);
 	}",NotBuggy,"Remove further unnecessary 'final' keywords

Remove it from

 * package private functions.

 * try blocks

 * for loops

this was done with the following python script:

$ cat f.py
import sys
import re
import os

def replaceFinal(m):
  return m.group(1) + ""("" +  m.group(2).replace('final ', '') + "")""

methodDecl = re.compile(r""^([\t ]*[a-zA-Z_ ]+)\(([^)]*)\)"")

def subst(fn):
  input = open(fn)
  os.rename(fn, fn + ""~"")

  dest = open(fn, 'w')
  for l in input:
    l = methodDecl.sub(replaceFinal, l)
    dest.write(l)
  dest.close()


for root, dirs, files in os.walk(""."", topdown=False):
  for f in files:
    if not f.endswith('.java'):
      continue

    full = os.path.join(root, f)
    print full
    subst(full)

Change-Id: If533a75a417594fc893e7c669d2c1f0f6caeb7ca
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the intention to remove unnecessary 'final' keywords from various parts of the Java code such as package-private methods, try blocks, and for loops. The change seen in the diff aligns with this intention, as it removes the 'final' keyword from a method parameter declaration.

This modification does not change the functionality or behavior of the code, as 'final' on a method parameter only prevents reassignment within the method and is not critical to execution or logic. There is no bug fix involved here; the change is purely a code style or cleanup modification.

Therefore, based on the analysis of the commit message and the provided diff, this commit does not indicate a bug fix. The answer is **NotBuggy**."
jgit,1850.json,6d370d837c5faa7caff2e6e3e4723b887f2fbdca,"@@ -1,3 +1,3 @@
-	public void setResult(final Result s) {
+	public void setResult(Result s) {
 		setResult(s, null);
 	}",NotBuggy,"Remove 'final' in parameter lists

Change-Id: Id924f79c8b2c720297ebc49bf9c5d4ddd6d52547
Signed-off-by: Han-Wen Nienhuys <hanwen@google.com>
",NotBuggy,"The commit message indicates the removal of 'final' from method parameters. The diff aligns with the commit message, showing removal of 'final' from the parameter in the `setResult` method. There is no indication of error handling updates or logical corrections pointing to a bug fix. This modification appears to be focused on style or code maintenance rather than addressing a bug. Thus, the answer is **NotBuggy**."
jgit,7804.json,effc2f34b8fe282ac26f7eb3be028bdfab651ee3,"@@ -1,8 +1,8 @@
-	private boolean isBinary(Entry entry) throws IOException {
+	private static boolean isBinary(Entry entry) throws IOException {
 		InputStream in = entry.openInputStream();
 		try {
 			return RawText.isBinary(in);
 		} finally {
 			safeClose(in);
 		}
 	}",NotBuggy,"[findBugs] Declare some private methods of WorkingTreeIterator static

Change-Id: I09cd39c367f408b5a963ff004f235f558990f338
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>",NotBuggy,"The commit message describes a code quality improvement rather than a bug fix. The method isBinary is modified from an instance method to a static method. The method does not access any instance-specific fields, so it's eligible to be declared static, which is a more appropriate design decision if the method only relies on its parameters and doesn't alter object state. However, this change does not imply a correction of erroneous behavior or a bug in logic. Therefore, the change aligns with the commit message but does not indicate a bug fix.

Thus, the conclusion is **NotBuggy**."
jgit,1077.json,17fb542e9eaca9dffc4c84a6d23608dadb4b1ea1,"@@ -1,4 +1,4 @@
 		public String getMessage() {
 			return MessageFormat.format(JGitText.get().largeObjectExceedsLimit,
-					getObjectName(), limit, size);
+					getObjectName(), Long.valueOf(limit), Long.valueOf(size));
 		}",NotBuggy,"Remove 86 boxing warnings

Use Integer, Character, and Long valueOf methods when
passing parameters to MessageFormat and other places
that expect objects instead of primitives

Change-Id: I5942fbdbca6a378136c00d951ce61167f2366ca4
",NotBuggy,"The diff shows changes in how parameters are passed to `MessageFormat.format`. In the original code, primitive `long` values are passed directly, but the updated code uses `Long.valueOf()` to convert these primitives to `Long` objects. This change addresses boxing warnings, where a primitive is used in a context that requires an object, like MessageFormat.

The commit message claims it removes 86 boxing warnings, and the code changes are consistent with this objective, focusing on converting primitives to their boxed object equivalents. There's no indication of a bug or logical error being fixed in this part of the code—only a removal of warnings related to auto-boxing.

Thus, the changes align with the commit message, tackling boxing warnings rather than correcting a logic or bug-related issue. The modifications here do not indicate a bug fix but rather adherence to better coding practices.

In conclusion, this is **NotBuggy**."
weka,8307.json,9aaad03db32231667841ba32d5c424452de6029f,"@@ -1,18 +1,20 @@
   public void updateClassifier(Instance instance) throws Exception {
-    if (m_Filter.numPendingOutput() > 0)
+    if (m_Filter.numPendingOutput() > 0) {
       throw new Exception(""Filter output queue not empty!"");
+    }
     if (!m_Filter.input(instance)) {
-      if (m_Filter.numPendingOutput() > 0)
+      if (m_Filter.numPendingOutput() > 0) {
         throw new Exception(""Filter output queue not empty!"");
+      }
 
       // nothing to train on if the filter does not make an instance available
       return;
       // throw new
       // Exception(""Filter didn't make the train instance immediately available!"");
     }
 
     m_Filter.batchFinished();
     Instance newInstance = m_Filter.output();
 
     ((UpdateableClassifier) m_Classifier).updateClassifier(newInstance);
   }",NotBuggy,"Now implements UpdateableBatchProcessor

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@11099 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The changes in the code are focused on enhancing readability and consistency by adding braces to the if statements. This update does not alter the logic or fix any potential bugs. The commit message indicates that the changes are about implementing an UpdateableBatchProcessor, unrelated to the existing code snippet. The adjustments in the code do not relate to fixing bugs, adding exception handling, or logical corrections. Therefore, this commit does not seem to focus on addressing a bug.

**NotBuggy**"
weka,31489.json,6567cae2752e3d3cbfb7eefd7f1f5230c3f800fa,"@@ -1,3 +1,3 @@
   public String getRevision() {
-    return RevisionUtils.extract(""$Revision: 1.7 $"");
+    return RevisionUtils.extract(""$Revision: 1.8 $"");
   }",NotBuggy,"Now uses the current class loader rather than the system class loader to load icons


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@4400 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The change in the diff is updating a revision string from `1.7` to `1.8`. This modification doesn't relate to class loader usage or loading icons, as stated in the commit message. No bug fix is apparent concerning the class loader. The answer is **NotBuggy**."
weka,25251.json,19e3b5a26bbba1eeced04be767b1839216872d5f,"@@ -1,7 +1,8 @@
   public int graphType() {
-    
-    if (m_Classifier instanceof Drawable)
-      return ((Drawable)m_Classifier).graphType();
-    else 
+
+    if (m_Classifier instanceof Drawable) {
+      return ((Drawable) m_Classifier).graphType();
+    } else {
       return Drawable.NOT_DRAWABLE;
+    }
   }",NotBuggy,"Further cleanup of option handling and general code cleanup, removing warnings.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10153 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions clean-up and warning removal. The code changes only add curly braces to improve readability and consistency based on coding standards, but they do not alter functionality or correctness of the existing implementation. There are no indications of bug fixes, logical corrections, or error-handling improvements. The message and changes reflect refactoring and code quality improvements, not bug fixes. Therefore, this commit does not reflect a bug fix.

**NotBuggy**"
weka,4982.json,1d7e46e21d7fb5c04f903c0b15f5348eaf41faaa,"@@ -1,19 +1,13 @@
   protected Vector computeMean(Instances data, double[] totalWeight, int aI) {
 
     Vector meanVector = new DenseVector(data.numAttributes() - 1);
     totalWeight[aI] = 0;
     for (Instance inst : data) {
       if (!inst.classIsMissing()) {
-        int index = 0;
-        for (int j = 0; j < inst.numAttributes(); j++) {
-          if (j != inst.classIndex()) {
-            meanVector.add(index, inst.weight() * inst.value(index));
-            index++;
-          }
-        }
+        meanVector.add(inst.weight(), instanceToVector(inst));
         totalWeight[aI] += inst.weight();
       }
     }
     meanVector.scale(1.0 / totalWeight[aI]);
     return meanVector;
   }",NotBuggy,"Further progress but probably not correct yet.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13005 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"This commit message states that the changes are ""probably not correct yet,"" which suggests a work-in-progress change rather than a definitive bug fix. The diff includes a significant modification inside the `computeMean` method: the iterative calculation and update to `meanVector` have been replaced by a call to `instanceToVector(inst)`.

This change clearly alters the logic of computing the mean vector. Previously, the mean was calculated by iteratively adding weighted attribute values from instances, excluding the class attribute. The new code indicates a shift to a presumably consolidated method, `instanceToVector(inst)`, to handle the conversion of an instance to a vector format before updating the `meanVector`.

However, without knowing the exact implementation of `instanceToVector()`, it is difficult to ascertain if the new approach accurately fulfills the intended calculation of the mean vector, especially since the commit message itself reflects uncertainty regarding correctness.

Given the message's acknowledgment of potential issues and the significant alteration to the data processing logic, this change is likely not targeted specifically at a known bug but rather represents ongoing development or refactoring.

Thus, based on the information available, the conclusion is **NotBuggy**."
weka,4307.json,99f772d675df4a96af5dd8b8e60e50c3fc90ce78,"@@ -1,26 +1,26 @@
-  public void setMatrix ( double[] v, boolean columnFirst ) {
+  public void setMatrix(double[] v, boolean columnFirst) {
     try {
-      if( v.length != m * n ) 
-	throw new IllegalArgumentException(""sizes not match."");
-      int i, j, count = 0;
-      if( columnFirst ) {
-	for( i = 0; i < m; i++ ) {
-	  for( j = 0; j < n; j++ ) {
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      if (v.length != m * n) {
+        throw new IllegalArgumentException(""sizes not match."");
       }
-      else {
-	for( j = 0; j < n; j++ ) {
-	  for( i = 0; i < m; i++ ){
-	    A[i][j] = v[count];
-	    count ++;
-	  }
-	}
+      int i, j, count = 0;
+      if (columnFirst) {
+        for (i = 0; i < m; i++) {
+          for (j = 0; j < n; j++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
+      } else {
+        for (j = 0; j < n; j++) {
+          for (i = 0; i < m; i++) {
+            A[i][j] = v[count];
+            count++;
+          }
+        }
       }
 
-    } catch( ArrayIndexOutOfBoundsException e ) {
-      throw new ArrayIndexOutOfBoundsException( ""Submatrix indices"" );
+    } catch (ArrayIndexOutOfBoundsException e) {
+      throw new ArrayIndexOutOfBoundsException(""Submatrix indices"");
     }
   }",NotBuggy,"Code clean-up: generic type arguments, elimination of FastVector, clean-up of option handling.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10374 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"This commit message indicates a code clean-up, not a bug fix. The changes primarily involve stylistic improvements: fixing spacing issues, aligning brackets, and enhancing readability. No logic errors or bug-related issues are addressed, as the core functionality remains unchanged. Additionally, the catch block was only reformatted but not logically altered. Therefore, the analysis concludes with **NotBuggy**."
weka,27566.json,7d9903c92f07f756915f15332672dfdcaf118c5d,"@@ -1,32 +1,39 @@
   public void buildClassifier(Instances data) throws Exception {
   
-    if (!data.classAttribute().isNumeric()) {
-      throw new Exception(""Class attribute has to be numeric for regression!"");
-    }
-    if (data.numInstances() == 0) {
-      throw new Exception(""No instances in training file!"");
-    }
-    if (data.checkForStringAttributes()) {
-      throw new Exception(""Can't handle string attributes!"");
+    if (!m_checksTurnedOff) {
+      if (!data.classAttribute().isNumeric()) {
+	throw new Exception(""Class attribute has to be numeric for regression!"");
+      }
+      if (data.numInstances() == 0) {
+	throw new Exception(""No instances in training file!"");
+      }
+      if (data.checkForStringAttributes()) {
+	throw new Exception(""Can't handle string attributes!"");
+      }
     }
 
     // Preprocess instances
     m_TransformedData = data;
-    m_TransformFilter = new NominalToBinaryFilter();
-    m_TransformFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
-    m_MissingFilter = new ReplaceMissingValuesFilter();
-    m_MissingFilter.setInputFormat(m_TransformedData);
-    m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
-    m_TransformedData.deleteWithMissingClass();
+    if (!m_checksTurnedOff) {
+      m_TransformFilter = new NominalToBinaryFilter();
+      m_TransformFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_TransformFilter);
+      m_MissingFilter = new ReplaceMissingValuesFilter();
+      m_MissingFilter.setInputFormat(m_TransformedData);
+      m_TransformedData = Filter.useFilter(m_TransformedData, m_MissingFilter);
+      m_TransformedData.deleteWithMissingClass();
+    } else {
+      m_TransformFilter = null;
+      m_MissingFilter = null;
+    }
     m_ClassIndex = m_TransformedData.classIndex();
 
     // Calculate attribute standard deviations
     calculateAttributeDeviations();
 
     // Perform the regression
     findBestModel();
 
     // Save memory
     m_TransformedData = new Instances(m_TransformedData, 0);
   }",NotBuggy,"Added more options


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1166 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message states ""Added more options"", suggesting the introduction of new functionality rather than a bug fix. Let's focus on assessing this change:

1. **Introduction of `m_checksTurnedOff`**: The checks for the data's attributes and other conditions now depend on the `m_checksTurnedOff` flag. This suggests an addition of an option to skip certain validations and processing steps, which aligns with the theme of ""adding more options"".

2. **Conditional Preprocessing**: The preprocessing step which applies filters and manipulates the data is now also conditional based on the `m_checksTurnedOff` flag. This allows the use of optional data transformation, likely increasing flexibility for users who need different preprocessing.

3. **Optional Filter Application**: By adding the logic to bypass creating and applying filters when `m_checksTurnedOff` is true, it ensures no unnecessary computation or memory usage if those transformations are not needed by the user.

The changes focus on increasing flexibility and efficiency within the `buildClassifier` method by giving users an option to bypass certain checks and processing. There is no indication in the diff that it addresses any past incorrect behavior or failures in the system beyond the addition of control over pre-existing behavior.

Because the diff primarily involves adding optional functionality and control rather than addressing bug fixes, the conclusion is **NotBuggy**."
weka,32015.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,44 +1,44 @@
    public Object readCollection(Element node) throws Exception {
-      Collection           coll;
-      Vector               v;
-      Vector               children;
+      Collection<Object>           coll;
+      Vector<Object>               v;
+      Vector<Element>               children;
       Element              child;
       int                  i;
       int                  index;
       int                  currIndex;
 
       // for debugging only
       if (DEBUG)
          trace(new Throwable(), node.getAttribute(ATT_NAME));
 
       m_CurrentNode = node;
       
       children = XMLDocument.getChildTags(node); 
-      v        = new Vector();
+      v        = new Vector<Object>();
 
       // determine highest index for size
       index    = children.size() - 1;
       for (i = 0; i < children.size(); i++) {
         child     = (Element) children.get(i);
         currIndex = Integer.parseInt(child.getAttribute(ATT_NAME));
         if (currIndex > index)
           index = currIndex;
       }
       v.setSize(index + 1);
 
 
       // put the children in the vector to sort them according their index
       for (i = 0; i < children.size(); i++) {
          child = (Element) children.get(i);
          v.set(
                Integer.parseInt(child.getAttribute(ATT_NAME)), 
                invokeReadFromXML(child));
       }
       
       // populate collection
-      coll = (Collection) Class.forName(
-                  node.getAttribute(ATT_CLASS)).newInstance();
+      coll = Utils.cast(Class.forName(node.getAttribute(ATT_CLASS)).
+                        newInstance());
       coll.addAll(v);
       
       return coll;
    }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message focuses on removing dependencies on `FastVector` and eliminating unchecked warnings. The code changes align with this by adding generic types to collections, which resolves unchecked warnings and type-safety issues. These updates don't fix any logic errors, but conform to type-safe practices in Java. The modifications don't directly indicate a bug fix; they improve type safety and maintain backward compatibility. The conclusion is **NotBuggy**."
weka,36275.json,ae9b86a4d68c6d5a823bd20243149bb187a082e9,"@@ -1,39 +1,40 @@
   protected static weka.core.converters.Loader readStepPropertyLoader(
     JSONNode loaderNode) throws WekaException {
 
     String clazz = loaderNode.getChild(CLASS).getValue().toString();
     try {
       weka.core.converters.Loader loader =
-        (weka.core.converters.Loader) Beans.instantiate(
-          JSONFlowUtils.class.getClassLoader(), clazz);
+        (weka.core.converters.Loader) WekaPackageClassLoaderManager.objectForName(clazz);
+          /* Beans.instantiate(
+          JSONFlowUtils.class.getClassLoader(), clazz); */
 
       if (loader instanceof OptionHandler) {
         String optionString =
           loaderNode.getChild(OPTIONS).getValue().toString();
         if (optionString != null && optionString.length() > 0) {
           ((OptionHandler) loader).setOptions(Utils.splitOptions(optionString));
         }
       }
 
       if (loader instanceof weka.core.converters.AbstractFileLoader) {
         String filePath = loaderNode.getChild(""filePath"").getValue().toString();
         if (filePath.length() > 0) {
 
           ((weka.core.converters.AbstractFileLoader) loader)
             .setSource(new File(filePath));
         }
       }
 
       if (loader instanceof weka.core.converters.FileSourcedConverter) {
         Boolean relativePath =
           (Boolean) loaderNode.getChild(""useRelativePath"").getValue();
         ((weka.core.converters.FileSourcedConverter) loader)
           .setUseRelativePath(relativePath);
 
       }
 
       return loader;
     } catch (Exception ex) {
       throw new WekaException(ex);
     }
   }",NotBuggy,"Changes necessary to support using child classloaders for packages

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@13476 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The changes in this diff replace the use of `Beans.instantiate` with `WekaPackageClassLoaderManager.objectForName`. This adjustment likely integrates a more dynamic or custom class loading mechanism, possibly to support a different class loader strategy, such as using child classloaders for specific packages. This aligns with the commit message which references supporting child classloaders.

The change does not specifically indicate a bug fix, but rather an architectural or design update to allow new functionality (use of child classloaders). There's no direct evidence of error-handling improvements, logical corrections, or exception-handling changes that would imply a bug fix.

Thus, based on the provided commit message and code modifications, the answer is **NotBuggy**."
weka,23944.json,08514d45c889fe77d6b7780452f396dd83daf165,"@@ -1,4 +1,5 @@
   public void setRowHidden(int index, boolean hidden) {
-    if ( (index >= 0) && (index < getRowCount()) )
+    if ((index >= 0) && (index < getRowCount())) {
       m_RowHidden[index] = hidden;
+    }
   }",NotBuggy,"DefaultListModels is not generic in Java 1.6. Cleaned up a further few classes.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@10204 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message discusses generics cleanup, unrelated to the change shown, which is a minor formatting correction of moving a brace. There's no bug fix here—only style is adjusted. The answer is **NotBuggy**."
weka,19765.json,1e7162781f076ec9a5024dfa5a04bec8d4e4e0f5,"@@ -1,10 +1,12 @@
   public void processHeadlessEvents(List<EventObject> headless) {
     // only process if we're not headless
     if (!java.awt.GraphicsEnvironment.isHeadless()) {
+      m_processingHeadlessEvents = true;
       for (EventObject e : headless) {
         if (e instanceof DataSetEvent) {
           acceptDataSet((DataSetEvent)e);
         }
       }
     }
+    m_processingHeadlessEvents = false;
   }",NotBuggy,"No longer generates image events when processing previously collected events received during headless execution (prevents charts from being generated twice - once when executed headless on the server and then a second time when results are retrieved).

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7680 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",Buggy,"The commit message states that the changes prevent image events from being generated twice. The code change introduces a flag, `m_processingHeadlessEvents`, which is set to true at the start of processing events and reset to false at the end. This flag could be a mechanism to manage the state during event processing, potentially preventing redundant operations. However, within the provided diff, there are no direct checks or conditions utilizing this flag. Still, the anticipation of this flag usage suggests managing states or preventing concurrent/reset processing, aligning with the commit message intent. 

This code relates to the prevention of duplicate event processing, suggesting it is a fix for behavioral issues potentially observed during previous headless executions, which would relate to what has been described in the commit message. Therefore, given this context and intent, the answer is **Buggy**."
weka,35996.json,c94302fbe3f0a3b1c29340ff055b6f4ef5a329b7,"@@ -1,41 +1,41 @@
   protected static Instances getMiningSchemaAsInstances(Element model,
                                                         Instances dataDictionary) 
     throws Exception {
-    FastVector attInfo = new FastVector();
+    ArrayList<Attribute> attInfo = new ArrayList<Attribute>();
     NodeList fieldList = model.getElementsByTagName(""MiningField"");
     int classIndex = -1;
     int addedCount = 0;
     for (int i = 0; i < fieldList.getLength(); i++) {
       Node miningField = fieldList.item(i);
       if (miningField.getNodeType() == Node.ELEMENT_NODE) {
         Element miningFieldEl = (Element)miningField;
         String name = miningFieldEl.getAttribute(""name"");
         String usage = miningFieldEl.getAttribute(""usageType"");
         // TO-DO: also missing value replacement etc.
 
         // find this attribute in the dataDictionary
         Attribute miningAtt = dataDictionary.attribute(name);
         if (miningAtt != null) {
           if (usage.length() == 0 || usage.equals(""active"") || usage.equals(""predicted"")) {
-            attInfo.addElement(miningAtt);
+            attInfo.add(miningAtt);
             addedCount++;
           }
           if (usage.equals(""predicted"")) {
             classIndex = addedCount - 1;
           }
         } else {
           throw new Exception(""Can't find mining field: "" + name 
                               + "" in the data dictionary."");
         }
       }
     }
     
     Instances insts = new Instances(""miningSchema"", attInfo, 0);
     //    System.out.println(insts);
     if (classIndex != -1) {
       insts.setClassIndex(classIndex);
     }
 
 
     return insts;
   }",NotBuggy,"New version of core package that  that does not depend on FastVector anymore. However, FastVector still exists, extending ArrayList, for backwards compatibility: obviously lots of code outside the core package uses FastVector. Also eliminated all unchecked compile time warnings from the core package.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@5953 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates that the core package was refactored to no longer depend on `FastVector`, which was replaced with `ArrayList` for compatibility reasons. The code changes reflect this update by swapping the `FastVector` with `ArrayList<Attribute>` and appropriately replacing the method `addElement()` with `add()`. This change corresponds with the commit message's indication of refactoring for compatibility and does not inherently indicate a bug fix. Additionally, there are no specific bug-related fixes, error-handling updates, or corrections evident in the provided code changes. The modifications are primarily concerned with refactoring to remove dependency.

Thus, there is no direct indication from these changes that a bug was fixed. The conclusion is **NotBuggy**."
weka,20858.json,60be28974823b8c8f15a54035f2cdee1449d7617,"@@ -1,6 +1,7 @@
             public void actionPerformed(ActionEvent e) {
-              bc.remove();
+              bc.remove(m_mainKFPerspective.getCurrentTabIndex());
               m_beanLayout.revalidate();
               m_beanLayout.repaint();
+              m_mainKFPerspective.setEditedStatus(true);
               notifyIsDirty();
             }",NotBuggy,"Fisrt stage of modernizing the KnowledgeFlow user interface and functionality.

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@7124 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message suggests that the changes are aimed at modernizing the user interface and functionality, without specifically mentioning a bug fix. The code diff reveals several changes:

1. **Method Parameter Change in `bc.remove`:** Previously, `bc.remove()` was called without parameters. The updated call specifies `m_mainKFPerspective.getCurrentTabIndex()`. This could be an enhancement to make the removal action more specific or target a particular element.

2. **Updating Edited Status:** The addition of `m_mainKFPerspective.setEditedStatus(true);` suggests that a new status indicator is added, potentially to track changes or edits made by the user in the interface. This aligns with a functionality enhancement rather than a bug fix.

3. **UI Refresh:** The calls to `revalidate()` and `repaint()` are retained, indicating that the UI component will be refreshed following the action. This is typical for UI changes and helps ensure that the interface accurately reflects the internal state.

4. **Dirty Notification:** The continued presence of `notifyIsDirty();` indicates that some mechanism exists to notify the application or user interface when changes have occurred.

Overall, these changes appear to be part of a planned enhancement to the functionality and interface, as described in the commit message, rather than addressing a bug. Therefore, the conclusion for this change is **NotBuggy**."
weka,15488.json,55b9fb2a4bd0d399e6823c494c1c78ae0af0979c,"@@ -1,45 +1,27 @@
   public void setOptions(String[] options) throws Exception {
     
-    String attributeIndex = Utils.getOption('C', options);
-    if (attributeIndex.length() != 0) {
-      if (attributeIndex.toLowerCase().equals(""last"")) {
-	setAttributeIndex(-1);
-      } else if (attributeIndex.toLowerCase().equals(""first"")) {
-	setAttributeIndex(0);
-      } else {
-	setAttributeIndex(Integer.parseInt(attributeIndex) - 1);
-      }
+    String attIndex = Utils.getOption('C', options);
+    if (attIndex.length() != 0) {
+      setAttributeIndex(attIndex);
     } else {
-      setAttributeIndex(-1);
+      setAttributeIndex(""last"");
     }
-    
-    String firstIndex = Utils.getOption('F', options);
-    if (firstIndex.length() != 0) { 
-      if (firstIndex.toLowerCase().equals(""last"")) {
-	setFirstValueIndex(-1);
-      } else if (firstIndex.toLowerCase().equals(""first"")) {
-	setFirstValueIndex(0);
-      } else {
-	setFirstValueIndex(Integer.parseInt(firstIndex) - 1);
-      }
+
+    String firstValIndex = Utils.getOption('F', options);
+    if (firstValIndex.length() != 0) {
+      setFirstValueIndex(firstValIndex);
     } else {
-      setFirstValueIndex(-1);
+      setFirstValueIndex(""first"");
     }
-     
-    String secondIndex = Utils.getOption('S', options);
-    if (secondIndex.length() != 0) {
-      if (secondIndex.toLowerCase().equals(""last"")) {
-	setSecondValueIndex(-1);
-      } else if (secondIndex.toLowerCase().equals(""first"")) {
-	setSecondValueIndex(0);
-      } else {
-	setSecondValueIndex(Integer.parseInt(secondIndex) - 1); 
-      }
+
+    String secondValIndex = Utils.getOption('S', options);
+    if (secondValIndex.length() != 0) {
+      setSecondValueIndex(secondValIndex);
     } else {
-      setSecondValueIndex(-1);
+      setSecondValueIndex(""last"");
     }
    
     if (getInputFormat() != null) {
       setInputFormat(getInputFormat());
     }
   }",NotBuggy,"Indices now start from 1 for (hopefully) all filters. Changed default behaviour of Remove...java filters in unsupervised.instances. Changed some meta classifiers accordingly.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1795 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message mentions a change in indexing behavior, specifically that indices now start from 1 and some default behaviors are changed. The code modification reflects this intention by altering the `setOptions` method. It simplifies the handling of indices from possibly handling them as zero-based or special cases (like ""first"" and ""last"") to handling them as string parameters potentially indicating positions starting from 1 or keywords like ""first"" and ""last"".

Previously, indices were adjusted (`Integer.parseInt(attributeIndex) - 1`) to potentially be zero-based. Now, the indices are taken directly from the input options (`setAttributeIndex(attIndex)`), suggesting they start from 1 or use keywords directly (e.g., ""first"", ""last""). The code changes are consistent with the commit message's claim about indices starting from 1, and they suggest improvements in handling to correct or unify behavior, especially since the commit message comments on this change being applicable to multiple filters.

Given that this change is not a direct bug fix regarding an error in logic or exception, but rather a behavioral and structural change that aligns with a new requirement or convention, it doesn't strictly represent a bug fix. Thus, the answer is **NotBuggy**."
weka,25044.json,22fe9ffdd42d2d198ece80fef7d71fe91de6c1d6,"@@ -1,4 +1,4 @@
   public Clusterer getClusterer() {
 
-    return m_Clusterer;
+    return m_wrappedClusterer;
   }",NotBuggy,"Has been rewritten.


git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@1183 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message ""Has been rewritten."" is vague and doesn't provide helpful context for the change. Analyzing the diff, we see that the method getClusterer() was returning a field named m_Clusterer and now returns m_wrappedClusterer. Without additional context, changing the returned object might be a fix or an intended alteration. There's no straightforward indication that this resolves a previously existing bug, such as an error correction or a logic adjustment. It seems more like a refactor or an API update rather than a clear bug fix. Given the lack of specific evidence of a bug being fixed, the conclusion is **NotBuggy**."
weka,9558.json,9632387e9fe84f73e41daa585774e86c8cc3c617,"@@ -1,92 +1,124 @@
   public void doGet(HttpServletRequest request, HttpServletResponse response)
     throws ServletException, IOException {
 
     if (!request.getRequestURI().startsWith(CONTEXT_PATH)) {
       return;
     }
 
     PrintWriter out = null;
     InputStream in = request.getInputStream();
     ObjectOutputStream oos = null;
 
-    String clientParam = request.getParameter(""client"");
-    boolean client = (clientParam != null && clientParam.equalsIgnoreCase(""y""));
+    String clientParamLegacy = request.getParameter(Legacy.LEGACY_CLIENT_KEY);
+    String clientParamNew = request.getParameter(JSONProtocol.JSON_CLIENT_KEY);
+    boolean clientLegacy =
+      clientParamLegacy != null && clientParamLegacy.equalsIgnoreCase(""y"");
+    boolean clientNew =
+      clientParamNew != null && clientParamNew.equalsIgnoreCase(""y"");
+
     String taskName = request.getParameter(""name"");
 
     NamedTask task = m_taskMap.getTask(taskName);
 
-    if (client) {
+    if (clientLegacy) {
       // response.setCharacterEncoding(""UTF-8"");
       // response.setContentType(""text/plain"");
       response.setContentType(""application/octet-stream"");
       OutputStream outS = response.getOutputStream();
       oos = new ObjectOutputStream(new BufferedOutputStream(outS));
+    } else if (clientNew) {
+      out = response.getWriter();
+      response.setCharacterEncoding(""UTF-8"");
+      response.setContentType(""application/json"");
     } else {
       out = response.getWriter();
       response.setCharacterEncoding(""UTF-8"");
       response.setContentType(""text/html;charset=UTF-8"");
       out.println(""<HTML>"");
       out.println(""<HEAD><TITLE>Schedule</TITLE></HEAD>"");
       out.println(""<BODY>"");
     }
 
     response.setStatus(HttpServletResponse.SC_OK);
 
     try {
       if (task == null) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR
-            + "": Can't find task "" + taskName;
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + "": Can't find task "" + taskName;
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""Can't find task "" + taskName);
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out
             .println(WekaServlet.RESPONSE_ERROR + "": Unknown task "" + taskName);
         }
       } else if (!(task instanceof Scheduled)) {
-        if (client) {
-          String errorResult = WekaServlet.RESPONSE_ERROR + ""'"" + taskName
-            + ""' "" + ""is not a scheduled task."";
+        if (clientLegacy) {
+          String errorResult =
+            WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
+              + ""is not a scheduled task."";
           oos.writeObject(errorResult);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> errorJ =
+            JSONProtocol.createErrorResponseMap(""'"" + taskName
+              + ""' is not a scheduled task"");
+          String encodedResponse = JSONProtocol.encodeToJSONString(errorJ);
+          out.println(encodedResponse);
+          out.flush();
         } else {
           out.println(WekaServlet.RESPONSE_ERROR + ""'"" + taskName + ""' ""
             + ""is not a scheduled task."");
         }
       } else {
         Schedule sched = ((Scheduled) task).getSchedule();
-        if (client) {
+        if (clientLegacy) {
           oos.writeObject(sched);
           oos.flush();
+        } else if (clientNew) {
+          Map<String, Object> scheduleJ = JSONProtocol.scheduleToJsonMap(sched);
+          Map<String, Object> jResponse =
+            JSONProtocol.createOKResponseMap(""OK. Schedule"");
+          JSONProtocol.addPayloadMap(jResponse, scheduleJ,
+            JSONProtocol.SCHEDULE_PAYLOAD_ID);
+
+          String encodedResonse = JSONProtocol.encodeToJSONString(jResponse);
+          out.println(encodedResonse);
+          out.flush();
         } else {
-          String optionsString = weka.core.Utils
-            .joinOptions(sched.getOptions());
+          String optionsString =
+            weka.core.Utils.joinOptions(sched.getOptions());
           out.println(optionsString + ""<p>"");
         }
       }
     } catch (Exception ex) {
-      if (client && oos != null) {
+      if (clientLegacy && oos != null) {
         oos.writeObject(WekaServlet.RESPONSE_ERROR + "" "" + ex.getMessage());
         oos.flush();
-      } else if (out != null) {
+      } else if (out != null && !clientNew) {
         out.println(""<p><pre>"");
         ex.printStackTrace(out);
         out.println(""</pre>\n"");
       }
       ex.printStackTrace();
     } finally {
-      if (!client && out != null) {
+      if (!clientLegacy && !clientNew && out != null) {
         out.println(""</BODY>\n</HTML>"");
       }
 
       if (out != null) {
         out.close();
         out = null;
       }
       if (oos != null) {
         oos.close();
         oos = null;
       }
     }
-
   }",NotBuggy,"Implementation for the new Knowledge Flow

git-svn-id: https://svn.cms.waikato.ac.nz/svn/weka/trunk@12561 e0a1b77d-ad91-4216-81b1-defd5f83fa92
",NotBuggy,"The commit message indicates an implementation for a new feature, specifically the ""new Knowledge Flow"". The code changes significantly expand upon the existing method to accommodate a new client communication protocol using JSON while maintaining the existing functionality for a legacy protocol.

Here's a breakdown of the changes made:

1. **New Protocol Handling:**
   - Introduces the `clientParamNew` and a corresponding `clientNew` boolean to check for new protocol requests, parallel to the legacy protocol (`clientParamLegacy` and `clientLegacy`).
   - Handles responses differently based on whether the request is for the legacy or new protocol. This includes setting the appropriate content type: `application/octet-stream` for the legacy protocol (binary) and `application/json` for the new protocol (JSON).

2. **Response Handling:**
   - For the `clientNew` condition, JSON responses are prepared using `JSONProtocol` utilities. Errors and schedules are encoded as JSON strings before being written to the response.
   - Introduces new JSON structures to accurately represent error messages and schedule information, moving away from a plain text or serialized Java object approach.

3. **Exception Handling:**
   - The error handling and clean-up have been expanded to support the new JSON protocol.

The changes reflect implementing a new feature with some additional exception-handling extensions for the new protocol. There is no evident correction of faulty logic or fixing bugs in the existing code. The changes are focused on introducing new functionality rather than fixing a defect. Therefore, the assessment is **NotBuggy**."
xerces2-j,109.json,1281996de10d3af1b51f0b4d78cfcd75d8ade6fe,"@@ -1,4 +1,4 @@
-    public void endElement(QName element)
+    public void endElement(QName element, Augmentations augs)
         throws XNIException {
-        super.endElement(toUpperCase(element));
+        super.endElement(toUpperCase(element), augs);
     } // endElement(QName)",NotBuggy,"Modify XNI to include Augmentations parameter on each XMLDocumentHandler call.
Modify samples.
Modify docs http://nagoya.apache.org/bugzilla/show_bug.cgi?id=5058


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317940 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The provided diff shows a clear modification to the `endElement` method by adding an `Augmentations` parameter to the method signature and the subsequent method call. This change aligns with the commit message, which states the intention to include an `Augmentations` parameter on each `XMLDocumentHandler` call. The change itself is not a fix for a bug but rather an enhancement or modification to the existing functionality, as it aligns with feature modifications mentioned in the commit message (e.g., augmenting the API to support additional functionality).

There is no indication of bug-related changes such as error-handling updates, logical corrections, or exception-handling improvements. The focus appears to be on feature enhancement rather than defect resolution.

Hence, based on the analysis, the modification is **NotBuggy**."
xerces2-j,6765.json,62b65e27622f6a1657e2e6a6c0776c95151c9dd0,"@@ -1,24 +1,27 @@
     public Object getParameter(String name) throws DOMException {
-		if (name.equals(Constants.DOM_ERROR_HANDLER)){
-			if (fErrorHandler != null){
-				return fErrorHandler.getErrorHandler();
-			}
-		}
+        
+        if (name.equals(Constants.DOM_ERROR_HANDLER)){
+            return (fErrorHandler != null) ? fErrorHandler.getErrorHandler() : null;
+        }
+        else if (name.equals(Constants.DOM_RESOURCE_RESOLVER)) {
+            return (fResourceResolver != null) ? fResourceResolver.getEntityResolver() : null;
+        }
+        
         try {
             boolean feature = getFeature(name);
             return (feature) ? Boolean.TRUE : Boolean.FALSE;
         } catch (Exception e) {
             Object property;
             try {
                 property = getProperty(name);
                 return property;
             } catch (Exception ex) {
                 String msg =
                     DOMMessageFormatter.formatMessage(
                         DOMMessageFormatter.DOM_DOMAIN,
                         ""FEATURE_NOT_SUPPORTED"",
                         new Object[] { name });
                 throw new DOMException(DOMException.NOT_SUPPORTED_ERR, msg);
             }
         }
     }",NotBuggy,"The XML Schema API [1] states that the DOMConfiguration of an XSLoader may
allow setting of a resource-resolver (LSResourceResolver). Supporting this
parameter so that applications may use their own resolver to locate
schema imports and includes.

[1] http://www.w3.org/Submission/2004/SUBM-xmlschema-api-20040309/xml-schema-api.html#Interface-Loader


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319940 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an update to support a resource-resolver in the DOMConfiguration for an XSLoader. The diff adds support for the `DOM_RESOURCE_RESOLVER` parameter, returning a resource resolver if it exists. This new functionality allows external resource resolution, aligning with the commit's intention. However, this change deals with feature additions rather than correcting errors or bugs in existing code. Hence, this change does not signify a bug fix but rather an enhancement.

The answer is **NotBuggy**."
xerces2-j,5928.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,7 +1,6 @@
-    public void startDocument() {
-
-        if (!canonical) {
-            out.println(""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>"");
+    public void startDocument(XMLLocator locator, String encoding) 
+        throws XNIException {
+        if (fDocumentHandler != null && !fOnlyPassPrefixMappingEvents) {
+            fDocumentHandler.startDocument(locator, encoding);
         }
-
-    } // startDocument()

+    } // startDocument(XMLLocator,String)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a merge or move operation and doesn't imply a bug fix. The code change modifies the `startDocument` method to handle more parameters and exceptions, suggesting refactoring or enhancement rather than fixing a bug. Given the lack of bug-related modifications such as error handling or logical corrections, and considering the commit message, the conclusion is **NotBuggy**."
xerces2-j,6658.json,42ca322ea8ac4be5de3f277f111700384c06f8d7,"@@ -1,4 +1,4 @@
     public XSSimpleType createTypeRestriction(String name, String targetNamespace,
-                                              short finalSet, XSSimpleType base) {
-        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false);
+                                              short finalSet, XSSimpleType base, XSObjectList annotations) {
+        return new XSSimpleTypeDecl((XSSimpleTypeDecl)base, name, targetNamespace, finalSet, false, annotations);
     }",NotBuggy,"as part of annotation support, enabling schema datatype factories to create XSSimpleType implementations containing lists of annotations


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319276 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The diff shows an added parameter, `XSObjectList annotations`, to a method to build `XSSimpleType` with annotation support. This pertains to functionality enhancement, aligning with the commit message. It doesn't imply a bug fix. The conclusion is **NotBuggy**."
xerces2-j,3176.json,1130527a86137ec0fa5201483136b96ac2b25135,"@@ -1,8 +1,9 @@
     public Node getFirstChild() {
 
         if (needsSyncChildren()) {
             synchronizeChildren();
         }
-    	return firstChild;
+        makeChildNode();
+    	return (Node) value;
 
     }   // getFirstChild():Node",NotBuggy,"This commit brinds yet another optimization to this DOM implementation.
The whole idea is to try and avoid to always creating a Text node to hold
the value of an attribute. The DOM spec requires it, so we still have
to do it in case getFirstChild() is called for instance. The reason
attribute values are stored as a list of nodes is so that they can carry
more than a simple string. They can also contain EntityReference nodes.
However, most of the times people only have a single string that they
only set and get through Element.set/getAttribute or Attr.set/getValue.
In this new version, the Attr node has a value pointer which can either
be the String directly or a pointer to the first ChildNode. A flag tells
which one it currently is.
Note that while we try to stick with the direct String as much as possible
once we've switched to a node there is no going back. This is because we
have no way to know whether the application keeps referring to the node
we once returned.

The gain in memory varies on the density of attributes in the document.
But in the tests I've run I've seen up to 12% of memory gain. And the good
thing is that it also leads to a slight gain in speed because we allocate
fewer objects! I mean, that's until we have to actually create the node...

To avoid too much duplicated code, I got rid of ParentNode and renamed
ChildAndParentNode, which I never really liked, to ParentNode for
simplicity, this doesn't make much of a difference in memory usage because
there are only very objects that are only a Parent. This is only true now
because AttrImpl now inherits directly from NodeImpl and has its own
implementation of the ParentNode's node behavior. So there is still some
duplicated code there.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@316776 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"Analyzing the commit message and the code changes:

1. **Commit Message Analysis**:
   - The commit message describes an optimization aimed at reducing memory usage and improving speed by avoiding unnecessary creation of `Text` nodes for attribute values.
   - The message also mentions structural changes to the DOM implementation, such as modifying class inheritance and reusing code more effectively.

2. **Code Diff Analysis**:
   - The `getFirstChild` method has been modified.
   - The previous code directly returned `firstChild`, while the current version calls `makeChildNode()` and returns `value`.
   - The `value` seems to be a pointer that the commit message talks about, which can either directly hold a string or point to the first child node.
   - The change from `firstChild` to `(Node) value` aligns with the optimization described in the commit message, where `value` could be dynamically adjusted between being a string or a node pointer depending on the application usage.

3. **Conclusion**:
   - The changes in the code are consistent with the commit message's description of optimization. They address the way attribute values are managed internally to avoid unnecessary node creation.
   - The modifications do not specifically fix a functionality bug but are targeted at improving memory and processing efficiency.

Given these points, the code changes primarily target performance improvements and memory optimization rather than fixing a bug. Therefore, the conclusion is **NotBuggy**."
xerces2-j,6593.json,1b20fdfd0e92e36d5d66b17da8c7cbeb6e59c899,"@@ -1,4 +1,3 @@
-    public XSObjectList getAnnotations() {
-        // REVISIT: SCAPI: to implement
-        return fAnnotations;
-    }

+        public XSObjectList getAnnotations() {
+            return annotations;
+        }",NotBuggy,"Store and expose annotation for simpleTypes and facets. Add a new interface to
represent enumeration and pattern facets.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319311 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message discusses storing and exposing annotations and mentions a new interface for facets. In contrast, the code diff shows the implementation of an existing getAnnotations method by returning the field annotations, presumably correcting a previous issue. Therefore, while not matching the commit message fully, this update does fix an aspect related to annotation handling.

Since the change involves implementing a previously unimplemented method, it is likely a bug fix (or a feature completion) related to annotations. Thus, the appropriate conclusion based on the provided information is **Buggy**."
xerces2-j,6062.json,5846060722ccc5d1a1b47182834caa718a3fa65b,"@@ -1,25 +1,25 @@
         protected boolean scanRootElementHook()
             throws IOException, XNIException {
-            if (fDTDValidator == null) {
-                fBindNamespaces = true;
-            } else if (!fDTDValidator.hasGrammar()) {
-                fBindNamespaces = true;
-                fPerformValidation = fDTDValidator.validate();
-                // re-configure pipeline
-                XMLDocumentSource source = fDTDValidator.getDocumentSource();
-                XMLDocumentHandler handler = fDTDValidator.getDocumentHandler();
-                source.setDocumentHandler(handler);
-                if (handler != null)
-                    handler.setDocumentSource(source);
-                fDTDValidator.setDocumentSource(null);
-                fDTDValidator.setDocumentHandler(null);
+            
+            if (fExternalSubsetResolver != null && !fSeenDoctypeDecl 
+                && (fValidation || fLoadExternalDTD)) {
+                scanStartElementName();
+                resolveExternalSubsetAndRead();
+                reconfigurePipeline();
+                if (scanStartElementAfterName()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
-
-            if (scanStartElement()) {
-                setScannerState(SCANNER_STATE_TRAILING_MISC);
-                setDispatcher(fTrailingMiscDispatcher);
-                return true;
+            else {
+                reconfigurePipeline();
+                if (scanStartElement()) {
+                    setScannerState(SCANNER_STATE_TRAILING_MISC);
+                    setDispatcher(fTrailingMiscDispatcher);
+                    return true;
+                }
             }
             return false;
 
         } // scanRootElementHook():boolean",NotBuggy,"Infrastructure work in the scanners to support the SAX2 Extensions 1.1
interface EntityResolver2, specifically getExternalSubset which
allows an application to provide an external subset for a document
which doesn't otherwise have one.

The scanners are now able to handle all three cases where a resolver
may be queried for external subset:

1) Neither an external or internal subset exist.
2) Only an internal subset exists.
3) No DOCTYPE declaration in the document.

In the third case, scanning of the root element is broken up into
three segments:

1) Scan the root element QName.
2) Query the resolver with the root name and base URI, if it 
locates an external subset then read it.
3) Scan the remainder of the start tag.

If an external subset is located the public id and system id
from the XMLInputSource returned from the resolver are
reported to doctypeDecl. This is what SAX expects.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@319878 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The changes in the provided diff are associated with supporting the SAX2 Extensions 1.1 interface `EntityResolver2`. The commit message explains a mechanism for allowing an application to provide an external subset for an XML document. This involves modifying the function `scanRootElementHook()` to:

1. Check for an `fExternalSubsetResolver` and whether a DOCTYPE has been seen, alongside conditions for validation or loading an external DTD.
2. Introduce methods like `scanStartElementName()`, `resolveExternalSubsetAndRead()`, and `reconfigurePipeline()`. These methods handle reading the start element and resolving an external subset if available, before reconfiguring the XML parsing pipeline.
3. Adjust handling of XML parsing flow to allow for integration of an external subset into the document processing pipeline under certain conditions.
4. Maintain existing scanning operations if the conditions for invoking the external subset logic aren't met.

Although these changes are labeled as ""infrastructure work"" and primarily target feature support, they directly address scanning behavior, piping control adjustments, and the incorporation of the external subset into XML parsing, which can unintentionally fix scenarios where root elements or DOCTYPE declarations were previously mismanaged or led to application errors.

Given the complexity of the changes and the nature of XML parsing, these adjustments do look like they might fix bugs, especially cases not handled previously because of missing subset applicability. The changes include:

- Handling previously unsupported parsing scenarios.
- Ensuring robust error-checking with the integration and resolution of external subsets.
- Potential corrections of any parsing errors related to external subsets, which were not handled before.

Overall, although the commit message chiefly denotes infrastructure work for supporting a feature, the actual code changes reasonably suggest that bug fixes could be an outcome, particularly resolving issues with the SAX2 parsing entity resolution. Therefore, the nature of the changes, focusing on fixing and extending XML parsing scenarios, implies bug fixes might be inherently addressed.

The answer to whether the commit is **Buggy** or **NotBuggy** is **Buggy** because of handled parsing scenarios not covered earlier."
xerces2-j,8338.json,34aab31cc59d829a17b9f74ce192423af5788d9f,"@@ -1,3 +1,4 @@
-    public void startExternalSubset(Augmentations augs) throws XNIException {
+    public void startExternalSubset(XMLResourceIdentifier identifier,
+                                    Augmentations augs) throws XNIException {
         fReadingExternalDTD = true;
     } // startExternalSubset(Augmentations)",NotBuggy," Add implementation for baseURI and documentURI (DOM Level 3).
 Add XMLResourceIdentifier to startExternalSubset() method defined in  XNI XMLDTDHandler to
allow supporting baseURI for an external subset.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318382 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes adding a parameter to the startExternalSubset method to support baseURI for an external subset. The code change reflects this precisely by introducing an XMLResourceIdentifier parameter. The change doesn't indicate a bug fix; it adds functionality to the method. The message aligns with the code modification, focusing on an enhancement rather than a bug correction. Therefore, the assessment is **NotBuggy**."
xerces2-j,1322.json,5453aaa1b3b312be38b47f18a2a5c6d9f874d3f0,"@@ -1,3 +1,3 @@
-    void setNextIndent( int indent )
+    public void setNextIndent( int indent )
     {
     }",NotBuggy,"Changed methods to public modifier


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315918 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change in access modifiers from package-private to public. The code diff shows a method `setNextIndent` being changed from default access to public access. This adjustment does not relate to bug fixes, as it aims to modify the access scope of the method likely for broader usability or API exposure. There is no indication of a logical error correction or exception handling associated with bugs.

Therefore, the changes reflected do not align with typical bug fix characteristics. The alterations are more about changing visibility rather than correcting erroneous behavior in the code. 

The conclusion is **NotBuggy**."
xerces2-j,29.json,e94cfece717a676ae2799d5c030d2ed5982d1e27,"@@ -1,19 +1,20 @@
     public void internalEntityDecl(String name, XMLString text,
-                                   XMLString nonNormalizedText)
+                                   XMLString nonNormalizedText,
+                                   Augmentations augs)
         throws XNIException {
 
         printIndent();
         fOut.print(""internalEntityDecl("");
         fOut.print(""name="");
         printQuotedString(name);
         fOut.print(',');
         fOut.print(""text="");
         printQuotedString(text.ch, text.offset, text.length);
         fOut.print(',');
         fOut.print(""nonNormalizedText="");
         printQuotedString(nonNormalizedText.ch, nonNormalizedText.offset,
                           nonNormalizedText.length);
         fOut.println(')');
         fOut.flush();
 
     } // internalEntityDecl(String,XMLString)",NotBuggy,"committing some (slightly modified to take care of regressions) massive XNI changes on behalf of Andy Clark.  These make use of XMLResourceIdentifier in start.*Entity calls, and add Augmentations to various DTD Handler callbacks.  Things yet to do:
1.  Make Augmentation callbacks consistent.  They are null sometimes, not others; need to make them null by default.
2.  Propagate XMLResourceIdentifier to remaining methods it could and is not being used in.
3.  Tie XMLResourceIdentifier and XMLLocator together.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@318116 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message discusses incorporating XMLResourceIdentifier and Augmentations, yet here augs only appears in an extended method signature (internalEntityDecl gets a new parameter, Augmentations augs). There's neither logic using augs nor changes suggesting a bug fix in existing behavior. The changes align with the message on adding Augmentations, but lack indications of bug fixes like corrections in error handling, logic, or exception handling. Therefore, this diff doesn't signal a bug fix. The answer is **NotBuggy**."
xerces2-j,6592.json,962564ff9af52f2561b55bcf3fc5f3f21e1f43e9,"@@ -1,3 +1,3 @@
-		public XSAnnotation getAnnotation() {
-			return annotation;
-		}

+        public XSAnnotation getAnnotation() {
+            return (XSAnnotation) annotations.item(0);
+        }",NotBuggy,"Committing code and unit tests for the following line items from the Xerces 2.8.0 release plan:
- Expose annotations in XSAttributeUse and XSParticle
- Support lists of annotations for other XSObjects 

git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@379204 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message mentions exposing annotations in XSAttributeUse and XSParticle but doesn't specify resolving any bugs. The code change modifies the `getAnnotation` method to return an item from the `annotations` list rather than a single `annotation` object. This reflects supporting lists of annotations rather than fixing any specific bug. The change seems to align with the feature development described in the message rather than addressing a bug.

Given this focus on feature development based on the commit message and code changes, the changes relate more to enhancement than to correcting errors. Therefore, the answer is **NotBuggy**."
xerces2-j,3145.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public void setNotationName(String name) {
         
-        if (syncData()) {
+        if (needsSyncData()) {
             synchronizeData();
         }
     	notationName = name;
 
     } // setNotationName(String)",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a global renaming of internal methods, and this single change in the diff reflects just such a renaming: `syncData()` to `needsSyncData()`. This change does not imply any bug fix—it is purely a renaming meant for clarity or consistency in method naming. As there are no adjustments to logic or error handling, it aligns with the descriptive commit message focused on renaming.

Therefore, the change is **NotBuggy**."
xerces2-j,3103.json,724c5904263e34696a38560774e273d8ba109a04,"@@ -1,8 +1,8 @@
     public Node getFirstChild() {
 
-        if (syncChildren()) {
+        if (needsSyncChildren()) {
             synchronizeChildren();
         }
     	return firstChild;
 
     }   // getFirstChild():Node",NotBuggy,"global renaming of some internal methods,
I too wish I got them right in the first place...


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@315905 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states a renaming of internal methods, and the diff shows a method name change from `syncChildren` to `needsSyncChildren`. This does not affect functionality; only the method's naming is altered to presumably reflect its purpose better. There are no bug-fix indications such as logic changes, condition adjustments, error handling improvements, or exception handling updates. This change strictly aligns with the commit message about renaming and does not indicate a bug fix. Thus, it is categorized as **NotBuggy**."
xerces2-j,7895.json,97033b9bcdb0ebcdfc9a24df498cab2d12790b44,"@@ -1,13 +1,19 @@
     synchronized SAXParser getSAXParser() {
-        if (fSAXParser != null) return fSAXParser;
+        if (fSAXParser != null) {
+            SAXParser parser = (SAXParser) fSAXParser.get();
+            if (parser != null) {
+                return parser;
+            }
+        }
         // REVISIT:  when schema handles XML 1.1, will need to 
         // revisit this (and the practice of not prepending an XML decl to the annotation string
-        IntegratedParserConfiguration config = new IntegratedParserConfiguration(fSymbolTable);
+        XML11Configuration config = new XML11Configuration(fSymbolTable);
         // note that this should never produce errors or require
         // entity resolution, so just a barebones configuration with
         // a couple of feature  set will do fine
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.NAMESPACES_FEATURE, true);
         config.setFeature(Constants.SAX_FEATURE_PREFIX + Constants.VALIDATION_FEATURE, false);
-        fSAXParser = new SAXParser(config);
-        return fSAXParser;
+        SAXParser parser = new SAXParser(config);
+        fSAXParser = new SoftReference(parser);
+        return parser;
     }",NotBuggy,"Improvement for writing annotations:
- IntegratedParserConfiguration is ancient and slow. Use the better performing XML11Configuration.
- Parser instances use quite a bit of memory. Store them in SoftReferences so they can be
  reclaimed if there is high demand for memory.
- Use adopt node when possible to avoid creating a copy of the DOM before attaching it to the target.
- Turn off the deferred DOM feature. There's no benefit to having it on since the target 
  is another document. We would end up traversing the entire DOM even when calling adoptNode.


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@348887 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes performance improvements and memory optimizations in handling annotations and parsers. Here's how those correspond to the code changes:

1. **Performance Improvement:** The original `IntegratedParserConfiguration` is replaced with `XML11Configuration`, reflecting what's mentioned in the commit as a better-performing configuration.

2. **Memory Optimization:** The `fSAXParser` which was previously stored directly as an instance is now stored inside a `SoftReference`. This change aligns with the commit message intention to allow the parser objects to be garbage collected if memory demand is high.

3. **Conditional Checks:** The change introduces checking whether the parser obtained from the `SoftReference` is not null before returning it. This ensures the method will create a new parser if the old one has been garbage collected, thus maintaining functionality.

4. **DOM Handling:** While not directly observable in this diff, the mention of adopting nodes and turning off the deferred DOM feature aligns with the kind of changes seen here where parser configurations are being adjusted.

These change mainly highlight memory management enhancements and performance tweaks with no direct indication of resolving a buggy behavior. Therefore, based on the commit message and the nature of the changes, which focus on improvements rather than explicit bug fixes, I conclude that the changes are **NotBuggy**."
xerces2-j,8568.json,6fec9ee0c874aba003c6652a39c4de16036f6e4f,"@@ -1,3 +1,3 @@
-    public void writeChildrenXml (XmlWriteContext context) throws IOException
-    {
-    }

+    public void setProperty(String propertyId, Object value)
+        throws XMLConfigurationException {
+    } // setProperty(String,Object)",NotBuggy,"Merge/move of Xerces 2 to the trunc!


git-svn-id: https://svn.apache.org/repos/asf/xerces/java/trunk@317483 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Merge/move of Xerces 2 to the trunc!"" does not align with the code change shown in the diff. The code modification involves the removal of a method `writeChildrenXml` and the addition of a new method `setProperty`. The commit message suggests a restructuring or relocation of the codebase rather than functional changes aimed at fixing bugs.

Evaluating the code changes:
- The change introduces a `setProperty` method with parameters `String propertyId` and `Object value`, and specifies it can throw an `XMLConfigurationException`. However, this method is empty.
- There is no modification or indication of bug fixing related to error handling, logic correction, or enhancement to prevent exceptions.

Given these observations:
- The commit message indicates an organizational change, which doesn't imply a bug fix.
- The addition of an empty method without further context does not indicate any immediate bug fix or feature implementation.

Therefore, the conclusion is **NotBuggy**."
hibernate-orm,7648.json,ff4774ad7ef488ce7505784f9591684d096bdb99,"@@ -1,3 +1,3 @@
-	public Schema getDefaultSchema() {
-		return implicitSchema;
+	public Namespace getDefaultNamespace() {
+		return implicitNamespace;
 	}",NotBuggy,"HHH-9974 - Rename org.hibernate.boot.model.relational.Schema to Namespace
",NotBuggy,"The commit message indicates a rename due to a refactor, as it references a change in naming from `Schema` to `Namespace`. The code diff reflects this change, updating the method from `getDefaultSchema` returning `Schema` to `getDefaultNamespace` returning `Namespace`. There are no indications of bug fixes; it is purely a nomenclature update aligning with the commit message description.

The changes do not suggest any functional corrections or error-handling improvements. Thus, the conclusion is **NotBuggy**."
hibernate-orm,19080.json,1e44e7420b7af10adc9547a461f02a6d979624f1,"@@ -1,17 +1,13 @@
 	public void afterReassociate(Object entity, SessionImplementor session) {
 		if ( getEntityMetamodel().getBytecodeEnhancementMetadata().isEnhancedForLazyLoading() ) {
 			LazyAttributeLoadingInterceptor interceptor = getEntityMetamodel().getBytecodeEnhancementMetadata().extractInterceptor( entity );
 			if ( interceptor == null ) {
-				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor(
-						entity,
-						null,
-						session
-				);
+				getEntityMetamodel().getBytecodeEnhancementMetadata().injectInterceptor( entity, session );
 			}
 			else {
 				interceptor.setSession( session );
 			}
 		}
 
 		handleNaturalIdReattachment( entity, session );
 	}",NotBuggy,"HHH-10267 - Support defining lazy attribute fetch groups
",NotBuggy,"The commit message refers to adding support for lazy attribute fetch groups, while the diff shows a code change related to the management of interceptors for lazy loading. The change simplifies the `injectInterceptor` call by removing a `null` argument, potentially allowing attributes to be loaded lazily.

While the modification is related to lazy loading, it doesn't appear to resolve a specific bug. It seems more like an enhancement or refactor concerning how interceptors are handled for entities with lazy attributes. The commit message focuses on support for a new feature rather than a bug fix.

Therefore, the answer is **NotBuggy**."
hibernate-orm,1521.json,0e17f9adf85e14cb34c385c7886ed0cddfabe0f6,"@@ -1,3 +1,3 @@
 	public <X> ValueExtractor<X> getExtractor(final JavaTypeDescriptor<X> javaTypeDescriptor) {
-		return (ValueExtractor<X>) new PGGeometryValueExtractor();
+		return (ValueExtractor<X>) new PGGeometryValueExtractor(javaTypeDescriptor);
 	}",NotBuggy,"HHH-6509 - Prepare introduction of Geolatte-geom library as model

Updates testing approach
Refactors ValueBinders and ValueExtractors
",NotBuggy,"The commit message primarily discusses preparing to introduce a library and refactoring, with no mention of bug fixes. The code change here modifies the constructor call for `PGGeometryValueExtractor`, now passing `javaTypeDescriptor` as an argument. This suggests an improvement or enhancement in functionality, possibly to ensure type safety or correct operation. However, the change aligns more with a refactor to update how objects are instantiated rather than correcting a bug.

Given that there is no mention of a bug being fixed in the commit message and the nature of the refactor, the change doesn't directly indicate a bug fix. Thus, the conclusion is **NotBuggy**."
hibernate-orm,19918.json,dc7cdf9d8803ff58191a35907414b7dd81210422,"@@ -1,3 +1,8 @@
 								public EntityDefinition toEntityDefinition() {
+									if ( getAssociationNature() != AssociationNature.ENTITY ) {
+										throw new WalkingException(
+												""Cannot build EntityDefinition from non-entity-typed attribute""
+										);
+									}
 									return (EntityPersister) aType.getAssociatedJoinable( ownerEntityPersister.getFactory() );
 								}",NotBuggy,"HHH-8276 - Integrate LoadPlans into UniqueEntityLoader (PoC)
",Buggy,"The commit diff adds a conditional check to ensure that the `toEntityDefinition` method operates only on entity-typed attributes. If the method is called on a non-entity attribute, a `WalkingException` is thrown. 

This change indicates:

1. **Error-Handling Improvement**: The addition of a precondition check improves robustness by preventing the method from proceeding when conditions are incompatible, which would otherwise cause logic errors or unexpected behavior.

2. **Logical Corrections**: This check ensures that only valid operations are performed, aligning with intended functionalities.

The commit message mentions integrating LoadPlans, which suggests a proof of concept (PoC) implementation, but the specific change here directly addresses potential misuse of the `toEntityDefinition` method, preventing errors by checking the association nature.

The change fits the context of addressing logical errors that could arise in incorrect method usage.

Thus, the change is indicative of a bug fix related to error handling in a potentially buggy situation (non-entity type attempted to be treated as an entity).

The answer is **Buggy**."
hibernate-orm,14224.json,208e789bb107503a5df7a62531570ececaa4c0ed,"@@ -1,10 +1,10 @@
 	public Object getProxy() {
 		try {
 			final ProxyConfiguration proxy = (ProxyConfiguration) proxyClass.newInstance();
-			proxy.$$_hibernate_set_interceptor( new PassThroughInterceptor( proxy, proxyClass.getName() ) );
+			proxy.$$_hibernate_set_interceptor( this.interceptor );
 			return proxy;
 		}
 		catch (Throwable t) {
 			throw new HibernateException( ""Unable to instantiate proxy instance"", t );
 		}
 	}",NotBuggy,"HHH-12786 Improve the basic proxy interceptor

Apart from cosmetic changes, we were testing in the equals() method that the
instance == the proxied object which will always be true.

We should use the argument of the equals() method instead to do the
comparison.

And we can do the comparison on the instance, instead of requiring
passing the proxiedObject into the interceptor.
",NotBuggy,"The commit message describes changes related to improvements in how the proxy interceptor handles equality checks and object comparisons. However, the diff provided shows an update to how the interceptor is set on a proxy instance.

In detail:

- Originally, a new `PassThroughInterceptor` was instantiated with the proxy and its class name every time a proxy was created.
- The updated code now directly uses `this.interceptor` instead of creating a new `PassThroughInterceptor`.

The commit message suggests other changes related to `equals()` method logic, but those are not visible in this specific diff snippet. Nonetheless, the use of `this.interceptor` instead of creating new instances is a design improvement rather than a direct bug fix.

Considering the diff's focus and the lack of detail related to bug fixes specifically in equality checking as per the commit message, this particular change appears more aligned with refactoring or optimization than fixing an actual defect.

Therefore, unless the update to directly use `this.interceptor` addresses a specific bug in the interceptor's behavior, we align this with an enhancement or improvement rather than a bug fix.

Thus, this particular change is **NotBuggy**."
hibernate-orm,19882.json,4620ff4b4fe670bbf121b709e9da6b418c24ee43,"@@ -1,3 +1,3 @@
 			public EntityDefinition getEntityDefinition() {
-				return AbstractEntityPersister.this;
+				return entityPersister;
 			}",NotBuggy,"HHH-7841 - Redesign Loader
",NotBuggy,"The commit message mentions a redesign of the Loader, specifically referencing HHH-7841. The change in the diff shows a modification in the way an EntityDefinition is obtained—from directly returning `AbstractEntityPersister.this` to returning `entityPersister`.

The modification doesn't explicitly indicate a bug fix. Instead, it suggests a change in design or refactoring by introducing a different way to access the `EntityDefinition`, probably to adhere to newer code patterns or improve architecture consistency. Without evidence of faulty behavior being corrected, such as logical errors or exception handling, the change is likely not related to a bug fix.

Thus, the diff's change aligns more with a redesign effort as mentioned in the commit message rather than addressing a bug. Therefore, I conclude this is **NotBuggy**."
hibernate-orm,12109.json,36dbd3a06e5867cc830b8537f6e24618a8a47137,"@@ -1,10 +1,12 @@
-	public InputStream getAsciiStream() throws SQLException {
+	private InputStream getStream() throws SQLException {
 		try {
-			if (needsReset) reader.reset();
+			if (needsReset) {
+				stream.reset();
+			}
 		}
-		catch (IOException ioe) {
+		catch ( IOException ioe) {
 			throw new SQLException(""could not reset reader"");
 		}
 		needsReset = true;
-		return new ReaderInputStream(reader);
+		return stream;
 	}",NotBuggy,"HHH-2412 - Support for JDBC4


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17767 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",NotBuggy,"The commit message indicates the addition of support for JDBC4. The diff shows a method `getAsciiStream` being renamed to `getStream`, where the code has swapped from using `ReaderInputStream(reader)` to directly returning `stream`. It also checks for `needsReset` on `stream` instead of `reader`.

Given the context, this change seems to realign the code with JDBC4 standards. However, changing from a `ReaderInputStream` to returning `stream` seems like a transition to a possibly more efficient or correct way of handling streams directly with JDBC4. The inclusion of handling `IOException` also suggests careful consideration of exception-handling, although it remains similar to the previous implementation.

While this modification doesn't directly highlight a bug fix related to logical errors in operations or error-handling, it is part of aligning with JDBC4, likely implying compliance or compatibility changes rather than fixing existing bugs.

Therefore, based on the provided diff and the context of transitioning to JDBC4, I would conclude **NotBuggy** as it appears more of an enhancement or refactor for compatibility rather than an explicit bug fix."
hibernate-orm,12790.json,617975fefb42fe5bd6d3ae7893e96077cb44c84a,"@@ -1,17 +1,17 @@
 	public Blob createBlob(InputStream inputStream, long length) {
 		try {
 			Blob blob = createBlob();
 			OutputStream byteStream = blob.setBinaryStream( 1 );
 			StreamUtils.copy( inputStream, byteStream );
 			byteStream.flush();
 			byteStream.close();
 			// todo : validate length written versus length given?
 			return blob;
 		}
 		catch ( SQLException e ) {
-			throw new IllegalStateException( ""Unable to prepare BLOB binary stream for writing"", e );
+			throw new JDBCException( ""Unable to prepare BLOB binary stream for writing"",e );
 		}
 		catch ( IOException e ) {
-			throw new IllegalStateException( ""Unable to write stream contents to BLOB"", e );
+			throw new HibernateException( ""Unable to write stream contents to BLOB"", e );
 		}
 	}",NotBuggy,"HHH-4560 - JDBC4 support inadvertently missed 1.4 compatibility


git-svn-id: https://svn.jboss.org/repos/hibernate/core/trunk@17951 1b8cb986-b30d-0410-93ca-fae66ebed9b2
",Buggy,"The commit message refers to maintaining compatibility with Java 1.4 while using JDBC4 features. The diff shows exception handling changes:

1. **Change in Exception Types**: 
   - `IllegalStateException` is replaced with `JDBCException` and `HibernateException`.
   - Using `JDBCException` and `HibernateException` aligns with JDBC and Hibernate-specific exception handling, which might ensure the application behaves correctly in environments integrated with JDBC and Hibernate.

2. **Relevance to Bug Fix**:
   - If the original exceptions (`IllegalStateException`) were inappropriate or not descriptive enough in this context, changing them can ensure better error handling and potentially address issues related to compatibility or proper exception propagation, which might manifest as bugs, especially in environments managing transactions or resources differently.

3. **Alignment with Commit Message**:
   - The exception handling mapping to JDBC and Hibernate-specific exceptions may likely help in addressing compatibility or interoperability issues inadvertently causing exceptions to be missed or misinterpreted in a mixed environment like JDBC with potential backward compatibility concerns.

The changes in exception handling could be part of ensuring the system handles errors in a way that's compatible with JDBC and Hibernate while maintaining compatibility with older Java versions. This seems indirectly related to a bug, focused on compatibility and proper exception contexts. Thus, considering these factors, the concluding assessment is **Buggy**."
hibernate-orm,3126.json,9e063ffa2577f06d98a9e912bb16d20424df8d6d,"@@ -1,3 +1,3 @@
 	public Getter getGetter(Class clazz) throws PropertyNotFoundException, MappingException {
-		return getPropertyAccessor(clazz).getGetter( clazz, name );
+		return getPropertyAccessStrategy( clazz ).buildPropertyAccess( clazz, name ).getGetter();
 	}",NotBuggy,"HHH-9837 - Remove reliance during annotation binding on org.hibernate.internal.util.ClassLoaderHelper
HHH-9841 - Redesign org.hibernate.property.PropertyAccessorFactory
",NotBuggy,"This change in the method `getGetter` reflects a redesign from using `getPropertyAccessor` to `getPropertyAccessStrategy` and `buildPropertyAccess`. The commit message outlines a redesign task (HHH-9841), not indicating a bug fix. There's no mention of error handling or logical fixes, only an alteration in approach or architecture, implying the modifications are more about design refactoring rather than fixing a bug. Therefore, the changes do not suggest a bug fix.

**NotBuggy**"
hibernate-orm,30500.json,2f92109c48fb487870af6d93ba8dc884838179d0,"@@ -1,13 +1,11 @@
-	public static Map<String, ParsedPersistenceXmlDescriptor> parse(
+	public static Map<String,ParsedPersistenceXmlDescriptor> parse(
 			URL persistenceXmlUrl,
 			PersistenceUnitTransactionType transactionType,
-			Map integration
-	) {
+			Map integration) {
 		PersistenceXmlParser parser = new PersistenceXmlParser(
 				ClassLoaderServiceImpl.fromConfigSettings( integration ),
 				transactionType
 		);
 
-		parser.doResolve( integration );
-		return parser.persistenceUnits;
+		return parser.doResolve( integration );
 	}",NotBuggy,"Revert ""HHH-11845 - Warn user when multiple persistence-units use the same name""

This reverts commit 1242fd9580fdfb2a8b14ebb419ccc5d40ae01a01.
",NotBuggy,"The commit message indicates a revert of a previous change, specifically reverting the addition of a feature that warned users about multiple persistence units with the same name. The diff modifies the return statement in the `parse` function to restore its earlier state, returning the result of `parser.doResolve` directly.

In this case, the focus is not on fixing a bug but rather on reverting a feature that was previously added. Reversion might be due to various reasons such as unintended side effects or further requirements for implementation, but it does not directly indicate a bug fix. 

Given the nature of this change, which is a reversion and not a correction or improvement of an error, the assessment should be **NotBuggy**."
hibernate-orm,24481.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
-		public boolean isPassNullsEnabled() {
-			return nativeParamRegistration.isPassNullsEnabled();
-		}

+	public boolean isPassNullsEnabled() {
+		return nativeParamRegistration.isPassNullsEnabled();
+	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,"The commit message indicates a merge of `hibernate-entitymanager` into `hibernate-core`, focusing on consolidating code for a successful compilation in the Hibernate project. This specific change simply adjusts the indentation or formatting of an existing method. There is no indication of a bug fix related to logic, error-handling, or functional aspects in this diff. Instead, it appears to be a cosmetic change made as part of the larger task indicated in the commit message.

Given that:
- The modification doesn't alter logic or behavior.
- There's no mention of a bug or erroneous behavior being addressed in the `isPassNullsEnabled()` method.

The changes are consistent with the refactoring and merging process stated in the commit message, without explicitly signaling a bug fix.

Therefore, the answer is **NotBuggy**."
hibernate-orm,13970.json,ba3359fe62be258638554fe23a2a0a6a50f7e732,"@@ -1,3 +1,3 @@
-		public boolean isEntityClass(CtClass classDescriptor) {
-			return wrappedContext.isEntityClass( classDescriptor );
-		}

+	public boolean isEntityClass(CtClass classDescriptor) {
+		return enhancementContext.isEntityClass( new UnloadedCtClass( classDescriptor ) );
+	}",NotBuggy,"HHH-11152: Added BytecodeProvider based on Byte Buddy
",NotBuggy,"The code change involves a modification in the `isEntityClass` method, which now uses `enhancementContext.isEntityClass` with a new `UnloadedCtClass` instance wrapper instead of directly passing the `classDescriptor` to `wrappedContext.isEntityClass`.

Here's a breakdown of what this change could imply:

1. **Refactor Instead of a Bug Fix**: 
   - The change in method signature and how the argument is being processed suggests a refactor rather than directly fixing a bug. It indicates an enhancement (as seen by moving to use `UnloadedCtClass`), rather than specifically correcting erroneous behavior.
   
2. **No Mention of Byte Buddy Impact**: 
   - The commit message indicates the introduction of Byte Buddy as a `BytecodeProvider`, but the change in this method does not directly indicate how it relates to Byte Buddy. It seems to be related to an enhancement or refactoring task rather than bug fixing.

3. **No Error Handling Changes**:
   - There are no changes in error handling, exception handling, or other direct fixes to problematic logic in this code.

Given the focus of the commit message, this change seems more aligned with an enhancement approach, particularly in supporting other functionality potentially related to Byte Buddy, rather than fixing a specific bug.

Therefore, based on the evidence provided, the changes do not appear to indicate a bug fix, leading to the conclusion being **NotBuggy**."
hibernate-orm,9325.json,08747fc2f4b33414ef05ac039b67e1d33c14db5f,"@@ -1,6 +1,6 @@
 	private Object resolveAny(String entityName, Serializable id, SharedSessionContractImplementor session)
 			throws HibernateException {
 		return entityName==null || id==null
 				? null
-				: session.internalLoad( entityName, id, false, false );
+				: session.internalLoad( entityName, id, eager, false );
 	}",NotBuggy,"HHH-13094 - Respect @Any.fetch setting to FetchType.EAGER
",Buggy,"The commit message indicates a fix related to respecting the `@Any.fetch` setting for eager fetching. The code change modifies the `internalLoad` method call, replacing the hardcoded `false` with `eager`. This change likely uses a variable or context-related condition, aligning with the `@Any.fetch` configuration, thereby addressing the wrong fetch strategy being used.

Here's the analysis broken down:
1. **Context Understanding**: The function `resolveAny` loads entities and the change impacts how loading is performed.
2. **Original Code**: The fetch strategy is hardcoded as `false`, likely not respecting an eager fetch requirement.
3. **Revised Code**: Replaces the second `false` with `eager`, implying a dynamic or driven approach by a variable or a setting (probably tied to `@Any.fetch`).
4. **Connection to Commit Message**: The commit states the need to respect `@Any.fetch` settings, and the change aligns with configuring the loading approach as per annotations.
5. **Conclusion**: The change resolves a logical oversight (bug) where the fetch type was not respected, which could lead to issues in how entities are loaded, particularly affecting performance or correctness in retrieval.

Therefore, this change is indeed a bug fix. The conclusion is **Buggy**."
hibernate-orm,15177.json,ae43670290d05db28c322acb67431fe88db537d2,"@@ -1,3 +1,3 @@
-	public String getCreateTemporaryTablePostfix() {
+	public String getCreateIdTableStatementOptions() {
 		return "" on commit preserve rows"";
 	}",NotBuggy,"HHH-9724 - More complete ""temp table"" coverage and allow Dialects to influence which strategy is used
",NotBuggy,"The commit message indicates enhancements related to ""temp table"" coverage and strategies in database dialects. The diff shows a method name change, which likely aligns method naming with strategy improvements, or logical code modifications elsewhere to improve temporary table handling. However, there is no direct indication of a bug fix in the presented change. There's no apparent error correction, handling improvement, or logical fix in just renaming a method. Given the context and provided information alone, this is more related to feature enhancement or refactoring rather than bug fixing. Therefore, my assessment is **NotBuggy**."
hibernate-orm,11271.json,87e3f0fd28d7229b0a35032292ff32f542c22010,"@@ -1,3 +1,3 @@
 	public int executeNativeUpdate(NativeSQLQuerySpecification specification, QueryParameters queryParameters) throws HibernateException {
-		return sessionImplementor.executeNativeUpdate( specification, queryParameters );
+		return delegate.executeNativeUpdate( specification, queryParameters );
 	}",NotBuggy,"HHH-10664 - Prep 6.0 feature branch - merge hibernate-entitymanager into hibernate-core (first sucessful full compile of consolidated hibernate-core)
",NotBuggy,"The commit message suggests a preparation step for merging the `hibernate-entitymanager` into `hibernate-core`, with no focus on bug fixes. The code change modifies a method call to use `delegate` instead of `sessionImplementor`, which is likely part of the consolidation process rather than a bug fix, as there's no indication of correcting incorrect behavior.

Given the context, if this change doesn't directly address an error or logical flaw in the existing functionality, it likely aligns more with codebase restructuring or refactoring consistent with project goals for refactoring and unifying modules.

Therefore, based on the information provided, the change is **NotBuggy**."
presto,14100.json,8cb9c88804a6c7f7cc995fc729b862933e8d6b4d,"@@ -1,4 +1,4 @@
-    public static long cardinality(@SqlType(HyperLogLogType.NAME) Slice serializedHll)
+    public static long cardinality(@SqlType(StandardTypes.HYPER_LOG_LOG) Slice serializedHll)
     {
         return HyperLogLog.newInstance(serializedHll).cardinality();
     }",NotBuggy,"Move type names to their own class
",NotBuggy,"The commit message states that type names are being moved to their own class, suggesting a refactoring or organizational change. The code change shows the replacement of `HyperLogLogType.NAME` with `StandardTypes.HYPER_LOG_LOG`. This alteration indicates that the reference to the HyperLogLog type has been consolidated or standardized into a different class or a utility class (`StandardTypes`). 

However, there is no indication that this specific change is correcting any bug. The change aligns with the commit message as it's about moving type names but doesn't describe fixing incorrect behavior, error-handling adjustments, or logical corrections related to bugs. 

It seems more like a code cleanup or an improvement in organization to maintain type references in a standard or centralized way. Given these observations, the changes do not indicate a bug fix.

The answer is **NotBuggy**."
presto,34438.json,279eb7e2af95bddf5d357327eaf249021ead9f8d,"@@ -1,10 +1,5 @@
     public <T> Optional<LocalProperty<T>> translate(Function<E, Optional<T>> translator)
     {
-        Optional<T> translated = translator.apply(column);
-
-        if (translated.isPresent()) {
-            return Optional.of(new SortingProperty<>(translated.get(), order));
-        }
-
-        return Optional.empty();
+        return translator.apply(column)
+                .map(translated -> new SortingProperty<>(translated, order));
     }",NotBuggy,"Simplify the use of Optionals
",NotBuggy,"The commit simplifies the code by refactoring the handling of `Optional`. The original code uses `isPresent()` to check and retrieve the value, which is unnecessarily verbose. The new code uses `map` to transform the value inside the `Optional` if present, directly returning the transformed result as an `Optional`.

The changes are focused on streamlining the code by leveraging `Optional`'s fluent API, without altering its logic or fixing bugs. This aligns with the commit message's emphasis on simplification, indicative of a refactoring effort rather than a bug fix.

The answer is **NotBuggy**."
presto,26381.json,472538a02722c5409649533f9a602b5b76d9dd2d,"@@ -1,23 +1,23 @@
     public Configuration getConfiguration(FileSystemContext context, URI uri)
     {
         @SuppressWarnings(""resource"")
         Configuration config = new CachingJobConf((factoryConfig, factoryUri) -> {
             try {
                 FileSystem fileSystem = (new Path(factoryUri)).getFileSystem(hiveHdfsConfiguration.getConfiguration(context, factoryUri));
                 checkState(fileSystem instanceof ExtendedFileSystem);
-                return new CachingFileSystem(
+                return new FileMergeCachingFileSystem(
                         factoryUri,
                         factoryConfig,
                         cacheManager,
                         (ExtendedFileSystem) fileSystem,
                         cacheValidationEnabled);
             }
             catch (IOException e) {
                 throw new PrestoException(GENERIC_INTERNAL_ERROR, ""cannot create caching FS"", e);
             }
         });
         Configuration defaultConfig = hiveHdfsConfiguration.getConfiguration(context, uri);
 
         copy(defaultConfig, config);
         return config;
     }",NotBuggy,"Add Alluxio based data caching

This change enables Alluxio client-side local data cache on the worker.
This change also does a bit of refactoring of caching related pieces.
To enable Alluxio local cache, use following configuration
cache.enabled=true
cache.type=ALLUXIO

Co-authored-by: Bin Fan <fanbin103@gmail.com>
Co-authored-by: Bin Feng <binfeng@alluxio.com>
",NotBuggy,"The commit message describes adding Alluxio-based data caching and includes a bit of refactoring related to caching. The code diff reflects a part of this process by changing the instantiation from `CachingFileSystem` to `FileMergeCachingFileSystem`. This likely indicates transitioning to a new caching implementation that might be compatible with Alluxio or better integrated, which aligns with the refactoring aspect mentioned in the commit message. However, this change doesn't specifically address a bug fix. Instead, it suggests an enhancement or re-architecture to support the new caching framework.

Therefore, while the commit is enhancing caching capabilities and possibly optimizing the system with Alluxio integration, it does not explicitly indicate a bug fix in terms of error handling, exception improvements, or logic corrections.

The conclusion for this analysis is **NotBuggy**."
presto,37097.json,9ffb60c93e86e94c0c5a5a4bc21d61337344615d,"@@ -1,4 +1,4 @@
-            public MetadataReader createMetadataReader()
-            {
-                return new DwrfMetadataReader();
-            }

+        public MetadataWriter createMetadataWriter()
+        {
+            return new DwrfMetadataWriter();
+        }",NotBuggy,"Add OrcEncoding enum to differentiate between ORC and DWRF
",NotBuggy,"The commit message states the addition of an `OrcEncoding` enum to differentiate between ORC and DWRF, but the diff provided shows a change from a method `createMetadataReader` to `createMetadataWriter`, both involving DWRF. There is no mention or evidence of the introduction of an enum or differentiation logic between ORC and DWRF as per the commit message.

The changes in the diff don't appear related to fixing a logic error or resolving a bug. Instead, this looks like a change in functionality where a writer is created instead of a reader. As such, without additional context linking these changes to a correction of erroneous behavior or bug, these changes do not suggest a bug fix.

Therefore, the final conclusion is **NotBuggy**."
presto,26237.json,c8447e695d5434c811fc53d4920eaa8df9ec8bd0,"@@ -1,12 +1,4 @@
-    private void startBackupCleanup()
+    public void startBackupCleanup()
     {
-        scheduler.scheduleWithFixedDelay(() -> {
-            try {
-                cleanBackupShards();
-            }
-            catch (Throwable t) {
-                log.error(t, ""Error cleaning backup shards"");
-                backupJobErrors.update(1);
-            }
-        }, 0, backupCleanerInterval.toMillis(), MILLISECONDS);
+        scheduler.submit(this::runBackupCleanup);
     }",NotBuggy,"Expose recovery and cleaning methods through JMX
",NotBuggy,"The commit message suggests exposing methods via JMX, likely involving adjusting method visibility or integrating with a JMX framework to manage or monitor these methods externally. The code change modifies `startBackupCleanup` from private to public, allowing external access, aligning with the idea of exposure through JMX.

However, the code change doesn't handle any logical errors, exceptions, or bugs in the existing functionality of `startBackupCleanup`. Instead, it focuses on refactoring and changing the method's access level. The move from using `scheduleWithFixedDelay` to `submit` indicates a change in how tasks are scheduled but doesn't directly reflect a bug fix. 

Thus, while the commit message is relevant to the code change, it doesn't indicate a bug fix, but rather a feature enhancement or code refactoring to expose functionality.

The change is **NotBuggy**."
presto,29934.json,db03518bf342c8fb74ace9ba25f2b0c78991953a,"@@ -1,24 +1,24 @@
-    protected void renameTable(String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
+    protected void renameTable(JdbcIdentity identity, String catalogName, SchemaTableName oldTable, SchemaTableName newTable)
     {
-        try (Connection connection = connectionFactory.openConnection()) {
+        try (Connection connection = connectionFactory.openConnection(identity)) {
             DatabaseMetaData metadata = connection.getMetaData();
             String schemaName = oldTable.getSchemaName();
             String tableName = oldTable.getTableName();
             String newSchemaName = newTable.getSchemaName();
             String newTableName = newTable.getTableName();
             if (metadata.storesUpperCaseIdentifiers()) {
                 schemaName = schemaName.toUpperCase(ENGLISH);
                 tableName = tableName.toUpperCase(ENGLISH);
                 newSchemaName = newSchemaName.toUpperCase(ENGLISH);
                 newTableName = newTableName.toUpperCase(ENGLISH);
             }
             String sql = format(
                     ""ALTER TABLE %s RENAME TO %s"",
                     quoted(catalogName, schemaName, tableName),
                     quoted(catalogName, newSchemaName, newTableName));
             execute(connection, sql);
         }
         catch (SQLException e) {
             throw new PrestoException(JDBC_ERROR, e);
         }
     }",NotBuggy,"Pass JdbcIdentity To DriverConnectionFactory#openConnection

Co-Authored-By: Ke Wang <ke1024@fb.com>
",NotBuggy,"The commit message indicates that this change involves passing a `JdbcIdentity` to the `openConnection` method of the `DriverConnectionFactory`. In the diff, the `renameTable` method signature is modified to include the `JdbcIdentity` parameter, and the `openConnection` method is updated to accept this parameter.

The primary focus of this change seems to be related to authentication or user identity management when establishing a database connection. By passing `JdbcIdentity`, it is possible to ensure that the connection is opened with the correct user credentials or context. This is likely part of an improvement or refactor related to connection management rather than a direct bug fix, unless there was a known issue caused by not passing the user identity previously, which is not specified in the commit message.

From the provided information, there is no explicit mention of a bug being fixed. Instead, the change seems to be an improvement or an enhancement in the handling of database connections.

Therefore, the conclusion is **NotBuggy**."
presto,17845.json,1e1df56d263e3338de3adc636df13ec5800e1413,"@@ -1,4 +1,4 @@
-    public ConnectorSplitManager getSplitManager()
+    public TransactionalConnectorSplitManager getSplitManager()
     {
         return splitManager;
     }",NotBuggy,"Make SystemTables transactional
",NotBuggy,"The given diff shows a change in the return type of the `getSplitManager` method from `ConnectorSplitManager` to `TransactionalConnectorSplitManager`. This reflects a transition from a non-transactional to a transactional split manager, which aligns directly with the commit message to make `SystemTables` transactional.

Here's the step-by-step reasoning:

1. **Commit Message Analysis**: The commit message is ""Make SystemTables transactional"". This suggests an intention to enhance the system tables with transaction capabilities or support.

2. **Code Change Examination**: 
    - The method `getSplitManager`'s return type is modified from `ConnectorSplitManager` to `TransactionalConnectorSplitManager`.
    - `TransactionalConnectorSplitManager` implies that the new split manager incorporates transactional capabilities that were not present with the standard `ConnectorSplitManager`.

3. **Relevance to the Message**: The change directly aligns with the commit message. By changing the return type to a transactional manager, the system tables now likely support transactions or are executed in a transactional manner.

4. **Indication of a Bug Fix**: While the commit might mean enhancing functionality to add transactions rather than fixing an immediate bug, moving to a transactional system could address underlying consistency or atomicity issues inherent in non-transactional systems, indirectly fixing potential bugs related to data integrity.

Given the above reasoning, the changes appear to aim at functional enhancement rather than directly fixing a bug. The primary intent is the introduction of transactional functionality rather than correcting an error. Therefore, the determination here would be **NotBuggy**."
presto,14579.json,7cf87bd218ba996089fd42fc2796e53c69d40d7b,"@@ -1,4 +1,4 @@
-    public static double log10(@SqlType(DoubleType.class) double num)
+    public static double log10(@SqlType(DoubleType.NAME) double num)
     {
         return Math.log10(num);
     }",NotBuggy,"Change @SqlType to take a String
",NotBuggy,"The commit message states a change from using a class reference to using a string, specifically referring to `@SqlType`. The code diff supports this, as it changes the parameter from `DoubleType.class` to `DoubleType.NAME`. This update reflects a change in how the `@SqlType` annotation specifies the type, switching from a class type to a string representation.

This change appears to focus on a code convention or an API adjustment rather than directly addressing a bug. The alteration does not indicate fixes for error handling, logical errors, or improvements in exception management. Instead, it aligns with a change in how type information is specified in the code.

Therefore, this commit does not suggest a bug fix and instead seems to be an adaptation or improvement based on a design decision. Hence, the conclusion is **NotBuggy**."
presto,23057.json,41c77cf7606f1e111c66c9a5ee720577f6268bac,"@@ -1,8 +1,8 @@
     public String toString()
     {
-        return MoreObjects.toStringHelper(this)
+        return toStringHelper(this)
                 .add(""major"", major)
                 .add(""minor"", minor)
                 .add(""update"", update)
                 .toString();
     }",NotBuggy,"Static import toStringHelper
",NotBuggy,"The commit message indicates a change involving a static import, which is a refactoring action meant to simplify the code by eliminating the need to qualify the `toStringHelper` method with its class name. The code diff reflects this by changing `MoreObjects.toStringHelper(this)` to `toStringHelper(this)`, assuming `toStringHelper` has been statically imported.

There are no indications of error handling updates, logical corrections, exception-handling improvements, or similar bug-related changes in this diff. It exclusively deals with code style or simplicity by utilizing static imports to potentially make the code more readable.

Given that the change aligns with the commit message but does not indicate a bug fix, the conclusion is **NotBuggy**."
presto,18660.json,54478b6f30fab15d960676ad2dbc1c0005c1fb47,"@@ -1,8 +1,7 @@
-    public Optional<PlanNode> apply(PlanNode node, Context context)
+    public Optional<PlanNode> apply(ApplyNode applyNode, Captures captures, Context context)
     {
-        ApplyNode applyNode = (ApplyNode) node;
         if (applyNode.getSubqueryAssignments().isEmpty()) {
             return Optional.of(applyNode.getInput());
         }
         return Optional.empty();
     }",NotBuggy,"Parameterize Rule with pattern output type, remove root node casts
",NotBuggy,"The commit message suggests changes related to parameterization and removal of root node casts. The diff shows removing an explicit cast by changing the method signature to accept an `ApplyNode` directly instead of a generic `PlanNode`. This aligns with the commit message but does not indicate a bug fix. Instead, it indicates code clean-up and possibly increased type safety by leveraging method signature over casting. There are no changes related to fixing logic errors, exception handling, or other bug-related corrections in this diff. The answer is **NotBuggy**."
presto,10382.json,7c58b7cfb786bc2dc1309c42e18479577f0d2ffc,"@@ -1,4 +1,4 @@
-        public int size()
-        {
-            return delegate.size();
-        }

+    public int size()
+    {
+        return delegate.size();
+    }",NotBuggy,"Rename LinkedHashQueue to FifoQueue

Also make it a top-level class
",NotBuggy,"The commit message does not appear to describe any bug fixes; instead, it talks about renaming a class and making it a top-level class. The diff, however, doesn't show any renaming or structural change. It purely changes the indentation of the `size()` method, which is a cosmetic change for code formatting purposes and not a modification of functionality.

Given that:

1. The commit message primarily describes a restructuring and renaming task.
2. The code diff indicates only changes to code formatting with no logical changes or bug-related updates.

The changes shown in the diff do not directly align with the commit message and do not indicate a bug fix. Therefore, I conclude with **NotBuggy**."
presto,11220.json,efd979afa5efcf616ac1ae0b806fcad3a0cf1ab0,"@@ -1,4 +1,4 @@
-    public void checkCanShowRoleGrants(TransactionId transactionId, Identity identity, String catalogName)
+    public void checkCanShowCurrentRoles(TransactionId transactionId, Identity identity, AccessControlContext context, String catalogName)
     {
-        denyShowRoleGrants(catalogName);
+        denyShowCurrentRoles(catalogName);
     }",NotBuggy,"Add AccessControlContext to store client information for security purposes
",NotBuggy,"The commit message indicates the addition of `AccessControlContext` for storing client information for security purposes. However, in the provided diff:

1. The method signature changes from `checkCanShowRoleGrants` to `checkCanShowCurrentRoles`. This suggests a refactor or renaming for clarity rather than a direct security enhancement or bug fix.
2. An `AccessControlContext context` parameter is added, but there's no implementation showing how this new parameter is used for security purposes. 
3. The invocations of `denyShowRoleGrants` change to `denyShowCurrentRoles`, aligning with the method name change.

While the modification includes the parameter intended for security reasons, the diff doesn't demonstrate any logic using `AccessControlContext` for security, nor does it resolve an existing bug explicitly.

The commit message and the diff seem slightly misaligned concerning the actual change demonstrated. While there might be an intention to use `AccessControlContext` for security, the diff implies more of a method renaming/update without implementing security logic.

Conclusion: **NotBuggy**"
presto,32284.json,2d550a5123000f7965f16c0c5c594fc030e349a0,"@@ -1,4 +1,4 @@
     public List<String> listSchemaNames(ConnectorSession session)
     {
-        return ImmutableList.of(SCHEMA_NAME);
+        return ImmutableList.copyOf(schemas);
     }",NotBuggy,"Support CREATE SCHEMA in blackhole connector
",NotBuggy,"The commit message indicates a new feature to support ""CREATE SCHEMA"" in a connector, not a bug fix. The code change updates the method for listing schema names from returning a single schema to potentially returning multiple schemas. This aligns with supporting multiple schemas but does not address any bug fixes. The answer is **NotBuggy**."
presto,28423.json,4edc6ea6dc2ee43794889122d51652a8c88be5a8,"@@ -1,4 +1,4 @@
     public String toString()
     {
-        return Objects.toStringHelper(this).toString();
+        return toStringHelper(this).toString();
     }",NotBuggy,"Update to Airbase 28 with Guava 18.0
",NotBuggy,"The commit message indicates an update to Airbase 28 with Guava 18.0. The code change replaces `Objects.toStringHelper(this)` with `toStringHelper(this)`. This is related to a Guava update, where `Objects.toStringHelper` was deprecated and moved to `MoreObjects.toStringHelper` in newer versions. However, this does not explicitly indicate a bug fix but rather an update to accommodate library changes, ensuring compatibility with the updated Guava library.

The change does not align with the indication of a bug fix in the commit message but notes a necessary adjustment for library compatibility. Therefore, the conclusion is **NotBuggy**."
presto,11869.json,4ae5d1ecaf56ad7ef3d07bd86090eec806a9cf23,"@@ -1,6 +1,7 @@
     private void addRowsToAggregates(Page page, int startPosition, int endPosition)
     {
+        Page region = page.getRegion(startPosition, endPosition - startPosition + 1);
         for (Aggregator aggregator : aggregates) {
-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));
+            aggregator.processPage(region);
         }
     }",NotBuggy,"Remove unnecessary copies in StreamingAggregationOperator

The previous implementation copied the aggregation group out of the
page for each aggregation operator which is wasteful and extremely
slow when the number of aggregates performed is large.

Performance improvement depends on the number of aggregations and
the number of groups contained in the page, but some workloads can
improve throughput by more than 100% and generate significantly
less garbage
",NotBuggy,"The commit message is about removing unnecessary copies for performance improvement in the `StreamingAggregationOperator`. The code changes align with the message: a `Page` region is created once and reused, whereas before, each iteration created a new region. This optimizes performance by reducing the creation of temporary objects (regions), resulting in less garbage and potential throughput improvements. The changes reflect performance optimization with no indication of a bug fix. The answer is **NotBuggy**."
netty,14898.json,e6c9ac968d3923080822dc36fe14aa10e38af15b,"@@ -1,59 +1,60 @@
     public void channelRead(final ChannelHandlerContext ctx, final Object msg) throws Exception {
         long size = calculateSize(msg);
         long now = TrafficCounter.milliSecondFromNano();
         if (size > 0) {
             // compute the number of ms to wait before reopening the channel
             long waitGlobal = trafficCounter.readTimeToWait(size, getReadLimit(), maxTime, now);
             Integer key = ctx.channel().hashCode();
             PerChannel perChannel = channelQueues.get(key);
             long wait = 0;
             if (perChannel != null) {
                 wait = perChannel.channelTrafficCounter.readTimeToWait(size, readChannelLimit, maxTime, now);
                 if (readDeviationActive) {
                     // now try to balance between the channels
                     long maxLocalRead;
                     maxLocalRead = perChannel.channelTrafficCounter.cumulativeReadBytes();
                     long maxGlobalRead = cumulativeReadBytes.get();
                     if (maxLocalRead <= 0) {
                         maxLocalRead = 0;
                     }
                     if (maxGlobalRead < maxLocalRead) {
                         maxGlobalRead = maxLocalRead;
                     }
                     wait = computeBalancedWait(maxLocalRead, maxGlobalRead, wait);
                 }
             }
             if (wait < waitGlobal) {
                 wait = waitGlobal;
             }
             wait = checkWaitReadTime(ctx, wait, now);
             if (wait >= MINIMAL_WAIT) { // At least 10ms seems a minimal
                 // time in order to try to limit the traffic
                 // Only AutoRead AND HandlerActive True means Context Active
-                ChannelConfig config = ctx.channel().config();
+                Channel channel = ctx.channel();
+                ChannelConfig config = channel.config();
                 if (logger.isDebugEnabled()) {
                     logger.debug(""Read Suspend: "" + wait + ':' + config.isAutoRead() + ':'
                             + isHandlerActive(ctx));
                 }
                 if (config.isAutoRead() && isHandlerActive(ctx)) {
                     config.setAutoRead(false);
-                    ctx.attr(READ_SUSPENDED).set(true);
+                    channel.attr(READ_SUSPENDED).set(true);
                     // Create a Runnable to reactive the read if needed. If one was create before it will just be
                     // reused to limit object creation
-                    Attribute<Runnable> attr = ctx.attr(REOPEN_TASK);
+                    Attribute<Runnable> attr = channel.attr(REOPEN_TASK);
                     Runnable reopenTask = attr.get();
                     if (reopenTask == null) {
                         reopenTask = new ReopenReadTimerTask(ctx);
                         attr.set(reopenTask);
                     }
                     ctx.executor().schedule(reopenTask, wait, TimeUnit.MILLISECONDS);
                     if (logger.isDebugEnabled()) {
                         logger.debug(""Suspend final status => "" + config.isAutoRead() + ':'
                                 + isHandlerActive(ctx) + "" will reopened at: "" + wait);
                     }
                 }
             }
         }
         informReadOperation(ctx, now);
         ctx.fireChannelRead(msg);
     }",NotBuggy,"Cleanup: replaced deprecated ctx.attr() and ctx.hasAttr() methods usage with ch.attr() and ch.hasAttr().

Motivation:

Will allow easy removal of deprecated methods in future.

Modification:

Replaced ctx.attr(), ctx.hasAttr() with ctx.channel().attr(), ctx.channel().hasAttr().

Result:

No deprecated ctx.attr(), ctx.hasAttr() methods usage.
",NotBuggy,"The commit message indicates the purpose is to replace deprecated methods with their recommended alternatives. The diff shows `ctx.attr()` and `ctx.hasAttr()` being replaced with `ctx.channel().attr()` and `ctx.channel().hasAttr()`. 

This aligns with the commit's purpose of cleaning up deprecated method usage rather than fixing a bug. Additionally, this change merely enhances code maintainability by preparing for future removals, rather than correcting erroneous or faulty behavior. 

Thus, the changes correspond precisely with the commit message, yet they do not directly indicate a bug fix. The focus is on refactoring to avoid deprecated methods rather than resolving incorrect logic or exceptions. The conclusion is **NotBuggy**."
netty,7557.json,ecfa241768499dae2bffa174a874e8f9d81d6d1f,"@@ -1,3 +1,3 @@
-        public void setTrailingHeaders(HttpHeaders trailingHeaders) {
+        void setTrailingHeaders(HttpHeaders trailingHeaders) {
             this.trailingHeaders = trailingHeaders;
         }",NotBuggy,"Make sure AggregatedFullHttpMessage.trailingHeaders() return non-null

When handling an oversized message, HttpObjectAggregator does not wait
until the last chunk is received to produce the failed message, making
AggregatedFullHttpMessage.trailingHeaders() return null.
",NotBuggy,"The commit message suggests a bug fix whereby `AggregatedFullHttpMessage.trailingHeaders()` should never return null. However, the actual change in the diff only modifies the accessibility of the `setTrailingHeaders` method from `public` to package-private. This diff does not appear to directly address the issue of ensuring `trailingHeaders()` returns a non-null value since it only changes the method's access level, not its behavior or the return value handling. 

Given this analysis, the change in the code does not directly reflect a bug fix related to the described null return issue. Thus, the conclusion is **NotBuggy** as the specific problem mentioned in the commit message isn't resolved by the given code alterations."
netty,525.json,00d2cea8bad744d15a5daed1e36590308f715ee3,"@@ -1,3 +1,3 @@
-    public QoS qualityOfService() {
+    public MqttQoS qualityOfService() {
         return qualityOfService;
     }",NotBuggy,"Overall clean-up on codec-mqtt

- Use simple string concatenation instead of String.format()
- Rewrite exception messages so that it follows our style
- Merge MqttCommonUtil and MqttValidationUtil into MqttCodecUtil
- Hide MqttCodecUtil from users
- Rename MqttConnectReturnCode.value to byteValue
- Rename MqttMessageFactory.create*() to new*()
- Rename QoS to MqttQoS
- Make MqttSubAckPayload.grantedQoSLevels immutable and add more useful
  constructor
",NotBuggy,"The provided commit message suggests refactoring and code maintenance tasks. The diff shows a class rename for improved clarity: `QoS` to `MqttQoS`. This aligns with the commit's intention to improve readability and maintain consistent naming conventions.

No indication of bug fixes, error-handling updates, or logical corrections is apparent in the diff. The changes focus solely on code readability and organization.

Therefore, the assessment concludes with **NotBuggy**."
netty,3934.json,3d81afb8a5ec31420fb503bc9476ecf23bb6bbfa,"@@ -1,9 +1,11 @@
         public void addTimeout(HashedWheelTimeout timeout) {
+            assert timeout.bucket == null;
+            timeout.bucket = this;
             if (head == null) {
                 head = tail = timeout;
             } else {
                 tail.next = timeout;
                 timeout.prev = tail;
                 tail = timeout;
             }
         }",NotBuggy,"Make sure cancelled Timeouts are able to be GC'ed fast.

Motivation:
At the moment the HashedWheelTimer will only remove the cancelled Timeouts once the HashedWheelBucket is processed again. Until this the instance will not be able to be GC'ed as there are still strong referenced to it even if the user not reference it by himself/herself. This can cause to waste a lot of memory even if the Timeout was cancelled before.

Modification:
Add a new queue which holds CancelTasks that will be processed on each tick to remove cancelled Timeouts. Because all of this is done only by the WorkerThread there is no need for synchronization and only one extra object creation is needed when cancel() is executed. For addTimeout(...) no new overhead is introduced.

Result:
Less memory usage for cancelled Timeouts.
",NotBuggy,"The commit message talks about improving the garbage collection of cancelled `Timeouts` by introducing a new queue to process cancelled tasks. However, the code diff provided here does not show the addition of any new queue or changes to the management of cancelled timeouts. It only includes an assertion to verify that the `bucket` of a `HashedWheelTimeout` is null before assigning it, which ensures correct usage but doesn't directly address the garbage collection or memory management issues stated in the commit message.

While this change ensures proper state before adding a timeout to a bucket and may be part of memory management improvements, by itself, it doesn't directly align with the main modification explained in the commit message. Without evidence of the introduced queue or cancellation processing logic in this diff, we cannot conclude if this resolves the stated motivation regarding memory usage of cancelled timeouts.

Therefore, based on the diff provided, we cannot assess this as a bug fix specifically related to the commit message. The assertion is a safety check rather than a fix for cancellation memory issues.

Hence, the assessment is **NotBuggy**."
netty,17881.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public ByteBuf resumeIntermediaryDeallocations() {
-            return this;
-        }

+    public ByteBuf markWriterIndex() {
+        return this;
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message describes the implementation of reference counting and the addition of several classes and interfaces related to reference counting. It does not mention anything about a bug fix.

The provided diff shows a method replacement: `resumeIntermediaryDeallocations` is removed, and `markWriterIndex` is added. This change aligns with the commit message, which describes changes related to reference counting. However, the change does not suggest any corrections related to error handling, logical corrections, or other improvements typically associated with a bug fix. The change seems more about altering functionality related to how buffers are handled, possibly for resource management or API evolution, rather than directly addressing a bug.

Since the commit message focuses on adding and replacing components for reference counting without indicating a bug fix, and the diff doesn't highlight bug-related corrections, the conclusion is **NotBuggy**."
netty,12649.json,17280116c48b91c2dd8988d625f749318150de3b,"@@ -1,11 +1,9 @@
-    private void fail(ChannelHandlerContext ctx, long frameLength) {
+    private void fail(long frameLength) {
         if (frameLength > 0) {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "": "" + frameLength + "" - discarded"");
         } else {
-            ctx.fireExceptionCaught(
-                    new TooLongFrameException(
-                            ""frame length exceeds "" + maxFrameLength + "" - discarding""));
+            throw new TooLongFrameException(
+                            ""frame length exceeds "" + maxFrameLength + "" - discarding"");
         }
     }",NotBuggy,"[#2643] Throw TooLongFrameException instead of using fireExceptionCaught

Motivation:

It's not always the case that there is another handler in the pipeline that will intercept the exceptionCaught event because sometimes users just sub-class. In this case the exception will just hit the end of the pipeline.

Modification:
Throw the TooLongFrameException so that sub-classes can handle it in the exceptionCaught(...) method directly.

Result:
Sub-classes can correctly handle the exception,
",Buggy,"The commit message explains the intention to address an issue where exceptions might not be handled if they are only fired down the pipeline using `ctx.fireExceptionCaught()`. The modification changes the behavior from firing an exception within the pipeline to directly throwing the `TooLongFrameException`.

Let’s break it down:

1. **Original Issue**: The exceptions were fired using `ctx.fireExceptionCaught()`, which depends on other handlers in the pipeline to be caught. If the pipeline does not have a handler specifically intercepting these exceptions, they may go unhandled, especially in cases where users subclass without providing such a handler.

2. **Modification**: The method `fail` no longer takes `ChannelHandlerContext ctx` as an argument since it no longer uses `ctx.fireExceptionCaught()`. Instead, the exception is directly thrown using the `throw` keyword.

3. **Result**: By throwing the `TooLongFrameException`, the sub-classes that override or implement `exceptionCaught(...)` can directly manage or react to this exception, enhancing robustness and clarity in exception handling.

This change aligns with the commit message to ensure that exceptions are managed more explicitly, preventing potential oversight in exception handling from occurring due to unhandled pipeline events. These changes effectively address the issue described.

Hence, the changes in the diff reflect a fix that better handles exceptions, consistent with the commit message, indicating a bug fix. The conclusion is **Buggy**."
netty,14065.json,59973e93dd7da715eee709788573e3515cc50238,"@@ -1,3 +1,3 @@
-    public String getPeerHost() {
+    public final String getPeerHost() {
         return wrapped.getPeerHost();
     }",NotBuggy,"Ensure X509KeyManager methods are called on the correct time when using server-side and support more methods of ExtendedSSLSession. (#8283)

Motivation:

Before when on server-side we just called the X509KeyManager methods when handshake() was called the first time which is not quite correct as we may not have received the full SSL hello / handshake and so could not extra for example the SNI hostname that was requested.
OpenSSL exposes the SSL_CTX_set_cert_cb function which allows to set a callback which is executed at the correct moment, so we should use it. This also allows us to support more methods of ExtendedSSLSession easily.

Modifications:

- Make use of new methods exposed by netty-tcnative since https://github.com/netty/netty-tcnative/pull/388 to ensure we select the key material at the correct time.
- Implement more methods of ExtendedOpenSslSession
- Add unit tests to ensure we are able to retrieve various things on server-side in the X509KeyManager and so verify it is called at the correct time.
- Simplify code by using new netty-tcnative methods.

Result:

More correct implementation for server-side usage and more complete implemented of ExtendedSSLSession.",NotBuggy,"The change in the diff is small but important. It makes the `getPeerHost` method `final`, meaning it cannot be overridden in subclasses. This ensures that the behavior provided by `wrapped.getPeerHost()` is consistently used whenever `getPeerHost` is called, and it cannot be inadvertently altered by derived classes, which might lead to incorrect implementations.

However, the commit message is focused on timing corrections for `X509KeyManager` method calls and extending support for `ExtendedSSLSession` methods. While making `getPeerHost` final could be indirectly related to ensuring consistency in retrieving peer-host information, it doesn't directly address timing or introduce new method support for `ExtendedSSLSession` as stated in the commit message. 

Without more extensive context or seeing the larger change set, it is challenging to decisively link this specific change to a bug fix directly associated with the commit message. The change seems more like a preventive step for maintaining code integrity rather than a direct bug fix related to timing or new functionality.

Thus, the specific change in this diff, given the context of the message, suggests a **NotBuggy** verdict, as it alone doesn't directly correspond to the main bugs or improvements highlighted in the commit message. However, it is a structural integrity improvement."
netty,15217.json,7d4c077492ba5b40595b0742e7b22182c544f7b7,"@@ -1,5 +1,5 @@
-    public ByteBuf setInt(int index, int value) {
-        wrapped.checkIndex(index, 4);
-        _setInt(index, value);
+    public ByteBuf setByte(int index, int value) {
+        checkIndex(index);
+        _setByte(index, value);
         return this;
     }",NotBuggy,"Add *UnsafeHeapByteBuf for improve performance on systems with sun.misc.Unsafe

Motivation:

sun.misc.Unsafe allows us to handle heap ByteBuf in a more efficient matter. We should use special ByteBuf implementation when sun.misc.Unsafe can be used to increase performance.

Modifications:

- Add PooledUnsafeHeapByteBuf and UnpooledUnsafeHeapByteBuf that are used when sun.misc.Unsafe is ready to use.
- Add UnsafeHeapSwappedByteBuf

Result:

Better performance when using heap buffers and sun.misc.Unsafe is ready to use.
",NotBuggy,"The commit message describes performance improvements by introducing new ByteBuf implementations that leverage sun.misc.Unsafe. However, the provided diff shows a method signature change from setInt to setByte, involving setting a single byte value instead of an integer.

This change does not align with the commit message outlining performance enhancements using sun.misc.Unsafe. Instead, it suggests a correction where setInt incorrectly referred to single bytes. Potentially fixing a bug, the name update better reflects the altered behavior, transitioning checks and internal method calls from integer to byte handling. However, no direct evidence relates to bug fixes in the context of erroneous functionality. 

Despite code adjustments appearing functional (not performance-driven), indicating a probable bug in the original signature, more broad context is essential to ascertain with confidence. Nevertheless, the modification seems corrective within a generic sense.

Thus, without strong evidence of performance motives linked to the commit message or direct ties to bug correction, we ultimately conclude the change as **NotBuggy**."
netty,9425.json,c8ca3299324a1e2b9ce501c95715cf10ea70c238,"@@ -1,4 +1,4 @@
     public SpdyHeadersFrame setInvalid() {
-        super.setInvalid();
+        invalid = true;
         return this;
     }",NotBuggy,"SPDY: update object hierarchy
",NotBuggy,"The commit message states ""SPDY: update object hierarchy,"" suggesting a change in the class structure or relationships between classes. However, the diff shows a specific change in the method `setInvalid`. The original code calls `super.setInvalid()`, which likely refers to a method in a superclass responsible for marking the object as invalid. The change directly sets an `invalid` field to `true` in this class instead.

This modification indicates a potential alteration in how invalid states are handled, sidelining the superclass logic. While the message does not directly mention a bug fix, changing the mechanism to mark an object as invalid can be critical if the inherited behavior was erroneous. Yet, we lack evidence of such an error.

Without clear signs of correcting erroneous behavior from the superclass, or additional context from the commit message highlighting a specific issue resolved by this change, we can't confidently classify this as a bug fix. Therefore, the alteration appears more aligned with an internal design change rather than addressing a specific bug.

The appropriate conclusion here is **NotBuggy**."
netty,1157.json,dfa3bbbf0035e6eb39403194c9aabae0f9c2c1a3,"@@ -1,3 +1,3 @@
-    public List<DnsCacheEntry> get(String hostname) {
+    public List<DnsCacheEntry> get(String hostname, DnsRecord[] additionals) {
         return Collections.emptyList();
     }",NotBuggy,"Add support for Client Subnet in DNS Queries (RFC7871)

Motivation:

RFC7871 defines an extension which allows to request responses for a given subset.

Modifications:

- Add DnsOptPseudoRrRecord which can act as base class for extensions based on EDNS(0) as defined in RFC6891
- Add DnsOptEcsRecord to support the Client Subnet in DNS Queries extension
- Add tests

Result:

Client Subnet in DNS Queries extension is now supported.
",NotBuggy,"The commit message and the code diff both focus on implementing support for Client Subnet in DNS Queries as per RFC7871, involving modifications to DNS-related classes. However, the change visible in the diff involves updating the method signature of `get` to include a new parameter, `DnsRecord[] additionals`. This change alone does not indicate a bug fix — it appears to be a feature addition to extend functionality, allowing additional DNS records to be handled, which aligns with the stated goal of supporting new RFC functionality.

The commit does not explicitly point to any logical corrections, error handling, exception handling, or immediate bug fixes. It seems oriented towards extending the capability of the code rather than fixing an existing problem.

Hence, given this analysis, the conclusion is **NotBuggy**."
netty,12457.json,3a9f47216143082bdfba62e8940160856767d672,"@@ -1,4 +1,3 @@
     public ByteBuf setShort(int index, int value) {
-        reject();
-        return this;
+        throw reject();
     }",NotBuggy,"Make retained derived buffers recyclable

Related: #4333 #4421 #5128

Motivation:

slice(), duplicate() and readSlice() currently create a non-recyclable
derived buffer instance. Under heavy load, an application that creates a
lot of derived buffers can put the garbage collector under pressure.

Modifications:

- Add the following methods which creates a non-recyclable derived buffer
  - retainedSlice()
  - retainedDuplicate()
  - readRetainedSlice()
- Add the new recyclable derived buffer implementations, which has its
  own reference count value
- Add ByteBufHolder.retainedDuplicate()
- Add ByteBufHolder.replace(ByteBuf) so that..
  - a user can replace the content of the holder in a consistent way
  - copy/duplicate/retainedDuplicate() can delegate the holder
    construction to replace(ByteBuf)
- Use retainedDuplicate() and retainedSlice() wherever possible
- Miscellaneous:
  - Rename DuplicateByteBufTest to DuplicatedByteBufTest (missing 'D')
  - Make ReplayingDecoderByteBuf.reject() return an exception instead of
    throwing it so that its callers don't need to add dummy return
    statement

Result:

Derived buffers are now recycled when created via retainedSlice() and
retainedDuplicate() and derived from a pooled buffer
",NotBuggy,"The commit message outlines various changes focused on making buffers recyclable, enhancing performance by reducing garbage collection pressure. Examining the diff, it modifies `setShort(int index, int value)` by changing it from calling `reject()` and returning the current instance to directly throwing the exception returned by `reject()`. Although this specific change targets exception handling and code consistency, it aligns more with non-bug-related improvements. The changes in the diff itself don't directly point to a bug fix but rather a refactoring for consistency. Therefore, this code change can be concluded as **NotBuggy**."
netty,14470.json,5ac84760c4f5d4f09a61fb5cd2cac7d95a782f9f,"@@ -1,11 +1,15 @@
     public SslContextBuilder keyManager(File keyCertChainFile, File keyFile, String keyPassword) {
-        if (forServer) {
-            checkNotNull(keyCertChainFile, ""keyCertChainFile required for servers"");
-            checkNotNull(keyFile, ""keyFile required for servers"");
+        X509Certificate[] keyCertChain;
+        PrivateKey key;
+        try {
+            keyCertChain = SslContext.toX509Certificates(keyCertChainFile);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid certificates: "" + keyCertChainFile, e);
         }
-        this.keyCertChainFile = keyCertChainFile;
-        this.keyFile = keyFile;
-        this.keyPassword = keyPassword;
-        this.keyManagerFactory = null;
-        return this;
+        try {
+            key = SslContext.toPrivateKey(keyFile, keyPassword);
+        } catch (Exception e) {
+            throw new IllegalArgumentException(""File does not contain valid private key: "" + keyFile, e);
+        }
+        return keyManager(key, keyPassword, keyCertChain);
     }",NotBuggy,"Allow to create SslContext from existing PrivateKey / X509Certificate

Motivation:

Sometimes the user already has a PrivateKey / X509Certificate which should be used to create a new SslContext. At the moment we only allow to construct it via Files.

Modifications:

- Add new methods to the SslContextBuilder to allow creating a SslContext from PrivateKey / X509Certificate
- Mark all public constructors of *SslContext as @Deprecated, the user should use SslContextBuilder
- Update tests to us SslContextBuilder.

Result:

Creating of SslContext is possible with PrivateKay/X509Certificate
",NotBuggy,"The commit message indicates that new functionality has been added to create an `SslContext` from an existing `PrivateKey` and `X509Certificate`, rather than solely from files. The code changes reflect this intention by introducing the conversion from file-based inputs to direct use of `PrivateKey` and `X509Certificate`. The `keyManager` method now processes files into these objects and captures any conversion issues with comprehensive error messages using exception handling. This update improves robustness but is primarily a feature enhancement rather than a bug fix; no bugs or errors are directly addressed.

Given this context, the changes are focused on adding functionality rather than fixing bugs, so the answer is **NotBuggy**."
netty,11382.json,f9001b9fc07a71a9d6eaf0462470416780302107,"@@ -1,21 +1,21 @@
-    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, HpackUtil.IndexType indexType,
+    private void encodeLiteral(ByteBuf out, CharSequence name, CharSequence value, IndexType indexType,
                                int nameIndex) {
         boolean nameIndexValid = nameIndex != -1;
         switch (indexType) {
             case INCREMENTAL:
                 encodeInteger(out, 0x40, 6, nameIndexValid ? nameIndex : 0);
                 break;
             case NONE:
                 encodeInteger(out, 0x00, 4, nameIndexValid ? nameIndex : 0);
                 break;
             case NEVER:
                 encodeInteger(out, 0x10, 4, nameIndexValid ? nameIndex : 0);
                 break;
             default:
                 throw new Error(""should not reach here"");
         }
         if (!nameIndexValid) {
             encodeStringLiteral(out, name);
         }
         encodeStringLiteral(out, value);
     }",NotBuggy,"HTTP/2 move internal HPACK classes to the http2 package

Motivation:
The internal.hpack classes are no longer exposed in our public APIs and can be made package private in the http2 package.

Modifications:
- Make the hpack classes package private in the http2 package

Result:
Less APIs exposed as public.
",NotBuggy,"The commit message describes moving internal HPACK classes to another package to limit their accessibility, suggesting a change in package visibility to enhance encapsulation. The provided diff, however, contains only a minor refactor: the code switches from HpackUtil.IndexType to IndexType, likely within the same package. This change does not indicate a bug fix; it seems related only to internal refactoring consistent with the commit's goal to adjust package visibility. There is no evidence of bug-related changes like error handling or logical corrections. Thus, the change does not align with the bug fix criteria. The answer is **NotBuggy**."
netty,16567.json,57063b6db0193f3d53847f3754005a1cd63d463a,"@@ -1,3 +1,3 @@
     protected int _getIntLE(int index) {
-        return buffer.getIntLE(index);
+        return unwrap().getIntLE(index);
     }",NotBuggy,"Implement the ByteBuf methods that should have been overridden

Motivation:

Some ByteBuf implementations do not override all necessary methods,
which can lead to potentially sub-optimal behavior.

Also, SlicedByteBuf does not perform the range check correctly due to
missing overrides.

Modifications:

- Add missing overrides
- Use unwrap() instead of direct member access in derived buffers for
  consistency
- Merge unwrap0() into unwrap() using covariant return type
- Deprecate AbstractDerivedByteBuf and its subtypes, because they were
  not meant to be public

Result:

Correctness
",Buggy,"The commit message mentions the need to implement missing `ByteBuf` method overrides, which could lead to sub-optimal behavior or incorrect function (e.g., missing range checks). The git diff indicates that the `_getIntLE` method now uses `unwrap()` instead of direct member access (`buffer`). This aligns with the commit's intent to correct an incorrect operation due to missing method overrides.

Switching to `unwrap()` suggests ensuring consistent behavior across potentially derived or sliced buffers, likely addressing oversight or incorrect assumptions in how data is accessed or interpreted. This refactor contributes to accurate range checks for `SlicedByteBuf`.

The mentioned improvements—adding missing method overrides and ensuring consistent buffer operations—also mark a bug fix addressing error potential due to the unintended absence of overrides or incorrect range checks.

In summary, this change aligns closely with the commit message, indicating a fix in potential incorrect behavior or logical inconsistency. Therefore, the conclusion is **Buggy**."
netty,17888.json,b9996908b1fac66526395414cc5c3a14a1567d4e,"@@ -1,3 +1,3 @@
-        public byte getByte(int index) {
-            throw new IndexOutOfBoundsException();
-        }

+    public byte getByte(int index) {
+        throw new IndexOutOfBoundsException();
+    }",NotBuggy,"Implement reference counting

- Related: #1029
- Replace Freeable with ReferenceCounted
- Add AbstractReferenceCounted
- Add AbstractReferenceCountedByteBuf
- Add AbstractDerivedByteBuf
- Add EmptyByteBuf
",NotBuggy,"The commit message describes a major change: implementing reference counting involving several new classes and interfaces. The provided diff, however, only shows minor formatting changes (indenting) to the `getByte` method, with no substantive modifications. This does not align with the commit message and does not indicate a bug fix. Therefore, the changes reflect maintenance or code quality improvement, not correction of flawed logic.

Thus, the answer is **NotBuggy**."
argouml,2154.json,992cfe1a2840c838a401450e5232b03826cb956c,"@@ -1,15 +1,22 @@
-    public void actionPerformed(ActionEvent e) {
-        super.actionPerformed(e);
-        Object source = e.getSource();
-        if (source instanceof UMLComboBox2) {
-            Object selected = ((UMLComboBox2) source).getSelectedItem();
-            Object target = ((UMLComboBox2) source).getTarget();
-            if (Model.getFacade().isACallAction(target)
-                    && Model.getFacade().isAOperation(selected)) {
-                if (Model.getFacade().getOperation(target) != selected) {
-                    Model.getCommonBehaviorHelper()
-                    .setOperation(target, selected);
-                }
-            }
-        }
-    }

+       public void actionPerformed(ActionEvent e) {
+           super.actionPerformed(e);
+           Object source = e.getSource();
+           Object oldAssoc = null;
+           Object newAssoc = null;
+           Object link = null;
+           if (source instanceof UMLComboBox2) {
+               UMLComboBox2 box = (UMLComboBox2) source;
+               Object o = box.getTarget();
+               if (Model.getFacade().isALink(o)) {
+                   link = o;
+                   oldAssoc = Model.getFacade().getAssociation(o);
+               }
+               Object n = box.getSelectedItem();
+               if (Model.getFacade().isAAssociation(n)) {
+                   newAssoc = n;
+               }
+           }
+           if (newAssoc != oldAssoc && link != null && newAssoc != null) {
+               Model.getCoreHelper().setAssociation(link, newAssoc);
+           }
+       }",NotBuggy,"Change the model when the combo selection is changed

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17668 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message notes a change in model update logic when a combo selection changes. The diff reflects these changes, indicating a more complex handling for combo box selections and associations.

- The previous implementation focused solely on setting an operation on a target if it was a call action and the selected item was an operation.
- The new implementation introduces handling for link and association changes:
  - It stores the current association linked to a target.
  - It checks if the selected item is a valid association and updates the link to the new association if it differs from the current one.

The change does not exhibit clear signs of a bug fix—such as error condition handling or bugs in logic from previous code—nor does the commit message describe fixing a bug. Rather, it reflects a functional improvement or feature update in response to a selection event, allowing associations to be updated.

Given the details, this commit is more about enhancing model change handling functionality rather than rectifying a previous logic error or bug. Thus, I conclude **NotBuggy**."
argouml,2119.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,7 +1,7 @@
-    private static Object getType(Object target) {
-        Object type = Model.getFacade().getType(target);
-        if (Model.getFacade().isAClassifierInState(type)) {
-            type = Model.getFacade().getType(type);
-        }
-        return type;
-    }

+        private static Object getType(Object target) {
+            Object type = Model.getFacade().getType(target);
+            if (Model.getFacade().isAClassifierInState(type)) {
+                type = Model.getFacade().getType(type);
+            }
+            return type;
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states that Actions should be made inner classes of whatever uses them, but the changes in the diff are unrelated to making Actions inner classes. Instead, the only modification shown is an indentation change, which doesn't impact the program's structural behavior or address any bugs.

There is no indication of a bug fix, as the logic and the method signature remain unchanged. The transformation from static method or structure reform isn't visible in the provided diff. The commit's claim about refactoring doesn't align with the observable changes.

Thus, based on this provided diff and the commit message, the analysis concludes with **NotBuggy**."
argouml,5773.json,026fc657d606390b8457286c18c7dc5b9fe57803,"@@ -1,3 +1,4 @@
             public void undo() {
-                showStereotypes = !showem;
+                showVisibilities = !showem;
+                showVisibilitiesSet = oldValid;
             }",NotBuggy,"showStereotype setting is for the explorer (ie project-wide).  move it back to the ProjectSettings and undeprecate the related methods.


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@16435 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states the focus is on 'showStereotype' settings for the explorer, moving it back into ProjectSettings and undeprecating related methods, yet this diff changes the undo method's logic, altering `showStereotypes` to `showVisibilities`. This change seems to correct the behavior of the undo action for some visibility setting rather than fixing issues with stereotype settings. 

No clear link to a bug is evident without more context; this might relate more to a refactor or another feature adjustment given the commit message content, which does not mention any bugs directly related to the changed code. Based on this diff alone, where the variable change looks intentional but not clearly bug-related as per the commit message's context, it leaves us concluding **NotBuggy**."
argouml,2643.json,2a18b45c346f54b0340fd7f7cf7fe8644919d2f0,"@@ -1,47 +1,36 @@
-    public void setTarget(Object theNewTarget) {
+    protected void setTarget(Object theNewTarget) {
+        assert (getTarget() == null);
+        assert (!(getTarget() instanceof Fig));
+        
         theNewTarget = theNewTarget instanceof Fig
             ? ((Fig) theNewTarget).getOwner() : theNewTarget;
         if (Model.getFacade().isAModelElement(theNewTarget)
                 || theNewTarget instanceof Diagram) {
-            if (getTarget() != null) {
-                Enumeration enumeration = elements();
-                while (enumeration.hasMoreElements()) {
-                    Object base = enumeration.nextElement();
-                    Model.getPump().removeModelEventListener(
-                        this,
-                        base,
-                        ""ownedElement"");
-                }
-                Model.getPump().removeModelEventListener(
-                    this,
-                    getTarget(),
-                    ""base"");
-            }
             setListTarget(theNewTarget);
             if (getTarget() != null) {
                 Collection bases = Model.getFacade().getBases(getTarget());
                 Iterator it = bases.iterator();
                 while (it.hasNext()) {
                     Object base =  it.next();
                     Model.getPump().addModelEventListener(
                         this,
                         base,
                         ""ownedElement"");
                 }
                 // make sure we know it when a classifier is added as a base
                 Model.getPump().addModelEventListener(
                     this,
                     getTarget(),
                     ""base"");
             }
             if (getTarget() != null) {
                 removeAllElements();
                 setBuildingModel(true);
                 buildModelList();
                 setBuildingModel(false);
                 if (getSize() > 0) {
                     fireIntervalAdded(this, 0, getSize() - 1);
                 }
             }
         }
     }",NotBuggy,"Get rid of any target listeners

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17506 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The diff shows modifications to the `setTarget` method, changing its access from `public` to `protected`, which suggests a restriction in the visibility and accessibility of the method, likely aiming to encapsulate its utilization better within the class's hierarchy.

The commit message, ""Get rid of any target listeners,"" correlates to the removal of code that was handling event listener registration and deregistration. The removed section includes:
- Removal of listeners tied to model elements through `Model.getPump().removeModelEventListener`.
- Removal of the enumeration that iterated through elements and removed model event listeners.

By eliminating this listener management logic, the commit seems to be simplifying or refactoring the notification mechanism associated with the target. However, this does not necessarily reflect a bug fix but rather an intentional change to the class’s behavior regarding listener management.

The asserts added at the beginning of the method:
```java
assert (getTarget() == null);
assert (!(getTarget() instanceof Fig));
```
These assertions ensure that the current target is null and not an instance of `Fig`, reinforcing preconditions within the method. Nevertheless, these are more about code quality and robustness than fixing an active bug.

Therefore, this change is about altering the behavior around how events and targets interact, focusing on deleting event listeners rather than addressing a specific erroneous behavior or bug.

Considering these observations, the changes described in the diff do not directly rectify any flawed behavior but rather restructure functionality (presumably to streamline or prevent undesired interactions). Thus, despite the changes having potential long-term correctness impacts, they don't qualify as a direct bug fix.

**NotBuggy**"
argouml,13058.json,c594f19e891bb817975f314e7f9a190bf073e003,"@@ -1,12 +1,10 @@
     private boolean isDeleteEventRequired(
             final Object element) {
-        if (element instanceof Property) {
-            synchronized (deleteEventIgnoreList) {
-                if (deleteEventIgnoreList.contains(element)) {
-                    deleteEventIgnoreList.remove(element);
-                    return false;
-                }
+        synchronized (deleteEventIgnoreList) {
+            if (deleteEventIgnoreList.contains(element)) {
+                deleteEventIgnoreList.remove(element);
+                return false;
             }
         }
         return true;
     }",NotBuggy,"Make sure that changing namespace or region does not fire a delete event

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19406 a161b567-7d1e-0410-9ef9-912c70fedb3f
",Buggy,"The commit message indicates a change to prevent a specific undesired behavior: firing a delete event when changing a namespace or region. The code change modifies the `isDeleteEventRequired` method by generalizing its logic. Previously, the synchronized block was inside a conditional check specifically for `Property` instances. The new code removes that conditional, applying the check to all element types, not just `Property`. This aligns with the commit message by broadening the applicability of the delete event suppression mechanism.

This change suggests a correction in logic, ensuring that delete events are not inadvertently fired due to changes in namespaces or regions—which could previously have been triggered if the element wasn't of type `Property`. This aligns with fixing a bug related to event handling.

Thus, the change addresses a flaw in the existing functionality, indicating that it is indeed a bug fix. The correct conclusion is **Buggy**."
argouml,2284.json,5df7baaa147ae1e07eda64a63bf1a7bd187a5176,"@@ -1,14 +1,20 @@
     protected void addAll(Collection col) {
         Object selected = getSelectedItem();
         fireListEvents = false;
         int oldSize = objects.size();
         for (Object o : col) {
-            addElement(o);
+            if (filter != null && filter.length() > 0) {
+        	if (Model.getFacade().getName(o).toLowerCase().startsWith(filter.toLowerCase())) {
+                    addElement(o);
+        	}
+            } else {
+                addElement(o);
+            }
         }
         setSelectedItem(external2internal(selected));
         fireListEvents = true;
         if (objects.size() != oldSize) {
             fireIntervalAdded(this, oldSize == 0 ? 0 : oldSize - 1, 
                     objects.size() - 1);
         }
     }",NotBuggy,"Allow filtering of namespaces in combo

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18737 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states ""Allow filtering of namespaces in combo,"" and the diff shows the method `addAll` checks if a filter is applied. If a filter string exists, it uses it to filter the collection by matching the names, adding only items that start with the filter string. This introduces new functionality rather than fixing an existing bug. The change does not address error handling, logical corrections, or exception handling improvements, which are typical signs of a bug fix.

Thus, this change is aligned with the commit message in introducing the feature of filtering but does not indicate a bug fix. Therefore, the assessment of this change is **NotBuggy**."
argouml,1917.json,02860b3ad2e8258250f31a3ea52907e4e4df61b0,"@@ -1,3 +1,3 @@
-    public Fig getNameDisplay() {
-        return nameFig;
+    public DiagramElement getNameDisplay() {
+        return nameDisplay;
     }",NotBuggy,"Remove all knowledge of FigNodeModelElement

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19249 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states the removal of all knowledge of FigNodeModelElement. The diff shows a change in the return type from `Fig` to `DiagramElement` within the `getNameDisplay` method. This change seems to be part of an abstraction or refactoring process, replacing specific implementation details with more generic interfaces or types.

The modifications do not indicate a bug fix as there are no signs of error-handling updates, logical corrections, exception-handling improvements, or other bug-related changes in the provided diff. The focus appears to be on altering the design or architecture, likely in line with removing knowledge of a specific class (FigNodeModelElement) rather than addressing any bug.

Thus, the change in the code does not align with a bug fix. The answer is **NotBuggy**."
argouml,14398.json,c1eb0dea7c36300fe037cd721449f3f0e45449ce,"@@ -1,10 +1,16 @@
     public void selectionChanged(GraphSelectionEvent selectionEvent) {
         removeAll();
         if (selectionEvent.getSelections().size() == 1) {
             Fig selectedFig = (Fig)selectionEvent.getSelections().get(0);
             DefaultMutableTreeNode tn = new DefaultMutableTreeNode(getDescr(selectedFig));
             buildTree(selectedFig, tn);
-            JTree tree = new JTree(tn);
-            add(tree);
+            if (selectedFig instanceof FigClassifierRole) {
+                MessageNodeBuilder.addNodeTree(tn, (FigClassifierRole)selectedFig);
+            }
+            FigTree tree = new FigTree(tn);
+            tree.expandAll();
+            
+            JScrollPane scroller = new JScrollPane(tree);
+            add(scroller);
         }
     }",NotBuggy,"Specialize FigInspector for FigClassifierRole so I can see the contained MessageNodes


git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@10031 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message specifies changes to make FigInspector specialize for FigClassifierRole to display contained MessageNodes. The changes in the git diff reflect this intention by adding logic to handle FigClassifierRole specifically. It calls an additional method, `MessageNodeBuilder.addNodeTree`, when `selectedFig` is an instance of FigClassifierRole. This method is likely responsible for enhancing the tree to incorporate MessageNode structures associated with FigClassifierRoles. Furthermore, the modification replaces `JTree` with `FigTree`, presuming `FigTree` is possibly a specialized tree component. Also, wrapping the tree in a `JScrollPane` improves user interaction by adding scrolling capabilities.

While the diff appears to improve functionality regarding specific roles in the diagram by enhancing how information is organized and displayed, it does not indicate a bug fix. Instead, it enhances the feature set of FigInspector concerning FigClassifierRoles, aligning with the commit message.

Thus, the changes are an enhancement rather than a bug fix. The answer is **NotBuggy**."
argouml,3102.json,e5ccb0e741ae79fe74f24969ededccffcb04652e,"@@ -1,3 +1,3 @@
     public String getLabel() {
-        return name;
+        return propertyName;
     }",NotBuggy,"Refactor getName() -> getPropertyName()

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18704 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message indicates a refactoring, specifically renaming a method from `getName()` to `getPropertyName()`. However, the diff shows a change within the `getLabel()` method where the return value is modified from `name` to `propertyName`. This change seems related to adjusting the internal implementation to align with a refactoring where the representation or context of `name` has likely been updated to `propertyName`.

This change does not indicate a bug fix; it is purely a change in semantics or naming in the context or domain. There are no modifications related to error handling, exception handling, or logical corrections that would suggest a bug fix.

Thus, the transition in variable names is consistent with refactoring, aimed at improving the code's readability or clarity without affecting its behavior.

In conclusion, based on the given diff and commit message, I determine this change as **NotBuggy**."
argouml,1992.json,a09bb80652d16906a2c08e561094768d32d17431,"@@ -1,3 +1,10 @@
     public boolean disable() {
+
+        PropPanelFactoryManager.removePropPanelFactory(propPanelFactory);
+
+        // TODO: Remove the casting to DiagramFactoryInterface2
+        // as soon as DiagramFactoryInterface is removed.
+        DiagramFactory.getInstance().registerDiagramFactory(
+                DiagramType.Activity, (DiagramFactoryInterface2) null);
         return true;
     }",NotBuggy,"Move all files down (I seem to created a rogue directory in previous commit)

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18519 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message suggests a reorganization of files due to a directory issue, which does not concern functionality changes. However, the diff shows code modifications related to disabling a component by removing a factory registration and updating a DiagramFactory registration. These changes pertain to internal function behavior rather than a file organization issue described in the commit message.

The modifications, which are about unregistering components, might relate to an existing issue with enabling/disabling features in the application. Nonetheless, without explicit mention or related context indicating a bug fix in the commit message, it's hard to classify these changes as bug-specific purely based on the diff provided. Since the commit message doesn't match addressing code behavior bugs and instead implies organizational changes, the changes are unrelated to a bug fix as described.

Thus, the conclusion is **NotBuggy**."
argouml,2047.json,9d400c77ecb6e128d8d0867107f60f1ea00b81d3,"@@ -1,3 +1,3 @@
-    protected String getDialogTitle() {
-        return Translator.localize(""dialog.title.add-contents"");
-    }

+        protected String getDialogTitle() {
+            return Translator.localize(""dialog.title.add-baseclasses"");
+        }",NotBuggy,"Make Actions inner classes of whatever uses them

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@17684 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message mentions restructuring actions by making them inner classes, which doesn't relate to the actual code changes shown in the diff. The code change alters the dialog title text, but it doesn't relate to restructuring logic or fixing a bug.

The change in the diff:
- Modifies the return value of a method, `getDialogTitle`, which returns a localized string from ""dialog.title.add-contents"" to ""dialog.title.add-baseclasses"".

This seems more like a change for either clarification or an interface adjustment rather than a bug fix. It doesn't indicate any rectification of logic or error correction in the code. The commit does not reflect any changes related to making actions inner classes or any bug-related fix.

Thus, there is no indication that this change addresses a bug based on the available information from the diff and the mismatch with the commit message.

**NotBuggy**"
argouml,11027.json,c82d790257c6dbfd74c4ead021354b2d7a75c6be,"@@ -1,3 +1,2 @@
     public void popupMenuCanceled(PopupMenuEvent e) {
-        LOG.info(""popupMenuCanceled"");
     }",NotBuggy,"Remove logging commited accidentally

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@15539 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states that logging was removed accidentally. The diff shows a single line of logging being removed from the `popupMenuCanceled` method. This change aligns with the commit message, which targets the removal of log statements rather than addressing a bug. The modification does not indicate a bug fix, as no error, exception, or logical issue within the functionality is being corrected here. Logging, especially informational, is typically not part of core functionality but rather for tracking execution during development or debugging sessions.

Consequently, the commit does not relate to a bug fix. Thus, the conclusion is **NotBuggy**."
argouml,14041.json,280cf1fa18a226a8dbf707f91b08ae68097be66c,"@@ -1,4 +1,3 @@
     public Object getCollaborationInstanceSet() {
-        // TODO: Need UML 2.x equivalent
-        return null /*CollaborationInstanceSet.class*/;
+        throw new NotYetImplementedException();
     }",NotBuggy,"Define more model element types that are used by activity diagrams

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18525 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The change in this commit replaces a placeholder implementation that returned `null` with an exception being thrown, specifically `NotYetImplementedException`. This adjustment suggests that the intent of the method has not yet been fulfilled and now clearly communicates this by throwing an exception instead of silently returning `null`.

The commit message does not indicate a bug fix; instead, it speaks about defining model element types for activity diagrams. The code change, although improving the communication of the method's current state in the codebase, does not necessarily fix a bug in functionality. It's more about setting expectations for future development.

**NotBuggy**"
argouml,2590.json,e7865aa82550c72061b5ffc4c566f5b716ae8299,"@@ -1,25 +1,40 @@
     public void buildPanel() {
 	// ///////////////////////////////////////
 	// Build the field
 	// ///////////////////////////////////////
 
-	checkbox = new JCheckBox();
-	// TODO ? find a Tool tips, add a label
-	// checkbox.setToolTipText(Translator.localize(""label.body.tooltip""));
-	checkbox.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	trueButton = new JRadioButton(Translator.localize(""misc.boolean.true""));
+	falseButton = new JRadioButton(Translator
+		.localize(""misc.boolean.false""));
+
+	trueFalseGroup = new ButtonGroup();
+	trueFalseGroup.add(trueButton);
+	trueFalseGroup.add(falseButton);
+
+	trueButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
+	falseButton.setFont(LookAndFeelMgr.getInstance().getStandardFont());
 
 	/**
 	 * on change : Change the value in the model
 	 */
-	checkbox.addActionListener(new ActionListener() {
+	trueButton.addActionListener(new ActionListener() {
+	    public void actionPerformed(ActionEvent e) {
+		updateModel();
+	    }
+	});
+
+	falseButton.addActionListener(new ActionListener() {
 	    public void actionPerformed(ActionEvent e) {
 		updateModel();
 	    }
 	});
 
 	// ///////////////////////////////////////
 	// Add field(s) to panel
 	// ///////////////////////////////////////
-	this.allField = new JScrollPane(checkbox);
+	JPanel panel = new JPanel();
+	panel.add(trueButton);
+	panel.add(falseButton);
+	this.allField = new JScrollPane(panel);
 
     }",NotBuggy,"Allow different types of initial value to be specified

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@19429 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message ""Allow different types of initial value to be specified"" suggests an enhancement rather than a bug fix. The diff reveals a switch from a JCheckBox to JRadioButtons grouped in a ButtonGroup. This change supports more than two states or a more explicit Boolean selection, fulfilling the commit message's intention to handle different initial values.

The JCheckBox inherently represents a single Boolean with only two states (checked or unchecked). In contrast, two JRadioButtons grouped allow explicit choice between true and false, which helps in cases where distinguishing between 'no selection' or null and true/false is crucial. However, this isn't fixing incorrect behavior; it expands functionality to manage initial values more flexibly.

Looking at the broader picture, the change is not focused on correcting erroneous code but enhancing user interface capabilities and associated functionalities. There's no direct indication that this resolves a bug; it rather improves the design to accommodate more nuanced scenarios.

In conclusion, while this enhances flexibility, it aligns with a feature enhancement rather than directly addressing a bug, so this would be deemed **NotBuggy**."
argouml,10209.json,604299fb46334e559a0cbc4aaf66e7471c0e693f,"@@ -1,12 +1,10 @@
     private void initFigs() {
         setEditable(false);
-        setBigPort(new FigRect(X, Y, STATE_WIDTH, HEIGHT, DEBUG_COLOR,
-                DEBUG_COLOR));
         head = new FigRect(X, Y, STATE_WIDTH, HEIGHT, LINE_COLOR,
                 SOLID_FILL_COLOR);
         // add Figs to the FigNode in back-to-front order
         addFig(getBigPort());
         addFig(head);
 
         setBlinkPorts(false); //make port invisible unless mouse enters
     }",NotBuggy,"Make code consistent. 
Remove the use of deprecated method setBigPort().

git-svn-id: http://argouml.tigris.org/svn/argouml/trunk@18728 a161b567-7d1e-0410-9ef9-912c70fedb3f
",NotBuggy,"The commit message states it removes the use of a deprecated method `setBigPort()`. The diff indeed shows `setBigPort()` being removed:

```java
-        setBigPort(new FigRect(X, Y, STATE_WIDTH, HEIGHT, DEBUG_COLOR,
-                DEBUG_COLOR));
```

The removal of the deprecated method aims to clean up the code for consistency, aligning with the commit message. There are no indications of a bug being fixed; rather, the change is maintaining code standards by addressing deprecations.

**Concluding: NotBuggy**"
docx4j,1091.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,26 +1,16 @@
     public static void main(String args[])
         throws IOException
     {
         if (args.length != 2)
         {
             System.err.println(
                 ""two arguments required: input filename and output filename"");
             System.exit(1);
         }
         FileInputStream  istream = new FileInputStream(args[ 0 ]);
-        try {
-            FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
-            try {
-                NPOIFSFileSystem fs = new NPOIFSFileSystem(istream);
-                try {
-                    fs.writeFilesystem(ostream);
-                } finally {
-                    fs.close();
-                }
-            } finally {
-                ostream.close();
-            }
-        } finally {
-            istream.close();
-        }
+        FileOutputStream ostream = new FileOutputStream(args[ 1 ]);
+
+        new OPOIFSFileSystem(istream).writeFilesystem(ostream);
+        istream.close();
+        ostream.close();
     }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",NotBuggy,"The code changes simplify the file system interaction by removing nested try-finally blocks and using a new class, OPOIFSFileSystem, replacing NPOIFSFileSystem. However, the commit message mentions password encryption changes, which aren't apparent here; the diff primarily involves code cleanup and API changes. Without visible encryption alterations, this doesn't align with the commit message stating functionality changes related to encryption. Thus, it points more to refactoring than a bug fix. The answer is **NotBuggy**."
docx4j,6115.json,6fc95a3b720163091946b0c51ad6200c6d780fb4,"@@ -1,7 +1,7 @@
 	public boolean shouldTraverse(Object o) {
 		
-		System.out.println(o.getClass().getName() );
+//		System.out.println(o.getClass().getName() );
 		
 		return !(o instanceof P
 				|| o instanceof org.docx4j.wml.Tbl); 
 	}",NotBuggy,"trivial
",NotBuggy,"The commit message states ""trivial,"" suggesting minor or non-functional changes. The diff comments out a `System.out.println()` line, typically used for debugging or logging. Removing or commenting out such a line doesn't alter the logic or behavior of the method `shouldTraverse`. This change is generally for code cleanliness or to prevent unnecessary console output, not for fixing a bug. Thus, the commit does not indicate a bug fix.

The final assessment is **NotBuggy**."
docx4j,25366.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<XMLGregorianCalendar> createDate(XMLGregorianCalendar value) {
+        return new JAXBElement<XMLGregorianCalendar>(_Date_QNAME, XMLGregorianCalendar.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a structural change towards a Maven multi-module project and is labeled as ""Work in progress."" The code change provided, however, shows a modification in the method's return type and parameters, from `JAXBElement<P.Dir>` to `JAXBElement<XMLGregorianCalendar>`. This change likely reflects an API modification or refactoring rather than a bug fix.

- The change may involve adapting the code to a new or altered schema, but it doesn't directly address any logical issue, error correction, or bug within the method itself.
- The commit message makes no mention of any bug-related changes.
- The change does not illustrate common signs of addressing a bug, such as error or exception handling improvements.

Given the information, it seems the change aligns more with project restructuring or ongoing development changes rather than directly fixing a particular bug. Therefore, the most logical conclusion is **NotBuggy**."
docx4j,20658.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public CTWrapSquare createCTWrapSquare() {
-        return new CTWrapSquare();
+    public CTPageSetup createCTPageSetup() {
+        return new CTPageSetup();
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",NotBuggy,"The commit message describes an update to complete DML generated from a specific schema. The change in the code modifies a method to create a different type of object (`CTPageSetup` instead of `CTWrapSquare`). This aligns with updating or expanding the code's capabilities to integrate more complete or correct implementations as per the schema (`dml__ROOT.xsd`).

There is no indication of a bug fix in this change, as it seems more related to expanding or adjusting functionality to match new specifications or schema updates.

Given no evidence of a bug being fixed, the conclusion is **NotBuggy**."
docx4j,14816.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,28 +1,26 @@
   public static String toShortString(DiffXEvent e) {
     // an element to open
-    if (e instanceof OpenElementEvent) {
+    if (e instanceof OpenElementEvent)
       return '<'+((OpenElementEvent)e).getName()+'>';
     // an element to close
-    } else if (e instanceof CloseElementEvent) {
+    else if (e instanceof CloseElementEvent)
       return ""</""+((CloseElementEvent)e).getName()+'>';
     // an attribute
-    } else if (e instanceof AttributeEvent) {
+    else if (e instanceof AttributeEvent)
       return ""@""+((AttributeEvent)e).getName();
     // a word
-    } else if (e instanceof WordEvent) {
+    else if (e instanceof WordEvent)
       return '""'+((CharactersEventBase)e).getCharacters()+'""';
     // a white space event
-    } else if (e instanceof SpaceEvent) {
+    else if (e instanceof SpaceEvent)
       return ""_s_"";
     // a single character
-    } else if (e instanceof CharEvent) {
+    else if (e instanceof CharEvent)
       return '\''+((CharactersEventBase)e).getCharacters()+'\'';
     // an ignorable space event
-    } else if (e instanceof IgnorableSpaceEvent) {
+    else if (e instanceof IgnorableSpaceEvent)
       return ""_i_"";
     // a single line
-    } else if (e instanceof LineEvent) {
-      return ""L#""+((LineEvent)e).getLineNumber();
-    }
+    else if (e instanceof LineEvent) return ""L#""+((LineEvent)e).getLineNumber();
     return ""???"";
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,"The git diff provided simplifies a method by modifying the formatting of the if-else statements. 

1. **Change Details**: 
    - The `if-else` statements are reformatted to remove brackets `{}` for single-line blocks. 
    - There are no changes in logic or functionality in how the events are handled.
    
2. **Commit Message Analysis**:
    - The commit message ""update to diffx-0.7.4"" suggests a version update. This usually involves changing code for compatibility or new features introduced in the updated library, not necessarily related to bug fixes.

3. **Relevance to Bug Fixes**:
    - These changes are purely stylistic to simplify code readability and have no logical impact on the code execution.
    - There aren’t any indications of fixing error-handling, logic issues, or performance problems in the diff itself.

Given the nature of the changes and the corresponding commit message, there are no indications that this update targets or fixes any specific bugs. 

**Conclusion: NotBuggy**"
docx4j,18855.json,beb51743e46de6c74bf4a56faa3fa1a4322fea8a,"@@ -1,3 +1,3 @@
-    public Theme.ExtLst createThemeExtLst() {
-        return new Theme.ExtLst();
+    public CTTextBody createCTTextBody() {
+        return new CTTextBody();
     }",NotBuggy,"Map the non-fonts bit to xsd:any, doesn't marshall namespaces properly with JAXB in Java 6, so model the non-font bits properly.
",Buggy,"The commit message discusses resolving an issue related to JAXB's marshalling of namespaces, particularly in the context of mapping non-font bits to xsd:any. The code change replaces the method `createThemeExtLst` which returned a `Theme.ExtLst` object with `createCTTextBody` which now returns a `CTTextBody` object.

Let's analyze this change in the context of the commit message and determine if it indicates a bug fix:

1. **Class and Method Change**: The change is replacing the creation of one type of object (`Theme.ExtLst`) with another (`CTTextBody`). This alteration suggests that the previously returned type was not correctly modeling the data structure required for whatever process the application undertook, potentially causing issues with serialization using JAXB.

2. **Context of JAXB**: JAXB is used for converting Java objects to XML and vice versa. If the original Java object (i.e., `Theme.ExtLst`) was not properly set up or if there was an incorrect mapping, it could lead to serialization/deserialization errors, especially concerning XML namespace handling as mentioned in the commit message.

3. **Modeling Correction**: The commit message states, ""model the non-font bits properly,"" which aligns with the code's shift from `Theme.ExtLst` to `CTTextBody`. This implies a correction in the data model to address a mismatch or improper representation, which was likely causing issues during JAXB operations.

4. **Fix Implication**: The commit seems intended to correct the Java representation of a certain XML structure possibly due to an incompatible or incorrect mapping that could have led to erroneous behavior or marshaling issues, particularly with namespaces in Java 6.

Given the alignment between the intention in the commit message and the changes in the code (i.e., correcting the data structure used in JAXB operations likely to fix improper namespace handling), it seems reasonable to conclude that this change is indeed fixing a bug.

Therefore, this commit can be concluded as **Buggy**."
docx4j,25367.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<Drawing> createRDrawing(Drawing value) {
-        return new JAXBElement<Drawing>(_RDrawing_QNAME, Drawing.class, R.class, value);
+    public JAXBElement<Variant> createVariant(Variant value) {
+        return new JAXBElement<Variant>(_Variant_QNAME, Variant.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a transition to a Maven multi-module project, suggesting structural changes. The diff shows a method's functionality being altered: it now creates a `JAXBElement` for `Variant` instead of `Drawing`, with accompanying parameter and class type changes. This does not indicate a bug fix, but rather a modification aligned with project reorganization. There's no information suggesting error-handling, logical corrections, or similar bug-oriented changes.

Thus, the modifications are related to refactoring within the project's structure rather than addressing a bug.

The conclusion is **NotBuggy**."
docx4j,6963.json,c9021fdc912cccadd839aea24b132ca922147258,"@@ -1,40 +1,40 @@
-	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.foray.font.format.Panose soughtPanose) {
+	private PhysicalFont getAssociatedPhysicalFont(String documentFontName, String orignalKey, org.docx4j.fonts.foray.font.format.Panose soughtPanose) {
 
 		log.debug(""Looking for "" + soughtPanose);
 		
 		String resultingPanoseKey;
 		
 //		// First try panose space restricted to this font family
 //		2009 03 22 - we don't have physicalFontFamiliesMap any more		
 //		if (orignalKey!=null) {
 //			PhysicalFontFamily thisFamily = 
 //				physicalFontFamiliesMap.get( PhysicalFonts.getPhysicalFonts().get(orignalKey).getName() );					
 //			
 //			log.debug(""Searching within family:"" + thisFamily.getFamilyName() );
 //			
 //			resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, 
 //					thisFamily.getPhysicalFonts(), MATCH_THRESHOLD_INTRA_FAMILY);    
 //			if ( resultingPanoseKey!=null ) {
 //				log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
 //	        	fm.setPhysicalFont( PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey) );													
 //				return fm;
 //			}  else {
 //				log.warn(""No match in immediate font family"");
 //			}
 //		} else {
 //			log.debug(""originalKey was null."");
 //		}
 		
 		// Well, that failed, so search the whole space
 		
 		//fm.setDocumentFont(documentFontName); ???
 		resultingPanoseKey = findClosestPanoseMatch(documentFontName, soughtPanose, PhysicalFonts.getPhysicalFonts(),
 				MATCH_THRESHOLD); 
 		if ( resultingPanoseKey!=null ) {
 			log.info(""--> "" + PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey).getEmbeddedFile() );
         	return PhysicalFonts.getPhysicalFonts().get(resultingPanoseKey);													
 		}  else {
 			log.warn(""No match in panose space"");
 			return null;
 		}
 	}",NotBuggy,"Include source code for fop-fonts, as org.docx4j.fonts.fop.
Use FOP 1.0.
Move panose to org.docx4j.fonts.foray
Add LICENSE, NOTICE
",NotBuggy,"The commit message describes organizational and structural changes related to font packages, specifically moving `panose` from one package to another. The code diff reflects this change by updating the import path for the `Panose` class from `org.foray.font.format.Panose` to `org.docx4j.fonts.foray.font.format.Panose`. This change aligns with restructuring, indicated in the commit message. The move doesn't suggest any error handling correction, logical fix, or performance-related modification directly related to any bugs.

The message also mentions other unrelated changes (e.g., including source code, adding LICENSE, NOTICE), which aren't addressed in this specific code diff. Therefore, the commit does not involve error handling updates, logical corrections, exception handling improvements, or any evident bug-related changes.

Thus, the assessment is **NotBuggy**."
docx4j,25396.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public JAXBElement<P.Dir> createPDir(P.Dir value) {
-        return new JAXBElement<P.Dir>(_PDir_QNAME, P.Dir.class, P.class, value);
+    public JAXBElement<BigInteger> createUi8(BigInteger value) {
+        return new JAXBElement<BigInteger>(_Ui8_QNAME, BigInteger.class, null, value);
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message mentions restructuring to a Maven multi-module project and labels the work as ""in progress."" There's no indication of a bug being fixed. The code change modifies the return type and parameters of a method: from `JAXBElement<P.Dir>` to `JAXBElement<BigInteger>`. This kind of change could be part of the ongoing project restructuring.

Given that the changes align with the task of reorganization rather than correcting errors, it is likely part of the development process rather than a bug fix. Hence, this change is classified as **NotBuggy**."
docx4j,14694.json,5eb0d359e63a65813137778bde2398fee5fa64af,"@@ -1,6 +1,7 @@
   public void delete(DiffXEvent e) {
-    if (e instanceof OpenElementEvent)
+    if (e instanceof OpenElementEvent) {
       push((OpenElementEvent)e, '-');
-    else if (e instanceof CloseElementEvent)
+    } else if (e instanceof CloseElementEvent) {
       pop();
+    }
   }",NotBuggy,"update to diffx-0.7.4
",NotBuggy,"This commit updates the code to include braces in the `if-else` statements for better readability and maintainability. The changes don't indicate any functional alterations or bug fixes; they purely enhance code structure and readability, likely to align with a style guide or preparation for a library update.

The connection between the commit message and the code modifications is weak in terms of a bug fix, as the commit message merely notes a version update. There's no evidence of a bug being addressed.

Therefore, this change does not indicate a bug fix. The appropriate conclusion is **NotBuggy**."
docx4j,27318.json,fcdd0ece2e6c040ce5b870108f0e83bef14bce58,"@@ -1,7 +1,3 @@
     public String getDashstyle() {
-        if (dashstyle == null) {
-            return ""solid"";
-        } else {
-            return dashstyle;
-        }
+        return dashstyle;
     }",NotBuggy,"VML generated classes, based on ECMA 376 1ed (rather than earlier draft).  
Includes @XmlType namespace annotations from revs 940 and 1129.
",NotBuggy,"The commit message indicates updates related to VML generated classes and annotations. However, the code diff only simplifies the getDashstyle method by removing the null check and default value return. This change does not fix a bug; it changes behavior. The original code treats null as ""solid,"" but now it directly returns dashstyle, possibly leading to null returns if dashstyle is unset. The commit message does not mention fixing a bug related to this change. Therefore, the answer is **NotBuggy**."
docx4j,22538.json,6e4c47a7dae3ec85edb61ade5dd7b49030cc641d,"@@ -1,3 +1,3 @@
-    public void setExt(CTOfficeArtExtension value) {
-        this.ext = value;
+    public void setExtLst(CTOfficeArtExtensionList value) {
+        this.extLst = value;
     }",NotBuggy,"More complete DML, generated from TC45 1.0 final, using dml__ROOT.xsd
",NotBuggy,"The commit message refers to updating DML, possibly metadata or schema changes, while the code modifies method and variable names. The changes do not indicate a bug fix nor imply rectifying incorrect functionality. Instead, they align with structural updates possibly related to schema adjustments. Thus, the answer is **NotBuggy**."
docx4j,8715.json,a440fc994771f87a66c1186134e990c646661e13,"@@ -1,3 +1,3 @@
-    public R createR() {
-        return new R();
+    public CTDefinedName createCTDefinedName() {
+        return new CTDefinedName();
     }",NotBuggy,"Move to maven multi-module project (Work in progress)
",NotBuggy,"The commit message indicates a structural change aimed at transitioning to a Maven multi-module project. It does not indicate a bug fix. The diff shows a change in method naming and return type from `R` to `CTDefinedName`, suggesting a refactor aligning with new or updated project structure, rather than fixing a bug. The alignment with modularization is in progress, not bug-related. The conclusion is **NotBuggy**."
docx4j,31212.json,ed8d15d352d0a2eaeac306e4ba9a3c83a3547558,"@@ -1,3 +1,3 @@
-        public BooleanDefaultTrue getSemiHidden() {
-            return semiHidden;
-        }

+    public BooleanDefaultTrue getSemiHidden() {
+        return semiHidden;
+    }",NotBuggy,"Change CT_FldChar to FldChar;
Style is freestanding, rather than an inner class of Styles.
(Other changes largely whitespace in license header)
",NotBuggy,"The given commit message discusses renaming a class and mentions changes related to the placement of styles. However, the provided diff reveals only changes in indentation, which implies altering the code's formatting without any logical or behavioral changes to the software.

Given that the diff does not reflect anything about renaming CT_FldChar to FldChar, any changes related to styles, or bug fixes, there is no correlation between the commit message and the diff.

The changes in the diff are purely stylistic and do not constitute a bug fix. Therefore, the code does not introduce or indicate any bug fix.

Thus, the conclusion is **NotBuggy**."
docx4j,1661.json,e29924b33ec0c0298ba4fc3f7a8c218c8e6cfa0c,"@@ -1,22 +1,48 @@
-   public static void main(final String args[]) throws IOException {
-      if (args.length == 0) {
-         System.err.println(""Must specify at least one file to view"");
-         System.exit(1);
-      }
-
-      boolean withSizes = false;
-      boolean newPOIFS = true;
-      for (int j = 0; j < args.length; j++) {
-         if (args[j].equalsIgnoreCase(""-size"") || args[j].equalsIgnoreCase(""-sizes"")) {
-            withSizes = true;
-         } else if (args[j].equalsIgnoreCase(""-old"") || args[j].equalsIgnoreCase(""-old-poifs"")) {
-            newPOIFS = false;
-         } else {
-            if(newPOIFS) {
-               viewFile(args[j], withSizes);
-            } else {
-               viewFileOld(args[j], withSizes);
+    public static void main(String[] args) throws Exception {
+        if (args.length == 0) {
+            System.err.println(""Must specify at least one file to dump"");
+            System.exit(1);
+        }
+        
+        boolean dumpProps = false, dumpMini = false;
+        for (int i = 0; i < args.length; i++) {
+            if (args[i].equalsIgnoreCase(""-dumprops"") ||
+                args[i].equalsIgnoreCase(""-dump-props"") ||
+                args[i].equalsIgnoreCase(""-dump-properties"")) {
+                dumpProps = true;
+                continue;
             }
-         }
-      }
+            if (args[i].equalsIgnoreCase(""-dumpmini"") ||
+                args[i].equalsIgnoreCase(""-dump-mini"") ||
+                args[i].equalsIgnoreCase(""-dump-ministream"") ||
+                args[i].equalsIgnoreCase(""-dump-mini-stream"")) {
+                dumpMini = true;
+                continue;
+            }
+            
+            System.out.println(""Dumping "" + args[i]);
+            FileInputStream is = new FileInputStream(args[i]);
+            NPOIFSFileSystem fs = new NPOIFSFileSystem(is);
+            is.close();
+
+            DirectoryEntry root = fs.getRoot();
+            File file = new File(root.getName());
+            file.mkdir();
+
+            dump(root, file);
+            
+            if (dumpProps) {
+                HeaderBlock header = getHeaderBlock(fs);
+                dump(fs, header.getPropertyStart(), ""properties"", file);
+            }
+            if (dumpMini) {
+                NPropertyTable props = getPropertyTable(fs);
+                int startBlock = props.getRoot().getStartBlock(); 
+                if (startBlock == POIFSConstants.END_OF_CHAIN) {
+                    System.err.println(""No Mini Stream in file"");
+                } else {
+                    dump(fs, startBlock, ""mini-stream"", file);
+                }
+            }
+        }
    }",NotBuggy,"repackaged poifs; password encryption (load/save) now works
",NotBuggy,"The commit message suggests a repackaging of POIFS and fixing password encryption functionalities. The code modifications indicate a significant restructuring, including the removal of options for old POIFS processing and adding new functionalities for dumping properties and mini-stream. However, the primary focus of the commit message is on password encryption, and there are no direct changes in the diff about encryption, suggesting the changes are more about program functionality than specifically addressing a bug. Therefore, based on the diff and the focus of the commit message, this commit does not clearly indicate a bug fix. Concluding with **NotBuggy**."
atmosphere,414.json,843374c2f23f04dfeca68025d80a9fc5c89846b5,"@@ -1,15 +1,15 @@
     public final static AtmosphereRequest wrap(HttpServletRequest request) {
         // Do not rewrap.
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
-            return AtmosphereRequestImpl.class.cast(request);
+            return (AtmosphereRequestImpl) request;
         }
 
         Builder b = new Builder();
         Enumeration<String> e = request.getAttributeNames();
         String s;
         while (e.hasMoreElements()) {
             s = e.nextElement();
             b.localAttributes.put(s, attributeWithoutException(request, s));
         }
         return b.request(request).build();
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message states that the code is being updated to JDK8. The only change in the diff is the replacement of `AtmosphereRequestImpl.class.cast(request)` with `(AtmosphereRequestImpl) request`. This is a stylistic change and does not introduce any new Java 8 features or fix bugs. The original method and the casting approach achieve the same result, but casting is more traditional in Java.

There are no signs of error handling updates, logical corrections, or bug-related changes in this diff. Therefore, this change aligns with a general code update rather than a bug fix.

The reasoning can be summarized as follows:
- The commit message indicates a general update to the JDK version, not specifically a bug fix.
- The code change involves a refactoring or simplification, not an error correction or logic change.
- The functionality remains equivalent, indicating the absence of a bug fix.

Thus, the assessment for this diff is **NotBuggy**."
atmosphere,1507.json,add336c1cc60c8189b00d21dd1a7b3408bc4754d,"@@ -1,6 +1,6 @@
     public void reconfigureInitParams(boolean reconfigureInitParams) {
         if (reconfigureInitParams) {
-            doInitParams(servletConfig);
+            doInitParams(servletConfig, reconfigureInitParams);
             doInitParamsForWebSocket(servletConfig);
         }
     }",NotBuggy,"Do not allow reloading the AsyncSupport from init-param
",NotBuggy,"The commit message suggests that there is a change to prevent the reloading of AsyncSupport via init parameters. The diff shows that the `doInitParams` method has an added parameter `reconfigureInitParams`. This change implies that the method may now take into account this boolean flag to possibly alter its behavior, potentially aligning with the idea of preventing reloading of AsyncSupport.

However, without further context or changes to `doInitParams` itself, we can't conclusively determine if this change directly prevents reloading AsyncSupport. The commit message does not directly align with just passing an additional parameter unless we know that this parameter controls reloading functionality somehow.

Given the information, it's difficult to directly associate this with a bug fix without seeing how `doInitParams` uses this parameter. The change appears more like a prelude or a necessary refactor for a functionality change, rather than fixing an outright bug.

Therefore, I would conclude this as **NotBuggy** based on the provided information alone. Further details on how this impacts `doInitParams` could change this assessment."
atmosphere,1267.json,5241afdf818b484412c44d27d97a2c7e4dacbb71,"@@ -1,11 +1,11 @@
     private static void fallbackToManualAnnotatedClasses(Class<?> mainClass, AtmosphereFramework f, AnnotationHandler handler) {
         logger.warn(""Unable to detect annotations. Application may fail to deploy."");
         f.annotationScanned(true);
-        for (Class a : coreAnnotations) {
+        for (Class<?> a : coreAnnotations) {
             try {
                 handler.handleProcessor(loadClass(mainClass, a.getName()));
             } catch (Exception e) {
                 logger.trace("""", e);
             }
         }
     }",NotBuggy,"Refactor
",NotBuggy,"The git diff shows a minor change where the code is updated to use a parameterized type `Class<?>` instead of the raw type `Class` in a for-loop iterating over `coreAnnotations`. This change is aligned with type-safety practices introduced in generics to prevent potential `ClassCastException` and clarify the code.

Examining the commit message, it simply states ""Refactor."" The code change does indeed reflect a refactoring: improving the code's clarity and maintaining its existing behavior without altering its functionality.

There is no indication of fixing a bug in the code. The purpose of the change is purely to enhance the type safety and readability of the code. This does not address a logical error, exception handling, or error-handling improvement indicative of a bug fix.

Therefore, considering the nature of the change and the provided commit message, the analysis concludes with **NotBuggy**."
atmosphere,2813.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,9 +1,9 @@
         public void onMessage(WebSocket socket, String text) {
             super.onMessage(socket, text);
             LOGGER.trace(""onMessage(String) {} "", socket);
-            DefaultWebSocket g2w = DefaultWebSocket.class.cast(socket);
+            DefaultWebSocket g2w = (DefaultWebSocket) socket;
             org.atmosphere.websocket.WebSocket webSocket = (org.atmosphere.websocket.WebSocket) g2w.getUpgradeRequest().getAttribute(""grizzly.webSocket"");
             if (webSocket != null) {
                 webSocketProcessor.invokeWebSocketProtocol(webSocket, text);
             }
         }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message indicates an update to JDK 8, suggesting changes related to Java language features or compatibility improvements. The diff alters a line of code that previously used `Class.cast()`, which was a common pattern for casting in some Java versions, to a more straightforward type cast `(DefaultWebSocket) socket`.

- **Relevance to commit message**: The change aligns with updating code since explicit casting using `(Type)` is more idiomatic in newer Java versions compared to `Class.cast()`.
- **Bug fix indicators**: The change seems more focused on code style or modernization rather than addressing a bug. There's no indication of fixing logic, error handling, or exceptions here.

Since the focus is on updating the code style for JDK 8 compatibility or conventions, and there's no sign of a correction to a bug or issue, the answer is **NotBuggy**."
atmosphere,588.json,b5b88b095c89dbb9914fd9ec8e0d999e2007fc55,"@@ -1,3 +1,7 @@
     public static AtmosphereResourceFactory resourceFactory() {
+        if (resourceFactoryDuplicate) {
+            throw new IllegalStateException(
+                    ""More than one instance has been stored. Universe cannot be used."");
+        }
         return resourceFactory;
     }",NotBuggy,"Throw exception if Universe methods are used when they are not reliable

The previous version causes a lot of logging about Universe not being reliable
even though you never use Universe. This changes the behavior so that you will
instead get an exception only when actually using the methods, when they will not
work properly.
",Buggy,"The commit message indicates that an exception should be thrown when Universe methods are unreliable. The code change adds logic to throw an `IllegalStateException` if `resourceFactoryDuplicate` is true, which suggests there are multiple instances, making the method usage unreliable. This aligns with the commit message context that emphasizes restricting method use when they're unreliable, thereby preventing unnecessary logging by making the issue explicit only when problematic code is executed.

Therefore, this change is a bug fix that ensures the program does not proceed into unreliable or erroneous states without notifying the user. This aligns with improved exception handling, specifically targeting behavior stabilization.

Thus, the conclusion is **Buggy**."
atmosphere,958.json,e1a548d5c483168f07249763b2b5e1971bfeaf1e,"@@ -1,5 +1,5 @@
-    protected void endRequest(AtmosphereResourceImpl r, boolean cancel) {
+    public void endRequest(AtmosphereResourceImpl r, boolean cancel) {
         if (completeLifecycle(r, cancel)) {
             config.framework().notify(Action.TYPE.CANCELLED, r.getRequest(false), r.getResponse(false));
         }
     }",NotBuggy,"Improve Logging, make method public
",NotBuggy,"The provided diff changes a method's accessibility from `protected` to `public`, allowing broader access. This modification aligns with the commit message stating ""make method public."" However, the commit message primarily emphasizes logging improvements, which aren't depicted in the diff. The change does not indicate an error-handling update, logical correction, or any modification related to a bug fix. Therefore, the change does not align with bug-fixing activity.

In conclusion, given the nature of the diff and the descriptions in the commit message, this change does not signify a bug fix.

**NotBuggy**"
atmosphere,2102.json,6603c01d44e431a43c5ae468feb76b09243d72d5,"@@ -1,35 +1,36 @@
     protected void retryInjection(AtmosphereFramework framework){
+        int maxTryPerCycle = maxTry;
         // Give another chance to injection in case we failed at first place. We may still fail if there is a strong
         // dependency between Injectable, e.g one depend on other, or if the Injectable is not defined at the right place
         // in META-INF/services/org/atmosphere/inject.Injectable
         Set<Field> fields = new HashSet<Field>();
         Object instance = null;
         final LinkedHashSet<Object> postponedMethodExecution = new LinkedHashSet<>(pushBackInjection);
-        while (!pushBackInjection.isEmpty() & maxTry-- > 0) {
+        while (!pushBackInjection.isEmpty() & maxTryPerCycle-- > 0) {
             Iterator<Object> t = new LinkedList(pushBackInjection).iterator();
             pushBackInjection.clear();
             while (t.hasNext()) {
                 instance = t.next();
                 fields.addAll(getInheritedPrivateFields(instance.getClass()));
                 try {
                     injectFields(fields, instance, framework, injectables);
                 } catch (IllegalAccessException e) {
                     logger.warn("""", e);
                 } finally {
                     fields.clear();
                 }
             }
         }
 
         if (!pushBackInjection.isEmpty()) {
             injectionFailed();
         }
 
         for (Object o : postponedMethodExecution) {
             try {
                 applyMethods(o, (Class<Object>) o.getClass());
             } catch (IllegalAccessException e) {
                 logger.warn("""", e);
             }
         }
     }",NotBuggy,"Re-try Injectable Injection: reset counter
",Buggy,"The commit message mentions resetting the counter for a retry mechanism. In the code diff, `maxTry` is replaced with `maxTryPerCycle`, which resets the decrementing counter for every initial call to the method, ensuring that the retry logic now uses the correct number of attempts for each cycle. Previously, the original `maxTry` was decremented globally across multiple calls, which could lead to incorrect limits on retries if the method was called multiple times. By introducing `maxTryPerCycle` and resetting it within the method, the logic now correctly applies the intended retry count independently for each invocation.

This change likely fixes a bug where the retry mechanism was not functioning as intended by reusing and decrementing the same `maxTry` value across multiple method calls. The reset ensures that each method invocation starts with the configured number of retries.

The change aligns with the commit message by addressing the retry logic issue, which can be considered a bug related to counting retries.

**Conclusion: Buggy**"
atmosphere,3067.json,8c46e966a9a873af2cf4d6b4f7577d7975d62d40,"@@ -1,3 +1,3 @@
     public String getContainerName() {
-        return super.getContainerName() + "" using javax.servlet/3.0 and jsr356/WebSocket API"";
+        return super.getContainerName() + "" and jsr356/WebSocket API"";
     }",NotBuggy,"Cosmetic
",NotBuggy,"The commit message ""Cosmetic"" suggests that the change is superficial and does not impact functionality. The diff confirms this, showing a simple text change in a return statement. It removes the description ""using javax.servlet/3.0"" from a concatenated String, likely to update or simplify the information returned by getContainerName().

There are no logical corrections, error-handling updates, or functional changes that indicate a bug fix. The change aligns with the commit message as cosmetic.

Therefore, the assessment for this change is **NotBuggy**."
atmosphere,2596.json,8c2994c415cab838ba6a4571eecb8934cf81c34b,"@@ -1,2 +1,2 @@
-                public void shutdown() {
-                }

+    public void shutdown() {
+    }",NotBuggy,"Refactor, no functional change
",NotBuggy,"The commit message states, ""Refactor, no functional change,"" and the diff shows changes to the indentation of the `shutdown` method. The function's implementation remains identical; only whitespace adjustments occur. The changes align with the commit message, indicating no bug fix was made. Thus, the correct conclusion is **NotBuggy**."
atmosphere,415.json,b9625cac7eaae32687fa4f10939f9d6cbb9749e5,"@@ -1,63 +1,63 @@
     public final static AtmosphereRequest cloneRequest(HttpServletRequest request, boolean loadInMemory, boolean copySession, boolean isDestroyable, boolean createSession) {
         Builder b;
         HttpServletRequest r;
 
         Cookie[] cs = request.getCookies();
         Set<Cookie> hs = Collections.synchronizedSet(new HashSet());
         if (cs != null) {
             for (Cookie c : cs) {
                 hs.add(c);
             }
         }
 
         boolean isWrapped = false;
         if (AtmosphereRequestImpl.class.isAssignableFrom(request.getClass())) {
             b = AtmosphereRequestImpl.class.cast(request).b;
             isWrapped = true;
         } else {
             b = new Builder();
             b.request(request);
         }
 
-        HttpSession session = null;
+        HttpSession session = request.getSession(false);
         if (copySession) {
             session = request.getSession(createSession);
             if (session != null) {
                 session = new FakeHttpSession(session);
             } else {
                 session = new FakeHttpSession("""", null, System.currentTimeMillis(), -1);
             }
         }
 
         b.servletPath(request.getServletPath())
                 .pathInfo(request.getPathInfo())
                 .contextPath(request.getContextPath())
                 .requestURI(request.getRequestURI())
                 .requestURL(request.getRequestURL().toString())
                 .method(request.getMethod())
                 .serverName(request.getServerName())
                 .serverPort(request.getServerPort())
                 .remoteAddr(request.getRemoteAddr())
                 .remoteHost(request.getRemoteHost())
                 .remotePort(request.getRemotePort())
                 .destroyable(isDestroyable)
                 .cookies(hs)
                 .session(session)
                 .principal(request.getUserPrincipal())
                 .authType(request.getAuthType())
                 .isSSecure(request.isSecure());
 
         if (loadInMemory) {
             String s = (String) attributeWithoutException(request, FrameworkConfig.THROW_EXCEPTION_ON_CLONED_REQUEST);
             boolean throwException = s != null && Boolean.parseBoolean(s);
             r = new NoOpsRequest(throwException);
             if (isWrapped) {
                 load(b.request, b);
             } else {
                 load(request, b);
             }
             b.request(r);
         }
 
         return isWrapped ? AtmosphereRequestImpl.class.cast(request) : b.build();
     }",NotBuggy," #2157 use the original session by default when cloning a request
",Buggy,"The commit message indicates that the change should use the original session by default when cloning a request. Let's analyze the diff to see if it relates to this intention:

1. **Session Handling Change:**
   - The original code initializes `session` with `null`.
   - The new code initializes `session` using `request.getSession(false)`, which retrieves the current session without creating a new one if it doesn't exist.

2. **Impact of the Change:**
   - The change directly affects the retrieval of the HTTP session object.
   - By using `getSession(false)`, the code ensures that the original session is accessed only if it exists. This change aligns with making sure the original session is used by default when cloning a request.

3. **Relevance to Commit Message:**
   - The commit clearly states the intent to use the original session by default. The code modification supports this by adjusting how the session is initially retrieved.
   - This adjustment prevents the unintentional creation of a new session, thus adhering to the original session usage.

This modification corrects the logic for handling sessions in the request cloning process. Given the context and the alignment with the commit message, this change can be interpreted as fixing an issue where the original session was not being used appropriately.

Therefore, the answer is **Buggy**."
atmosphere,2073.json,894a09e40995ab99985763f1cdd1e05fed0fd18b,"@@ -1,3 +1,3 @@
-    public boolean supportedType(Type t) {
-        return false;
+    public Object injectable(AtmosphereResource resource) {
+        return null;
     }",NotBuggy,"Refactor Injection
",NotBuggy,"The commit message states ""Refactor Injection,"" and the change in the code modifies the method name and return type from `supportedType(Type t)` to `injectable(AtmosphereResource resource)`. These modifications align with a refactor since they likely relate to altering code structure or design without affecting behavior. This suggests the changes are part of a structural alteration rather than a functional one. Since there is no evidence of error-handling updates, logical corrections, or other signs of bug-related changes, I conclude that the commit does not involve a bug fix. Therefore, the answer is **NotBuggy**."
atmosphere,11.json,1894ff3002e438f057a2e01b781ba5b538d6ca2b,"@@ -1,10 +1,9 @@
     public Object broadcast() {
         try {
-            return (Object) b.broadcast(message).get();
-        } catch (InterruptedException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
-        } catch (ExecutionException ex) {
-            LoggerUtils.getLogger().log(Level.SEVERE, null, ex);
+            return b.broadcast(message).get();
+        }
+        catch (Exception ex) {
+            logger.error(""failed to broadcast message: "" + message, ex);
         }
         return null;
     }",NotBuggy,"migrated from jul logging to SLF4J API.  converted most usages of Exception.printStackTrace() and calls to System.out and System.err to logging calls.  tests now run with logback as the logging implementation and write output to stdout and log file, adjust logback-test.xml in test resources directory as appropriate.
",NotBuggy,"The commit diff shows a change in the error-handling mechanism within the `broadcast` method. Previously, there were separate catch blocks for `InterruptedException` and `ExecutionException`, logging them with Java Util Logging's `LoggerUtils`. The updated code replaces this with a single catch block for a general `Exception`, utilizing SLF4J's `logger.error` method. 

Analysis:

1. **Migration of Logging Framework**: The commit message explicitly states the migration from JUL to SLF4J as one objective. This is substantiated by the changes in the catch block, replacing `LoggerUtils.getLogger().log` with `logger.error`.

2. **Improvement of Error Messaging**: The new logging statement provides contextual information by including the message that failed to broadcast, which wasn't present before. This is an improvement in terms of logging detail but not necessarily indicative of a direct bug fix.

3. **General Exception Handling**: The general `catch (Exception ex)` instead of specific exceptions (`InterruptedException` and `ExecutionException`) could be seen as simplifying error handling. However, broad exception catching doesn't necessarily resolve a bug unless unhandled exceptions previously caused issues.

4. **Alignment with Commit Message**: The changes, as evidenced by the commit message, focus on a logging mechanism update rather than a bug in the logic or error handling itself. If a bug fix, it would be described alongside the change in logging framework, not owing to the framework change alone.

In conclusion, the alterations align with the commit's focus on logging framework migration rather than fixing a functional bug. Hence, the changes are **NotBuggy**."
atmosphere,2900.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,5 +1,5 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         CometContext ctx = CometEngine.getEngine().getCometContext(atmosphereCtx);
         resume(r.getRequest(false), ctx);
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"This commit message suggests an update related to the adoption of JDK 8 features. The code modification in the diff changes the return type of the method `complete` to specify a generic type `AsyncSupport<AtmosphereResourceImpl>`. This change enhances type safety by using generics, which is a feature supported in earlier Java versions but could align with JDK 8's improvements in type inference.

The modification itself does not directly indicate a bug fix. Instead, it improves code clarity and safety through the use of generics. There's no indication of logical corrections, error handling improvements, or other typical signs of bug fixes.

Given this analysis, the change aligns with improving or modernizing the codebase to adhere to newer Java version features rather than fixing a bug.

Therefore, the conclusion is **NotBuggy**."
atmosphere,3053.json,a04f6f742d7aca4e198aca78b48f75eb4ea4bc3a,"@@ -1,15 +1,15 @@
-    public AsyncSupport complete(AtmosphereResourceImpl r) {
+    public AsyncSupport<AtmosphereResourceImpl> complete(AtmosphereResourceImpl r) {
         AtmosphereRequest req = r.getRequest(false);
         CountDownLatch latch = null;
 
         if (req.getAttribute(LATCH) != null) {
             latch = (CountDownLatch) req.getAttribute(LATCH);
         }
 
         if (latch != null) {
             latch.countDown();
         } else if (req.getAttribute(AtmosphereResourceImpl.PRE_SUSPEND) == null) {
             logger.trace(""Unable to resume the suspended connection"");
         }
         return this;
     }",NotBuggy,"Update code to JDK8
",NotBuggy,"The commit message indicates an update to JDK8, and the diff modifies the method signature to include a generic type, `AsyncSupport<AtmosphereResourceImpl>`, rather than just `AsyncSupport`. This change leverages Java 8's generic type improvements, aligning with the commit message's intent to adopt JDK8 features, not fixing a bug. Thus, the answer is **NotBuggy**."
mockito,57.json,f00adf79925cd424394da4bb471c98326d3b5017,"@@ -1,3 +1,3 @@
-    public static <T> NewOngoingStubbing<T> when(T methodCall) {
+    public static <T> OngoingStubbing<T> when(T methodCall) {
         return MOCKITO_CORE.when(methodCall);
     }",NotBuggy,"In order to keep exported, client-facing interfaces fluent, renamed NewOngoingStubbing into OngoingStubbing. Had to change one interface name as well in the process.

--HG--
rename : src/org/mockito/internal/progress/OngoingStubbing.java => src/org/mockito/internal/progress/IOngoingStubbing.java
rename : src/org/mockito/stubbing/NewOngoingStubbing.java => src/org/mockito/stubbing/OngoingStubbing.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401516
",NotBuggy,"The commit's purpose is to rename interfaces to maintain a consistent and fluent API, as indicated by the commit message. The diff shows the renaming of `NewOngoingStubbing` to `OngoingStubbing` in a method signature. No logical or functional code changes are present that would suggest a bug fix. This aligns with the commit intent to refactor for consistency rather than addressing any bugs. Therefore, the answer is **NotBuggy**."
mockito,855.json,87670dcd2a782a8218fa3c559a5da20c9e181476,"@@ -1,3 +1,3 @@
-    public Location getLocation() {
+    public LocationImpl getLocation() {
         return invocation.getLocation();
     }",NotBuggy,"Renamed Location so that we can hoist up an interface to the public api.

--HG--
rename : src/org/mockito/internal/debugging/Location.java => src/org/mockito/internal/debugging/LocationImpl.java
",NotBuggy,"The commit message describes a refactoring activity: renaming a class and its purpose to prepare for a public API interface. The diff shows a change in the return type from `Location` to `LocationImpl`, which aligns with renaming `Location` to `LocationImpl`. This is indicative of structural changes rather than fixing a bug. No bug-related changes such as logical corrections or error-handling improvements are apparent here.

My conclusion is **NotBuggy**."
mockito,1295.json,3ea1dd62f553eb5e9c1b4502552479848aa12e5a,"@@ -1,3 +1,3 @@
     public String toString() {
-        return ""same("" + new ValuePrinter().appendValue(wanted).toString() + "")"";
+        return ""same("" + ValuePrinter.print(wanted) + "")"";
     }",NotBuggy,"Tidied up unit tests and the documentation.
",NotBuggy,"The commit message indicates tidying up of unit tests and documentation, yet the code change is in the `toString()` method implementation. The modification simplifies the code by directly using `ValuePrinter.print(wanted)`, likely for clarity or conciseness. This alteration doesn't correspond to the commit message's focus on unit tests or documentation. Furthermore, there's no evident bug being fixed in this line of code; the change seems to be for code improvement rather than correcting an error.

Hence, the status of this commit should be **NotBuggy** because it neither addresses the stated commit message changes nor fixes a bug based on the given code snippet."
mockito,20.json,f7c1b82139cf23a37f9775ee07c6d8ce0952650e,"@@ -1,3 +1,4 @@
     public static boolean booleanThat(Matcher<Boolean> matcher) {
-        return reportMatcher(matcher).returnFalse();
+        reportMatcher(matcher);
+        return false;
     }",NotBuggy,removed HandyReturnValues,NotBuggy,"The changes in the provided diff involve modifying a method named `booleanThat`. Specifically, the method previously called `reportMatcher(matcher).returnFalse()` and now calls `reportMatcher(matcher)` followed by `return false`.

Let's analyze the changes step by step:

1. **Original Version**: 
    - The method calls `reportMatcher(matcher)` and immediately invokes `returnFalse()` on the result.
    - This suggests that `reportMatcher(matcher).returnFalse()` was a chain of method calls likely designed to return a boolean value, possibly a utility pattern used throughout the codebase.

2. **Updated Version**:
    - `reportMatcher(matcher)` is still invoked, but `returnFalse()` is no longer part of this expression.
    - The method explicitly returns `false` without chaining any further method after `reportMatcher(matcher)`.

**Analysis**:
- The change simplifies the method by removing what could be a utility or helper function (`returnFalse()`) and directly returns `false` instead.
- The commit message states ""removed HandyReturnValues,"" which seems to suggest that functionalities related to convenient or utility return values (such as `returnFalse()`) are being phased out or refactored for simplicity or clarity.

**Conclusion**:
- The change reflects a refactoring or simplification rather than a bug fix, given that functionality regarding the return value remains consistent (it always returns `false`).
- No direct bug is addressed or fixed with this change. It adheres to the commit message by removing a pattern or utility (`HandyReturnValues`).

Therefore, conclude with **NotBuggy**."
mockito,590.json,3658876bfe641290844c7378da991952bbfcaa05,"@@ -1,3 +1,3 @@
-    private String pluralize(int number) {
+    public static String pluralize(int number) {
         return number == 1 ? ""1 time"" : number + "" times"";
     }",NotBuggy,"Made certain things simpler + further refactoring to get rid of cause-based stack traces

--HG--
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%401294
",NotBuggy,"The commit message suggests simplification and refactoring, focusing on removing cause-based stack traces. The code change alters the `pluralize` method's access from private to public and adds `static`. This change doesn't relate to error handling or bug fixes but is likely part of a refactor to improve accessibility or utility. The change doesn't involve stack trace adjustment, error handling, or logical corrections indicating a bug fix. Therefore, the code modification aligns more with refactoring rather than addressing a bug.

The commit described doesn't match the change since there is no clear removal or simplification of stack traces in this snippet. However, the modification fits within a ""simplification"" context by allowing broader access to a utility function. Still, this isn't indicative of fixing a bug per se, more just making the method accessible.

Thus, the answer is **NotBuggy**."
mockito,976.json,f668427341272626209d0b21d1cb1e349a7186ad,"@@ -1,3 +1,3 @@
-    private boolean canRecoverFromFailure(VerificationMode verificationMode) {
-        return !(verificationMode instanceof AtMost || verificationMode instanceof Only || verificationMode instanceof NoMoreInteractions);
+    protected boolean canRecoverFromFailure(VerificationMode verificationMode) {
+        return !(verificationMode instanceof AtMost || verificationMode instanceof NoMoreInteractions);
     }",NotBuggy,"Added early completion on definite failure to timeout() too, by refactoring to combine after() and timeout() implementations
",NotBuggy,"The commit message indicates a refactor to handle early completion in a timeout scenario, suggesting improvements in failure detection. The code change alters access from private to protected and removes a check against the `Only` class, which likely relates to refactoring efforts rather than direct bug fixing. The actual combination of `after()` and `timeout()` implementations is not evidenced in this diff. Without more context or evidence of an actual bug being fixed within this change, the modifications seem to support refactoring goals, possibly aligning with internal architectural adjustments rather than a specific bug correction.

**NotBuggy**"
mockito,251.json,67e4b5884a1ff5f314cf633194e468131d0f2b53,"@@ -1,16 +1,16 @@
     public <T> T createMock(MockCreationSettings<T> settings, MockHandler handler) {
-        Class<? extends T> type = bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings));
+        Class<? extends T> type = createMockType(settings);
 
         Instantiator instantiator = Plugins.getInstantiatorProvider().getInstantiator(settings);
         try {
             T instance = instantiator.newInstance(type);
             MockMethodInterceptor mockMethodInterceptor = new MockMethodInterceptor(asInternalMockHandler(handler), settings);
             mocks.put(instance, mockMethodInterceptor);
             if (instance instanceof MockAccess) {
                 ((MockAccess) instance).setMockitoInterceptor(mockMethodInterceptor);
             }
             return instance;
         } catch (org.mockito.internal.creation.instance.InstantiationException e) {
             throw new MockitoException(""Unable to create mock instance of type '"" + type.getSimpleName() + ""'"", e);
         }
     }",NotBuggy,"Rework MockMaker.createMockType to take MockCreationSettings
",NotBuggy,"The commit message indicates a method is reworked to take `MockCreationSettings`, and the diff reflects this by replacing the method `bytecodeGenerator.mockClass(mockWithFeaturesFrom(settings))` with `createMockType(settings)`. The focus here is on changing how the mock type is determined or created, but there's no direct evidence of fixing a bug—just a refactor or enhancement in the API usage within the logic of mock creation. The functionality appears consistent overall.

Given the code changes and the commit message, it's more of an internal rework rather than a bug fix. There are no direct signs of any error handling improvements or logical corrections that suggest a bug fix.

Thus, this commit is classified as **NotBuggy**."
mockito,1818.json,4ac04a1e40e015a457a3cd05e9bcb3b2ba27410d,"@@ -1,3 +1,3 @@
         public T should(VerificationMode mode) {
-            return verify(mock, mode);
+            return Mockito.verify(mock, mode);
         }",NotBuggy,#NAME?,Buggy,"The code change updates a method call from `verify(mock, mode);` to `Mockito.verify(mock, mode);`. The commit message is missing, so we must infer the purpose solely from the code change.

Here's the reasoning:

1. **Context Understanding**: The change involves a call to the `verify` method. In Java testing, particularly with Mockito, `verify` is used to confirm that certain interactions with mock objects occurred as expected.

2. **Modification Analysis**: The modification from `verify(mock, mode);` to `Mockito.verify(mock, mode);` suggests there may have been an issue where the method was not correctly resolved to `Mockito.verify`. This could have caused a compile error if `verify` was ambiguously resolved or referred to an unexpected method.

3. **Bug Examination**: If `verify` was meant to invoke `Mockito.verify` and this was causing a problem (e.g., due to a static import that was not present, or name conflicts within the scope), then this change would indeed fix a bug.

4. **Conclusion**: By explicitly qualifying the `verify` method with `Mockito`, it ensures the correct method is always called, which implies a resolution to a potential invocation error.

Given that this change corrects a potential method invocation issue that would prevent proper functionality, it points out a bug fix related to code correctness.

The conclusion is **Buggy**."
mockito,1051.json,9eebf1cfb0062e36634fd6d5bcbc047af0ae8fcc,"@@ -1,3 +1,3 @@
-    public void wantedButNotInvoked(PrintableInvocation wanted) {
+    public void wantedButNotInvoked(DescribedInvocation wanted) {
         throw new WantedButNotInvoked(createWantedButNotInvokedMessage(wanted));
     }",NotBuggy,"Made the PrintableInvocation deprecated and created DescribedInvocation instead. I didn't like the PrintableInvocation that much, DescribedInvocation better describes the purpose and lives in a better package, too.
",NotBuggy,"The commit message indicates a deprecation of `PrintableInvocation` in favor of `DescribedInvocation`, which is a refactoring or an API design improvement. The code change reflects this by replacing `PrintableInvocation` with `DescribedInvocation` in a method parameter. This change does not suggest any fixes to errors or incorrect behavior. Instead, it is an improvement for better clarity and structure within the codebase. There are no indicators of bug fixes such as error-handling updates, logic corrections, or exception handling improvements.

Therefore, the changes align with the commit message but do not indicate a bug fix. The assessment is **NotBuggy**."
mockito,1329.json,4bc732153e83b9490d6d94c2e27ea2f6ac9dc10c,"@@ -1,3 +1,3 @@
     public Object getMock() {
-        return mock;
+        return toInspect;
     }",NotBuggy,"Removed unnecessary type, we can use public type instead

- Instead of MockitoMock, we can simply use our public API: MockingDetails. It allows us to reuse / dogfood our public API and have cleaner codebase.
- Added new public API: mockingDetails.getMock(). I needed it to implement APIs for Spring Boot integration. Plus, it just make sense to add this method to make MockingDetails type more complete.
",NotBuggy,"The commit message explains a refactor rather than a bug fix. The change adjusts the method getMock() to return toInspect instead of mock, aligning with new public API objectives. It supports cleaner code without indicating bug resolution. The answer is **NotBuggy**."
mockito,1759.json,5f41dde62faa4c3c381d29cb550243fa2378e8d0,"@@ -1,4 +1,4 @@
     public static byte[] aryEq(byte[] value) {
-        LastArguments.reportMatcher(new ArrayEquals(value));
+        LastArguments.instance().reportMatcher(new ArrayEquals(value));
         return null;
     }",NotBuggy,"some refactorings, enabled some ingnored tests

--HG--
rename : src/org/mockito/exceptions/NotAMockMethodException.java => src/org/mockito/exceptions/MissingMethodInvocationException.java
rename : src/org/mockito/internal/ExpectedInvocation.java => src/org/mockito/internal/InvocationWithMatchers.java
rename : src/org/mockito/internal/MockitoOperations.java => src/org/mockito/internal/MockitoState.java
extra : convert_revision : svn%3Aaa2aecf3-ea3e-0410-9d70-716747e7c967/trunk%4030
",NotBuggy,"The commit message mentions refactorings and enabling tests, indicating that the changes are not primarily focused on fixing bugs. The code diff reflects a refactoring effort, changing a static method call to an instance method call. This change likely aligns with a broader refactoring strategy but doesn't indicate a specific bug fix, such as error-handling updates or logical corrections. Therefore, this commit is classified as **NotBuggy**."
mockito,715.json,e3d77c7cb21eb5e3cc8f240f2e9cab3273a3a515,"@@ -1,3 +1,3 @@
     public Stubber doThrow(Class<? extends Throwable> toBeThrown, Class<? extends Throwable>... nextToBeThrown) {
-        return null;
+        return stubber().doThrow(toBeThrown, nextToBeThrown);
     }",NotBuggy,"Implemented doThrow varag syntax for lenient() mode
",NotBuggy,"The commit message suggests implementing a functionality related to a method that handles exceptions (doThrow vararg syntax for lenient() mode). The code change modifies the `doThrow` method to call another `doThrow` method from a `stubber()` instance, passing the provided exception classes. 

The change doesn't directly imply a bug fix. Instead, it appears to be the implementation of a previously incomplete or placeholder method, where the method originally returned null, indicating it may not have been functioning as intended. The implementation enables the handling of multiple exceptions through varargs, aligning with the purpose indicated in the commit message.

There's no explicit indication of fixing an error-handling or logical flaw introduced elsewhere, but rather it enables new functionality or completes the method's logic as initially expected.

Therefore, based on the information, the most appropriate conclusion is **NotBuggy**. This change is more about functionality introduction or completion than fixing an existing bug."
mockito,849.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,3 +1,3 @@
-    public Invocation getInvocation() {
+    public InvocationImpl getInvocation() {
         return this.invocation;
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message indicates a renaming operation from `Invocation` to `InvocationImpl`. The provided diff shows a corresponding change in the method return type, which aligns with the renaming operation. This is not a bug fix, but rather a code organization or refactoring effort to more accurately reflect the class's implementation and naming conventions.

The change involves a structural modification, where `Invocation` is likely an interface or abstract type, and `InvocationImpl` is a concrete class. The refactoring suggests using the interface type where possible, which is a common best practice for promoting flexibility in object-oriented design. However, the method's return type is explicitly changed to the concrete class `InvocationImpl`, which seems to contradict this approach slightly, unless it's intentional for some specific usage of the class's concrete implementation.

Overall, the given changes are consistent with the commit message focusing on renaming and do not indicate any specific bug-related fixes.

The conclusion is **NotBuggy**."
mockito,810.json,a99d63b477bac80720fa25214ef7c3eb73576627,"@@ -1,8 +1,8 @@
-    public LocationImpl getLastLocation(List<Invocation> invocations) {
+    public LocationImpl getLastLocation(List<InvocationImpl> invocations) {
         if (invocations.isEmpty()) {
             return null;
         } else {
-            Invocation last = invocations.get(invocations.size() - 1);
+            InvocationImpl last = invocations.get(invocations.size() - 1);
             return last.getLocation();
         }
     }",NotBuggy,"Renamed Invocation -> InvocationImpl before using the interface type where possible...

--HG--
rename : src/org/mockito/internal/invocation/Invocation.java => src/org/mockito/internal/invocation/InvocationImpl.java
",NotBuggy,"The commit message describes a renaming operation to change the class name from `Invocation` to `InvocationImpl`. The code diff aligns with this message, showing a change in the method `getLastLocation` where the parameter and variable types are updated from `Invocation` to `InvocationImpl`. This is a refactoring task intended to reflect the renaming action and improve code clarity by distinguishing between interfaces and their implementations.

There is no indication of a bug fix from the code change itself. The modification does not include logical changes, error handling updates, exception management, or any corrections to existing functionality; it strictly involves a rename to reflect a conceptual change in how the code base handles the `Invocation` construct, moving towards using interfaces appropriately.

Thus, the changes in the diff align with the described intentions in the commit message, supporting the refactoring related to naming conventions but not directly addressing any bugs.

The conclusion is **NotBuggy**."
mockito,867.json,ccb369df05a9724ac9a8ab7abfd797b0ab01097e,"@@ -1,5 +1,4 @@
     public Invocation createInvocation(Object target, MockCreationSettings settings, Method method, Callable realMethod, Object... args) {
-        //TODO SZ - move real method away from InterceptedInvocation
-        InterceptedInvocation.RealMethod.FromCallable superMethod = new InterceptedInvocation.RealMethod.FromCallable(realMethod);
+        RealMethod.FromCallable superMethod = new RealMethod.FromCallable(realMethod);
         return MockMethodInterceptor.createInvocation(target, method, args, superMethod, settings);
     }",NotBuggy,"Moved inner class to top level

This way we can use it in other context like the new public API for creating instances of Invocation.
",NotBuggy,"The commit message specifies moving an inner class to the top level for broader usage, a refactor enhancing reusability. No bug fix is implied, as changes do not signal logical errors or corrections. The answer is **NotBuggy**."
mongo-java-driver,189.json,35bbbae5b9afb9a83c76c21bebb547f2b71426d3,"@@ -1,3 +1,3 @@
-    public DBObject findOne(DBObject o, DBObject fields, DBObject orderBy) {
+    public DBObject findOne(final DBObject o, final DBObject fields, final DBObject orderBy) {
         return findOne(o, fields, orderBy, getReadPreference());
     }",NotBuggy,"Added interfaces to aid maintaining backward compatibility in driver-compat.  Unimplemented methods throw exceptions.  Added QuickTour to driver-compat to drive out unimplemented functionality
",NotBuggy,"The diff shows only the addition of the `final` keyword to the parameters of the `findOne` method. This change ensures that the parameters cannot be modified within the method, which is primarily a style or design improvement to prevent accidental changes to the parameters' values. It does not address any existing bug or logical error within the code. 

The commit message suggests changes related to backward compatibility and adding interfaces, with a focus on unimplemented methods throwing exceptions, which doesn't align with the simple modification in the diff.

Given the nature of the change:

- There is no indication of error-handling improvements, logical corrections, or exception-handling related to bug fixes.
- The commit message does not correlate directly with the affected code in this diff.

Based on this analysis, the commit does not seem to fix a bug as its primary intention with this particular diff.

The conclusion is **NotBuggy**."
mongo-java-driver,8694.json,bcfca4ff8ecacdb7b1df6fcbb9c5d044ed4c431e,"@@ -1,6 +1,7 @@
     private BsonDocument getCommand(final ConnectionDescription description) {
-        BsonDocument commandDocument = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
+        BsonDocument command = new BsonDocument(""dropIndexes"", new BsonString(namespace.getCollectionName()))
                                                .append(""index"", new BsonString(indexName));
-        appendWriteConcernToCommand(writeConcern, commandDocument, description);
-        return commandDocument;
+        putIfNotZero(command, ""maxTimeMS"", maxTimeMS);
+        appendWriteConcernToCommand(writeConcern, command, description);
+        return command;
     }",NotBuggy,"Added maxTimeMS support for index operations

CreateIndexOperation and DropIndexOperation now support
maxTimeMS

JAVA-2537
",NotBuggy,"The commit message indicates the addition of `maxTimeMS` support for index operations, which aligns with the changes in the code. In the diff, the method `putIfNotZero(command, ""maxTimeMS"", maxTimeMS);` is added to insert the `maxTimeMS` field into the command document if its value is not zero. This is consistent with what is mentioned in the commit message.

However, this update is related to adding a new feature (support for maxTimeMS) rather than fixing a bug. No signs of bug-related changes, error-handling updates, logical corrections, or exception-handling improvements are present in this diff. Therefore, the appropriate conclusion for the nature of this change is **NotBuggy**."
mongo-java-driver,11486.json,9ae1e5cf7cebeab5ffec07a4ef4203f199ea5ac7,"@@ -1,3 +1,3 @@
-    public int getInsertedCount() {
-        throw getUnacknowledgedWriteException();
-    }

+            public List<BulkWriteUpsert> getUpserts() {
+                throw getUnacknowledgedWriteException();
+            }",NotBuggy,"Updated BulkWriteResult to follow the past tense convention in core for acknowledged operations.
",NotBuggy,"The commit message specifies an update to follow a past tense naming convention for acknowledged operations. However, the code change in the diff does not reflect a name change from present to past tense or any naming modification. Instead, it shows a replacement of a method `getInsertedCount()` with `getUpserts()`, both of which throw an exception.

This change does not correspond with the commit message about following naming conventions. It instead replaces one method with a completely different one, which is unrelated to naming conventions and seems more about functionality changes rather than grammar or tense conformity.

Additionally, there's no indication that the behavior or exception handling changed in a way that reflects fixing a bug or error handling improvement.

Therefore, the mismatch between the commit message and the actual code changes implies this is not a straightforward fix related to any functional or operational issue, especially concerning bugs.

The assessment is **NotBuggy**."
mongo-java-driver,6838.json,aa2cfa6c3692bd4c26a52e673c0c1a1b52b2b26c,"@@ -1,3 +1,3 @@
-    public WriteResult getWriteResult() {
-        return writeResult;
+    public WriteConcernResult getWriteConcernResult() {
+        return writeConcernResult;
     }",NotBuggy,"Changed property type in WriteConcernException from WriteResult to WriteConcernResult

 JAVA-1395
",Buggy,"The commit message describes a type change in a property within the `WriteConcernException` class, from `WriteResult` to `WriteConcernResult`. The diff shows a method name change from `getWriteResult()` returning `WriteResult` to `getWriteConcernResult()` returning `WriteConcernResult`. Here's the analysis:

1. **Method Return Type Change**: The code change directly switches the type of the property and the method name to reflect the new type. This type change indicates a modification in the class's API.

2. **Alignment with Commit Message**: The code changes match the commit message, which explicitly describes this alteration in the type from `WriteResult` to `WriteConcernResult`.

3. **Potential Bug Indication**: 
   - If `WriteResult` was causing issues, like mismatched expectations or errors in reading concern results, switching to `WriteConcernResult` might resolve such issues. Given the specificity of the change, it's likely addressing an error related to handling write concerns.
   - The change may imply a correction of logical error in type, which can be a bug fix, ensuring that the exception accurately reflects the correct concern result type, potentially fixing misbehaviors arising from using the wrong type.

Consequently, due to the change likely resolving a logical/API mismatch regarding result handling, it suggests a bug fix. Therefore, the conclusion is **Buggy**."
mongo-java-driver,787.json,c5da37698cd4c57932a05dd0bdd51685d277ad49,"@@ -1,3 +1,7 @@
     private BsonDocument wrap(DBObject document, DBEncoder encoder) {
-        return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        if (encoder == null) {
+            return wrap(document);
+        } else {
+            return new BsonDocumentWrapper<DBObject>(document, new DBEncoderAdapter(encoder));
+        }
     }",NotBuggy,"driver-compat tests all passing except for field name validation
",Buggy,"The commit message mentions driver-compat tests passing, and this change involves adding a conditional check for a `null` encoder. 

Here's a step-by-step analysis:

1. **Previous Behavior:** The original method would always return a `BsonDocumentWrapper<DBObject>` regardless of whether the `encoder` parameter was `null` or not.

2. **Current Change:** The updated code introduces a conditional check using `if (encoder == null)`. If the `encoder` is `null`, it calls an overloaded `wrap(document)` method, presumably to handle cases where encoding is not required.

3. **Implication of Change:** This change implies that previously, if an `encoder` was `null`, the code would proceed incorrectly which might lead to errors if the `DBEncoderAdapter` didn't handle `null` values correctly. With this update, the presence of the `if-else` block ensures that the method can correctly handle a `null` scenario.

4. **Relevance to Commit Message:** The commit message indicates that tests are passing with the exception of field name validation. The code change relates to error-checking by improving robustness when handling `null` encoders.

Given the above analysis, this change is indicative of a bug fix related to correctly handling potential `null` input scenarios with error prevention. Consequently, the commit aligns with the characteristics of a bug fix.

Thus, the conclusion is **Buggy**."
mongo-java-driver,9253.json,679f5ff0d4d7578ad1dea71f7caf3bf54f920051,"@@ -1,3 +1,3 @@
-    public BsonDocument getCriteria() {
-        return criteria;
+    public BsonDocument getFilter() {
+        return filter;
     }",NotBuggy,"Replaced all uses of criteria with filter in public APIs
",NotBuggy,"The commit message states ""Replaced all uses of criteria with filter in public APIs."" The diff shows that a method named `getCriteria` was renamed to `getFilter`, along with its instance variable. This change aligns with the commit message, indicating a structural rename in the public API. 

The change is focused on consistent terminology and does not address any bug-related fixes. There is no indication of logic errors or enhancements in error handling or exception management related to the change, which would typically signify a bug fix.

Thus, this change appears to be a refactoring of the API for clarity or consistency, not a bug fix. Therefore, my conclusion is **NotBuggy**."
mongo-java-driver,4729.json,85caac9ec625d101e7d678276027ed8c063f66a6,"@@ -1,3 +1,3 @@
-    public Publisher<Void> insertOne(final TDocument document) {
+    public Publisher<InsertOneResult> insertOne(final TDocument document) {
         return insertOne(document, new InsertOneOptions());
     }",NotBuggy,"Insertions now return a result

Added InsertOneResult and InsertManyResult to insert operations

JAVA-1788
",NotBuggy,"The commit message indicates that insert operations now return a result by introducing `InsertOneResult` and `InsertManyResult`. The diff changes the return type of the `insertOne` method from `Publisher<Void>` to `Publisher<InsertOneResult>`. This aligns with the commit message and represents a functional enhancement rather than a bug fix. The modification provides more informative outcomes for insert operations by returning specific results instead of just a `void` indication.

Since this change is about improving functionality and returning more useful information, rather than correcting erroneous behavior, it does not indicate a bug fix.

Thus, the conclusion is **NotBuggy**."
mongo-java-driver,2768.json,84d8d7c95e0ebaa072717816471d4cf1c9539817,"@@ -1,13 +1,11 @@
     public T decode(final BsonReader reader, final DecoderContext decoderContext) {
-        if (decoderContext.hasCheckedDiscriminator()) {
-            if (!specialized) {
-                throw new CodecConfigurationException(""Cannot decode using an unspecialized generic ClassModel"");
-            }
-            InstanceCreator<T> instanceCreator = classModel.getInstanceCreator();
-            decodeProperties(reader, decoderContext, instanceCreator);
-            return instanceCreator.getInstance();
-        } else {
-            return getCodecFromDocument(reader, classModel.useDiscriminator(), classModel.getDiscriminatorKey(), registry,
-                    discriminatorLookup, this).decode(reader, DecoderContext.builder().checkedDiscriminator(true).build());
+        try {
+            return pojoCodec.decode(reader, decoderContext);
+        } catch (CodecConfigurationException e) {
+            throw new CodecConfigurationException(
+                    format(""An exception occurred when decoding using the AutomaticPojoCodec.%n""
+                            + ""Decoding into a '%s' failed with the following exception:%n%n%s%n%n""
+                            + ""A custom Codec or PojoCodec may need to be explicitly configured and registered to handle this type."",
+                            pojoCodec.getEncoderClass().getSimpleName(), e.getMessage()), e);
         }
     }",NotBuggy,"Added an automatic setting to the PojoCodecProvider

Automatically creates a PojoCodec for any unseen class that contains
serializable / deserializable properties.

JAVA-2567
",Buggy,"The changes in the diff focus on using an automatic PojoCodec for decoding. The refactored code wraps the decoding process in a try-catch block, catching CodecConfigurationExceptions and throwing a new, detailed exception. This provides additional context about the failure and suggests configuring a custom Codec if necessary.

Key observations:

1. **Before the Change**: The previous code attempted to decode based on a condition involving a discriminator. If not specialized, it threw an error. Otherwise, it used a fallback method to get a codec and decode.

2. **After the Change**: The new approach uses a single call to `pojoCodec.decode()`. If it fails, it throws a customized exception that provides more information and guidance.

3. **Error-handling Improvement**: The change enhances error handling by providing more specific and user-friendly messages. It suggests possible solutions when decoding fails, which can be vital for troubleshooting.

The commit message does not indicate a direct bug fix but introduces an automatic setting for the PojoCodecProvider. However, aligning with improved error reporting and handling unconfigured types suggests it potentially resolves issues in dynamic type management.

Considering these points, while the primary objective is not directly a specific bug fix, the improvements in error management and configuration could indicate addressing previously unhandled scenarios, errors, or usability issues. Therefore, the conclusion for this case is:

**Buggy**"
mongo-java-driver,916.json,f148733678e3585a89dc2eb5b580cf37170c9396,"@@ -1,3 +1,3 @@
-                    public void apply(final GridFSFile gridFSFile) {
+                    public void accept(final GridFSFile gridFSFile) {
                         System.out.println(gridFSFile.getFilename());
                     }",NotBuggy,"Remove deprecated MongoIterable#forEach(Block<T>) method

JAVA-3194
",NotBuggy,"This commit message indicates the removal of a deprecated method, specifically `MongoIterable#forEach(Block<T>)`. However, the diff only shows a change in method name from `apply` to `accept`, likely aligning with Java's functional interfaces or addressing a deprecated method usage. It does not explicitly suggest a bug fix, as the primary focus is on aligning with updated method naming conventions or libraries.

There is no evidence of handling errors, fixing logical issues, or improving exception handling related to a bug. The change appears to be more about code modernization or deprecation handling rather than fixing a bug.

Therefore, the analysis concludes with **NotBuggy**."
mongo-java-driver,6716.json,59906debe95ea760891c8bae855bca239575dc72,"@@ -1,3 +1,3 @@
-    public boolean getRetryWrites() {
+    public Boolean getRetryWritesValue() {
         return retryWrites;
     }",NotBuggy,"Only apply retryWrites/compressors from connection string if set

JAVA-3057
JAVA-3058
",NotBuggy,"The commit message indicates changes related to `retryWrites` settings based on specific conditions. The diff shows a method name change, but does not indicate a bug fix or logic adjustment, only a renaming. Method renaming from `getRetryWrites` to `getRetryWritesValue` likely improves clarity or consistency with style guidelines, but it does not directly imply a bug fix or condition check involving applying retryWrites/compressors.

The commit does not show any conditionals or logic changes that would suggest a bug fix related to how `retryWrites` is applied. The code diff has no concrete bug correction or feature behavior change evident from the given information. 

Therefore, I conclude that the change is **NotBuggy**."
mongo-java-driver,3068.json,fa60415971bd69a1e4cfbfc57fdfa1ca17ec329b,"@@ -1,7 +1,7 @@
-    private void writeList(final BsonWriter writer, final List<Object> list, final EncoderContext encoderContext) {
+    private void writeIterable(final BsonWriter writer, final Iterable<Object> list, final EncoderContext encoderContext) {
         writer.writeStartArray();
         for (final Object value : list) {
             writeValue(writer, encoderContext, value);
         }
         writer.writeEndArray();
     }",NotBuggy,"When encoding a Document, encode all Iterable instances as BSON arrays, rather than just List instances.  This is consistent with the behavior of the DBObject encoder,
so this will make it easier to migrate from DBObject to Document

 JAVA-1761
",NotBuggy,"The commit message indicates a change in encoding logic to handle all `Iterable` instances instead of just `List` instances, aligning `Document` behavior with `DBObject`. The code diff reflects this change by updating the method parameter from `List<Object>` to `Iterable<Object>`, allowing any iterable structure to be treated as a BSON array. 

Though this change enhances compatibility and flexibility for `Document` objects, it does not correct an erroneous behavior or resolve a defect in the current system's functionality. It is an enhancement to ensure consistency with the `DBObject` encoder, particularly aiding in migration.

The alignment with `DBObject` suggests adapting behavior rather than fixing a malfunction. Consequently, despite being a logical adjustment, it doesn’t directly indicate a bug fix aimed at rectifying incorrect behavior, but rather it extends existing functionality to be more inclusive of different `Iterable` types.

Thus, this commit should be categorized as **NotBuggy** because it reflects an improvement or feature adjustment rather than an explicit fix of a malfunction."
mongo-java-driver,9860.json,5ef2985d8da07b58d49b4c517b0e7276e7366ccc,"@@ -1,3 +1,3 @@
     public void deleteOne(final Bson filter, final SingleResultCallback<DeleteResult> callback) {
-        delete(filter, false, callback);
+        deleteOne(filter, new DeleteOptions(), callback);
     }",NotBuggy,"Collation updates

Collation set on a per operation basis rather than globally.

JAVA-2241
",NotBuggy,"The commit message ""Collation updates"" suggests adjustments related to collation settings. Although the diff shows a change in method invocation from `delete` to `deleteOne` with a new `DeleteOptions` parameter, it does not directly indicate a bug fix concerning collation or any issue being resolved universally. Instead, this change appears to introduce a refined way to handle deletions, possibly allowing more options (such as collation) on a per-operation basis rather than affecting a larger scope. Without explicit indication that this solves a defect or erroneous behavior previously present, it suggests an enhancement or refinement rather than a bug fix.

Therefore, based on the diff and the commit message, I conclude **NotBuggy**."
mongo-java-driver,2518.json,50f7d6cc46dd3b94ed46fef91dde77844245fd04,"@@ -1,6 +1,6 @@
-    public boolean istInt64(final Object key) {
+    public RegularExpression getRegularExpression(final Object key, final RegularExpression defaultValue) {
         if (!containsKey(key)) {
-            return false;
+            return defaultValue;
         }
-        return get(key).isInt64();
+        return get(key).asRegularExpression();
     }",NotBuggy,"Added missing BsonDocument and BsonValue methods, and added test specs for both classes
",NotBuggy,"The commit message talks about adding missing methods and tests for BsonDocument and BsonValue. However, the diff changes the method signature and logic from handling Int64 to dealing with RegularExpressions. This is more of a feature change or refactor than a straightforward bug fix. The method `istInt64` was replaced by `getRegularExpression`, altering its purpose and return type.

No indication suggests this resolves a bug. It aligns with ""added missing methods"" but doesn't support bug fixes. Thus, the answer is **NotBuggy**."
mongo-java-driver,8113.json,7cc4be219a15b08c69dfba8d02c3e014f808b90d,"@@ -1,36 +1,19 @@
-    public <A> void read(
-            final ByteBuffer dst,
-            final long timeout, final TimeUnit unit,
-            final A attach, final CompletionHandler<Integer, ? super A> handler) {
-        checkReadOnly(dst);
-        if (!dst.hasRemaining()) {
-            completeWithZeroInt(attach, handler);
-            return;
-        }
-        group.startRead(
-                registeredSocket,
-                new ByteBufferSet(dst),
-                timeout, unit,
-                new LongConsumer() {
-                    @Override
-                    public void accept(final long c) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.completed((int) c, attach);
-                            }
-                        });
-                    }
-                },
-                new Consumer<Throwable>() {
-                    @Override
-                    public void accept(final Throwable e) {
-                        group.executor.submit(new Runnable() {
-                            @Override
-                            public void run() {
-                                handler.failed(e, attach);
-                            }
-                        });
-                    }
-                });
-    }

+  public <A> void read(
+      ByteBuffer dst,
+      long timeout,
+      TimeUnit unit,
+      A attach,
+      CompletionHandler<Integer, ? super A> handler) {
+    checkReadOnly(dst);
+    if (!dst.hasRemaining()) {
+      completeWithZeroInt(attach, handler);
+      return;
+    }
+    group.startRead(
+        registeredSocket,
+        new ByteBufferSet(dst),
+        timeout,
+        unit,
+        c -> group.executor.submit(() -> handler.completed((int) c, attach)),
+        e -> group.executor.submit(() -> handler.failed(e, attach)));
+  }",NotBuggy,"Revendored tlschannel to 0.3.2

JAVA-3588
",NotBuggy,"The commit message ""Revendored tlschannel to 0.3.2"" with no explicit mention of bug fixes suggests vendor library version update, not a bug fix. The Java diff simplifies lambda expressions, but logic remains the same. There are no apparent error-handling improvements or logical corrections made—it’s primarily stylistic change for readability and modernize the syntax using lambdas. Given the lack of specific evidence of a bug being fixed, the answer is **NotBuggy**."
mongo-java-driver,5025.json,24780d53705de06d511af7e5cae18081449c9401,"@@ -1,11 +1,6 @@
     public Publisher<ObjectId> uploadFromStream(final ClientSession clientSession, final String filename, final AsyncInputStream source,
                                                 final GridFSUploadOptions options) {
-        return new SingleResultObservableToPublisher<ObjectId>(
-                new Block<SingleResultCallback<ObjectId>>() {
-                    @Override
-                    public void apply(final SingleResultCallback<ObjectId> callback) {
-                        wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source), options,
-                                callback);
-                    }
-                });
+        return new SingleResultObservableToPublisher<>(
+                callback -> wrapped.uploadFromStream(clientSession.getWrapped(), filename, toCallbackAsyncInputStream(source),
+                        options, callback));
     }",NotBuggy,"Publisher<Success> => Publisher<Void>

Removed the Success enum to represent a successful operation. Idioms
have changed since the 1.0 version of the MongoDB Reactive Streams driver
and although null is an invalid value for a publisher. A Publisher<Void>
has come to represent empty publisher that only signals when a publisher has
either completed or errored.

Removed the Scala type alias of Completed to Void. This brings the
implementation inline with the driver it wraps.

JAVA-3398
",NotBuggy,"The commit message describes changes to the API, specifically shifting from `Success` enumeration to `Void` for publishers, and removing Scala type aliasing. This reflects a broader design adjustment rather than addressing a bug in the implementation. The `uploadFromStream` method's change uses a lambda expression instead of an anonymous inner class, which is unrelated to a bug fix but improves code readability and conciseness.

The diff doesn't reflect a large logical correction, error-handling update, or explicit indication of a bug fix. It's primarily focused on code style and API evolution to align with new idioms. Given this context, the commit does not indicate a bug fix.

Thus, the analysis concludes with **NotBuggy**."
junit5,948.json,a7507742dd8a690ebd1da503eadb4cbd2965fa9e,"@@ -1,3 +1,3 @@
-	public static MethodSource from(Method method) {
-		return new MethodSource(method);
+	public static MethodSource from(Method testMethod) {
+		return new MethodSource(testMethod);
 	}",NotBuggy,"Polish MethodSource (test source)
",NotBuggy,"The commit message refers to this change as a ""polish"" of the MethodSource method, which generally implies a non-functional update aimed at improving code clarity or style. In this diff, the only change is the renaming of a parameter from `method` to `testMethod`. This is likely a cosmetic change to enhance code readability, indicating that the parameter represents a test method.

There's no indication of a bug being fixed here. The modification doesn't address error-handling, logic errors, or exception handling, which are strong indicators of bug fixes. The change merely makes the code clearer by using a more descriptive parameter name.

Therefore, this change aligns with the commit message's statement of polishing, rather than fixing. 

**NotBuggy**"
junit5,387.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isFinal(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message states that precondition checks are being moved from `ModifierSupport` to `ReflectionUtils`, implying an internal refactor rather than a bug fix.

The code change involves the removal of a precondition check using `Preconditions.notNull(clazz, ""Class must not be null"");`, which checks that the `clazz` parameter is not null. The responsibility for this null check is presumably being transferred to another utility class, `ReflectionUtils`, suggesting that `ReflectionUtils.isFinal(clazz)` handles the null check internally.

There is no explicit indication in either the diff or the commit message that this change resolves a bug, as the main purpose appears to be refactoring the code to centralize or relocate functionality. The diff does not show any indication of error-handling updates, logical corrections, or exception-handling improvements specific to a bug fix.

Therefore, this change is an example of code refactoring or project restructuring rather than directly fixing a bug.

The conclusion is **NotBuggy**."
junit5,1662.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<URI> getSelectedUris() {
-		return selectedUris;
+		return this.selectedUris;
 	}",NotBuggy,"Polishing
",NotBuggy,"The commit message states ""Polishing,"" and the code change involves a minor stylistic modification that uses `this.selectedUris` instead of simply `selectedUris`. This is a non-functional change aimed at improving code readability or adhering to style conventions rather than fixing a bug. There are no corrections or error-handling modifications present that suggest a bug fix.

Thus, the change aligns with the commit message which implies refinement, not a functional update. The conclusion is **NotBuggy**."
junit5,1670.json,52a9de0933495ee1aa7443de10d0c823a734d462,"@@ -1,3 +1,3 @@
 	public List<String> getSelectedPackages() {
-		return selectedPackages;
+		return this.selectedPackages;
 	}",NotBuggy,"Polishing
",NotBuggy,"The change simply adds the `this` keyword for clarity. It doesn't alter functionality or fix a bug. The term ""polishing"" in the commit message suggests code style or readability improvements rather than bug fixes. The answer is **NotBuggy**."
junit5,1710.json,75de66e17a98aa9627ad388338aacae8d8f17484,"@@ -1,3 +1,3 @@
-			public String generateDisplayNameForClass(Class<?> testClass) {
-				return replaceUnderscore(DEFAULT.generateDisplayNameForClass(testClass));
-			}

+		public String generateDisplayNameForClass(Class<?> testClass) {
+			return replaceUnderscores(super.generateDisplayNameForClass(testClass));
+		}",NotBuggy,"Document display name generation

Move pre-defined generators to DisplayNameGenerator interface.
",NotBuggy,"The commit message indicates a documentation change and mentions moving pre-defined generators, which suggests refactoring or improving structure. However, this specific diff focuses on changing `replaceUnderscore` to `replaceUnderscores` and using `super.generateDisplayNameForClass(testClass)` instead of `DEFAULT.generateDisplayNameForClass(testClass)`. These changes likely aim to correct method behavior or align it with inheritance patterns rather than fixing a specific functional bug. 

The commit message about moving pre-defined generators also indicates broader changes than what's visible in this diff alone. The diff provided doesn't evidence a functional bug fix, particularly regarding error-handling or logical errors. The commit is more oriented towards improving or restructuring the code, as inferred from the interface mention in the commit message, rather than resolving a specific bug.

Therefore, based solely on this specific diff and without evidence of a direct bug being fixed, the appropriate conclusion is **NotBuggy**."
junit5,327.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,7 +1,7 @@
 	private DiscoverySelector selectMethod(List<Class<?>> classes, Method method) {
 		if (classes.size() == 1) {
 			return DiscoverySelectors.selectMethod(classes.get(0), method);
 		}
 		int lastIndex = classes.size() - 1;
-		return new NestedMethodSelector(classes.subList(0, lastIndex), classes.get(lastIndex), method);
+		return DiscoverySelectors.selectNestedMethod(classes.subList(0, lastIndex), classes.get(lastIndex), method);
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"The commit message describes the introduction of an API for selecting and executing individual tests in inherited nested classes, focusing on selectors in a testing framework. 

The code change in the diff shows a refactoring in the `selectMethod` method. Initially, it used a `NestedMethodSelector` directly to handle a list of classes and a method. The change updates it to use a new API method, `DiscoverySelectors.selectNestedMethod`, which implies an enhancement in handling nested methods, probably by leveraging existing platform mechanisms and providing a standardized way of selecting methods for test discovery.

The code update aligns with the commit message, as it focuses on improving how selectors for nested elements are implemented and used. However, this change doesn't indicate a bug fix; rather, it introduces enhancements and new features for handling nested selections in testing, which is consistent with the commit message's scope.

Therefore, since the change is not directly fixing a bug but instead adding and improving features, the conclusion is **NotBuggy**."
junit5,544.json,18fc3b8e2dc6be2be6b269aaa110df80430a9567,"@@ -1,9 +1,9 @@
-	public static <T> Predicate<T> excludeMatchingClasses(String pattern) {
+	public static <T> Predicate<T> excludeMatchingClasses(String patterns) {
 		// @formatter:off
-		return Optional.ofNullable(pattern)
+		return Optional.ofNullable(patterns)
 				.filter(StringUtils::isNotBlank)
 				.map(String::trim)
-				.map(ClassNamePatternFilterUtils::<T>createPredicateForNonBlankPattern)
+				.map(ClassNamePatternFilterUtils::<T>createPredicateFromPatterns)
 				.orElse(object -> true);
 		// @formatter:on
 	}",NotBuggy,"Polish contribution

See #2181
",NotBuggy,"The commit message ""Polish contribution"" suggests a non-functional refinement or cleanup of the code rather than a bug fix. 

Analyzing the diff:
1. The method name and parameter in the `excludeMatchingClasses` method are changed from `pattern` to `patterns`. This suggests a shift from a singular pattern to potentially handling multiple patterns.
2. The lambda method is also changed from `createPredicateForNonBlankPattern` to `createPredicateFromPatterns`. The new method name aligns with the plural form, indicating an adaptation to possibly handle multiple patterns or a more complex pattern logic.

These changes do not explicitly point to a bug fix but rather to a possible enhancement or a renaming for clarity or expanded functionality to handle multiple patterns. From the provided information, this does not indicate any error-handling updates, logical corrections, exception-handling improvements, or direct bug fixes.

Conclusion: The changes do not indicate a bug fix but rather a refinement or enhancement. Therefore, this commit is **NotBuggy**."
junit5,1084.json,75cde97e14e04d21907f9367ac25f4f88b20a9d9,"@@ -1,3 +1,3 @@
-	List<Class<?>> getEnclosingClasses() {
-		return enclosingClasses;
+	public List<Class<?>> getEnclosingClasses() {
+		return nestedClassSelector.getEnclosingClasses();
 	}",NotBuggy,"Introduce API to select and execute individual tests in inherited nested classes (#2045)

* Move nested elements selectors to JUnit Platform

* Add factory methods in DiscoverySelectors to build selectors for nested elements

* Add Java doc on new selectors

* Add dedicated methods to nested selectors in SelectorResolver

* Add unit tests for nested elements selectors

* Reuse ClassSelector in NestedClassSelector implementation

* Add selector tests

* Naming

* Remove star imports

* Update release notes

* Add Javadoc to public methods

* Add @API on new API methods in DiscoverySelectors

* Replace spaces by tabs

* Add getter for class/method names on new selectors

* Small refactor

* Add toString methods for selectors

* Add reference to classes used in nested selectors

* Forgot spotless

* Add getMethodParameterTypes() in NestedMethodSelector
",NotBuggy,"The commit message clearly indicates the introduction of a new API to select and execute individual tests in inherited nested classes. The code change provided in the diff is about making the method `getEnclosingClasses()` public and changing its implementation to delegate to `nestedClassSelector.getEnclosingClasses()`. 

The change does not indicate a bug fix but instead suggests an enhancement or feature addition related to the functionality of selecting nested classes, aligning with the commit message about API introduction.

There is no indication of correcting an error, exception handling, or any other kind of bug fix-specific changes in the provided diff. Therefore, the changes are in line with expanding the API as described in the commit message.

The conclusion is **NotBuggy**."
junit5,379.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isNotPrivate(Class<?> clazz) {
-		Preconditions.notNull(clazz, ""Class must not be null"");
 		return ReflectionUtils.isNotPrivate(clazz);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message states that precondition checks have been moved from `ModifierSupport` to `ReflectionUtils`. In the provided diff, a precondition check (`Preconditions.notNull(clazz, ""Class must not be null"")`) is removed from the `isNotPrivate` method in this file.

The removal of the precondition check here likely means it was indeed moved to a different location as described in the commit message. Without seeing the change in `ReflectionUtils`, we assume the precondition check is not just removed but relocated, as the message suggests relocating rather than eliminating the functionality altogether.

This commit does not demonstrate a bug fix in the sense of addressing erroneous behavior in existing code but rather suggests a refactoring or re-organization effort to tidy up the code structure by moving the precondition checks to another class. It aligns more with code maintenance, not fixing bugs.

Therefore, this change is categorized as **NotBuggy**."
junit5,707.json,dff526f3f0dcebb101f40446f5d5df207160f7a4,"@@ -1,14 +1,16 @@
-	private CsvParserSettings buildParserSettings() {
+	private static CsvParserSettings createParserSettings(String delimiter, String lineSeparator, char quote,
+			String emptyValue) {
+
 		CsvParserSettings settings = new CsvParserSettings();
 		settings.getFormat().setDelimiter(delimiter);
 		settings.getFormat().setLineSeparator(lineSeparator);
 		settings.getFormat().setQuote(quote);
 		settings.getFormat().setQuoteEscape(quote);
 		settings.setEmptyValue(emptyValue);
 		settings.setAutoConfigurationEnabled(false);
 		// Do not use the built-in support for skipping rows/lines since it will
 		// throw an IllegalArgumentException if the file does not contain at least
 		// the number of specified lines to skip.
 		// settings.setNumberOfRowsToSkip(annotation.numLinesToSkip());
 		return settings;
 	}",NotBuggy,"Polish contribution and Csv[File]Source internals

See: #1958 and #1972
",NotBuggy,"The commit message indicates a refinement or improvement ('Polish contribution and Csv[File]Source internals'), rather than a specific bug fix. The code diff involves:

1. Renaming the method `buildParserSettings` to `createParserSettings` and making it `static`.
2. Changing the method signature to accept parameters like `delimiter`, `lineSeparator`, `quote`, and `emptyValue`.

The function logic itself doesn't undergo any change that suggests a bug fix, but rather an enhancement or refactoring to make the method more flexible by accepting parameters instead of using hardcoded or previously set values.

Given these changes, there is no direct indication of a bug being fixed; instead, the code is being cleaned up or made more efficient in some respects. Thus, the evaluation of these changes relative to the commit message is:

**NotBuggy**"
junit5,389.json,26754bd9dcd12b1f84d1aae53104468e6a70a70d,"@@ -1,4 +1,3 @@
 	public static boolean isFinal(Member member) {
-		Preconditions.notNull(member, ""Member must not be null"");
 		return ReflectionUtils.isFinal(member);
 	}",NotBuggy,"Move precondition checks from ModifierSupport to ReflectionUtils
",NotBuggy,"The commit message states that precondition checks are being moved from one class to another. The diff shows the removal of a null check for a `Member` object. The removed line implies that `Preconditions.notNull(member, ""Member must not be null"");` ensures that the `member` is not null before proceeding to check if it is final.

No context of a corresponding addition in `ReflectionUtils` is shown, but assuming from the message, the null precondition is moved to `ReflectionUtils`. As such, this change is not indicative of a bug fix itself, but rather a refactor of where the precondition check is conducted.

The commit seems to focus on code organization and not directly on fixing a bug within the logical correctness or handling of the `member` object. Without additional context that `ReflectionUtils` now handles the null check, it is presumed to be intentional reorganization for clarity or maintainability.

Therefore, the conclusion is **NotBuggy** because it reorganizes code rather than corrects a logical or error-related issue."
junit5,2167.json,26a780900f56fc083602fa2fddb644640b772117,"@@ -1,3 +1,3 @@
 	public static void assertTimeout(Duration timeout, Executable executable) {
-		assertTimeout(timeout, executable, () -> null);
+		AssertTimeout.assertTimeout(timeout, executable);
 	}",NotBuggy,"Move implementation of Assertions into separate classes (#559)

",NotBuggy,"The commit message mentions moving the implementation of assertions into separate classes. The code diff shows a method call being updated to use a presumably new class (`AssertTimeout`) to perform the operation. This aligns with the intent to refactor the code by organizing it better, likely for improved maintainability and clarity, rather than fixing a specific bug.

The change doesn't specifically address a bug but rather refactors the code. The commit message and code changes are consistent with a refactoring effort rather than a bug fix. There are no indications of error-handling updates, logical corrections, exception-handling improvements, or other changes typically associated with bug fixes.

Therefore, the assessment is **NotBuggy**."
junit5,526.json,afb80b5612afb4e6c99e0b2730c69da1fc581548,"@@ -1,10 +1,10 @@
-	private String determineSubpackageName(Path rootDir, Path classFile) {
-		Path relativePath = rootDir.relativize(classFile.getParent());
-		String pathSeparator = rootDir.getFileSystem().getSeparator();
+	private String determineSubpackageName(Path baseDir, Path classFile) {
+		Path relativePath = baseDir.relativize(classFile.getParent());
+		String pathSeparator = baseDir.getFileSystem().getSeparator();
 		String subpackageName = relativePath.toString().replace(pathSeparator, PACKAGE_SEPARATOR_STRING);
 		if (subpackageName.endsWith(pathSeparator)) {
 			// Workaround for JDK bug: https://bugs.openjdk.java.net/browse/JDK-8153248
 			subpackageName = subpackageName.substring(0, subpackageName.length() - pathSeparator.length());
 		}
 		return subpackageName;
 	}",NotBuggy,"Polish ClasspathScanner
",NotBuggy,"The commit message suggests a minor improvement (""Polish"") to the `ClasspathScanner`, indicating perhaps code readability or style adjustments rather than bug fixes. 

Examining the diff:
- The method `determineSubpackageName` was refactored to use a parameter named `baseDir` instead of `rootDir`.
- This is a simple renaming refactor; it does not affect functionality or fix bugs.

The change aligns with the commit message in that it's a minor refinement. No evidence indicates a bug fix. Therefore, the conclusion is **NotBuggy**."
junit5,393.json,ba5e223c4bb1720e59664723f27d97c373c39f74,"@@ -1,4 +1,4 @@
-	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classTester,
+	public static List<Class<?>> findAllClassesInClasspathRoot(URI root, Predicate<Class<?>> classFilter,
 			Predicate<String> classNameFilter) {
-		return ReflectionUtils.findAllClassesInClasspathRoot(root, classTester, classNameFilter);
+		return ReflectionUtils.findAllClassesInClasspathRoot(root, classFilter, classNameFilter);
 	}",NotBuggy,"Polish JavaDoc for ReflectionSupport
",NotBuggy,"The commit message states that the change is about polishing JavaDoc, yet the code diff shows a method parameter name change from `classTester` to `classFilter`. This change appears cosmetic, intended for clarity or alignment of naming conventions, not indicative of a functional bug fix.

The change from `classTester` to `classFilter` enhances readability by using consistent terminology, possibly aligning with other parts of the codebase or documentation. However, it doesn't address any logical or functional errors in the code itself.

Given that the change aligns with the stated goal of documentation polish and does not fix any logic errors or improve error handling, this change does not relate to a bug fix.

The conclusion is **NotBuggy**."
junit5,1920.json,480a1b2c9b6e54a7016bc8a14ebdb7b7f98909d3,"@@ -1,3 +1,3 @@
 	public static void assertEquals(Short expected, short actual) {
-		assertEquals(expected, (Short) actual);
+		AssertEquals.assertEquals(expected, (Short) actual);
 	}",NotBuggy,"Add more overloaded variants of assertEquals-methods

#1710
",NotBuggy,"The commit message mentions adding overloaded variants of `assertEquals`, but the code change is simply modifying how an existing `assertEquals` method calls another. It updates the method call from `assertEquals` to `AssertEquals.assertEquals`, which suggests a namespace or class reference was corrected. There is no indication of additional overloads being introduced.

This change does not inherently indicate a bug fix—it seems more like a refactoring or perhaps a correction in how the method is being called relative to its intended namespace or class usage. However, it doesn't fix an error-handling issue, logical mistake, or exception-related problem.

Hence, the changes do not align with addressing a bug as per the given commit message. Therefore, the conclusion is **NotBuggy**."
Essentials,258.json,e3fa24375342bdf58494c26cc125acb7d621cc87,"@@ -1,4 +1,4 @@
 	public boolean teleport(Entity entity)
 	{
-		throw new UnsupportedOperationException(Util.i18n(""notSupportedYet""));
+		throw new UnsupportedOperationException(_(""notSupportedYet""));
 	}",NotBuggy,"Switch to the new I18n class and format cleanup of all classes
",NotBuggy,"The commit message discusses switching to a new I18n class and formatting cleanup. The code modifies a method that throws an UnsupportedOperationException, changing the localized message retrieval from `Util.i18n(""notSupportedYet"")` to `_('notSupportedYet')`. While this aligns with the commit message, it doesn't address bugs—it updates internationalization. No error-handling, logical corrections, or exception-handling improvements indicate a bug fix. Thus, it's **NotBuggy**."
Essentials,496.json,f5bf895a0ef64905ab4b3ca0fbdbfcf92d09ee21,"@@ -1,6 +1,10 @@
 	public void setLastLocation(Location loc)
 	{
+		if (loc == null || loc.getWorld() == null)
+		{
+			return;
+		}
 		lastLocation = loc;
 		config.setProperty(""lastlocation"", loc);
 		config.save();
 	}",NotBuggy,"Don't set last location if it's invalid
",Buggy,"The commit message indicates that the change is meant to prevent setting the last location if it's invalid. The code modification aligns perfectly with this message, as it introduces a check to ensure the `Location` object and its world are not null before assigning it to `lastLocation` and saving it in the configuration.

Here’s the step-by-step analysis:

1. **Null Check**: The code introduces a condition to check if `loc` (the `Location` object) is null.
2. **World Check**: It also checks if the `getWorld()` method of `loc` returns null. This would indicate an invalid location, as a `Location` should be in a specific world.
3. **Early Return**: If either condition is true, the method returns immediately without setting the `lastLocation` or saving it to the configuration. This prevents potential exceptions or logical errors that would arise from trying to use or save an invalid location.

These changes prevent the assignment and saving of an invalid location, effectively addressing what is likely a bug connected to invalid `Location` objects. Therefore, the commit both accurately describes the change and represents a legitimate bug fix. Thus, the overall conclusion is **Buggy**."
Essentials,431.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean setLeashHolder(Entity arg0)
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message discusses updating to a new version of Bukkit and disabling a fallback feature, but the diff only shows a minor change in a comment within an exception. The actual logic or functionality of the method hasn't changed, and no indications of a bug fix are present in this change. The same exception is thrown, and the comment change does not implement any new logic.

Therefore, there is no indication that this change aligns with a bug fix, or that it particularly relates to the main points highlighted in the commit message. 

The answer is **NotBuggy**."
Essentials,947.json,46476b36d99818dfeebc21120b5fda31b031e301,"@@ -1,7 +1,20 @@
 	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		final IText input = new TextInput(sender, ""custom"", true, ess);
 		final IText output = new KeywordReplacer(input, sender, ess);
 		final TextPager pager = new TextPager(output);
-		pager.showPage(commandLabel, args.length > 0 ? args[0] : null, null, sender);
+		String chapter = commandLabel;
+		String page;
+
+		if (commandLabel.equalsIgnoreCase(""customtext"") && args.length > 0 && !NumberUtil.isInt(commandLabel))
+		{
+			chapter = args[0];
+			page = args.length > 1 ? args[1] : null;
+		}
+		else
+		{
+			page = args.length > 0 ? args[0] : null;
+		}
+		
+		pager.showPage(chapter, page, null, sender);
 	}",NotBuggy,"Allow sending customtext chapters as a parameter instead of reading commandLabel
",Buggy,"The commit message indicates an update for passing custom text chapters as a parameter instead of depending on the commandLabel. Let's examine the changes:

1. **Original Code**: The original code calls `pager.showPage(commandLabel, args.length > 0 ? args[0] : null, null, sender);`. This approach uses `commandLabel` as the main chapter identifier and uses the first `args` element as a potential page number.

2. **Modified Code**:
   - The changes introduce a new logic to determine `chapter` and `page`.
   - If `commandLabel` is ""customtext"", and `args` has at least one element that isn't an integer, `chapter` is set to `args[0]`, and `page` may be set to `args[1]`.
   - If the above condition isn't met, `chapter` is defaulted to `commandLabel`, and `page` uses `args[0]` if available.

3. **Analysis**:
   - This update allows more flexibility, likely a response to a functional bug where `commandLabel` alone wasn't sufficient to pass chapter info.
   - The condition `!NumberUtil.isInt(commandLabel)` checks to ensure `commandLabel` isn't an integer misused as a page number, suggesting a refinement to prevent potential errors.

The changes provide a new feature to let clients specify `chapters` through `args`, resolving limitations in the previous implementation where `commandLabel` was overloaded. This indicates a bug correction related to input handling.

Therefore, these revisions align with the commit message about improving parameter passing, reflecting a bug fix in function signature usage. Consequently, the answer is **Buggy**."
Essentials,428.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public double getHealthScale()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The code change involves only the removal of a comment within an `UnsupportedOperationException`. This does not affect functionality, fix a bug, or implement any new behavior. The commit mentions updating Bukkit and disabling a vanilla fallback, but the code change does not address these aspects. It merely removes a part of a comment within an exception, which is often used during development to indicate unimplemented features or parts of the code yet to be completed.

The modification doesn't introduce any logical corrections, error-handling updates, or fixes related to system behavior.

Given the information:
1. The removal is purely cosmetic and doesn't affect the application's execution.
2. The commit message does not directly relate to the modified code, as the update doesn't align with bug fixes, performance improvements, or the comment about disabling vanilla fallback.

As a result, the change is primarily an editing cleanup, not a bug fix.

Therefore, the conclusion is **NotBuggy**."
Essentials,1851.json,7f65879cb6dc28084ecfff57b047941cc106ecf2,"@@ -1,4 +1,4 @@
 	public String getCurrencySymbol()
 	{
-		return config.getString(""currency-symbol"", ""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
+		return config.getString(""currency-symbol"", ""$"").concat(""$"").substring(0, 1).replaceAll(""[0-9]"", ""$"");
 	}",NotBuggy,"Catch null eco symbol.
",NotBuggy,"The commit message indicates the change is intended to handle a ""null eco symbol"". However, the modification does not seem to address null handling for `currency-symbol`.

- **Original Code**: Returns the first character of the `currency-symbol` or ""$"" if null. Then, it replaces any numeric characters with ""$"".
- **Modified Code**: Concatenates a `$` to the end of `currency-symbol`, before taking the substring and performing the replacement.

The change does not address situations where `config.getString(""currency-symbol"")` might return null; instead, it alters the logic by appending an extra `$`, which could be an oversight leading to unintended results but doesn't specifically target null safety.

Given this, the adjustment doesn't align with the commit message's intent to catch null values, as the method doesn't change how `null` values are dealt with, leaving potential null-pointer exceptions unresolved.

Therefore, based on the provided information, the change does not directly correspond to a bug fix as described in the commit message. The conclusion is **NotBuggy**."
Essentials,92.json,109c26fa8f0f015191409834b93e247d00706ae5,"@@ -1,13 +1,19 @@
 	public void onBlockBreak(final BlockBreakEvent event)
 	{
 		if (ess.getSettings().areSignsDisabled())
 		{
 			event.getHandlers().unregister(this);
 			return;
 		}
-
-		if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+		try
+		{
+			if (protectSignsAndBlocks(event.getBlock(), event.getPlayer()))
+			{
+				event.setCancelled(true);
+			}
+		}
+		catch (MaxMoneyException ex)
 		{
 			event.setCancelled(true);
 		}
 	}",NotBuggy,"[Breaking] Add exception when players have exceeded their account limit.
This might effect some plugins which hook Essentials for economy without using the API
",Buggy,"The commit message mentions adding an exception related to account limits, specifically stating that an exception should be thrown when players exceed their account limit. 

In the provided diff, there is a significant change:

1. A `try-catch` block is introduced around the `protectSignsAndBlocks` method call.
2. It handles the specific `MaxMoneyException`, which seems to be related to exceeding account limits.

This explicitly catches an exception when a player attempts to break a block under certain conditions (likely related to their economic status, which ties back to account limits), aligning with the commit message's focus on exception handling when limits are exceeded. The handling of this exception by setting the event as cancelled suggests the change does indeed introduce new behavior geared towards managing an unintended state, i.e., players exceeding their limits.

Given the context and the change, the diff is addressing a logical oversight or potential bug related to account limits and exceptions. Therefore, the appropriate conclusion is **Buggy**."
Essentials,625.json,406295a5488f8b105325b7c8c918f257f484d911,"@@ -1,9 +1,9 @@
 	public void setProperty(final String path, final Location loc)
 	{
-		setProperty((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
-		setProperty((path == null ? """" : path + ""."") + ""x"", loc.getX());
-		setProperty((path == null ? """" : path + ""."") + ""y"", loc.getY());
-		setProperty((path == null ? """" : path + ""."") + ""z"", loc.getZ());
-		setProperty((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
-		setProperty((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
+		set((path == null ? """" : path + ""."") + ""world"", loc.getWorld().getName());
+		set((path == null ? """" : path + ""."") + ""x"", loc.getX());
+		set((path == null ? """" : path + ""."") + ""y"", loc.getY());
+		set((path == null ? """" : path + ""."") + ""z"", loc.getZ());
+		set((path == null ? """" : path + ""."") + ""yaw"", loc.getYaw());
+		set((path == null ? """" : path + ""."") + ""pitch"", loc.getPitch());
 	}",NotBuggy,"Updated Essentials to work with R5
",NotBuggy,"The commit message indicates an update to ensure compatibility with R5, implying no specific bug fix but rather an adaptation or upgrade for compatibility. The code changes involve replacing calls to `setProperty` with `set`. Without additional context, it does not clearly show a bug fix unless `set` method is part of an updated API or framework in R5 that performs better or fixes known issues related to `setProperty`. Without evidence that these changes address an issue—just that they've adapted for compatibility—the conclusion remains **NotBuggy**."
Essentials,1533.json,51bd0d88f393dbd675d2227e84d24fe9d6313712,"@@ -1,9 +1,9 @@
 	public static void spawnmob(final IEssentials ess, final Server server, final User user, final List<String> parts, final List<String> data, int mobCount) throws Exception
 	{
 		final Block block = LocationUtil.getTarget(user.getBase()).getBlock();
 		if (block == null)
 		{
-			throw new Exception(_(""unableToSpawnMob""));
+			throw new Exception(tl(""unableToSpawnMob""));
 		}
 		spawnmob(ess, server, user.getSource(), user, block.getLocation(), parts, data, mobCount);
 	}",NotBuggy,"Future Java versions may not support _ as identifier.
",NotBuggy,"The commit message refers to a change regarding the use of underscores (_) as identifiers in anticipation of future Java versions potentially disallowing this. In the code diff, we see a replacement of _ with tl. This change is consistent with the commit message, which implies a preventive update to avoid future issues related to using underscores as identifiers. However, this does not indicate a bug fix in the current functionality; it is a modification to future-proof the code. As there is no indication of correcting a logical or runtime error, the changes do not reflect a bug fix.

The conclusion is **NotBuggy**."
Essentials,429.json,b637d8ac2dd1c17eb2b468f62eb1f72ba892cc10,"@@ -1,4 +1,4 @@
 	public boolean isLeashed()
 	{
-		throw new UnsupportedOperationException(""Not supported yet.""); //To change body of generated methods, choose Tools | Templates.
+		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Update Bukkit to 1.7.3-R0.3 - Bukkit: 1903 CB: 3020
This also disables vanilla fallback, use new aliases file.
",NotBuggy,"The commit message discusses updating an application (Bukkit) and disabling a feature. However, the code change removes a comment without altering logic or behavior. This neither fixes a bug nor reflects the commit message. The answer is **NotBuggy**."
Essentials,2380.json,0f83fb9dceb0edac24600b4838166c09fa002c8a,"@@ -1,27 +1,27 @@
-	protected void run(Server server, CommandSender sender, String commandLabel, String[] args) throws Exception
+	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws NotEnoughArgumentsException
 	{
 		if (args.length < 1)
 		{
 			throw new NotEnoughArgumentsException();
 		}
 
 		final List<Player> matches = server.matchPlayer(args[0]);
 
 		if (matches.isEmpty())
 		{
 			sender.sendMessage(""ￂﾧcThere are no players matching that name."");
 		}
 
 		for (Player p : matches)
 		{
 			try
 			{
 				final boolean toggle = EssentialsXMPP.getInstance().toggleSpy(p);
 				sender.sendMessage(""XMPP Spy "" + (toggle ? ""enabled"" : ""disabled"") + "" for "" + p.getDisplayName());
 			}
 			catch (Exception ex)
 			{
 				sender.sendMessage(""Error: "" + ex.getMessage());
 			}
 		}
 	}",NotBuggy,"More cleanup

git-svn-id: https://svn.java.net/svn/essentials~svn/trunk@1570 e251c2fe-e539-e718-e476-b85c1f46cddb
",NotBuggy,"The commit message ""More cleanup"" is non-specific and does not indicate a bug fix. The code change consists of two main updates:

1. Added `final` to method parameters, which is generally a code style improvement to indicate that the parameters should not be reassigned within the method.

2. Changed the exception type thrown when the argument length is less than one, from a generic `Exception` to a specific `NotEnoughArgumentsException`. This enhancement improves error specificity and clarity but doesn't necessarily indicate a fix for incorrect behavior. 

The message implies general code maintenance, aligning with cleaner code practices rather than resolving a bug. No specific logical or functional correction typical of bug fixes is evident, like addressing a malfunction, misbehavior, or incorrect calculation.

Thus, considering the nature of the changes and the commit message ""More cleanup"" doesn't imply a bug fix was intended. The conclusion is **NotBuggy**."
Essentials,2816.json,55b083ddbf75a7e21d9bb55fafcd9fd88afa64c8,"@@ -1,3 +1,3 @@
-	public void onPlayerChat(final PlayerChatEvent event)
+	public void onPlayerChat(final AsyncPlayerChatEvent event)
 	{
 	}",NotBuggy,"CB #2289 Bukkit #1512

Change PlayerChatEvent to AsyncPlayerChatEvent
",Buggy,"The commit message indicates a change from `PlayerChatEvent` to `AsyncPlayerChatEvent`, which is reflected in the code diff. This modification deals with event handling within a Minecraft server plugin, where asynchronous operations can prevent server lag during chat event processing.

Analyzing this change:
1. **Event Type Change**: Switching from `PlayerChatEvent` to `AsyncPlayerChatEvent` indicates the original synchronous event handling could cause performance issues or thread blocking as chat events may involve time-consuming operations.
2. **Intent Alignment**: The commit message precisely describes the alteration made in the code, indicating clear intent to improve performance or bug fix by migrating to an async process.
3. **Potential Bug**: Although not explicitly fixing a bug, this change can be considered a bug fix for inefficiency or race condition issues that may arise from handling chat events synchronously.

Given the context, switching to an asynchronous event handling mechanism can prevent potential performance or concurrency bugs often seen in server environments. Thus, the answer is **Buggy**."
Essentials,1707.json,444ec11dc173b51c13859526718cd1e0935d3631,"@@ -1,4 +1,4 @@
 	public boolean canBuild(final Player base, final String group)
 	{
-		return hasPermission(base, ""essentials.build"");
+		return false;
 	}",NotBuggy,"Split Ess protect into EssProtect and EssAntiBuild
New permissions:
essentials.build - same as build: true
essentials.build.interact.<id> - whitelist single interact block
essentials.build.place.<id> - whitelist single place block
essentials.build.break.<id> - whitelist single break block
essentials.build.use.<id> - whitelist single use item
",NotBuggy,"The commit message indicates a split of functionality into two separate components, EssProtect and EssAntiBuild, with the introduction of new permissions. The code change modifies the `canBuild` method from returning a permission check to always returning `false`, which suggests the intention to completely restrict building until conditions are defined elsewhere with the new permissions structure.

This change is not about fixing a bug. Instead, it is about restructuring and introducing new permission handling related to building and interaction. The commit message aligns with the concept of permissions and functionality split but doesn't mention any corrections to erroneous behavior.

Therefore, the code modifications are part of a feature update or refactoring in line with the described split of functionality and not a bug fix. This results in the conclusion of **NotBuggy**."
Essentials,316.json,15c088e6dee3b470b7de80c95f7ec639613593e8,"@@ -1,4 +1,3 @@
 	public void recalculatePermissions()
 	{
-		throw new UnsupportedOperationException(""Not supported yet."");
 	}",NotBuggy,"Bukkit Permission system
New config setting: use-bukkit-permissions
",NotBuggy,"The commit message mentions introducing a new configuration setting related to a ""Bukkit Permission system,"" yet the code diff solely removes an `UnsupportedOperationException` without adding any new behavior or configuration. The absence of new logic or settings related to ""use-bukkit-permissions"" suggests that the commit message and the code diff do not align. Given the focus on removing an exception without addressing any logical correction or configuration update, there is no direct indication of a bug fix related to the commit message context.

Reasoning:

1. **Commit Message Analysis**: Suggests introduction of a new configuration for permissions.
2. **Code Change Analysis**: Removes the throw of an `UnsupportedOperationException` in a function, which implies that the functionality is now expected to be either supported or to be implemented soon.

While the removal of the exception may imply preparation for future implementation, there's no immediate implementation or direct correction of incorrect behavior present in this commit.

Conclusion: **NotBuggy**"
Essentials,814.json,ff46b39f2b42abbe4ce8dc39e564f3dc71a2a104,"@@ -1,4 +1,4 @@
-	protected void run(final Server server, final CommandSender sender, final String commandLabel, final String[] args) throws Exception
+	protected void run(final Server server, final CommandSource sender, final String commandLabel, final String[] args) throws Exception
 	{
 		sender.sendMessage(_(""helpConsole""));
 	}",NotBuggy,"Extract CommandSender to CommandSource, this should prevent Ess user object leaks.
",Buggy,"The commit message discusses extracting `CommandSender` to `CommandSource` to prevent ""Ess user object leaks"". The diff shows a type change of parameter `sender` from `CommandSender` to `CommandSource` in the `run` method.

**Analysis**:
- **Commit Message**: The message indicates a change aimed at preventing leaks of user-related objects, suggesting a bug fix related to resource management or memory handling.
- **Code Changes**: The change modifies the parameter type, presumably to a more appropriate or safer implementation (`CommandSource` instead of `CommandSender`). If `CommandSource` is better equipped to handle object life cycles or has tighter controls that prevent leaks, then it could resolve the issue.

The change is aligned with the intent to address a bug because it replaces a component that may have been responsible for causing unintended object persistence (leaks). The context implies a transition to `CommandSource` to handle objects more effectively, likely adhering to expected lifetimes or encapsulation requirements, which is commonly a bug-related concern.

Concluding, based on the provided commit message and associated code modification details, this adjustment is aimed at fixing a resource management problem, indicating a bug fix.

**Buggy**"
commons-io,729.json,55bae88d398feed4a3008ec5e97eb8b85c983b1e,"@@ -1,3 +1,3 @@
     public static String toString(URI uri, Charset encoding) throws IOException {
-        return toString(uri.toURL(), toCharset(encoding));
+        return toString(uri.toURL(), Charsets.toCharset(encoding));
     }",NotBuggy,"[IO-318] Add Charset sister APIs to method that take a String charset name. FileUtils.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1307412 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of Charset-compatible APIs. The code change updates a method call to use `Charsets.toCharset(encoding)`. This isn't a bug fix, but rather an API improvement for better Charset handling. The answer is **NotBuggy**."
commons-io,677.json,6aa00766b9103d75c763160280429af8e1f0549e,"@@ -1,6 +1,6 @@
-    public static void readFully(InputStream input, byte[] buffer, int offset, int length) throws IOException {
-        int actual = read(input, buffer, offset, length);
+    public static void readFully(final InputStream input, final byte[] buffer, final int offset, final int length) throws IOException {
+        final int actual = read(input, buffer, offset, length);
         if (actual != length) {
             throw new EOFException(""Length to read: "" + length + "" actual: "" + actual);
         }
     }",NotBuggy,"Use final where possible.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1415850 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message ""Use final where possible"" directly aligns with the changes in the code diff. The modifications involve adding the `final` keyword to the method parameters and local variables. This change is related to code style and immutability rather than fixing any bug or logic error in the code. There is no indication of error-handling updates, logical corrections, or exception-handling improvements that would suggest a bug fix.

Therefore, based on the provided diff and commit message, the conclusion is **NotBuggy**."
commons-io,545.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(ClassNameMatcher m) {
+    public ValidatingObjectInputStream accept(final ClassNameMatcher m) {
         acceptMatchers.add(m);
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message states that the change involves using `final` for parameters. The code diff shows the addition of the `final` keyword to the parameter `m` in the method `accept`. This change is primarily for improving code readability and preventing modification of the parameter within the method, aligning with the commit message.

There is no indication that this change is intended to fix a bug. It appears to be solely for code quality improvement by making parameters immutable within the method context.

Therefore, based on the analysis of the commit message and the changes made, the conclusion is **NotBuggy**."
commons-io,1030.json,7c111fecd837f22f1f8c1faa38dbef0f4a0e91ee,"@@ -1,5 +1,5 @@
-    public static <T extends FileVisitor<? super Path>> T visitFileTree(final Path directory, final T visitor)
+    public static <T extends FileVisitor<? super Path>> T visitFileTree(final T visitor, final Path directory)
             throws IOException {
         Files.walkFileTree(directory, visitor);
         return visitor;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor for counting longs and big integers. Add cleaning visitor.
",NotBuggy,"This change refactors the method signature by swapping the order of the parameters. The method functionality remains the same. There is no indication of a bug fix—this is a straightforward refactoring for method signature consistency or readability. The commit message does not indicate a bug fix that corresponds with this specific change. Therefore, the conclusion is **NotBuggy**."
commons-io,763.json,b7ec4f7e855ecb77c8bf42fdd97f123d4588edd2,"@@ -1,3 +1,3 @@
-    public <T extends Throwable> List<T> getCauseList(Class<T> clazz) {
+    public <T extends Throwable> List<T> getCauseList(final Class<T> clazz) {
         return (List<T>) causeList;
     }",NotBuggy,Use final.,NotBuggy,"The commit message ""Use final"" reflects the change made in the code. Inserting the `final` keyword for a method parameter (`clazz` in this case) prevents reassignment within the method but doesn't alter functionality or fix bugs. Therefore, this update does not imply a bug fix. Consequently, the appropriate classification for this change is **NotBuggy**."
commons-io,1152.json,8db20da2ea53b8598e92fc0672482a74acb06845,"@@ -1,11 +1,7 @@
-    private static void validateMoveParameters(final File src, final File dest) throws FileNotFoundException {
-        if (src == null) {
-            throw new NullPointerException(""Source must not be null"");
-        }
-        if (dest == null) {
-            throw new NullPointerException(""Destination must not be null"");
-        }
-        if (!src.exists()) {
-            throw new FileNotFoundException(""Source '"" + src + ""' does not exist"");
+    private static void validateMoveParameters(final File source, final File destination) throws FileNotFoundException {
+        Objects.requireNonNull(source, ""source"");
+        Objects.requireNonNull(destination, ""destination"");
+        if (!source.exists()) {
+            throw new FileNotFoundException(""Source '"" + source + ""' does not exist"");
         }
     }",NotBuggy,"Use Objects.requireNonNull() instead of custom check.
",NotBuggy,"The commit message states that `Objects.requireNonNull()` replaces a custom null check. The diff aligns with this: it replaces explicit null checks with `Objects.requireNonNull()`, which streamlines the code but maintains functionality. This is not indicative of a bug fix, but rather a code improvement for cleaner and more standardized null checking. There are no changes that fix incorrect logic or unexpected behavior in this diff.

Therefore, the conclusion is **NotBuggy**."
commons-io,543.json,14b52eb4f630f32c66122977331bd894b9474978,"@@ -1,4 +1,4 @@
-    public ValidatingObjectInputStream accept(Pattern pattern) {
+    public ValidatingObjectInputStream accept(final Pattern pattern) {
         acceptMatchers.add(new RegexpClassNameMatcher(pattern));
         return this;
     }",NotBuggy,Use final for parameters.,NotBuggy,"The commit message indicates that the change was to make method parameters `final`. The code diff aligns with this message by marking the `pattern` parameter as `final`. This change is typically for code safety, to prevent reassignment of the parameter within the method, but it does not relate to any bug fix.

The commit does not address logical errors, exception handling, or error handling improvements—common indicators of bug fixes. The introduction of `final` is mainly for design and maintenance purposes, making the intent of the method clear that `pattern` will not change.

Therefore, the change matches the commit message and does not signify a bug being fixed. Hence, the judgment is **NotBuggy**."
commons-io,336.json,46ab2532070e129ac54508bfb7a5be33a6b797bc,"@@ -1,12 +1,12 @@
     public int read() throws IOException {
         int c = EOF;
         while (reader != null) {
             c = reader.read();
             if (c == EOF) {
-                reader = readers.hasNext() ? readers.next() : null;
+                reader = nextReader();
             } else {
                 break;
             }
         }
         return c;
     }",NotBuggy,"[IO-458] Add a SequenceReader similar to java.io.SequenceInputStream.

Refactor commmon code in a private method.",NotBuggy,"This commit renames code with a private method `nextReader()`, replacing `readers.hasNext() ? readers.next() : null`. These changes enhance readability but don't address bugs. Variety indicates refactoring for improved maintainability. The answer is **NotBuggy**."
commons-io,475.json,1170279dc85c2bbdea594b053c56b379a72a2104,"@@ -1,53 +1,53 @@
         private String readLine() throws IOException {
 
             String line = null;
             int newLineMatchByteCount;
 
-            boolean isLastFilePart = (no == 1);
+            boolean isLastFilePart = no == 1;
 
             int i = currentLastBytePos;
             while (i > -1) {
 
                 if (!isLastFilePart && i < avoidNewlineSplitBufferSize) {
                     // avoidNewlineSplitBuffer: for all except the last file part we
                     // take a few bytes to the next file part to avoid splitting of newlines
                     createLeftOver();
                     break; // skip last few bytes and leave it to the next file part
                 }
 
                 // --- check for newline ---
                 if ((newLineMatchByteCount = getNewLineMatchByteCount(data, i)) > 0 /* found newline */) {
                     final int lineStart = i + 1;
                     int lineLengthBytes = currentLastBytePos - lineStart + 1;
 
                     if (lineLengthBytes < 0) {
                         throw new IllegalStateException(""Unexpected negative line length=""+lineLengthBytes);
                     }
                     byte[] lineData = new byte[lineLengthBytes];
                     System.arraycopy(data, lineStart, lineData, 0, lineLengthBytes);
 
                     line = new String(lineData, encoding);
 
                     currentLastBytePos = i - newLineMatchByteCount;
                     break; // found line
                 }
 
                 // --- move cursor ---
                 i -= byteDecrement;
 
                 // --- end of file part handling ---
                 if (i < 0) {
                     createLeftOver();
                     break; // end of file part
                 }
             }
 
             // --- last file part handling ---
             if (isLastFilePart && leftOver != null) {
                 // there will be no line break anymore, this is the first line of the file
                 line = new String(leftOver, encoding);
                 leftOver = null;
             }
 
             return line;
         }",NotBuggy,"Remove noisy and unnecessary parens. Especially painful in return statements: ""return (foo + bar);"" vs. ""return foo + bar;""

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1302748 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states the intention to ""Remove noisy and unnecessary parens,"" focusing on improving code readability by eliminating redundant parentheses, particularly in return statements. 

Analyzing the provided diff:

- The change made is in the assignment of the `isLastFilePart` variable. Originally, it included parentheses: `(no == 1)`. After the change, it became `no == 1`. 
- The use of parentheses in the original version was indeed redundant because the equality check `no == 1` naturally evaluates to a boolean value that can be directly assigned to `isLastFilePart`.
- The modification aligns with the commit message's objective to simplify expressions by removing unnecessarily verbose syntax.
  
The code changes focus solely on cosmetic refactoring to improve code readability and do not impact the functionality or the logic leading to a bug fix.

Thus, the changes do not constitute a bug fix, and the conclusion would be **NotBuggy**."
commons-io,1046.json,154cb7648ffde863b2548eba10b330c51a103c6a,"@@ -1,4 +1,4 @@
-    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attrs) throws IOException {
-        super.preVisitDirectory(dir, attrs);
+    public FileVisitResult preVisitDirectory(final Path dir, final BasicFileAttributes attributes) throws IOException {
+        super.preVisitDirectory(dir, attributes);
         return accept(dir) ? FileVisitResult.CONTINUE : FileVisitResult.SKIP_SUBTREE;
     }",NotBuggy,"[IO-632] Add PathUtils for operations on NIO Path.

Refactor to add PathUtils.copyDirectory(Path, Path, CopyOption...)
",NotBuggy,"The provided diff involves a minor refactor where the parameter name `attrs` is changed to `attributes` in the method `preVisitDirectory`. It appears to be a cosmetic change for readability or consistency. There are no changes in functionality or logic, nor are there adjustments related to bug fixes such as error handling or exception management.

The commit message indicates the addition of a utility for operations on `NIO Path`, specifically `PathUtils.copyDirectory`, suggesting significant larger-scale refactoring or functional additions not visible in this diff. However, the diff itself does not pertain to any functional addition, bug fix, or any logic change associated with such ""PathUtils"" operations, or a specific copyDirectory implementation.

Therefore, based on the available diff and commit message, there is no indication of a bug being fixed in the provided change excerpt. This is essentially a non-functional refactor concerning parameter naming.

The conclusion is **NotBuggy**."
commons-io,627.json,3e947380d02dbd6d8f4e7590e3282498a16d6d32,"@@ -1,3 +1,3 @@
-    public static BufferedWriter asBufferedWriter(final Writer writer) {
+    public static BufferedWriter buffer(final Writer writer) {
         return writer instanceof BufferedWriter ? (BufferedWriter) writer : new BufferedWriter(writer);
     }",NotBuggy,"IO-233 IO-330 Add Methods for Buffering Streams/Writers To IOUtils

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/io/trunk@1483859 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that methods for buffering streams/writers are being added to IOUtils, but this diff only renames a method from `asBufferedWriter` to `buffer`. The functionality remains the same, suggesting this is a refactor for clarity or consistency rather than a bug fix. There are no corrections to logic or error handling that indicate a bug was addressed.

Thus, the code change does not align with the commit message's implications of a bug fix; it aligns more with an API improvement or refactoring for future enhancements as part of adding new buffering methods.

Based on this, the changes are classified as **NotBuggy**."
commons-io,353.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,17 +1,17 @@
     public int read(final byte[] pBuffer, final int pOffset, final int pLength) throws IOException {
         int result = 0;
         IOException ioe = null;
         try {
             result = super.read(pBuffer, pOffset, pLength);
-        } catch (IOException pException) {
+        } catch (final IOException pException) {
             ioe = pException;
         }
         if (ioe != null) {
             noteError(ioe);
         } else if (result == -1) {
             noteFinished();
         } else if (result > 0) {
             noteDataBytes(pBuffer, pOffset, result);
         }
         return result;
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message states that the change is aimed at consistently using the `final` modifier with local variables. This diff reflects a change that adds `final` to the `IOException` parameter in the catch block. 

- The use of `final` in this context does not change the logic of the program nor does it fix any bugs. It simply ensures that the variable `pException` cannot be reassigned after it's caught, which is a style preference and a good practice for safety but does not indicate any bug fix.

Given the commit message and the diff, the modification aligns well with the intention of the commit, which is to apply consistent use of the `final` modifier. However, this change does not relate to fixing a bug.

Therefore, the conclusion is **NotBuggy**."
commons-io,754.json,56f2a758983611adea7308e90ae9934c973efa72,"@@ -1,12 +1,12 @@
     public static void writeChunked(final char[] data, final Writer output) throws IOException {
         if (data != null) {
             int bytes = data.length;
             int offset = 0;
             while (bytes > 0) {
-                int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
+                final int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
                 output.write(data, offset, chunk);
                 bytes -= chunk;
                 offset += chunk;
             }
         }
     }",NotBuggy,Consistently use the final modifier with local variables.,NotBuggy,"The commit message indicates a stylistic change: adding `final` to local variables. The code modification aligns with this by declaring the `chunk` variable as `final`, signifying it won't be reassigned. There's no bug fix here, just an enforcement of constant immutability for the variable. The answer is **NotBuggy**."
commons-io,1022.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,4 +1,4 @@
     public static boolean directoryContentEquals(final Path path1, final Path path2, final int maxDepth,
-            LinkOption[] linkOptions, FileVisitOption... fileVisitOptions) throws IOException {
+            LinkOption[] linkOptions, FileVisitOption[] fileVisitOptions) throws IOException {
         return new RelativeSortedPaths(path1, path2, maxDepth, linkOptions, fileVisitOptions).equals;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",NotBuggy,"In this diff, the change is modifying the method's signature from using varargs (`FileVisitOption...`) to using an array (`FileVisitOption[]`). This alteration doesn't indicate a bug fix; rather, it aligns with static typing and prevents issues with varargs that can sometimes cause unintended behavior but isn't a direct bug fix. The commit message indicates the implementation of new functionality for directory content equality and doesn't mention fixing a bug directly. Therefore, this change doesn't directly reflect a bug fix but more of a refinement or improvement based on feedback.

Given the commit message's focus on implementing new functionality and integrating feedback, combined with the broader improvement context rather than addressing a specific problem or incorrect behavior in the code, the change should be categorized as **NotBuggy**."
commons-io,1020.json,11f0abe7a3fb6954b2985ca4ab0697b2fb489e84,"@@ -1,34 +1,34 @@
     public static boolean directoryAndFileContentEquals(final Path path1, final Path path2,
-            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption... fileVisitOption)
+            final LinkOption[] linkOptions, final OpenOption[] openOptions, final FileVisitOption[] fileVisitOption)
             throws IOException {
         // First walk both file trees and gather normalized paths.
         if (path1 == null && path2 == null) {
             return true;
         }
         if (path1 == null ^ path2 == null) {
             return false;
         }
         if (!Files.exists(path1) && !Files.exists(path2)) {
             return true;
         }
         final RelativeSortedPaths relativeSortedPaths = new RelativeSortedPaths(path1, path2, Integer.MAX_VALUE,
                 linkOptions, fileVisitOption);
         // If the normalized path names and counts are not the same, no need to compare contents.
         if (!relativeSortedPaths.equals) {
             return false;
         }
         // Both visitors contain the same normalized paths, we can compare file contents.
         final List<Path> fileList1 = relativeSortedPaths.relativeFileList1;
         final List<Path> fileList2 = relativeSortedPaths.relativeFileList2;
         for (Path path : fileList1) {
             final int binarySearch = Collections.binarySearch(fileList2, path);
             if (binarySearch > -1) {
                 if (!fileContentEquals(path1.resolve(path), path2.resolve(path), linkOptions, openOptions)) {
                     return false;
                 }
             } else {
                 throw new IllegalStateException(String.format(""Unexpected mismatch.""));
             }
         }
         return true;
     }",NotBuggy,"[IO-648] Implement directory content equality.

Integrate feedback from Michael Osipov.",NotBuggy,"The diff modifies the directoryAndFileContentEquals method's signature by changing the last parameter from an indeterminately sized FileVisitOption varargs parameter to a fixed-size FileVisitOption array. This adjustment reflects better parameter management, likely based on feedback to improve code stability and readability.

However, this change does not directly indicate a bug fix related to the functionality of directory content equality. There are no logical corrections, error-handling improvements, or consistent data validation adjustments that typically suggest resolving a bug. The refactor seems minor and more about code quality and adherence to a specific feedback protocol, possibly for standardization or maintainability purposes.

Given the commit message, this change does not address a bug related to directory and file content equality. The modification doesn't enhance or correct any inherently flawed logic within the method itself. Therefore, the commit is primarily about integrating feedback rather than fixing an existing bug in the functionality.

Thus, the conclusion is **NotBuggy**."
wicket,134.json,2e1812d71cd5be231e57f82f7132f62295ef07c0,"@@ -1,4 +1,14 @@
 		public void detach(final IRequestCycle requestCycle)
 		{
 			delegate.detach(requestCycle);
+
+			if (logData == null)
+			{
+				ILogData delegateData;
+				if (delegate instanceof ILoggableRequestHandler)
+					delegateData = ((ILoggableRequestHandler)delegate).getLogData();
+				else
+					delegateData = new NoLogData();
+				logData = new DelegateLogData(delegateData);
+			}
 		}",NotBuggy,"WICKET-4074 RequestLogger needs a clear separation of concerns, courtesy of Emond Papegaaij


git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1183064 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a focus on improving the separation of concerns within the RequestLogger. The diff adds logic in the `detach` method to handle `logData` initialization. It checks if `logData` is null and, based on the type of `delegate`, assigns appropriate log data using either the `getLogData` method or `new NoLogData()` as a fallback. 

This appears to be a structural or design improvement to better encapsulate the logging behavior within the RequestLogger, rather than a bug fix. The changes ensure that `logData` is always initialized correctly, separating the concerns of handling logging data from the main logic, aligning with the commit message goal of separation of concerns.

Given the context, while this change is a design improvement and doesn't correct a previous error or unintended behavior, it doesn't address a specific bug as detailed in a bug tracking system or rectify erroneous behavior.

Therefore, the changes more accurately represent a refactoring effort and design improvement rather than a strict bug fix, leading to the conclusion of **NotBuggy**."
wicket,5717.json,83f3fa8231c877705edf3e7250a11fe18252ac1e,"@@ -1,5 +1,5 @@
-	public NumberTextField setMaximum(final Double maximum)
+	public NumberTextField<N> setMaximum(final N maximum)
 	{
 		this.maximum = maximum;
 		return this;
 	}",NotBuggy,"WICKET-3241 Add support for the new HTML 5 input types

Relax the type of (Number|Range)TextField.
Now it can be any subtype of java.lang.Number

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1053609 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an enhancement to support HTML 5 input types by relaxing the type constraints on NumberTextField. The code change reflects this adjustment by changing the method signature from accepting only Double to accepting any subtype of Number. This change aligns with the commit message's goal to enhance functionality rather than fix a bug. The modification is focused on broadening compatibility with more number types for HTML 5 input fields rather than correcting a defect.

Therefore, the conclusion is **NotBuggy**."
wicket,8450.json,2ec1b5524431393274d459e2da2649dcf4446233,"@@ -1,4 +1,4 @@
 	public String getCharacterEncoding()
 	{
-		return null;
+		return characterEncoding;
 	}",NotBuggy,"WICKET-6063 Add support for WebSocketRequest#getUrl() and other properties which are available in the handshake request
",NotBuggy,"The commit message indicates the addition of WebSocket request support and properties available in handshake requests. However, the code change shown in the diff addresses character encoding retrieval by returning a previously uninitialized variable, `characterEncoding`, instead of `null`.

The change implies an improvement in functionality, potentially aligning with better WebSocket request handling, as character encoding is a property typically relevant in request contexts. However, the commit message does not explicitly mention encoding or related changes, focusing instead on WebSocket-specific properties like URL.

The modification could be part of the overall improvements intended by the commit message, especially if character encoding handling is crucial for handshake requests. Yet it's not specifically a bug fix, as there's no indication that the previous `null` return caused a malfunction in the context of WebSocket requests.

Without clear evidence in the commit message indicating this change is a bug fix, this modification appears more related to an enhancement or feature implementation.

In conclusion, the observed change is **NotBuggy** with respect to the commit message's specific claims about bug fixing."
wicket,11326.json,d2be7fd368cdca02dd7481490de587bcc1a6e4cd,"@@ -1,13 +1,15 @@
-	protected OrderByLink newOrderByLink(String id, String property, ISortStateLocator stateLocator)
+	protected OrderByLink newOrderByLink(final String id, final String property,
+		final ISortStateLocator stateLocator)
 	{
-		return new OrderByLink(id, property, stateLocator, OrderByLink.VoidCssProvider.getInstance())
+		return new OrderByLink(id, property, stateLocator,
+			OrderByLink.VoidCssProvider.getInstance())
 		{
 			private static final long serialVersionUID = 1L;
 
 			@Override
 			protected void onSortChanged()
 			{
 				OrderByBorder.this.onSortChanged();
 			}
 		};
 	}",NotBuggy,"let eclipse do a final cleanup. No functional change

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1062370 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that there should be no functional change. The diff shows a change in formatting and the addition of `final` keywords as a coding style and readability improvement, without changing the function's behavior or logic. These changes align with the message and don't suggest a bug fix. The answer is **NotBuggy**."
wicket,2324.json,d1710298c7e371f260299f732c58d0bf4d647161,"@@ -1,9 +1,9 @@
 	public boolean getStatelessHint(Component component)
 	{
-		if (this instanceof IBehaviorListener)
+		if (this instanceof IRequestListener)
 		{
 			// this behavior implements a callback interface, so it cannot be stateless
 			return false;
 		}
 		return true;
 	}",NotBuggy,"WICKET-6137 remove RequestListenerInterface
",NotBuggy,"The commit mentions the removal of `RequestListenerInterface`, and the change in the diff indicates a replacement of the condition `this instanceof IBehaviorListener` with `this instanceof IRequestListener`. This suggests an update reflecting the probable removal of the `IBehaviorListener` interface or a correct alignment to the intended logic as per the commit context.

The code modification does not reflect a bug fix but an adaptation to changed code structure or hierarchy, aligning behavior classes with the removal/refactoring of interfaces. Therefore, it is a structural change closely tied to code organization rather than fixing an error.

The given changes do not reflect direct signs of a bug fix (such as logic errors, exception handling improvements, etc.) but rather reflect structural or refactoring activity to align with codebase updates related to interface changes.

Thus, this change is **NotBuggy**."
wicket,1688.json,9220f60d68e223ff1bf5d0dd5cd0f3112a1a2396,"@@ -1,4 +1,21 @@
 	public IResourceCachingStrategy getCachingStrategy()
 	{
+		if (resourceCachingStrategy == null)
+		{
+			final IResourceVersion resourceVersion;
+
+			if (application.usesDevelopmentConfig())
+			{
+				// use file last modified for resource cache keys
+				resourceVersion = new RequestCycleCachedResourceVersion(new LastModifiedResourceVersion());
+			}
+			else
+			{
+				// use md5 message digest for resource cache keys
+				resourceVersion = new CachingResourceVersion(new MessageDigestResourceVersion());
+			}
+			resourceCachingStrategy =
+				new FilenameWithVersionResourceCachingStrategy(resourceVersion);
+		}
 		return resourceCachingStrategy;
 	}",NotBuggy,"WICKET-3846 rework resource caching so it will by default work in clusters, also made the whole caching stuff pluggable and easier to extend

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@1141298 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"This diff adds logic to determine the `IResourceCachingStrategy` based on the application's configuration mode—development or otherwise. It uses different resource versioning techniques depending on the mode: `LastModifiedResourceVersion` for development to track file changes via modification time, and `MessageDigestResourceVersion` for production to ensure consistent versioning via MD5 hashes. These changes aim to enable resource caching in clusters and make caching behavior extensible and customizable.

The commit message does not indicate a specific bug fix but rather a rework of the caching mechanism to improve functionality in clustered environments and to make the caching strategy more flexible. The enhancements focus on configuration and performance rather than directly addressing a specific bug.

Given that the changes provide a new structure for resource caching and do not specifically target a bug fix, the conclusion is **NotBuggy**."
wicket,3339.json,d06f84d1b87011e5c152c5fb3f05ae50c1c58cda,"@@ -1,4 +1,4 @@
-			public Object getContainerRequest()
-			{
-				return null;
-			}

+				public Object getContainerRequest()
+				{
+					return null;
+				}",NotBuggy,"WICKET-5698 WebApplication#unmount() unmounts the whole compound mapper if some of its inner ones matches
",NotBuggy,"The given diff shows only a change in indentation, and there are no changes to logic or functionality in the code. The commit message mentions a specific issue with the `unmount()` method but offers no evidence of adjustments to it. The provided diff does not align with the commit message's focus on unmounting functionality, suggesting that either we are not seeing the most relevant changes, or this portion of the code change is unrelated to the bug fix.

Given the data provided, the changes made do not directly reflect a bug fix related to the commit message. The answer is **NotBuggy**."
wicket,11304.json,ed7c99768ab5764a76d77a1282d28b2f048fd1b8,"@@ -1,35 +1,43 @@
 			protected void populateItem(Item<IColumn<T, S>> item)
 			{
 				final IColumn<T, S> column = item.getModelObject();
 
-				WebMarkupContainer header = null;
+				WebMarkupContainer header;
 
 				if (column.isSortable())
 				{
 					header = newSortableHeader(""header"", column.getSortProperty(), stateLocator);
 				}
 				else
 				{
 					header = new WebMarkupContainer(""header"");
 				}
 
 				if (column instanceof IStyledColumn)
 				{
 					CssAttributeBehavior cssAttributeBehavior = new DataTable.CssAttributeBehavior()
 					{
 						private static final long serialVersionUID = 1L;
 
 						@Override
 						protected String getCssClass()
 						{
 							return ((IStyledColumn<?, S>)column).getCssClass();
 						}
 					};
 
 					header.add(cssAttributeBehavior);
 				}
 
+				if (column.getColspan() > 1) {
+					header.add(AttributeModifier.replace(""colspan"", column.getColspan()));
+				}
+
+				if (column.getRowspan() > 1) {
+					header.add(AttributeModifier.replace(""rowspan"", column.getRowspan()));
+				}
+
 				item.add(header);
 				item.setRenderBodyOnly(true);
 				header.add(column.getHeader(""label""));
 			}",NotBuggy,"WICKET-6095 Multiline headers in DataTable
",NotBuggy,"This commit introduces changes to the DataTable header rendering logic. The adjustments add support for `colspan` and `rowspan` attributes to allow for multiline headers in a DataTable. Here's a breakdown:

1. **Variable Initialization Change**: Previously, `header` was initialized to `null` and then conditionally assigned a value. The change removes unnecessary initialization, which is a code-quality improvement, but not related to a bug fix.

2. **Colspan and Rowspan**: The code modifications introduce logic to add `colspan` and `rowspan` attributes if their values are greater than 1. This allows the headers of the DataTable to span multiple columns or rows, supporting multiline headers. This change delivers functionality enhancement, aligning with the commit message focused on multiline headers.

While the changes enhance the functionality to support more complex layouts, they don't directly indicate a bug fix. The commit message also doesn't suggest the existence of incorrect behavior that was corrected. 

Therefore, based on these observations, the conclusion is **NotBuggy**."
wicket,3741.json,86fa5a7ac1e5e0aab3a2f47d423ffc53afb9f49c,"@@ -1,23 +1,23 @@
 	public BufferedWebResponse get(Object key)
 	{
 		BufferedWebResponse result = null;
 		Value value;
 		synchronized (this)
 		{
 			value = (Value)super.get(key);
 		}
 		if (value != null)
 		{
-			Duration elapsedTime =  Duration.between(value.creationTime, Instant.now());
-			if (lifetime.compareTo(elapsedTime) > 0)
+			Duration elapsedTime = Time.now().subtract(value.creationTime);
+			if (lifetime.greaterThan(elapsedTime))
 			{
 				result = value.response;
 			}
 			else
 			{
 				// expired, remove it
 				remove(key);
 			}
 		}
 		return result;
 	}",NotBuggy,"Revert ""Removed wicket-util date entities""

This reverts commit a8bb129af777670e6315142945155de3813d7b83.
",Buggy,"The commit message indicates a revert of a previous change, specifically related to ""wicket-util date entities."" In the diff presented, we observe code being restored to the way it was before commit a8bb129af777670e6315142945155de3813d7b83. The key change here is the reversal of duration and time comparison logic. 

1. The code originally used `Time.now().subtract(value.creationTime)` and `lifetime.greaterThan(elapsedTime)` for the check.
2. The change that was made (which is now being reverted) adopted `Duration.between(value.creationTime, Instant.now())` and `lifetime.compareTo(elapsedTime) > 0` instead.

The act of reverting implies that the original change might have introduced incorrect behavior or a bug. Maybe the new method of calculating and comparing durations was not functioning correctly or introduced undesired side effects.

Given that a revert is often performed in response to errors or regressions caused by a previous change, this indicates that the original commit a8bb129af777670e6315142945155de3813d7b83 possibly had a bug. Therefore, the act of reverting it suggests a correction, aligning well with the concept of a bug fix.

Based on this reasoning, I would conclude that the changes in this diff are likely related to fixing a bug introduced by the previous commit. Thus, the answer is **Buggy**."
wicket,6083.json,9e028af34cc432ddbffd6e09e82bf46d6226c0c8,"@@ -1,4 +1,5 @@
 				public boolean getDefaultFormProcessing()
 				{
+					// do not process the whole form
 					return false;
 				}",NotBuggy,"WICKET-6348 minor addition: update raw input when component is not contained in form
this wasn't done in 7.x either, but for consistency the raw input should be updated, so it is available when a validation fails
",NotBuggy,"The commit message indicates a minor addition for updating raw input when a component is not in a form, specifically for consistency when validation fails. The diff shows an update to the method `getDefaultFormProcessing()` to return `false`, which suggests that the component should not process the entire form by default. 

However, the comment added (`// do not process the whole form`) and the return statement (`return false;`) do not directly correlate with updating the raw input specifically or affecting validation logic directly based on the diff provided. This change seems more like a configuration or behavior adjustment rather than a bug fix.

The commit message itself references the WICKET-6348 issue for background, which might provide more context, but based purely on the provided diff, there is no direct evidence of a bug fix in handling raw input or dealing with failed validations.

Therefore, considering the scope of this specific diff:

- The change ensures that `getDefaultFormProcessing()` returns `false`, which is likely a behavior adjustment.
- The commit message mentions consistency, which suggests this is more about aligning behavior across different versions or use cases, not fixing an explicit bug.

Consequently, based on the provided code and message:

The answer is **NotBuggy**."
wicket,4077.json,7c40e4dafa75973bf99b34567d977f310a5f02d0,"@@ -1,9 +1,9 @@
 	protected boolean shouldFilter(AppendingStringBuffer responseBuffer)
 	{
 		// To avoid reading the whole buffer for non-Ajax responses
 		// read just the first N chars. A candidate can start with:
 		// <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><ajax-response>
 		int min = Math.min(150, responseBuffer.length());
 		String firstNChars = responseBuffer.substring(0, min);
-		return firstNChars.contains(XmlAjaxResponse.START_ROOT_ELEMENT);
+		return firstNChars.contains(XmlPartialPageUpdate.START_ROOT_ELEMENT);
 	}",NotBuggy,"WICKET-5929 renamed *AjaxResponse to *PartialPageUpdate; moved all XML related encoding into XmlPartialPageUpdate
",NotBuggy,"The commit message describes a renaming task and moving XML-related encoding to a new class, which seems to indicate a refactor rather than a direct bug fix. The code diff reflects this renaming from `XmlAjaxResponse.START_ROOT_ELEMENT` to `XmlPartialPageUpdate.START_ROOT_ELEMENT`. The functionality remains unchanged, and there's no indication of bug fix actions such as logical or exception handling adjustments. These changes strictly adhere to renaming and do not target bug resolution.

The commit involves structural organization and naming updates, suggesting a code cleanup or refactoring rather than addressing bugs. Thus, the alignment of the commit message with the code changes supports a non-bug-fix operation. 

Conclusively, these code modifications are more about cleanup and reorganization as part of the task specified in the commit message and do not correspond to a specific bug fix.

**NotBuggy**"
wicket,39.json,64024788d5122cfe9a9b173b165ac7c069b7f665,"@@ -1,14 +1,14 @@
 		public boolean equals(Object obj)
 		{
 			if (this == obj)
 			{
 				return true;
 			}
 			if (obj instanceof QueryParameter == false)
 			{
 				return false;
 			}
 			QueryParameter rhs = (QueryParameter)obj;
-			return Objects.equal(getName(), rhs.getName()) &&
-				Objects.equal(getValue(), rhs.getValue());
+			return Objects.equal(getName(), rhs.getName())
+					&& Objects.equal(getValue(), rhs.getValue());
 		}",NotBuggy,"wip on decoupling and extracting wicket-request. also the request encoding is now properly propagated to url handling.

git-svn-id: https://svn.apache.org/repos/asf/wicket/trunk@924375 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The given commit message talks about decoupling, extracting, and improving request handling, specifically mentioning encoding propagation. However, the code change only reformats the `equals` method's return statement for readability, preserving existing logic. There's no evidence of decoupling, extraction, or encoding adjustments within this small diff snippet. 

As reformatted code often aids readability without affecting logic or behavior, it does not pertain to a bug fix or directly align with the significant refactoring and functional improvements described in the commit message.

Considering the analysis above, the changes do not indicate a bug fix. The answer is **NotBuggy**."
wicket,10296.json,83740b9e5f9ca91ae185bc5683b6c94a5349b85b,"@@ -1,19 +1,20 @@
 	private Form<?> getCallbackForm()
 	{
 		Boolean insideModal = form.visitParents(ModalWindow.class,
 			new IVisitor<ModalWindow, Boolean>()
 			{
+				@Override
 				public void component(final ModalWindow object, final IVisit<Boolean> visit)
 				{
 					visit.stop(true);
 				}
 			});
 		if ((insideModal != null) && insideModal)
 		{
 			return form;
 		}
 		else
 		{
 			return form.getRootForm();
 		}
 	}",NotBuggy,"Add @Override to all overridden methods

This includes all places @Override is permitted in Java 6, since Wicket
6 requires Java 6.
",NotBuggy,"The commit message indicates that the purpose of the change is to add `@Override` annotations to overridden methods, aligning with Java 6 practices. The diff shows the addition of the `@Override` annotation to the `component` method, which is part of an anonymous class implementing `IVisitor`.

The change matches the commit message, which explicitly states the intent to add `@Override` annotations. This update is about code style and ensuring correct use of annotations, which improves understandability and maintainability but does not address any logical errors or runtime issues (i.e., bugs).

Therefore, this change is primarily about code annotation improvements and not about fixing a bug. Hence, I conclude that the overall nature of the change is **NotBuggy**."
wicket,2209.json,ca03c252b5745bdccfb7c8cdffcc705846593ec1,"@@ -1,5 +1,6 @@
-	public void removePage(final String sessionId, final int pageId)
+	public void removePage(IPageContext context, IManageablePage page)
 	{
-		pagesCache.removePage(sessionId, pageId);
-		removePageData(sessionId, pageId);
+		getRequestData(context).remove(page);
+
+		getDelegate().removePage(context, page);
 	}",NotBuggy,"WICKET-6563 new IPageStore implementation

this closes #283
",NotBuggy,"The given commit message mentions a new `IPageStore` implementation, which suggests the addition of new functionality rather than a direct bug fix. The changes in the code reflect the modification of the `removePage` method, which now uses an `IPageContext` and an `IManageablePage` instead of the previous parameters. This approach seems to encapsulate or restructure the way pages are removed, potentially in alignment with the newly implemented `IPageStore`.

Here's a breakdown of the changes:
- The method signature for `removePage` has been changed. Instead of using a `sessionId` and `pageId`, it now uses `IPageContext` and `IManageablePage`. This suggests an API improvement or refactor for better integration or abstraction.
- The lines within the method body have changed from directly interacting with a `pagesCache` object to using methods `getRequestData(context).remove(page)` and `getDelegate().removePage(context, page)`. This implies a shift towards more abstraction and delegation, likely to improve code maintainability or adhere to a new design introduced with `IPageStore`.

These changes suggest that the codebase is undergoing a structural modification, likely to support the new `IPageStore` implementation. There are no indications in the commit message or the code diff that a bug is being fixed directly—no error-handling updates, logical corrections, or other signs of bug fixes can be inferred.

Overall, the changes do not align with traditional bug fixes but rather indicate functional enhancements or refactoring. Therefore, this commit should be considered **NotBuggy**."
wicket,4773.json,fe1e621081ebe22c22f6e5b5fa98d51e44d50d76,"@@ -1,4 +1,4 @@
 	public static <Z> CompoundPropertyModel<Z> of(IModel<Z> model)
 	{
-		return new CompoundPropertyModel<Z>(model);
+		return new CompoundPropertyModel<>(model);
 	}",NotBuggy,"Java 7 diamonds
",NotBuggy,"The commit message ""Java 7 diamonds"" refers to the use of the diamond operator introduced in Java 7, which allows type inference during class instantiation without explicitly specifying the generic type parameters.

The code diff:
- Changes the instantiation of `CompoundPropertyModel<Z>` to use the diamond operator, simplifying it to `CompoundPropertyModel<>`.

This change is a refactor for readability and maintainability, taking advantage of Java 7's type inference features. It does not address a bug, nor does it affect the logic of the code. The functionality remains the same.

Therefore, the commit does not fix a bug; it is strictly a syntactic improvement.

**Conclusion: NotBuggy**"
vraptor4,1191.json,8757c99184f25ee826446209ff419f1120cf6131,"@@ -1,4 +1,4 @@
 	public void showAllMusicsAsHTTP() {
 		result.use(http()).body(""<p class=\""content\"">""+
-			dao.listAll().toString()+""</p>"");
+			musicDao.listAll().toString()+""</p>"");
 	}",NotBuggy,"Improving validation to musicjungle
",Buggy,"The commit message refers to ""Improving validation,"" but this change updates the variable `dao` to `musicDao` in the `showAllMusicsAsHTTP` method. This change doesn't relate to validation improvement. It looks more like a refactoring or fixing an improper reference that would have caused a runtime error when trying to call `listAll()` on an incorrect object, which indirectly contributes to the overall stability of the application.

However, the primary focus on validation improvement is not directly addressed by this code change, as there are no visible updates implying enhanced validation processing or checks. The code change corrects a potential bug related to the correct use of an object, suggesting a resolution to a defect due to referencing what potentially was an incorrect or outdated object instance.

Since resolving incorrect instance references can qualify as bug fixes, and considering the mismatch between the commit message and code change, we note that it does potentially address an error.

Thus, we can conclude with **Buggy** because it indicates a fix for an erroneous reference potentially causing failures."
vraptor4,142.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,20 +1,20 @@
-		public String toString() {
+		public String getLink() {
 			Method method = null;
 
 			if (getMethodsAmountWithSameName() > 1) {
 				method = new Mirror().on(controller).reflect().method(methodName).withArgs(getClasses(args));
 				if (method == null && args.isEmpty()) {
 					throw new IllegalArgumentException(""Ambiguous method '"" + methodName + ""' on "" + controller + "". Try to add some parameters to resolve ambiguity, or use different method names."");
 				}
 			} else {
 				method = findMethodWithName(controller, methodName);
 			}
 
 			if(method == null) {
 				throw new IllegalArgumentException(
 					String.format(""There are no methods on %s named '%s' that receives args of types %s"",
 							controller, methodName, Arrays.toString(getClasses(args))));
 			}
 
 			return context.getContextPath() + router.urlFor(controller, method, getArgs(method));
 		}",NotBuggy,"Linker does not need to be a Map anymore
",NotBuggy,"The commit message suggests a design change where a ""Linker"" component doesn't need to be `Map` anymore, but the provided diff doesn't reflect changes regarding Maps or related structures. Instead, the diff modifies a method signature from `toString()` to `getLink()`. 

Let's analyze:

1. **Method Signature Change**: The change from `toString()` to `getLink()` primarily represents a design alteration, now intending for the method to specifically provide a ""link"". This is likely more descriptive semantically and aligns better, potentially with new usage patterns or requirements.

2. **Functionality of Method**: The content within the method appears unchanged except for the name. The commit suggests that such a change might relate to how links are handled or obtained, which could affect interfaces or how the component interacts with others.

3. **Relation to Maps**: The diff does not show any changes related to Maps, so it is either part of a broader commit or document, or simply misaligned in this particular diff representation.

From this specific diff, the changes do not indicate a bug fix. Their nature is more refactor-oriented, related to method signature modification, likely for code clarity or adhering to new coding standards. Without additional context on how the ""Linker"" or ""Map"" elements interact post-change, it's safe to classify this change based on what's presented.

Conclusion: **NotBuggy**. The commit does not explicitly address bug fixes but involves naming changes for clarity/readability or new design requirements."
vraptor4,1047.json,634361fe0135b236651256d7c97945dec4f2c26d,"@@ -1,8 +1,8 @@
 	public void start() {
 		ControllerMethod method = controllerMethod.get();
-		stackStartingEvent.fire(new StackStarting(method));
+		interceptorsReadyEvent.fire(new InterceptorsReady(method));
 		LinkedList<InterceptorHandler> handlers = cache.getInterceptorHandlers();
 		internalStack.addFirst(handlers.iterator());
 		this.next(method, controllerInstance.get().getController());
 		internalStack.poll();
 	}",NotBuggy,"Renaming all events, to add some naming consistency
",NotBuggy,"The commit message indicates a renaming action to achieve naming consistency. In the provided diff, the variable `stackStartingEvent` is renamed to `interceptorsReadyEvent`, and similarly, the `StackStarting` event is renamed to `InterceptorsReady`. There are no logical changes, error-handling updates, or any other modifications suggesting a bug fix; it purely focuses on renaming for consistency.

Thus, the nature of this change aligns well with the commit message's objective of renaming for consistency without addressing any bug-related issues.

The conclusion is **NotBuggy**."
vraptor4,1127.json,d63f531e2548c47d274077d9938be60c100d3caa,"@@ -1,5 +1,3 @@
 	private boolean isWebsocketRequest(HttpServletRequest request) {
-		// according to the Websocket spec (https://tools.ietf.org/html/rfc6455): The WebSocket Protocol 
-		// 5. The request MUST contain an |Upgrade| header field whose value MUST include the ""websocket"" keyword.
-	    return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
+		return request.getHeader(""Upgrade"") != null && request.getHeader(""Upgrade"").toLowerCase().contains(""websocket"");
 	}",NotBuggy,Moving comments within the method to the method's javadoc.,NotBuggy,"The commit message states that comments within the method are being moved to the method's Javadoc. The diff shows the removal of comments within the `isWebsocketRequest` method, but it doesn't display the new location of these comments, potentially in a separate documentation update.

The code functionality is not altered by this change because the comments were non-executable, and there are no modifications to the logic or any indication of a bug fix.

Given the nature of the changes, they are purely related to documentation and do not indicate a bug fix in the code itself.

Thus, the changes are consistent with the commit message and are not related to a bug fix. The conclusion is **NotBuggy**."
vraptor4,800.json,68bc254803a4e712c85d525df1319f1a6d68600c,"@@ -1,7 +1,7 @@
-	private File checkFile(File file) throws IOException {
+	private File checkFile(File file) throws FileNotFoundException {
 		if (!file.exists()) {
 			throw new FileNotFoundException(""File "" + file.getName() + ""doesn't exists"");
 		}
 		
 		return file;
 	}",NotBuggy,"Downgrading exception
",NotBuggy,"The commit message states ""Downgrading exception,"" which aligns with the change in the code. The exception being thrown is changed from a more general `IOException` to a specific `FileNotFoundException`. This is typically done for better error handling and conveys the precise nature of the error more accurately. However, downgrading an exception in this way doesn't indicate a bug fix but rather a change in exception granularity. The change is more about clarifying the type of exception that should be handled when the file does not exist.

There's no indication of any bug being fixed with this change, as it doesn't correct a logical error or an incorrect behavior; instead, it enforces stricter type-checking for the specific condition being checked.

Therefore, the conclusion is **NotBuggy**."
vraptor4,601.json,41ba6271a5f50496c4ada07f67c52b27ca15ef33,"@@ -1,3 +1,3 @@
 	public void indented() {
-		builder.setPrettyPrinting();
+		getGsonBuilder().setPrettyPrinting();
 	}",NotBuggy,"Uses protected getter for GsonBuilder to allow for extension
",NotBuggy,"The commit message indicates refactoring to use a protected getter for GsonBuilder, allowing for better extensibility. The code change updates the method call from directly accessing an object to using a getter, aligning it with the commit's extension-focused goal. No bug fix is implied; the goal is to enhance architecture or maintainability without addressing a defect. The answer is **NotBuggy**."
vraptor4,62.json,8660dccf28a2ebee23fc51fe61980b0f8ca4639b,"@@ -1,3 +1,3 @@
-		protected Map<Object, Linker> delegate() {
-			return Collections.emptyMap();
-		}

+	protected Map<Class<?>, Object> delegate() {
+		return Collections.emptyMap();
+	}",NotBuggy,"Linker does not need to be a Map anymore
",NotBuggy,"The commit message indicates a structural change where the `Linker` class or variable should no longer be a Map. The change in the diff modifies the method signature of `delegate()` from returning `Map<Object, Linker>` to `Map<Class<?>, Object>`. This indicates a significant change in the expected types for the keys and values within the map, suggesting a broader structural adjustment rather than a bug fix.

The change does not specifically address error handling, logical corrections, or exception handling, which are common indicators of bug fixes. Instead, it appears to be more about code structure or refactoring, perhaps aligning with a change in the design decision about how parts of the codebase should interact or represent data.

Given that the modification aligns with a design change articulated in the commit message, this change doesn't necessarily indicate a bug fix. It's more about restructuring or refactoring the codebase to align with updated design decisions, which does not imply the presence of a bug with incorrect behavior that needed fixing.

Therefore, the assessment concludes with **NotBuggy**."
vraptor4,98.json,45b2bcef87bdd4c342f05b0134a3231749dc380d,"@@ -1,5 +1,5 @@
-	private <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
+	protected <T> void includeParametersInFlash(final Class<T> type, Method method, Object[] args) {
 		if (args != null && args.length != 0) {
 			flash.includeParameters(DefaultControllerMethod.instanceFor(type, method), args);
 		}
 	}",NotBuggy,"Changing visibility of methods includeParametersInFlash and acceptsHttpGet
",NotBuggy,"The commit message indicates a change in method visibility, which aligns with the code change shown in the diff. The method `includeParametersInFlash` has its visibility altered from `private` to `protected`. This change does not imply a bug fix; it primarily affects encapsulation and potentially the class's subclassing functionality. The change seems to be about altering access levels, not correcting an error.

Given this analysis, the commit message accurately describes the change, and there is no bug fix indicated in this specific diff segment. Thus, the conclusion is **NotBuggy**."
vraptor4,1124.json,68ec20ba9f131586d351821b75a9d3e7c842ef4c,"@@ -1,7 +1,8 @@
 	private URL getResource(String path) throws ServletException {
 		try {
 			return servletContext.getResource(path);
 		} catch (MalformedURLException e) {
-			throw new ServletException(""Something went wrong when trying to locate a beans.xml file"", e);
+			logger.error(""Something went wrong when trying to locate a beans.xml file"", e);
+			return null;
 		}
 	}",NotBuggy,"returning null instead of rethrowing exception
",Buggy,"The commit message states that instead of rethrowing an exception, the method will now return null in case of a `MalformedURLException`. Let's analyze the changes and the commit message:

1. **Exception Handling**: Previously, the `MalformedURLException` was caught and wrapped in a `ServletException`, which was then thrown. This would propagate the error up and require handling elsewhere or result in a failure if not caught.

2. **Current Change**: The change logs the error using `logger.error` and then returns `null`. This indicates that the function will now handle the exception by logging the issue and providing a null value indicating failure, instead of interrupting the flow with an exception.

3. **Alignment with Message**: The commit message accurately describes the change: the function now returns `null` instead of throwing an exception.

4. **Bug Fix Indication**: This change modifies the error-handling approach. It may address a bug where the function's tendency to throw exceptions was problematic for application flow, causing disruptions or requiring additional try-catch logic elsewhere. By returning null, the application could process this case differently, perhaps in existing logic designed to deal with missing resources.

Considering these points, the change directly corresponds with the commit message and modifies how an exception scenario is handled, potentially addressing a usability defect or logical issue in handling resource location problems.

Therefore, the assessment is **Buggy** because it reflects a shift from error propagation to error handling and resolution within the method, potentially fixing errors linked with previous exception handling logic."
vraptor4,853.json,8fbc9e23ee5038a35159f408a961f98de86b562a,"@@ -1,6 +1,6 @@
 	public <V, K> CacheStore<K, V> createCacheWrapper(int capacity) {
 		Cache<K, V> guavaCache = CacheBuilder.newBuilder()
 			.maximumSize(capacity)
 			.build();
-		return new GuavaCacheWrapper<K,V>(guavaCache);
+		return new GuavaCacheWrapper<>(guavaCache);
 	}",NotBuggy,"squid:S1192, squid:S2293 - String literals should not be duplicated, The diamond operator should be used
",NotBuggy,"The commit message refers to improvements like reducing string duplication and using the diamond operator where applicable. The diff adjusts a `GuavaCacheWrapper` constructor to use the diamond operator, removing redundant type arguments. This change addresses the simpler code style by reducing verbosity and improving readability, aligning with the part of the message about using the diamond operator.

There's no indication that the change fixes a bug. The modification does not relate to error handling, exception handling, or logical correction. It appears to be purely a style and maintainability enhancement, particularly for enhancing the use of Java 7 features.

Hence, the diff does not correspond to a bug fix but is instead about code quality improvements. Therefore, the conclusion is **NotBuggy**."
vraptor4,645.json,fc1de5e5d28c1b21ea5c9e87120c1cfec3b1e1ee,"@@ -1,4 +1,4 @@
 	public Serializer recursive() {
-		this.serializee.setRecursive(true);
+		builder.getSerializee().setRecursive(true);
 		return this;
 	}",NotBuggy,"Reusing Serializee from builder
",NotBuggy,"The commit message suggests reusing a `Serializee` object from a `builder` instead of using a local instance. The code change aligns with this message by switching from `this.serializee` to `builder.getSerializee()`. This reduces redundant instantiation, but does not indicate a bug fix—it optimizes object reuse instead. The explanation doesn't mention bug fixes, error handling, or logic corrections. Therefore, the assessment is **NotBuggy**."
vraptor4,113.json,62fe19ddb7501c4371a8646e2f4316cb70ee0eaf,"@@ -1,7 +1,7 @@
 	public Object[] consumeParameters(ControllerMethod method) {
-		Object[] args = (Object[]) session.getAttribute(nameFor(method));
-		if (args != null) {
-			session.removeAttribute(nameFor(method));
-		}
+		String param = nameFor(method);
+		Object[] args = (Object[]) session.getAttribute(param);
+		session.removeAttribute(param);
+
 		return args;
 	}",NotBuggy,"Removing unnecessary statement
",NotBuggy,"The code change simplifies the session manipulation by consolidating variable usage and removing an unnecessary null check before removing the attribute from the session. Specifically, the code initially checked if `args` was non-null before removing the attribute from the session, but now the attribute removal happens unconditionally. This change is more efficient but does not inherently fix a bug, as the behavior remains functionally equivalent. The commit message describes the action accurately by mentioning the removal of an unnecessary statement, which aligns with the code changes.

- The commit does not introduce any explicit bug fixes or address incorrect behavior.
- The change improves code efficiency by removing redundant operations without altering the logic or error handling.

Thus, the alignment between the commit message and the code modification suggests it is a refactoring for clarity and efficiency, not a bug fix.

The conclusion is **NotBuggy**."
vraptor4,881.json,743c3c430bf1b46974df6ef038892a42ef97c667,"@@ -1,6 +1,6 @@
 	public Map<String, Collection<Message>> getGrouped() {
 		if (grouped == null) {
-			grouped = Multimaps.index(delegate, new GroupByCategory()).asMap();
+			grouped = index(delegate, groupByCategory).asMap();
 		}
 		return grouped;
 	}",NotBuggy,"Static instances and static import
",NotBuggy,"The commit message ""Static instances and static import"" suggests a change in how static members or methods are used. Looking at the diff, the code modification replaces the method call `Multimaps.index` with `index`, presumably due to a static import of the `index` method. Additionally, it appears that the `GroupByCategory` class or instance might now be referenced as a static import or static instance, indicated by the change from `new GroupByCategory()` to `groupByCategory`.

This refactoring uses static imports and possibly static instances, aligning with the commit message. However, there is no indication of a bug fix in this change. The modification aims at simplifying the code and potentially optimizing how resources (like instances) are utilized, but does not imply correcting any logical or functional issues.

Therefore, despite the relevance to the commit message, the changes do not indicate a bug fix. The assessment is **NotBuggy**."
vraptor4,935.json,7f19b84c6bed93f11e11501f35c21330780bd540,"@@ -1,8 +1,8 @@
-	private void addAll(Set<ConstraintViolation<Object>> errors) {
-		for (ConstraintViolation<Object> v : errors) {
+	public <T> void addAll(Set<ConstraintViolation<T>>  errors) {
+		for (ConstraintViolation<T> v : errors) {
 			String msg = interpolator.interpolate(v.getMessageTemplate(), new BeanValidatorContext(v), locale);
 			String category = v.getPropertyPath().toString();
 			add(new SimpleMessage(category, msg));
 			logger.debug(""added message {}={} for contraint violation"", category, msg);
 		}
 	}",NotBuggy,"Relaxing visibility
",NotBuggy,"The commit message ""Relaxing visibility"" aligns with the change in the code. The visibility of the method `addAll` is changed from `private` to `public`, indicating an intention to make the method accessible from other classes or packages. Additionally, the method now uses generics (`<T>`) to increase its flexibility with different types. However, there is no indication of a bug fix. The change does not address any error-handling, logical correction, or exception-handling improvements that are typically associated with bug fixes.

Given the nature of the modifications and the commit message, this change is about enhancing accessibility and flexibility rather than fixing a bug.

Therefore, the assessment is **NotBuggy**."
vraptor4,1163.json,b92a72ec752eea09e6e59da5b42aafae87e198c0,"@@ -1,3 +1,3 @@
 	public void add(Music music) {
-		session.save(music);
+		getSession().save(music);
 	}",NotBuggy,"Configures JPA and uses EntityManager instead of Hibernate Session;
",NotBuggy,"The commit message states a configuration of JPA and switching from Hibernate `Session` to `EntityManager`. However, the code diff shows a minor change in how a session is accessed, from using a `session` variable directly to calling `getSession()`. This change suggests a refactor or encapsulation without indicating a shift to JPA or `EntityManager` usage directly from `Session`.

There are no indications of bug fixes such as error handling updates, logical corrections, or exception handling improvements. Thus, the actual change does not align with a bug fix but rather a code style or architectural adjustment.

Thus, the commit's code changes do not explicitly indicate a bug fix. Therefore, I conclude with **NotBuggy**."
flink,31570.json,1ab79f65fabdb47d7d041557132b4978db4bc9e2,"@@ -1,4 +1,9 @@
 	public CatalogBaseTable copy() {
-		return new HiveCatalogView(
-			this.getOriginalQuery(), this.getExpandedQuery(), this.getSchema().copy(), new HashMap<>(this.getProperties()), getComment());
+		return new CatalogViewImpl(
+			getOriginalQuery(),
+			getExpandedQuery(),
+			getSchema().copy(),
+			new HashMap<>(getProperties()),
+			getComment()
+		);
 	}",NotBuggy,"[FLINK-12965][table][hive] unify catalog view implementations

This PR unified implementations of CatalogView.

This closes #8882.
",NotBuggy,"The commit message discusses unifying catalog view implementations, not fixing bugs. The code changes replace `HiveCatalogView` with `CatalogViewImpl` in the `copy` method, aligning with the message of unification. There's no indication of logical corrections or error-handling improvements typically associated with a bug fix. The code change aligns with the task described in the commit message and doesn't address any specific bug-related issue. Therefore, the conclusion is **NotBuggy**."
flink,32529.json,891075aefcfd702142c90cab51759b40765c5a5d,"@@ -1,3 +1,3 @@
 	public <T> T accept(QueryOperationVisitor<T> visitor) {
-		return visitor.visitSort(this);
+		return visitor.visit(this);
 	}",NotBuggy,"[hotfix][table-api] Renamed methods of TableOperationsVisitor
",NotBuggy,"The commit message states a hotfix involving renaming methods of `TableOperationsVisitor`. However, the code change does not demonstrate any method renaming; instead, it shows a modification in the method being called from `visitSort(this)` to `visit(this)`. 

Analyzing the change:
- The original method call `visitSort(this)` is replaced with `visit(this)`. 
- This might indicate a logical change in how the visitor pattern is applied or simplify the implementation by possibly overloading `visit` for different types of operations instead of using a more specific `visitSort`.

The commit does not directly mention fixing a bug, only renaming. However, if `visitSort` was incorrect or redundant and could lead to incorrect operations being visited, this modification might implicitly fix an error by ensuring the correct `visit` method is uniformly called.

The information provided in the commit message does not explicitly identify bug-related corrections, and the change seems more aligned with addressing clarification and simplification rather than directly fixing a fault. Thus, based solely on the diff and the provided message, this can be judged as **NotBuggy**."
flink,25786.json,8d67aa55f1c11e167a936f1f2a9d954db5960673,"@@ -1,13 +1,12 @@
 	public O returns(Class<OUT> typeClass) {
-		if (typeClass == null) {
-			throw new IllegalArgumentException(""Type class must not be null."");
-		}
+		requireNonNull(typeClass, ""type class must not be null"");
 		
 		try {
-			TypeInformation<OUT> ti = (TypeInformation<OUT>) TypeExtractor.createTypeInfo(typeClass);
-			return returns(ti);
+			return returns(TypeInformation.of(typeClass));
 		}
 		catch (InvalidTypesException e) {
-			throw new InvalidTypesException(""The given class is not suited for providing necessary type information."", e);
+			throw new InvalidTypesException(""Cannot infer the type information from the class alone."" +
+					""This is most likely because the class represents a generic type. In that case,"" +
+					""please use the 'returns(TypeHint)' method instead."", e);
 		}
 	}",NotBuggy,"[FLINK-2788] [apis] Add TypeHint class to allow type-safe generic type parsing

This closes #1744
",NotBuggy,"The diff primarily modifies error-handling logic within the `returns` method. Instead of explicitly checking for `null` and throwing an `IllegalArgumentException`, the updated code uses `requireNonNull`, which is more concise but functionally identical to the previous null check.

Moreover, the main change is the modification in exception messaging when catching an `InvalidTypesException`. The original message was generalized, simply stating that the given class couldn't provide the necessary type information. However, the revised message provides more context by suggesting the potential cause of the exception—namely, that it may involve a generic type declaration—and advises using a different method (`returns(TypeHint)`) in such cases.

These changes reflect enhanced clarity in exception handling and user guidance. However, they don't necessarily indicate a direct bug fix, but rather an improvement in user feedback and usability.

Given these observations, the modifications do not align with a typical bug fix, but rather they improve usability and error messaging. Therefore, the conclusion is **NotBuggy**."
flink,3845.json,f728129bdb8c3176fba03c3e74c65ed254146061,"@@ -1,5 +1,5 @@
 	public int releaseMemory() {
-		// The pipelined subpartition does not react to memory release requests. The buffers will be
-		// recycled by the consuming task.
+		// The pipelined subpartition does not react to memory release requests.
+		// The buffers will be recycled by the consuming task.
 		return 0;
 	}",NotBuggy,"[FLINK-5169] [network] Make consumption of InputChannels fair
",NotBuggy,"The commit message suggests changes related to fair consumption of `InputChannels`, but the provided code diff shows only a minor comment formatting adjustment without modifying logic or addressing fairness. No changes indicate a bug fix or logic correction, which means this part of the commit is simply a comment adjustment unrelated to the commit message's main intent. Thus, the commit changes shown in the diff do not reflect any bug fix.

The answer is **NotBuggy**."
flink,12245.json,c401da36dfb5ff05527ed769babe16cbfdfc80be,"@@ -1,3 +1,3 @@
-	Optional<String> getPyExecutable() {
-		return Optional.ofNullable(pyExecutable);
+	Optional<String> getEntryPointScript() {
+		return Optional.ofNullable(entryPointScript);
 	}",NotBuggy,"[FLINK-16668][python] Improve PythonDriver to parse python dependency info from configuration

This closes #11682.
",NotBuggy,"The commit message states an intention to improve the PythonDriver by parsing Python dependency info from configuration. However, the provided diff only changes a method name from `getPyExecutable` to `getEntryPointScript` and the variable it references from `pyExecutable` to `entryPointScript`. This appears more like a refactor or a change to align names with their actual use or meaning, rather than an actual functional improvement or bug fix.

There is no indication that this change involves parsing configuration or directly addresses any bugs related to Python dependencies based solely on this snippet. Thus, despite what the commit message implies, the diff does not appear to reflect a bug fix or directly improving functionality related to parsing dependency information.

The answer is **NotBuggy**."
flink,291.json,4452be3a6c1c165b7d808cb4b779e6c2ecd69f01,"@@ -1,3 +1,3 @@
-	public long getMin() {
-		return (long) descriptiveStatistics.getMin();
-	}

+		public double getMin() {
+			return min;
+		}",NotBuggy,"[FLINK-12982][metrics] improve DescriptiveStatisticsHistogramStatistics performance

Instead of redirecting DescriptiveStatisticsHistogramStatistics calls to
DescriptiveStatistics, it takes a point-in-time snapshot using an own
UnivariateStatistic implementation that
a) calculates min, max, mean, and standard deviation in one go (as opposed to
   four iterations over the values array!)
b) caches pivots for the percentile calculation to speed up retrieval of
   multiple percentiles/quartiles

As a result, this roughly increases value retrieval performance by 120% when
accessing typical statistics in a metrics reporter, e.g. the InfluxDB reporter:
count, min, max, mean, stddev, p50, p75, p95, p98, p99, p999.
",NotBuggy,"This commit primarily focuses on performance improvement rather than bug fixing. The changes optimize the way statistics data is calculated by storing a pre-computed snapshot instead of repeatedly querying DescriptiveStatistics. Altering `getMin()` to return a stored value `min` does not directly address any incorrect behavior but likely plays a role in the overall performance enhancement strategy detailed in the commit message. 

There's no indication in the commit message or the diff pointing to a correction of faulty logic or erroneous calculations. The emphasis is on optimizing the performance of statistic retrieval rather than addressing a defect. Consequently, this commit does not indicate a bug fix.

Thus, the conclusion is **NotBuggy**."
flink,15309.json,d01d369701c8f79b06d60ee0f731392d3da9b912,"@@ -1,4 +1,4 @@
 	public GraphCsvReader fieldDelimiterEdges(String delimiter) {
-		this.EdgeReader.fieldDelimiter(delimiter);
+		this.edgeReader.fieldDelimiter(delimiter);
 		return this;
 	}",NotBuggy,"[FLINK-1520] [gelly] add types methods and make formatting changes to the graph csv reader

This squashes the following commits:

[FLINK-1520] [gelly] add named types methods for reading a Graph from CSV input,
with and without vertex/edge values. Change the examples and the tests accordingly.

[FLINK-1520] [gelly] corrections in Javadocs; updated documentation

This closes #1149
",NotBuggy,"The commit message primarily discusses adding new functionality and formatting, with no mention of bug fixes. The change in the diff corrects capitalization of `EdgeReader` to `edgeReader`, aligning it with typical Java naming conventions and likely fixing a variable name bug. However, the commit message doesn't specify a bug fix. The alteration aligns with typical cleanup or style adjustments rather than explicitly correcting a logical error or a bug. Thus, the analysis leans towards the refactoring aspect rather than a bug fix. Thus, the answer is **NotBuggy**."
flink,16953.json,1f7150fc6037406f0656de1babdbaa498c514bb5,"@@ -1,14 +1,15 @@
 	public <L, R> SingleOutputStreamOperator<R> select(
 			final OutputTag<L> timeoutOutputTag,
 			final PatternTimeoutFunction<T, L> patternTimeoutFunction,
 			final TypeInformation<R> outTypeInfo,
 			final PatternSelectFunction<T, R> patternSelectFunction) {
 		return CEPOperatorUtils.createTimeoutPatternStream(
 			inputStream,
 			pattern,
 			comparator,
 			clean(patternSelectFunction),
 			outTypeInfo,
 			timeoutOutputTag,
-			clean(patternTimeoutFunction));
+			clean(patternTimeoutFunction),
+			lateDataOutputTag);
 	}",NotBuggy,"[FLINK-9476][cep] Emit late elements in CEP as sideOutput

This closes #6104
",NotBuggy,"The commit message indicates that the change addresses a feature enhancement by emitting late elements as side outputs in the CEP (Complex Event Processing) system. The modification in the code involves adding a `lateDataOutputTag` as an additional parameter in the `select` method. This suggests that the code now has the capability to handle late data separately by directing it to a different output, which aligns with the commit message's intention to emit late elements as side outputs.

However, this change does not specifically indicate a fix for a bug within the code. Instead, it implies an improvement to the functionality by introducing a new feature to handle late elements effectively, which is more about enhancement rather than correcting an error in the existing implementation.

Therefore, the appropriate classification for this change, considering the context provided, is **NotBuggy** because it relates to a feature enhancement rather than a direct bug fix."
flink,19917.json,24eb47ac86299920aa137d5b6394a248ff58d19e,"@@ -1,3 +1,3 @@
-	protected int getLength() {
-		return 2;
+	public int getLength() {
+		return 4;
 	}",NotBuggy,"Renaming part 2 (core and java api)
",NotBuggy,"The commit message indicates part of a renaming task concerning ""core and java api,"" but the diff shows a change in method access and return value: 

1. **Access Modifier Change:** The method `getLength` is changed from `protected` to `public`. This is a significant change as it alters the method's visibility.

2. **Return Value Change:** The return value of the method changes from `2` to `4`. This could indicate a change in the logic or the intended output of the method; however, without additional context, it's unclear whether this addresses a bug or a refactoring decision tied to the broader renaming.

The commit message makes no mention of changing method length calculation or visibility, and the changes seem unrelated to renaming. Instead, they suggest internal or API modifications while the message lacks relevant context.

Due to this disconnect and lack of clarity that this fixes any specific bug, and aligning with a renaming intent indicates **NotBuggy**."
flink,1138.json,033c69f9477c6352865e7e0da01296dd778ffe59,"@@ -1,5 +1,5 @@
-	public Class<FlatCombineFunction<IN, OUT>> getStubType() {
+	public Class<GroupCombineFunction<IN, OUT>> getStubType() {
 		@SuppressWarnings(""unchecked"")
-		final Class<FlatCombineFunction<IN, OUT>> clazz = (Class<FlatCombineFunction<IN, OUT>>) (Class<?>) FlatCombineFunction.class;
+		final Class<GroupCombineFunction<IN, OUT>> clazz = (Class<GroupCombineFunction<IN, OUT>>) (Class<?>) GroupCombineFunction.class;
 		return clazz;
 	}",NotBuggy,"[Flink-1780] Rename FlatCombineFunction to GroupCombineFunction

This closes #530
",NotBuggy,"The commit message indicates a renaming task from FlatCombineFunction to GroupCombineFunction. The code diff reflects this change by updating method return and variable types from FlatCombineFunction to GroupCombineFunction. This kind of change is consistent with a refactor or renaming task rather than a bug fix.

There are no indications of changes involving bug fixes, such as logic corrections or error-handling updates. Therefore, the focus here is purely on renaming to maintain or improve clarity or alignment with a naming convention, rather than correcting functionality.

The answer is **NotBuggy**."
flink,39338.json,5cebfb76c717568ede8f7e8a82e9bbdf774c19ed,"@@ -1,12 +1,22 @@
 	private void finishAndReportAsync(Map<OperatorID, OperatorSnapshotFutures> snapshotFutures, CheckpointMetaData metadata, CheckpointMetrics metrics) {
+		final Future<?> channelWrittenFuture;
+		if (unalignedCheckpointEnabled) {
+			ChannelStateWriteResult writeResult = channelStateWriter.getWriteResult(metadata.getCheckpointId());
+			channelWrittenFuture = CompletableFuture.allOf(
+				writeResult.getInputChannelStateHandles(),
+				writeResult.getResultSubpartitionStateHandles());
+		} else {
+			channelWrittenFuture = FutureUtils.completedVoidFuture();
+		}
 		// we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit
 		executorService.execute(new AsyncCheckpointRunnable(
 			snapshotFutures,
 			metadata,
 			metrics,
+			channelWrittenFuture,
 			System.nanoTime(),
 			taskName,
 			closeableRegistry,
 			env,
 			asyncExceptionHandler));
 	}",NotBuggy,"[FLINK-16587][checkpointing] Spill the in-flight input and output buffers during checkpointing.
",NotBuggy,"The commit message indicates an enhancement related to checkpointing, specifically spilling in-flight buffers. The code diff introduces logic to handle channel state writing as part of the async checkpointing process. This involves creating a future to wait for channel state handles when unaligned checkpointing is enabled. The change improves how asynchronous tasks manage state but does not fix a specific pre-existing bug. There is no evidence of error correction, exception handling, or logical fixes related to defects in the current implementation.

Thus, these changes do not align with a bug fix, but rather an enhancement or feature addition to support unaligned checkpointing. Therefore, the conclusion is **NotBuggy**."
flink,35096.json,9a6ca547d6bd261730c46519f6bffa0b699ec218,"@@ -1,3 +1,3 @@
-		public Iterable<UV> values() throws Exception {
-			return internalMap.values();
+		public Iterable<MV> values() throws Exception {
+			return new ValuesIterable(this.iterator());
 		}",NotBuggy,"[FLINK-13266][table] Relocate blink runtime classes to avoid class clashes
",Buggy,"This commit modifies the return type of a method `values()` and changes its implementation. Specifically, it replaces returning `internalMap.values()` directly with returning a new `ValuesIterable` constructed with `this.iterator()`.

Here's an analysis of the changes:

1. **Method Return Type Change**: The return type changes from `Iterable<UV>` to `Iterable<MV>`. This could indicate a logical correction or a compatibility adjustment, but it is not clear from this diff alone what the reason is.

2. **Implementation Change**: Instead of returning `internalMap.values()`, it now returns a `ValuesIterable(this.iterator())`. This suggests that the method no longer directly exposes the values from the `internalMap`, possibly indicating a refactor or addressing encapsulation concerns.

3. **Possible Bug Fix**: 
   - If the original code exposed internal implementation details inappropriately or caused class clash issues as implied by the commit message, this change can be perceived as a bug fix. 
   - The change could be aimed at preventing direct access to internal map values which might cause unintended side effects or conflicts due to the clashes the commit message mentions.

4. **Commit Message Context**:
   - The message speaks to relocating runtime classes to avoid clashes, hinting at class/interface conflicts resolved by this change.

Given these insights, the changes align with addressing an issue that could relate to unexpected behavior or class interfacing conflicts.

Considering this, the assessment of whether this is a bug fix or not hinges primarily on resolving any underlying issues that the commit message hints at. Since there's an indication that the change addresses clashing (likely causing issues), it fits the category of a bug fix.

Thus, I conclude with **Buggy**."
flink,20306.json,bb76fda2444969c4e3a05e05485c43b533d8a499,"@@ -1,3 +1,3 @@
 	public boolean isImmutableType() {
-		return true;
+		return false;
 	}",NotBuggy,"Added direct array serializers for all primitives
",NotBuggy,"The commit message talks about adding direct array serializers for all primitives, but the code change modifies the `isImmutableType` method to return `false` instead of `true`. This change seems unrelated to the addition of direct array serializers as described in the commit message.

To analyze whether the change indicates a bug fix:

1. **Context Understanding**: The method `isImmutableType` would generally be used to indicate whether instances of the object are immutable. Changing the return value can potentially fix a bug where the immutability status was incorrectly reported.

2. **Relevance to Message**: The commit message does not mention anything about immutability or fixing bugs related to type reporting. The change reflects an internal logic alteration relevant to potentially object lifecycle or mutability but unrelated to the commit message content regarding serializers.

3. **Impact of Code Change**:
   - If `isImmutableType` was returning `true` incorrectly and this property caused issues elsewhere (e.g., in array serialization handling), this change corrects that.
   - Without further context, the specific relationship between this method's return value and array serialization can't be definitively assessed from the information provided.

Given that this change's relevance to a bug fix can't be conclusively derived from the message or code provided and that switching immutability status might not necessarily address a clear bug scenario without additional context, this leads me to conclude this does not clearly indicate a bug fix on its own. 

Conclusion: **NotBuggy**."
flink,22520.json,1a10fbef644ad32a3358711bfa5a167118186482,"@@ -1,3 +1,3 @@
-				public URL nextElement() {
-					return iter.next();
-				}

+			public URL nextElement() {
+				return iter.next();
+			}",NotBuggy,"[FLINK-11952][1/3] Make ChildFirstClassLoader a top-level class in flink-core
",NotBuggy,"The commit message suggests a restructuring change, specifically making `ChildFirstClassLoader` a top-level class in `flink-core`. However, the provided diff shows a change in indentation or alignment of the method `nextElement()`, which does not relate to the main intent mentioned in the commit message. The change is superficial and does not indicate a bug fix or logical change in the code. The method's functionality remains the same, and no bug-related context is provided.

Therefore, based on the diff and commit message, the answer is **NotBuggy**."
flink,41571.json,8cdf2ff7e5817acc0c239ce31c098daf33d326b7,"@@ -1,3 +1,3 @@
-		public void setPendingTransaction(TXN pendingTransaction) {
+		public void setPendingTransaction(TransactionHolder<TXN> pendingTransaction) {
 			this.pendingTransaction = pendingTransaction;
 		}",NotBuggy,"[FLINK-7784] [kafka011-producer] Make TwoPhaseCommitSinkFunction aware of transaction timeouts.

TwoPhaseCommitSinkFunction allows to configure a transaction timeout. The
timeout can be used to log warnings if the transaction's age is appraoching
the timeout, and it can be used to swallow exceptions that are likely
irrecoverable. This commit also integrates these changes to the
FlinkKafkaProducer011.

This closes #4910.
",NotBuggy,"The commit message indicates an enhancement in the `TwoPhaseCommitSinkFunction`, specifically making it aware of transaction timeouts, including logging warnings and handling exceptions. 

However, the provided change is a modification in the type of `pendingTransaction` from `TXN` to `TransactionHolder<TXN>`. This implies a structural change to how transactions are managed, possibly to hold additional metadata such as timestamps related to transaction timeouts. 

While this change on its own doesn't directly indicate a bug being fixed, it likely supports the new functionality around transaction timeouts described in the commit message. The commit message does not state explicitly that it is fixing a bug, but it does describe enhancing the functionality in a way that could implicitly resolve related issues or prevent bugs tied to transaction handling by improving robustness and error acknowledgment mechanisms.

Given the analysis above and due to the lack of direct indication of existing buggy behavior being addressed in this diff alone, this specific change in isolation might not reflect a straightforward bug fix. Therefore, the conclusion based on the provided diff is **NotBuggy**."
eclipseJdt,25888.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,4 +1,4 @@
 public void goForHeaders(){
 	super.goForHeaders();
-	isFirst = true;
+	this.isFirst = true;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"This commit focuses on code style and clarity rather than functional changes. Adding `this.` to a field reference (like changing `isFirst` to `this.isFirst`) helps indicate it is a class member, but it doesn't alter the underlying logic or address a functional bug. The commit message indicates a clean-up related to code style adjustments rather than bug fixing. Therefore, the conclusion is **NotBuggy**."
eclipseJdt,20964.json,8245bd73c0aef7bf5f6de5def8941104b56db6f7,"@@ -1,3 +1,3 @@
 public int getNumberOfParameters() {
-	return fParameterTypes == null ? 0 : fParameterTypes.length;
+	return this.parameterTypes == null ? 0 : this.parameterTypes.length;
 }",NotBuggy,"*** empty log message ***
",Buggy,"The code change fixes a potential bug regarding variable naming. The change is correcting the reference from `fParameterTypes` to `this.parameterTypes`. This suggests a bug where the method would have referenced a possibly undefined `fParameterTypes` instead of the correct `this.parameterTypes`, which would lead to incorrect behavior. Therefore, even though the commit message is empty, the nature of the code change aligns with what would be considered a bug fix.

The conclusion is **Buggy**."
eclipseJdt,15535.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(SingleVariableDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message indicates that a comment is added to an empty block. The diff shows a comment ""// default implementation: do nothing"" added to the empty method endVisit. This aligns with the commit message and is not related to fixing a bug, but merely documents the empty block's intent. The changes do not address error-handling updates, logical corrections, exception-handling improvements, or any other signs of a bug-related fix.

Thus, the appropriate conclusion for this change is **NotBuggy**."
eclipseJdt,18964.json,527f8171257d427ef8c7c37dd3ea4724f920d7c5,"@@ -1,3 +1,3 @@
-boolean isKnownType(String qualifiedTypeName) {
+public boolean isKnownType(String qualifiedTypeName) {
 	return typeLocators.containsKey(qualifiedTypeName);
 }",NotBuggy,"Changes for API tools
",NotBuggy,"The diff shows a visibility change for the method `isKnownType`, from package-private to public. This modification is focused on altering the API exposure of the method rather than fixing a bug. There is no indication of any bug fix, error handling update, logical correction, or exception handling improvement in this change.

The commit message ""Changes for API tools"" correlates with the code change, as making a method public might be necessary for tools that rely on API access but doesn't indicate any bug-related fixes.

Thus, based on the provided change and commit message, the answer is **NotBuggy**."
eclipseJdt,5067.json,3c02a6af2c4b681d96ee9d070d231fb04b85b5fd,"@@ -1,3 +1,3 @@
 	public boolean isVarArgs() {
-		return (this.type.bits & IsVarArgs) != 0;
+		return this.type != null &&  (this.type.bits & IsVarArgs) != 0;
 	}",NotBuggy,"*** empty log message ***
",Buggy,"The code change addresses a potential null pointer exception in the method `isVarArgs()` by adding a null check for `this.type` before accessing its `bits` field. This prevents runtime errors when `this.type` is null, which is a common scenario when a bug exists. The absence of a commit message makes it unclear what specifically was intended, but the change clearly aims to improve robustness against null values, indicating a fix for a potential bug.

The update introduces error handling by preventing an exception (null pointer exception) that would otherwise occur if `this.type` is null. This is a classic symptom of a bug fix, as it resolves a logical issue in the code flow.

Therefore, the code modification aligns with typical bug-fix indicators such as exception handling and logical corrections. Thus, this change should be classified as **Buggy**."
eclipseJdt,19335.json,f69c200469412ca3b93e80844c6c1f4e077169ac,"@@ -1,3 +1,3 @@
-protected OpenableElementInfo createElementInfo() {
+protected Object createElementInfo() {
 	return new OpenableElementInfo();
 }",NotBuggy,"*** empty log message ***
",NotBuggy,"In this diff, we see a change in the return type of the `createElementInfo` method from `OpenableElementInfo` to `Object`. This modification can have significant implications:

1. **Type Safety**: Changing the return type to `Object` reduces type safety, as `OpenableElementInfo` must be explicitly cast to its original type when used.

2. **Flexibility**: This change could be intended to allow `createElementInfo` to return different types of objects in future implementations. However, this flexibility might not align with the intended use and can introduce runtime errors if improperly handled.

3. **Bug Indication**: The change might aim to fix a specific issue, but without a detailed commit message, identifying the exact reason for the modification is challenging. Given no context, it doesn't explicitly reflect error handling or bug correction, nor does it introduce exception handling or resolve logical flaws.

Ultimately, with the limited information provided in the diff and the absence of a commit message, it's difficult to conclusively identify this change as a bug fix. It might simply be a refactor or preparation for future code changes.

Thus, based on the analysis:

**NotBuggy**"
eclipseJdt,11399.json,f389464fbe74a4e5c3adfde8847a7200db37d80c,"@@ -1,3 +1,3 @@
 public int hash(int key) {
-	return (key & 0x7FFFFFFF) % keyTable.length;
+	return (key & 0x7FFFFFFF) % this.keyTable.length;
 }",NotBuggy,"HEAD - Use classfile pool to create class files
",NotBuggy,"The commit message suggests modifying the process of creating class files using a classfile pool, but the code diff shows a minor change within a `hash` method. Here, an expression updates to use `this.keyTable.length` instead of `keyTable.length`, implying that `keyTable` might be a class-level variable. This change clarifies the scope but doesn’t suggest bug fixing as such, instead offering a localization improvement.

The current change ensures that the correct class instance's `keyTable` is used, which might have been the underlying issue if any incorrect results were due to shadowing or incorrect field access. While this could potentially resolve unintended behavior if the codebase allows for shadowed variables, the provided diff itself does not strongly indicate a critical bug fix, nor does it relate to the commit message's supposed use case.

Considering these points, the diff and commit message do not align to suggest a bug fix concerning class files and therefore should be categorized as **NotBuggy**."
eclipseJdt,20530.json,046b7e91db887d0087d68280c71a1c85744c622c,"@@ -1,13 +1,13 @@
 	public char[][] fullInclusionPatternChars() {
 
-		if (this.fullCharInclusionPatterns == UNINIT_PATTERNS) {
+		if (this.fullInclusionPatternChars == UNINIT_PATTERNS) {
 			int length = this.inclusionPatterns.length;
-			this.fullCharInclusionPatterns = new char[length][];
+			this.fullInclusionPatternChars = new char[length][];
 			IPath prefixPath = this.path.removeTrailingSeparator();
 			for (int i = 0; i < length; i++) {
-				this.fullCharInclusionPatterns[i] = 
+				this.fullInclusionPatternChars[i] = 
 					prefixPath.append(this.inclusionPatterns[i]).toString().toCharArray();
 			}
 		}
-		return this.fullCharInclusionPatterns;
+		return this.fullInclusionPatternChars;
 	}",NotBuggy,"*** empty log message ***
",NotBuggy,"This diff modifies a method involved in handling inclusion patterns by correcting variable and method names. It consistently changes the variable name from `fullCharInclusionPatterns` to `fullInclusionPatternChars`. The old code uses a different naming convention, which could have caused misalignment between declarations and references if not corrected throughout. Although the change improves readability and consistency, no logic is altered that suggests an error in functionality being addressed.

The commit message is empty, which provides no context for understanding the purpose of this change. However, based on the diff alone, it appears that the modifications are more about refactoring for clearer naming rather than fixing an existing bug in the logic. There's no indication of a logical flaw, exception, or error condition corrected.

Therefore, my conclusion is **NotBuggy**."
eclipseJdt,21417.json,f69aa90ed015a30d4053155fb28339fdbd4fbd07,"@@ -1,8 +1,5 @@
 	public void _ldiv(int pc) {
-		writeTabs();
-		buffer
-			.append(pc)
-			.append(Util.bind(""disassembler.tab"")) //$NON-NLS-1$
-			.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
+		dumpPcNumber(pc);
+		buffer.append(OpcodeStringValues.BYTECODE_NAMES[IOpcodeMnemonics.LDIV]);
 		writeNewLine();
 	}",NotBuggy,"Improvement for disassembled string
",NotBuggy,"The diff simplifies the _ldiv method by replacing a sequence of chained method calls with a single call to dumpPcNumber and an append operation on the buffer. The new method likely consolidates handling of the program counter (pc) number, but without seeing its implementation, we can't analyze behavior changes, if any. The diff and the message point more to a code improvement or refactoring rather than a bug fix, as it aims to streamline the code for disassembled string representation without clear indication of rectifying a prior defect. Therefore, the conclusion is **NotBuggy**."
eclipseJdt,26771.json,f7a77c5e8210599982f60934db59bb901b30cb8c,"@@ -1,9 +1,9 @@
-	public void buildStarting(ICompilationParticipantResult[] files, boolean isBatch){
+	public void buildStarting(BuildContext[] files, boolean isBatch){
 		// this gets called multiple times during a build.
 		// This gets called:
 		// 1) after ""aboutToBuild"" is called.
         // 2) everytime an incremental build occur because of newly generated files
         // this gets called.
 		if( _buildRound == 0 )
 			_isBatch = isBatch;
 	}",NotBuggy,"HEAD - improve compilation participant API
",NotBuggy,"The commit message mentions improving the compilation participant API, and the code change reflects this. The modification replaces `ICompilationParticipantResult[]` with `BuildContext[]` as the parameter type for the `buildStarting` method. This suggests an improvement or refactor in the method signature, possibly to align with updated APIs or enhance functionality.

However, there's no indication that this change addresses a bug or error. The method's logic and structure remain unchanged, and the primary change is in the parameter's type, likely for API or architectural improvement purposes rather than bug-related reasons.

Therefore, the changes align with the commit message of API improvement, but they do not indicate a bug fix.

The answer is **NotBuggy**."
eclipseJdt,15494.json,d8d3d5200f27e422421ae0303798b8a7f8e75502,"@@ -1,2 +1,3 @@
 	public void endVisit(FieldDeclaration node) {
+		// default implementation: do nothing
 	}",NotBuggy,"Add comment to empty blocks
",NotBuggy,"The commit message states that a comment was added to an empty block, and the diff shows that a comment was indeed added to the endVisit method. The method itself is left unchanged, indicating no bug fixes or logic alterations, merely an addition of a comment to clarify the intended inaction in the method. The change aligns with the commit message, which accurately describes the modification as a non-functional comment addition.

The modification does not address bug fixing, error handling, or logical corrections. Thus, it does not constitute a bug fix.

Conclusion: **NotBuggy**"
eclipseJdt,16095.json,24c9e84b2ec4fea87b1bf2f444e5d541a0e3f479,"@@ -1,4 +1,4 @@
-	public boolean subtreeMatch(ASTMatcher matcher, Object other) {
+	final boolean subtreeMatch0(ASTMatcher matcher, Object other) {
 		// dispatch to correct overloaded match method
 		return matcher.match(this, other);
 	}",NotBuggy,"Reduce API footprint
",NotBuggy,"The commit message mentions reducing the API footprint. This is typically accomplished by restricting access to methods or changing their visibility to prevent them from being part of the public API surface. 

In the provided diff:

- The `subtreeMatch` method, which is likely a public API method given its visibility, was renamed to `subtreeMatch0` and made `final`. If it used to be `public`, this change possibly narrows its accessibility or clearly marks it as internal or non-overridable, aligning with the goal of reducing the API footprint by indicating it is not meant for public extension or overriding.

Overall, the intended purpose appears to align with the commit message. This change doesn't imply a bug fix; rather, it looks like an API change refactor.

**Conclusion: NotBuggy**"
eclipseJdt,19189.json,bd6803034b95b7e0dd8c0cbcd0aead0a5c726f65,"@@ -1,11 +1,11 @@
 private void rehash() {
-	QualifiedNameSet newSet = new QualifiedNameSet(elementSize * 2); // double the number of expected elements
+	QualifiedNameSet newSet = new QualifiedNameSet(this.elementSize * 2); // double the number of expected elements
 	char[][] current;
-	for (int i = qualifiedNames.length; --i >= 0;)
-		if ((current = qualifiedNames[i]) != null)
+	for (int i = this.qualifiedNames.length; --i >= 0;)
+		if ((current = this.qualifiedNames[i]) != null)
 			newSet.add(current);
 
 	this.qualifiedNames = newSet.qualifiedNames;
 	this.elementSize = newSet.elementSize;
 	this.threshold = newSet.threshold;
 }",NotBuggy,"HEAD - Clean-up pass 1: add 'this.' to fields +remove 'this.' to msg send + remove trailing white spaces
",NotBuggy,"The commit message indicates that the changes involve formatting adjustments, specifically adding 'this.' to fields, removing 'this.' from message sends, and cleaning up trailing white spaces. 

In the provided diff:
1. `this.` is added to `elementSize` and `qualifiedNames` to explicitly refer to instance fields.
2. There are no message send changes or trailing white space removals apparent in the diff.

The changes are consistent with the commit message's focus on code formatting. The diff does not indicate any logical changes or fixes that would imply a bug fix.

Thus, the changes in this diff are purely stylistic and for code clarity and consistency, rather than fixing a bug. Given the changes match the commit message dealing with clean-up without directly addressing a bug, the answer is **NotBuggy**."
eclipseJdt,21045.json,3b79f63e00cbebb1fe326313cd6905bd51bae92d,"@@ -1,3 +1,3 @@
 	public int getNameSourceEnd() {
-		return this.nameEnd;
+		return -1;
 	}",NotBuggy,"HEAD - 260257
",NotBuggy,"This commit changes the `getNameSourceEnd()` method to consistently return `-1` instead of the value of `this.nameEnd`. Such a change suggests a behavior modification but does not inherently indicate a bug fix. A bug fix would typically correct incorrect logic, handle exceptions more appropriately, or address some erroneous behavior.

To determine if this is truly a bug fix, we would need more context, such as whether returning `this.nameEnd` was causing issues in other parts of the code or if the use of `-1` in this context signals an error condition that needs to be accounted for elsewhere. 

Without additional context indicating that the old return value caused specific problems (like incorrect calculation or exception), it's challenging to conclude definitively that this change addresses a pre-existing bug. Since no such context is provided here, and the commit message (which is just a hash) also offers no information, we treat this as a code behavior change.

Given the information presented, this change seems to be altering functionality without sufficient evidence of a bug being fixed.

**NotBuggy**"
commons-lang,2261.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,6 +1,6 @@
-    public static Short[] nullToEmpty(Short[] array) {
+    public static Short[] nullToEmpty(final Short[] array) {
         if (array == null || array.length == 0) {
             return EMPTY_SHORT_OBJECT_ARRAY;
         }
         return array;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message clearly states the purpose of the change: adding the `final` modifier to method parameters. This modification is meant to signal that the parameter reference should not change within the method. It is a refactor aimed at maintaining code integrity and readability, rather than fixing any functional bugs.

In the code diff provided:
- The parameter `array` is given the `final` modifier, which ensures the reference to the array cannot be changed within the method. However, the content of the array can still be modified.
- The addition of the `final` keyword does not fix any logical errors or errors in handling, nor does it improve performance or correctness in the existing function's operations.

Since this change aligns with the commit message but does not indicate a bug fix, it should be classified as **NotBuggy**."
commons-lang,852.json,93a3b6b251429030ee0393a8483d2a225a7b0db2,"@@ -1,4 +1,4 @@
-    public ToStringBuilder<T> append(double value) {
+    public ToStringBuilder append(double value) {
         style.append(buffer, null, value);
         return this;
     }",NotBuggy,"Remove generics as they provide little value and get in the way

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@829635 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message describes removing generics from the method signature because they are deemed unnecessary and may complicate the code. The change in the diff aligns with the commit message by removing the generic type `<T>` from the `ToStringBuilder` class. 

This change does not indicate a bug fix. Instead, it's a design decision to simplify the API by removing what the author considers an unneeded generic declaration. No corrections to algorithmic logic, error handling, or bug fixes are evident in this diff.

Thus, the conclusion is **NotBuggy**."
commons-lang,2074.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,3 +1,3 @@
-    public static final Processor getProcessor() {
+    public static Processor getProcessor() {
         return getProcessor(SystemUtils.OS_ARCH);
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"The commit message ""Prevent redundant modifiers"" refers to removing unnecessary `static final` modifiers from the method `getProcessor()`. 

1. **Code Change Analysis**:
   - The original method was declared as `public static final`, implying it is a static method (shared across all instances) and ""final"" (cannot be overridden further if it were in a subclass context).
   - Removing the `static final` modifiers while keeping `public` still allows it to be accessed from outside its class but indicates it may not need to be a class-level method or constant. 
   - The `final` modifier is redundant for methods since the `static` keyword already allows calls without requiring an instance, and methods can't be overridden due to final together with static.

2. **Code Intent and Commit Message Alignment**:
   - The modification removes unnecessary method-level modifiers, aligning well with the commit message ""Prevent redundant modifiers.""
   - There are no bug fixes related to logic or functionality in the method. The change is purely about code simplification.

3. **Conclusion**:
   - The change doesn't fix a specific logical or functional bug but simplifies the code by removing redundancy.
   - Given this explanation, the change should be categorized as **NotBuggy**."
commons-lang,1759.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,10 +1,10 @@
-    public static String reverseDelimited(String str, char separatorChar) {
+    public static String reverseDelimited(final String str, final char separatorChar) {
         if (str == null) {
             return null;
         }
         // could implement manually, but simple way is to reuse other,
         // probably slower, methods.
         String[] strs = split(str, separatorChar);
         ArrayUtils.reverse(strs);
         return join(strs, separatorChar);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that the change involves adding the `final` modifier to method parameters. The provided code diff aligns with the message, as it shows the parameters `str` and `separatorChar` being marked as `final`. This change primarily aims at indicating that the parameters should not be reassigned within the method, which can help in preventing accidental modifications and improving code clarity.

However, the addition of the `final` keyword does not introduce any change in the logic or functionality of the method. It is employed here as a code style preference rather than addressing any wrong behavior or bug in the code. 

Therefore, the modifications in this commit do not reflect a bug fix.

The answer is **NotBuggy**."
commons-lang,772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static boolean isPrimitiveWrapper(Class<?> type) {
+    public static boolean isPrimitiveWrapper(final Class<?> type) {
         return wrapperPrimitiveMap.containsKey(type);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The provided commit message indicates that the change involves adding the `final` modifier to method parameters. This modification is a code improvement for defensive programming, ensuring that the parameter's reference cannot change within the method. However, such a change does not alter the functionality or fix any bugs within the code. The diff and the commit message align as both deal with adding the `final` modifier to the method parameter.

Since there is no indication of error-handling updates, logical corrections, or improvements related to bug fixes, the answer is **NotBuggy**."
commons-lang,1787.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String strip(String str) {
+    public static String strip(final String str) {
         return strip(str, null);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The diff shows the addition of the `final` keyword to the method parameter `str` in the `strip` method. Adding the `final` modifier to a method parameter prevents reassignment of that parameter within the method. This change is generally meant to improve code readability and maintainability but does not address any logic errors, bug fixes, or exception handling improvements in the code.

The commit message accurately describes the change (adding `final` modifiers), and there is no indication of the change fixing any bugs. Therefore, the analysis concludes that these changes are not indicative of a bug fix.

The answer is **NotBuggy**."
commons-lang,1773.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator( String str, String separator, int max ) {
+    public static String[] splitByWholeSeparator( final String str, final String separator, final int max ) {
         return splitByWholeSeparatorWorker(str, separator, max, false);
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates the addition of the `final` modifier to method parameters for immutability and potentially better optimization. This change does not correlate with a bug fix; instead, it's used to indicate that method parameters should not be modified within the method body, which is more about code style and safety rather than resolving a functional issue. The changes are consistent with the commit message and do not address a functional bug.

Therefore, the conclusion is **NotBuggy**."
commons-lang,1672.json,96c30e248d6d27a1df8e26be14aa10c633168cdb,"@@ -1,3 +1,3 @@
-    public static boolean isNoneBlank(CharSequence... css) {
+    public static boolean isNoneBlank(final CharSequence... css) {
       return !isAnyBlank(css);
     }",NotBuggy,"Use final consistently. 

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606051 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The provided diff shows a change where the method parameter `css` is now declared as `final`. Declaring a parameter as `final` means its reference cannot be changed within the method. This change is related to code style and immutability practice but does not affect the logic or behavior of the method itself regarding bug fixes or error corrections.

The commit message ""Use final consistently"" aligns with the change in the code, indicating that this refactor was to improve code consistency and style. There is no mention or indication of a bug fix, and the code change doesn't suggest a correction of any logic or error previously present.

Therefore, this change is stylistic and aimed at maintaining code standards rather than fixing a bug.

The conclusion is **NotBuggy**."
commons-lang,186.json,dd74cf1da57ed432ce38631baebe6882419b01e6,"@@ -1,19 +1,18 @@
     public static int distance(final Class<?> child, final Class<?> parent) {
         if (child == null || parent == null) {
             return -1;
         }
 
         if (child.equals(parent)) {
             return 0;
         }
 
         final Class<?> cParent = child.getSuperclass();
         int d = BooleanUtils.toInteger(parent.equals(cParent));
 
         if (d == 1) {
             return d;
-        } else {
-            d += distance(cParent, parent);
-            return d > 0 ? d + 1 : -1;
         }
+        d += distance(cParent, parent);
+        return d > 0 ? d + 1 : -1;
     }",NotBuggy,"Statement unnecessarily nested within else clause.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1606088 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates removing unnecessary nesting within an `else` clause. The code change precisely reflects this: the statement inside the `else` clause is adjusted for clarity and simplicity by being placed outside the `else`. 

There are no signs of a bug fix or changes in the logic or error handling of the function. The functionality remains unchanged; it's purely a refactor for code clarity. Therefore, this change aligns perfectly with the commit message, emphasizing cleanup rather than a bug fix.

Thus, the changes are **NotBuggy**."
commons-lang,2630.json,5292526e476ffbb19c6613a98464054236c86ace,"@@ -1,8 +1,8 @@
         public Character next() {
             if (hasNext == false) {
                 throw new NoSuchElementException();
             }
-            char cur = current;
+            final char cur = current;
             prepareNext();
             return Character.valueOf(cur);
         }",NotBuggy,"Add final modifier to local variables.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436770 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the change involves adding the `final` modifier to local variables, which aligns with the code change shown in the diff. The addition of `final` to the `cur` variable in this case is not related to a bug fix—it is primarily for style, potentially preventing accidental reassignment. There is no indication of an error-handling update, logical correction, exception-handling improvement, or any bug-related change directly in the code.

Therefore, this change does not align with being a bug fix; the commit is consistent with its message regarding style improvement but does not indicate a bug being fixed. The assessment concludes with **NotBuggy**."
commons-lang,136.json,c79f82491a8bae9c3b9895f296a8519cd0dfe7ff,"@@ -1,15 +1,19 @@
     public static void removeFinalModifier(Field field) {
         Validate.isTrue(field != null, ""The field must not be null"");
 
         try {
             if (Modifier.isFinal(field.getModifiers())) {
                 Field modifiersField = Field.class.getDeclaredField(""modifiers"");
                 modifiersField.setAccessible(true);
-                modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                try {
+                    modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);
+                } finally {
+                    modifiersField.setAccessible(false);
+                }
             }
         } catch (NoSuchFieldException ignored) {
             // The field class contains always a modifiers field
         } catch (IllegalAccessException ignored) {
              // The modifiers field is made accessible
         }
     }",NotBuggy,"removeFinalModifier does not clean up after itself.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1562985 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message refers to the method `removeFinalModifier` not cleaning up after itself. The diff shows an update within this method to ensure that the `modifiersField` accessibility is reset after attempting to change the field's modifiers.

Originally, the method modified accessibility without resetting it afterwards, potentially leaving the `modifiersField` in an unintended accessible state. The updated code now uses a `try`...`finally` block to ensure that the `setAccessible(false)` method is called, restoring the field's original accessibility state regardless of whether an exception occurs. 

The nature of these changes focuses on ensuring that resources or states modified within a method are properly cleaned up or restored, which is characteristic of a bug fix. The issue resolved here involves proper resource management and encapsulation integrity, which are crucial for method correctness and predictable behavior.

Thus, these changes align with the commit message's indication of cleaning up after itself, and it directly involves resolving an issue (bug) related to lingering side effects.

The answer is **Buggy**."
commons-lang,1942.json,3728344459e91509c4b5b80d67a50c98063e08bd,"@@ -1,14 +1,14 @@
-    public static short max(final short[] array) {
+    public static short max(final short... array) {
         // Validates input
         validateArray(array);
     
         // Finds and returns max
         short max = array[0];
         for (int i = 1; i < array.length; i++) {
             if (array[i] > max) {
                 max = array[i];
             }
         }
     
         return max;
     }",NotBuggy,"LANG-1008: Change min/max methods in NumberUtils/IEEE754rUtils from array input parameters to varargs. This also closes #23 from github. Thanks to Thiago Andrade.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1593118 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates a change from array input parameters to varargs for certain methods. The diff reflects this change by modifying the `max` method's parameter from a short array (`short[]`) to varargs (`short... array`). This modification aligns with the commit message and functionally remains the same, as varargs is effectively a syntactic sugar allowing multiple arguments to be passed as an array. There is no indication of bug fixes like error-handling updates, logical corrections, or exception handling in the specific change provided. 

The conversion to varargs simplifies the API without altering the core functionality or addressing any previously existing bugs. Therefore, based on the provided diff, this does not indicate a bug fix.

The conclusion is **NotBuggy**."
commons-lang,2067.json,3a818ed6a833f083a2db9bb6804c1bdb43b9b0ec,"@@ -1,4 +1,4 @@
-    private static final void init_X86_64Bit() {
+    private static void init_X86_64Bit() {
         Processor processor = new Processor(Processor.Arch.BIT_64, Processor.Type.X86);
         addProcessors(processor, ""x86_64"", ""amd64"", ""em64t"", ""universal"");
     }",NotBuggy,"Prevent redundant modifiers
",NotBuggy,"The diff shows the removal of the `final` modifier from a private static method declaration. The commit message states ""Prevent redundant modifiers."" In Java, the `final` modifier for a method is redundant for `private` methods because private methods cannot be overridden. This change improves code readability without affecting functionality or fixing a bug. Thus, the modification matches the commit message but does not indicate a bug fix. 

The conclusion is **NotBuggy**."
commons-lang,1223.json,9072a7615a54d5e753cf088326d9e14f1a5a6751,"@@ -1,7 +1,7 @@
     public EqualsBuilder append(final int lhs, final int rhs) {
         if (isEquals == false) {
             return this;
         }
-        isEquals = (lhs == rhs);
+        isEquals = lhs == rhs;
         return this;
     }",NotBuggy,"PMD: Useless parentheses

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1671669 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates that the changes involve removing useless parentheses to improve code readability, as flagged by a tool like PMD. The change itself is a removal of unnecessary parentheses around a simple comparison expression. This is a refactor for code clarity and does not imply any bug in functionality. Therefore, the changes do not indicate a bug fix.

Thus, the appropriate conclusion for this change is **NotBuggy**."
commons-lang,1772.json,5bd622dab027ef37001a630a7c825c5b8c19d1db,"@@ -1,3 +1,3 @@
-    public static String[] splitByWholeSeparator(String str, String separator) {
+    public static String[] splitByWholeSeparator(final String str, final String separator) {
         return splitByWholeSeparatorWorker( str, separator, -1, false ) ;
     }",NotBuggy,"Add final modifier to method parameters.

git-svn-id: https://svn.apache.org/repos/asf/commons/proper/lang/trunk@1436768 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message states that the final modifier is added to method parameters. This change in the diff aligns with that message by applying the final keyword to the parameters str and separator. Adding final makes it clear that the parameters will not be reassigned within the method, but it does not indicate a bug fix. Instead, it enhances code clarity and potential compiler optimizations. There are no signs of error-handling updates, logical corrections, exception-handling improvements, or other indications of addressing a bug.

The answer is **NotBuggy**."
elasticsearch,45328.json,bd7efa908ab69831757d45ca7519c286097777dc,"@@ -1,20 +1,20 @@
-    static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
+    public static int generateShardId(IndexMetaData indexMetaData, @Nullable String id, @Nullable String routing) {
         final String effectiveRouting;
         final int partitionOffset;
 
         if (routing == null) {
             assert(indexMetaData.isRoutingPartitionedIndex() == false) : ""A routing value is required for gets from a partitioned index"";
             effectiveRouting = id;
         } else {
             effectiveRouting = routing;
         }
 
         if (indexMetaData.isRoutingPartitionedIndex()) {
             partitionOffset = Math.floorMod(Murmur3HashFunction.hash(id), indexMetaData.getRoutingPartitionSize());
         } else {
             // we would have still got 0 above but this check just saves us an unnecessary hash calculation
             partitionOffset = 0;
         }
 
         return calculateScaledShardId(indexMetaData, effectiveRouting, partitionOffset);
     }",NotBuggy,"Add ability to split shards (#26931)

This change adds a new `_split` API that allows to split indices into a new
index with a power of two more shards that the source index.  This API works
alongside the `_shrink` API but doesn't require any shard relocation before
indices can be split.

The split operation is conceptually an inverse `_shrink` operation since we
initialize the index with a _syntetic_ number of routing shards that are used
for the consistent hashing at index time. Compared to indices created with
earlier versions this might produce slightly different shard distributions but
has no impact on the per-index backwards compatibility.  For now, the user is
required to prepare an index to be splittable by setting the
`index.number_of_routing_shards` at index creation time.  The setting allows the
user to prepare the index to be splittable in factors of
`index.number_of_routing_shards` ie. if the index is created with
`index.number_of_routing_shards: 16` and `index.number_of_shards: 2` it can be
split into `4, 8, 16` shards. This is an intermediate step until we can make
this the default. This also allows us to safely backport this change to 6.x.

The `_split` operation is implemented internally as a DeleteByQuery on the
lucene level that is executed while the primary shards execute their initial
recovery. Subsequent merges that are triggered due to this operation will not be
executed immediately. All merges will be deferred unti the shards are started
and will then be throttled accordingly.

This change is intended for the 6.1 feature release but will not support pre-6.1
indices to be split unless these indices have been shrunk before. In that case
these indices can be split backwards into their original number of shards.
",NotBuggy,"The code change involves making the method `generateShardId` public, but the accompanying commit message focuses on introducing a `_split` API to shard indices. There is no direct indication from the commit message or the code change that this modification is a bug fix. Changing the method's access level seems necessary for enabling functionality related to the split operation outside its initial package or class. However, there's no evidence indicating that the original state of the method as non-public caused an incorrect behavior or bug.

The commit message is about adding a new feature rather than fixing a bug. The change in the method's visibility could be meant to support the new API's functionality without correcting any issues in existing behavior.

Therefore, the modification is more aligned with expanding access for new functionalities rather than resolving a defect. It is supporting the new `_split` API but doesn't showcase bug symptoms or resolutions.

The answer is **NotBuggy**."
elasticsearch,38113.json,85c26d682a5a6fbaf345d66b9a783b78f4b1cc63,"@@ -1,19 +1,19 @@
             public boolean advanceExact(int doc) throws IOException {
                 if (bytesValues.advanceExact(doc)) {
                     count = bytesValues.docValueCount();
                     grow();
                     for (int i = 0; i < count; ++i) {
                         final BytesRef value = bytesValues.nextValue();
                         script.setNextAggregationValue(value.utf8ToString());
                         Object run = script.run();
-                        CollectionUtils.ensureNoSelfReferences(run);
+                        CollectionUtils.ensureNoSelfReferences(run, ""ValuesSource.BytesValues script"");
                         values[i].copyChars(run.toString());
                     }
                     sort();
                     return true;
                 } else {
                     count = 0;
                     grow();
                     return false;
                 }
             }",NotBuggy,"Call ensureNoSelfReferences() on _agg state variable after scripted metric agg script executions (#31044)

Previously this was called for the combine script only. This change checks for self references for
init, map, and reduce scripts as well, and adds unit test coverage for the init, map, and combine cases.

",Buggy,"The commit message states that the change involves calling `ensureNoSelfReferences()` on the `_agg` state variable after various script executions, and that this was previously done only for the combine script. 

In the provided diff, the code modification changes the `ensureNoSelfReferences(run)` method call to `ensureNoSelfReferences(run, ""ValuesSource.BytesValues script"")`. While this change appears to add a description to the method call, it doesn't directly show multiple script execution contexts like init, map, or reduce being modified. Instead, it reflects a potential enhancement to the existing call, perhaps improving debugging or tracing capabilities by including context information. 

However, the message refers to expanding a safety check, which involves ensuring that the `run` object does not refer back to itself, an important step in preventing bugs related to cyclic dependencies or recursion which can lead to runtime failures.

Given this reasoning, the commit message and the code modification appear aligned in the context of expanding or improving safety checks, albeit indirectly through descriptive updates, suggesting a broader application of a check could be perceived from other unspecified parts of the codebase as described in the commit message.

There is an indication that this change is part of a bug fix, as it addresses a self-referencing prevention issue that was previously only handled in limited scenarios. The improvement in error-checking is a good sign that the original logic may have been error-prone without these additional checks.

Therefore, based on the provided information and reasoning, this commit can be categorized as **Buggy**."
elasticsearch,38632.json,48b0908fc62a4047c8b254a3c3b52c973019561a,"@@ -1,3 +1,3 @@
-    Comparable<?>[] values() {
+    Comparable[] values() {
         return values;
     }",NotBuggy,"Make InternalComposite key comparable

Keys are compared in BucketSortPipelineAggregation so making key type (ArrayMap) implement Comparable. Maps are compared using the entry set's iterator so ordered maps order is maintain. For each entry first comparing key then value. Assuming all keys are strings. When comparing entries' values if type is not identical and\or type not implementing Comparable, throwing exception. Not implementing equals() and hashCode() functions as parent's ones are sufficient. Tests included.",NotBuggy,"The commit message suggests that the change relates to enhancing comparability for a composite key by implementing Comparable. The change in the diff involves altering the return type of the `values()` method from `Comparable<?>[]` to `Comparable[]`, removing the wildcard. Since the change does not include any typical bug-fix indications like error handling, logical corrections, or exception handling improvements, it aligns more with enhancements for new functionality, like making certain operations possible or safer. Given these considerations, this commit does not seem focused on fixing a specific bug but rather on expanding functionality or improving design to support a new requirement.

The answer is **NotBuggy**."
elasticsearch,12197.json,9e8cfbb40d7596eb56151fc8a30234fb336c0a43,"@@ -1,3 +1,3 @@
     public String getName() {
-        return ""xpack_ml_get_overall_buckets_action"";
+        return ""ml_get_overall_buckets_action"";
     }",NotBuggy,"[ML] Deprecate X-Pack centric ML endpoints (#36315)

This commit is part of our plan to deprecate and
ultimately remove the use of _xpack in the REST APIs.

Relates #35958",NotBuggy,"The commit message indicates a deprecation of X-Pack centric endpoints, which aligns with the code changes that remove ""xpack_"" from a method name. This does not indicate a bug fix but rather an effort towards cleaning up or refactoring the codebase to remove deprecated API usage. Therefore, the changes are focused on API deprecation and cleanup, rather than fixing a bug.

**NotBuggy**"
elasticsearch,39219.json,7bd2abe48af5a651b54bcd5bcb41c88e29390be0,"@@ -1,3 +1,3 @@
-    protected List<Bucket> getBucketsInternal() {
+    public List<Bucket> getBuckets() {
         return emptyList();
     }",NotBuggy,"Change Terms.Bucket to an interface (#24492)

This commit changes the Terms.Bucket abstract class to an interface, so
that it's easier for the Java High Level Rest Client to provide its own
implementation.

In its current state, the Terms.Bucket abstract class inherits from
InternalMultiBucketAggregation.InternalBucket which forces subclasses to
implement Writeable and exposes a public getProperty() method that relies
on InternalAggregation. This two points make it difficult for the Java
High Level Rest Client to implement the Terms and Terms.Bucket correctly.
This is also different from other MultiBucketsAggregation like Range
which are pure interfaces.

Changing Terms.Bucket to an interface causes a method clashes for the
`getBuckets()` method in InternalTerms. This is because:
 - InternalTerms implements Terms which declared a
 `List<Terms.Bucket> getBuckets()` method
 - InternalTerms extends InternalMultiBucketAggregation which declares a
 `List<? extends InternalBucket> getBuckets()` method
 - both overrides the MultiBucketsAggregation
 `List<? extends Bucket> getBuckets()` method

 There was no clashes before this change because Terms.Bucket extends
 InternalBucket and conformed to both declaration. With Terms.Bucket now
 an interface, the getBuckets() method in the Terms interface is changed
 to avoid method clash. This is a breaking change in the Java API but
 it's a straightforward change and the Terms multi bucket aggregation
 interface is also more coherent with the other Range, Histogram,
 Filters, AdjacencyMatrix etc that all return a `List<? extends Bucket>`.",NotBuggy,"The commit message describes altering `Terms.Bucket` from an abstract class to an interface to enable easier implementations by external clients. This requires method refactoring due to inheritance and interface mismatches, specifically with the `getBuckets()` method. The diff shows that a method previously named `getBucketsInternal()` was made public and overridden to return a `List<Bucket>`, eliminating internal versus external method naming issues and making it suitable for the interface `Terms.Bucket`.

This change addresses compatibility and implementation architecture rather than adjusting any existing faulty logic, error-handling, or exception-management. Consequently, this modification does not align with a bug fix but rather a design refactor intended to unbind architectural constraints.

Given this reasoning, the changes do not directly fix a bug but update the code structure for consistency and extendability, making the outcome **NotBuggy**."
elasticsearch,61546.json,b70f925ccb735dc84d59598de06df6bf35bd4bdc,"@@ -1,3 +1,3 @@
-    Configuration getConfiguration() {
-        return configuration;
+    public String getMajor() {
+        return major;
     }",NotBuggy,"Support downloading JDKs with legacy version format (#51587)

",NotBuggy,"The commit message mentions support for downloading JDKs with a legacy version format, which implies changes related to version-handling functionality. The code change, however, shows a modification from a method returning `Configuration` to a method returning `String major`. 

There is no clear indication that this modification is related to bug fixing. The change reflects an alteration of the interface's method signature by replacing a method that returns a `Configuration` with one that returns a `String`, presumably representing the major version.

Since this change appears to emphasize functionality modification rather than correcting erroneous behavior, it does not suggest a bug fix. Furthermore, the commit message suggests feature enhancement rather than fixing a specific bug. The naming indicates a possible intent related to handling version information, potentially aligning it with version format handling. However, this alone does not imply a correction of errors or failures in the existing system.

Thus, the assessment of this change is **NotBuggy**."
elasticsearch,39443.json,a0b004acb5965508782f5430c240f71580478832,"@@ -1,4 +1,4 @@
-    public IPv4RangeAggregatorFactory addRange(String key, String from, String to) {
+    public IPv4RangeAggregatorBuilder addRange(String key, String from, String to) {
         addRange(new Range(key, from, to));
         return this;
     }",NotBuggy,"Split AggregatorFactory into AggregatorBuilder and AggregatorFactory
",NotBuggy,"The commit message suggests refactoring by splitting a class into two entities. The code change renames a method's return type from `IPv4RangeAggregatorFactory` to `IPv4RangeAggregatorBuilder`, aligning with the commit's purpose of separating functionality. There's no indication of a bug fix, error handling, or logical correction in the code snippet provided. The primary intention looks to be a structural change rather than addressing a defect.

Thus, based on the provided information, the answer is **NotBuggy**."
elasticsearch,8046.json,2879e6717eedd6caad60e4fca3991ed4a9faad38,"@@ -1,11 +1,15 @@
     private static BytesReference filterSource(FetchSourceContext fetchSourceContext, BytesReference source) throws IOException {
+        if (fetchSourceContext.includes().length == 0 && fetchSourceContext.excludes().length == 0) {
+            return source;
+        }
+
         Set<String> includes = Set.of(fetchSourceContext.includes());
         Set<String> excludes = Set.of(fetchSourceContext.excludes());
 
         XContentBuilder builder =
             new XContentBuilder(XContentType.SMILE.xContent(), new BytesStreamOutput(source.length()), includes, excludes);
         XContentParser sourceParser = XContentHelper.createParser(NamedXContentRegistry.EMPTY,
             DeprecationHandler.THROW_UNSUPPORTED_OPERATION, source, XContentType.SMILE);
         builder.copyCurrentStructure(sourceParser);
         return BytesReference.bytes(builder);
     }",NotBuggy," Enrich processor configuration changes (#45466)

Enrich processor configuration changes:
* Renamed `enrich_key` option to `field` option.
* Replaced `set_from` and `targets` options with `target_field`.

The `target_field` option behaves different to how `set_from` and
`targets` worked. The `target_field` is the field that will contain
the looked up document.

Relates to #32789",NotBuggy,"The commit message discusses configuration changes related to options like `enrich_key`, `set_from`, `targets`, and `target_field`, and doesn't mention bug fixes. The diff adds an early return in `filterSource` when both the `includes` and `excludes` arrays are empty, which could be a performance enhancement or correct behavior when no filtering is needed, but the commit message does not mention performance or logic issues this change addresses. Hence, there's no clear indication the change is fixing a bug. The commit revolves around configuration adjustments, and the code change seems related to skipping unnecessary processing.

Therefore, the code change does not appear to directly address any bug fixed by this commit. The assessment concludes with **NotBuggy**."
elasticsearch,63479.json,df2acb3d9d81118e5dcf64979e0fa716beec8164,"@@ -1,10 +1,10 @@
     protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
                             ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, MULTIVALUE_MODE_FIELD)) {
+        if (MULTIVALUE_MODE_FIELD.match(currentFieldName)) {
             if (token == XContentParser.Token.VALUE_STRING) {
                 otherOptions.put(MULTIVALUE_MODE_FIELD, parser.text());
                 return true;
             }
         }
         return false;
     }",NotBuggy,"Remove some more usages of ParseFieldMatcher in favour of using ParseField directly

Relates to #19552
Relates to #22130
",NotBuggy,"The commit message indicates a shift from using `ParseFieldMatcher` to utilizing `ParseField` directly. This change substitutes a method call to `parseFieldMatcher.match(currentFieldName, MULTIVALUE_MODE_FIELD)` with `MULTIVALUE_MODE_FIELD.match(currentFieldName)`. 

Key points:

1. **Functionality Coverage**: 
   - The change uses `ParseField`'s internal `match` method directly, which should provide equivalent functionality if `parseFieldMatcher` was used only for this standard match process.
   
2. **Alignment With Commit Message**:
   - The revision aligns with the commit message goal of eliminating unnecessary `ParseFieldMatcher` usages.

3. **Bug Indicators**:
   - The primary focus here is code simplification and direct API utilization rather than functionality correction or bug fixing.
   - No indications of error-handling changes or logic correction typically indicative of a bug resolution.

Given that the changes are centered around refactoring rather than fixing incorrect behavior, this commit is more about improving code maintainability and simplicity. Therefore, it does not demonstrate a bug fix. The appropriate conclusion is **NotBuggy**."
elasticsearch,26493.json,78f9af19c668886c682d5d1d74f5fb6f2a4c5e77,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(accessToken, refreshToken);
+        return Objects.hash(accessToken, refreshToken, realmName, username);
     }",NotBuggy,"Invalidate Token API enhancements - HLRC (#36362)

* Adds Invalidate Token API enhancements to HLRC

Relates: #35388
",NotBuggy,"The commit message refers to adding enhancements to the Invalidate Token API within HLRC. The change in the diff updates the `hashCode` method to include additional fields: `realmName` and `username`. This could be part of the API's enhancement to ensure that hashing accounts for more attributes, likely improving functionality reliability.

However, this change does not directly reflect a bug fix, as it is an enhancement. The `hashCode` update would ensure that instances are uniquely represented, but this alone doesn't suggest that there was an erroneous behavior previously—though it might prevent future issues related to hash collisions or equality checks.

Considering the changes and the nature of the commit message, the modifications don't inherently demonstrate a correction of a bug; instead, they align more with enhancement efforts:

- The `hashCode` modification suggests improved identity representation.
- No specific bug or incorrect behavior is evident from this singular change.

Therefore, the conclusion is **NotBuggy**."
elasticsearch,25327.json,92eb324776f26e0edf8b7957c81950890b8ce4d9,"@@ -1,9 +1,9 @@
     public CancelTasksResponse cancel(CancelTasksRequest cancelTasksRequest, RequestOptions options ) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(
             cancelTasksRequest,
             RequestConverters::cancelTasks,
             options,
-            parser -> CancelTasksResponse.fromXContent(parser),
+            CancelTasksResponse::fromXContent,
             emptySet()
         );
     }",NotBuggy,"REST high-level Client: remove deprecated API methods (#31200)

This commit removes all the API methods that accept a `Header` varargs
argument, in favour of the newly introduced API methods that accept a
`RequestOptions` argument.

Relates to #31069",NotBuggy,"The provided commit message describes the removal of deprecated API methods taking `Header` varargs, favoring methods accepting `RequestOptions`. This change, however, only refactors the lambda expression passed to `restHighLevelClient.performRequestAndParseEntity` by using a method reference instead of a lambda expression (`parser -> CancelTasksResponse.fromXContent(parser)` to `CancelTasksResponse::fromXContent`). The change does not directly relate to removing deprecated methods or suggest a bug fix, as the primary update is a syntactical refinement that doesn't alter functionality.

Hence, based on the information provided in this diff and the commit message, this is not indicative of a bug fix. Therefore, the conclusion is **NotBuggy**."
elasticsearch,44847.json,690136327fe2b08c14dcf7bcd140ceb6f10ae96f,"@@ -1,7 +1,6 @@
         public synchronized void reset() {
             super.reset();
-            version = UNKNOWN;
             fileDetails.clear();
             sourceThrottlingInNanos = UNKNOWN;
             targetThrottleTimeInNanos = UNKNOWN;
         }",NotBuggy,"Cleanup Deadcode o.e.indices (#44931)

* none of this is used anywhere",NotBuggy,"The commit message indicates dead code cleanup, and the diff shows the removal of the line `version = UNKNOWN;` from the `reset` method. The comment in the commit message suggests that `version` is not used elsewhere, effectively making this line a piece of dead code. Removing unused code is a cleanup activity rather than a bug fix, as the elimination of this code does not resolve any incorrect behavior but rather improves code quality by removing redundancies. There is no indication that this change corrects any malfunction or erroneous logic.

Therefore, the relevance between the commit message and the code modifications indicates that this change is a refactor for the sake of code cleanliness rather than a response to a bug.

Concluding, the answer is **NotBuggy**."
elasticsearch,24653.json,28bb1a3ac0c529beb38cd3553b50e095a07df13b,"@@ -1,5 +1,5 @@
     public final GraphExploreResponse explore(GraphExploreRequest graphExploreRequest,
                                                              RequestOptions options) throws IOException {
         return restHighLevelClient.performRequestAndParseEntity(graphExploreRequest, GraphRequestConverters::explore,
-                options, GraphExploreResponse::fromXContext, emptySet());
+                options, GraphExploreResponse::fromXContent, emptySet());
     }",NotBuggy,"XPack/HLRC request/response compatibility tests (#34547)


Relates #34451 , #29827
",Buggy,"The commit message suggests that this change is about adding request/response compatibility tests. However, the actual change in the diff corrects a method reference from `fromXContext` to `fromXContent` in the `explore` function.

While the commit message refers to something broader than the changes shown, let's analyze the actual code change:

- The change from `GraphExploreResponse::fromXContext` to `GraphExploreResponse::fromXContent` likely corrects a bug. It replaces a possibly incorrect method reference (`fromXContext`) with the correct one (`fromXContent`), which seems to address how responses are parsed or processed.

This looks like a bug fix because:
1. It changes a method reference, which might have caused incorrect behavior at runtime (such as failing to parse a response correctly).
2. The context of response handling suggests that this change corrects an error where a mistaken method might have been used originally.

Thus, despite the overall commit message focusing on compatibility tests, this specific change aligns with a bug fix. Therefore, I conclude **Buggy**."
elasticsearch,55784.json,7e3cd6a01931a95bfad785490f3319ed79f258f4,"@@ -1,5 +1,17 @@
-    public Status getStatus() {
-        return new Status(sliceId, total.get(), updated.get(), created.get(), deleted.get(), batch.get(), versionConflicts.get(),
-                noops.get(), bulkRetries.get(), searchRetries.get(), timeValueNanos(throttledNanos.get()), getRequestsPerSecond(),
-                getReasonCancelled(), throttledUntil());
+    public BulkByScrollTask.Status getStatus() {
+        return new BulkByScrollTask.Status(
+            sliceId,
+            total.get(),
+            updated.get(),
+            created.get(),
+            deleted.get(),
+            batch.get(),
+            versionConflicts.get(),
+            noops.get(),
+            bulkRetries.get(),
+            searchRetries.get(),
+            timeValueNanos(throttledNanos.get()),
+            getRequestsPerSecond(),
+            task.getReasonCancelled(),
+            throttledUntil());
     }",NotBuggy,"reindex: automatically choose the number of slices (#26030)

In reindex APIs, when using the `slices` parameter to choose the number of slices, adds the option to specify `slices` as ""auto"" which will choose a reasonable number of slices. It uses the number of shards in the source index, up to a ceiling. If there is more than one source index, it uses the smallest number of shards among them.

This gives users an easy way to use slicing in these APIs without having to make decisions about how to configure it, as it provides a good-enough configuration for them out of the box. This may become the default behavior for these APIs in the future.
",NotBuggy,"The commit message describes adding a feature to automatically select the number of slices in reindex APIs when the ""slices"" parameter is set to ""auto"". However, the provided code diff modifies the return of `getStatus()` from simply returning `Status` to returning `BulkByScrollTask.Status`, with likely no change in behavior other than perhaps refactoring for improved type safety or clarity. 

There are no visible changes related to automatically determining the number of slices or handling parameters set to ""auto"". The changes are more about code structure and maintaining or updating references rather than enhancing functionality regarding slices configuration. 

Therefore, the provided changes do not directly address a bug fix; instead, they seem to streamline or refactor existing code structures for defining status objects. They do not appear to relate to error-handling or logical corrections indicative of a bug fix with the ""slices"" functionality mentioned in the commit. 

The answer is **NotBuggy**."
elasticsearch,33161.json,d8b4556e2dfa48b38baf614e3ac99196be07055c,"@@ -1,39 +1,12 @@
             public Scorer scorer(LeafReaderContext context) throws IOException {
                 Scorer subQueryScorer = subQueryWeight.scorer(context);
                 if (subQueryScorer == null) {
                     return null;
                 }
-                final LeafScoreFunction leafFunction = function.getLeafScoreFunction(context);
-                Scorer scriptScorer = new Scorer(this) {
-                    @Override
-                    public float score() throws IOException {
-                        int docId = docID();
-                        float subQueryScore = subQueryScoreMode == ScoreMode.COMPLETE ? subQueryScorer.score() : 0f;
-                        float score = (float) leafFunction.score(docId, subQueryScore);
-                        if (score == Float.NEGATIVE_INFINITY || Float.isNaN(score)) {
-                            throw new ElasticsearchException(
-                                ""script score query returned an invalid score: "" + score + "" for doc: "" + docId);
-                        }
-                        return score;
-                    }
-                    @Override
-                    public int docID() {
-                        return subQueryScorer.docID();
-                    }
-
-                    @Override
-                    public DocIdSetIterator iterator() {
-                        return subQueryScorer.iterator();
-                    }
-
-                    @Override
-                    public float getMaxScore(int upTo) {
-                        return Float.MAX_VALUE; // TODO: what would be a good upper bound?
-                    }
-                };
+                Scorer scriptScorer = makeScriptScorer(subQueryScorer, context, null);
 
                 if (minScore != null) {
                     scriptScorer = new MinScoreScorer(this, scriptScorer, minScore);
                 }
                 return scriptScorer;
             }",NotBuggy,"Add explanations to script score queries (#46693)

While function scores using scripts do allow explanations, they are only
creatable with an expert plugin. This commit improves the situation for
the newer script score query by adding the ability to set the
explanation from the script itself.

To set the explanation, a user would check for `explanation != null` to
indicate an explanation is needed, and then call
`explanation.set(""some description"")`.
",NotBuggy,"The commit message indicates an improvement related to setting explanations in script score queries, specifically targeting better integration with explanations in scripts. 

The changes in the diff primarily showcase a refactoring of the `scorer` method within a Java class. The previous inline creation of a `Scorer` instance has been replaced by a call to `makeScriptScorer`. This refactoring improves readability and maintenance but doesn't directly address bug fixes. The method `makeScriptScorer` suggests a consolidation of scorer creation logic, potentially integrating explanation handling. However, the diff itself doesn't directly showcase enhancements to error handling, bug fixing, or correcting previous faulty logic.

Given the context and the commit message, this change seems to address refactoring for improved functionality or usability by abstracting scorer creation but doesn't explicitly address bug-related concerns. The alignment of the code changes with the commit message suggests improvements but not bug fixations.

Thus, based on the evidence directly available, the changes are **NotBuggy**."
checkstyle,1745.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,11 +1,11 @@
-    private static String getInstanceName(DetailAST aMethodCallAst)
+    private static String getInstanceName(DetailAST methodCallAst)
     {
         final String methodCallName =
-                FullIdent.createFullIdentBelow(aMethodCallAst).getText();
+                FullIdent.createFullIdentBelow(methodCallAst).getText();
         final int lastDotIndex = methodCallName.lastIndexOf('.');
         String instanceName = """";
         if (lastDotIndex != -1) {
             instanceName = methodCallName.substring(0, lastDotIndex);
         }
         return instanceName;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The commit message does not specify a bug fix; instead, it implies a general update or refactor relating to prefixes and possibly addressing an issue labeled #512. The code changes involve renaming a parameter from `aMethodCallAst` to `methodCallAst`, which does not alter functionality but increases code readability or style compliance.

There are no changes to logic error handling or correctness and no indication of a bug fix in the commit message or the code. Hence, the code modification is a stylistic improvement, not a bug fix.

The conclusion is **NotBuggy**."
checkstyle,1737.json,bb1b29851f14bf7eaf5656e01c285c0f5f3b9000,"@@ -1,4 +1,4 @@
-    public void setAllowedDistance(int aAllowedDistance)
+    public void setAllowedDistance(int allowedDistance)
     {
-        this.mAllowedDistance = aAllowedDistance;
+        this.allowedDistance = allowedDistance;
     }",NotBuggy,"Prefixes, coding, #512
",NotBuggy,"The provided code diff shows a straightforward renaming of a method parameter and an instance variable from `aAllowedDistance` and `mAllowedDistance` to `allowedDistance`. This seems to be a code cleanup effort to improve naming conventions rather than a bug fix. There are no indications of changes in logic, error handling, or functionality associated with bug fixing.

The commit message does not imply a bug fix but instead references ""Prefixes, coding, #512,"" which suggests either a task or ticket number but provides no specific clues about a bug.

Therefore, based on the diff and the commit message, this change appears to focus on code readability or consistency improvements rather than addressing a defect. 

Thus, the conclusion is **NotBuggy**."
checkstyle,2819.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,10 +1,10 @@
     private static DetailAST findTokenWhichBeginsTheLine(DetailAST root) {
-        DetailAST tokenWhichBeginsTheLine;
+        final DetailAST tokenWhichBeginsTheLine;
         if (isUsingOfObjectReferenceToInvokeMethod(root)) {
             tokenWhichBeginsTheLine = findStartTokenOfMethodCallChain(root);
         }
         else {
             tokenWhichBeginsTheLine = root.getFirstChild().findFirstToken(TokenTypes.IDENT);
         }
         return tokenWhichBeginsTheLine;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message mentions enforcing a style rule (FinalLocalVariableCheck) rather than bug fixes. The code change adds the `final` modifier to a local variable, improving code clarity and constancy but not altering logic or fixing bugs. The answer is **NotBuggy**."
checkstyle,627.json,b8ca6a585b824e91b3b8c72dd5cc53c0eb0ab0f1,"@@ -1,11 +1,11 @@
     private void tagSuppressions(Collection<TextBlock> comments)
     {
         for (TextBlock comment : comments) {
             final int startLineNo = comment.getStartLineNo();
             final String[] text = comment.getText();
-            tagCoontLine(text[0], startLineNo, comment.getStartColNo());
+            tagCommentLine(text[0], startLineNo, comment.getStartColNo());
             for (int i = 1; i < text.length; i++) {
-                tagCoontLine(text[i], startLineNo + i, 0);
+                tagCommentLine(text[i], startLineNo + i, 0);
             }
         }
     }",NotBuggy,"Prefixes, checkstyle, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/TreeWalker.java
",NotBuggy,"This commit message mentions ""Prefixes"" and ""checkstyle"" without indicating a specific bug fix. The diff shows a straightforward renaming of the method from `tagCoontLine` to `tagCommentLine`, likely correcting a typographical error. Although such a change can prevent potential bugs due to confusion or missing methods, the commit message itself doesn't specify a bug fix, and the change is primarily a refactoring to clarify code. Therefore, without explicit context of a bug resolution beyond a typo fix, the answer is **NotBuggy**."
checkstyle,2956.json,58219b9bd31c8f7fbc4f4bba0eeb811851f276b3,"@@ -1,12 +1,12 @@
-    private boolean isIgnoredParam(DetailAST aParamDef)
+    private boolean isIgnoredParam(DetailAST paramDef)
     {
         boolean result = false;
-        if (mIgnorePrimitiveTypes) {
-            final DetailAST parameterType = aParamDef.
+        if (ignorePrimitiveTypes) {
+            final DetailAST parameterType = paramDef.
                     findFirstToken(TokenTypes.TYPE).getFirstChild();
-            if (mPrimitiveDataTypes.contains(parameterType.getType())) {
+            if (primitiveDataTypes.contains(parameterType.getType())) {
                 result = true;
             }
         }
         return result;
     }",NotBuggy,"Prefixes, checks, #512

Conflicts:
	src/main/java/com/puppycrawl/tools/checkstyle/checks/NewlineAtEndOfFileCheck.java
",NotBuggy,"The commit message ""Prefixes, checks, #512"" and conflict resolution note don't indicate a bug fix. The code changes involve renaming variables by removing a prefix (`m`) and using camelCase, making the codebase more consistent with Java naming conventions. There's no evident logic or functionality change, error-handling improvement, or bug correction in the code segment shown. 

The changes made are purely for stylistic or organizational consistency rather than fixing any functional issues. 

The conclusion is **NotBuggy**."
checkstyle,1180.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,9 +1,9 @@
-    private boolean isEmptyLoop(DetailAST aAST, int aParentType)
+    private boolean isEmptyLoop(DetailAST ast, int parentType)
     {
-        return mAllowEmptyLoops
-            && (isEmptyBlock(aAST, aParentType, TokenTypes.LITERAL_FOR)
-                    || isEmptyBlock(aAST,
-                            aParentType, TokenTypes.LITERAL_WHILE)
-                            || isEmptyBlock(aAST,
-                                    aParentType, TokenTypes.LITERAL_DO));
+        return allowEmptyLoops
+            && (isEmptyBlock(ast, parentType, TokenTypes.LITERAL_FOR)
+                    || isEmptyBlock(ast,
+                            parentType, TokenTypes.LITERAL_WHILE)
+                            || isEmptyBlock(ast,
+                                    parentType, TokenTypes.LITERAL_DO));
     }",NotBuggy,"Prefixes, whitespace, #512
",NotBuggy,"The commit message ""Prefixes, whitespace, #512"" with no mention of a bug fix, combined with code changes that only involve renaming variables (`mAllowEmptyLoops` to `allowEmptyLoops` and `aAST` to `ast`, `aParentType` to `parentType`) indicates that these changes are likely cosmetic or stylistic. These edits do not actively resolve any logical or error-related issues in the code.

Given the lack of error-handling improvements, logical corrections, or exception-handling changes, the code modifications do not suggest a bug fix. Therefore, the changes do not align with a bug-fixing intention based on the provided diff and commit message.

The conclusion is **NotBuggy**."
checkstyle,2157.json,750b733bd124eaea34d4692a1bb8d0fd0fe90976,"@@ -1,4 +1,4 @@
-    public void setIgnoreName(boolean aValue)
+    public void setIgnoreName(boolean value)
     {
-        mIgnoreName = aValue;
+        ignoreName = value;
     }",NotBuggy,"Prefixes, naming, #512
",NotBuggy,"The commit message ""Prefixes, naming, #512"" suggests a focus on refactoring variable names and possibly addressing an issue referenced by ""#512"". The code diff involves renaming a method parameter and corresponding class variable from `aValue` and `mIgnoreName` to `value` and `ignoreName`, respectively. This change is a code clean-up activity meant to improve readability and adhere to naming conventions rather than a bug fix. The use of `aValue` and `mIgnoreName` was likely non-standard naming, and it has been corrected to improve clarity.

There are no evident error-handling modifications, logical corrections, or exception-handling improvements in the provided diff, so this does not indicate a bug fix in the traditional sense of correcting faulty behavior.

Therefore, the assessment based on this diff and commit message is **NotBuggy**."
checkstyle,2400.json,50c9bb69bcd93e6fbf272670e8b9d95f2f451c5f,"@@ -1,4 +1,4 @@
-    public void setAllowMarkerInterfaces(boolean aFlag)
+    public void setAllowMarkerInterfaces(boolean flag)
     {
-        mAllowMarkerInterfaces = aFlag;
+        allowMarkerInterfaces = flag;
     }",NotBuggy,"Prefixes, design, #512
",NotBuggy,"The commit message ""Prefixes, design, #512"" suggests that the changes might be part of a broader refactoring or design consistency task, possibly in response to an issue or feature request tagged as #512. 

Analyzing the code changes:
- The method parameter name changes from `aFlag` to `flag`.
- The instance variable name changes from `mAllowMarkerInterfaces` to `allowMarkerInterfaces`.

These changes are purely cosmetic or stylistic, focusing on renaming for consistency and clearer coding standards. Such changes do not indicate any fixing of a bug but rather enhance code readability or align with coding guidelines.

Since there are no logical, error-handling, exception-handling, or functional changes in the diff, the changes do not reflect a bug fix.

Therefore, the conclusion is **NotBuggy**."
checkstyle,1279.json,d97a2df674716efb7deda50180ae933bdc46d68a,"@@ -1,4 +1,4 @@
-    public void setEachLine(boolean aEachLine)
+    public void setEachLine(boolean eachLine)
     {
-        mEachLine = aEachLine;
+        this.eachLine = eachLine;
     }",NotBuggy,"Prefixes, whitespace, #512
",NotBuggy,"The given commit message ""Prefixes, whitespace, #512"" is vague and does not offer specific information about a bug fix. The diff shows a simple renaming of a parameter and instance variable from `aEachLine` to `eachLine`, which is a code style improvement rather than a bug fix. There are no changes affecting functionality, error handling, or logical flaws that would indicate a bug. 

Thus, the change is more focused on code readability and potentially addressing style guidelines, rather than fixing any bugs. Therefore, this commit does not align with a bug fix, based on both the changes and the commit message content.

The conclusion is **NotBuggy**."
checkstyle,2496.json,16dac636b246c658cad31ee9b2e45c53b851d4f2,"@@ -1,4 +1,4 @@
-    public final void setSortImportsInGroupAlphabetically(boolean aValue)
+    public final void setSortImportsInGroupAlphabetically(boolean value)
     {
-        mSortImportsInGroupAlphabetically = aValue;
+        sortImportsInGroupAlphabetically = value;
     }",NotBuggy,"Prefixes, imports, #512
",NotBuggy,"The provided commit diff shows a change in the naming of a method parameter and a corresponding instance variable. The parameter name has been changed from `aValue` to `value`, and the instance variable name has been updated from `mSortImportsInGroupAlphabetically` to `sortImportsInGroupAlphabetically`.

Analyzing these changes:
- **Parameter Rename**: This change improves the readability and consistency with typical naming conventions. There is no indication of a bug in parameter naming conventions; this is a matter of style.
  
- **Instance Variable Rename**: Changing `mSortImportsInGroupAlphabetically` to `sortImportsInGroupAlphabetically` seems to be for consistency or style purposes as well. This is likely to conform to a general naming style (e.g., removing Hungarian notation or prefixes like `m` for member variables).

These modifications do not reflect any error-handling updates, logical corrections, or fixes related to improper function. They are more aligned with style consistency and readability improvements.

Considering the information and focusing on the commit's changes, these reflect a refactor in naming conventions rather than addressing logical or functional bugs.

Thus, the conclusion is **NotBuggy**."
checkstyle,2285.json,5f68bb50fd191fec409f5edb90755a18fce3e0ac,"@@ -1,6 +1,12 @@
     public void beginTree(DetailAST rootAst)
     {
         immutableClassShortNames.clear();
-        final List<String> shortNames = getClassShortNames(immutableClassCanonicalNames);
-        immutableClassShortNames.addAll(shortNames);
+        final List<String> classShortNames =
+                getClassShortNames(immutableClassCanonicalNames);
+        immutableClassShortNames.addAll(classShortNames);
+
+        ignoreAnnotationShortNames.clear();
+        final List<String> annotationShortNames =
+                getClassShortNames(ignoreAnnotationCanonicalNames);
+        ignoreAnnotationShortNames.addAll(annotationShortNames);
     }",NotBuggy,"Visibility Modifier Check, ignore annotated fields
",NotBuggy,"The commit message states ""Visibility Modifier Check, ignore annotated fields,"" and the changes in the code seem related to handling annotations for ignoring certain fields. Here's a detailed breakdown:

1. **ignoreAnnotationShortNames.clear();**: This line initializes the `ignoreAnnotationShortNames` list by clearing any existing data. This indicates preparation to handle new data or reset its state.

2. **final List<String> annotationShortNames = getClassShortNames(ignoreAnnotationCanonicalNames);**: Retrieves a list of short names from the `ignoreAnnotationCanonicalNames`. This suggests that there are canonical names of annotations that need to be considered for some processing, likely related to ignoring fields marked with these annotations.

3. **ignoreAnnotationShortNames.addAll(annotationShortNames);**: Populates the `ignoreAnnotationShortNames` with the list obtained, aligning with the commit message's suggestion to check visibility modifiers and ignore certain annotated fields.

Although the message does not mention a bug directly, the changes made could be integral to fixing potential oversight wherein fields marked with specific annotations were not previously ignored as intended in visibility checks.

Considering that the message describes a functional update rather than directly addressing a defect, and without additional context to indicate a pre-existing bug, the changes appear more corrective and feature-oriented rather than explicit bug-fixing.

Therefore, conclude with **NotBuggy**."
checkstyle,1285.json,0d8e93ce79ea33b72b3e904d8796984271ee3d8d,"@@ -1,17 +1,17 @@
     private static DetailAST getWhitespaceFollowedNode(DetailAST ast) {
-        DetailAST whitespaceFollowedAst;
+        final DetailAST whitespaceFollowedAst;
         switch (ast.getType()) {
             case TokenTypes.TYPECAST:
                 whitespaceFollowedAst = ast.findFirstToken(TokenTypes.RPAREN);
                 break;
             case TokenTypes.ARRAY_DECLARATOR:
                 whitespaceFollowedAst = getArrayDeclaratorPreviousElement(ast);
                 break;
             case TokenTypes.INDEX_OP:
                 whitespaceFollowedAst = getIndexOpPreviousElement(ast);
                 break;
             default:
                 whitespaceFollowedAst = ast;
         }
         return whitespaceFollowedAst;
     }",NotBuggy,"minor: enforce FinalLocalVariableCheck on checkstyle code
",NotBuggy,"The commit message indicates an enforcement of the `FinalLocalVariableCheck` rule, which requires local variables to be marked as `final` where possible. The changes in the diff reflect this enforcement by adding the `final` keyword to the `whitespaceFollowedAst` variable declaration.

This change is purely related to style and adherence to coding conventions. It does not alter the logic or behavior of the code, nor does it fix any bugs. Its purpose is to improve code maintainability and robustness by indicating that the variable will not be reassigned after its initial assignment.

Therefore, the modifications do not indicate a bug fix. This change is categorized as **NotBuggy**."
checkstyle,2860.json,1bd7f9bdf3dc678b70e79481971aebc50db69dc7,"@@ -1,4 +1,4 @@
-    public final void setAllowSamelineMultipleAnnotations(boolean aAllow)
+    public final void setAllowSamelineMultipleAnnotations(boolean allow)
     {
-        mAllowSamelineMultipleAnnotations = aAllow;
+        allowSamelineMultipleAnnotations = allow;
     }",NotBuggy,"Prefixes, annotation, #512
",NotBuggy,"The commit message ""Prefixes, annotation, #512"" is vague and doesn't specify a particular bug fix. Examining the diff, the changes involve renaming a method parameter and its corresponding class member variable, likely for consistency or clarity. The logic of the method itself remains unchanged. These modifications suggest a refactoring for naming conventions, not the correction of a functionality or logic error.

No indicators, such as error-handling or logical corrections, imply a bug has been addressed. Since the update doesn't align with the usual characteristics of a bug fix, it appears to be non-bug-related.

**Conclusion: NotBuggy**"
checkstyle,1965.json,eb5b977a475c2240db5efc83aec8973d734895eb,"@@ -1,13 +1,13 @@
         public Optional<FinalVariableCandidate> findFinalVariableCandidateForAst(DetailAST ast) {
-            Optional<FinalVariableCandidate> result = Optional.absent();
+            Optional<FinalVariableCandidate> result = Optional.empty();
             DetailAST storedVariable = null;
             final Optional<FinalVariableCandidate> candidate =
-                Optional.fromNullable(scope.get(ast.getText()));
+                Optional.ofNullable(scope.get(ast.getText()));
             if (candidate.isPresent()) {
                 storedVariable = candidate.get().variableIdent;
             }
             if (storedVariable != null && isSameVariables(storedVariable, ast)) {
                 result = candidate;
             }
             return result;
         }",NotBuggy,"Pull #3293: Replace Guava's Optional with Java's native (#3293)

",NotBuggy,"The commit message indicates a transition from Guava's `Optional` to Java's native `Optional`. The code changes reflect this migration: replacing `Optional.absent()` with `Optional.empty()` and `Optional.fromNullable()` with `Optional.ofNullable()`. These changes align with the commit message and focus on utilizing Java's built-in `Optional`. There's no indication of a bug fix, but rather a refactor to use standard JDK features. Thus, the proper conclusion is **NotBuggy**."
checkstyle,3762.json,b94bac0719851b65b59b4aafc5bd3ef029321ed7,"@@ -1,4 +1,4 @@
     public Charset getCharset()
     {
-        return mCharset;
+        return charset;
     }",NotBuggy,"Prefixes, api, #512
",NotBuggy,"The commit message ""Prefixes, api, #512"" is vague and does not describe the specific change being made in the code. It lacks context about what the modification is intended to address.

Analyzing the code change:
- The code modification alters the return statement in the `getCharset` method from returning `mCharset` to returning `charset`. This suggests a change in variable naming, possibly addressing a naming convention or refactoring towards a more uniform codebase.

Based on these points:
1. The original variable name `mCharset` was likely a member variable, suggesting Hungarian notation, which denotes member variables.
2. The change to `charset` implies an attempt to standardize naming conventions. This often happens in cleanup or refactoring efforts.
3. There is no direct indication from the diff that resolves a bug or fixes an error that affects the functionality or explicitly addresses a bug-prone situation.

Considering the lack of explicit bug fix details or error corrections and the nature of the changes, this diff does not seem to pertain to a bug fix.

Therefore, the conclusion is **NotBuggy**."
lombok,2687.json,a05360a8eaba0de61f16f75816daf5a5af0a4567,"@@ -1,5 +1,4 @@
 	@Override public boolean visit(QualifiedNameReference node, ClassScope scope) {
-		setGeneratedBy(node, source);
-		applyOffsetExpression(node);
+		fixPositions(setGeneratedBy(node, source));
 		return super.visit(node, scope);
 	}",NotBuggy,ecj @Builder @Singular support for j.u. sets and maps.,NotBuggy,"The commit message describes adding support for `@Builder` and `@Singular` annotations in relation to Java utility sets and maps. However, the code diff reveals changes in position handling for `QualifiedNameReference` nodes, suggesting a change in how positions are set or fixed.

Let's analyze the diff:
- The method `visit` previously applied `setGeneratedBy` and `applyOffsetExpression` separately.
- In the revised code, `fixPositions` is used, encapsulating the result of `setGeneratedBy`.
- This change implies a refactoring or correction in the way positions are managed, potentially resolving an issue regarding node position handling but not directly related to the functionality described in the commit message.

There is no direct indication of changes related to Java utility sets and maps within the scope of the presented diff. Instead, it appears to correct an issue with position handling, which may not correlate with the described functionality addition.

Therefore, the changes do not directly indicate a bug fix for the functionality described in the commit message. Without further context, this adjustment alone does not reveal a bug fix related to Java utility sets and maps as described.

Conclusion: **NotBuggy**."
lombok,2499.json,fb401d4887895d1ebb8529d6323797f1bc8072a2,"@@ -1,3 +1,3 @@
-	public static EclipseSingularsRecipes get() {
-		return INSTANCE;
-	}

+				@Override public ReturnStatement get() {
+					return chain ? new ReturnStatement(new ThisReference(0, 0), 0, 0) : null;
+				}",NotBuggy,ecj: setter methods now return self(),NotBuggy,"The diff changes the return type and behavior of the `get()` method from returning an instance to potentially returning a new `ReturnStatement`. The commit message mentions modifying setter methods to return `self()`; however, this specific change doesn't directly align with or mention setter methods or returning `self()`. Instead, it seems to focus on altering how `get()` operates, which isn't a typical setter.

Given that the change deviates from the commit message and doesn't indicate correcting incorrect behavior or handling errors within the application logic, it doesn't qualify as addressing a bug directly. Thus, the conclusion for this change remains **NotBuggy**."
lombok,1375.json,5b4ab9dae8940973ce8b87c8c2a0388b768dac89,"@@ -1,4 +1,7 @@
-	@Override public java.util.List<Name> listFieldsToBeGenerated(JavacNode builderType, SingularData data) {
-		String p = data.getPluralName().toString();
-		return Arrays.asList(builderType.toName(p + ""$key""), builderType.toName(p + ""$value""));
+	@Override public java.util.List<Name> listMethodsToBeGenerated(SingularData data, JavacNode builderType) {
+		if (useGuavaInstead(builderType)) {
+			return guavaMapSingularizer.listMethodsToBeGenerated(data, builderType);
+		}
+		
+		return super.listMethodsToBeGenerated(data, builderType);
 	}",NotBuggy,added the ‘always use guava’ lombok.config option for @Singular @Builder.,NotBuggy,"The commit message discusses adding an option related to Guava use in a configuration, particularly affecting Lombok's @Singular @Builder constructs. The code change shows a method name adjustment from listFieldsToBeGenerated to listMethodsToBeGenerated and introduces a conditional check to delegate generation to guavaMapSingularizer if `useGuavaInstead(builderType)` returns true.

The change is mostly focused on functionality enhancement and feature implementation rather than fixing a bug. It seems to be adding conditional behavior based on configuration rather than correcting incorrect behavior. 

Thus, the assessment that this involves directly fixing a bug isn't strongly supported by the information given. The change aligns more with feature enhancement in terms of configuration options and utilization rather than a bug fix.

Therefore, based on the given diff and commit description, the conclusion is **NotBuggy**."
lombok,2745.json,35691e83edffdadd5ef438793eec9c968e8bfd35,"@@ -1,9 +1,9 @@
-	public static TypeReference[] copyTypes(TypeReference[] refs) {
+	public static TypeReference[] copyTypes(TypeReference[] refs, ASTNode source) {
 		if ( refs == null ) return null;
 		TypeReference[] outs = new TypeReference[refs.length];
 		int idx = 0;
 		for ( TypeReference ref : refs ) {
-			outs[idx++] = copyType(ref);
+			outs[idx++] = copyType(ref, source);
 		}
 		return outs;
 	}",NotBuggy,"Massive change to the eclipse handlers: They now set the 'generatedBy' flag which we can use to patch eclipse in specific places to ignore generated nodes.
",NotBuggy,"The commit message mentions updates to ""eclipse handlers"" to set a 'generatedBy' flag, but this diff shows the addition of an `ASTNode source` parameter to the `copyTypes` method. The change in copyTypes seems to delegate an additional argument to another method, possibly to support the ‘generatedBy’ functionality, but no modifications related to a 'generatedBy' flag directly appear in this snippet.

No direct indication of a bug fix is evident here. Adjusting the method signature and passing an extra parameter appears to be part of a broader feature update or enhancement rather than a bug fix. The commit message describes a significant change concerning newly handled functionality and doesn't specifically relay any bug-related content that matches the modifications within this snippet of code.

Based on this snippet alone, it seems more about feature enhancement or preparatory framework improvement rather than fixing a specific bug. Therefore, I conclude **NotBuggy**."
lombok,2695.json,0234039babafa1098c337136a97f4e1f94e35855,"@@ -1,5 +1,5 @@
 	@Override public boolean visit(SingleMemberAnnotation node, BlockScope scope) {
 		setGeneratedBy(node, source);
-		applyOffset(node, newSourceStart, newSourceEnd);
+		applyOffset(node);
 		return super.visit(node, scope);
 	}",NotBuggy,"Removed statics and lots of parameters, reimplemented recalcSourcePosition
",NotBuggy,"The commit message mentions removing statics and parameters, focusing on a method re-implementation, but does not mention a bug fix. The diff removes parameters from `applyOffset`, likely reflecting refactoring or simplification. There's no direct bug-related context, so the assessment is **NotBuggy**."
lombok,962.json,4be46113e81292a88cd5fdb3a5ce18fbcffd570d,"@@ -1,7 +1,7 @@
-	public static LombokOptions replaceWithDelombokOptions(Context context) {
+	public static Javac6BasedLombokOptions replaceWithDelombokOptions(Context context) {
 		Options options = Options.instance(context);
 		context.put(optionsKey, (Options)null);
-		LombokOptions result = new LombokOptions(context);
+		Javac6BasedLombokOptions result = new Javac6BasedLombokOptions(context);
 		result.putAll(options);
 		return result;
 	}",NotBuggy,"Compiler options can now be specified for JDK 8 or JDK 6/7 or lower.

After this was finalized I realize that we might consider moving entirely
to String based options, instead of inferring the options from the
provided enum. This setup does have the benefit of throwing exceptions when
options are not present.
",NotBuggy,"The commit message discusses compiler options but doesn't mention bugs or fixes. The diff changes a class type from LombokOptions to Javac6BasedLombokOptions, possibly addressing compatibility with different JDK versions. However, without explicit indication of incorrect behavior being resolved, it's not clearly a bug fix. The answer is **NotBuggy**."
lombok,244.json,eca219ee6433cd964f0549a114a791ca4eb9f0fa,"@@ -1,21 +1,20 @@
 	private <T> T readObject(JCTree tree, String fieldName, T defaultValue) {
 		Class<?> tClass = tree.getClass();
 		Map<String, Field> c = reflectionCache.get(tClass);
 		if (c == null) reflectionCache.put(tClass, c = new HashMap<String, Field>());
 		Field f = c.get(fieldName);
 		if (f == null) {
 			try {
-				f = tClass.getDeclaredField(fieldName);
+				f = Permit.getField(tClass, fieldName);
 			} catch (Exception e) {
 				return defaultValue;
 			}
-			f.setAccessible(true);
 			c.put(fieldName, f);
 		}
 		
 		try {
 			return (T) f.get(tree);
 		} catch (Exception e) {
 			return defaultValue;
 		}
 	}",NotBuggy,"eliminate ‘you are using private API’ warnings by streamlining all reflective access via a class that uses sun.misc.Unsafe to arrange access. From the nqzero permit-reflect library.
",NotBuggy,"The commit message explains the purpose of the modification: to eliminate warnings about using private APIs by utilizing a class from the nqzero permit-reflect library to handle reflection through a centralized mechanism, sun.misc.Unsafe. This change involves replacing `getDeclaredField` with `Permit.getField`, allowing access to fields without making them accessible directly via `setAccessible(true)`. This addresses ""you are using private API"" warnings by providing an alternative mechanism for field access.

Let's analyze the change step-by-step:

1. **Removal of Direct Reflection Handling**: 
    - The change replaces `f = tClass.getDeclaredField(fieldName);` with `f = Permit.getField(tClass, fieldName);`. This indicates a transition from using Java's reflection directly to an abstraction, likely provided by the Permit class, that uses sun.misc.Unsafe internally.

2. **Setting Accessible**:
    - The line `f.setAccessible(true);` is removed. This step was necessary when directly using reflection to access private fields, but with `Permit.getField`, this step isn't needed anymore as the abstraction presumably handles the accessibility using other means.

3. **Cache and Default Value Handling**:
    - The rest of the method revolves around caching field references and handling exceptions, which remains unchanged, so these changes focus purely on how fields are accessed reflectively.

4. **Purpose of Change**:
    - The commit aims to streamline the code to mitigate ""using private API"" warnings by employing a library specifically designed to handle such operations and leverage the capabilities of `sun.misc.Unsafe`. 

Though this change updates the accessing mechanism, it doesn’t directly indicate a bug fix, such as a logical error or exception correction. Instead, it mitigates warnings related to using internal Java APIs. 

In conclusion, the modification does not explicitly align with typical bug symptoms like runtime errors or incorrect logic. Hence, the conclusion is **NotBuggy**."
lombok,817.json,f07948d4959cfdb81bb0c540ebcbe879df0109fb,"@@ -1,3 +1,3 @@
 	@Override public int hashCode() {
-		return path.hashCode();
+		return eclipseIniPath.hashCode();
 	}",NotBuggy,"The installer is now waaay smarter on linuxes, and should support, out of the box and automatically, debian and debianesques like ubuntu.
",NotBuggy,"The change in the diff adjusts the `hashCode` method from using `path.hashCode()` to `eclipseIniPath.hashCode()`. This is not related to installer functionality on Linux systems and does not align with the commit message about improving installer smartness. There is no indication of a bug fix in the context of Linux installers or otherwise, as this change seems to be more of a refactor or adjustment related to how `hashCode` is computed for this particular class, likely to align with the logic expected for instances of this type. Therefore, the appropriate assessment of this change is **NotBuggy**."
lombok,2906.json,bb66465751b8f0b129e53e639854f95fcdb3dca8,"@@ -1,20 +1,12 @@
 	public static TypeReference createTypeReference(String typeName, Annotation source) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		long p = (long)pS << 32 | pE;
 		
-		TypeReference typeReference;
-		if (typeName.contains(""."")) {
-			
-			char[][] typeNameTokens = fromQualifiedName(typeName);
-			long[] pos = new long[typeNameTokens.length];
-			Arrays.fill(pos, p);
-			
-			typeReference = new QualifiedTypeReference(typeNameTokens, pos);
-		}
-		else {
-			typeReference = null;
-		}
+		char[][] typeNameTokens = fromQualifiedName(typeName);
+		long[] pos = new long[typeNameTokens.length];
+		Arrays.fill(pos, p);
 		
+		TypeReference typeReference = new QualifiedTypeReference(typeNameTokens, pos);
 		setGeneratedBy(typeReference, source);
 		return typeReference;
 	}",NotBuggy,"Custom log declaration
",NotBuggy,"In this diff, there is a significant structural change to the function `createTypeReference`. Previously, the method contained conditional logic to handle both fully qualified and non-qualified type names differently. If the `typeName` contained a dot, it was treated as a qualified type name, and a `QualifiedTypeReference` was created. If not, the method would return `null`.

The update removes the conditional check and always creates a `QualifiedTypeReference`, regardless of whether the `typeName` contains a dot. This suggests the method would now handle all type names as qualified, which can be either an enhancement or a simplification depending on the whole framework logic, but it doesn't necessarily indicate a bug fix. 

The commit message does not mention any bug fixes; instead, it refers to a ""Custom log declaration,"" which doesn’t seem to align with the content of the code changes related to type references. The changes made do not directly point to any bug being addressed, such as fixing incorrect behavior, exception handling, or logical errors. Therefore, based solely on this analysis, the changes appear to streamline the process but aren't directly fixing a known bug.

Therefore, the changes as a whole should be classified as **NotBuggy**."
lombok,2557.json,519f95c2410c2e43c60e81233cad105552975606,"@@ -1,4 +1,6 @@
 	private static final boolean toBoolean(Object expr, boolean defaultValue) {
 		if (expr == null) return defaultValue;
+		if (expr instanceof FalseLiteral) return false;
+		if (expr instanceof TrueLiteral) return true;
 		return ((Boolean) expr).booleanValue();
 	}",NotBuggy,builder @Singleton now works fully in eclipse as well! feature done!,NotBuggy,"The current commit message highlights a feature enhancement related to `@Singleton` in Eclipse, not addressing any specific bug fix. The code change introduces `FalseLiteral` and `TrueLiteral` checks into the `toBoolean` method to complete its logic when converting objects to boolean values. This modification strengthens input type handling but aligns more with a feature enhancement or robustness improvement than a bug fix. Consequently, the changes don't directly signify a bug fix. Hence, the conclusion is **NotBuggy**."
lombok,2948.json,a31e9ffdac96737f5a441efa6c80a2394bba63d1,"@@ -1,3 +1,3 @@
-	public A getAst() {
+	public EclipseAST getAst() {
 		return ast;
 	}",NotBuggy,"use delared ast Type instead of generic one saves a lot of ""checkcasts"" in the byte code
",NotBuggy,"The commit message focuses on optimizing by specifying a concrete type rather than a generic one. This change directly reflects that by updating the return type from a generic type `A` to a specific `EclipseAST`. It suggests an enhancement to performance by reducing type checks (checkcasts) during runtime. There is no indication of a bug fix in the commit message or the code changes. The change is performance-related and about type specificity, not bug-related. Therefore, the assessment is **NotBuggy**."
lombok,824.json,df84dffa62fc51231e9215d04b9751dba6c6cf4d,"@@ -1,17 +1,17 @@
 	private static String generateWriteErrorMessage() {
 		String osSpecificError;
 		
-		switch (EclipseFinder.getOS()) {
+		switch (IdeFinder.getOS()) {
 		default:
 		case MAC_OS_X:
 		case UNIX:
 			osSpecificError = "":\nStart terminal, go to the directory with lombok.jar, and run: sudo java -jar lombok.jar"";
 			break;
 		case WINDOWS:
 			osSpecificError = "":\nStart a new cmd (dos box) with admin privileges, go to the directory with lombok.jar, and run: java -jar lombok.jar"";
 			break;
 		}
 		
 		return "", probably because this installer does not have the access rights.\n"" +
 		""Try re-running the installer with administrative privileges"" + osSpecificError;
 	}",NotBuggy,"Massive update to how the installer works, and added netbeans to the installer.
",NotBuggy,"The commit message indicates changes related to the installer and adding NetBeans to it. However, the provided code diff does not involve any significant installer logic changes or NetBeans integration updates. Instead, it shows a refactor by changing a method call from `EclipseFinder.getOS()` to `IdeFinder.getOS()`. This change aligns with a possibly broader code restructuring or renaming but doesn't appear to directly relate to fixing any installation issues.

No changes in the error-handling logic, control flow, or functionality reflect a bug fix. The method and its associated functionality remain the same, only altering the source of obtaining the operating system information.

Since the changes do not align with fixing a bug or improving error handling, the conclusion is **NotBuggy**."
lombok,1913.json,143143276da646bcc9a195d827364f8499187a6a,"@@ -1,12 +1,14 @@
 	private void printValue(ConfigurationKey<?> key, Object value, boolean verbose, Collection<String> history) {
-		if (verbose) out.printf(""# %s\n"", key.getDescription());
+		if (verbose) out.printf(""# %s%n"", key.getDescription());
 		if (value == null) {
-			out.printf(""clear %s\n"", key.getKeyName());
+			out.printf(""clear %s%n"", key.getKeyName());
 		} else if (value instanceof List<?>) {
-			for (Object element : (List<?>)value) out.printf(""%s += %s\n"", key.getKeyName(), element);
+			List<?> list = (List<?>)value;
+			if (list.isEmpty()) out.printf(""clear %s%n"", key.getKeyName());
+			for (Object element : list) out.printf(""%s += %s%n"", key.getKeyName(), element);
 		} else {
-			out.printf(""%s = %s\n"", key.getKeyName(), value);
+			out.printf(""%s = %s%n"", key.getKeyName(), value);
 		}
 		if (!verbose) return;
-		for (String modification : history) out.printf(""# %s\n"", modification);
+		for (String modification : history) out.printf(""# %s%n"", modification);
 	}",NotBuggy,"[configuration] Have the ConfigurationApp print platform-specific newLine characters, and print clear for empty lists.
",NotBuggy,"The commit message and code changes match. The diff includes two key changes:

1. **Platform-Specific Newline Characters:** The code replaces `\n` with `%n` for platform-specific newline characters in `printf`. This change is consistent with the commit message's first objective, aligning the application with different platform conventions for newlines.

2. **Print ""Clear"" for Empty Lists:** The code also handles empty lists by printing ""clear"" for lists without elements. This is exactly what the commit message refers to as ""print clear for empty lists.""

While these changes improve platform compatibility and provide clearer output, there are no signs of bug fixes regarding logical errors or exception handling that would imply a bug fix is being addressed. The modifications are more about enhancing the output format and consistency.

Therefore, while the changes improve functionality according to the commit message, they do not specifically indicate a direct bug fix. The conclusion is **NotBuggy**."
lombok,1884.json,713822db9894cf99184aff57b3387f99846aa870,"@@ -1,7 +1,13 @@
 			@Override public Object parse(String value) {
 				try {
 					return Enum.valueOf(rawType, value);
 				} catch (Exception e) {
-					return Enum.valueOf(rawType, value.toUpperCase());
+					StringBuilder sb = new StringBuilder();
+					for (int i = 0; i < value.length(); i++) {
+						char c = value.charAt(i);
+						if (Character.isUpperCase(c) && i > 0) sb.append(""_"");
+						sb.append(Character.toUpperCase(c));
+					}
+					return Enum.valueOf(rawType, sb.toString());
 				}
 			}",NotBuggy,"Changed the lombok.nonNull.exceptionType configkey to be an enum, partly to enforce only sensible exceptions and partly to make it possible to have the message be ‘x is null’ when throwing IAEs.",NotBuggy,"The commit message indicates a change to enhance exception handling, specifically to enforce valid exceptions and improve exception messages when using an enum for lombok.nonNull.exceptionType. The name parsing logic in the code diff has been modified to handle enum names more flexibly. The previous logic simply calls `toUpperCase()`, which won't correctly translate camelCase or mixed-case input strings to the expected enum format.

### Analysis:

1. **Original Code Logic**:
   - The original logic attempts a lookup with the input value as it is.
   - If it fails, it tries the input value converted fully to uppercase.

2. **Changed Code Logic**:
   - The new logic iterates through each character of the input string.
   - If an uppercase character is found and it's not the start of the string, an underscore is prepended before appending the uppercase character.
   - This dynamically transforms camelCase strings to UPPER_SNAKE_CASE, which matches traditional Java enum naming conventions.

### Conclusion:
The changes align with the commit's aim to enforce sensible exceptions by ensuring that enum lookups handle diverse input formats correctly. This update improves robustness and user-friendliness but does not directly indicate a bug fix. Instead, it adds enhancement to parsing and handling enum names. The message mentioned a partly related issue but does not suggest correcting malfunctioning behavior that manifested as a bug. Therefore, the modification is more about general code quality improvement rather than a direct bug fix.

The answer is **NotBuggy**."
lombok,2541.json,09ea02e4f5752e615be2ff5177be1fb328702a5b,"@@ -1,11 +1,11 @@
-	private LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
+	public LocalDeclaration createLocalDeclaration(ASTNode source, char[] dollarFieldName, TypeReference type, Expression initializer) {
 		int pS = source.sourceStart, pE = source.sourceEnd;
 		LocalDeclaration tempVar = new LocalDeclaration(dollarFieldName, pS, pE);
 		setGeneratedBy(tempVar, source);
 		tempVar.initialization = initializer;
 		tempVar.type = type;
 		tempVar.type.sourceStart = pS; tempVar.type.sourceEnd = pE;
 		setGeneratedBy(tempVar.type, source);
 		tempVar.modifiers = Modifier.FINAL;
 		return tempVar;
 	}",NotBuggy,"added package-infos to lots of packages, updated the javadoc in these, and made lots of private/package private methods 'public' in packages that aren't actually public API. Tumi asked for this, and I can imagine this is useful for others who are hacking add-ons to lombok. The package-info files clarify that we aren't committing to long-term stability of anything except the stuff directly in the lombok package, lombok.experimental, and lombok.extern.
",NotBuggy,"The commit message describes making several methods public and updating documentation. The diff aligns with this, as a private method is changed to public. This increases accessibility for add-on development, not addressing bugs. The answer is **NotBuggy**."
hibernate-search,521.json,1741d8fb1dd68d1fbe72e5650c3815e39e9426fa,"@@ -1,20 +1,19 @@
 	private TypeMetadata doGetTypeMetadataFor(Class<?> clazz, XClass xClass, ParseContext parseContext) {
-		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, configContext )
+		TypeMetadata.Builder typeMetadataBuilder = new TypeMetadata.Builder( clazz, parseContext.getIndexManagerType(), configContext )
 				.boost( getBoost( xClass ) )
-				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) )
-				.analyzerReference( configContext.getDefaultLuceneAnalyzerReference() );
+				.boostStrategy( AnnotationProcessingHelper.getDynamicBoost( xClass ) );
 
 		initializePackageLevelAnnotations( packageInfo( clazz ), configContext );
 
 		initializeClass(
 				typeMetadataBuilder,
 				true,
 				EMPTY_PREFIX,
 				parseContext,
 				configContext,
 				false,
 				null
 		);
 
 		return typeMetadataBuilder.build();
 	}",NotBuggy,"HSEARCH-2222 Change the way ScopedAnalyzer are managed

Also provide sensible default analyzer in the remote case.
",NotBuggy,"The commit message indicates changes in how `ScopedAnalyzer` are managed and mentions providing a default analyzer in a remote context. Within the diff, the `TypeMetadata.Builder` changed to include a reference to the `parseContext.getIndexManagerType()` rather than just the context itself. Additionally, the initialization of the `analyzerReference`, which was set to `configContext.getDefaultLuceneAnalyzerReference()`, has been removed.

The changes focus on modifying how metadata is built for a certain type by altering the parameters and removing the default analyzer reference, aligning with the commit message regarding analysis management. However, these modifications are structural and relate to feature adjustments or improvements in behavior rather than directly to fixing a bug. There is also no direct evidence of bug fixes like error handling or logical corrections.

Given the context and changes made, they appear to reflect more of a refactoring or improvement to add or adjust features rather than fixing an existing defect.

Therefore, the conclusion is **NotBuggy**."
hibernate-search,5198.json,db76c736dbdcd54cfc605472cadcbb5f3a4051b1,"@@ -1,3 +1,3 @@
-	public JestClient getClient() {
+	public RestClient getClient() {
 		return client;
 	}",NotBuggy,"HSEARCH-2581 Use the official Elasticsearch REST client instead of Jest
",NotBuggy,"The commit message ""HSEARCH-2581 Use the official Elasticsearch REST client instead of Jest"" indicates a change in the backend library used to interact with Elasticsearch. The provided diff shows a modification in the method return type from `JestClient` to `RestClient`, which aligns with the commit message, suggesting a shift from using Jest to the official Elasticsearch REST client.

This change does not directly imply a bug fix but rather a replacement of an external library, possibly for reasons such as better support, more features, or alignment with future practices. The message does not mention any specific bug, nor do the code changes highlight any bug fix; they simply conform with the stated transition.

Thus, this change is primarily focused on improving the architecture or aligning with a strategic change rather than explicitly fixing a bug. Therefore, the analysis concludes with **NotBuggy**."
hibernate-search,7079.json,6524d5233928c0ca691e79cc0f28a6b6116d3b1b,"@@ -1,3 +1,3 @@
 	public Address getAddress() {
-		return channel.getAddress();
+		return channelContainer.getChannel().getAddress();
 	}",NotBuggy,"HSEARCH-1459 Refactor ServiceManager and ServiceProvider
",NotBuggy,"The commit message indicates a refactor related to `ServiceManager` and `ServiceProvider`. However, the provided code diff shows a change in how an `Address` is obtained. Instead of directly calling `channel.getAddress()`, the updated code calls `channelContainer.getChannel().getAddress()`. This change suggests a modification in the access pattern or encapsulation logic, but it doesn't directly relate to any introduced or fixed bug, nor it distinctly matches the refactoring of `ServiceManager` and `ServiceProvider` as per the commit message.

The change could imply a restructuring rather than a bug fix, focusing on perhaps better architectural or design pattern application. Since there isn't any evident sign of a bug fix from the information provided, and it aligns with a refactor theme, the conclusion would be **NotBuggy**."
hibernate-search,2895.json,8c9a35568e74f8c4d21eda2a6f517a51c7fad55b,"@@ -1,4 +1,4 @@
-	public PropertySpatialMapping bottomGridLevel(int bottomGridLevel) {
-		spatial.put( ""bottomQuadTreeLevel"", bottomGridLevel );
+	public PropertySpatialMapping bottomSpatialHashLevel(int bottomSpatialHashLevel) {
+		spatial.put( ""bottomSpatialHashLevel"", bottomSpatialHashLevel );
 		return this;
 	}",NotBuggy,"HSEARCH-1327 Reword reference to Quad Tree for Spatial Hash in spatial module
",NotBuggy,"This diff renames a method parameter and a map key from ""bottomQuadTreeLevel"" to ""bottomSpatialHashLevel,"" reflecting a change in terminology rather than fixing a bug. The change aligns with the commit message focusing on rewording rather than addressing any incorrect behavior or logic in the code. There is no indication of error handling, logical correction, or exception handling improvement related behavior in this change, so it doesn't signal a bug fix.

Hence, the changes are primarily semantic and address the nomenclature used in the code rather than resolving a defect. Therefore, the evaluation based on the commit message and the provided code suggests **NotBuggy**."
hibernate-search,5149.json,ee9e8d7bcbb74216bde5b521e9d7275cc944bbe7,"@@ -1,7 +1,7 @@
 	public void add(BulkableElasticsearchWork<?> work) {
 		currentBulkWorks.add( work );
-		if ( currentBulkWorks.size() >= MAX_BULK_SIZE ) {
+		if ( currentBulkWorks.size() >= maxBulkSize ) {
 			flushBulked();
 			flushBulk();
 		}
 	}",NotBuggy,"HSEARCH-2764 Set the minim bulk size to 1 for stream work orchestration

For stream works, we only submit single-work changesets, which means
the decision on whether to bulk the work or not will always happen
immediately after each work, when we only have one work to bulk.
Thus if we set the minimum to a value higher than 1, we would always
decide not to start a bulk (because there would always be only one
work to bulk), which would result in terrible performance.
",NotBuggy,"The commit message explains that setting a minimum bulk size of 1 is necessary for stream work orchestration to avoid performance issues. However, the code change shown in the diff does not directly address or modify any minimum bulk size setting. Instead, it changes a constant `MAX_BULK_SIZE` to a variable `maxBulkSize`, suggesting that bulk size is now configurable rather than hardcoded.

The change itself doesn't fix a bug in the code but rather introduces flexibility in the bulk size configuration. The described bug in the commit message, where a too-high minimum bulk size leads to performance issues, is not directly resolved in the diff since there is no indication that the bulk size limit itself was adjusted to 1; it just became variable.

Thus, in relation to the commit message, the change isn't directly about fixing the specific bug but enabling configuration that could potentially fix it elsewhere in the code where `maxBulkSize` is set. 

Since the commit does not reflect an immediate bug-fixing action due to its nature of changing from a constant to a variable (likely for flexibility/configuration purposes), this particular diff should be categorized as **NotBuggy**."
hibernate-search,6283.json,359d8d7e1ab9ba2d9abc0aa8c4a16c7f61526996,"@@ -1,6 +1,6 @@
 	public void addDelete(String entityClassName, byte[] id) {
-		GenericRecord delete = new GenericData.Record( schemas.get( ""Delete"" ) );
+		GenericRecord delete = new GenericData.Record( protocol.getType( ""Delete"" ) );
 		delete.put( ""class"", entityClassName );
 		delete.put( ""id"", ByteBuffer.wrap( id ) );
 		operations.add( delete );
 	}",NotBuggy,"HSEARCH-757 Use Avro Protocol to implement proper schema relations

The previous solution was fragile and protocols accept several types with cross references
",Buggy,"The code change replaces `schemas.get(""Delete"")` with `protocol.getType(""Delete"")`. This alters how a `GenericRecord` for deletes is instantiated, suggesting a shift from a specific schema management strategy to utilizing a protocol type management strategy, likely Avro protocol types.

### Analysis

1. **Commit Message Context**: The commit message indicates a transition to using Avro protocols for schema relations. This signals a move towards more robust and maintainable schema handling, potentially addressing issues like fragility and cross-referencing limitations inherent in the previous implementation.

2. **Code Changes**: 
   - Replacing `schemas.get(""Delete"")` with `protocol.getType(""Delete"")` suggests a foundational change in how the system handles record types. 
   - Using protocol types can imply better consistency and potential bug fixes around schema management or interactions that depend on these types.

3. **Inference**: 
   - The change aligns with addressing a robustness issue with schema handling, as specified in the commit message. 
   - Although the primary intent could be a structural or architectural enhancement rather than a direct bug fix, the context implies improvement of fragile previous behavior.
   - The focus on protocols suggests resolving issues tied to cross-referencing schemas, which can indeed fall into bug fixes if previous operations were unreliable due to schema type handling.

4. **Conclusion**: The alterations are likely substantial and enhance functionality consistency, addressing implicit issues (or bugs) related to the previous approach.

The changes show signs of addressing an inherent problem in the existing code with schema handling, thus the answer is **Buggy**."
hibernate-search,1714.json,afb48fce1e1b306db8ec9f5f14fd5e9eba2afb0b,"@@ -1,3 +1,3 @@
 			public Function<HashSet<IndexedTypeIdentifier>, IndexedTypeSet> finisher() {
-				return IndexedTypesSets::fromSafeHashSet;
+				return IndexedTypeSets::fromSafeHashSet;
 			}",NotBuggy,"HSEARCH-1404 Rename IndexedTypesSets to IndexedTypeSets
",NotBuggy,"The commit message indicates a renaming of `IndexedTypesSets` to `IndexedTypeSets`. The code change reflects this renaming by updating a method reference from `IndexedTypesSets::fromSafeHashSet` to `IndexedTypeSets::fromSafeHashSet`. This modification aligns with the semantic of the commit message, focusing on renaming rather than fixing a bug. It doesn't involve error-handling updates, logical corrections, or other signs of a bug fix. Therefore, the change is strictly a refactor and not a bug fix. The answer is **NotBuggy**."
hibernate-search,6888.json,2bcf160fa7b80fa94734705d593e3cbb50ee7aa5,"@@ -1,50 +1,50 @@
 	private FetchingStrategy createCriteriaFetchingStrategy(
 			CacheMode cacheMode, int entityFetchSize, Integer maxResults) throws Exception {
 		Class<?> entityType = jobData.getEntityType( entityName );
 		Object upperBound = SerializationUtil.deserialize( serializedUpperBound );
 		Object lowerBound = SerializationUtil.deserialize( serializedLowerBound );
 		Set<Criterion> customQueryCriteria = jobData.getCustomQueryCriteria();
 
 		EntityTypeDescriptor typeDescriptor = jobData.getEntityTypeDescriptor( entityType );
 		IdOrder idOrder = typeDescriptor.getIdOrder();
 
 		return (session, lastCheckpointInfo) -> {
-			Criteria criteria = session.createCriteria( entityType );
+			Criteria criteria = new CriteriaImpl( entityType.getName(), session.unwrap( SessionImplementor.class ) );
 
 			// build orders for this entity
 			idOrder.addAscOrder( criteria );
 
 			// build criteria using job context data
 			customQueryCriteria.forEach( c -> criteria.add( c ) );
 
 			// build criteria using bounds
 			if ( upperBound != null ) {
 				criteria.add( idOrder.idLesser( upperBound ) );
 			}
 			if ( lastCheckpointInfo != null ) {
 				criteria.add( idOrder.idGreater( lastCheckpointInfo.getLastProcessedEntityId() ) );
 			}
 			else if ( lowerBound != null ) {
 				criteria.add( idOrder.idGreaterOrEqual( lowerBound ) );
 			}
 
 			if ( maxResults != null ) {
 				int remaining;
 				if ( lastCheckpointInfo != null ) {
 					remaining = maxResults - lastCheckpointInfo.getProcessedEntityCount();
 				}
 				else {
 					remaining = maxResults;
 				}
 				criteria.setMaxResults( remaining );
 			}
 
 			return criteria.setReadOnly( true )
 					.setCacheable( false )
 					.setLockMode( LockMode.NONE )
 					.setFlushMode( FlushMode.MANUAL )
 					.setCacheMode( cacheMode )
 					.setFetchSize( entityFetchSize )
 					.scroll( ScrollMode.FORWARD_ONLY );
 		};
 	}",NotBuggy,"HSEARCH-2910 JSR-352: Avoid logging warnings caused by internal usage of the legacy Criteria API
",NotBuggy,"The commit message indicates an intention to avoid warnings related to the legacy Criteria API usage. The code diff replaces the usage of `session.createCriteria(entityType)` with `new CriteriaImpl(entityType.getName(), session.unwrap(SessionImplementor.class))`.

Here’s the reasoning for determining whether this change addresses a bug:

1. **Legacy API Usage**: The old Hibernate `session.createCriteria` method is part of the legacy Criteria API, which may lead to deprecation warnings or issues in newer Hibernate versions. This diff manually creates a `CriteriaImpl` object, potentially avoiding issues with the deprecated method.

2. **Unwrapping SessionImplementor**: The change includes `session.unwrap(SessionImplementor.class)`. This suggests more direct access to the underlying session implementation. This could fix issues related to indirect accesses in the legacy API usage.

3. **Warnings Avoidance**: The commit message explicitly states that the changes are intended to avoid warnings. This shows that the diff is addressing a specific problem related to the internal usage of a deprecated or legacy approach.

Overall, given that the change is aimed at eliminating warnings caused by legacy API usage, it can be seen as functioning to correct or mitigate issues related to how the code interacts with the underlying libraries. However, since it focuses on avoiding warnings due to legacy usage rather than fixing a functional bug or error, it is more of a preventive measure related to upgrading and maintaining code rather than fixing a directly observable bug.

The answer is **NotBuggy**."
hibernate-search,75.json,bb3f94df96e6e487ba231e4e201064cbb850aa02,"@@ -1,3 +1,3 @@
-	protected final void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
+	protected void applyToLuceneOptions(LuceneOptions luceneOptions, String name, Number value, Document document) {
 		luceneOptions.addNumericFieldToDocument( name, value, document );
 	}",NotBuggy,"HSEARCH-997 Adding short numeric bridge
",NotBuggy,"The diff change the method `applyToLuceneOptions` from `protected final` to just `protected`. This modification primarily impacts the method's ability to be overridden in subclasses, removing the `final` keyword that previously prevented overrides. However, the commit message does not indicate any bug fix, but rather an enhancement related to adding a ""short numeric bridge,"" which likely requires extending this method in subclasses. The change appears to enable this extension rather than fixing an incorrect behavior or error in the previous code.

Additionally, the message doesn't mention any specific issues with the method's previous functionality that would suggest it was buggy. Therefore, this change does not suggest a bug fix.

Based on this analysis, the conclusion is **NotBuggy**."
hibernate-search,5090.json,3bf6cd975dc1182ff31acb0eb5da642003664708,"@@ -1,3 +1,3 @@
-		public GetIndexMappingWork build() {
-			return new GetIndexMappingWork( this );
+		public GetIndexTypeMappingsWork build() {
+			return new GetIndexTypeMappingsWork( this );
 		}",NotBuggy,"HSEARCH-2581 Make GsonService part of the ElasticsearchWorkExecutionContext

And make JestAPIFormatter a utils class taking a GsonService as a parameter,
since it's only ever used where an execution context is available.
",NotBuggy,"The commit message describes a change in the organization of services and utility classes within the code but doesn't mention a specific bug fix. The code diffs reveal a renaming incident or replacement from `GetIndexMappingWork` to `GetIndexTypeMappingsWork`. Without evidence of fixing incorrect behavior or error handling, the change is likely intended to express improved code semantics, organization, or align with refactored code structure.

The renaming seems related to consistency or clarity rather than directly addressing a bug. Structurally, it aligns with the rest of the commit message stating a re-organization of classes and method parameters to reflect a new context or abstraction layer.

Given the evidence provided, there is no direct indication of a bug being fixed in this diff. The focus is on transitioning to a different class that aligns with the new organization or refactoring strategy.

Therefore, the conclusion is **NotBuggy**."
hibernate-search,2698.json,45debe412916f6c6b4b4fbc84270dca378f0153d,"@@ -1,7 +1,7 @@
-	public Map<String, AnalyzerDef> build() {
-		Map<String, AnalyzerDef> result = new LinkedHashMap<>();
+	public SimpleLuceneAnalysisDefinitionRegistry build() {
+		SimpleLuceneAnalysisDefinitionRegistry registry = new SimpleLuceneAnalysisDefinitionRegistry();
 		for ( Map.Entry<String, LuceneAnalyzerDefinitionContextImpl> entry : definitions.entrySet() ) {
-			result.put( entry.getKey(), entry.getValue().build() );
+			registry.register( entry.getKey(), entry.getValue().build() );
 		}
-		return result;
+		return registry;
 	}",NotBuggy,"HSEARCH-2726 Store Lucene analyzer definitions in a dedicated class

This doesn't change much for now, but it will allow future commits
to add more information to the ""analysis definition registry"",
in particular normalizer definitions.
",NotBuggy,"The commit message highlights that this change is primarily structural, setting up future improvements rather than addressing any current functional issues. The diff shows a change from returning a `Map<String, AnalyzerDef>` to returning a `SimpleLuceneAnalysisDefinitionRegistry` object, with adjustments on how analyzer definitions are stored. The refactoring is aimed at enhancing how information, like analyzer and normalizer definitions, is managed, but does not directly resolve a bug.

Given the context provided in the commit message and the nature of the code changes, there are no indications of a bug being fixed in this commit. The focus seems to be on preparing the codebase for future enhancements and organization improvements.

The modifications align with the commit message, which states the creation of a dedicated class for more effective management of Lucene analyzer definitions—indicating a structural change rather than a bug fix.

Therefore, the conclusion is **NotBuggy**."
hibernate-search,4788.json,9451d18a3cf1f82dc134dcb6ddbd19ff771c1e03,"@@ -1,7 +1,7 @@
-		public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
-			if ( value == null ) {
-				return;
-			}
+	public void set(String name, Object value, Document document, LuceneOptions luceneOptions) {
+		if ( value == null ) {
+			return;
+		}
 
-			luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
-		}

+		luceneOptions.addFieldToDocument( name, convertToString( (Date) value ), document );
+	}",NotBuggy,"HSEARCH-2245 Handle Calendar and resolution in the Elasticsearch specific BridgeProvider

No tests yet as tests are blocked by HSEARCH-2246
",NotBuggy,"The given diff reflects a change in whitespace and indentation only. The logic and operations executed within the `set` method remain unaltered. The commit message describes handling Calendar and resolution in an Elasticsearch-specific `BridgeProvider`, but there's no implementation change that addresses this issue in the provided code changes. Since there's no indication of bug-related changes or any functional modifications, this change seems stylistic. The answer is **NotBuggy**."
hibernate-search,3749.json,0049cce1f0e0f6588e4f65bfa8f941267839a22e,"@@ -1,3 +1,3 @@
-		protected int size() {
-			return values.length;
-		}

+			public int size(int[] array) {
+				return array.length;
+			}",NotBuggy,"HSEARCH-2510 Avoid the extra iterable instantiation in CollectionHelper.iteratorFromArray

Because there will be someone who's not happy with that extra
instantiation, I'm sure.

Calls to array accessors seem to be inlined, so there shouldn't be much
execution overhead.
",NotBuggy,"The commit message indicates changes to avoid an extra iterable instantiation in CollectionHelper, while the diff shows a method signature change in size methods. The diff changes a method from returning a fixed array's size to accepting an array parameter and then returning its size. Both relate to performance but the diff is not about instantiation optimization. There's no indication of a bug fix. Hence, the answer is **NotBuggy**."
hibernate-search,5464.json,ab4312a79532524ca5cf2bceaa1e173d4bd10630,"@@ -1,10 +1,8 @@
 		SearchResult scroll(String scrollId) {
-			try ( ServiceReference<ElasticsearchDialectProvider> dialectProvider =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchDialectProvider.class );
-					ServiceReference<ElasticsearchWorkProcessor> processor =
-							getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchWorkProcessor.class ) ) {
-				ElasticsearchWork<SearchResult> work = dialectProvider.get().getDialect().getWorkFactory()
+			try ( ServiceReference<ElasticsearchService> elasticsearchService =
+					getExtendedSearchIntegrator().getServiceManager().requestReference( ElasticsearchService.class ) ) {
+				ElasticsearchWork<SearchResult> work = elasticsearchService.get().getWorkFactory()
 						.scroll( scrollId, getScrollTimeout() ).build();
-				return processor.get().executeSyncUnsafe( work );
+				return elasticsearchService.get().getWorkProcessor().executeSyncUnsafe( work );
 			}
 		}",NotBuggy,"HSEARCH-2581 Make ElasticsearchService the single entry point for operations on the Elasticsearch server

This means in particular that the Elasticsearch service will manage the
bootstrapping of every component involved, which will allow us to avoid
circular dependencies in the next few commits.

Also, it seems cleaner, and it brings us one step closer to supporting
simulatenous access to multiple Elasticsearch clusters.
",NotBuggy,"The commit message mentions restructuring to make `ElasticsearchService` the single entry point for operations related to the Elasticsearch server, aiming for cleaner code and avoiding circular dependencies.

Analyzing the code changes:

1. **Service Replacement**: The diff replaces references to `ElasticsearchDialectProvider` and `ElasticsearchWorkProcessor` with a single reference to `ElasticsearchService`.
   
2. **Centralization**: This change is focused on organizational and architectural improvements in the system—centralizing access through `ElasticsearchService` instead of obtaining each dependency independently.

3. **Avoiding Circular Dependencies**: This aligns well with the commit message's goal of avoiding circular dependencies by using `ElasticsearchService` to manage dependencies internally.

4. **Functional Consistency**: The logic for creating `ElasticsearchWork` and executing it remains functionally equivalent, ensuring that the behavior of the `scroll` method should continue to work as intended.

Given that this change is more about refactoring and improving the architecture for dependency management without evidence of a specific bug being fixed, it should be considered as addressing architectural improvements rather than fixing a bug.

Therefore, the changes do not indicate a bug fix but rather an organizational refactor aimed at code cleanliness and structure improvements. Thus, the conclusion is **NotBuggy**."
hibernate-search,1557.json,5cd16e6bac5161f228af80752581fd3bb0baa733,"@@ -1,4 +1,4 @@
 	public void initialize(Properties props, WorkerBuildContext context) {
 		// no-op
-		log.warn( ""initialized \""blackhole\"" backend. Index changes will be prepared but discarded!"" );
+		log.initializedBlackholeBackend();
 	}",NotBuggy,"HSEARCH-675 - Replace slf4j with JBoss Logging (except for archetype)
",NotBuggy,"The commit message indicates a change from SLF4J to JBoss Logging. The code modification replaces a direct SLF4J warning log call with a JBoss Logging method (initializedBlackholeBackend). There's no indication of a bug being fixed; it focuses on switching logging frameworks while preserving the logging message's intent. The change improves maintainability and consistency with the project's logging framework but does not address a logic error or bug in functionality.

Thus, the assessment concludes with **NotBuggy**."
wildfly,13917.json,874e6032075a06b60e01bd96fe82d32a564bdc04,"@@ -1,2 +1,3 @@
-    protected void writeAttributes(final XMLExtendedStreamWriter writer, final SubsystemMarshallingContext context) throws XMLStreamException {
+    public void undeploy(final DeploymentUnit context) {
+
     }",NotBuggy,"AS7-4386 Add the option to configure a default distinct name for remote EJB's
",NotBuggy,"The commit message indicates that the change is about adding an option to configure a default distinct name for remote EJBs. However, the code diff shows a change in method signature from `protected void writeAttributes` to `public void undeploy` with no implementation. 

This change does not seem related to the configuration of a default distinct name for remote EJBs, as the diff shows the introduction of an empty `undeploy` method, and the existing method `writeAttributes` is removed instead of being modified to accommodate the newly specified feature. Additionally, there's no code present that suggests the configuration of distinct names for EJBs.

There is no indication of a bug being fixed in this commit; it appears to be a structural code change that doesn't align with the commit message. The presence of an empty method also hints that the implementation is not complete.

Therefore, the changes don't reflect a bug fix as they don't address any error-handling, logical corrections, or exception-handling improvements. Hence, the conclusion is **NotBuggy**."
wildfly,7993.json,315a9c2129eb1ed4921ed4ea128e94c5014ff010,"@@ -1,8 +1,8 @@
-    static void addConnectorServiceConfigs(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
+    static void addConnectorServiceConfigurations(final OperationContext context, final Configuration configuration, final ModelNode model)  throws OperationFailedException {
         if (model.hasDefined(CommonAttributes.CONNECTOR_SERVICE)) {
             final List<ConnectorServiceConfiguration> configs = configuration.getConnectorServiceConfigurations();
             for (Property prop : model.get(CommonAttributes.CONNECTOR_SERVICE).asPropertyList()) {
                 configs.add(createConnectorServiceConfiguration(context, prop.getName(), prop.getValue()));
             }
         }
     }",NotBuggy," [WFLY-12859]:  Refactoring code to centralize the configuration of the broker.
",NotBuggy,"The commit message indicates a refactor to centralize broker configuration. The code change renames a method from `addConnectorServiceConfigs` to `addConnectorServiceConfigurations`. This is a simple renaming for clarity and does not suggest a bug fix; there is no logic change or bug-related context. Therefore, the answer is **NotBuggy**."
wildfly,21431.json,8da4de218c105bcf583f886e35a107b43708cdee,"@@ -1,38 +1,37 @@
     public void readElement(final XMLExtendedStreamReader reader, final ParseResult<JBossServiceXmlDescriptor> value) throws XMLStreamException {
         final JBossServiceXmlDescriptor serviceXmlDescriptor = new JBossServiceXmlDescriptor();
         final List<JBossServiceConfig> serviceConfigs = new ArrayList<JBossServiceConfig>();
         serviceXmlDescriptor.setServiceConfigs(serviceConfigs);
         value.setResult(serviceXmlDescriptor);
 
         final int count = reader.getAttributeCount();
         for(int i = 0; i < count; i++) {
-            final Attribute attribute = Attribute.of(reader.getAttributeName(i));
+            final QName attributeName = reader.getAttributeName(i);
+            final Attribute attribute = Attribute.of(attributeName);
             final String attributeValue = reader.getAttributeValue(i);
             switch(attribute) {
                 case MODE:
                     serviceXmlDescriptor.setControllerMode(JBossServiceXmlDescriptor.ControllerMode.of(attributeValue));
                     break;
-                default:
-                    throw unexpectedContent(reader);
             }
         }
 
         while (reader.hasNext()) {
             switch (reader.nextTag()) {
                 case COMMENT:
                     break;
                 case END_ELEMENT:
                     return;
                 case START_ELEMENT:
                     switch(Element.of(reader.getName())) {
                         case MBEAN:
                             serviceConfigs.add(parseMBean(reader));
                             break;
                         case UNKNOWN:
                             unexpectedContent(reader);
                             break;
                     }
                     break;
             }
         }
     }",NotBuggy,"Start to finish up parsed service deployment.  Testing, etc...
",NotBuggy,"The commit message suggests working on parsed service deployment, involving testing. The changes within the diff primarily focus on handling XML element attributes and structure but don't directly address a bug fix. Here’s a detailed analysis:

1. **Change in Attribute Handling**: 
   - The line `final Attribute attribute = Attribute.of(reader.getAttributeName(i));` is refactored to introduce a new variable `QName attributeName`, which stores the result of `reader.getAttributeName(i)`, and then used to create `Attribute` with `attributeName`. This looks like a minor refactor without altering functionality—it's likely for readability or future modification considerations.

2. **Exception Handling**: 
   - The removal of the `default` case and its `unexpectedContent(reader)` method invocation inside the attribute switch statement implies that unknown attributes will no longer throw an exception. This could be an intended change or oversight, but without additional context, it doesn’t clearly indicate addressing a bug.

3. **Structural Changes**:
   - There haven't been modifications to core logic or algorithm fixes that typically signal a bug fix.

Overall, the refactor of the attribute handling doesn't represent a bug fix, rather it aligns more with code cleanup or structural organization in preparation for additional changes. No explicit error correction or functional improvement indicates a bug fix.

Therefore, while the commit is part of a broader effort to prepare the service deployment feature, the specific changes in this diff do not clearly indicate a bug fix. Thus, I would conclude **NotBuggy**."
wildfly,9094.json,98592f3855adb79a6b9af100072db28f290929e5,"@@ -1,3 +1,3 @@
-    public Context getValue() throws IllegalStateException {
-        return context;
+    public ComponentView getValue() throws IllegalStateException, IllegalArgumentException {
+        return instance;
     }",NotBuggy,"Rework EE Component model for extensibility into EJBs and beyond
",NotBuggy,"The commit message states a rework for extensibility, which generally implies modifying code structures to accommodate future changes or integrations, rather than fixing specific errors. In the provided diff, the return type of the method `getValue()` is changed from `Context` to `ComponentView`, and the return variable is changed from `context` to `instance`. Additionally, the method signature is updated to potentially throw an `IllegalArgumentException` in the future.

Such changes focus on altering the existing method to return a broader or different type and reflect a design rework to enhance flexibility—there is no immediate indication of a bug being fixed. 

Based on this, the result is **NotBuggy**."
wildfly,1756.json,546d5ace217411e91be9573c4ddf0c4e88fc6654,"@@ -1,17 +1,19 @@
     private void parseBackups(XMLExtendedStreamReader reader, PathAddress cacheAddress, Map<PathAddress, ModelNode> operations) throws XMLStreamException {
 
-        PathAddress backupsAddress = cacheAddress.append(BackupsResourceDefinition.PATH);
+        PathAddress address = cacheAddress.append(BackupsResourceDefinition.PATH);
+        ModelNode operation = Util.createAddOperation(address);
+        operations.put(address, operation);
 
         while (reader.hasNext() && (reader.nextTag() != XMLStreamConstants.END_ELEMENT)) {
             XMLElement element = XMLElement.forName(reader.getLocalName());
             switch (element) {
                 case BACKUP: {
-                    this.parseBackup(reader, backupsAddress, operations);
+                    this.parseBackup(reader, address, operations);
                     break;
                 }
                 default: {
                     throw ParseUtils.unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5672 Implicitly added resources should be added by parent add operation instead of the parser
",NotBuggy,"The commit message refers to ensuring that resources are added by the parent operation, as opposed to by a parser. The code diff reflects this by adding a new operation to an operations map for a constructed PathAddress. This use of the Util.createAddOperation method suggests an explicit creation and addition of resources rather than leaving it to the parser.

Here's a step-by-step analysis of the changes:
1. **Variable Rename and Addition**:
   - The code renames a variable from `backupsAddress` to `address`. This alone doesn't indicate a bug fix or change in functionality.
   - More importantly, the addition of `ModelNode operation = Util.createAddOperation(address);` and the subsequent `operations.put(address, operation);` suggests that resources are now being added more explicitly. This aligns with the commit message's content about resources being managed by the parent add operation rather than implicitly through parsing.

2. **Maintenance of Functionality**:
   - The call to `this.parseBackup(reader, address, operations);` was adjusted to maintain association with the newly defined address and operations map.

3. **Commit Message Alignment**:
   - The commit message describes a shift in resource management, and the changes contribute to this by ensuring the resources are explicitly added during parsing, which directly aligns with the commit message explanation.

Overall, the code change addresses the issue noted in the commit message by changing how resources are handled during the parsing process, aligning with the commit message’s description and suggesting a logical correction in the process.

Given that this reflects more of a logical correction in handling rather than fixing a bug (in terms of an error in the existing functionality), the nature of the change is more about refining the architecture rather than fixing an outright bug. Based on this clarification, the answer should be **NotBuggy**."
wildfly,14902.json,d9e37a5a768a6ba4f4a3fdd692cab4d5d6064667,"@@ -1,8 +1,8 @@
             public ManagedReference getReference() {
                 try {
-                    final Object value = objectFactoryClassInstance.getObjectInstance(name, null, null, environment);
+                    final Object value = new InitialContext().lookup(lookup);
                     return new ValueManagedReference(new ImmediateValue<Object>(value));
                 } catch (Exception e) {
                     throw new RuntimeException(e);
                 }
             }",NotBuggy,"Revert ""AS7-6121: re-implements subsystem 'lookup' binding service with a linkref, allowing Context#lookupLink() to return the link name""

This reverts commit 9ce9d0010c78fe957fc92b4180e53ea60702994d.
",NotBuggy,"The commit message indicates a reversion of a previous change. The issue seems to involve how an object is retrieved, switching from using an object factory method (objectFactoryClassInstance.getObjectInstance) back to a more straightforward JNDI lookup (new InitialContext().lookup). While reversions often indicate addressing problems introduced by the original change, this diff merely reverses the implementation without explicit error handling, logical, or exception improvements. Based on the information alone, the change suggests returning to a previous state, typically not a sign of a fixed bug but a restoration. Thus, without further context from the reverted commit, it is best to conclude that this is **NotBuggy**."
wildfly,12438.json,92d027aaed28b83a6057d45690c0bbf8b5cecbaa,"@@ -1,4 +1,4 @@
         void close() {
-            this.clientMappingRegistry.removeListener(this);
+            this.listenerRegistration.close();
             this.clusterTopologyListeners.clear();
         }",NotBuggy,"Update usage of deprecated Group listener API.
",NotBuggy,"The commit message indicates an update due to a deprecated API usage, focusing on replacing old methods with new ones for maintaining up-to-date code practices. The diff specifically replaces `this.clientMappingRegistry.removeListener(this);` with `this.listenerRegistration.close();`, suggesting a switch from a deprecated API to a supported one. This change aligns with best practices but doesn't explicitly indicate a bug fix.

Thus, while this modification keeps the codebase modern and warning-free, it does not necessarily address an existing bug or faulty behavior in the application, but rather follows the guidance to transition away from deprecated methods.

The conclusion is **NotBuggy**."
wildfly,13536.json,b63c847db3fc4c885c809d837aee4361711166f8,"@@ -1,11 +1,11 @@
         void executeRuntime(final OperationContext context, final ModelNode operation) throws OperationFailedException {
             final String opName = operation.require(ModelDescriptionConstants.OP).asString();
             if (!opName.equals(ModelDescriptionConstants.READ_ATTRIBUTE_OPERATION)) {
-                throw MESSAGES.unknownOperations(opName);
+                throw EjbLogger.ROOT_LOGGER.unknownOperations(opName);
             }
 
             final TimerImpl timer = getTimer(context, operation);
 
             readAttribute(timer, context.getResult());
             context.completeStep(OperationContext.RollbackHandler.NOOP_ROLLBACK_HANDLER);
         }",NotBuggy,"[WFLY-2864] WildFly EJB module
",NotBuggy,"The code change involves substituting an error message source from `MESSAGES` to `EjbLogger.ROOT_LOGGER` when an unknown operation is detected. This alteration modifies logging behavior rather than addressing any logic or functionality-related issues. While it may provide better logging or align with a new logging strategy, this change does not suggest a correction of a bug affecting application behavior or logic.

The commit message ""[WFLY-2864] WildFly EJB module"" is more about a module update and doesn't suggest a bug fix. The specific change regards logging modifications, which aim to enhance log output or standardize logging across different parts of the application rather than fixing an error in the code logic.

Therefore, the evaluation of this commit concludes with **NotBuggy**."
wildfly,9080.json,c4dceafe3f52e61a8bb318f10722eae193da8c62,"@@ -1,3 +1,3 @@
-    protected AtomicReference<ManagedReference> getInstanceReference() {
-        return instanceReference;
+    public Object getInstanceData(Object key) {
+        return instanceData.get(key);
     }",NotBuggy,"WFLY-2021 Change EE interceptors to be stateless

All state is now stored on the ComponentInstance, and can be setup during the post construct
phase. This largely removes the need for InterceptorFactory. At most a single instance of
each interceptor chain will be created per component, this should result in a massive decrease
in the amount of memory a component instance, and an instance reference allocates.
",NotBuggy,"The commit message indicates changes to how interceptors handle state in the context of WildFly, aiming to make them stateless and improve memory usage. The diff shows modifications to a method related to instance data retrieval. Initially, it returns an `AtomicReference<ManagedReference>`, whereas the new method retrieves data from `instanceData` with a `key`. The change aligns with making interceptors stateless by removing dependencies on possibly shared mutable references and moving towards a per-key data retrieval mechanism.

Here’s the evaluation of whether the diff points to a bug fix:
- **State Handling**: The focus is on reducing state within interceptors. The change from a protected `AtomicReference` to a method fetching data with a key suggests a strategic move rather than a correction of erroneous logic.
- **Memory Efficiency**: The change in methodology should enhance memory efficiency but doesn't necessarily correct a malfunction or defect.

The diff showcases a refactoring effort to support the system architecture requirements outlined in the commit description but does not depict a typical bug-fixing pattern such as correcting logical errors or fixing exception handling. 

Therefore, the conclusion based on the provided information is **NotBuggy**."
wildfly,14713.json,45fe4a432cf42b096be038ece2ff80b69c9d930c,"@@ -1,3 +1,3 @@
-    public void rebind(final String name, final Object obj) throws NamingException {
-        throw MESSAGES.readOnlyNamingContext();
+    public void rebind(final String name, final Object object) throws NamingException {
+        rebind(parseName(name), object);
     }",NotBuggy,"Introduce writable naming context
",NotBuggy,"The commit message indicates the introduction of a writable naming context. The code change removes a read-only exception on `rebind` and calls a `rebind` method with parsed parameters. This aligns with making the naming context writable by allowing rebindings instead of throwing an exception. However, this does not directly imply a bug fix but rather an enhancement or added functionality. The change doesn't indicate a correction in logic, error handling, or exception management commonly associated with bug fixes.

Therefore, the appropriate conclusion for whether this change indicates a bug fix is **NotBuggy**."
wildfly,13160.json,8fec6d40a157018646ee131584fe5b88e31ef230,"@@ -1,53 +1,56 @@
     protected void parseRemote(final XMLExtendedStreamReader reader, List<ModelNode> operations) throws XMLStreamException {
         final int count = reader.getAttributeCount();
         final PathAddress ejb3RemoteServiceAddress = SUBSYSTEM_PATH.append(SERVICE, REMOTE);
         ModelNode operation = Util.createAddOperation(ejb3RemoteServiceAddress);
         final EnumSet<EJB3SubsystemXMLAttribute> required = EnumSet.of(EJB3SubsystemXMLAttribute.CONNECTOR_REF,
                 EJB3SubsystemXMLAttribute.THREAD_POOL_NAME);
         for (int i = 0; i < count; i++) {
             requireNoNamespaceAttribute(reader, i);
             final String value = reader.getAttributeValue(i);
             final EJB3SubsystemXMLAttribute attribute = EJB3SubsystemXMLAttribute.forName(reader.getAttributeLocalName(i));
             required.remove(attribute);
             switch (attribute) {
                 case CLIENT_MAPPINGS_CLUSTER_NAME:
                     EJB3RemoteResourceDefinition.CLIENT_MAPPINGS_CLUSTER_NAME.parseAndSetParameter(value, operation, reader);
                     break;
                 case CONNECTOR_REF:
                     EJB3RemoteResourceDefinition.CONNECTOR_REF.parseAndSetParameter(value, operation, reader);
                     break;
                 case THREAD_POOL_NAME:
                     EJB3RemoteResourceDefinition.THREAD_POOL_NAME.parseAndSetParameter(value, operation, reader);
                     break;
+                case EXECUTE_IN_WORKER:
+                    EJB3RemoteResourceDefinition.EXECUTE_IN_WORKER.parseAndSetParameter(value, operation, reader);
+                    break;
                 default:
                     throw unexpectedAttribute(reader, i);
             }
         }
         if (!required.isEmpty()) {
             throw missingRequired(reader, required);
         }
         // each profile adds it's own operation
         operations.add(operation);
 
         final Set<EJB3SubsystemXMLElement> parsedElements = new HashSet<EJB3SubsystemXMLElement>();
         while (reader.hasNext() && reader.nextTag() != XMLStreamConstants.END_ELEMENT) {
             EJB3SubsystemXMLElement element = EJB3SubsystemXMLElement.forName(reader.getLocalName());
             switch (element) {
                 case CHANNEL_CREATION_OPTIONS: {
                     if (parsedElements.contains(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS)) {
                         throw unexpectedElement(reader);
                     }
                     parsedElements.add(EJB3SubsystemXMLElement.CHANNEL_CREATION_OPTIONS);
                     this.parseChannelCreationOptions(reader, ejb3RemoteServiceAddress, operations);
                     break;
                 }
                 case PROFILES: {
                     parseProfiles(reader, operations);
                     break;
                 }
                 default: {
                     throw unexpectedElement(reader);
                 }
             }
         }
     }",NotBuggy,"WFLY-5886 Remote EJB dispatches to thread pools twice
",Buggy,"The commit message mentions a specific bug (WFLY-5886) related to Remote EJB dispatching to thread pools twice. The code change includes the addition of handling a new XML attribute, `EXECUTE_IN_WORKER`. This attribute appears to address how the EJB remote service handles thread pool execution, possibly avoiding unnecessary dispatching to thread pools.

1. **Code Context**: The change is in a method that parses XML configuration for EJB3 remote services, setting parameters based on XML attributes.

2. **Specific Code Change**: The addition of handling for `EXECUTE_IN_WORKER` suggests that this attribute could control whether a certain operation is executed within an existing worker thread, potentially avoiding redundant thread pool dispatching.

3. **Relation to Commit Message**: The commit message directly mentions a bug about dispatching to thread pools twice. The code change introduces a parameter that might optimally dictate thread execution strategies, aligning well with the intent to fix such a dispatching issue.

4. **Potential Bug Fix Indicators**:
   - The change adds logic related to how tasks are dispatched, which aligns with the problem described in the commit message.
   - The XML attribute name `EXECUTE_IN_WORKER` suggests control over thread execution that could indeed resolve the double dispatching bug.

Considering these points, the code change aligns with the commit message's goal to fix an issue concerning thread pool use in Remote EJB, indicating a bug fix. Therefore, the conclusion is **Buggy**."
wildfly,777.json,59f6d84cb689c50e44f68aeacd5a41d7e0056fd6,"@@ -1,6 +1,8 @@
     public void schedule(I id) {
-        BeanEntry<I> entry = this.factory.findValue(id);
-        if (entry != null) {
-            this.schedule(id, entry);
+        try (Batch batch = this.batcher.createBatch()) {
+            BeanEntry<I> entry = this.factory.findValue(id);
+            if (entry != null) {
+                this.schedule(id, entry);
+            }
         }
     }",NotBuggy,"WFLY-12587 Distributed web/SFSB expiration after failover schedules premature expiration
",Buggy,"The commit message states that the change addresses distributed web/Stateful Session Bean (SFSB) expiration issues after failover. The diff shows the addition of a try-with-resources block containing a `Batch` object for the scheduling operation. This ensures proper resource management and may fix issues related to scheduling operations not properly committed or managed, which could lead to premature expiration due to improper batch handling.

By introducing `Batch` management, the change likely prevents scheduling operations from running into unexpected states after failover, which aligns with the problem outlined in the commit message. The previous version might have failed to ensure all operations complete as expected, potentially causing premature expiration.

These modifications align with the commit message and indicate a logical correction to prevent the premature expiration reported in bug WFLY-12587.

Therefore, the changes in the diff indicate a bug fix, so the correct conclusion is **Buggy**."
wildfly,16565.json,01a8fd7e3132ffed1bf87443991c7560051b2156,"@@ -1,3 +1,3 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getStatus().name()));
+                context.getResult().set(new ModelNode(ctx.isQueueNewRequests()));
             }",NotBuggy,"Add some more mod_cluster information
",NotBuggy,"The commit message indicates an addition of information related to mod_cluster, but the diff reveals a change in the logic where the result is now set based on `ctx.isQueueNewRequests()` instead of `ctx.getStatus().name()`. This change modifies what information is returned but does not explicitly indicate a bug fix. Without more context about the intended behavior, this seems like a logical change rather than a correction of erroneous behavior. Thus, the alignment with a bug fix is weak.

The change aligns with providing different mod_cluster information, as stated in the commit message, suggesting it modifies what data is being provided rather than fixing a bug in the existing logic. Therefore, the answer is **NotBuggy**."
wildfly,8286.json,cc78bb76673be1d963bb01205cd4e5dcce1c3949,"@@ -1,41 +1,41 @@
     private ValueConfig parseValueValue(final XMLExtendedStreamReader reader) throws XMLStreamException {
         ValueConfig value = null;
         while (reader.hasNext()) {
             switch (reader.next()) {
                 case END_ELEMENT:
                     if (value == null)
                         throw new IllegalArgumentException(""Null value: "" + reader.getLocation());
                     return value;
                 case START_ELEMENT:
-                    switch (Element.of(reader.getName())) {
+                    switch (Element.of(reader.getLocalName())) {
                         case VALUE:
                             value = parseValue(reader);
                             break;
                         case INJECT:
                             value = parseInject(reader);
                             break;
                         case VALUE_FACTORY:
                             value = parseValueFactory(reader);
                             break;
                         case LIST:
                             value = parseList(reader);
                             break;
                         case SET:
                             value = parseSet(reader);
                             break;
                         case MAP:
                             value = parseMap(reader);
                             break;
                         default:
                             throw unexpectedElement(reader);
                     }
                     break;
                 case CHARACTERS:
                     StringValueConfig svc = new StringValueConfig();
                     svc.setValue(reader.getText());
                     value = svc;
                     break;
             }
         }
         throw unexpectedElement(reader);
     }",NotBuggy,"Allow for legacy MC bean deployment to be parsed as well.
",Buggy,"The commit message suggests a functionality enhancement to support legacy MC bean deployment parsing, though the code change might also address a bug. Analyzing the change:

1. **Code Change**:
   - The modified line changes `Element.of(reader.getName())` to `Element.of(reader.getLocalName())`.
   
2. **Implications of the Change**:
   - **`getName()` vs `getLocalName()`**: In XML parsing, `getName()` typically provides the qualified name (including namespace if present), while `getLocalName()` retrieves the name without namespace.
   - Changing to `getLocalName()` indicates that previously, the method may have struggled to correctly identify elements that belonged to a namespace, potentially causing the parser to fail in some situations.

3. **Relation to Message**:
   - The change aligns with the idea of enhancing the parser to support legacy configurations, as these might not conform strictly to new namespace standards.
   - Additionally, using `getLocalName()` can resolve parsing issues where the element was improperly identified due to namespace complications—a common parsing bug.

4. **Conclusion**:
   - Given that the change both accommodates legacy configurations and potentially fixes a parsing misinterpretation, it does indicate the presence of a bug related to element identification.

Therefore, the changes likely resolve a parsing bug related to incorrect handling of XML namespaces, aligning with the commit message’s mention of enhanced parsing capabilities for legacy deployment. The answer is **Buggy**."
wildfly,16563.json,dee5c658c668a3752e7273dc0b271b84b16aa753,"@@ -1,3 +1,7 @@
             protected void handleNode(OperationContext context, ModClusterStatus.Node ctx, ModelNode operation) throws OperationFailedException {
-                context.getResult().set(new ModelNode(ctx.getUri().toString()));
+                ModelNode list = new ModelNode(ModelType.LIST);
+                for(String alias : ctx.getAliases()) {
+                    list.add(alias);
+                }
+                context.getResult().set(list);
             }",NotBuggy,"WFLY-4992 add nodes alises
",NotBuggy,"The commit message states ""add nodes aliases,"" and the code changes correspond to this by modifying the `handleNode` method to include Node's aliases in the result, instead of just setting a single URI. This does not indicate a direct bug fix but rather an enhancement or new feature to include more data in the result.

Here's a breakdown of the reasoning:
1. **Commit Message Relevance**: The commit message mentions adding node aliases, which aligns with the change to include aliases in the result. This suggests the change is intentional and aligned with the commit message.
   
2. **Code Change**: Previously, the code set the result to a node's URI. The modification changes the result to a list containing all aliases of the node, reflecting an enhancement.
   
3. **Bug Fix Indicators**: No typical bug-fix indicators are present (e.g., no error-handling updates, logical corrections, or exception improvements).

Given no indicators of fixing a bug and the alignment with the commit message about adding node aliases, the conclusion is **NotBuggy**."
javaparser,4978.json,ee156d8b09ad78f5eb75e0cc1b8d74999f16cafe,"@@ -1,3 +1,3 @@
-		public Pos transform(Pos pos) {
+		public Position transform(Position pos) {
 			return lookup(pos).transform(pos);
 		}",NotBuggy,Apply recorded position mapping in a parser post-processing phase.,NotBuggy,"The commit message states that a recorded position mapping is applied during the parser post-processing phase, suggesting improvement or adjustment to position handling. 

Analyzing the code diff:
- The change involves a method signature alteration, wherein the method parameter and return type change from `Pos` to `Position`.
- The code calls a method `transform(pos)`, suggesting it transforms a position object but does not directly indicate changes beyond type adjustments.

There is no explicit evidence of a bug fix in the logic or error handling within this specific diff, as it predominantly involves renaming or refactoring from `Pos` to `Position`, possibly for clearer naming consistency or integration purposes. Without further context or additional code changes demonstrating corrected behavior or error resolution, the diff does not directly indicate a bug fix.

Therefore, based on the available information, the assessment concludes with **NotBuggy**."
javaparser,6046.json,dcf4b43e5c2f05d22bad9567dcd98d9cdf707c82,"@@ -1,4 +1,4 @@
     public SymbolReference<ResolvedMethodDeclaration> solveMethod(String name, List<ResolvedType> argumentsTypes,
-                                                                  boolean staticOnly, TypeSolver typeSolver) {
-        return getContext().solveMethod(name, argumentsTypes, staticOnly, typeSolver);
+                                                                  boolean staticOnly) {
+        return getContext().solveMethod(name, argumentsTypes, staticOnly);
     }",NotBuggy,"#1910 Removed TypeSolver parameter from various methods.
",NotBuggy,"The commit message indicates the removal of a `TypeSolver` parameter from various methods, which is reflected in the change shown in the diff. The `solveMethod` function no longer takes a `TypeSolver` argument.

This change is consistent with what the commit message describes, ensuring that the unnecessary parameter is not passed. It is a refactor to clean up the code and improve its design, potentially making the code easier to maintain and less error-prone by reducing the complexity or redundancy in method signatures. 

However, this change does not directly indicate a bug fix. The removal doesn't seem to correct a logic error or remedy incorrect behavior in the code. Instead, it likely focuses on code simplification or refactoring. 

Thus, based on the provided diff and commit message, the conclusion is **NotBuggy**."
javaparser,166.json,adb78b277512189cc5f8ecb076d9bf8106f9e1e0,"@@ -1,3 +1,3 @@
-    public boolean isWhiteSpaceOrComment() {
-        return child instanceof Comment;
+    public final boolean isWhiteSpaceOrComment() {
+        return isWhiteSpace() || isComment();
     }",NotBuggy,"issue823: distinguish between spaces and comments
",Buggy,"Examining the code diff, the `isWhiteSpaceOrComment` method has been modified to distinguish between whitespace and comments explicitly by separately checking for both conditions using `isWhiteSpace()` and `isComment()` methods, as opposed to only checking if a child is an instance of `Comment`. This change aligns with the commit message stating the need to differentiate between spaces and comments. By splitting the logic, the method has been updated to correctly identify both whitespace and comments, correcting any prior inaccuracy where only comments were considered.

Given the nature of this change, it appears to fix a logical issue where whitespace content wasn't correctly identified, and now it is. The commit message clearly matches the code changes, fixing an oversight or bug in the original implementation.

Thus, the correct conclusion for this change is **Buggy**."
javaparser,5042.json,26b23bccac8c67aed1023f0b2c142c3a7f3d5f08,"@@ -1,23 +1,22 @@
     public List<ParseResult<CompilationUnit>> tryToParseParallelized(String startPackage) throws IOException {
         assertNotNull(startPackage);
         logPackage(startPackage);
         final Path path = packageAbsolutePath(root, startPackage);
-        ParallelParse parse = new ParallelParse(path, new ParallelParse.VisitFileCallback() {
-            @Override
-            public FileVisitResult process(Path file, BasicFileAttributes attrs) {
-                if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
-                    Path relative = root.relativize(file.getParent());
-                    try {
-                        tryToParse(relative.toString(), file.getFileName().toString(), new JavaParser(
-                                SourceRoot.this.javaParser.getParserConfiguration()));
-                    } catch (IOException e) {
-                        Log.error(e);
-                    }
+        ParallelParse parse = new ParallelParse(path, (file, attrs) -> {
+            if (!attrs.isDirectory() && file.toString().endsWith("".java"")) {
+                Path relative = root.relativize(file.getParent());
+                try {
+                    tryToParse(
+                            relative.toString(),
+                            file.getFileName().toString(),
+                            new JavaParser(parserConfiguration));
+                } catch (IOException e) {
+                    Log.error(e);
                 }
-                return FileVisitResult.CONTINUE;
             }
+            return FileVisitResult.CONTINUE;
         });
         ForkJoinPool pool = new ForkJoinPool();
         pool.invoke(parse);
         return getCache();
     }",NotBuggy,"Redo SourceRoot again
",NotBuggy,"The commit message ""Redo SourceRoot again"" suggests changes to the implementation of the SourceRoot class in order to adjust or improve its functionality. Let's analyze the code diff to determine whether this change is related to a bug fix.

1. **Code Changes**:
   - The existing anonymous class implementation for `VisitFileCallback` has been replaced with a lambda expression.
   - The method calls within the lambda remain similar to the original, maintaining the file parsing logic.
   - The call to `JavaParser`'s constructor has been simplified by directly using `parserConfiguration` instead of `SourceRoot.this.javaParser.getParserConfiguration()`.
   - The code structure is more concise with lambda usage, but logically equivalent.
   
2. **Analysis**:
   - This refactor, moving from an anonymous inner class to a lambda expression, is primarily a syntactic and occasionally stylistic improvement, taking advantage of Java's lambda features.
   - The shift in the way `parserConfiguration` is referenced does not constitute a fix to logic, but rather a code simplification.
   - The core logic of file handling and parsing remains unchanged.

3. **Conclusion**: 
   - The change does not indicate a bug fix—it refactors the code without altering its core functionality. 
   - The commit message doesn't specify bug-related changes or bugs resolved by these adjustments.

The modifications are mainly code clean-up and improvements in conciseness without impacting existing bug-related logic. Therefore, the answer is **NotBuggy**."
javaparser,3272.json,6d06ae94c521628da4bcd76675f3660fee8ebf76,"@@ -1,3 +1,3 @@
-    public ModuleProvidesStmtMetaModel getMetaModel() {
-        return JavaParserMetaModel.moduleProvidesStmtMetaModel;
+    public ModuleProvidesDirectiveMetaModel getMetaModel() {
+        return JavaParserMetaModel.moduleProvidesDirectiveMetaModel;
     }",NotBuggy,"Module statements -> directives
",NotBuggy,"The commit message indicates a name change from ""statements"" to ""directives"" within a module. The code change reflects this by updating the method return type and the meta-model reference to match this new naming convention. There is no indication of a bug fix here, as the changes are renaming items rather than correcting a malfunction or error. The primary focus is on aligning with new terminology or improved clarity.

Overall, the modifications adhere to the commit message accurately, but they don't relate to fixing a bug. Instead, they are more about updating the naming convention or possibly adhering to a new standard or specification.

So, the conclusion is **NotBuggy**."
javaparser,225.json,c806b3355d14040857d754094f07595d7f84e02c,"@@ -1,3 +1,3 @@
-    boolean isProcessed() {
+    final boolean isProcessed() {
         return isProcessed;
     }",NotBuggy,"JavaDoc, newline at end of files and made methods final
",NotBuggy,"The commit message mentions documentation updates, file-ending newlines, and making methods final. The git diff shows only the `isProcessed` method being made final, without changes to JavaDoc or newlines. Making methods final is primarily a design decision to prevent overriding, not a bug fix. There is no indication of bug-related changes such as error handling or logical corrections. The commit message and code modification are consistent regarding making the method final, but there is no indication of a bug fix in this commit.

Therefore, the conclusion is **NotBuggy**."
javaparser,6917.json,37f742068fc5f198eef8c019d7f9673423c0b36a,"@@ -1,13 +1,13 @@
-    static String extractParameterName(CtBehavior method, int paramNumber) {
+    static Optional<String> extractParameterName(CtBehavior method, int paramNumber) {
         MethodInfo methodInfo = method.getMethodInfo();
         CodeAttribute codeAttribute = methodInfo.getCodeAttribute();
         if (codeAttribute != null) {
             LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute
                     .tag);
             if (attr != null) {
                 int pos = Modifier.isStatic(method.getModifiers()) ? 0 : 1;
-                return attr.variableName(paramNumber + pos);
+                return Optional.ofNullable(attr.variableName(paramNumber + pos));
             }
         }
-        return null;
+        return Optional.empty();
     }",NotBuggy,"Adding javadoc comment to explain why the name might not be available.

Changed return value from String to Optional to avoid null. Renamed tests to conform to code style.
",NotBuggy,"The commit message states changes in return type from `String` to `Optional<String>` to avoid returning `null`, which aligns with the modernization of avoiding nulls and using `Optional` for potentially absent values. This transformation enhances code readability and robustness by explicitly signaling the potential non-availability of a value. However, this change does not indicate a bug fix but rather an enhancement or refactor aimed at improving code quality. There are no error-handling updates or corrections addressing logical faults.

Therefore, the conclusion is **NotBuggy**."
javaparser,535.json,621f1f76216a62bf1631fb337ba6c126c326a0cc,"@@ -1,5 +1,5 @@
     public String print(Node node) {
-        final PrettyPrintVisitor visitor = configuration.getVisitorFactory().apply(configuration);
+        final VoidVisitor<Void> visitor = configuration.getVisitorFactory().apply(configuration);
         node.accept(visitor, null);
-        return visitor.getSource();
+        return visitor.toString();
     }",NotBuggy,"Make toString's printer configuration accessible
",NotBuggy,"The code change involves a revision in the type of visitor object used in the `print` method and alters how the output is retrieved. Specifically:

1. The type of `visitor` is changed from `PrettyPrintVisitor` to `VoidVisitor<Void>`.
2. The method used to retrieve a string representation changes from `visitor.getSource()` to `visitor.toString()`.

Analyzing the commit message and code changes:

- **Commit Message Analysis**: The commit message states making the printer configuration accessible, which doesn't directly imply a bug fix. It suggests improving or exposing some functionalities, potentially for customization or extending access.
- **Code Change Analysis**: The change in visitor type might relate to using a more generic or interface-based structure (`VoidVisitor<Void>` instead of `PrettyPrintVisitor`). The shift from `getSource()` to `toString()` suggests either a simplification or a unification of method calls for obtaining the string output. However, there is no direct indication of fixing any incorrect behavior or logical error.

Given this analysis, the modifications seem more like an enhancement or refactor for better functionality exposure rather than correcting a bug.

Therefore, the conclusion for this commit is **NotBuggy**."
javaparser,6134.json,5d8d0cb7947efcf2282f7d056302dd905f348532,"@@ -1,3 +1,3 @@
-    public ClassDeclaration declaringType() {
+    public ResolvedClassDeclaration declaringType() {
         return classDeclaration;
     }",NotBuggy,"more work on converting to moved classes
",NotBuggy,"The commit message indicates restructuring or refactoring relating to class modifications, suggesting ongoing work on codebase organization. The change involves replacing a return type of a method, from `ClassDeclaration` to `ResolvedClassDeclaration`. This suggests an adaptation to possibly a new or refactored API or class library, but it doesn't specifically point to a bug fix.

The change doesn't indicate any corrections related to error handling, exception handling, or logical validity directly attributable to bug fixing. Instead, it's likely a structural change to accommodate a different class structure—possibly part of a larger refactoring or upgrade effort. 

Thus, without additional context pointing to bug fixes or issues resolved by this specific type change, the modification doesn't strongly align with being a bug fix on its own. The conclusion is **NotBuggy**."
javaparser,5700.json,e15d5a8aa84a34917e4ef47135dfc62e58ea7650,"@@ -1,3 +1,3 @@
-    public List<Type> getFormalArgumentTypes() {
+    public List<ResolvedType> getFormalArgumentTypes() {
         return formalArgumentTypes;
     }",NotBuggy,"correcting module core to use new classes
",NotBuggy,"The commit message states a correction regarding the usage of new classes within the module core. The change shown in the diff seems to address that by updating the return type of the method `getFormalArgumentTypes` from `List<Type>` to `List<ResolvedType>`. This kind of modification typically reflects a refactor to align with updates in the codebase, particularly when new classes are integrated and used in place of older ones.

However, this type of change alone doesn't clearly indicate a direct bug fix. It rather suggests an adaptation or improvement in the interface or implementation to stay current with the newer design or requirements of the system. Without additional context or evidence of error correction, the modification appears to be more about code evolution than resolving a bug.

Thus, given the information presented and the nature of the modification, the conclusion is **NotBuggy**."
javaparser,1394.json,9a62aa11f05a3eb80b8d11a756ff63be0b259b95,"@@ -1,3 +1,3 @@
     public Integer visit(MethodCallExpr n, Void arg) {
-        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0);
+        return (n.getArguments().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getScope().isPresent() ? n.getScope().get().accept(this, arg) : 0) * 31 + (n.getTypeArguments().isPresent() ? n.getTypeArguments().get().accept(this, arg) : 0) * 31 + (n.getComment().accept(this, arg));
     }",NotBuggy,"Use superclass fields for hashcode generation too
",NotBuggy,"The commit message suggests enhancing hashcode generation by including superclass fields. However, the code change adds `n.getComment().accept(this, arg)` to the calculation. This does not involve superclass fields, but it does enhance coverage of all potential fields in the class. There's no indication of fixing a bug, as it focuses more on improving the completeness of the hash computation by including the comment. This can be considered a functional adjustment rather than a bug fix.

Thus, the changes in the code don't align perfectly with the intent expressed in the commit message, and they don't clearly indicate the resolution of a bug in terms of a logical error or exception handling omission. Therefore, the analysis leads us to conclude: **NotBuggy**."
javaparser,640.json,ca2adb511d3e8c8b73c63a5da6cb3b8e49fe5564,"@@ -1,3 +1,3 @@
     public ResolvedIntersectionType resolve() {
-        return getSymbolResolver().resolve(this, ResolvedIntersectionType.class);
+        return getSymbolResolver().toResolvedType(this, ResolvedIntersectionType.class);
     }",NotBuggy,"divide SymbolResolver.resolve in two methods
",NotBuggy,"The commit message indicates the intention to refactor by dividing a method into two separate methods. However, the provided diff shows a single change: replacing a call to `resolve` with `toResolvedType`. There's no evidence in this diff of dividing a method into two; instead, it simply changes the method being called.

1. **Commit Message Consistency**: The commit message describes a refactoring to split a method, but this diff indicates only a method name change, suggesting a possible refactor or method behavior alteration.

2. **Code Change Analysis**: 
   - The original method `resolve` is replaced with `toResolvedType`. If `resolve` was supposed to divide into two methods and the replacement `toResolvedType` suggests one such method, the diff shows just one half of the described change.
   - The change doesn't necessarily address any bug-related behavior unless the `resolve` method had issues requiring correction via `toResolvedType`. However, this can't be inferred without additional context or the actual implementation details of both methods.

3. **Bug Fix Indicators**: 
   - There's no indication of error handling, exception adjustments, or logic correction that would hint explicitly at a bug being fixed in this diff alone. Without knowing the inner logic of `resolve` and `toResolvedType`, the effectiveness of the bug fix remains unclear.

Conclusively, given the provided information, the diff doesn't reflect the stated purpose of dividing a method, nor does it clearly indicate a bug fix. Therefore, based on the available context:

**NotBuggy**"
javaparser,3650.json,54abb2027d67c0efc1c82712175e06d7eed2593b,"@@ -1,5 +1,2 @@
     public void ifSynchronizedStmt(Consumer<SynchronizedStmt> action) {
-        if (isSynchronizedStmt()) {
-            action.accept(asSynchronizedStmt());
-        }
     }",NotBuggy,"Be more like Java Symbol Solver (for ifType)
",NotBuggy,"The commit message indicates changes to align with the behavior of a Java Symbol Solver (likely a specific behavior or style preference), not necessarily a bug fix. The code update removes conditional checking and calls the consumer directly, suggesting a change in design or functionality without any clear indication of error correction or bug solving. There's no error-handling, logical correction, or exception management update evident in the diff. Therefore, these changes do not align with a bug fix; they seem to be more about refactoring or changing behavior to match a certain style or expectation. 

The conclusion is **NotBuggy**."
javaparser,5313.json,356ae8dcc930e7ff6e418c30f49c922c08279786,"@@ -1,3 +1,3 @@
     public static CompilationUnit parseResource(final ClassLoader classLoader, final String path, Charset encoding) throws IOException {
-        return handleResult(parser.parseResource(classLoader, path, encoding));
+        return handleResult(newParser().parseResource(classLoader, path, encoding));
     }",NotBuggy,"Make StaticJavaParser thread safe
",Buggy,"The commit message indicates a modification aimed at making `StaticJavaParser` thread-safe. The change in the diff modifies a method to create a new parser instance each time `parseResource` is called, instead of using a potentially shared `parser` object. This alteration ensures that concurrent calls to `parseResource` do not use shared mutable state, which is a classic source of threading issues. As this change addresses thread-safety concerns, it aligns with correcting a significant bug in multithreaded environments. Hence, the answer is **Buggy**."
javaparser,946.json,9a03bc6291c871b2d705a17b4cad923bc954ba84,"@@ -1,3 +1,3 @@
     public Integer visit(final AnnotationMemberDeclaration n, final Void arg) {
-        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().accept(this, arg)) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
+        return (n.getDefaultValue().isPresent() ? n.getDefaultValue().get().accept(this, arg) : 0) * 31 + (n.getModifiers().hashCode()) * 31 + (n.getName().accept(this, arg)) * 31 + (n.getType().accept(this, arg)) * 31 + (n.getAnnotations().accept(this, arg));
     }",NotBuggy,"Revert ""Create a Modifier node""
",Buggy,"The commit message indicates a revert of a previous change related to creating a Modifier node. In the diff, we see that the code for visiting modifiers within annotation member declarations has been changed. Specifically, it reverts a call to accept the `visit` method on a `Modifier` node back to using standard `hashCode()` method for `n.getModifiers()`.

This suggests that the previous approach that used `accept(this, arg)` led to an undesired outcome, potentially breaking or changing the expected behavior, possibly introducing a bug. Reverting back to `hashCode()` could be a fix for an inconsistency or error introduced by the previous implementation.

Since the commit message indicates a revert, and reverts typically indicate undoing a previous error or undesirable change, the context provided here fits a bug fix profile, especially since this impacts a key method that could affect program behavior significantly.

Therefore, the answer is **Buggy**."
intellij-community,24004.json,9c7a04d5af8b287ea37ac662bb86d527e0253acd,"@@ -1,3 +1,9 @@
   protected ActionToolbar createToolbar() {
-    return null;
+    ActionGroup actionGroup = createLeftToolbarActionGroup();
+    if (actionGroup != null) {
+      return ActionManager.getInstance().createActionToolbar(""TextEditorWithPreview"", actionGroup, true);
+    }
+    else {
+      return null;
+    }
   }",NotBuggy,"IDEA-CR-48269: PY-34819 TextEditorWithPreview and Markdown SplitFileEditor changed according to UX/UI team recommendations

* Right action group customization added in TextEditorWithPreview
* Gutter alignment turned off for the left toolbar
* Reserved spaces removed from the toolbars

GitOrigin-RevId: bb53affaa3ace7e1191757631ed9756fa2c22b6c",NotBuggy,"This commit message mentions UX/UI changes with no reference to bug fixes. The diff replaces a hardcoded `null` return in `createToolbar()` with code that conditionally returns a toolbar based on an action group. This is indicative of a feature or improvement rather than a bug fix, aligning with UX/UI enhancements noted in the commit message. The answer is **NotBuggy**."
intellij-community,33570.json,62eeca9c70c26cc115590b8ddfdab4d4cfa83eb7,"@@ -1,20 +1,19 @@
   public ProjectTemplate[] createTemplates(String group, WizardContext context) {
     Collection<Pair<URL, ClassLoader>> urls = myGroups.getValue().get(group);
     List<ProjectTemplate> templates = new ArrayList<ProjectTemplate>();
     for (Pair<URL, ClassLoader> url : urls) {
       try {
         List<String> children = UrlUtil.getChildrenRelativePaths(url.first);
         for (String child : children) {
           if (child.endsWith(ZIP)) {
             URL templateUrl = new URL(url.first.toExternalForm() + ""/"" + child);
-            String name = getTemplateName(child);
-            templates.add(new LocalArchivedTemplate(name, templateUrl, url.second));
+            templates.add(new LocalArchivedTemplate(templateUrl, url.second));
           }
         }
       }
       catch (IOException e) {
         LOG.error(e);
       }
     }
     return templates.toArray(new ProjectTemplate[templates.size()]);
   }",NotBuggy,"new project wizard: adding project templates
",NotBuggy,"The commit message indicates the addition of project templates in a new project wizard. However, the diff shows the removal of a template name extraction (`getTemplateName(child)`) during the creation of `LocalArchivedTemplate`. The diff does not contain any added functionality or new features like new templates, suggesting a change or simplification in how templates are handled rather than a feature addition. Given this mismatch between the commit message and code diff, the changes don't seem to be part of adding new functionality as described by the commit message. Also, there's no indication of a bug fix, as the change mainly involves the construction of `LocalArchivedTemplate` without a name. There are no signs of logical corrections or error-handling updates related to a bug. 

Hence, this change does not align with a bug fix. 

**NotBuggy**"
intellij-community,15453.json,ccb245867bc3b8f973f3fe9db7fd9a02a1169f49,"@@ -1,3 +1,3 @@
   public VirtualFile refreshAndFindFileByPath(@NotNull String path) {
-    return VfsImplUtil.refreshAndFindFileByPath(this, path);
+    return isValid(path) ? VfsImplUtil.refreshAndFindFileByPath(this, path) : null;
   }",NotBuggy,"[vfs] early .jar path validation (EA-95201)
",Buggy,"The commit message states there's an early .jar path validation added. The code change introduces a check using `isValid(path)` before calling `refreshAndFindFileByPath`. If the path is not valid, it immediately returns `null`, effectively preventing further processing on potentially invalid paths. This is a bug fix as it adds a validation step to ensure correct behavior and likely prevents errors that arise from handling invalid paths.

Thus, the changes correspond with the commit message by addressing potential issues that could be caused by invalid paths, which means the changes are relevant to a bug fix.

**Buggy**"
intellij-community,27721.json,3fee046d2007f647e44c0a3744adfba605aad47d,"@@ -1,3 +1,3 @@
           public Dimension getPreferredSize() {
-            return new Dimension(super.getPreferredSize().width, JBUI.scale(44));
+            return new Dimension(super.getPreferredSize().width, JBUIScale.scale(44));
           }",NotBuggy,"[vcs-log] cleanup RootCellRenderer

GitOrigin-RevId: bba216626193ac756cd133b8ae9bc4e577abea42
",NotBuggy,"The commit message mentions a cleanup of `RootCellRenderer`, which suggests a potential improvement or refactor rather than a bug fix. The diff shows a change from `JBUI.scale(44)` to `JBUIScale.scale(44)`. This appears to be a renaming rather than addressing a functionality bug or error in logic. 

There is no indication that the original method usage resulted in incorrect behavior that required fixing; rather, it seems to be aligning the code with a naming convention or updated library usage, which is commonly seen in cleanup operations.

Given these observations, this change does not indicate a bug fix but more of a code maintenance or refactoring task. 

Thus, the conclusion is **NotBuggy**."
intellij-community,25643.json,3b06cfe23b19153bd54c129cb9eba8cae7b4654e,"@@ -1,10 +1,9 @@
-          public Builder setModuleName(
-              int index, String value) {
-            if (value == null) {
+            public Builder setTypeId(String value) {
+              if (value == null) {
     throw new NullPointerException();
   }
-  ensureModuleNameIsMutable();
-            moduleName_.set(index, value);
-            
-            return this;
-          }

+  bitField0_ |= 0x00000001;
+              typeId_ = value;
+              
+              return this;
+            }",NotBuggy,"external compiler: compile scope based on build targets
",NotBuggy,"The commit message refers to changes related to compiling scope based on build targets, but the code diff shows a method signature and logic change. Specifically:

1. The `setModuleName` method is replaced by a `setTypeId` method.
2. The new method also includes field manipulation with `bitField0_`.

These changes don't directly align with the message about compile scope adjustments. Instead, they indicate a refactor or API change rather than a bug fix. The modifications don't show signs of error handling corrections, logical errors, or exception handling improvements typical of bug fixes. Therefore, the changes are not aligned with a bug fix context based on the information provided.

**NotBuggy**"
intellij-community,21829.json,8f877cd64f6e456fa8b4bb6cc67a65a473358191,"@@ -1,3 +1,3 @@
   public boolean isDeclarationWriteAccess(PsiElement element) {
-    return element instanceof PyTargetExpression;
+    return element instanceof PyTargetExpression || element.getParent() instanceof PyDelStatement;
   }",NotBuggy,"detect 'del x' as write access (PY-9784)
",Buggy,"The commit message indicates a change to detect a 'del x' operation as a write access within a code analysis tool for Python code (given the PY-9784 bug reference, which suggests an issue related to Python language features). The diff modification addresses this by extending the condition checked in the `isDeclarationWriteAccess` method. Originally, the method returned true only if the element was an instance of `PyTargetExpression`. The modification adds an additional condition to also return true if the element's parent is an instance of `PyDelStatement`.

Here's a step-by-step analysis:

1. **Understanding the Context**:
   - The context indicates this is likely part of a tool used for analyzing Python code, and it's specifically looking at accessing elements (variables or expressions) in Python.
   - The `isDeclarationWriteAccess` method determines whether a particular element represents a write access in the context of the code.

2. **Original Code**:
   - Before the change, write access was detected only if the element was a `PyTargetExpression`. This likely covers cases where a variable is assigned a new value.

3. **Change Made**:
   - The modification adds another scenario where write access is acknowledged: when the element is part of a `PyDelStatement`. 
   - In Python, the `del` statement deletes a reference to a variable, which can be considered a form of write access since it changes the variable's state by removing it.

4. **Relevance to Bug Fix**:
   - The commit message suggests this is addressing a bug (PY-9784), likely a missed detection of the 'del' statement as a form of write access in the tool's logic.
   - By including `PyDelStatement`, the tool now correctly reflects the write operation nature of 'del', aligning with expected behavior of recognizing all forms of write accesses.

5. **Conclusion**:
   - The change directly corresponds to the commit message, clearly aiming to rectify an oversight in write access detection concerning the 'del' statement in Python.
   - This adjustment indicates a fix for an issue where previously the code did not recognize certain Python code manipulations correctly, thereby aligning it with the commit description.

Based on the analysis, the modification aligns with fixing the specified bug related to handling Python code, and addresses the detection issue outlined in the commit message. Thus, the change is a **Buggy** fix."
intellij-community,20356.json,9f970709265d9f147b917bd74bd19667f5b8536d,"@@ -1,5 +1,5 @@
-  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<String> nameConsumer) {
+  public final void editNewSchemeName(@NotNull String preferredName, boolean isProjectScheme, @NotNull Consumer<? super String> nameConsumer) {
     String name =
       SchemeNameGenerator.getUniqueName(preferredName, schemeName -> getModel().containsScheme(schemeName, isProjectScheme));
     mySchemesCombo.startEdit(name, isProjectScheme, nameConsumer);
   }",NotBuggy,"bounded wildcards in API
",NotBuggy,"The change in the diff involves altering the method signature to use a bounded wildcard (`<? super String>`) for the `Consumer` type. This allows for more flexibility, enabling the method to accept a `Consumer` of `String` or any of its superclasses. While this adjustment improves API flexibility by making it more general, it does not indicate a bug fix. The commit message, ""bounded wildcards in API,"" aligns with the change, as it specifies using a bounded wildcard to enhance generality in API design. There's no indication that this resolves a bug or fixes an error in the existing code; instead, it appears to be a refactoring for improved API design.

Thus, the change is about API enhancement and not a bug fix, so the conclusion is **NotBuggy**."
intellij-community,9028.json,571f34ea65d8637b1c3b3f8022ba8956a9eaaa59,"@@ -1,6 +1,6 @@
       private void process(@NotNull CaretEvent event) {
         Caret caret = event.getCaret();
-        if (caret == myCaretModel.getPrimaryCaret()) {
+        if (caret == caretModel.getPrimaryCaret()) {
           applyFocusMode(caret);
         }
       }",NotBuggy,"extract FocusModeModel
",NotBuggy,"The provided Git diff depicts a single line change where `myCaretModel` is replaced with `caretModel`. The commit message, ""extract FocusModeModel"", suggests restructuring or refactoring code, potentially by moving or reorganizing it to be more modular or focused around a new model. 

Analyzing the code change:
- The change replaces one variable with another, potentially indicating a shift in where or how data is being accessed or managed.
- There is no direct indication of fixing incorrect logic, handling exceptions, or addressing bugs. 

Given these elements, the change aligns more with code refactoring rather than addressing a bug. The update does not appear to fix incorrect behavior, as it might suggest changes related to architectural or design decisions, particularly considering the message's focus on extracting a model rather than resolving functionality issues.

Hence, the conclusion is **NotBuggy**."
intellij-community,5202.json,32ebeeb487dc2bb7d5327285e8cc7930f92a22ff,"@@ -1,10 +1,9 @@
-  private Presentation update(AnAction action) {
+  private Presentation update(AnAction action, UpdateStrategy strategy) {
     if (myUpdatedPresentations.containsKey(action)) {
       return myUpdatedPresentations.get(action);
     }
 
-    AnActionEvent event = createActionEvent(action);
-    Presentation presentation = doUpdate(myModalContext, action, event) ? event.getPresentation(): null;
+    Presentation presentation = strategy.update.fun(action);
     myUpdatedPresentations.put(action, presentation);
     return presentation;
   }",NotBuggy,"update menu and toolbar actions with timeout, display non-updated actions using their default/previous presentation
",NotBuggy,"The commit message describes adding a timeout and handling non-updated actions by using default or previous presentations. The code modifies the `update` method to include an `UpdateStrategy` parameter, which might handle different updating strategies like timeouts.

The changes introduce a new parameter, `UpdateStrategy strategy`, and replace the existing update logic with a call to `strategy.update.fun(action)`. This allows for more flexible and potentially conditional updates, consistent with the commit message describing a timeout feature, which would likely involve changing the update logic. However, the diff does not explicitly show timeout logic or default value handling if updates fail.

The changes involve copying functionality elsewhere for what could be conditional processing, and though not explicitly visible, this likely supports described updates akin to timeout implementation indirectly by delegating the update process to a strategy.

Despite no direct evidence in the diff of a bug (no explicit error handing bugs are shown), the concept in the commit message appears as a preventative strategy to circumvent issues with delayed actions.

Given these observations, the changes do not clearly correspond to a bug being fixed, more like an enhancement, even if aligned with the commit message. Thus, the decision is: **NotBuggy**."
intellij-community,15839.json,3d7fcfef434c97bb682789e4d4aabbd56e856816,"@@ -1,14 +1,14 @@
-  private void setSelectedEditor(final FileEditor editor) {
+  private void setSelectedEditor(@NotNull FileEditor editor) {
     final EditorWithProviderComposite composite = getEditorComposite(editor);
     if (composite == null) return;
 
     final FileEditor[] editors = composite.getEditors();
     for (int i = 0; i < editors.length; i++) {
       final FileEditor each = editors[i];
       if (editor == each) {
         composite.setSelectedEditor(i);
         composite.getSelectedEditor().selectNotify();
         break;
       }
     }
   }",NotBuggy,"cleanup
",NotBuggy,"The commit message ""cleanup"" suggests changes meant to refine or tidy up the code without altering its functionality. In the diff, there is a single change where a `@NotNull` annotation is added to the `editor` parameter in the `setSelectedEditor` method. This addition:

1. Suggests that the `editor` parameter should never be null when passed to the method.
2. Serves to improve code readability and maintainability by enforcing non-null checks at compile time if supported by the tooling.

However, this change doesn't fix any bugs or modify existing logic directly; it merely enhances the code's robustness by clarifying nullability contracts. It aligns with the commit message ""cleanup"" as it contributes to code quality without altering behavior.

Given the nature of this modification, it is clear that it is not related to fixing a bug, but rather a code quality improvement.

The conclusion is **NotBuggy**."
intellij-community,27442.json,e253d6a0dc44dfd8cea8853fbd6c2bb6de9c0038,"@@ -1,5 +1,5 @@
-  public void applyChanges(@NotNull JavaModuleExtensionImpl modified) {
+  public void applyChanges(@NotNull JpsJavaModuleExtensionImpl modified) {
     setLanguageLevel(modified.myLanguageLevel);
     setOutputUrl(modified.myOutputUrl);
     setTestOutputUrl(modified.myTestOutputUrl);
   }",NotBuggy,"new project model: more java-specific properties added to the model
",NotBuggy,"The commit message mentions adding more Java-specific properties to a model. However, the diff shows that the method's parameter type has changed from JavaModuleExtensionImpl to JpsJavaModuleExtensionImpl. This suggests a refactoring to potentially use a more specific or relevant type in the method signature. No new properties or Java-specific enhancements are evident in the change, nor does it indicate a bug fix. Therefore, the commit doesn't align with the message nor does the change itself indicate a bug fix. The answer is **NotBuggy**."
intellij-community,19676.json,8440df0fda18a651251e13ac1b5a693c85d1a617,"@@ -1,26 +1,26 @@
     private static void appendNode(Node node, StringBuilder sb) {
       if (node.target == ERROR_NODE) {
         sb.append(node.method);
         return;
       }
       else if (node.target != null) {
         appendNode(node.target, sb);
         sb.append('.');
       }
       sb.append(node.method).append('(');
       boolean first = true;
-      for (Object arg : (node.args == null ? ArrayUtil.EMPTY_OBJECT_ARRAY : node.args)) {
+      for (Object arg : (node.args == null ? ArrayUtilRt.EMPTY_OBJECT_ARRAY : node.args)) {
         if (first) first = false;
         else sb.append(',').append(' ');
         if (arg instanceof Node) {
           appendNode((Node)arg, sb);
         }
         else if (arg instanceof String) {
           sb.append('\""').append(StringUtil.escapeStringCharacters((String)arg)).append('\""');
         }
         else if (arg instanceof Number) {
           sb.append(arg);
         }
       }
       sb.append(')');
     }",NotBuggy,"prefer ArrayUtilRt

GitOrigin-RevId: f4b11d48949267d778f6cdab4ef8ac60eb85aa2e
",NotBuggy,"The commit message mentions a preference change to use `ArrayUtilRt` instead of another utility, in this case, `ArrayUtil`. The replacement of `ArrayUtil.EMPTY_OBJECT_ARRAY` with `ArrayUtilRt.EMPTY_OBJECT_ARRAY` does not indicate a bug fix but rather a refactoring or a preference for using a different utility class.

Examining the code changes:
- The logic of the code remains the same. The array used in the for-loop when `node.args` is null has been swapped from one constant to another, which suggests an alignment with standards or preferred practices, rather than a correction of faulty behavior.

The commit message aligns with the change as it clearly states a preference shift, not a bug correction. There is no indication of error-handling improvements, logical corrections, or exception handling related to bugs.

Hence, we conclude with **NotBuggy**."
intellij-community,21985.json,b5c92ae0ba1ee1964a0e18b02eb49ceafaf8679e,"@@ -1,16 +1,16 @@
-  public PyType getCallableType(@NotNull Callable callable, @NotNull TypeEvalContext context) {
+  public PyType getCallableType(@NotNull PyCallable callable, @NotNull TypeEvalContext context) {
     if (callable instanceof PyFunction) {
       final String qualifiedName = callable.getQualifiedName();
       if (qualifiedName != null && qualifiedName.startsWith(""PyQt"")){
         final QualifiedName name = QualifiedName.fromDottedString(qualifiedName);
         final String qtVersion = name.getComponents().get(0);
         final String docstring = ((PyFunction)callable).getDocStringValue();
         if (docstring != null && docstring.contains(""[signal]"")) {
           final PyClass aClass = PyClassNameIndex.findClass(qtVersion + ""."" + ourQtBoundSignal, callable.getProject());
           if (aClass != null)
             return new PyClassTypeImpl(aClass, false);
         }
       }
     }
     return null;
   }",NotBuggy,"Rename com.jetbrains.python.psi.Callable to PyCallable so as not to clash with java.util.concurrent.Callable
",NotBuggy,"The commit message indicates a refactoring: renaming to prevent class name clashes. This aligns with the code changes, where `Callable` is replaced with `PyCallable`, likely to avoid conflicts with `java.util.concurrent.Callable`. This is a housekeeping change rather than a bug fix. The answer is **NotBuggy**."
intellij-community,13487.json,47f3db3350684a82b8dead8a0b4f7c961e0ed685,"@@ -1,3 +1,3 @@
           public boolean hasLine() {
-            return ((bitField0_ & 0x00000040) == 0x00000040);
+            return instance.hasLine();
           }",NotBuggy,"protobuf version advanced to 3.0.0
",NotBuggy,"The commit message indicates an update to the protobuf version, but the code change involves modifying the logic for the `hasLine` method. Specifically, it changes the bitwise operation used to check the presence of a field to a direct method call. This could be related to changes in how protobuf objects should be accessed due to version advancement, which might involve altering generated code patterns or APIs.

However, since the commit message does not mention a bug fix and only refers to a version update, and given that the change in code could be a response to the new version's requirements rather than a bug fix in behavior, the change alone isn't definitive of a bug fix but rather adaptability to the new protobuf version.

The overall context and the commit message do not specifically align with an indication of a bug fix, focusing instead on a version increment. Therefore, the classification is **NotBuggy**."
intellij-community,26056.json,4f5720f30aaff736459a6dc2d23912bf00f0d745,"@@ -1,18 +1,18 @@
   private void configureStartingScriptPanel(final Project project, final PyConsoleOptionsProvider.PyConsoleSettings optionsProvider) {
-    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myStartScript), project, PythonFileType.INSTANCE) {
+    myEditorTextField = new EditorTextField(createDocument(project, optionsProvider.myCustomStartScript), project, PythonFileType.INSTANCE) {
       @Override
       protected EditorEx createEditor() {
         final EditorEx editor = super.createEditor();
         editor.setVerticalScrollbarVisible(true);
         return editor;
       }
 
       @Override
       protected boolean isOneLineMode() {
         return false;
       }
     };
     myStartingScriptPanel.setLayout(new BorderLayout());
     myStartingScriptPanel.add(myEditorTextField, BorderLayout.CENTER);
     myConsoleSettings = optionsProvider;
   }",NotBuggy,"Setting to remove separator line in console (PY-2573).
",NotBuggy,"The commit message indicates a change related to removing a separator line in the console, specifically referencing a Python issue (PY-2573). The code diff, however, shows a change from `optionsProvider.myStartScript` to `optionsProvider.myCustomStartScript`. This alteration seems to update which script or configuration is being utilized in the console setup.

There is no explicit mention or visible change that directly corresponds to removing a separator line within the console. Instead, it seems the update is more about changing a property or configuration reference within the code related to console script handling.

As there is no clear alignment between the commit message and the code there's nothing indicating it as a bug fix related to addressing a console display issue or error. Thus, the relevance to a console separator line toggle is not evident from the changes shown.

**NotBuggy**"
hector,1607.json,8751ec479084eca0622c2f26f6c0b02577ad2a15,"@@ -1,4 +1,4 @@
   public String system_update_keyspace(KsDef ks_def)
-      throws InvalidRequestException, TException {
+      throws InvalidRequestException, TException, SchemaDisagreementException {
     return client.system_update_keyspace(ks_def);
   }",NotBuggy,"v0.8 specific changes for bringing up to date with latest cassandra trunk
",NotBuggy,"The commit message states that the updates are intended to align the project with the latest Cassandra trunk, suggesting compatibility or feature updates rather than strictly bug fixes. 

The code change adds a new exception, `SchemaDisagreementException`, to the method signature of `system_update_keyspace`. This kind of change typically indicates an effort to handle a specific error that may occur due to schema disagreements during keyspace updates. While adding an exception handling improvement is a good practice and ensures robustness, it doesn't necessarily mean there was a known bug in the original code without further context indicating that the lack of this specific exception handling previously caused issues.

Hence, without further evidence that the original code was definitively buggy, this change alone is more about aligning with expected behavior when using the latest Cassandra version.

Considering these aspects, my conclusion is **NotBuggy**."
hector,2275.json,747116cab7950e8466a5b7b89c223866472dab49,"@@ -1,21 +1,21 @@
       public Column execute(Cassandra.Client cassandra) throws HectorException {
         try {          
           if ( queryLogger.isDebugEnabled() ) {
-            queryLogger.debug(""---------\nColumnFamily: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
+            queryLogger.debug(""---------\nColumnFamily multiget: {} slicePredicate: {}"", columnFamilyName, activeSlicePredicate.toString());
           }
           long startTime = System.nanoTime();
-          Map<ByteBuffer, List<ColumnOrSuperColumn>> rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
+          rows = cassandra.multiget_slice(keySerializer.toBytesList(_keys), columnParent, activeSlicePredicate.toThrift(), 
               ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
           long duration = System.nanoTime() - startTime;
 
-
+          
           if ( queryLogger.isDebugEnabled() ) {
             queryLogger.debug(""Execution took {} microseconds on host {}\n----------"", duration/1000, getCassandraHost());
           }
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }
         hasValues = true;
 
         return null;
       }",NotBuggy,"initial cut at automatic multiget
",NotBuggy,"The commit message, ""initial cut at automatic multiget,"" indicates that this change is the first attempt to implement an automatic multiget feature. The modifications in the code snippet do suggest focusing on enhancing or implementing this feature, particularly with the adjustment of debug logging to mention ""ColumnFamily multiget"" and some alterations in how results are being stored and processed.

However, there is no clear indication of a bug fix here. The changes primarily relate to:
- Altering a debug log statement to reflect the feature of multiget.
- There are some slight modifications in variable usage, but they don't seem to directly address any errors or erroneous behavior.
- The overall structure and logic remain unchanged, aside from these adjustments for logging and perhaps preliminary setting up for a multiget feature.

The absence of clear evidence for a bug being resolved suggests that this implementation change is an enhancement or a feature extension rather than a bug fix. Therefore, this commit reflects an initial feature implementation and doesn't resolve any explicitly mentioned bugs.

Consequently, the conclusion is **NotBuggy**."
hector,1823.json,a54d8e467f611b1e3467ad27145004668efc0d6d,"@@ -1,5 +1,5 @@
   public HColumn<N, V> setValue(V value) {
     notNull(value, ""value is null"");
-    this.value = value;
+    this.column.value = valueSerializer.toByteBuffer(value);
     return this;
   }",NotBuggy,"huge efficiency improvement in hcolumimpl. tied a lot closer to thrift Column, but the efficiency gains are worth it and we are still behind an interface
",NotBuggy,"The change in the code replaces the direct assignment of the `value` to storing its serialized form within a `column` object. The commit message discusses efficiency improvements by closely integrating with Thrift's `Column`. This adjustment points towards optimizing how column values are stored, potentially reducing overhead or improving throughput by ensuring data is stored in a more transport-efficient format.

The focus of the commit is on efficiency improvements rather than resolving any incorrect logic or error handling indicative of a bug fix. Consequently, the change aligns with performance enhancement objectives rather than bug resolution.

Based on this analysis, conclude the assessment with **NotBuggy**."
hector,2272.json,229041fa14667b7d0ea1064511055e96713b230e,"@@ -1,13 +1,13 @@
   private void maybeExecuteSlice(N columnName) {
     if ( columnNames == null ) {
       columnNames = new HashSet<N>();
     }
     if ( columns == null ) {
-      columns = new HashMap<N, Column>();
+      columns = new HashMap<N, HColumn<N,ByteBuffer>>();
     }
     if ( columns.get(columnName) == null ) {
       columnNames.add(columnName);
       activeSlicePredicate.setColumnNames(columnNames);
       doExecuteSlice();
     }    
   }",NotBuggy,"added more innards to hcolumnfamily, test coverage for such
",NotBuggy,"The commit message talks about adding ""more innards to hcolumnfamily"" and test coverage, but doesn't specify a bug fix. The diff updates the type of the `columns` map from `HashMap<N, Column>` to `HashMap<N, HColumn<N,ByteBuffer>>`, which suggests a more specific or intended data structure is being used. This change aligns with internal refactoring for better structure or clarity, but it doesn't explicitly point to a bug fix like handling exceptions, correcting logic errors, or fixing incorrect outputs in the existing code.

Since the commit message doesn't describe fixing a bug and the code changes don't indicate a direct bug fix, the conclusion is **NotBuggy**."
hector,516.json,50daddbde6d764dd15c3c1fc6b74da2ea8da240d,"@@ -1,12 +1,12 @@
   public <T> AbstractComposite setComponent(int index, T value,
       Serializer<T> s, String comparator, boolean inclusive) {
     serialized = null;
 
     while (components.size() <= index) {
       components.add(null);
     }
-    components.set(index, new Component(value, s, comparator, inclusive));
+    components.set(index, new Component(value, null, s, comparator, inclusive));
 
     return this;
 
   }",NotBuggy,"Made deserialization of components lazy and occur at get() time
",NotBuggy,"The commit message mentions making deserialization of components lazy, occurring at get() time, suggesting a change in when the deserialization happens to potentially enhance performance or avoid unnecessary computation. However, the code change in the diff shows an adjustment in how components are set within a list in the AbstractComposite class.

The modification involves altering the constructor call for a Component object. Originally, the constructor received four parameters: `value, s, comparator, inclusive`. After the change, it receives five parameters: `value, null, s, comparator, inclusive`, with `null` included as a second argument.

This change seems to support the claim of making deserialization lazy. By setting a component with `null`, potentially for a later deserialization, it aligns with the message indicating that deserialization operations are deferred or made conditional on when they are retrieved via a `get()` call.

In terms of bug fixing, while this change does not directly address a bug such as an error-handling update or a logical correction, it transforms the object's behavior concerning serialization, indicating a performance optimization rather than a direct bug correction.

In this context, the modifications do not strictly align with classical definitions of bug fixes like correcting an erroneous state or preventing application failure due to logic errors or exceptions. Thus, based on the given diff and the description, this change is more about performance optimization rather than a typical bug fix.

Therefore, the conclusion here is **NotBuggy**."
hector,1926.json,12bad5fac77e564df933c1893fd3210e16152326,"@@ -1,27 +1,27 @@
 	public void copy() throws HectorException {
 		if (this.cf == null) {
 			throw new HectorException(""Unable to clone row with null column family"");
 		}
 		if (this.rowKey == null) {
 			throw new HectorException(""Unable to clone row with null row key"");
 		}
 		if (this.destinationKey == null) {
 			throw new HectorException(""Unable to clone row with null clone key"");
 		}
 
 		ColumnFamilyTemplate<K, ByteBuffer> template = new ThriftColumnFamilyTemplate<K, ByteBuffer>(this.keyspace, this.cf, this.keySerializer, this.bs);
 		Mutator<K> mutator = HFactory.createMutator(this.keyspace, this.keySerializer, new BatchSizeHint(1, this.mutateInterval));
 		ColumnFamilyUpdater<K, ByteBuffer> updater = template.createUpdater(this.destinationKey, mutator);
 
-		SliceQuery<K, ByteBuffer, V> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.valueSerializer).
+		SliceQuery<K, ByteBuffer, ByteBuffer> query = HFactory.createSliceQuery(this.keyspace, this.keySerializer, this.bs, this.bs).
 						setColumnFamily(this.cf).
 						setKey(this.rowKey);
 
-		ColumnSliceIterator<K, ByteBuffer, V> iterator = new ColumnSliceIterator<K, ByteBuffer, V>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
+		ColumnSliceIterator<K, ByteBuffer, ByteBuffer> iterator = new ColumnSliceIterator<K, ByteBuffer, ByteBuffer>(query, this.bs.fromBytes(new byte[0]), this.bs.fromBytes(new byte[0]), false);
 		while (iterator.hasNext()) {
-			HColumn<ByteBuffer, V> column = iterator.next();
-			updater.setValue(column.getName(), column.getValue(), this.valueSerializer);
+			HColumn<ByteBuffer, ByteBuffer> column = iterator.next();
+			updater.setValue(column.getName(), column.getValue(), this.bs);
 		}
 
 		template.update(updater);
 	}",NotBuggy,"Changed class to use ByteBuffer as the column value serializer to accomodate CF with different value types
",NotBuggy,"The commit message states that the class has been changed to use `ByteBuffer` as the column value serializer to accommodate column families (CF) with different value types. Let's analyze the changes in the code:

1. **Column Family Template**: 
   - The column family template is instantiated with `ByteBuffer` as the second type parameter instead of `V`. This aligns with the commit message of generalizing value types to use `ByteBuffer`.

2. **Slice Query**:
   - The type `V` is replaced with `ByteBuffer` in the instantiation of `SliceQuery`. This change suggests that any specific value type `V` is now replaced with a more generic `ByteBuffer`, facilitating serialization of different data types.

3. **Column Slice Iterator**:
   - Similarly, the `ColumnSliceIterator` now uses `ByteBuffer` instead of `V`, further supporting the use of a generic serializer that can cater to multiple value types.

4. **Updater and Setting Values**:
   - The change from `column.getValue(), this.valueSerializer` to `column.getValue(), this.bs` indicates the method to set values has been altered to use `ByteBuffer` serialization, consistent with the commit message.

These changes indicate that the refactoring is aimed at changing the serialization strategy to a more flexible and generic `ByteBuffer` strategy, as stated in the commit message. Since there is no indication of a bug fix, error handling change, or exception handling improvement within the code, the commit does not primarily address a bug.

The changes in the code are in line with the commit message, focusing on accommodating different value types using `ByteBuffer`, rather than fixing a bug.

Given these observations, the conclusion is **NotBuggy**."
hector,2776.json,6f80d0e241cea8f85ccab3514c6b45fdd9239856,"@@ -0,0 +1,3 @@
+  public void setUseThriftFramedTransport(boolean useThriftFramedTransport) {
+    this.useThriftFramedTransport = useThriftFramedTransport;
+  }",NotBuggy,"added framed transport to CHC
",NotBuggy,"The commit message suggests that framed transport was added to CHC. The provided diff displays an addition of a method to set a boolean flag, presumably controlling whether to use Thrift framed transport. However, this code change by itself does not indicate a bug fix, as it introduces a new feature or option rather than correcting existing behavior. There is no indication of error-handling updates, logical corrections, or exception handling improvements that would suggest a bug was present.

Therefore, the change appears to be a feature addition rather than a bug fix. The answer is **NotBuggy**."
hector,22.json,1af2d5ee84680a2abfe86f1710cce6569d2d5b7a,"@@ -1,3 +1,3 @@
-  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfSuperMapDef) {
-    this.cfBaseMapDef = cfSuperMapDef;
+  public void setCfBaseMapDef(CFMappingDef<? super T, I> cfBaseMapDef) {
+    this.cfBaseMapDef = cfBaseMapDef;
   }",NotBuggy,"refactored ClassCacheMgr to make a leaner more readable class and prevent bloat, and make parallel work a bit easier
",NotBuggy,"The commit message indicates a refactor for readability and maintainability. The diff shows a method argument name change to reflect its assignment target more accurately, enhancing code clarity but not altering functionality. No bug fix intent is visible, aligning with the refactor goal in the message.

Thus, the conclusion is **NotBuggy**."
hector,927.json,8996eec24a06cfc68a8ef691674d2c187af07b60,"@@ -1,14 +1,11 @@
   public static Subject loginService(String serviceName) throws LoginException {
-    LoginContext loginCtx = new LoginContext(serviceName,
-        new CallbackHandler() {
-          // as we use .keytab file there is no need to specify any options in
-          // callback
+    LoginContext loginCtx = new LoginContext(serviceName, new CallbackHandler() {
+          // as we use .keytab file there is no need to specify any options in callback
           public void handle(Callback[] callbacks) throws IOException,
               UnsupportedCallbackException {
           }
         });
 
     loginCtx.login();
-
     return loginCtx.getSubject();
   }",NotBuggy,"Edit sample files and better jadavocs
",NotBuggy,"The commit message indicates edits to sample files and improvements in Javadocs, but the code change involves reformatting the instantiation of `LoginContext` by adjusting whitespace and line breaks. There is no indication of fixing a bug in the logic or error handling of the method. Therefore, the message does not align with a bug fix, and the refactoring appears to be stylistic in nature without functional impact.

Thus, the verdict is **NotBuggy**."
hector,1382.json,6c42b8d05ffc0aef7de1c5e5e67c1ded7e4f45d4,"@@ -1,11 +1,5 @@
   public <N> MutationResult deleteCounter(final K key, final String cf, final N counterColumnName, 
       final Serializer<N> nameSerializer) {
-    return new MutationResultImpl(keyspace.doExecute(new KeyspaceOperationCallback<Void>() {
-        @Override
-        public Void doInKeyspace(KeyspaceService ks) throws HectorException {
-          ks.removeCounter(keySerializer.toByteBuffer(key), ThriftFactory.createColumnPath(cf, counterColumnName, 
-              nameSerializer));
-          return null;
-        }
-    }));
+    addCounterDeletion(key,cf,counterColumnName,nameSerializer);
+    return execute();
   }",NotBuggy,"initial cleanup to support #354
",NotBuggy,"The diff shows a refactor of the `deleteCounter` method, replacing the inline operation with two separate method calls: `addCounterDeletion` and `execute`. This change seems to modularize the method for better organization or preparation for further enhancements, but it doesn't specifically indicate a bug fix. There's no indication of improved error handling or logical corrections related directly to bug fixing.

The commit message ""initial cleanup to support #354"" suggests that these changes are preliminary steps to prepare for future functionality referenced by an issue or task number #354, rather than fixing a bug.

Since the modifications seem more like a structural cleanup and refactor without any clear indication of a bug being addressed, I'll conclude with **NotBuggy**."
hector,2389.json,da2edd4944bc071a11859fb61f76df5112b06a80,"@@ -1,14 +1,11 @@
       public Map<ByteBuffer,List<ColumnOrSuperColumn>> execute(Cassandra.Client cassandra) throws HectorException {
-        Map<ByteBuffer,List<ColumnOrSuperColumn>> cosc = new LinkedHashMap<ByteBuffer, List<ColumnOrSuperColumn>>();
         try {          
           List<K> keyList = new ArrayList<K>();
           Iterators.addAll(keyList, keys.iterator());
-          cosc = cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
+          return cassandra.multiget_slice(keySerializer.toBytesList(keyList), columnParent,
               (workingSlicePredicate == null ? activeSlicePredicate.setColumnNames(columnValueSerializers.keySet()).toThrift() : workingSlicePredicate.toThrift()),              
             ThriftConverter.consistencyLevel(consistencyLevelPolicy.get(operationType)));
         } catch (Exception e) {
           throw exceptionsTranslator.translate(e);
         }        
-
-        return cosc;
       }",NotBuggy,"remove unnecessary allocation
",NotBuggy,"Examining the code changes, the diff removes an unnecessary allocation by eliminating the creation of the `cosc` map before it is overwritten with a value returned from the `cassandra.multiget_slice()` method. The removal of the map instantiation and the direct return of the method's result streamlines the function, thus enhancing efficiency.

The commit message states ""remove unnecessary allocation"", which aligns perfectly with the code changes observed. There is no explicit indication of a bug being fixed, as the change seems focused primarily on optimizing resource usage rather than correcting flawed logic or error handling.

Given that the commit message and changes are about performance improvement without explicit evidence of a bug fix, the assessment is **NotBuggy**."
hector,1732.json,f8ad7a5aa29b297d3f6281a4d061f8dd9da0da4e,"@@ -1,5 +1,5 @@
-  public  RangeSubSlicesQuery<SN,N,V> setSuperColumn(SN sc) {
+  public RangeSubSlicesQuery<SN, N, V> setSuperColumn(SN sc) {
     Assert.notNull(sc, ""sc can't be null"");
     superColumn = sc;
     return this;
   }",NotBuggy,"Refactor: Extract interface from RangeSlicesQuery, RangeSubSlicesQuery, SuperSlicesQuery, SubSliceQuery and SuperSliceQuery and move them to the api package
",NotBuggy,"The commit message describes a refactor by extracting interfaces from several classes and moving them to another package. However, the actual diff only shows a minor formatting change with spacing between generic type parameters and does not include any interface extraction or package relocations.

The commit message and the provided code changes don't align, and there are no bug fixes indicated through this minor change in formatting. The commit appears to be part of a larger refactor, but with only this diff, it is unrelated to any bug fix activity.

Thus, the answer is **NotBuggy**."
hector,587.json,ea13a02750c3c4dba496c54ee459a0f2e749ee43,"@@ -1,19 +1,6 @@
   public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(""HLockManagerConfigurator [keyspaceName="");
-    builder.append(keyspaceName);
-    builder.append("", lockManagerCF="");
-    builder.append(lockManagerCF);
-    builder.append("", rowsCacheEnabled="");
-    builder.append(rowsCacheEnabled);
-    builder.append("", locksTTLInMillis="");
-    builder.append(locksTTLInMillis);
-    builder.append("", backOffRetryDelayInMillis="");
-    builder.append(backOffRetryDelayInMillis);
-    builder.append("", replicationFactor="");
-    builder.append(replicationFactor);
-    builder.append("", numberOfLockObserverThreads="");
-    builder.append(numberOfLockObserverThreads);
-    builder.append(""]"");
-    return builder.toString();
+    return ""HLockManagerConfigurator [keyspaceName="" + keyspaceName + "", lockManagerCF="" + lockManagerCF
+        + "", rowsCacheEnabled="" + rowsCacheEnabled + "", locksTTLInMillis="" + locksTTLInMillis
+        + "", backOffRetryDelayInMillis="" + backOffRetryDelayInMillis + "", replicationFactor="" + replicationFactor
+        + "", numberOfLockObserverThreads="" + numberOfLockObserverThreads + "", maxSelectSize="" + maxSelectSize + ""]"";
   }",NotBuggy,"Made ack size configurable.  The first 10 elements is more than
sufficient to guarantee ordered locking.  ",NotBuggy,"The commit message discusses making ""ack size configurable,"" but the diff shows only a modification in the `toString` method for an object. The update modifies how the string representation is built (from StringBuilder to string concatenation) and includes a new property, `maxSelectSize`, which is not mentioned in the original string representation. This change reflects adding this field into the debug output of the configuration class.

Since the code change is only related to updating how an object’s properties are converted to a string -- presumably for logging or debugging -- it does not fix any bugs. The commit message speaks to configuration aspects but does not align with the changes depicted here, indicating no direct bug fix but rather a refactor or logging/naming enhancement.

Thus, given the absence of a direct indication of a bug fix in the code changes provided, the assessment is **NotBuggy**."
hector,24.json,c6fd6f3fa896a6f616cb2332dacbf9491316dfb0,"@@ -1,3 +1,3 @@
-  public Map<Object, CFMappingDef<? extends T, I>> getDerivedClassMap() {
+  public Map<Object, CFMappingDef<? extends T>> getDerivedClassMap() {
     return derivedClassMap;
   }",NotBuggy,"a POJO's ID was genericized in most places as <I> but this doesn't make sense when starting to use complex types as the key - so it has been removed
",NotBuggy,"The commit message discusses removing generic IDs due to complexities with certain key types. The diff changes the method signature, removing the generic type `<I>` from `CFMappingDef`. This aligns with the commit message, focusing on design rather than fixing a bug. The changes do not indicate correction of erroneous behavior but rather simplify or adapt the design to accommodate new requirements. Therefore, the conclusion is **NotBuggy**."
hector,2693.json,b0dc2661d56bb80142771050c0124ea013fe4e31,"@@ -1,7 +1,9 @@
-      public List<TokenRange> execute(Cassandra.Client cassandra) throws HectorException {
+      public String execute(Cassandra.Client cassandra) throws HectorException {
         try {
-          return cassandra.describe_ring(keyspace);
+          log.info(""in execute with client {}"", cassandra);
+          return cassandra.describe_cluster_name();
         } catch (Exception e) {
           throw xtrans.translate(e);
         }
+
       }",NotBuggy,"Refactor: Seperate the thrift classes from the Cluster interface so it doesn't import any thrift specific classes and create a ThriftCluster implementation
",NotBuggy,"Analyzing the given commit message and git diff:

1. **Commit Message Intent**: The commit message suggests refactoring to separate Thrift-specific classes from the Cluster interface. It mentions creating a ThriftCluster implementation to remove direct dependency on Thrift classes within a particular interface.

2. **Code Changes in Diff**:
   - **Return Type Change**: The return type of the `execute` method was changed from `List<TokenRange>` to `String`.
   - **Method Behavior Change**: The method previously returned the result of `describe_ring(keyspace)`, which likely was a list of token ranges. It now logs a message and returns `describe_cluster_name()` instead.
   - These changes alter what the method returns, suggesting a shift in functionality or what this part of the application intends to do.

3. **Analysis**:
   - The change aligns with refactoring—altering internal details for improved design without changing external behavior expectations (if any interfaces or classes would encapsulate such details).
   - Although the changes directly impact functionality (switching from token range description to cluster name), nothing explicitly aligns with bug fixes like error handling improvement or correcting flawed logic.
   - This change indicates preparing code for a cleaner type separation and modular design which is what refactoring commonly aims for, suggesting this is not a bug fix but rather preparation for future architectural improvements.

The changes do not point to fixing a bug directly but rather to aligning the codebase with architectural modifications as implied in the commit message. Thus, it's a refactoring change:

**NotBuggy**"
cucumber-jvm,1199.json,669dd85aad21b1c030d21bf3def4d770ca4f891e,"@@ -1,3 +1,3 @@
-    void execute(Scenario scenario) throws Throwable {
+    void execute(TestCaseState scenario) {
         delegate.execute(scenario);
     }",NotBuggy,"[Core] Rename Scenario to TestCaseState
",NotBuggy,"The commit message states that ""Scenario"" is being renamed to ""TestCaseState"", and the diff shows a change in the method signature consistent with this description. The method name `execute` remains the same, and the functionality also remains unchanged as the code still delegates execution to `delegate.execute(scenario)` without apparent behavioral changes. This indicates renaming rather than fixing logical errors or bugs. There is no indication of error-handling updates, logical corrections, or exception improvements that would suggest a bug fix.

Thus, the analysis concludes that the change is consistent with a simple renaming and doesn't point to a bug being fixed.

The conclusion is **NotBuggy**."
cucumber-jvm,565.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,7 +1,7 @@
     public boolean equals(Object o) {
         if (this == o) return true;
         if (o == null || getClass() != o.getClass()) return false;
-        CucumberLocation that = (CucumberLocation) o;
+        GherkinMessagesLocation that = (GherkinMessagesLocation) o;
         return line == that.line &&
             column == that.column;
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",NotBuggy,"This commit message and the code change reflect adding support for the Gherkin `Rule` keyword, specifically by transitioning to use `GherkinMessagesLocation` instead of `CucumberLocation`. This change aligns with the modification described in the commit message but doesn't imply a bug fix; rather, it suggests a feature enhancement or architecture improvement. There's no indication that the mentioned change or committed update fixes a bug.

Therefore, the classification of this commit is **NotBuggy**."
cucumber-jvm,978.json,365f45a2df0bff71567bd457d663185f5f3cc788,"@@ -1,11 +1,11 @@
     private List<Node> reversedPoppedNodes(List<Node> currentStack, List<Node> newStack) {
         for (int i = 0; i < currentStack.size() && i < newStack.size(); i++) {
             if (!currentStack.get(i).equals(newStack.get(i))) {
                 return currentStack.subList(i, currentStack.size());
             }
         }
         if (newStack.size() < currentStack.size()) {
             return currentStack.subList(newStack.size(), currentStack.size());
         }
-        return Collections.emptyList();
+        return emptyList();
     }",NotBuggy,"[Plugin] Add TestSourceParsed event (#1895)

Cucumber scenarios and individual examples in a scenario outline are compiled
into pickles. These pickles are wrapped by a `TestCase`. As such
Cucumbers internal representation lacks any hierarchy. I.e. once compiled
into a a pickle a scenario is no longer associated with a feature file.

However consumers of Cucumbers output generally expect results to be reported
in hierarchical fashion. This event allows test cases to be associated with
 with a `Node` in the hierarchy. The `TestSourceParsed` event provides this information. 

A node is an element in a tree. It has a name, keyword and location. A node can optionally be container of yet more nodes. 

```
└── TestSourceParsed
      ├── Node
      └── Container
      │ ├── Node
      │ └── Node
      └── Container
        └── Container
           ├── Node
           └── Node
```

Note that this information is intentionally abstract. To recreate a a facsimile of the original feature file it is still recommended to use the AST produced by a Gherkin parser. And to avoid conflict with other tools is recommended that this parser is shaded.

Closes: #1870",NotBuggy,"The commit message describes adding a new event in the context of a plugin for test case structuring but does not mention bug fixes. The code change modifies how an empty list is returned, transitioning from `Collections.emptyList()` to `emptyList()`. This change is likely a stylistic or organizational preference rather than addressing a bug. The commit aligns with enhancing functionality on a structural level, likely related to the new feature, not a bug fix. Hence, the analysis concludes the commit as **NotBuggy**."
cucumber-jvm,700.json,4d2aa2b173a286d766da8ada1546733fd26d5664,"@@ -1,7 +1,7 @@
-    private List<Class<?>> findClassesForUris(List<URI> baseUris, String basePackageName, Predicate<Class<?>> classFilter) {
+    private List<Class<?>> findClassesForUris(List<URI> baseUris, String packageName, Predicate<Class<?>> classFilter) {
         return baseUris.stream()
-            .map(baseUri -> findClassesForUri(baseUri, basePackageName, classFilter))
+            .map(baseUri -> findClassesForUri(baseUri, packageName, classFilter))
             .flatMap(Collection::stream)
             .distinct()
             .collect(toList());
     }",NotBuggy,"[Core] Use consistent naming in scanners

From a consumer the package being scanned is the `packageName` rather
then the `basePackageName` which only makes sense ocne the package is
compared to some other package.
",NotBuggy,"The changes in the diff relate to renaming a method parameter from `basePackageName` to `packageName`, indicating an attempt to use consistent and potentially clearer naming. There is no indication of error handling or logical corrections that suggest a bug fix; this update appears to be a refactoring to improve code readability by making parameter names clearer and more aligned with their usage.

The commit message also matches the modifications made in the code, focusing on naming consistency rather than bug fixing.

After analyzing the code and commit message, the changes do not indicate a bug fix. Therefore, I conclude with **NotBuggy**."
cucumber-jvm,608.json,4f2ded527763a56e55bc6713e56ba6fa6ef9b359,"@@ -1,3 +1,3 @@
-    static InjectionProvider<?>[] setUpInjectionProviders(final String resourceName) {
-        return new CucumberNeedleConfiguration(resourceName).getInjectionProviders();
+    static InjectionProvider<?>[] setUpInjectionProviders() {
+        return new CucumberNeedleConfiguration().getInjectionProviders();
     }",NotBuggy,"[Needle] Move ObjectFactory to io.cucumber.needle
",NotBuggy,"The commit message mentions moving `ObjectFactory` to a different package, which isn't reflected in this diff. Instead, this change removes a `resourceName` parameter from `setUpInjectionProviders` and alters `CucumberNeedleConfiguration` to operate without it. There's no indication of error handling, logical correction, or a bug fix within this change. The commit message does not align with the code changes, indicating a possible oversight, but focusses on packaging rather than bug fixing. Therefore, this diff reflects refactoring or code simplification, not a bug fix.

The conclusion is **NotBuggy**."
cucumber-jvm,468.json,f2eeac155f0893001bda9bf3a0161d151e2eff38,"@@ -1,3 +1,3 @@
-        Type getType() {
-            return type;
-        }

+    public Type getType() {
+        return type;
+    }",NotBuggy,"[Core] Extract CoreStepDefinitions from Java and Java8 implementation

Removes the `CucumberExpression` creation from the backend modules and
will allow a type registry to be created for each pickle.
",NotBuggy,"This change modifies the access level of the `getType` method from package-private (default) to public. There is no indication that this change fixes a bug. Instead, it seems more related to refactoring or clarifying access permissions, possibly to aid in the reorganization or extension mentioned in the commit message. The commit details focus on extracting step definitions and improving the type registry mechanism, which doesn't directly correlate with a typical bug fix in this context.

Thus, this change is more about enabling accessibility or restructuring as part of a larger module refactor or feature, not correcting a defect.

The answer is **NotBuggy**."
cucumber-jvm,744.json,e029121744ad9f3736894a8e751d4b50fe59bf79,"@@ -1,3 +1,3 @@
-        public void configureTypeRegistry(cucumber.api.TypeRegistry typeRegistry) {
+        public void configureTypeRegistry(io.cucumber.core.api.TypeRegistry typeRegistry) {
             //noop
         }",NotBuggy,"[Core] Move core api packages to io.cucumber.core.api
",NotBuggy,"This commit changes the package name from `cucumber.api` to `io.cucumber.core.api`. The change reflects a refactoring or organizational change in package structure rather than fixing a bug. The commit message aligns with the modification, indicating a relocation of core API packages. There is no indication of a bug fix in terms of functionality, error handling, or logical corrections.

The conclusion is **NotBuggy**."
cucumber-jvm,830.json,a5499df8f8ed5c534ffe43a6fe7825e560f222a4,"@@ -1,4 +1,4 @@
-    public static void main(String[] argv) {
+    public static void main(String... argv) {
         byte exitStatus = run(argv, Thread.currentThread().getContextClassLoader());
         System.exit(exitStatus);
     }",NotBuggy,"Print supported languages details (#1290)

Prints code, name, and native name of supported languages
in a three column list.

```
af              Afrikaans                 Afrikaans
am              Armenian                  հայերեն
ar              Arabic                    العربية
ast             Asturian                  asturianu
az              Azerbaijani               Azərbaycanca
...

",NotBuggy,"The commit message describes an enhancement to print details about supported languages, but the code change does not address this functionality. Instead, the change modifies the main method's signature to use varargs (`String... argv`) instead of a simple array (`String[] argv`). This change does not implement the described enhancement and does not indicate a bug fix.

There is no alignment between the commit message stating the addition of a feature for printing language details and the actual code change. As such, no bug fixes are identified in the diff.

**NotBuggy**"
cucumber-jvm,1183.json,f72d25ce870c7bd40c1e8b65ef4ca32633ba9c50,"@@ -1,3 +1,3 @@
-    List<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
+    Collection<DefaultParameterTransformerDefinition> getDefaultParameterTransformers() {
         return defaultParameterTransformers;
     }",NotBuggy,"[Core] Use sorted set to improve performance
",NotBuggy,"The commit message indicates a performance improvement by using a ""sorted set,"" but the diff only changes the return type from `List` to `Collection`. This change does not directly address performance improvements, especially not related to a sorted set. If there were changes from a `List` to a `SortedSet` or a more efficient data structure in parts of the code not visible here, it might align with the commit message. However, based on the provided diff alone, there's no evidence of a significant performance optimization. The commit message and the code changes are not aligned in terms of performance improvement claims.

Since there's no indication of a bug fix in the code changes, I would conclude **NotBuggy**."
cucumber-jvm,1070.json,3e4dae2c35f99d424f7a1a5f447ba36aced51aa1,"@@ -1,3 +1,3 @@
-    public int getTotal() {
-        return passed + failed + skipped + pending + undefined;
-    }

+        public int getTotal() {
+            return passed + failed + skipped + pending + undefined;
+        }",NotBuggy,"Report summaries and  in the same way as the CLI
",NotBuggy,"The diff in the code shows a change in indentation of the `getTotal()` method without any change in the logic or functionality of the code itself. The commit message mentions ""Report summaries and in the same way as the CLI,"" implying some change in output reporting, possibly aligning the format or structure with a command-line interface; however, no such modification is evident in this code. 

The change is purely stylistic, focusing solely on formatting without affecting execution. Therefore, it does not involve a bug fix or an improvement aligned with the commit message about syncing summaries with CLI output.

Hence, the commit's change does not align with the message regarding a functional or reporting update. The appropriate conclusion for this analysis is **NotBuggy**."
cucumber-jvm,208.json,fe092af8acad11799b24f396a50f870d3f2792a0,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return Objects.hash(line, column);
+        return Objects.hash(uri);
     }",NotBuggy,"[Core] Support Gherkin Rule keyword (#1840)

* [Core] Support Gherkin Rule keyword

Introduces the `gherkin-messages` module which will add support for the Gherkin
`Rule` keyword. Because the rule keyword is not supported by the `json` or
`html` formatter it will be an opt-in module.
",NotBuggy,"The commit message talks about adding support for the Gherkin `Rule` keyword through a new module, but the provided diff alters the `hashCode` method to only include `uri` instead of `line` and `column`. This change doesn't directly relate to or indicate a bug fix; rather, it seems unrelated or possibly part of a larger refactor or feature addition process. The modification impacts how hash codes are generated, possibly affecting collection behavior, but it doesn't indicate error handling or bug fixing. Without additional context from other parts of the change, this specific diff seems to be more about internal data structure modifications than fixing a specific bug.

Thus, based solely on this diff and without more context aligning it to a bug fix, this change should be marked as **NotBuggy**."
cucumber-jvm,5.json,e63b108cc53f6773d21c4c8fabec86e0c4909a95,"@@ -1,16 +1,18 @@
     public <T> T getInstance(final Class<T> type) {
         final Object instance = standaloneInstances.get(type);
         if (instance != null) {
             return type.cast(instance);
         }
         final Instance<T> selected = container.select(type);
         if (selected.isUnsatisfied()) {
-            final Unmanaged.UnmanagedInstance<T> value = new Unmanaged<>(container.getBeanManager(), type).newInstance();
+            BeanManager beanManager = container.getBeanManager();
+            Unmanaged<T> unmanaged = new Unmanaged<>(beanManager, type);
+            Unmanaged.UnmanagedInstance<T> value = unmanaged.newInstance();
             value.produce();
             value.inject();
             value.postConstruct();
             standaloneInstances.put(type, value);
             return value.get();
         }
         return selected.get();
     }",NotBuggy,"[CDI2] Apply spotless formatting
",NotBuggy,"The commit message states that the changes apply ""spotless formatting,"" indicating a focus on code styling or formatting rather than bug-fixing. Reviewing the actual code changes:

1. A single line is split into three lines:
   - `final Unmanaged.UnmanagedInstance<T> value = new Unmanaged<>(container.getBeanManager(), type).newInstance();`
   - is transformed into:
     ```java
     BeanManager beanManager = container.getBeanManager();
     Unmanaged<T> unmanaged = new Unmanaged<>(beanManager, type);
     Unmanaged.UnmanagedInstance<T> value = unmanaged.newInstance();
     ```

These adjustments appear to clarify the long line by breaking it into more comprehensible individual parts. This practice is generally oriented toward improving readability or meeting a certain code style guide, consistent with the commit's claim about ""apply spotless formatting.""

No logical or functional changes are made to address a bug or improve functionality. The code before and after the formatting performs the same operations in the same order, and there are no changes related to error handling or logical corrections indicative of a bug fix.

Therefore, the alterations align well with the declaration of applying formatting changes and not fixing any bug. This commit should be classified as **NotBuggy**."
cucumber-jvm,793.json,f64093d5dde6b8d9f28cea614f759cd166876fa6,"@@ -1,12 +1,12 @@
         private <T> T cacheNewInstance(Class<T> type) {
             try {
                 Constructor<T> constructor = type.getConstructor();
                 T instance = constructor.newInstance();
                 instances.put(type, instance);
                 return instance;
             } catch (NoSuchMethodException e) {
-                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need DI, put cucumber-picocontainer on the classpath"", type), e);
+                throw new CucumberException(String.format(""%s doesn't have an empty constructor. If you need dependency injection, put cucumber-picocontainer on the classpath"", type), e);
             } catch (Exception e) {
                 throw new CucumberException(String.format(""Failed to instantiate %s"", type), e);
             }
         }",NotBuggy,"[Core] Throw exception when multiple object factories are found (#1832)

When multiple object factories are available on the classpath Cucumber
would print out a warning and fall back to the default object factory.

With the introduction of `cucumber.object-factory` the user can specify
exactly which object factory should be used. There should be no need for
a soft failure anymore.",NotBuggy,"The commit message describes changes related to handling multiple object factories, including throwing an exception under certain conditions. However, the provided code diff only contains a minor edit in the exception message, changing ""DI"" to ""dependency injection"". It doesn't relate to the described behavior change of handling multiple object factories. There's no bug fix here, just a refinement in messaging. The commit message doesn't align with this specific change in the diff.

**NotBuggy**"
cucumber-jvm,1330.json,312039f90cbaf3a63b1e5f259f558575a2c060fc,"@@ -1,3 +1,3 @@
-    public boolean isMultiThreaded() {
-        return threads > 1;
+    public int getCount() {
+        return count;
     }",NotBuggy,"[Core] Refactor runtime options parsing (#1670)

Re-factoring of the runtime options parsing. Tried to achieve several goals:
 * Use a builder rather then CLI arguments to represents options in parsing
 * Pull default behaviour setting up into the runners
 * Pull the different runtime option sources into the runners
 * Run JUnit and TestNG with zero output by default

## Motivation and Context

Cucumber was build with hexagonal architecture in mind. Unfortunately one of the ports that was frequently reused was the command line interface. This resulted in a complex and complicated chain of methods that construct and then parse CLI arguments. This system was hard to understand and change.

On top of this Cucumber accepts commandline options from both the CLI and the environment. This however happened in the bowels of option parsing. Making it non obvious and again hard to follow.

Because the command line interface was used as a port it also imposed the command line defaults on all runners. However for JUnit and TesstNG the default progress formatter nor summary plug-in are particularly useful. Ideal tests are quite.

Finally this structure also forces us to pull in implementation defaults from different runners into the core (e.g. `junitOptions`). This re factoring will allow JUnit and TestNG to define their own `@CucumberOptions` annotation. 

## Related stuff

 * #1476
 * #1537
 * #1135
 * #1029",NotBuggy,"The commit message describes a refactoring effort to improve runtime options parsing within a software project, focusing on architecture and separation of concerns. It emphasizes using builders for options, separating CLI parsing from core logic, and setting runner-specific defaults. However, the provided code diff changes a method related to multithreading by replacing `isMultiThreaded()` with `getCount()`, altering its functionality.

Analyzing the code change:

- **Original Code**: The method `isMultiThreaded()` checks if the number of threads is greater than 1, returning a boolean. This informs whether the application is running in a multi-threaded environment.
  
- **Modified Code**: The new method `getCount()` simply returns the count (presumably the number of threads, given the context), changing the method's purpose from a boolean check to providing a numerical value.

The commit message's intent is to refactor runtime option parsing, focusing on architectural improvements rather than fixing bugs. It doesn't explicitly suggest altering multithreading functionality, nor does it address any specific bug related to `isMultiThreaded()`.

The method change does not align well with the commit message's refactoring goals. In a typical refactor, changing method behavior without clear justification goes against the intent of maintaining existing functionality while improving the code's structure. However, the change could still be part of an overarching refactor if it fits broader goals, such as simplifying how thread configurations are handled.

Since the change to `isMultiThreaded()` does not directly reflect bug fixing, aligns incompletely with the message, and isn't flagged as crucial to the restructuring, this indicates that the change is more concerned with refactoring rather than directly fixing any identified issue.

Given these points, the change is **NotBuggy**."
cucumber-jvm,889.json,1bd057452353bdb2c343b9460d7b4e36f05f8f75,"@@ -1,6 +1,4 @@
-    public void addPlugin(Object plugin) {
+    public void addPlugin(Formatter plugin) {
         plugins.add(plugin);
-        if (plugin instanceof Formatter) {
-            setEventBusFormatterPlugins(plugin);
-        }
+        setEventBusFormatterPlugins(plugin);
     }",NotBuggy,"Improve plugin and formatter structure

The following improvements have been made

1. All plugin interfaces (Formatter, StepDefinitionReporter,
   SummaryPrinter) now extend the Plugin interface.

2. Updated documentation to make it clear what each plugin does.

3. All plugins have been made final. They are not designed for
   extension.

4. Moved android formatters into formatter package to limit visibility
   of TestSourcesModel.

5. Classes in runtime/android have been made final and have had their
   visibility reduced. They are not designed for extension.
",NotBuggy,"This commit message outlines improvements related to the structure and interfaces of plugins, rather than bug fixes. Let's analyze the diff:

1. **Code Changes Description:**
   - The method `addPlugin` had its parameter type changed from `Object` to `Formatter`.
   - Removed the instance check and moved the `setEventBusFormatterPlugins` call outside the check, thus it will be executed for every `Formatter` plugin added.

2. **Reasoning:**
   - Changing the parameter type enforces type safety, ensuring that only objects implementing `Formatter` are passed to `addPlugin`. This reduces potential runtime exceptions but assumes the caller should naturally follow this restriction.
   - The removal of the instance check streamlines the logic since only `Formatter` types are accepted by the method now, meaning the check is unnecessary.
   - No explicit indication of bug fixes is present. Instead, the improvement enhances code clarity and reduces runtime type-checking, directly aligning with the commit message focusing on structural improvements.

Given these points, the commit changes don't indicate a bug fix but rather structural improvements and code refactoring as per the commit message.

Conclusion: **NotBuggy**"
hazelcast,26584.json,67630aeab5cc4398c3f84a394807269a890bc2e0,"@@ -1,20 +1,22 @@
-    private static int calculateCleanupOperationCount(HazelcastProperties properties, int partitionCount, int partitionThreadCount) {
-        String stringValue = properties.getString(CLEANUP_OPERATION_COUNT);
+    private static int calculateCleanupOperationCount(HazelcastProperties properties,
+                                                      final HazelcastProperty cleanupOpCountProperty,
+                                                      int partitionCount, int partitionThreadCount) {
+        String stringValue = properties.getString(cleanupOpCountProperty);
         if (stringValue != null) {
             return parseInt(stringValue);
         }
 
         // calculate operation count to be sent by using partition-count.
         final double scanPercentage = 0.1D;
         final int opCountFromPartitionCount = (int) (partitionCount * scanPercentage);
 
         // calculate operation count to be sent by using partition-thread-count.
         final int inflationFactor = 3;
         int opCountFromThreadCount = partitionThreadCount * inflationFactor;
 
         if (opCountFromPartitionCount == 0) {
             return opCountFromThreadCount;
         }
 
         return min(opCountFromPartitionCount, opCountFromThreadCount);
     }",NotBuggy,"JCache automatic expiration

JCache expiration mechanism only worked for entries that were touched after their expiration time. Therefore, an entry and its backups could stay in the record store forever. This PR introduces a periodic cache cleanup task for removing expired entries. JCache expired entries are removed from the record stores and backups eventually.
",Buggy,"The commit message describes an update to a cache's expiration mechanism involving periodic cleanup of expired entries, which aligns with addressing a bug in the expiration logic. The diff involves modifying calculateCleanupOperationCount, allowing a flexible HazelcastProperty parameter for dynamic cleanup operation count determination. While not directly correlating to expired entry removal, it suggests foundational changes for periodic cleanup. Though not directly a bug fix, it's part of an update addressing expiration issues, making it a **Buggy** change."
hazelcast,27157.json,58c2cdbbe75d9a9855f727db2f0d2f7304099098,"@@ -1,3 +1,3 @@
-    public ItemListenerConfig setImplementation(ItemListener implementation) {
-        throw new UnsupportedOperationException(""This config is read-only"");
+    public PNCounterConfig setReplicaCount(int replicaCount) {
+        throw new UnsupportedOperationException(""This config is read-only PN counter: "" + getName());
     }",NotBuggy,"Move read-only config to internal package (#15569)

Move read-only config to internal package

Moves all read-only config classes to internal package, removes the
deprecation annotations as these classes will stay in 4.0 and marks the
getAsReadOnly methods as PrivateApi.",NotBuggy,"The commit message talks about moving classes to an internal package and making some methods private, focusing on a reorganization for encapsulation, rather than functionality changes. In the diff, however, it seems that methods and exception messages have been altered. The functionality—throwing UnsupportedOperationException—remains the same, but these changes don't appear to correct any misbehavior in logic or handle errors differently. These changes match the commit's mention of making parts of the API private indirectly (changes include method signature adjustments indicating private/internal usage), but lack evidence of bug fixes such as error handling, logical corrections, or exception handling improvements.

Given no indication of a bug fix beyond method organization, the answer is **NotBuggy**."
hazelcast,36355.json,244d65a80c4d14c1f76dd185faeee689fd2422a2,"@@ -1,3 +1,3 @@
-    public boolean send(PreVoteResponse response, RaftEndpoint target) {
-        return send(new PreVoteResponseOp(raftGroupId, response), target);
+    public boolean send(PreVoteResponse response, RaftMember target) {
+        return send(new PreVoteResponseOp(groupId, response), target);
     }",NotBuggy,"Raft session management implementation & lock improvements

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"The diff shows a change in method parameters and variable names. Specifically, `RaftEndpoint` is replaced with `RaftMember`, and `raftGroupId` is changed to `groupId`. This suggests a refactoring or an API update to match new data structures or conventions. The commit message talks about session management and lock improvements in Raft, which this specific change does not address.

No evidence suggests a bug fix—like adjusting error-handling or resolving incorrect logic. The changes align more with evolving code for better structure or compatibility rather than addressing a functional bug.

Thus, the conclusion is **NotBuggy**."
hazelcast,33686.json,ae4e6e8a8211f695a851c2fe8d9f6f53ad042751,"@@ -1,5 +1,5 @@
     public long tailSequence() {
         GenericOperation op = new GenericOperation(name, OPERATION_TAIL);
         InternalCompletableFuture f = invoke(op);
-        return (Long) f.getSafely();
+        return (Long) f.join();
     }",NotBuggy,"InvocationFuture method renaming to align with CompletableFuture

Renamed InvocationFuture.set to complete
Renamed InternalCompletableFuture.getSafely to join
",NotBuggy,"The commit message indicates method renaming for alignment purposes rather than a bug fix. The code diff reflects this rename, changing `getSafely` to `join`, which corresponds with the described changes in the commit message. There is no indication of a bug being fixed, as the change is purely terminological to match naming conventions. Therefore, the changes do not indicate a bug fix but rather a consistency or naming alignment effort. 

The conclusion is **NotBuggy**."
hazelcast,38624.json,43243bf3287025ae21ad8ecd8c962b986d921c17,"@@ -1,4 +1,4 @@
-    public DistributedClassloadingConfig setProviderFilter(String providerFilter) {
+    public UserCodeDeploymentConfig setProviderFilter(String providerFilter) {
         this.providerFilter = providerFilter;
         return this;
     }",NotBuggy,"Distributed Classloading renamed to User Code Deployment
",NotBuggy,"The commit message indicates a renaming of ""Distributed Classloading"" to ""User Code Deployment"". The provided diff reflects this by renaming the class from `DistributedClassloadingConfig` to `UserCodeDeploymentConfig`. The change strictly involves renaming and does not alter the functional or logical behavior of the code. There are no signs of correcting an error or handling exceptions differently.

Therefore, the changes align well with the commit message but do not indicate a bug fix; they represent a refactoring or renaming task. The conclusion is **NotBuggy**."
hazelcast,32184.json,4307a6aa07fbae5d93d5b5b8c50b598332183f8e,"@@ -1,22 +1,22 @@
     public Object invoke(Data key, EntryProcessor entryProcessor, Object[] arguments, int completionId) {
         long now = Clock.currentTimeMillis();
-        long start = isStatisticsEnabled() ? System.nanoTime() : 0;
+        long startNanos = isStatisticsEnabled() ? Timer.nanos() : 0;
         R record = records.get(key);
         boolean isExpired = processExpiredEntry(key, record, now);
         if (isExpired) {
             record = null;
         }
         if (isStatisticsEnabled()) {
             if (recordNotExistOrExpired(record, isExpired)) {
                 statistics.increaseCacheMisses(1);
             } else {
                 statistics.increaseCacheHits(1);
             }
-            statistics.addGetTimeNanos(System.nanoTime() - start);
+            statistics.addGetTimeNanos(Timer.nanosElapsed(startNanos));
         }
         CacheEntryProcessorEntry entry = createCacheEntryProcessorEntry(key, record, now, completionId);
         injectDependencies(entryProcessor);
         Object result = entryProcessor.process(entry, arguments);
         entry.applyChanges();
         return result;
     }",NotBuggy,"use Timer to measure elapsed time (#16780)

",NotBuggy,"The diff involves a change from using `System.nanoTime()` to a custom `Timer` class for measuring elapsed nanoseconds, which is aligned with the commit message regarding measuring elapsed time. However, this change does not indicate a bug fix but an enhancement or refactor for possibly improving the timing accuracy or abstraction. There isn't any direct indication of a bug fix, such as correcting logic or addressing an error.

Given that, the reasoning is as follows:

1. **Functional Adjustment**: The code switches from `System.nanoTime()` to `Timer.nanos()` and `Timer.nanosElapsed()`, likely for abstraction or providing more control over time measurements.

2. **Statistics Collection**: The particular change affects how elapsed time is calculated and recorded in statistics, probably for consistency or better precision across different measurements.

3. **Refactor Over Bug Fix**: There are no signs of incorrect behavior correction, such as error-handling additions or logical corrections, which would indicate a bug fix. It appears to be a refactor focusing on encapsulating or improving time measurement.

Thus, based on the provided information, there isn't enough evidence to consider this change a bug fix. It seems more of an improvement or non-functional modification.

The conclusion is **NotBuggy**."
hazelcast,5483.json,02825e67b1bf9fac68116e5504e605a70cbf2182,"@@ -1,5 +1,5 @@
     void validate() {
         if (className == null && listenerImplementation == null) {
-            throw new IllegalArgumentException(""One of class name or listener implementation must be not null"");
+            throw new IllegalArgumentException(""Either class name or listener implementation must be not null"");
         }
     }",NotBuggy,"Improve javadocs & typos (#11716)

",NotBuggy,"The commit message indicates improvements to Javadocs and typos, but the actual code change involves modifying an exception message for clarity. The functionality remains unchanged, and the modification does not address any logic or error-handling bug. Therefore, this change aligns with improving readability, but it does not indicate a bug fix. The answer is **NotBuggy**."
hazelcast,28495.json,e7657e3c867ba1ac879355b5c8751e772ee0cadd,"@@ -1,8 +1,18 @@
     protected ExceptionAction onException(Throwable t) {
         raftInvocationContext.updateKnownLeaderOnFailure(groupId, t);
 
-        if (shouldFailOnIndeterminateOperationState() && (t instanceof MemberLeftException)) {
-            return THROW_EXCEPTION;
+        if (t instanceof IndeterminateOperationState) {
+            if (isRetryableOnIndeterminateOperationState()) {
+                if (indeterminateException == null) {
+                    indeterminateException = t;
+                }
+                return RETRY_INVOCATION;
+            } else if (shouldFailOnIndeterminateOperationState()) {
+                return THROW_EXCEPTION;
+            } else if (indeterminateException == null) {
+                indeterminateException = t;
+            }
         }
+
         return isRetryable(t) ? RETRY_INVOCATION : op.onInvocationException(t);
     }",NotBuggy,"Implement Raft based IAtomicReference, ISemaphore and ICountdownLatch

- Implement auto-removal of missing Raft members:
If a Raft member is missing for some time, it will be automatically
removed from the CP sub-system if enabled.

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"The commit message describes implementing Raft-based coordination features as well as auto-removal of missing Raft members. However, this diff focuses on handling exceptions related to ""IndeterminateOperationState."" Here's the breakdown:

1. **Original Code**: 
   - The code checks if the exception is an instance of `MemberLeftException` and should fail on indeterminate operation state, it would throw the exception by returning `THROW_EXCEPTION`.

2. **Modified Code**:
   - The check now handles if the exception is an instance of `IndeterminateOperationState`.
   - Inside this check: 
     - If retryable, it sets `indeterminateException` and returns `RETRY_INVOCATION`.
     - If it should fail on indeterminate state, it returns `THROW_EXCEPTION`.
     - If none of these conditions match, it sets `indeterminateException`.

3. **Analysis**:
   - The change is focused on refining error handling related to indeterminate operation states.
   - This is not a direct bug fix but an extension/enhancement of functionality to accommodate a new condition (`IndeterminateOperationState`).
   - The change provides more granular control over different exception states and how the system should react.

4. **Conclusion**: 
   - While improved exception handling might prevent potential issues under certain circumstances, the focus of this change isn't directly correcting erroneous behavior but enhancing control. Thus, the changes align more closely with a feature implementation or refinement rather than addressing a bug.
  
The answer is **NotBuggy**."
hazelcast,31303.json,faa6428c28eba0b8147e38196fe3711c46773ac8,"@@ -1,3 +1,3 @@
     public String toString() {
-        return attribute + ""="" + value;
+        return attributeName + ""="" + value;
     }",NotBuggy,"1. Introducing predicates on arrays and collection.

Let's have following classes:

class Body {
  Collection<Limb> limbs;
  Map<String, String> pocket;
}

class Limb {
  String name;
  String[] nails;
}

You can can use predicates like this:

IMap<Integer, Body> map = getMap();
Predicate p = Predicates.equals('limb[0].name', 'hand'); // <- this matches all bodies where 1st limb has name 'hand'
Collection<Body> result = map.values(p);

You can also use SQLPredicate:
Predicate p = new SQLPredicate('limb[0].name = hand');
Collection<Body> result = map.values(p);

You can use a wildcard instead of the exact position parameters:
Predicate p = Predicates.equals('limb[any].name', 'hand'); // <- this matches all bodies where any limb has name 'hand'

This is another possible query:
Predicate p = Predicates.equals('limb[0].nails[any]', = 'red'); // <- this matchinges all bodies where 1st limb has any nail with red color

You can create indexes on collections/arrays. But bear in mind the attribute used in index definition has to be the same as the one used in the query:

<indexes>
  <index ordered=""false"">limb[any].name</index>
</indexes>

The this predicate will use the index:
Predicate p = Predicates.equals('limb[any].name', 'hand');

but this one will not:
Predicates.equals('limb[0].name', 'hand')

You would have to create another index:
<indexes>
  <index ordered=""false"">limb[0].name</index>
</indexes>

2. Introducing custom value extractors
You can also define a custom attribute using a ValueExtractor that encompasses an extraction logic.
The extraction logic may just extract the underlying value or group, reduce or transform it.
Reflection-based extraction is the default mechanism - ValueExtractor(s) are an alternative way of getting values from objects.

How to use a ValueExtractor?
First, extend the com.hazelcast.query.extractor.ValueExtractor class and implement the extract() method.
Then define a new attribute with registering the above-mentioned extractor in the configuration of the map.

Please have a look at some code samples:

* How to define a new custom attribute?
<code>
MapAttributeConfig attributeConfig = new MapAttributeConfig();
extractorConfig.setName(""currency"");
extractorConfig.setExtractor(""com.bank.CurrencyExtractor"");
</code>

* How to register the newly-defined attribute in a configuration of a Map?
<code>
MapConfig mapConfig = (...);
mapConfig.addMapAttributeConfig(attributeConfig);
</code>

Extractors may be also defined in the XML configuration.
Please, bear in mind that an extractor may not be added while the map has been instantiated. All extractor have to be defined upfront in the map's initial configuration.
",NotBuggy,"The commit message describes new features related to predicates for arrays/collections and value extractors, while the code change updates a `toString` method to use `attributeName` instead of `attribute`. The alteration doesn't indicate a bug fix related to functionality or logic errors; it seems more like a small improvement or correction in variable naming to match the intended identifier. The commit message doesn't align with the code change, which neither introduces new logic nor directly relates to the features described.

As there's no clear bug fix involved, the answer is **NotBuggy**."
hazelcast,10974.json,ce1b80bdee78c9a39a7567ac835772242b11715c,"@@ -1,3 +1,3 @@
     ManagedService createManagedService() {
-        return new MapManagedService(getMapServiceContext());
+        return new MapManagedService(mapServiceContext);
     }",NotBuggy,"Renamed MapServiceContextAwareFactory to MapServiceFactory
",NotBuggy,"The commit message suggests a renaming action related to a class named MapServiceContextAwareFactory, but the provided diff doesn't show any renaming or reference to such classes. Instead, the diff shows a change from `getMapServiceContext()` to `mapServiceContext`, which indicates a refactoring from a method call to directly accessing a field. 

The commit message and the code change are not aligned; there is no indication of a renaming. The code change seems minor and involves possibly a refactoring for performance or clarity (avoiding a method call).

Considering the code diff and the commit message, there is no indication of a bug fix in the code itself. This change does not seem to address a bug.

Therefore, the conclusion is **NotBuggy**."
hazelcast,3827.json,2fcead2b9c8264248187f203e7739970b7230d16,"@@ -1,4 +1,3 @@
-    protected static ClientMessage prepareResponse(MessageHandlerContext context, Object... objects) {
-
+    public String getMethodName() {
         return null;
     }",NotBuggy,"Message handling task impl
ClientMessage improvement and Parameters base
Authentication impls
MessageTask Factory
",NotBuggy,"The given diff shows a method replacement in which a method `prepareResponse` is removed, and another method `getMethodName` is added. However, there's no context within the provided diff for analyzing the new method since it only has a return statement to return `null`. As for the commit message, it mentions improvements and implementation tasks related to messaging, authentication, and a message task factory, but does not provide specific details regarding bug fixes. 

The removal of `prepareResponse` and its replacement with `getMethodName` doesn't indicate handling any specific errors, exceptions, or incorrect logic corrections from the given information—it's a change in method presence but with insufficient details to conclude if it directly addresses a bug.

Given the changes and the commit message, it seems like this is part of a broader refactoring or new feature implementation, rather than addressing a specific bug. Therefore, based on the provided information, the changes do not directly indicate a bug fix.

Thus, the conclusion is **NotBuggy**."
hazelcast,14416.json,253b515b1b443a2035cec75d0b576fa3ee7f8667,"@@ -1,3 +1,3 @@
-        private RowBatch getBatch() {
-            return batch;
-        }

+    public int getBatchSize() {
+        return batchSize;
+    }",NotBuggy,"SQL send/receive operators (#16848)

",NotBuggy,"The commit message ""SQL send/receive operators"" does not clearly describe the specific bug being fixed. Instead, it suggests a feature enhancement or a change related to SQL operations.

In the provided git diff, the function `getBatch()` is replaced by `getBatchSize()`, which changes from returning a ""RowBatch"" object to returning an integer ""batchSize."" This is a change in the API, shifting what is being exposed from potentially multiple rows to a mere size indication (possibly the number of rows).

This modification does not hint at any bug fix. There's no indication of error handling, logical correction, or an issue with exception handling based on the context provided. The change seems more like a refactoring or an API design choice rather than a direct bug resolution.

Given the mismatch with the expected functionality change related to a bug fix, the conclusion here is **NotBuggy**."
hazelcast,37315.json,98e86ecc1e48b6b3b442235a7a0a783346b89d59,"@@ -1,5 +1,5 @@
     public boolean eval(Object arg) {
-        final QueryEntry entry = (QueryEntry) arg;
+        final QueryableEntry entry = (QueryableEntry) arg;
         final Data keyData = entry.getKeyData();
         return (key == null || key.equals(keyData)) && predicate.apply((Map.Entry) arg);
     }",NotBuggy,"Cached QueryEntry implementation
",NotBuggy,"The commit message refers to a ""Cached QueryEntry implementation,"" but the change is simply updating the type of a variable from `QueryEntry` to `QueryableEntry`. This could be related to internal refactoring or improved design but doesn't indicate any specific bug fixes. There are no apparent error-handling updates, logical corrections, or explicit references to solving a bug in the code.

Given this information, the change doesn't align with a bug fix based on the available data. Thus, the conclusion is **NotBuggy**."
hazelcast,35549.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,5 +1,4 @@
     protected void toString(StringBuilder sb) {
         super.toString(sb);
-        sb.append("", expectedRound="").append(expectedRound)
-          .append("", invocationUid="").append(invocationUid);
+        sb.append("", invocationUid="").append(invocationUid).append("", timeoutMillis="").append(timeoutMillis);
     }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"This change updates the `toString` method of a class, altering the format of its output by removing `expectedRound` and adding `timeoutMillis`. The commit message is focused on refactoring, module relocation, and preparation activities for a release, none of which directly relate to bug fixes but rather organizational or feature-related updates. 

Changing the `toString` method generally impacts logging or debugging output and is not typically related to fixing bugs in logic or functionality unless the format itself was incorrect and causing problems. However, such a scenario is unlikely unless specified, considering the mentioned context in the commit message.

Given that the commit message and code changes focus on refactoring, organizing, and preparing for a release without explicitly referring to a bug or symptom being corrected, it seems this change is more about improving or altering debug output as part of broader project reorganization rather than addressing a specific bug.

Thus, the conclusion is **NotBuggy**."
hazelcast,35758.json,cab58ba9e0a6b904b0c0ddb659fc339aaee9c86c,"@@ -1,3 +1,3 @@
-        private static AcquireResult notAcquired(Collection<LockInvocationKey> cancelled) {
-            return new AcquireResult(NOT_LOCKED, cancelled);
-        }

+    static AcquireResult failed(Collection<LockInvocationKey> cancelled) {
+        return new AcquireResult(FAILED, INVALID_FENCE, cancelled);
+    }",NotBuggy,"Prepare CP Subsystem for release

- Move RaftGroupId and RaftMember to core module
- Rename packages to cp.internal...
- Move Raft packages to core module & remove Raft modules
- Make use of Hazelcast's proxy layer for Raft proxies
- Implement XML configuration for the CP subsystem
- Prepare REST API for CP subsystem management
- Prepare CP subsystem management script

Co-authored-by: Ensar Basri Kahveci <ebkahveci@gmail.com>
Co-authored-by: Mehmet Dogan <mehmet@dogan.io>
",NotBuggy,"The commit message outlines preparation activities for a release, including module movements, renaming, and implementing configurations and APIs. The change in the diff alters a method ""notAcquired"" to ""failed"" with an updated `AcquireResult`. This modifies the result type and adds an `INVALID_FENCE` parameter to the constructor. This change does not imply a bug fix, instead, it reflects an enhancement or adaptation for better alignment with new release requirements or terminologies.

There is no clear indication of error-handling updates, logical corrections, or exception improvements that would typically signify a bug fix. Instead, the alteration seems to align with a broader refactor or feature enhancement as part of the release preparation.

Thus, the review concludes with **NotBuggy**."
jetty,11818.json,33c11dffaa86d09b33fd350f07cd9c548b7d3b7c,"@@ -1,9 +1,28 @@
-    public DecoderWrapper getWrapperFor(Class<?> type)
+    public Wrapper getWrapperFor(Class<?> type)
     {
-        DecoderMetadata metadata = getMetadataFor(type);
-        if (metadata != null)
+        synchronized (activeWrappers)
         {
-            return newWrapper(metadata);
+            Wrapper wrapper = activeWrappers.get(type);
+
+            // Try parent (if needed)
+            if ((wrapper == null) && (parentFactory != null))
+            {
+                wrapper = parentFactory.getWrapperFor(type);
+            }
+
+            if (wrapper == null)
+            {
+                // Attempt to create Wrapper on demand
+                DecoderMetadata metadata = metadatas.getMetadataByType(type);
+                if (metadata == null)
+                {
+                    return null;
+                }
+                wrapper = newWrapper(metadata);
+                // track wrapper
+                activeWrappers.put(type,wrapper);
+            }
+
+            return wrapper;
         }
-        return null;
     }",NotBuggy,"JSR-356 - cleaning up Decoder and Encoder lifecycle.
",Buggy,"The commit message suggests cleanup of Decoder and Encoder lifecycle, indicating an aim to improve code structure and possibly fix lifecycle issues related to resource management.

Analyzing the provided code diff:

1. The method signature changes from returning a `DecoderWrapper` to a more general `Wrapper`, suggesting a broader utility.
   
2. The function implementation modifies how wrappers are fetched or created:
   - It uses a synchronized block, ensuring thread safety when accessing shared resources (`activeWrappers`).
   - The method first checks if a `Wrapper` already exists for the given type in `activeWrappers`.
   - If not present, it attempts to retrieve the wrapper from a `parentFactory`.
   - If still absent, the method tries to create a new `Wrapper` using metadata about the type.
   - This newly created `Wrapper` is then stored in `activeWrappers`.

3. The previous version of the method would return `null` if the metadata didn’t exist. The new version attempts more steps (such as checking parent factory) to return a valid object, potentially reducing null returns.

These changes indicate a cleanup and improvement in how the wrapping mechanism handles lifecycle management. This prevents possible runtime issues (such as creating multiple wrappers for the same type unnecessarily or failing to retrieve existing wrappers due to concurrency issues).

Since the changes aim to streamline the workflow of obtaining or creating wrappers, they likely fix underlying issues with resource management or lifecycle handling that were previously causing bugs. Therefore, these changes align with the commit message and likely reflect an indirect bug fix.

Thus, the final conclusion is **Buggy**."
jetty,6323.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,4 +1,4 @@
-        public XmlParser.Node getRoot ()
-        {
-            return _root;
-        }

+    public int getMinorVersion()
+    {
+        return _minorVersion;
+    }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message states ""Reconcile jetty-8 and jetty-7 webapp configuration mechanisms,"" suggesting a goal of harmonizing or merging configurations between different versions of Jetty. However, the provided diff shows the removal of the `getRoot()` method and the addition of the `getMinorVersion()` method, neither of which clearly aligns with fixing a bug related to configuration mechanisms.

1. **Removed Method:** The `getRoot()` method was removed. Typically, the removal of a method may mean it is no longer necessary, possibly due to a change in the underlying architecture or because it has been replaced by a different approach. 

2. **Added Method:** The `getMinorVersion()` method was added. This method likely provides a specific piece of information (the minor version number) that could be useful for compatibility or configuration purposes between Jetty versions.

3. **Relevance to Bug Fix:** There is no explicit indication in the code change that a specific bug has been fixed. These changes reflect more of a structural or informational update, aligning with the stated goal of reconciling configuration mechanisms but not directly indicating a bug resolution.

Considering the lack of evidence in the diff to suggest a bug fix and that the changes seem oriented toward enhancing compatibility or configuration state handling, the correct conclusion is **NotBuggy**."
jetty,11565.json,e69b41e9cdb3c1d10a426adc3a6d474aeaeb2bab,"@@ -1,4 +1,5 @@
     public void customizeParamsOnClose(LinkedList<IJsrParamId> params)
     {
-        params.addFirst(JsrParamPath.INSTANCE);
+        super.customizeParamsOnClose(params);
+        params.addFirst(JsrPathParamId.INSTANCE);
     }",NotBuggy,"JSR-356 Adding suppport for @PathParam
",NotBuggy,"The commit message indicates an enhancement by adding support for `@PathParam`. The code change includes calling a `super.customizeParamsOnClose(params)` method before adding a specific path parameter instance (`JsrPathParamId.INSTANCE`) to the `params` list. This suggests an extension of functionality rather than an error correction. There's no indication of error-handling updates, logical corrections, or exception-handling improvements which are typically associated with bug fixes. Therefore, this change aligns more with an enhancement instead of addressing a bug.

**NotBuggy**"
jetty,5631.json,0301de099d876124b5a0b26da171105c63ab0632,"@@ -1,10 +1,6 @@
     private static void addListeners(ContainerLifeCycle container)
     {
-        WebSocketCdiListener listener = new WebSocketCdiListener(container);
-
-        if (listener != null)
-        {
-            container.addLifeCycleListener(listener);
-            container.addEventListener(listener);
-        }
+        WebSocketCdiListener listener = new WebSocketCdiListener();
+        container.addLifeCycleListener(listener);
+        container.addEventListener(listener);
     }",NotBuggy,"Delaying decoration of websocket endpoints
",NotBuggy,"The commit message indicates a delay in the decoration of WebSocket endpoints, which is likely related to how listeners are added to the container. 

Analyzing the code changes:
- The code removes a null check for a newly instantiated `WebSocketCdiListener`. 
- The original code had a redundant null check since `new WebSocketCdiListener(container)` would not typically return null unless the constructor itself had specific logic leading to it — a common issue can be the constructor throwing exceptions rather than returning null.
- By simplifying the addition of the listener without the null check, the change likely reflects a refactor or optimization rather than a bug fix. The listener is now added more straightforwardly, without the unnecessary null-check logic that would never be executed given a straightforward constructor without returning or throwing logic that avoids it.

The commit message doesn't directly imply a bug but rather an adjustment in the process of decorating/initializing WebSocket endpoints, suggesting an improvement or refactor.

Since this change does not fix an explicit bug in the logic or behavior of the application but simplifies the code, I conclude that the changes do not indicate a bug fix.

**NotBuggy**"
jetty,10058.json,2b43ae931cdbadcc2db001aed740ed91be1a75e8,"@@ -1,9 +1,9 @@
     public void bindToENC(String localName)
     throws NamingException
     {
         //TODO - check on the whole overriding/non-overriding thing
         InitialContext ic = new InitialContext();
         Context env = (Context)ic.lookup(""java:comp/env"");
-        Log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
+        __log.debug(""Binding java:comp/env/""+localName+"" to ""+objectNameString);
         NamingUtil.bind(env, localName, new LinkRef(objectNameString));
     }",NotBuggy,"improved debugging of JNDI. own logger

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2608 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message suggests improvements for debugging by using a custom logger. The code change replaces a static `Log` debug statement with `__log`, likely indicating the use of an instance or custom logger. This does not fix a bug but enhances logging capabilities. The answer is **NotBuggy**."
jetty,12021.json,745f757552698139934a6f403d9c7396cb06044a,"@@ -1,17 +1,20 @@
     public void init(EndpointConfig config)
     {
-        LOG.debug(""init({})"",config);
+        if (LOG.isDebugEnabled())
+        {
+            LOG.debug(""init({})"",config);
+        }
 
         // Instantiate all declared encoders
         for (EncoderMetadata metadata : metadatas)
         {
             Wrapper wrapper = newWrapper(metadata);
             activeWrappers.put(metadata.getObjectType(),wrapper);
         }
 
         // Initialize all encoders
         for (Wrapper wrapper : activeWrappers.values())
         {
             wrapper.encoder.init(config);
         }
     }",NotBuggy,"Guarded calls to LOG.debug() with if (LOG.isDebugEnabled()) to reduce allocation of varargs Object[].
",NotBuggy,"The commit message describes changing `LOG.debug()` call to be conditional on `LOG.isDebugEnabled()` to prevent unnecessary allocation of `Object[]` for varargs when logging is disabled. The code modification matches this description exactly. It ensures that the potentially costly operation of preparing the debug message is only executed if debug logging is enabled, which is a common and effective performance optimization technique in logging.

The change aligns with the message and does not tag any bug in logic or functionality but focuses on an optimization. The original code would not cause incorrect behavior, but might lead to unnecessary resource usage when debug-level logging wasn't enabled.

Given this alignment and the nature of the change being purely performance-related, the correct assessment is **NotBuggy**."
jetty,14333.json,c9f4513a89474e86da004c5e4ac2cba23c7f7a83,"@@ -1,5 +1,5 @@
-    public Request idleTimeout(long timeout)
+    public Request timeout(long timeout, TimeUnit unit)
     {
-        this.idleTimeout = timeout;
+        this.timeout = unit.toMillis(timeout);
         return this;
     }",NotBuggy,"Reworked HTTP client API, removing usage of Future.
",NotBuggy,"The commit message declares a rework of the HTTP client API with a focus on removing the use of `Future`. However, the provided code diff does not showcase any changes related to `Future`. Instead, it demonstrates a modification of a method from `idleTimeout` to `timeout`, adding a `TimeUnit` parameter to convert the given timeout to milliseconds. This change may improve flexibility or readability, but it does not directly address the concerns mentioned in the commit message, nor does it imply a bug fix.

The change is more of a refactor or API improvement, considering the adjustment in method signature and functionality, rather than a direct correction of an error in the system. As the change is focused on API enhancement rather than correcting a faulty behavior, the appropriate conclusion is **NotBuggy**."
jetty,10063.json,e10c14c77aee8c4c0aaf1b7cd0def390239dbf2b,"@@ -1,20 +1,20 @@
-    protected void save (Object scope)
+    protected void save (Object object)
     throws NamingException
     {
-        __log.debug(""SAVE {} in {}"",this,scope);
+        __log.debug(""SAVE {} in {}"",this,_scope);
         InitialContext ic = new InitialContext();
         NameParser parser = ic.getNameParser("""");
-        Name prefix = NamingEntryUtil.getNameForScope(scope);
+        Name prefix = NamingEntryUtil.getNameForScope(_scope);
       
         //bind the NamingEntry into the context
         Name namingEntryName = NamingEntryUtil.makeNamingEntryName(parser, getJndiName());
         namingEntryName.addAll(0, prefix);
-        namingEntryNameString = namingEntryName.toString();
-        NamingUtil.bind(ic, namingEntryNameString, this);
+        _namingEntryNameString = namingEntryName.toString();
+        NamingUtil.bind(ic, _namingEntryNameString, this);
                 
         //bind the object as well
         Name objectName = parser.parse(getJndiName());
         objectName.addAll(0, prefix);
-        objectNameString = objectName.toString();
-        NamingUtil.bind(ic, objectNameString, objectToBind);
+        _objectNameString = objectName.toString();
+        NamingUtil.bind(ic, _objectNameString, object);
     }",NotBuggy,"332937 Added Destroyable interface and reworked dependent lifecycles, specially of JNDI
Also added Dumpable interface to improve and standardize the output of nested structures while debugging

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2615 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message describes the addition of interfaces for object lifecycle management and debugging and a rework of lifecycle management, especially for JNDI. The changes in the diff align with this intent by modifying the method `save` to correctly reference class variables (`_scope`, `_namingEntryNameString`, `_objectNameString`) instead of potentially outdated or mis-referenced parameters/local variables (`scope`, `objectNameString`, `namingEntryNameString`). This potentially prevents scope-related issues in JNDI operations by ensuring that the correct instance variables are used, which aligns with the goal of reworking dependent lifecycles. Although the changes indirectly involve lifecycle or ""state management"" of objects tied to JNDI, they do not explicitly indicate a bug fix, such as an incorrect operation or error handling.

Given this is a rework and improvement of lifecycle management rather than a straightforward bug fix, the changes are better described as aligning with the enhancements of lifecycle (Destroyable and Dumpable interfaces) and don't specifically illustrate a bug needing fixing. Thus, the answer is **NotBuggy**."
jetty,6021.json,722f39080072328301279dceb1c74b4c83b5035f,"@@ -1,4 +1,4 @@
     public void appendJSON(final StringBuffer buffer, Convertible converter)
     {
-        appendJSON((StringBuffer)buffer,converter);
+        appendJSON((Appendable)buffer,converter);
     }",NotBuggy,"numerous findbugs inspired cleanups
",NotBuggy,"The change made in this diff involves altering the method `appendJSON` to accept a parameter of type `Appendable` instead of `StringBuffer`. This modification could be inspired by feedback from static analysis tools like FindBugs, which often recommend using more flexible interfaces such as `Appendable` over concrete classes like `StringBuffer`. 

Analyzing the details:

1. **Type Change**: The method signature changes from `StringBuffer` to `Appendable`.
2. **Method Functionality**: The functionality remains the same if `buffer` is of a type that implements `Appendable` (which `StringBuffer` does).
3. **Motivation**: This is likely a code cleanup or refactor to adhere to best practices by making the method more flexible. It doesn't directly address any bugs in logic or functionality.

The change is not indicative of a bug fix because it does not alter existing behavior (assuming correct usage of the API) or correct erroneous logic. Instead, it makes the method more adaptable and future-proof.

Therefore, the conclusion is **NotBuggy**."
jetty,5349.json,fe773d22e1473b230538558b6992a157a62bc350,"@@ -1,4 +1,4 @@
     public List<File> getWebInfLib()
     {
-        return webInfJars;
+        return _webInfJars;
     }",NotBuggy,"JETTY-846 Support maven-war-plugin overlay configuration with jetty:run
",NotBuggy,"The commit message refers to supporting a specific configuration for Jetty involving Maven overlays. The code change simply renames a variable from `webInfJars` to `_webInfJars`. This kind of change typically indicates a coding style or convention update (such as indicating a private or instance variable with a prefix) instead of a functional bug fix. There is no clear indication that this rename directly supports the functionality mentioned in the commit message or that it fixes any particular bug related to the described feature.

Thus, the change doesn't align with the bug fix type of enhancement as described in the commit message. Therefore, I would conclude this as **NotBuggy**."
jetty,7053.json,314f94decc0e814baf31f500502020e05fcb73a2,"@@ -1,42 +1,37 @@
         public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException
         {
             Servlet s;
             synchronized(this)
             {
                 if(_stack.size()>0)
                     s=(Servlet)_stack.pop();
                 else
                 {
                     try
                     {
-                        s = (Servlet) newInstance();
-                        s = getServletHandler().customizeServlet(s);
+                        s = newInstance();
                         s.init(_config);
                     }
                     catch (ServletException e)
                     {
                         throw e;
                     }
-                    catch (IOException e)
-                    {
-                        throw e;
-                    }
                     catch (Exception e)
                     {
                         throw new ServletException(e);
                     }
                 }
             }
             
             try
             {
                 s.service(req,res);
             }
             finally
             {
                 synchronized(this)
                 {
                     _stack.push(s);
                 }
             }
         }",NotBuggy,"320073 Reconcile jetty-8 and jetty-7 webapp configuration mechanisms

git-svn-id: svn+ssh://dev.eclipse.org/svnroot/rt/org.eclipse.jetty/jetty/trunk@2145 7e9141cc-0065-0410-87d8-b60c137991c4
",NotBuggy,"The commit message discusses reconciling webapp configuration mechanisms between Jetty versions, which implies aligning different internal handling rather than focusing on fixing specific bugs. The code changes involve:

1. **Removal of Duplicate Code**: The `IOException` catch block was removed. Since `IOException` is a subclass of `Exception`, the catch block for `Exception` already handles `IOException` appropriately by throwing a `ServletException`. This removal cleans up redundancy without changing functionality.
   
2. **Creation and Initialization**: The `customizeServlet` method call was removed. This could be part of reconciling Jetty configurations, shifting configuration responsibilities instead of fixing a specific bug.

The changes focus on cleanup and configuration alignment rather than addressing glitches or unintended behavior in the existing functionality. There is no indication of error-handling improvements or logical corrections in these adjustments that would specifically target a bug.

Hence, the changes do not indicate a bug fix, so the outcome is **NotBuggy**."
jetty,13676.json,aeb27cd461d2faa4b77413e029f04f4b13e79777,"@@ -1,14 +1,15 @@
     public void forwardFailure(List<Response.ResponseListener> listeners, Response response, Throwable failure)
     {
         notifyBegin(listeners, response);
         for (Iterator<HttpField> iterator = response.getHeaders().iterator(); iterator.hasNext();)
         {
             HttpField field = iterator.next();
             if (!notifyHeader(listeners, response, field))
                 iterator.remove();
         }
         notifyHeaders(listeners, response);
         if (response instanceof ContentResponse)
-            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()));
+            // TODO: handle callback
+            notifyContent(listeners, response, ByteBuffer.wrap(((ContentResponse)response).getContent()), new Callback.Adapter());
         notifyFailure(listeners, response, failure);
     }",NotBuggy,"431642 - Implement ProxyServlet using Servlet 3.1 async I/O.

Intermediate commit that implements asynchronous content in HttpClient,
for the HTTP protocol, passing the tests.

This work needs to be extended to FCGI and SPDY and finally implement
the asynchronous proxy servlet.
",NotBuggy,"The commit message describes implementing a proxy servlet using asynchronous operations. Although this diff shows enhancements for better handling of content notification and including a callback (important for asynchronous processing), it lacks evidence of fixing a defect in the code. Instead, it seems to be part of an ongoing development to transition to asynchronous processing, aligning more with feature development rather than a direct bug fix.

Here is the detailed reasoning:

1. **Callback Introduction**: The code now includes a `Callback.Adapter()` when calling `notifyContent`. This suggests preparation for asynchronous handling but does not explicitly convey a bug fix or change in logic for defect resolution.

2. **Commit Message Alignment**: The message highlights implementing asynchronous I/O and mentions ongoing work rather than resolving a particular defect. It targets extending functionality, which suggests it is part of a feature development effort.

3. **Existing TODO**: The `// TODO: handle callback` comment indicates the changes are a work in progress, not necessarily correcting an error.

Given this analysis, the diff does not hinge on a specific defect repair; it is closely aligned with enhancing or building new capabilities. Therefore, the changes in the context provided are **NotBuggy**."
jetty,10705.json,c2f1d23f2baff2587e261cf4034be9dc86eb616b,"@@ -1,4 +1,5 @@
-    public void setMasked(boolean mask)
+    public WebSocketFrame setMasked(boolean mask)
     {
         this.masked = mask;
+        return this;
     }",NotBuggy,"Making WebSocketFrame use builder pattern
",NotBuggy,"The commit message states a transition to a builder pattern for the `WebSocketFrame` class. In the provided diff, the method `setMasked` is modified to return a `WebSocketFrame` object, rather than having a `void` return type. This modification aligns with the builder pattern, which typically involves returning the current object to enable method chaining. 

There is no indication of a bug fix in this change. The focus is on improving the class design to support a builder pattern, enhancing usability by allowing method chaining.

Given that the commit message is about design refactoring rather than fixing a specific bug, the change aligns with improving the code pattern rather than addressing a bug in the code.

Therefore, the assessment of the change in relation to the commit message is **NotBuggy**."
jetty,3380.json,fd099aa77d54cd06463ccba3da3c9425c0c71437,"@@ -1,38 +1,37 @@
-    private V getBest(int t,byte[] b,int offset,int len)
+    private V getBest(int t,String s,int offset,int len)
     {
         int node=0;
         for(int i=0; t!=0 && i<len; i++)
         {
-            char c=StringUtil.lowercases[b[offset+i]&0x7f];
+            char c=s.charAt(offset+i);
+            if(isCaseInsensitive() && c<128)
+                c=StringUtil.lowercases[c];
 
             while (t!=0)
             {
                 int row = ROW_SIZE*t;
                 char n=_tree[row];
                 int diff=n-c;
                 
                 if (diff==0)
                 {
                     node=t;
                     t=_tree[row+EQ];
                     
                     // if this node is a match, recurse to remember 
                     if (_key[node]!=null)
                     {
-                        V best=getBest(t,b,offset+i+1,len-i-1);
+                        V best=getBest(t,s,offset+i+1,len-i-1);
                         if (best!=null)
                             return best;
                         return (V)_value[node];
                     }
                     
                     break;
                 }
 
-                if (diff<0)
-                    t=_tree[row+LO];
-                else
-                    t=_tree[row+HI];
+                t=_tree[row+((diff<0)?LO:HI)];
             }
         }
         return null;
     }",NotBuggy,"404511 Replaced all StringMap usage with Tries
",NotBuggy,"The commit message indicates replacing `StringMap` usage with `Tries`, but the diff doesn't show a specific indication of the type of data structure being used, such as migration from a `StringMap` to a `Trie`. The diff, however, changes the method signature and internals of `getBest` to work with a `String` instead of a `byte[]`. Moreover, it updates character processing logic accordingly. There is no outright indication of a bug fix in this diff, but more of a refactor or change in the method to improve or alter its functionality, possibly making it work with a different data format or fitting a new underlying data structure (like `Trie`). This doesn't directly point to a bug fix.

Given there is no clear indication of a bug fix from just this change alone, and the mentioned migration from `StringMap` to `Tries` isn't directly visible or evident, this would be labeled as **NotBuggy**."
jetty,4526.json,720263151596d37014013c1a0033a8dc0fab9d5e,"@@ -1,39 +1,4 @@
     public static String unquoteOnly(String s)
     {
-        if (s==null)
-            return null;
-        if (s.length()<2)
-            return s;
-
-        char first=s.charAt(0);
-        char last=s.charAt(s.length()-1);
-        if (first!=last || (first!='""' && first!='\''))
-            return s;
-
-        StringBuilder b = new StringBuilder(s.length() - 2);
-        boolean escape=false;
-        for (int i=1;i<s.length()-1;i++)
-        {
-            char c = s.charAt(i);
-
-            if (escape)
-            {
-                escape=false;
-                if (!isValidEscaping(c))
-                {
-                    b.append('\\');
-                }
-                b.append(c);
-            }
-            else if (c=='\\')
-            {
-                escape=true;
-            }
-            else
-            {
-                b.append(c);
-            }
-        }
-
-        return b.toString(); 
+        return unquoteOnly(s, false);
     }",NotBuggy,"396886 MultiPartFilter strips bad escaping on filename=""...""
",Buggy,"The commit message mentions fixing behavior related to stripping bad escaping from filenames, which aligns with the changes seen in the code. The original method `unquoteOnly` has detailed logic for handling escape characters and quotes, potentially addressing malformed inputs.

In the provided diff, the method `unquoteOnly` has been reduced to simply calling another method `unquoteOnly(s, false)`. Although the detailed implementation of the logic for processing the input string was removed, we can infer that the functionality may have been refactored or replaced elsewhere to address the stripping of bad escaping, given the commit message context.

The original code included mechanisms to handle escaped characters and checked for valid escaping, which is typically related to bug fixes concerning string processing errors, such as ensuring certain characters aren't incorrectly escaped. While the actual implementation here is removed, the context in the commit message strongly suggests the changes address a specific type of input error handling.

Thus, considering both the commit message and the likelihood of improving the handling of input strings by refactoring or fixing the string processing logic, the changes are consistent with a bug-related update. This leads to the conclusion:

**Buggy**"
jclouds,27006.json,1fc9b0e2597ee0e11d8ae09db478c2dbf516912b,"@@ -1,6 +1,7 @@
       public UriBuilder query(Multimap<String, ?> parameters) {
-         checkNotNull(parameters, ""parameters"");
+         Multimap<String, QueryValue> queryValueMultimap = Multimaps.transformValues(
+               checkNotNull(parameters, ""parameters""), QUERY_VALUE_TRANSFORMER);
          query.clear();
-         query.putAll(parameters);
+         query.putAll(queryValueMultimap);
          return this;
       }",NotBuggy,"Move QueryParam encoding to a separate class.

The patch implements a QueryValue class, which encodes the underlying
value based on whether the ""encoded"" flag is set. This class is used
by the RestAnnotationProcessor to propagate the @Encoded value set on
any parameters.

Since the encoding is now handled by the QueryValue instances, we
should no longer call encodeQueryLine() in the URI builder and instead
call buildQueryLine(). The caveat is that we need to make sure all of
the parameters that may need to be encoded are converted to QueryValue
objects. This is done by converting Object instances to QueryValue by
an instance of the TransformObjectToQueryValue when adding any query
parameters to the URI.
",NotBuggy,"Let's analyze the changes in the context of the commit message and determine if they indicate a bug fix:

1. **Commit Message Context**: The commit message describes a refactor where query parameter encoding is moved to a separate class called `QueryValue`. The encoding logic is encapsulated within instances of this new class. Consequently, there is a shift in how query parameters are handled, converting them into `QueryValue` objects, and indicates changes in encoding strategy.

2. **Code Changes**:
   - The `query` method in the `UriBuilder` class was modified. 
   - The original code directly used the `parameters` passed to it. The updated code introduces a transformation of these parameters using `QUERY_VALUE_TRANSFORMER`, converting them into `QueryValue` instances.
   - Instead of directly adding parameters to `query`, the transformed `queryValueMultimap` (a `Multimap` of `QueryValue`) is added.

3. **Objective of Changes**:
   - The change implements the new behavior as outlined in the commit message: encoding handling is moved to the `QueryValue` class, ensuring uniformity and correctness of query parameter encoding.
   - The change ensures that the `query` method now complies with the new encoding strategy by converting all parameters in the form of `QueryValue`, adhering to the commit's intention.

4. **Assessment for Bug Fix**:
   - The changes appear to be structural, focusing on a more organized way to handle query parameter encoding.
   - There is no specific indication of an error correction, logic fix, or problem resolution typical of a bug fix.
   - The change mainly introduces a new strategy for handling parameters that seems more like a refactor and enhancement.

Considering these aspects, the changes are well-aligned with the commit message and reflect a refactor rather than a bug fix. Hence, we conclude **NotBuggy**."
jclouds,18683.json,359b6b88d04d232f6f01d3f42429062e6c795b83,"@@ -1,10 +1,10 @@
    public String apply(byte[] from) {
       return Joiner.on(':').join(transform(partition(asList(from), 1), new Function<List<Byte>, String>() {
 
          @Override
          public String apply(List<Byte> from) {
-            return CryptoStreams.hex(toArray(from));
+            return base16().lowerCase().encode(toArray(from));
          }
 
       }));
    }",NotBuggy,"Move to Guava 14 BaseEncoding and Hashing
",NotBuggy,"The Git diff shows a change from using `CryptoStreams.hex` to `base16().lowerCase().encode` for encoding bytes to a hexadecimal string. This modification aligns with the commit message, which states the use of Guava 14's BaseEncoding. The change indicates a transition to a newer library feature rather than fixing an error in the original code.

In this context, the update is about library usage rather than addressing any pre-existing anomaly or error in logic, output, or behavior. Hence, this modification does not suggest a bug fix.

Therefore, I conclude the commit to be **NotBuggy**."
jclouds,16909.json,308911162109cd540f9246f8fcf6f6f7478bbd8d,"@@ -1,4 +1,4 @@
    public ParseObjectFromHeadersAndHttpContent setContext(HttpRequest request) {
       this.uri = request.getEndpoint();
-      return setPath(GeneratedHttpRequest.class.cast(request).getArgs().get(0).toString());
+      return setPath(GeneratedHttpRequest.class.cast(request).getInvocation().getArgs().get(0).toString());
    }",NotBuggy,"refactored internal code and tests to use FunctionalReflection
",NotBuggy,"The commit message indicates a refactoring, focusing on migrating internal code to use FunctionalReflection. However, the diff modifies how the path is set by changing the method chain from `getArgs()` to `getInvocation().getArgs()`. This change implies an internal restructuring or API modification rather than a bug fix, as it doesn't necessarily correct erroneous behavior but adjusts to a new method access pattern, potentially aligning with the mentioned refactoring. The change doesn't directly suggest a bug resolution from incorrect logic or error handling.

Given this reasoning, the changes align with updating the method chaining structure likely in line with the `FunctionalReflection` shift rather than fixing an inherent bug:

- The method call modification seems to be about adapting to a new class structure or version.
- There's no indication that this resolves an exception or error causing incorrect behavior.

Therefore, the change corresponds more to a structural update rather than a bug fix.

The assessment is **NotBuggy**."
jclouds,10478.json,3ac6f475e7d3baa61c3547a4f9693f2e130ea09d,"@@ -1,4 +1,4 @@
-      public AdminAccess.Builder adminFullName(String adminFullName) {
+      public Builder adminFullName(String adminFullName) {
          this.adminFullName = adminFullName;
          return this;
       }",NotBuggy,"unwound dependencies relating to ssh keys and crypt
",NotBuggy,"The commit message mentions unwinding dependencies related to SSH keys and crypt, but the code change shows a simple refactor where the inner class name `AdminAccess.Builder` is shortened to `Builder`. This change is likely an internal refactoring for code clarity or consistency and does not directly address SSH keys, cryptography, or any apparent bug fix. There is no indication that this refactor relates to resolving a bug, nor does it involve logic correction or error handling.

Given this analysis, the code change does not align with fixing a bug but rather with refactoring or code cleanup. Therefore, I would conclude with **NotBuggy**."
jclouds,26927.json,a625127fd2cf05c5c33c03d1649be558f76cbc2d,"@@ -1,10 +1,3 @@
                         public void close() throws IOException {
-                           try {
-                              method.invoke(injectee);
-                           } catch (InvocationTargetException ie) {
-                              Throwable e = ie.getTargetException();
-                              throw new IOException(e.getMessage());
-                           } catch (IllegalAccessException e) {
-                              throw new IOException(e.getMessage());
-                           }
+                           invokeOnInjectee(method, injectee);
                         }",NotBuggy,"added ability to look up constructors as Invokables
",NotBuggy,"The commit message refers to adding support for constructor lookups as ""Invokables,"" suggesting new functionality rather than a bug fix. The code change refactors exception handling by removing inline code and centralizing it in `invokeOnInjectee`. While it enhances readability and maintainability, it makes no logic or error-handling alterations that would suggest a bug fix. This aligns with new features rather than correcting existing issues. Therefore, the answer is **NotBuggy**."
jclouds,26813.json,d5f7f8b07e6a2be7631bd82d04fb1d21406307ec,"@@ -1,3 +1,3 @@
-      public int hashCode() {
-         return Objects.hashCode(value, someOtherValue);
-      }

+         public int hashCode() {
+            return delegate.hashCode();
+         }",NotBuggy,"refactored json internals to use Reflections2
",NotBuggy,"The commit message mentions refactoring JSON internals to use `Reflections2`, but the code diff addresses changes in the `hashCode()` method of a class. Specifically, it refactors the `hashCode()` method to delegate its hash code calculation to another object's `hashCode()` method instead of using `Objects.hashCode()` for individual fields.

This modification does not involve the use of `Reflections2` nor does it directly indicate any JSON processing logic; instead, it appears to simplify or change how `hashCode()` is calculated, using `delegate.hashCode()` instead.

There is no indication of a bug being fixed here. The commit message and the code change appear to be unrelated in terms of functionality, as it doesn’t directly involve using `Reflections2` or processing JSON internals. Therefore, this appears to be more of a refactor or code cleanup rather than a bug fix.

The answer is **NotBuggy**."
jclouds,14460.json,3af4d3a5bb345253c58d8ff8fbdbaff0307653f0,"@@ -1,7 +1,7 @@
-      public static Protocol fromValue(String value) {
+      public static State fromValue(String value) {
          try {
             return valueOf(value.toUpperCase());
          } catch(IllegalArgumentException e) {
             return UNKNOWN;
          }
       }",NotBuggy,"Use enum for *Rule.state
",NotBuggy,"The commit message mentions an update involving the use of an enum for *Rule.state, while the code changes a method from returning a `Protocol` to returning a `State`. This change seems unrelated to the bug fixes or performance improvements indicated in the commit message. Moreover, the modification doesn't necessarily correct any incorrect logic or handle any exceptions that would hint at a bug fix. This appears to be more aligned with a refactoring effort involving code organization or clarity, rather than addressing a bug.

Therefore, based on this specific change and the lack of direct alignment with error-handling improvements or logical corrections, I conclude that the change is **NotBuggy**."
jclouds,13261.json,d9c65046d0a46839b4d05570817c43a7df5b5bd6,"@@ -1,3 +1,3 @@
    public String getAccount() {
-      return account;
+      return this.account;
    }",NotBuggy,"cloudstack: adjusting beans to use newer de/serialization annotations
",NotBuggy,"Analyzing the code diff and the commit message:

1. **Commit Message**: The commit message indicates an adjustment in the use of de/serialization annotations in some beans within the cloudstack project. This implies a refactoring or enhancement of serialization logic, likely for consistency with newer standards or improved performance.

2. **Code Diff**: The code modification here reflects a minor change in how the `account` field is accessed within its getter method by using `this.account` instead of just `account`. This change is typically a stylistic preference, clearer for developers as it explicitly indicates the access of an instance variable.

- The change from `return account;` to `return this.account;` doesn't alter behavior or fix logic errors. It enhances code readability and aligns with best practices by explicitly showing that `account` belongs to the current instance.

3. **Evaluation**: This specific change doesn't address de/serialization annotations or indicate a bug fix. It's not directly connected to any serialization logic or deserialization annotations. No bug is described or resolved by this minor edit.

Concluding from the analysis, the code change doesn't directly relate to any bug or bug-related aspect as stated in the commit message. The adjustment is a minor coding style improvement.

**NotBuggy**"
jclouds,6344.json,40d9f8bfe75f48f2a317aecdd5439fd6a9840d62,"@@ -1,5 +1,5 @@
-      public ListenableFuture<Boolean> create(Throwable t) {
+      public Boolean createOrPropagate(Throwable t) throws Exception {
          if (returnValueOnCodeOrNull(t, false, equalTo(400)) != null)
-            return immediateFuture(false);
+            return false;
          throw propagate(t);
       }",NotBuggy,"changes that facilitate synchronous fallbacks and http invocation
",NotBuggy,"The commit message describes changes related to synchronous fallbacks and HTTP invocation. The diff shows that the method `create` was renamed to `createOrPropagate`, and it now returns a `Boolean` directly instead of a `ListenableFuture<Boolean>`. This indicates a shift from asynchronous to synchronous handling.

Here are key observations:

1. **Method Signature Change**: The return type has changed from `ListenableFuture<Boolean>` to `Boolean`. This suggests a move from asynchronous operations to synchronous ones, which aligns with the commit message about facilitating synchronous fallbacks.

2. **Immediate Return Change**: The code changes an `immediateFuture(false)` return to just `false`, reflecting the new synchronous nature.

3. **Exception Propagation**: It now explicitly throws an `Exception`, which suggests a different approach to handling errors, consistent with the commit message mentioning HTTP invocation adjustments. This might align better with synchronous error handling strategies.

4. **No Bug Reference**: While the changes facilitate synchronous operation, there is no explicit reference to a bug fix, only architectural or functional changes towards a potentially new operation model.

The commit does not appear to address a specific bug in the code but rather shifts the method's operation and intended use pattern. Therefore, the changes are not primarily bug-oriented but are more about facilitating synchronous functionality, as stated in the commit message.

**NotBuggy**"
jclouds,13530.json,bff49f8311cce1e45fe57e7e1fb43419b3af00ba,"@@ -1,3 +1,3 @@
    public int hashCode() {
-      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, hostTags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
+      return Objects.hashCode(id, allocationState, averageLoad, capabilities, clusterId, clusterName, clusterType, cpuAllocated, cpuNumber, cpuSpeed, cpuUsed, cpuWithOverProvisioning, created, disconnected, diskSizeAllocated, diskSizeTotal, events, hasEnoughCapacity, tags, hypervisor, ipAddress, localStorageActive, jobId, jobStatus, lastPinged, managementServerId, memoryAllocated, memoryTotal, memoryUsed, name, networkKbsRead, networkKbsWrite, osCategoryId, osCategoryName, podId, podName, removed, state, type, version, zoneId, zoneName);
    }",NotBuggy,"compatible tags across 2.0 and 4.0
",NotBuggy,"The commit message mentions ""compatible tags across 2.0 and 4.0,"" which suggests introducing or synchronizing a feature for compatibility rather than fixing a bug. The code change itself modifies the hashCode method by adding `tags` to the fields used for generating the hash code. This change is more about ensuring consistent behavior across different versions rather than addressing a logical or functional error.

As no error handling, exception management, or logical bug rectification is evident in this simple addition of a field for hash consistency, it seems unrelated to a bug fix.

Therefore, the changes do not indicate a bug fix. The answer is **NotBuggy**."
jclouds,5234.json,b1877bbee57afc4886627c4ad69cda50aca622ab,"@@ -1,3 +1,3 @@
-      public Builder archiveSizes(Integer... archiveSizes) {
-         return archiveSizes(Arrays.asList(archiveSizes));
+      public T archiveSizes(Integer... in) {
+         return archiveSizes(ImmutableList.copyOf(in));
       }",NotBuggy,"glesys: bean cleaning, using ConstructorProperties for deserialization (note no serialization annotations as we don't currently serialize any of these beans)
",NotBuggy,"The commit message suggests refactoring for deserialization using `ConstructorProperties`, and mentions no serialization annotations due to no serialization of these beans. However, the code change mainly simplifies a method by transforming an array to an `ImmutableList` instead of using `Arrays.asList`. This isn't directly related to deserialization or indicative of a bug fix. The focus is on refactoring for immutability, not fixing bugs. Thus, the analysis concludes with **NotBuggy**."
jclouds,5905.json,602002660abb287d684e6a68d0ea5f908279e13a,"@@ -1,4 +1,8 @@
    public DomainOptions responsiblePerson(String responsiblePerson) {
+      responsiblePerson = responsiblePerson.replaceAll(""@"", ""."");
+      if (!responsiblePerson.endsWith(""."")) {
+         responsiblePerson = responsiblePerson + ""."";
+      }
       formParameters.put(""resp_person"", responsiblePerson);
       return this;
    }",NotBuggy,"Improving javadocs for Archive, Domain, Email and Server GleSYS clients
",Buggy,"The commit message indicates an update to the Java documentation, but the code diff shows a modification to method logic in `DomainOptions`. It replaces `@` with `.` in the `responsiblePerson` string and appends a period if not already present. Such changes imply validating or normalizing input data, typically to correct input mishandling.

Here's a step-by-step breakdown:
- **Initial Processing:** The method processes `responsiblePerson` input.
- **Replace Operation:** Replacing ""@"" with ""."" suggests reformatting for email-like strings or domain handling, hinting at potential past input format issues.
- **Ends-With Check:** Ensures the string ends with a period. If not already present, it adds one, likely to prevent downstream processing errors or ensure consistency.

This update, in contrast to the commit message about Javadocs improvement, does not match the expected documentation changes. The logic alterations suggest a fix for a bug related to incorrect handling of input strings for the `responsiblePerson`.

Based on this analysis, despite the discrepancy with the commit message, the code changes imply a bug fix. Thus, the conclusion is **Buggy**."
jclouds,27602.json,edde721937115beeee01ea4076d60b4ecf381807,"@@ -1,6 +1,6 @@
-   protected Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
+   protected final Supplier<Map<String, Supplier<URI>>> regionIdToURISupplier(
             AtomicReference<AuthorizationException> authException, @Named(PROPERTY_SESSION_INTERVAL) long seconds,
             RegionIdToURISupplier uncached) {
       return MemoizedRetryOnTimeOutButNotOnAuthorizationExceptionSupplier.create(authException, uncached, seconds,
                TimeUnit.SECONDS);
    }",NotBuggy,"JCLOUDS-962: Make all @Provider methods final
",NotBuggy,"The commit message states that it makes all @Provider methods final. The diff changes a method by adding the `final` keyword to its signature. This change aligns with the commit message, which is focused on encapsulation and not on fixing errors or bugs. 

Adding `final` to a method suggests preventing overrides, which is more about design decisions than bug fixing. There's no evidence of error handling or logical corrections typically associated with a bug fix. 

Therefore, the conclusion is **NotBuggy**."
jclouds,11802.json,5ae1ba44d243824d6a33c9547876b71fc4a272ec,"@@ -1,3 +1,3 @@
-   public OperatingSystem build() {
-      return new OperatingSystem(family, name, version, arch, description, is64Bit);
-   }

+      public OperatingSystem build() {
+         return new OperatingSystem(family, name, version, arch, description, is64Bit);
+      }",NotBuggy,"added default operating system types to savvis
",NotBuggy,"The commit message mentions adding default operating system types, but the diff only shows a minor formatting change with indentation adjustments. There is no indication of a functional change, no addition of default operating system types, nor any signs of a bug being fixed within this diff.

This discrepancy suggests that the code change does not align with the commit message intent, as it suggests a simple cosmetic change rather than implementing new functionality or fixing a bug.

Thus, I would conclude that the changes are **NotBuggy**."
jclouds,27976.json,04fe89b75a90ae197fca1a5128c52986e5beb100,"@@ -1,5 +1,4 @@
    public void exit() {
       checkState(caller.get() != null, ""No scoping block in progress"");
-      callerEnclosingType.remove();
       caller.remove();
    }",NotBuggy,"update to guava 14.0-rc2
",NotBuggy,"The commit message states an update to a new version of a library, Guava 14.0-rc2. However, the code changes involve removing a call to `callerEnclosingType.remove()`, which could impact the behavior of the `exit` method in terms of its scoping mechanism or cleanup process. The commit message does not mention any bug fix or reasoning for why this line was removed, making the relationship between the message and this change unclear.

Without additional context or indication that this line's removal fixes a specific problem—such as an obsolete method call due to the Guava update or a bug that has been addressed in the update—it's difficult to conclude that this is a bug fix purely from the provided information.

Therefore, based on the information available, the changes do not explicitly indicate a bug fix. The answer is **NotBuggy**."
okhttp,755.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
-  public String name(int index) {
-    return percentDecode(encodedName(index), true);
+  public String value(int index) {
+    return percentDecode(encodedValue(index), true);
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",NotBuggy,"The commit message indicates a package name change, but the diff shows a method name change from `name` to `value` and a change in the method `percentDecode`'s parameter from `encodedName(index)` to `encodedValue(index)`. This doesn't align with the commit message about renaming a package and suggests a change in functionality or method logic rather than a package rename.

Because there is no indication from the commit changes alone that this addresses a bug (such as correcting logic, error handling, etc.) and it seems unrelated to the stated commit purpose, we conclude:

**NotBuggy**"
okhttp,1728.json,d3c57bd4f38d5d3944da88266dd9b474a84d802b,"@@ -1,3 +1,3 @@
   @Override public String toString() {
-    throw new UnsupportedOperationException();
+    return url;
   }",NotBuggy,"HttpURL parsing.

This includes everything but character encoding. The implementation
is built around one oversized method that runs top to bottom; I found
this was easier to follow than the switch statement recommended by the
URL standard.
",NotBuggy,"The commit message discusses HTTP URL parsing implementation details, but the code change addresses the `toString()` method. Originally, `toString()` threw an `UnsupportedOperationException`, indicating it didn't work as intended. The update returns a string, likely representing the URL, which suggests previously omitted functionality is now operational. However, there's no indication of a bug fix or of error handling regarding URL parsing or character encoding. The focus is completing functionality, not fixing a bug. Hence, the answer is **NotBuggy**."
okhttp,131.json,1a1cc18f26c381f405d62f5b9262b3113736d014,"@@ -1,3 +1,5 @@
   public static void main(String[] args) throws IOException {
-    new GetExample().run();
+    GetExample example = new GetExample();
+    String response = example.run(""https://raw.github.com/square/okhttp/master/README.md"");
+    System.out.println(response);
   }",NotBuggy,"Update samples and marketing site.
",NotBuggy,"The commit message indicates updates to samples and a marketing site. The code change modifies an example by creating an instance and capturing a response from a URL. This seems related to updating a sample for demonstration purposes rather than fixing any bug. Consequently, there is no indication of a bug fix, error handling correction, or logical bug correction in the diff.

Step-by-step breakdown:

1. **Class instantiation and method modification**: The change initiates an instance of `GetExample` and modifies the argument of the `run` method.
2. **Output display**: The new version captures the result of `example.run()` into a `String` and prints it.

These modifications suggest enhancing the functionality of the example, potentially for showcasing in a sample context or marketing site, aligning with the commit message.

Thus, the coding changes are consistent with the commit message without indicating a bug fix. The conclusion is **NotBuggy**."
okhttp,2091.json,22a03b59fac9fa58cb59a6f240c8280a2a1eb801,"@@ -1,3 +1,3 @@
-  public void trackConditionalCacheHit() {
-    // This method is treated as optional.
+  @Override public void trackConditionalCacheHit() {
+    // This method is optional.
   }",NotBuggy,"Drop ResponseSource.
",NotBuggy,"The commit message says ""Drop ResponseSource,"" suggesting the removal of specific functionality. The code adds an @Override annotation to trackConditionalCacheHit and slightly alters a comment. This doesn't involve dropping ResponseSource functionality. The code modification doesn't align with the commit message, nor does it reflect a bug fix, as no bug-related changes are evident, such as error handling or logic corrections. It primarily appears to be a minor annotation addition. Therefore, the answer is **NotBuggy**."
okhttp,1794.json,7028195274f67d2c72f4c5ee472d0dcd6a97c1e4,"@@ -1,12 +1,13 @@
   public boolean isRedirect() {
     switch (code) {
+      case HTTP_PERM_REDIRECT:
       case HTTP_TEMP_REDIRECT:
       case HTTP_MULT_CHOICE:
       case HTTP_MOVED_PERM:
       case HTTP_MOVED_TEMP:
       case HTTP_SEE_OTHER:
         return true;
       default:
         return false;
     }
   }",NotBuggy,"Handle response code 308 - Permanent Redirect
",Buggy,"The commit message states that a new HTTP response code, 308 (Permanent Redirect), is being handled. The code modification reflects this by adding an additional case for `HTTP_PERM_REDIRECT` in the `isRedirect()` method's switch statement. This ensures that when the response code is 308, the `isRedirect()` method correctly identifies it as a redirect, which might not have been handled before.

Such handling of previously unhandled conditions to ensure functionality works correctly under all specified cases is indicative of a bug fix. Without this change, the system could fail to recognize a 308 status code as a redirect, leading to incorrect behavior.

Based on this analysis:

1. **Identification**: The modification is directly aligned with the commit message, which mentions handling a specific HTTP code.
2. **Correction**: The addition of `HTTP_PERM_REDIRECT` suggests that previously the method didn't account for redirect responses using the 308 status code, indicating a logical error.
3. **Functionality Impact**: This correction ensures accurate functionality in response processing.

Conclusively, the changes made in this diff do represent a bug fix. Hence, the final verdict is **Buggy**."
okhttp,1402.json,e74e3f3bf744ef7f4d8ee724a7cf2347e486cfab,"@@ -1,11 +1,22 @@
-  public static boolean skipAll(Source in, int timeoutMillis) throws IOException {
-    // TODO: Implement deadlines everywhere so they can do this work.
-    long startNanos = System.nanoTime();
-    Buffer skipBuffer = new Buffer();
-    while (NANOSECONDS.toMillis(System.nanoTime() - startNanos) < timeoutMillis) {
-      long read = in.read(skipBuffer, 2048);
-      if (read == -1) return true; // Successfully exhausted the stream.
-      skipBuffer.clear();
+  public static boolean skipAll(Source source, int duration, TimeUnit timeUnit) throws IOException {
+    long now = System.nanoTime();
+    long originalDuration = source.timeout().hasDeadline()
+        ? source.timeout().deadlineNanoTime() - now
+        : Long.MAX_VALUE;
+    source.timeout().deadlineNanoTime(now + Math.min(originalDuration, timeUnit.toNanos(duration)));
+    try {
+      Buffer skipBuffer = new Buffer();
+      while (source.read(skipBuffer, 2048) != -1) {
+        skipBuffer.clear();
+      }
+      return true; // Success! The source has been exhausted.
+    } catch (InterruptedIOException e) {
+      return false; // We ran out of time before exhausting the source.
+    } finally {
+      if (originalDuration == Long.MAX_VALUE) {
+        source.timeout().clearDeadline();
+      } else {
+        source.timeout().deadlineNanoTime(now + originalDuration);
+      }
     }
-    return false; // Ran out of time.
   }",NotBuggy,"Move cache writing out of the transport.

This is necessary to unblock network interceptors, where the interceptor
may elect to rewrite the response body. If we've already cached the
original response body, we're too late.
",NotBuggy,"The commit message describes moving cache writing out of a transport layer, which is necessary for proper interception of network responses. The code changes illustrate a method modification involving timeout handling for reading from a source. 

Changes in `skipAll` method:
- **Timeout Handling Improvement:** The updated method now sets a deadline for the source's timeout based on either its existing deadline or the provided duration. This ensures robust timeout management in situations where the source's response needs to be completely consumed within a specific time.
- **Exception Handling Adjustments:** The updated version includes handling for `InterruptedIOException` to indicate when the source wasn't fully read due to time constraints, which reflects better error handling.
- **Restoration of Previous State:** The source's deadline is either cleared or reset to its original value after the method operations. This ensures that subsequent operations won't be adversely affected by the temporary timeout change.

While the changes improve the handling of source reading by accurately managing timeouts and handling exceptions, they don't directly relate to caching or the primary concern of network interception as described in the commit message. The adjustments are more about enhancing robustness and precision in deadline management and exception handling rather than fixing a specific bug related to cache writing or network interception.

Therefore, while the changes represent a significant improvement in the robustness of timeout operations, they do not indicate a specific bug fix related to cache writing or network interception context.

Conclusion: **NotBuggy**."
okhttp,649.json,8e201297fe9e6b5c908848c3cef633966e49c0d9,"@@ -1,3 +1,3 @@
-  public List<String> getHeaders() {
+  public Headers getHeaders() {
     return headers;
   }",NotBuggy,"Update MockWebServer to use Headers.
",NotBuggy,"The commit message indicates a change in MockWebServer to use Headers. The diff shows a method's return type changing from List<String> to Headers. This aligns with the commit message, focusing on internal API modifications instead of a bug fix. The change doesn't denote handling of errors, logical corrections, or other bug-fixing signs. It appears to be an API update. Therefore, the conclusion is **NotBuggy**."
okhttp,303.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,7 +1,6 @@
   public void open(String accessToken) throws IOException {
-    if (webSocketCall != null) throw new IllegalStateException();
+    if (webSocket != null) throw new IllegalStateException();
 
     RtmStartResponse rtmStartResponse = slackApi.rtmStart(accessToken);
-    webSocketCall = slackApi.rtm(rtmStartResponse.url);
-    webSocketCall.enqueue(this);
+    webSocket = slackApi.rtm(rtmStartResponse.url, this);
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",NotBuggy,"The commit involves transitioning from using `webSocketCall` to `webSocket`, in line with migrating to an asynchronous WebSockets API. The focus here is on an architectural change to support asynchronous operations, and not directly on bug fixes. The change from `webSocketCall` to `webSocket` reflects a structural shift rather than a correction to erroneous behavior in the code.

There are no specific fixes related to error handling, logical errors, exception handling, or other indications of previously buggy behavior within the provided diff. Therefore, this change appears to align with the migration aspect of the commit message without indicating a direct bug fix.

Therefore, I conclude that this change is **NotBuggy**."
okhttp,852.json,7131fb250c34882c32f084c5b01a28ad0b6de25c,"@@ -1,3 +1,3 @@
       @Override public Call newWebSocketCall(OkHttpClient client, Request originalRequest) {
-        return new RealCall(client, originalRequest, true);
+        return RealCall.newRealCall(client, originalRequest, true);
       }",NotBuggy,"Make EventListener public and begin implementing Dns events.
",NotBuggy,"The commit message talks about making `EventListener` public and implementing DNS events, but the diff shows a change in the method used to create a new `RealCall` object. This involves changing the instantiation from a constructor to a static factory method. The change could relate to refactoring for consistency, clarity, or planned extensibility but does not indicate a fix related to bugs or error handling.

Given that the change involves modifying how an instance of `RealCall` is created and no evidence of addressing a bug in `EventListener` or DNS events is present in the provided diff, the answer is **NotBuggy**."
okhttp,1412.json,e786a376426b938738d3333f52891c68339ae187,"@@ -1,6 +1,6 @@
-  public static <T> int indexOf(T[] array, T value) {
+  public static int indexOf(Comparator<String> comparator, String[] array, String value) {
     for (int i = 0, size = array.length; i < size; i++) {
-      if (equal(array[i], value)) return i;
+      if (comparator.compare(array[i], value) == 0) return i;
     }
     return -1;
   }",NotBuggy,"Handle either TLS_ or SSL_ prefixes for cipher suite names.

Closes: https://github.com/square/okhttp/issues/3173
",NotBuggy,"The commit message indicates that the change is supposed to allow handling of either `TLS_` or `SSL_` prefixes for cipher suite names. However, the code diff provided reflects a modification to the `indexOf` method, changing it to use a `Comparator<String>` for comparison instead of using the `equal` method directly.

Here's the step-by-step analysis:

1. **Method Signature Change**: 
   - Original: `public static <T> int indexOf(T[] array, T value)`
   - Modified: `public static int indexOf(Comparator<String> comparator, String[] array, String value)`
   - The modification changes the method to specifically operate on `String` arrays, with a `Comparator<String>` for comparison, which adds more flexibility by allowing custom comparison logic.

2. **Comparison Logic Change**:
   - Original: `if (equal(array[i], value)) return i;`
   - Modified: `if (comparator.compare(array[i], value) == 0) return i;`
   - Instead of using the `equal` method, it now uses a `Comparator` to determine equality, which allows for more sophisticated comparison, potentially including handling for different prefixes such as `TLS_` or `SSL_`.

3. **Relevance to Commit Message**:
   - The change allows for customized comparison logic through the `Comparator<String>`, which could feasibly be used to compare strings with the `TLS_` and `SSL_` prefixes being treated as equivalent or interchangeable.
   - Although the code diff does not directly show the handling of `TLS_` or `SSL_`, the introduction of `Comparator<String>` suggests preparatory work for enabling this behavior.

The modifications align with enhancing the flexibility needed for handling the cipher suite name prefixes (`TLS_` and `SSL_`), although they don't directly indicate a bug fix. The change seems more about adding capability rather than fixing a specific bug. Based on provided information, the correct assessment is **NotBuggy**."
okhttp,185.json,c9a89876de476983f273edbf108c365127c18c5e,"@@ -1,3 +1,3 @@
   public static void main(String... args) throws Exception {
-    new PerCallSettings().run();
+    new CertificatePinning().run();
   }",NotBuggy,"Rename com.squareup.okhttp to okhttp3

Maven group changes will come in follow up.
",NotBuggy,"The given diff shows a change in the main method which replaces `new PerCallSettings().run();` with `new CertificatePinning().run();`. The commit message states that there's a package renaming from `com.squareup.okhttp` to `okhttp3`, which doesn't correlate with the specific change seen in the diff.

The diff does not reflect any renaming effort, such as package declarations or import statements being adjusted. Instead, the change swaps which class's `run()` method is executed in the main function, indicating a change in application behavior, not a package rename.

The commit message does not accurately describe the change in the diff. Since this modification doesn't involve correcting logic, exceptions, or errors, it's unlikely indicative of a bug fix.

Thus, the changes do not align with the commit message and do not represent fixing a bug. Therefore, the evaluation is **NotBuggy**."
okhttp,1274.json,40dddb9dcf8ba654e16f6497020043b38b6d22a1,"@@ -1,25 +1,26 @@
   private void checkResponse(Response response) throws ProtocolException {
     if (response.code() != 101) {
       throw new ProtocolException(""Expected HTTP 101 response but was '""
           + response.code() + "" "" + response.message() + ""'"");
     }
 
     String headerConnection = response.header(""Connection"");
     if (!""Upgrade"".equalsIgnoreCase(headerConnection)) {
       throw new ProtocolException(""Expected 'Connection' header value 'Upgrade' but was '""
           + headerConnection + ""'"");
     }
 
     String headerUpgrade = response.header(""Upgrade"");
     if (!""websocket"".equalsIgnoreCase(headerUpgrade)) {
       throw new ProtocolException(
           ""Expected 'Upgrade' header value 'websocket' but was '"" + headerUpgrade + ""'"");
     }
 
     String headerAccept = response.header(""Sec-WebSocket-Accept"");
-    String acceptExpected = Util.shaBase64(key + WebSocketProtocol.ACCEPT_MAGIC);
+    String acceptExpected = ByteString.encodeUtf8(key + WebSocketProtocol.ACCEPT_MAGIC)
+        .sha1().base64();
     if (!acceptExpected.equals(headerAccept)) {
       throw new ProtocolException(""Expected 'Sec-WebSocket-Accept' header value '""
           + acceptExpected + ""' but was '"" + headerAccept + ""'"");
     }
   }",NotBuggy,"Migrate MockWebServer to the async web sockets API.
",NotBuggy,"The commit message suggests a migration to an async web sockets API, but the provided code changes focus on handling the WebSocket handshake protocol, specifically:

1. It updates the computation of `acceptExpected` from using `Util.shaBase64()` to using a more clearly articulated combination of methods: `ByteString.encodeUtf8().sha1().base64()`. Presumably, this fits better with the new async API, though it's essentially the same logic to produce the SHA1 hash in base64.

The key change here is not fixing bugs but changing implementation details to align with API differences. This change ensures that `acceptExpected` is calculated using a compatible method for the WebSocket handshake, which is a necessary part of the migration to a new API. However, there's no explicit mention or indication in the code diff that points to fixing any existing bug.

Thus, despite being related to a migration effort rather than a specific bug fix, we can infer that the technical correction ensures compatibility with the new async framework.

Since this commit primarily targets migration/compatibility rather than correcting a defect, it is **NotBuggy**."
okhttp,1474.json,64df3922164136ac4587df235222d564b016ce25,"@@ -1,11 +1,11 @@
   public void setSettings(Settings settings) throws IOException {
     synchronized (writer) {
       synchronized (this) {
         if (shutdown) {
           throw new ConnectionShutdownException();
         }
         okHttpSettings.merge(settings);
-        writer.settings(settings);
       }
+      writer.settings(settings);
     }
   }",NotBuggy,"Don't hold a lock on the connection when writing settings. (#3867)

This breaks our concurrency rules. Fortunately it's only academic; only
MockWebServer ever writes settings, and it only does so in very specialized
tests.",Buggy,"The commit message states a change related to concurrency rules by avoiding holding a lock on the connection when writing settings. The diff shows that the `writer.settings(settings);` call is moved from within the synchronized block on `this` to outside of it. This change is likely made to adhere to proper concurrency practices, minimizing the scope of the lock held on `this` and reducing contention, though it mentions this issue is mostly academic since the functionality is primarily used in testing scenarios.

The change aligns with the commit message and addresses a potential concurrency bug by reducing inappropriate lock use, even though its practical implications may be limited in the current usage context. However, it is correcting a code practice issue that could be problematic if the code or its use cases expand.

Given this alignment and the nature of the improvement, the answer is **Buggy** due to the concurrency bug adjustment."
okhttp,262.json,3699d5c9fd0ad78fc52e3ea317951f9d485f656f,"@@ -1,33 +1,34 @@
   public void run() throws Exception {
     RequestBody requestBody = new RequestBody() {
       @Override public MediaType contentType() {
         return MEDIA_TYPE_MARKDOWN;
       }
 
       @Override public void writeTo(BufferedSink sink) throws IOException {
         sink.writeUtf8(""Numbers\n"");
         sink.writeUtf8(""-------\n"");
         for (int i = 2; i <= 997; i++) {
           sink.writeUtf8(String.format("" * %s = %s\n"", i, factor(i)));
         }
       }
 
       private String factor(int n) {
         for (int i = 2; i < n; i++) {
           int x = n / i;
           if (x * i == n) return factor(x) + "" ￃﾗ "" + i;
         }
         return Integer.toString(n);
       }
     };
 
     Request request = new Request.Builder()
         .url(""https://api.github.com/markdown/raw"")
         .post(requestBody)
         .build();
 
-    Response response = client.newCall(request).execute();
-    if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
+    try (Response response = client.newCall(request).execute()) {
+      if (!response.isSuccessful()) throw new IOException(""Unexpected code "" + response);
 
-    System.out.println(response.body().string());
+      System.out.println(response.body().string());
+    }
   }",NotBuggy,"Better documentation and examples on closing response bodies.

Also implement Closeable on Response to make it easier to do the right thing.
",NotBuggy,"The commit message states that the changes are aimed at improving documentation and handling of response bodies, specifically related to closing them. The code diff aligns with this message as it modifies response handling by using a try-with-resources block, ensuring that the response is automatically closed. Implementing the interface `Closeable` is not shown in this diff, meaning that if such a change was made elsewhere, it is not part of this specific diff snippet.

Here's the analysis of changes made:
1. Prior to the change, the response was not wrapped in a try-with-resources block. If anything went wrong after `response` was assigned but before it was closed, resources might not be released, potentially leading to resource leaks.
2. The updated code snippet wraps the response in a try-with-resources statement, ensuring that the response is closed properly and automatically, even if an exception is thrown. This change significantly improves the robustness of the code by mitigating the risk of resource leaks.

This addition does not indicate a bug fix related to incorrect functionality or logic errors but instead improves resource management, possibly anticipating problems in more complex scenarios.

While the refactoring improves code reliability and makes it align with best practices, based on the given commit message and the nature of the change, this is not directly addressing a bug in the typical sense (e.g., incorrect output or malfunction). Thus, the conclusion is **NotBuggy**."
okhttp,1692.json,c07f55ac92d1e931260711edae987b51bee0be3d,"@@ -1,3 +1,3 @@
-  public String getScheme() {
+  public String scheme() {
     return scheme;
   }",NotBuggy,"Drop 'get' prefix on Challenge value type.
",NotBuggy,"The commit message states that the 'get' prefix is being dropped from a method in a type called ""Challenge"", which is consistent with the modification shown in the diff. The change in the diff involves renaming the method `getScheme()` to `scheme()`, which is a straightforward naming change.

Looking at the nature of this change, it is a refactoring for naming consistency or style purposes, which does not indicate a bug fix. It does not involve any error handling, logic corrections, or exception handling improvements.

Thus, the modification aligns with the commit message as a naming update rather than a bug fix. Therefore, the evaluation is **NotBuggy**."
openmrs-core,6817.json,2f888dceb5bdb71589beaaddf47d66a20fda689d,"@@ -1,4 +1,3 @@
 	public void updateConceptIndex(Concept concept) throws APIException {
-		checkIfLocked();
-		dao.updateConceptWord(concept);
+		Context.updateSearchIndexForObject(concept);
 	}",NotBuggy,"TRUNK-2999: Investigate replacing concept word with full text search
",NotBuggy,"The commit message suggests an investigation into replacing concept word indexing with full-text search. The code changes reflect this transition. The method `updateConceptIndex` now calls `Context.updateSearchIndexForObject` instead of `dao.updateConceptWord`, indicating a switch to a different indexing mechanism, likely a full-text search. The removal of `checkIfLocked()` implies that the new approach either doesn't require this check or handles it internally.

The change does not explicitly indicate a bug fix, as it primarily reflects a refactoring or enhancement aimed at improving search capabilities. The commit message refers to an investigation and potential enhancement rather than directly addressing a bug. Even though the change could resolve underlying inefficiencies or limitations (often classified as bugs) in the old system, without explicit indication of a bug being fixed, it is perceived more as a feature or system improvement.

Therefore, I conclude that the changes do not directly indicate a bug fix. The answer is **NotBuggy**."
openmrs-core,7534.json,fac6dd5b510edc40e5e8a1a96b2da9c6da01ac5c,"@@ -1,3 +1,3 @@
 	public static String drugToString(Drug drug) {
-		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + FormConstants.HL7_LOCAL_DRUG;
+		return drug.getDrugId() + ""^"" + drug.getName() + ""^"" + HL7Constants.HL7_LOCAL_DRUG;
 	}",NotBuggy,"move HL7-related constants from FormConstants to HL7Constants - #2338
Author: PKirwa

git-svn-id: http://svn.openmrs.org/openmrs/trunk@14004 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message states that HL7-related constants are being moved from FormConstants to HL7Constants. The diff shows the code change reflects this action by replacing `FormConstants.HL7_LOCAL_DRUG` with `HL7Constants.HL7_LOCAL_DRUG`, which aligns with the commit message. This change is about refactoring and organizing the constants more logically by placing HL7-related constants into HL7Constants instead of FormConstants. It does not suggest any bug fix since there is no indication of changes to logic, error handling, or exception handling beyond relocating a constant. Thus, the answer is **NotBuggy**."
openmrs-core,7488.json,c1b7d42d30069758be06afdbe4083759622cfd66,"@@ -1,5 +1,5 @@
 	public void globalPropertyDeleted(String propertyName) {
 		// reset the value
-		defaultLocaleCache = null;
-		localesAllowedListCache = null;
+		setDefaultLocaleCache(null);
+		setLocalesAllowedListCache(null);
 	}",NotBuggy,"TRUNK-4232: SonarQube - Write to static field from instance method
",NotBuggy,"The commit message refers to a SonarQube issue regarding writing to a static field from an instance method. The code modifies the access to `defaultLocaleCache` and `localesAllowedListCache` by replacing direct assignments with setter methods `setDefaultLocaleCache` and `setLocalesAllowedListCache`. 

This change likely encapsulates the field access, providing a controlled way to update these static fields, which could enhance code maintainability and adhere to best practices flagged by SonarQube.

Given the context in the commit message, the change appears to address a coding practice issue rather than a logical error or bug within the software's functionality. Instead of directly fixing a bug, it's refining the code to align with best practices.

Based on this evaluation, the conclusion is **NotBuggy**."
openmrs-core,1419.json,fb1e831dc7929e004b486c06545a4cb4517914cb,"@@ -1,8 +1,8 @@
 	public static File getDevelopmentDirectory(String moduleId) {
-		String directory = System.getProperty(""uiFramework.development."" + moduleId);
+		String directory = System.getProperty(moduleId + "".development.directory"");
 		if (StringUtils.isNotBlank(directory)) {
 			return new File(directory);
 		}
 		
 		return null;
 	}",NotBuggy,"Renaming VM argument from uiFramework.development.moduleId to
moduleId.development.directory - TRUNK-4739",NotBuggy,"The commit message indicates a change in naming convention for a system property key from ""uiFramework.development.moduleId"" to ""moduleId.development.directory"". The code reflects this change by adjusting the string concatenation logic for retrieving the directory from system properties. This is a straightforward refactor to accommodate a naming change and does not address a bug fix or alter the functionality of the code.

Given that the commit message and the code change are aligned in describing a renaming convention rather than fixing a bug, the appropriate conclusion is **NotBuggy**."
openmrs-core,4987.json,3a3f703f9373ebb6d76917406d7fb6f006c1434e,"@@ -1,4 +1,5 @@
 	public ConceptMapType getConceptMapTypeByName(String name) throws DAOException {
-		return (ConceptMapType) sessionFactory.getCurrentSession().createQuery(
-		    ""from ConceptMapType cmt where cmt.name = :name"").setString(""name"", name).uniqueResult();
+		Criteria criteria = sessionFactory.getCurrentSession().createCriteria(ConceptMapType.class);
+		criteria.add(Restrictions.ilike(""name"", name, MatchMode.EXACT));
+		return (ConceptMapType) criteria.uniqueResult();
 	}",NotBuggy,"Fetching concept reference terms and map types by name should be case insensitive - 3073

git-svn-id: http://svn.openmrs.org/openmrs/trunk@26058 5bac5841-c719-aa4e-b3fe-cce5062f897a
",Buggy,"The commit message indicates that the method for fetching concept reference terms and map types by name should be case insensitive. The code change modifies the query to use `Restrictions.ilike`, which performs a case-insensitive match (LIKE in SQL) for the `name` attribute, specifically matching the exact string regardless of case.

This change aligns perfectly with the commit message, as the getConceptMapTypeByName method was altered to perform a case-insensitive match, which addresses the described requirement. The original query used an exact case-sensitive equality match (`=`), and the update modifies it to meet the new case insensitivity requirement.

Given that the code is now designed to fix this issue of case sensitivity as described in the commit message, it can be considered a logical correction addressing a bug (namely, the insensitivity of name matching).

Thus, the nature of the problem and its resolution through this commit justifies that the change indicates a bug fix. Therefore, the conclusion is **Buggy**."
openmrs-core,3457.json,225d8cd669d2e31ba80b0f39bd381b1ebc8c11d7,"@@ -1,13 +1,9 @@
 	public void validate(Object obj, Errors errors) {
-		if (!ValidateUtil.isValidationOn()) {
-			return;
-		}
-		
 		PatientIdentifier pi = (PatientIdentifier) obj;
 		try {
 			validateIdentifier(pi);
 		}
 		catch (Exception e) {
 			errors.reject(e.getMessage());
 		}
 	}",NotBuggy,"Revert ""Provide configuration setting that can allow turning off validation in""

This reverts commit 0d0ace1911fd780b7ae8dfb2bec3fc4cb4e279bc.
",Buggy,"The commit message states a reversion of a previous change. The diff removes a validation check that could be turned off using a configuration setting. This rollback indicates that the earlier change, which allowed skipping validation, might have led to errors or undesirable behavior by bypassing necessary validation checks. 

Reverting to include consistent validation implies fixing potential issues introduced by skipping that validation. However, this specific revert doesn't fit the typical description of a direct bug fix; it's more about restoring previous behavior that includes validation. If the commit being reverted introduced a bug by allowing validation to be skipped, then this revert would indeed rectify that mistake.

There isn't a direct fix in the traditional sense (like correcting a faulty logic or exception handling). However, since reverts often seek to address unintended negative effects from previous changes, this action can imply a bug-related solution.

Given this context, we conclude with **Buggy** because reverting restores necessary validation, likely addressing issues that arose from it being turned off."
openmrs-core,7636.json,2fefe9576c8a5fb60fa92e1ad36f9d6aeafe3333,"@@ -1,42 +1,52 @@
 	private void migrateFrequenciesToCodedValue(JdbcConnection connection, Set<String> uniqueFrequencies)
 	        throws CustomChangeException, SQLException, DatabaseException {
 		PreparedStatement updateDrugOrderStatement = null;
 		Boolean autoCommit = null;
 		try {
 			autoCommit = connection.getAutoCommit();
 			connection.setAutoCommit(false);
 			updateDrugOrderStatement = connection
 			        .prepareStatement(""update drug_order set frequency = ? where frequency_text = ?"");
+			
+			updateDrugOrderStatement.setNull(1, Types.INTEGER);
+			updateDrugOrderStatement.setNull(2, Types.VARCHAR);
+			updateDrugOrderStatement.executeUpdate();
+			updateDrugOrderStatement.clearParameters();
+			
 			for (String frequency : uniqueFrequencies) {
-				Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
-				if (conceptIdForFrequency == null) {
-					throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+				if (StringUtils.isBlank(frequency)) {
+					updateDrugOrderStatement.setNull(1, Types.INTEGER);
+				} else {
+					Integer conceptIdForFrequency = UpgradeUtil.getConceptIdForUnits(frequency);
+					if (conceptIdForFrequency == null) {
+						throw new CustomChangeException(""No concept mapping found for frequency: "" + frequency);
+					}
+					Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection
+					        .getUnderlyingConnection(), conceptIdForFrequency);
+					if (orderFrequencyId == null) {
+						throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
+					}
+					
+					updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				}
-				Integer orderFrequencyId = UpgradeUtil.getOrderFrequencyIdForConceptId(connection.getUnderlyingConnection(),
-				    conceptIdForFrequency);
-				if (orderFrequencyId == null) {
-					throw new CustomChangeException(""No order frequency found for concept "" + conceptIdForFrequency);
-				}
-				
-				updateDrugOrderStatement.setInt(1, orderFrequencyId);
 				updateDrugOrderStatement.setString(2, frequency);
 				updateDrugOrderStatement.executeUpdate();
 				updateDrugOrderStatement.clearParameters();
 			}
 			connection.commit();
 		}
 		catch (DatabaseException e) {
 			handleError(connection, e);
 		}
 		catch (SQLException e) {
 			handleError(connection, e);
 		}
 		finally {
 			if (autoCommit != null) {
 				connection.setAutoCommit(autoCommit);
 			}
 			if (updateDrugOrderStatement != null) {
 				updateDrugOrderStatement.close();
 			}
 		}
 	}",NotBuggy,"TRUNK-4403 Blank drug order frequencies and dosing units should be converted to null during upgrade
",Buggy,"The commit message indicates a fix to handle blank drug order frequencies by converting them to null. The code change reflects this intention by checking if the `frequency` is blank using `StringUtils.isBlank(frequency)`. If it is blank, it sets the frequency to null in the database. This change aligns with the commit message and prevents potential issues with incorrect frequency handling during upgrades.

This update specifically targets handling blank frequencies as null, which can be seen as a logical correction to ensure data integrity during the migration process. Therefore, it addresses the concern mentioned in the commit message and corrects a situation that could be considered problematic.

Thus, the conclusion is **Buggy**, as this aligns with addressing a bug where blank frequencies might not have been correctly managed prior to this update."
openmrs-core,339.json,b54304f90aaa12e7ffc6b51b808d95e500b3fce4,"@@ -1,5 +1,6 @@
 	public Set<PersonAddress> getAddresses() {
-		if (addresses == null)
+		if (addresses == null) {
 			addresses = new TreeSet<PersonAddress>();
+		}
 		return this.addresses;
 	}",NotBuggy,"TRUNK-4246: if/else/for/while/do statements should always use curly braces
",NotBuggy,"The commit message indicates a stylistic change to enforce the use of curly braces in if/else/for/while/do statements. This change adds curly braces to the if statement:

```java
if (addresses == null) {
    addresses = new TreeSet<PersonAddress>();
}
```

This change aims to improve code readability and maintainability by preventing errors that can arise from adding statements to a control structure without braces. However, it does not fix an existing bug; it only addresses coding style according to the specified guideline TRUNK-4246.

Thus, this change aligns with the commit message but is unrelated to a functional bug fix in the code and only pertains to code style practices.

The final conclusion is **NotBuggy**."
openmrs-core,6956.json,823d9d0ddb38979543da23aa9e4ed2cb54bca064,"@@ -1,15 +1,3 @@
 	public Field saveField(Field field) throws APIException {
-		Date now = new Date();
-		
-		if (field.getCreator() == null)
-			field.setCreator(Context.getAuthenticatedUser());
-		if (field.getDateCreated() == null)
-			field.setDateCreated(now);
-		
-		if (field.getFieldId() != null) {
-			field.setChangedBy(Context.getAuthenticatedUser());
-			field.setDateChanged(now);
-		}
-		
 		return dao.saveField(field);
 	}",NotBuggy,"Merged uuids branch to trunk

git-svn-id: http://svn.openmrs.org/openmrs/trunk@7606 5bac5841-c719-aa4e-b3fe-cce5062f897a
",NotBuggy,"The commit message indicates a merge of a branch into the trunk, which typically involves integrating features or changes from one branch into the main codebase. The code diff shows a simplification of the `saveField` method by removing checks and operations related to setting creator, creation date, and modification details (such as changed by and date changed).

Here's the analysis:

1. **Removed Functionality**: 
   - The code that sets the creator and date created if they are null has been removed.
   - Similarly, the code that updates the `changedBy` and `dateChanged` fields if a `fieldId` exists has been removed.

2. **Potential Impact**:
   - This could affect how new fields are created and how changes to existing fields are tracked. Previously, if a field was newly created or modified, these fields ensured that audit information (who created/modified it and when) was stored. Removing these lines means this information might no longer be captured.

3. **Relation to Merge**:
   - A merge typically aggregates changes from another branch. The nature of the change hints that these modifications were possibly made for other reasons, potentially involving improvements or refactoring done on the `uuids` branch.

4. **Assessment for Bug Fix**:
   - While audit information is relevant for tracking changes, the removal itself does not indicate a bug fix. Typically, bug fixes correct errors or issues, whereas this change appears to simplify or refactor code without directly addressing a defect or error.

Based on this analysis, there's no indication from the diff or the commit message that these changes were made to fix a bug. Therefore, I conclude with **NotBuggy**."
openmrs-core,3668.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
-	public boolean isDirty() {
+	public boolean getDirty() {
 		return dirty;
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message discusses deprecating `isXyz()` methods when corresponding `getXyz()` methods exist. The code change renames the method `isDirty()` to `getDirty()`, aligning with the outlined purpose of replacing deprecated methods with expected non-deprecated ones. However, this change does not indicate a bug fix. It addresses code maintenance and consistency but not a functionality, logic, or error-handling issue that could classify it as fixing a bug. The change aligns with the intention mentioned in the commit message, which focuses on deprecation rather than resolving bugs.

Therefore, the conclusion is **NotBuggy**."
openmrs-core,3687.json,2da55993f40c43555f00ca4660242241bf285bc6,"@@ -1,3 +1,3 @@
 	public boolean isHidden() {
-		return isHidden;
+		return getIsHidden();
 	}",NotBuggy,"TRUNK-2205: deprecated isXyz() getter methods having correspoding getXyz()

changed deprecated methods to call expected methods
",NotBuggy,"The commit message indicates a change related to deprecating specific getter methods that have corresponding alternatives. The diff shows that the deprecated method `isHidden()` now calls `getIsHidden()` instead of directly returning the variable `isHidden`. This suggests an internal refactoring to replace deprecated method content with the recommended alternative.

This change aligns with the commit message but does not fix a bug. Instead, it is a maintenance task to direct deprecated functionality to up-to-date methods, likely for consistency or future-proofing. No error handling, logical corrections, or exception handling improvements are evident.

For these reasons, the conclusion is **NotBuggy**."
openmrs-core,5512.json,95075392b5c295d7ca5e94244ea5338dcaae7ae3,"@@ -1,9 +1,11 @@
 	public boolean isSecretAnswer(User u, String answer) throws DAOException {
 		
 		if (answer == null || answer.equals("""")) {
 			return false;
 		}
 		
-		String answerOnRecord = getLoginCredential(u).getSecretAnswer();
-		return (answer.equals(answerOnRecord));
+		LoginCredential credentials = getLoginCredential(u);
+		String answerOnRecord = credentials.getSecretAnswer();
+		String hashedAnswer = Security.encodeString(answer.toLowerCase() + credentials.getSalt());
+		return (hashedAnswer.equals(answerOnRecord));
 	}",NotBuggy,"TRUNK-3709 - Secret answer should be encrypted
",Buggy,"The commit message indicates an enhancement of security by encrypting the secret answer. The code change reflects this by switching from comparing plaintext answers to comparing a hashed form of the answer. By using `Security.encodeString` and incorporating a salt from `credentials`, the code enhances the security of the secret answer verification process, aligning with the commit message.

This change modifies how secret answers are stored and verified, which directly relates to addressing a potential bug or security vulnerability in the way user answers were traditionally handled—by switching to encrypted answers, the system becomes more secure against possible exposure and misuse of sensitive information.

Thus, the changes indeed pertain to fixing a security-related issue with secret answer handling, accurately reflecting the commit message.

The analysis concludes with **Buggy** since it indicates a bug/security fix."
openmrs-core,7457.json,da12f21c545d8c4994dccbd2d08bc4c962811d9d,"@@ -1,16 +1,16 @@
 	public static byte[] generateNewSecretKey() {
 		// Get the KeyGenerator
 		KeyGenerator kgen = null;
 		try {
 			kgen = KeyGenerator.getInstance(OpenmrsConstants.ENCRYPTION_KEY_SPEC);
 		}
 		catch (NoSuchAlgorithmException e) {
-			throw new APIException(""Could not generate cipher key"", e);
+			throw new APIException(""could.not.generate.cipher.key"", null, e);
 		}
 		kgen.init(128); // 192 and 256 bits may not be available
 		
 		// Generate the secret key specs.
 		SecretKey skey = kgen.generateKey();
 		
 		return skey.getEncoded();
 	}",NotBuggy,"TRUNK-4500: Use of APIException not localized (mostly)
",Buggy,"The commit message indicates a change related to localization of `APIException` messages. The code diff shows that an `APIException` message string is changed from a direct string to what is likely a message key (""could.not.generate.cipher.key"") intended for localization. The additional `null` parameter following the message key suggests support for message arguments, which is a common pattern for localized messages.

This modification aligns with the commit message and addresses an issue by making exception messages suitable for localization. Though it's not fixing a bug in the traditional sense (e.g., a logical error or exception handling error), it addresses a defect in internationalization, which can be considered a bug related to user experience or application localization.

Therefore, considering the context and the connection between the commit message and the code change, the changes align with addressing a bug-like behavior from an internationalization perspective.

The appropriate conclusion in this context is **Buggy**."
openmrs-core,6524.json,e374a4c37825bbe530cdf9a449906f6d8a0c18a9,"@@ -1,3 +1,3 @@
 	public Integer getCountOfEncounters(String query, boolean includeVoided) {
-		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, includeVoided));
+		return OpenmrsUtil.convertToInteger(dao.getCountOfEncounters(query, null, includeVoided));
 	}",NotBuggy,"Encounter autocomplete should filter results by patient - TRUNK-3231

Encounter autocomplete should filter results by patient - TRUNK-3231
",NotBuggy,"The commit message suggests that the change should filter autocomplete results by patient, but the provided diff does not show any filtering logic related to patients added. Instead, it adds an extra `null` argument to a method call. Without additional context indicating that this change is tied to filtering by patient, it seems like a placeholder or an oversight rather than an actual fix for the stated issue. Thus, based on the given diff, the answer is **NotBuggy**."
openmrs-core,6208.json,48386f10175bf557801706f76c79e1413067673b,"@@ -1,15 +1,15 @@
 	public List<Obs> getObservations(List<Person> whom, List<Encounter> encounters, List<Concept> questions,
 	                                 List<Concept> answers, List<PERSON_TYPE> personTypes, List<Location> locations,
 	                                 List<String> sort, Integer mostRecentN, Integer obsGroupId, Date fromDate, Date toDate,
 	                                 boolean includeVoidedObs, String accessionNumber) throws APIException {
 		
 		if (sort == null) {
-			sort = new Vector<>();
+			sort = new ArrayList<>();
 		}
 		if (sort.isEmpty()) {
 			sort.add(""obsDatetime"");
 		}
 		
 		return dao.getObservations(whom, encounters, questions, answers, personTypes, locations, sort, mostRecentN,
 		    obsGroupId, fromDate, toDate, includeVoidedObs, accessionNumber);
 	}",NotBuggy,"TRUNK-5245 Replace Vector collections with ArrayList (#2445)

",NotBuggy,"The commit message indicates the replacement of Vector with ArrayList. The diff reflects this change where a Vector is replaced with an ArrayList for the `sort` list initialization. This change is primarily for performance optimization and modernizing the code, as ArrayList is generally preferred over Vector due to non-synchronization and better performance. No bug fix is directly implied by this change, and the commit message does not indicate a bug but merely an update to use a different collection type.

The diff aligns perfectly with the commit message, which focuses on replacing a data structure for performance and clarity. There is no mention of correcting a flaw or an error in functionality, only an update for better performance or compliance with current standards.

Therefore, the conclusion for this commit should be **NotBuggy**."
hadoop,65931.json,a36274d69947648dbe82721220cc5240ec5d396d,"@@ -1,7 +1,8 @@
   public void prettyPrint(StringBuilder sb) {
-    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s"",
+    sb.append(String.format(""%-5s %-20s %-7d %-8s %-6s %-20s %-20s"",
         fileStatus.isDirectory() ? ""dir"" : ""file"",
         fileStatus.getPath().toString(), fileStatus.getLen(),
-        isEmptyDirectory.name(), isDeleted));
+        isEmptyDirectory.name(), isDeleted,
+        fileStatus.getETag(), fileStatus.getVersionId()));
     sb.append(fileStatus);
   }",NotBuggy,"HADOOP-16085. S3Guard: use object version or etags to protect against inconsistent read after replace/overwrite.

Contributed by Ben Roling.

S3Guard will now track the etag of uploaded files and, if an S3
bucket is versioned, the object version.

You can then control how to react to a mismatch between the data
in the DynamoDB table and that in the store: warn, fail, or, when
using versions, return the original value.

This adds two new columns to the table: etag and version.
This is transparent to older S3A clients -but when such clients
add/update data to the S3Guard table, they will not add these values.
As a result, the etag/version checks will not work with files uploaded by older clients.

For a consistent experience, upgrade all clients to use the latest hadoop version.
",NotBuggy,"The commit message describes enhancements to the S3Guard that involve tracking the ETag and version of S3 objects to improve consistency. This diff updates the `prettyPrint` method to include the ETag and version information in the output. This change corresponds with the commit message details about adding these fields for better consistency management.

While the commit includes a larger context of adding new functionality (tracking ETags and version IDs), this specific diff updates string formatting for pretty printing and doesn't directly illustrate a bug fix. Instead, it reflects an enhancement rather than a correction of any previous incorrect behavior, so we can't attribute this change specifically to fixing a bug as described in the commit message.

Overall, even though the larger commit supports improved consistency handling and could prevent errors in certain circumstances, this specific code change is more related to displaying additional information. Therefore, this part of the change can be considered **NotBuggy**."
hadoop,4963.json,1096917649fd951be633e5619518764f23cca645,"@@ -1,16 +1,16 @@
-  public static FileDiffList loadFileDiffList(DataInputStream in,
+  public static FileDiffList loadFileDiffList(DataInput in,
       FSImageFormat.Loader loader) throws IOException {
     final int size = in.readInt();
     if (size == -1) {
       return null;
     } else {
       final FileDiffList diffs = new FileDiffList();
       FileDiff posterior = null;
       for(int i = 0; i < size; i++) {
         final FileDiff d = loadFileDiff(posterior, in, loader);
         diffs.addFirst(d);
         posterior = d;
       }
       return diffs;
     }
   }",NotBuggy,"HDFS-4611. Update FSImage for INodeReference.


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1463332 13f79535-47bb-0310-9956-ffa450edef68
",NotBuggy,"The commit message indicates an update to FSImage for INodeReference, suggesting changes related to the file system image handling. The code modification involves changing a parameter type from `DataInputStream` to `DataInput`. This update likely increases flexibility by accepting any `DataInput` rather than specifically a `DataInputStream`, but it doesn't inherently indicate a bug fix; it seems like a general improvement or refactoring for better abstraction and flexibility.

No explicit bug is addressed or fixed with these changes—the focus is more on general code improvement rather than correcting erroneous behavior. The method already performs its function correctly but is now more adaptable to different sources of input data.

Thus, the changes do not indicate a bug fix. The final assessment of this commit is **NotBuggy**."
hadoop,44452.json,89e5c44f9e891a3579384c3fa3766937cd4970f1,"@@ -1,28 +1,27 @@
   void startTimelineClient(final Configuration conf)
       throws YarnException, IOException, InterruptedException {
     try {
       appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
         @Override
         public Void run() throws Exception {
-          if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,
-              YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {
+          if (YarnConfiguration.timelineServiceEnabled(conf)) {
             // Creating the Timeline Client
-            if (newTimelineService) {
+            if (timelineServiceV2) {
               timelineClient = TimelineClient.createTimelineClient(
                   appAttemptID.getApplicationId());
             } else {
               timelineClient = TimelineClient.createTimelineClient();
             }
             timelineClient.init(conf);
             timelineClient.start();
           } else {
             timelineClient = null;
             LOG.warn(""Timeline service is not enabled"");
           }
           return null;
         }
       });
     } catch (UndeclaredThrowableException e) {
       throw new YarnException(e.getCause());
     }
   }",NotBuggy,"YARN-4356. Ensure the timeline service v.2 is disabled cleanly and has no
impact when it's turned off. Contributed by Sangjin Lee.
",NotBuggy,"The commit message states the changes ensure the timeline service v.2 can be disabled cleanly with no impact when turned off. Upon analyzing the provided diff, we notice the following changes:

1. **Condition Change for Service Enablement**:
   - The original code directly checks the configuration setting using `conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)`.
   - The updated code encapsulates this logic into a method call: `YarnConfiguration.timelineServiceEnabled(conf)`. This may centralize or alter the logic, making it easier to maintain, though the precise implementation of `timelineServiceEnabled` is not shown here.

2. **Selection Based on Service Version**:
   - The previous code checked the variable `newTimelineService` to decide on which version of the TimelineClient to instantiate.
   - The updated code checks `timelineServiceV2`, which suggests a more explicit variable pertinent to the version being referred to.

These modifications align with the commit message's goal to cleanly manage the enablement and version selection of the timeline service. The changes do not directly indicate a bug fix but instead suggest a refactor for better feature toggling and version handling. The behavior when the service is disabled (setting `timelineClient` to `null`) remains consistent, supporting the claim about no adverse impact when the service is off.

Therefore, considering the provided details and the nature of enhancements rather than corrections to faulty logic, the assessment is **NotBuggy**."
hadoop,81927.json,75dd866bfb8b63cb9f13179d4365b05c48e0907d,"@@ -1,11 +1,12 @@
-  private void createRootDirRecursively(String path) throws Exception {
-    String pathParts[] = path.split(""/"");
-    Preconditions.checkArgument(pathParts.length >= 1 && pathParts[0].isEmpty(),
+  public void createRootDirRecursively(String path) throws Exception {
+    String[] pathParts = path.split(""/"");
+    Preconditions.checkArgument(
+        pathParts.length >= 1 && pathParts[0].isEmpty(),
         ""Invalid path: %s"", path);
     StringBuilder sb = new StringBuilder();
 
     for (int i = 1; i < pathParts.length; i++) {
       sb.append(""/"").append(pathParts[i]);
       create(sb.toString());
     }
   }",NotBuggy,"HADOOP-14773. Extend ZKCuratorManager API for more reusability. (Íñigo Goiri via Subru).
",NotBuggy,"The commit message mentions extending the `ZKCuratorManager API` for reusability. The diff changes the method `createRootDirRecursively` from private to public, which indeed makes the method reusable outside of its current class. There are no logical or functional bug fixes in this diff, only a visibility change to increase reuse. The change to the checkArgument line is merely a stylistic modification for readability and does not imply a bug fix.

Therefore, no bug is being fixed in this commit; it aligns with making the API more reusable as per the commit message. The correct assessment of this commit is **NotBuggy**."
hadoop,79614.json,5e7cfdca7b73a88bf3c3f1e5eb794a24218cce52,"@@ -1,4 +1,4 @@
-  public FSDataOutputStreamBuilder setReplication(short replica) {
+  public B replication(short replica) {
     replication = replica;
-    return this;
+    return getThisBuilder();
   }",NotBuggy,"HADOOP-14394. Provide Builder pattern for DistributedFileSystem.create. (lei)
",NotBuggy,"The commit message focuses on introducing the Builder pattern for `DistributedFileSystem.create`. The diff shows a method name change and a shift to returning `getThisBuilder()` instead of `this`. These changes align with implementing a Builder pattern, which typically involves returning an instance of the builder class rather than the method itself. 

Such changes are usual in Builder pattern implementations to allow method chaining and fluid interfaces. The modification of method names and return types here does not indicate fixing a bug; it represents an enhancement or refactoring to improve API design.

Therefore, the changes do not pertain to a bug fix; they align more closely with infrastructure improvements for code readability and usability.

Thus, the result is **NotBuggy**."
hadoop,6690.json,69e5f90e9febf37d2cdd69c485729c448ac3cabc,"@@ -1,6 +1,8 @@
     public String toString() {
       StringBuilder builder = new StringBuilder();
       builder.append(""RemoveCachePoolOp ["");
-      builder.append(""poolName="" + poolName + ""]"");
+      builder.append(""poolName="" + poolName);
+      appendRpcIdsToString(builder, rpcClientId, rpcCallId);
+      builder.append(""]"");
       return builder.toString();
     }",NotBuggy,"HDFS-5385. Caching RPCs are AtMostOnce, but do not persist client ID and call ID to edit log.  (Chris Nauroth via Colin Patrick McCabe)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534345 13f79535-47bb-0310-9956-ffa450edef68
",Buggy,"The commit message mentions addressing an issue with caching RPCs and their IDs, which wasn't persistent previously. The diff modifies the `toString` method to include `rpcClientId` and `rpcCallId` in the string representation via the `appendRpcIdsToString` method. While this doesn't directly modify persistence logic, it is consistent with a change related to logging or debugging RPC identifiers. This adjustment likely aids in debugging or monitoring, aligning with the described problem of identifying RPC calls.

Given that it's part of addressing the issue where the RPC identifiers aren't persistent, an improved string representation might be a step toward resolving the broader problem, though it doesn't appear to be the entire fix.

Since the change does contribute to handling RPC identifiers better, possibly aiding in debugging or understanding persistence-related issues, the answer is **Buggy**, as it still indicates a bug-related change addressing the problem mentioned in the commit message."
hadoop,14378.json,8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34,"@@ -1,3 +1,4 @@
-  public Long getJournalCTime(String journalId) throws IOException {
-    return jn.getJournalCTime(journalId);
+  public Long getJournalCTime(String journalId,
+                              String nameServiceId) throws IOException {
+    return jn.getJournalCTime(journalId, nameServiceId);
   }",NotBuggy,"HDFS-12553. Add nameServiceId to QJournalProtocol. Contributed by Bharat Viswanadham
",NotBuggy,"The commit message indicates a change to the `QJournalProtocol` by adding a `nameServiceId` parameter to the `getJournalCTime` method. The code modifies the method signature to include `nameServiceId`, which corresponds precisely with the commit message.

Assessing the nature of this change:

1. **Addition of Parameters**: The method now requires `nameServiceId` in addition to `journalId`. This implies an expansion of the API surface, typically for additional context or functionality rather than a direct bug fix.

2. **Functionality Change**: This change suggests functionality likely requiring `nameServiceId`, perhaps to differentiate between multiple services or configurations. It doesn't inherently suggest a bug but rather an enhancement or new feature capability.

3. **Behavior Impact**: The addition ensures `nameServiceId` is used wherever the method is called, changing the external behavior but not necessarily correcting an internal bug.

Since the change aligns with an API extension for new features or configurations rather than directly addressing errors or exceptions, it indicates an enhancement rather than a fix for any specific bug.

The conclusion for this commit is **NotBuggy**."
hadoop,34508.json,ab2bda57bd9ad617342586d5769121a4fef4eab1,"@@ -1,8 +1,9 @@
     public void transition(ContainerImpl container, ContainerEvent event) {
+      container.setIsPaused(false);
       // Pause the process/process-grp if it is supported by the container
       container.dispatcher.getEventHandler().handle(
           new ContainersLauncherEvent(container,
               ContainersLauncherEventType.RESUME_CONTAINER));
       ContainerResumeEvent resumeEvent = (ContainerResumeEvent) event;
       container.addDiagnostics(resumeEvent.getDiagnostic() + ""\n"");
     }",NotBuggy,"YARN-9428. Add metrics for paused containers in NodeManager. Contributed by Abhishek Modi.
",NotBuggy,"The commit message indicates the addition of metrics for paused containers, likely implying a feature enhancement, not a bug fix. The code change itself entails setting a container's state to ""not paused"" during the transition associated with processing a resume event. This change alters the state of the container to reflect that it is no longer paused.

- **Assess the relevance of change to message:**
  - The change does not explicitly relate to adding metrics. Instead, it alters the state behavior of a container when transitioning to resume operation.

- **Evaluate for indications of bug fixes:**
  - Making sure the container is not marked paused when receiving a resume event could rectify improper state management, but this is not directly referenced as a bug fix in the message. Instead, it looks like a supportive change for correct feature implementation.

Given that the commit message focuses on adding metrics and the code change seems more aligned with ensuring operations correctly reflect a container's state change, the changes do not directly indicate a bug fix suggested by the message.

Therefore, based on the given information, the conclusion is **NotBuggy**."
hadoop,64072.json,dab00da19f25619ccc71c7f803a235b21766bf1e,"@@ -1,12 +1,5 @@
-      public void close() throws IOException {
-        try {
-          super.close();
-        } finally {
-          if (!closed.getAndSet(true)) {
-            if (!bufferFile.delete()) {
-              LOG.warn(""delete({}) returned false"",
-                  bufferFile.getAbsoluteFile());
-            }
-          }
-        }
-      }

+        public synchronized void close() {
+          LOG.debug(""ByteBufferInputStream.close() for {}"",
+              ByteBufferBlock.super.toString());
+          byteBuffer = null;
+        }",NotBuggy,"HADOOP-14028. S3A BlockOutputStreams doesn't delete temporary files in multipart uploads or handle part upload failures.
Contributed by Steve Loughran.

(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)
",NotBuggy,"The commit message references issues in S3A BlockOutputStreams around temporary file deletion and upload failures, suggesting a focus on resource management and exception handling.

### Analysis of the Code Diff:

1. **Original Code:**
   - The `close` method attempted to ensure that a `bufferFile` was deleted. 
   - It marked a resource as closed (`closed.getAndSet(true)`) and logged a warning if the file deletion failed.

2. **Modified Code:**
   - The redefined `close` method does not check or attempt to delete any files.
   - Instead, it simply logs a debug message and sets a `byteBuffer` reference to `null`.

### Reasoning:

- **Deletion and Resource Management:** The original code handled deletion of a file to free disk resources. Removing this logic could mean cleanup is managed elsewhere or no longer necessary for this part of the code. However, if no alternative mechanism is provided, it could lead to orphaned files.
  
- **Locking and Synchronization:** The modified method is synchronized. Though not explicitly mentioned in the commit message, synchronization often implies an effort to avoid concurrency issues—a common consideration in handling resources like file streams.
  
- **Logging:** There is additional logging, which might support better debug and tracking capabilities but is not directly related to fixing a bug about failures or deletions.

### Conclusion:

While the commit message discusses bug fixes related to temporary file deletion in multipart uploads, the specific removal of file deletion logic from the `close` method in this diff might mean the responsibility has shifted elsewhere or is part of a larger change not fully visible in this snippet. Therefore, by itself, the change does not directly resolve a bug as described in the commit message.

Thus, without confirmation of broader context indicating the problem is addressed elsewhere, this code change can be assessed as **NotBuggy**."
hadoop,79999.json,2b452b4e6063072b2bec491edd3f412eb7ac21f3,"@@ -1,16 +1,13 @@
   void init() {
     if (used.get() < 0) {
       used.set(0);
+      if (!shouldFirstRefresh) {
+        // Skip initial refresh operation, so we need to do first refresh
+        // operation immediately in refresh thread.
+        initRefeshThread(true);
+        return;
+      }
       refresh();
     }
-
-    if (refreshInterval > 0) {
-      refreshUsed = new Thread(new RefreshThread(this),
-          ""refreshUsed-"" + dirPath);
-      refreshUsed.setDaemon(true);
-      refreshUsed.start();
-    } else {
-      running.set(false);
-      refreshUsed = null;
-    }
+    initRefeshThread(false);
   }",NotBuggy,"HDFS-14986. ReplicaCachingGetSpaceUsed throws ConcurrentModificationException. Contributed by Aiphago.
",Buggy,"The commit message states a fix for a ConcurrentModificationException in ReplicaCachingGetSpaceUsed. The code diff shows a reorganization of the `init` method, simplifying and centralizing the thread initialization logic into an `initRefeshThread` method. This approach likely addresses concurrent modification issues by ensuring that thread operations are managed in a controlled manner. The changes appear to streamline the process of determining when and how the refresh threads are initialized, which is crucial for avoiding concurrent access issues.

Here's a step-by-step breakdown of how these changes align with the commit message regarding a bug fix:

1. **Refactoring and Simplification**: The changes remove conditional branching for starting the refresh thread based on the `refreshInterval`. Instead, the refactoring unifies the thread initialization through `initRefeshThread`, potentially reducing opportunities for concurrent modification by creating a consistent mechanism for thread startup.

2. **Immediate Refresh with First-Time Skip Logic**: The introduction of a conditional to immediately start the refresh operation if `shouldFirstRefresh` is false indicates a controlled scenario when to skip or delay refresh. This may address situations where premature concurrent operations led to the original exception.

3. **Addressing Concurrency Concerns**: By moving thread handling to a designated method, the refactor likely simplifies synchronization or infection points where concurrent modifications can happen, directly linked to the mentioned exception.

4. **Direct Commit Message Relation**: The commit message explicitly mentions fixing a `ConcurrentModificationException`, which strongly suggests the changes are motivated by improving thread safety and coherence in concurrent operations, directly correlating to the bug.

Thus, based on the code changes and the context from the commit message, the changes do indeed aim to resolve a bug related to concurrency. Therefore, the analysis concludes with **Buggy**."
hadoop,9493.json,1594b472bb9df7537dbc001411c99058cc11ba41,"@@ -1,3 +1,3 @@
-  public float getVolumeDataDensity() {
+  public double getVolumeDataDensity() {
     return volumeDataDensity;
   }",NotBuggy,"HDFS-9543. DiskBalancer: Add Data mover. Contributed by Anu Engineer.
",NotBuggy,"The commit message indicates the addition of a Data Mover feature to the DiskBalancer component, contributed by Anu Engineer. However, the code change shown in the diff is merely a change in the return type of a method from `float` to `double`, which is likely intended to provide more precision for the 'volumeDataDensity' value.

There is no indication that this specific code change is directly resolving a bug. Instead, it enhances the precision of the return value, possibly as part of a larger feature addition or refactoring which might support the ""Data mover"" feature mentioned in the commit message. Nevertheless, this diff alone doesn't suggest a direct bug fix, such as error-handling updates, logical corrections, or exception-handling improvements.

Therefore, the answer is **NotBuggy**."
hadoop,37365.json,9875325d5c63f343809907d06bf48a298035a611,"@@ -1,28 +1,28 @@
   public static ReservationAllocationStateProto buildStateProto(
       ReservationAllocation allocation) {
     ReservationAllocationStateProto.Builder builder =
         ReservationAllocationStateProto.newBuilder();
 
-    builder.setAcceptanceTimestamp(allocation.getAcceptanceTime());
+    builder.setAcceptanceTime(allocation.getAcceptanceTime());
     builder.setContainsGangs(allocation.containsGangs());
     builder.setStartTime(allocation.getStartTime());
     builder.setEndTime(allocation.getEndTime());
     builder.setUser(allocation.getUser());
     ReservationDefinitionProto definitionProto = convertToProtoFormat(
         allocation.getReservationDefinition());
     builder.setReservationDefinition(definitionProto);
 
     for (Map.Entry<ReservationInterval, Resource> entry :
         allocation.getAllocationRequests().entrySet()) {
       ResourceAllocationRequestProto p =
           ResourceAllocationRequestProto.newBuilder()
           .setStartTime(entry.getKey().getStartTime())
           .setEndTime(entry.getKey().getEndTime())
           .setResource(convertToProtoFormat(entry.getValue()))
           .build();
       builder.addAllocationRequests(p);
     }
 
     ReservationAllocationStateProto allocationProto = builder.build();
     return allocationProto;
   }",NotBuggy,"YARN-4340. Add list API to reservation system. (Sean Po via wangda)
",NotBuggy,"The commit message describes adding a list API to the reservation system, while the code change modifies a method involved in building a proto object from a reservation allocation. Specifically, the code change updates a method name from `setAcceptanceTimestamp` to `setAcceptanceTime`.

This change does not seem related to adding a new list API. Instead, it looks like a correction or alignment with a changed or corrected method name in a related class or proto definition. This change could be related to code maintenance or a small error correction but doesn't indicate the introduction of a new API as described in the commit message.

There is no indication of a bug fix related to the logic or functionality of the reservation allocation, just a possible method signature adjustment. Therefore, the code change in this diff does not seem to align with the message about adding a new API, nor does it indicate a bug fix.

Based on these observations, the opinion is **NotBuggy**."
hadoop,10353.json,d1d4e16690cc85f7f22fbead9cf596260819b561,"@@ -1,3 +1,3 @@
   boolean sendImmediately() {
-    return readyToSend;
+    return readyToSend && monotonicNow() - ibrInterval >= lastIBR;
   }",NotBuggy,"HDFS-9710. DN can be configured to send block receipt IBRs in batches.
",NotBuggy,"The change modifies the condition for sending block receipt IBRs. Earlier, it relied solely on the `readyToSend` flag, but now it also checks if a certain time interval (`ibrInterval`) has passed since the last IBR was sent. This reflects a logical correction ensuring that IBRs are sent in batches instead of immediately, aligning with the commit message's intention to batch send IBRs.

Although this change aligns with new functionality to batch send IBRs, it primarily introduces new logic rather than fixing an error in existing functionality. The modification seems more of an enhancement or feature addition, consistent with the commit message, but not directly resolving a bug.

For features or enhancements, the conclusion is **NotBuggy**."
hadoop,60643.json,a81144daa012e13590725f67f53e35ef84a6f1ec,"@@ -1,26 +1,23 @@
-          public Iterator<ApplicationTimeoutMapProto> iterator() {
-            return new Iterator<ApplicationTimeoutMapProto>() {
-              private Iterator<ApplicationTimeoutType> iterator =
-                  applicationTimeouts.keySet().iterator();
+      public Iterator<StringStringMapProto> iterator() {
+        return new Iterator<StringStringMapProto>() {
+          private Iterator<String> iterator = schedulingProperties.keySet()
+              .iterator();
 
-              @Override
-              public boolean hasNext() {
-                return iterator.hasNext();
-              }
+          @Override
+          public boolean hasNext() {
+            return iterator.hasNext();
+          }
 
-              @Override
-              public ApplicationTimeoutMapProto next() {
-                ApplicationTimeoutType key = iterator.next();
-                return ApplicationTimeoutMapProto.newBuilder()
-                    .setTimeout(applicationTimeouts.get(key))
-                    .setApplicationTimeoutType(
-                        ProtoUtils.convertToProtoFormat(key))
-                    .build();
-              }
+          @Override
+          public StringStringMapProto next() {
+            String key = iterator.next();
+            return StringStringMapProto.newBuilder()
+                .setValue(schedulingProperties.get(key)).setKey(key).build();
+          }
 
-              @Override
-              public void remove() {
-                throw new UnsupportedOperationException();
-              }
-            };
-          }

+          @Override
+          public void remove() {
+            throw new UnsupportedOperationException();
+          }
+        };
+      }",NotBuggy,"YARN-7666. Introduce scheduler specific environment variable support in ApplicationSubmissionContext for better scheduling placement configurations. (Sunil G via wangda)

Change-Id: I0fd826490f5160d47d42af2a9ac0bd8ec4e959dc
",NotBuggy,"The commit message indicates an enhancement to support scheduler-specific environment variables in `ApplicationSubmissionContext`, while the code change modifies an iterator class to work with a different set of properties called `schedulingProperties` instead of `applicationTimeouts`.

Key observations are:

1. **Purpose of Change**: The original `iterator()` method focused on iterating over `applicationTimeouts`, whereas the updated version targets `schedulingProperties`. The change aligns with the commit message suggesting that the new feature involves handling scheduling configurations.

2. **Iterator Logic**: The override methods—`hasNext()`, `next()`, and `remove()`—remain consistent in their structure. The fundamental change is in what they're iterating over and hence manipulating: changing from `ApplicationTimeoutMapProto` to `StringStringMapProto`.

3. **Constructing Protos**: In the `next()` method, the data types have shifted from `ApplicationTimeoutMapProto` to `StringStringMapProto`, indicating a transition towards handling string-based key-value pairs, which fits the notion of scheduler-specific configurations more closely aligned with environmental variables.

4. **Absence of Bug Fix Indicators**: There is no indication of error handling or exception improvements, logical corrections, or any previous error fixes directly associated with bug resolution. The shift is explicitly tied to adding features.

The change caters to adding new functionality per the commit message, as opposed to fixing a bug. The code's structural integrity suggests developing a new feature without modifying error handling or logic that indicates a bug fix.

Thus, the determination for this change based on the analysis is **NotBuggy**."
hadoop,14879.json,1b5451bf054c335188e4cd66f7b4a1d80013e86d,"@@ -1,8 +1,7 @@
     public String getLongUsage() {
-      TableListing listing = AdminHelper.getOptionDescriptionListing();
-      listing.addRow(""<path>"", ""The path of the directory ""
-          + ""from which the erasure coding policy will be unset."");
-      return getShortUsage() + ""\n""
-          + ""Unset the erasure coding policy for a directory.\n\n""
-          + listing.toString();
+      return getShortUsage() + ""\n"" +
+          ""Get the list of supported erasure coding codecs and coders.\n"" +
+          ""A coder is an implementation of a codec. A codec can have "" +
+          ""different implementations, thus different coders.\n"" +
+          ""The coders for a codec are listed in a fall back order.\n"";
     }",NotBuggy,"HDFS-11794. Add ec sub command -listCodec to show currently supported ec codecs. Contributed by SammiChen.
",NotBuggy,"The commit message is about adding a new sub-command `-listCodec` to show supported erasure coding codecs. However, the specific diff provided is a modification of a method `getLongUsage()`. This change updates the usage information to describe listing supported erasure coding codecs and their implementations. It removes the previous functionality related to unsetting the erasure coding policy for a directory.

From the changes, this appears to be related to an enhancement or feature addition rather than a bug fix. The update aligns with part of the commit's intent to detail supported codecs but doesn't indicate any bug-related changes like error handling or logic corrections.

Therefore, based on the provided diff and context, the modifications do not address a bug fix.

**NotBuggy**"
RxJava,6339.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,105 +1,109 @@
         void drain() {
             if (getAndIncrement() != 0) {
                 return;
             }
 
             int missed = 1;
 
             final Subscriber<? super R> downstream = this.downstream;
             final SimpleQueue<T> queue = this.queue;
             final AtomicThrowable error = this.error;
             Iterator<? extends R> iterator = this.currentIterator;
             long requested = this.requested.get();
             long emitted = this.emitted;
             final int limit = prefetch - (prefetch >> 2);
             boolean canRequest = sourceMode != QueueFuseable.SYNC;
 
             for (;;) {
                 if (cancelled) {
                     queue.clear();
                     clearCurrentSuppressCloseError();
                 } else {
                     boolean isDone = upstreamDone;
                     if (error.get() != null) {
                         downstream.onError(error.get());
                         cancelled = true;
                         continue;
                     }
 
                     if (iterator == null) {
                         T t;
 
                         try {
                             t = queue.poll();
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         boolean isEmpty = t == null;
 
                         if (isDone && isEmpty) {
                             downstream.onComplete();
                             cancelled = true;
                         }
                         else if (!isEmpty) {
                             if (canRequest && ++consumed == limit) {
                                 consumed = 0;
                                 upstream.request(limit);
                             }
 
                             Stream<? extends R> stream;
                             try {
                                 stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                                 iterator = stream.iterator();
 
                                 if (iterator.hasNext()) {
                                     currentIterator = iterator;
                                     currentCloseable = stream;
                                 } else {
                                     iterator = null;
                                 }
                             } catch (Throwable ex) {
+                                Exceptions.throwIfFatal(ex);
                                 trySignalError(downstream, ex);
                             }
                             continue;
                         }
                     }
                     if (iterator != null && emitted != requested) {
                         R item;
 
                         try {
                             item = Objects.requireNonNull(iterator.next(), ""The Stream.Iterator returned a null value"");
                         } catch (Throwable ex) {
+                            Exceptions.throwIfFatal(ex);
                             trySignalError(downstream, ex);
                             continue;
                         }
 
                         if (!cancelled) {
                             downstream.onNext(item);
                             emitted++;
 
                             if (!cancelled) {
                                 try {
                                     if (!iterator.hasNext()) {
                                         iterator = null;
                                         clearCurrentRethrowCloseError();
                                     }
                                 } catch (Throwable ex) {
+                                    Exceptions.throwIfFatal(ex);
                                     trySignalError(downstream, ex);
                                 }
                             }
                         }
 
                         continue;
                     }
                 }
 
                 this.emitted = emitted;
                 missed = addAndGet(-missed);
                 if (missed == 0) {
                     break;
                 }
                 requested = this.requested.get();
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message states that ""missing throwIfFatal calls"" have been added. The code changes involve inserting `Exceptions.throwIfFatal(ex)` at several points in the method `drain`.

Let's analyze the changes:

1. **Purpose of `throwIfFatal`:** This utility function checks if an exception is ""fatal"" and re-throws it if so. Fatal exceptions, such as `OutOfMemoryError` or `VirtualMachineError`, should not be caught and handled in the same way as other exceptions. They typically indicate serious failures that might not be recoverable and should terminate the application.

2. **Context of Changes:**
   - **Poll Operation:** A `throwIfFatal` call is added after catching an exception from `queue.poll()`. If a fatal exception occurs here, it will be re-thrown, ensuring proper handling.
   - **Mapper Application:** In the section where a `Stream` is generated from the `mapper.apply(t)` call, the `throwIfFatal` call now ensures fatal exceptions are properly propagated.
   - **Iterator Operations:** The `throwIfFatal` calls added around `iterator.next()` and `iterator.hasNext()` ensure that any fatal errors during iteration are not suppressed.

3. **Bug Fix Nature:**
   - Previously, fatal exceptions might have been caught and passed to `trySignalError`, potentially mishandling them as non-fatal errors. This did not respect the typical Java practice of allowing fatal exceptions to propagate and potentially crash the process, which could lead to inconsistent or unstable states if left unaddressed.
   - Adding these calls aligns with common best practices in exception handling, where fatal exceptions are treated distinctively due to their nature.

4. **Relevance to Commit Message:** 
   - The changes directly reflect the commit's intent. The message states the addition of ""missing `throwIfFatal` calls,"" which is precisely what has been modified in the code.
   - This commit is fixing a flaw (or ""bug"") in the current exception-handling logic by ensuring that fatal exceptions are appropriately re-thrown instead of being handled like usual exceptions.

Considering the analysis above, the code changes align with the commit message and indeed correct the handling of certain classes of exceptions. Thus, the modifications can be regarded as a bug fix. The answer is **Buggy**."
RxJava,6463.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,13 +1,13 @@
         public T poll() {
             if (iterator == null) {
                 return null;
             }
             if (!once) {
                 once = true;
             } else {
                 if (!iterator.hasNext()) {
                     return null;
                 }
             }
-            return ObjectHelper.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
+            return Objects.requireNonNull(iterator.next(), ""Iterator.next() returned a null value"");
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a simple refactor: replacing `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. This aligns with the code change, which switches to using the standard Java `Objects` utility. There's no indication of a bug fix; the functionality remains unchanged.

1. **Functionality**: Both methods throw a `NullPointerException` if the argument is `null`, with the same message format. The behavior and intent are the same.

2. **Commit Message Alignment**: The commit message accurately describes the change. It aims to streamline or align the code with Java's standard library methods, not to address a bug.

3. **Bug-Related Indications**: There is no evidence of error-handling updates, logic corrections, or exceptions fixed related to a bug in this specific modification.

Concluding from the above points, the changes in the diff align with the commit message but do not indicate a bug fix, leading to the conclusion: **NotBuggy**."
RxJava,6464.json,af17c6e268d0e2fe4675feb46c19a29227cb135b,"@@ -1,3 +1,10 @@
         public boolean isEmpty() {
-            return iterator == null || !iterator.hasNext();
+            Iterator<T> it = iterator;
+            if (it != null) {
+                if (!once || it.hasNext()) {
+                    return false;
+                }
+                clear();
+            }
+            return true;
         }",NotBuggy,"3.x: [Java 8] Add Observable operators + cleanup (#6797)

",NotBuggy,"The code change involves the `isEmpty` method, which checks if an iterator is empty. Originally, it returned `true` if the iterator was `null` or had no more elements. The modification introduces a new logic flow:

- It stores the `iterator` in a local variable `it`.
- If `it` is not `null`, it checks the `once` variable; if `once` is `false` or `it` has more elements with `hasNext()`, it returns `false`.
- If `it` is `null`, or `once` is `true` and the iterator has no elements, it calls `clear()` and returns `true`.

This change appears to enhance the logic by tracking the state with `once` and potentially clearing some state with `clear()`. However, the primary focus of the commit message is adding Observable operators and performing cleanup. The commit message does not mention any specific bug fix, and the changes do not obviously point towards a resolution of a bug, such as an exception handling improvement or a logical error correction.

Thus, while the changes introduce a potentially more robust logic or state management, they do not indicate a direct bug fix based on the information provided. Therefore, the assessment is **NotBuggy**."
RxJava,6454.json,a249f4f1241f1c0ebe8e03dc80bdd16e29962c53,"@@ -1,23 +1,3 @@
     protected void subscribeActual(Subscriber<? super T> s) {
-        Iterator<T> iterator;
-        try {
-            iterator = stream.iterator();
-
-            if (!iterator.hasNext()) {
-                EmptySubscription.complete(s);
-                closeSafely(stream);
-                return;
-            }
-        } catch (Throwable ex) {
-            Exceptions.throwIfFatal(ex);
-            EmptySubscription.error(ex, s);
-            closeSafely(stream);
-            return;
-        }
-
-        if (s instanceof ConditionalSubscriber) {
-            s.onSubscribe(new StreamConditionalSubscription<T>((ConditionalSubscriber<? super T>)s, iterator, stream));
-        } else {
-            s.onSubscribe(new StreamSubscription<>(s, iterator, stream));
-        }
+        subscribeStream(s, stream);
     }",NotBuggy,"3.x: [Java 8] Add blockingStream & flatMapStream to Flowable (#6779)

",NotBuggy,"The commit message indicates the addition of `blockingStream` and `flatMapStream` to `Flowable`. However, the diff shows a simplification of the `subscribeActual` method by extracting its logic into a new helper method `subscribeStream`.

Here are the key considerations:
1. **Extraction of Logic:** The change refactors existing code by wrapping the logic related to iterator handling and stream subscription within `subscribeStream`. This aids in code reuse and readability but does not independently add the stated functionalities (`blockingStream` and `flatMapStream`).
   
2. **No Reference to Added Functionality:** The diff does not illustrate the actual addition of `blockingStream` or `flatMapStream` methods to the codebase. Instead, it optimizes the existing subscription process.

3. **Handling and Closing Resource:** The previous implementation had explicit error handling, stream completion, and cleanup (via `closeSafely`). The change likely encapsulates these steps within `subscribeStream`, but doesn’t necessarily imply a bug fix or a new feature.

While the modification enhances maintainability and simplifies logic, assessing from the given diff alone, it neither exhibits a bug fix nor directly correlates with adding the new streaming methods specified in the commit message. Therefore, the answer is **NotBuggy**."
RxJava,5791.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static int getIntProperty(boolean enabled, String key, int defaultNotFound, int defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return Integer.parseInt(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message indicates that this change adds missing `throwIfFatal` calls. In the diff, a call to `Exceptions.throwIfFatal(ex)` was added to the catch block that previously only returned `defaultNotFound`. 

This means that if a ""fatal"" exception (like an `OutOfMemoryError` or other critical errors) occurs, it will be rethrown instead of being handled as a normal exception. This behavior is crucial in Java applications to avoid suppressing critical errors mistakenly, which can prevent the JVM from correctly handling them and potentially lead to more significant issues.

The addition of this call aligns well with the commit message, signaling a correction to the error-handling logic, ensuring that fatal exceptions are not inadvertently suppressed. Therefore, the nature of the change indeed addresses a bug related to handling exceptions appropriately.

Based on this analysis, the conclusion is **Buggy**."
RxJava,6341.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,7 +1,8 @@
         void clearCurrentSuppressCloseError() {
             try {
                 clearCurrentRethrowCloseError();
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 RxJavaPlugins.onError(ex);
             }
         }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message mentions adding ""missing throwIfFatal calls,"" suggesting an improvement in error handling. The diff shows inserting `Exceptions.throwIfFatal(ex)` in `clearCurrentSuppressCloseError`'s catch block. This method call is significant during error handling because it rethrows unrecoverable errors (like `OutOfMemoryError`, `StackOverflowError`, etc.), aligning with best practices for handling fatal exceptions. Previously, these fatal errors might have been swallowed or mishandled, potentially causing undefined behavior or hiding severe issues.

This addition is characteristic of a bug fix since it corrects error handling to ensure fatal exceptions are propagated as required. Therefore, this change aligns with the commit message intent to add crucial error checks or addresses an oversight that could have led to improper program execution under certain conditions.

Considering these points, the changes indicate a bug fix. Therefore, the conclusion is **Buggy**."
RxJava,6378.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,23 +1,24 @@
     protected void subscribeActual(Observer<? super R> observer) {
         if (source instanceof Supplier) {
             Stream<? extends R> stream = null;
             try {
                 @SuppressWarnings(""unchecked"")
                 T t = ((Supplier<T>)source).get();
                 if (t != null) {
                     stream = Objects.requireNonNull(mapper.apply(t), ""The mapper returned a null Stream"");
                 }
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 EmptyDisposable.error(ex, observer);
                 return;
             }
 
             if (stream != null) {
                 ObservableFromStream.subscribeStream(observer, stream);
             } else {
                 EmptyDisposable.complete(observer);
             }
         } else {
             source.subscribe(new FlatMapStreamObserver<>(observer, mapper));
         }
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The commit message suggests adding ""missing throwIfFatal calls,"" and the diff confirms this. Including `Exceptions.throwIfFatal(ex);` addresses potentially fatal exceptions (like `OutOfMemoryError`) to ensure the code gracefully handles them. This is a bug handling issue, specifically related to ensuring robust error handling. This aligns with error and exception management improvements that indicate a bug fix. Therefore, the appropriate conclusion is **Buggy**."
RxJava,5792.json,a97d871ee7161fc9f4684d95cae3e94340cd0ccf,"@@ -1,14 +1,15 @@
     static boolean getBooleanProperty(boolean enabled, String key, boolean defaultNotFound, boolean defaultNotEnabled, Function<String, String> propertyAccessor) {
         if (enabled) {
             try {
                 String value = propertyAccessor.apply(key);
                 if (value == null) {
                     return defaultNotFound;
                 }
                 return ""true"".equals(value);
             } catch (Throwable ex) {
+                Exceptions.throwIfFatal(ex);
                 return defaultNotFound;
             }
         }
         return defaultNotEnabled;
     }",NotBuggy,"3.x: Add missing throwIfFatal calls (#6801)

",Buggy,"The change in this commit adds a call to `Exceptions.throwIfFatal(ex)` inside a catch block, which is a pattern used to rethrow certain ""fatal"" exceptions that should not be swallowed. This can include errors like `OutOfMemoryError` or `VirtualMachineError`, which typically should not be caught and handled as normal exceptions.

By adding this method call, the developer ensures that such fatal exceptions are not suppressed and can be appropriately rethrown to indicate that something unrecoverable has happened. This change improves the robustness and correctness of the error-handling logic.

Given that the commit message is about adding missing throwIfFatal calls and the change directly implements this, it aligns with the commit message's description. This addition indicates a bug fix related to error handling, albeit a more subtle one, by correcting potentially improper exception management.

Thus, the conclusion is **Buggy**."
RxJava,6466.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,56 +1,56 @@
         public void run(long n) {
             long emitted = 0L;
             Iterator<T> iterator = this.iterator;
             ConditionalSubscriber<? super T> downstream = this.downstream;
 
             for (;;) {
 
                 if (cancelled) {
                     clear();
                     break;
                 } else {
                     T next;
                     try {
-                        next = ObjectHelper.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
+                        next = Objects.requireNonNull(iterator.next(), ""The Stream's Iterator returned a null value"");
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (downstream.tryOnNext(next)) {
                         emitted++;
                     }
 
                     if (cancelled) {
                         continue;
                     }
 
                     try {
                         if (!iterator.hasNext()) {
                             downstream.onComplete();
                             cancelled = true;
                             continue;
                         }
                     } catch (Throwable ex) {
                         Exceptions.throwIfFatal(ex);
                         downstream.onError(ex);
                         cancelled = true;
                         continue;
                     }
 
                     if (emitted != n) {
                         continue;
                     }
                 }
 
                 n = get();
                 if (emitted == n) {
                     if (compareAndSet(n, 0L)) {
                         break;
                     }
                     n = get();
                 }
             }
         }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates replacing `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. The change in the diff reflects exactly that; it replaces the custom utility method `ObjectHelper.requireNonNull` with the standard JDK utility method `Objects.requireNonNull` for null-checks when iterating over elements.

This change is primarily a refactor aimed at improving code maintainability by using the standard Java utility methods instead of custom implementations. It does not directly address any bug or correction in logic but is more about standardizing the codebase and possibly simplifying it by reducing custom code.

Given that there is no indication of a logic error or bug specific to the null-check utility being used, the commits do not signify a bug fix but rather a code enhancement or refactor.

Therefore, the changes do not align with a bug fix based on the provided information, and the conclusion is **NotBuggy**."
RxJava,7468.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArray(SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), false, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",NotBuggy,"The commit involves a change in the `mergeArray` method that modifies the usage of the `flatMapSingle` operator in the `Flowable` library. Specifically, it alters the concurrency control parameter from being simply `sources.length` to `Math.max(1, sources.length)`. This appears to address a potential issue when `sources.length` could be zero, which would lead to ineffective or unwanted behavior. Ensuring at least a concurrency of 1 prevents possible non-operation or errors when no sources are present.

Although this change improves the robustness of the method by preventing potential issues when `sources.length` is zero, it does not align directly with traditional bug fixes such as handling exceptions or correcting faulty logic in existing functionality. Instead, it refines method behavior to appropriately handle edge cases, which is an enhancement or a preventive measure rather than a strict bug fix.

Therefore, given the nature of the change and considering definitions of ""bug fix,"" this commit should be classified as **NotBuggy** because it enhances functionality instead of fixing a clear-cut bug."
RxJava,8113.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromStream(@NonNull Stream<T> stream) {
-        ObjectHelper.requireNonNull(stream, ""stream is null"");
+        Objects.requireNonNull(stream, ""stream is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromStream<>(stream));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message states that `ObjectHelper.requireNonNull` is being replaced with `Objects.requireNonNull`. This change doesn't indicate a bug fix; rather, it suggests a refactoring for consistency or modernization, perhaps to utilize the built-in Java `Objects` utility method over a custom one. No bug-related keywords or error-handling improvements are present in the change. The switch from one non-null check to another does not change the method’s logic or fix an existing bug. The purpose here seems aligned with code cleanup or optimization rather than resolving a defect.

Thus, the commit does not indicate a bug fix. Therefore, it is **NotBuggy**."
RxJava,8410.json,f1a795da0d4cdfb3133206ad8e3887bdb8ba948b,"@@ -1,3 +1,3 @@
     public final <R> Maybe<R> concatMapSingle(@NonNull Function<? super T, ? extends SingleSource<? extends R>> mapper) {
-        return flatMapSingleElement(mapper);
+        return flatMapSingle(mapper);
     }",NotBuggy,"3.x: Swap Maybe.flatMapSingle and Maybe.flatMapSingleElement (#6891)

* Deleted Maybe.flatMapSingle

And replaced tests with flatMapSingle().toSingle()

* Renamed Maybe.flatMapSingleElement to flatMapSingle

* Deleted unused MaybeFlatMapSingle operator

* Renamed operator MaybeFlatMapSingleElement to MaybeFlatMapSingle

* Add Ignore to XFlatMapTest.maybeSingle
",NotBuggy,"The commit message describes a renaming operation within the Maybe class, specifically changing `flatMapSingleElement` to `flatMapSingle`. The diff reflects this change by updating the method being called within `concatMapSingle`. There are no indications of bug fixes or error handling improvements in this commit. The change is primarily a renaming and restructuring of method usage rather than addressing any underlying bug.

Therefore, based on the analysis of the commit message and the code diff, the conclusion is **NotBuggy**."
RxJava,8112.json,6ba932c9a3752c6d85d65bd2292c3a47cab64fd6,"@@ -1,4 +1,4 @@
     public static <T> Flowable<@NonNull T> fromCompletionStage(@NonNull CompletionStage<T> stage) {
-        ObjectHelper.requireNonNull(stage, ""stage is null"");
+        Objects.requireNonNull(stage, ""stage is null"");
         return RxJavaPlugins.onAssembly(new FlowableFromCompletionStage<>(stage));
     }",NotBuggy,"Replace ObjectHelper.requireNonNull with Objects.requireNonNull (#6771)

",NotBuggy,"The commit message indicates a replacement of the method `ObjectHelper.requireNonNull` with `Objects.requireNonNull`. This is a typical refactor to use a standard library method over a custom one but does not indicate a bug fix. No fix of bug-specific logic, error handling, or exception processing is evident in the provided changes.

**NotBuggy**"
RxJava,7469.json,1ede60786a1a8f00f1fb3968d0701a323ed86f2f,"@@ -1,3 +1,3 @@
     public static <T> Flowable<T> mergeArrayDelayError(@NonNull SingleSource<? extends T>... sources) {
-        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, sources.length);
+        return Flowable.fromArray(sources).flatMapSingle(Functions.identity(), true, Math.max(1, sources.length));
     }",NotBuggy,"3.x: Use more appropriate operators when delegating to Flowable ops (#6888)

* 3.x: Use more appropriate operators when delegating to Flowable ops

* Remove now-unused classes
",Buggy,"The commit message states that more appropriate operators are used with Flowable operations. The specific change in the code substitutes `sources.length` with `Math.max(1, sources.length)`. This modification ensures that the `flatMapSingle` method's concurrency parameter is never less than 1, preventing potential runtime errors when `sources.length` is 0. Previously, if `sources.length` was 0, it could cause unexpected behavior since 0 is not a valid concurrency level. 

This code change enhances robustness and correctness by safeguarding against invalid input scenarios. Although the message does not explicitly mention a bug fix, the alteration addresses a potential bug where invalid concurrency levels could lead to improper functioning.

For this reason, the change can be considered as a bug fix due to the underlying issue it addresses with the `flatMapSingle` method, matching the commit message's emphasis on using more appropriate operators.

The conclusion is **Buggy**."
RxJava,6762.json,ecfb6b741d411de9d060b1c4b1fcd90ff7530854,"@@ -1,3 +1,4 @@
     static AutoCloseable toAutoCloseable(@NonNull Disposable disposable) {
+        Objects.requireNonNull(disposable, ""disposable is null"");
         return disposable::dispose;
     }",NotBuggy,"3.x: Improve Javadocs of Completable and some others (#6809)

",Buggy,"The provided code modification introduces a null check for the `disposable` parameter, ensuring it is not `null` before proceeding to convert it into an `AutoCloseable`. This change enhances the robustness of the code by preventing potential `NullPointerException` at runtime when `disposable::dispose` is invoked.

However, the commit message primarily mentions improvements to Javadoc documentation, which is unrelated to the functional change made in the code. The added null check is more in line with error-handling improvements than documentation enhancements.

Given that the null check prevents a possible runtime error, this is indeed a bug fix, as it handles a potential bug condition where passing a `null` `disposable` could lead to an exception. Therefore, despite the disconnect with the commit message, the actual code change indicates a bug fix.

Thus, the conclusion for this change is **Buggy**."
junit4,587.json,83a25d440d118b7d10779a884d35f31045949e27,"@@ -1,3 +1,3 @@
             public int compare(T o1, T o2) {
-                return fSorter.compare(describeChild(o1), describeChild(o2));
+                return sorter.compare(describeChild(o1), describeChild(o2));
             }",NotBuggy,"Treatments for parallel execution
",NotBuggy,"The commit message refers to ""Treatments for parallel execution,"" but the diff shows a straightforward renaming: changing `fSorter` to `sorter`. This change suggests code cleanup or refactoring to use a possibly correctly initialized/accessible sorter, but it does not explicitly relate to parallel execution or indicate a bug fix such as a synchronization issue or concurrent modification problem. Without additional context, this change doesn't align with bug fixing or parallel execution enhancements.

The answer is **NotBuggy**."
junit4,522.json,627b85abfd9c1f6c69b14f9d33fb292868fe6826,"@@ -1,3 +1,4 @@
-    public static void assertThrows(Class<? extends Throwable> expectedThrowable, ThrowingRunnable runnable) {
-        expectThrows(expectedThrowable, runnable);
+    public static <T extends Throwable> T assertThrows(Class<T> expectedThrowable,
+            ThrowingRunnable runnable) {
+        return assertThrows(null, expectedThrowable, runnable);
     }",NotBuggy,"rename expectThrows() to assertThrows()
",NotBuggy,"This commit message indicates a rename operation from `expectThrows()` to `assertThrows()`, but the diff makes changes to the method signature beyond just a rename. It changes the return type from `void` to `T`, indicating `assertThrows` now returns an instance of the expected throwable, which extends its functionality rather than just renaming. However, this does not necessarily indicate a bug fix; it's more of an enhancement or API modification since there's no indication of a specific bug being addressed in behavior or logic.

Thus, in terms of whether this change indicates a bug fix, the answer is **NotBuggy**."
junit4,313.json,5501ad5d546427b228b208da15cec997e0b13cc8,"@@ -1,3 +1,3 @@
-	public RememberingListener listener() {
+	public RunListener listener() {
 		return new RememberingListener();
 	}",NotBuggy,"Finished Max javadoc
",NotBuggy,"The commit message states ""Finished Max javadoc"", suggesting documentation completion, but the code change modifies a method return type. This refactor adjusts the type returned by the listener method, which is not related to documentation but instead suggests a design change. This does not indicate a bug fix, as it likely involves a design or structural modification rather than addressing an error. The answer is **NotBuggy**."
junit4,738.json,86b9346e4c8fa263810dbd55a2775081194b1d6f,"@@ -1,3 +1,3 @@
     public int hashCode() {
-        return uniqueId.hashCode();
+        return fUniqueId.hashCode();
     }",NotBuggy,"Ensure serialization compatibility where possible

Not possible for org.junit.runner.Result
",NotBuggy,"The change in the git diff is a modification from `uniqueId` to `fUniqueId` within the `hashCode()` method. This implies a possible refactor to use a different variable for generating the hash code. However, without additional context, this single change does not reflect a bug fix. It seems more geared towards ensuring that the object maintains consistent behavior perhaps aligning with serialization expectations.

The commit message indicates an effort to ensure serialization compatibility where possible but also notes that it's not possible for a particular class (`org.junit.runner.Result`). The change to `hashCode()` may be part of a broader effort to maintain consistent serialization but doesn't directly fix an observable bug like incorrect logic, errors, or exceptions.

Given these points, the modification appears more related to code correctness or refactoring rather than resolving a specific bug. Therefore, the assessment is **NotBuggy**."
junit4,271.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     private String missingExceptionMessage() {
-        String expectation= StringDescription.toString(fMatcherBuilder.build());
+        String expectation= StringDescription.toString(matcherBuilder.build());
         return format(missingExceptionMessage, expectation);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message discusses removing 'f' prefixes from field names and mentions changes to coding styles, neither of which suggest a bug fix. The code modification aligns with changing a field name from `fMatcherBuilder` to `matcherBuilder`, which appears to relate directly to removing the 'f' prefix as described in the commit message. There are no implications of error-handling updates, exception handling improvements, or logical corrections that would indicate a bug fix. This aligns with a refactor rather than addressing a bug.

Therefore, the conclusion is **NotBuggy**."
junit4,657.json,fbb1efed56f2b495d68a598d5345cbc95b118d35,"@@ -1,3 +1,3 @@
     public boolean isStatic() {
-        return Modifier.isStatic(fMethod.getModifiers());
+        return Modifier.isStatic(getModifiers());
     }",NotBuggy,"Remove duplicate code.
",NotBuggy,"The commit message states that duplicate code has been removed, but the provided diff does not show removal of duplicate lines—it shows a modification to streamline or correct existing code. Specifically, it changes the way modifiers are accessed by using a presumably existing method `getModifiers()` instead of directly accessing `fMethod.getModifiers()`. This change may simplify the code or ensure consistency but does not align with the commit message of removing duplicate code.

The modification does not clearly indicate a bug fix, such as correcting logical errors or handling edge cases. Without additional context showing a broader removal of duplicates, it does not suggest a bug fix based on the given information alone.

Therefore, the conclusion is **NotBuggy**."
junit4,498.json,9ac4cc842d70dc7ef266f4d677d90d9b177e4be6,"@@ -1,5 +1,5 @@
     static public void assertEquals(String message, long expected, long actual) {
         if (expected != actual) {
-            failNotEquals(message, expected, actual);
+            failNotEquals(message, Long.valueOf(expected), Long.valueOf(actual));
         }
     }",NotBuggy,"Use explicit auto-boxing.
",NotBuggy,"The change in the Git diff refactors how primitive long values are boxed into Long objects within the method failNotEquals. Previously, the boxing was implicit. The commit replaces this with explicit calls to Long.valueOf, matching the commit message stating ""Use explicit auto-boxing"". 

However, the change doesn't address any bug. The implicit and explicit boxing achieve the same result. This refactor does not correct any logical or runtime errors, nor does it address error handling, exception management, or other bug-related aspects.

Therefore, the analysis of the commit relative to the change is as follows:

1. **Commit Message Relevance**: The commit message accurately describes the code change.
2. **Sign of Bug Fix**: No bugs are fixed in this change; it's purely a refactor to clarify intent or adhere to a coding style preference.

Based on this evaluation, the conclusion is **NotBuggy**."
junit4,163.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     protected void describeMismatchSafely(T item, Description description) {
         description.appendText(""message "");
-        fMatcher.describeMismatch(item.getMessage(), description);
+        matcher.describeMismatch(item.getMessage(), description);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message discusses coding style changes and removing the 'f' prefix from field names. The code change aligns with this by renaming `fMatcher` to `matcher`. It doesn't address any logic, error handling, or exception management changes, indicating no bug fix. Hence, the answer is **NotBuggy**."
junit4,267.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     private boolean isAnyExceptionExpected() {
-        return fMatcherBuilder.expectsThrowable();
+        return matcherBuilder.expectsThrowable();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message states that 'f' prefixes are being removed from field names. The diff aligns with this message by renaming `fMatcherBuilder` to `matcherBuilder`. This is a refactoring change aimed at coding style improvements, not a bug fix. There are no signs of a logical correction, error handling, or other bug-related changes in this diff.

Thus, the answer is **NotBuggy**."
junit4,663.json,02c328028b4d32c15bbf0becc9838e54ecbafcbf,"@@ -1,7 +1,11 @@
     public Runner safeRunnerForClass(Class<?> testClass) {
         try {
-            return runnerForClass(testClass);
+            Runner runner = runnerForClass(testClass);
+            if (runner != null) {
+                configureRunner(runner);
+            }
+            return runner;
         } catch (Throwable e) {
             return new ErrorReportingRunner(testClass, e);
         }
     }",NotBuggy,"Add Ordering, Orderable and @OrderWith (#1130)

* Add Ordering, Orderable and @OrderWith.

These APIs allow arbitrary ordering of tests, including randomization.
",NotBuggy,"The commit message discusses adding new features for ordering tests, not fixing bugs. The code changes involve configuring a test runner without addressing specific errors, logical fixes, or performance improvements indicative of a bug fix. The answer is **NotBuggy**."
junit4,165.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,4 +1,4 @@
     public void describeTo(Description description) {
         description.appendText(""exception with cause "");
-        description.appendDescriptionOf(fMatcher);
+        description.appendDescriptionOf(matcher);
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"The commit message mentions several changes: removing 'f' prefixes, adding LEGACY_CODING_STYLE, and changing CODING_STYLE. However, the diff only shows the removal of the 'f' prefix from a field name (`fMatcher` to `matcher`). This is a straightforward refactoring activity to align with a possible naming convention change. The given diff doesn't represent a bug fix, but rather a renaming for readability or style consistency.

- **Refactoring**: The code change from `fMatcher` to `matcher` suggests that field names may be updated to conform to a new naming style. No functional logic related to error handling, exception management, or logic correction is modified here.
  
There's no indication in this snippet of correction to incorrect behavior, error handling, or performance improvement that can be associated with a bug.

Based on the change presented, it aligns with part of the commit message regarding field renaming but doesn't align with a bug fix. Thus, the answer is **NotBuggy**."
junit4,788.json,b01900fddca5ff54258e621025ccdbbc75f880f1,"@@ -1,8 +1,8 @@
-    FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
+    static FilterFactory createFilterFactory(Class<? extends FilterFactory> filterFactoryClass)
             throws FilterFactoryNotCreatedException {
         try {
             return filterFactoryClass.getConstructor().newInstance();
         } catch (Exception e) {
             throw new FilterFactoryNotCreatedException(e);
         }
     }",NotBuggy,"FilterFactoryFactory renamed to FilterFactories and its methods made static.
ClassUtil renamed to Classes for consistency.
",NotBuggy,"The commit message does not indicate any bug fixes; rather, it describes a refactoring and renaming exercise. The change seen in the diff aligns with the commit message as it changes the method `createFilterFactory` to be static, consistent with the intention to make methods in the renamed class `FilterFactories` static.

There are no indications of a bug fix since the method's logic remains unchanged, and the commit message focuses solely on naming and structural consistency.

Therefore, the assessment of this commit is **NotBuggy**."
junit4,702.json,95bd62b2ffd80394d1a17e9078a4d79478423030,"@@ -1,3 +1,3 @@
 			protected void notifyListener(RunListener each) throws Exception {
-				each.testIgnoredReason(description, e);
+				each.testAssumptionFailed(description, e);
 			};",NotBuggy,"Start separating the idea of failed assumptions from ignored tests
",NotBuggy,"The commit message discusses separating failed assumptions from ignored tests. The code modification reflects this by changing the method called on the `RunListener` interface from `testIgnoredReason` to `testAssumptionFailed`. This change aligns precisely with the intent stated in the commit message.

Here's a step-by-step analysis of the change:

1. **Before the change**: The code calls `each.testIgnoredReason(description, e);`, implying that a test is ignored because of some reason described by `e`.

2. **After the change**: The code now calls `each.testAssumptionFailed(description, e);`, indicating that instead of being ignored, the test is marked as having a failed assumption.

3. **Relevance to Commit Message**: The commit message states the intention to separate failed assumptions from ignored tests. Changing the method call from handling an ignored test to handling a failed assumption is precisely what is described.

While this modification changes the behavior and categorization of tests, it doesn't explicitly indicate a bug fix from the content of the diff and commit message alone. It appears to be more of a refactor or a change in semantics rather than a direct bug resolution.

Thus, the changes don't seem to indicate a traditional bug fix like a logical error or issue resolution, so this would be classified as **NotBuggy**."
junit4,640.json,df00d5eced3a7737b88de0f6f9e3673f0cf88f88,"@@ -1,3 +1,3 @@
     public Class<?> getDeclaringClass() {
-        return fMethod.getDeclaringClass();
+        return method.getDeclaringClass();
     }",NotBuggy,"remove 'f' prefix from field names, add LEGACY_CODING_STYLE, change CODING_STYLE
",NotBuggy,"This change removes the 'f' prefix from the field name, which is consistent with the commit message of altering field names. The change reflects a renaming, not addressing an error or bug in the functionality of the code. This is a refactoring for improved code style or consistency rather than a bug fix. Therefore, the answer is **NotBuggy**."
junit4,348.json,a72b0dbef4b01e8ad0b832d9a579dd7fabd5a071,"@@ -1,3 +1,3 @@
-	public static Executioner methods() {
-		return new ParallelExecutioner(false, true);
+	public static Computer methods() {
+		return new ParallelComputer(false, true);
 	}",NotBuggy,"Clean up parallel execution naming.
",NotBuggy,"The commit message describes cleaning up parallel execution naming, and the diff shows a change in method return type and instantiation: from `Executioner` to `Computer` and from `ParallelExecutioner` to `ParallelComputer`. There's no indication of a bug fix, only a naming or design consistency improvement. The purpose appears to be more about making naming more intuitive or aligned with new terms rather than correcting any erroneous behavior. 

Given that the change focuses on renaming and not on fixing a fault in the logic or behavior of the code, it supports the claim of a clean-up, not a bug fix.

Thus, I conclude with **NotBuggy**."
facebook-android-sdk,3514.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,5 +1,5 @@
-    public static final Set<LoggingBehavior> getLoggingBehaviors() {
+    public static Set<LoggingBehavior> getLoggingBehaviors() {
         synchronized (loggingBehaviors) {
             return Collections.unmodifiableSet(new HashSet<LoggingBehavior>(loggingBehaviors));
         }
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message ""Facebook Android SDK 4.0"" does not mention a bug fix. The only change here is the removal of the `final` keyword from the method signature, which means the method can now potentially be overridden in subclasses. This modification doesn't directly suggest fixing an error or bug in the system; instead, it alters the method's behavior in terms of inheritance and extensibility.

The change itself does not clarify any error-handling improvements, logical corrections, or other signs of bug-related changes. Instead, it may indicate a design improvement by allowing further flexibility of method usage, but there's no direct indication of fixing an existing issue.

Given the context and the nature of the modification, the conclusion is **NotBuggy**."
facebook-android-sdk,1334.json,bc17a04007ed19a340cfb2753c07f927e514b5ad,"@@ -1,7 +1,7 @@
     private void logAuthorizationMethodStart(String method) {
         Bundle bundle = newAuthorizationLoggingBundle(pendingRequest.getAuthId());
         bundle.putLong(EVENT_PARAM_TIMESTAMP, System.currentTimeMillis());
         bundle.putString(EVENT_PARAM_METHOD, method);
 
-        appEventsLogger.logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
+        getAppEventsLogger().logSdkEvent(EVENT_NAME_LOGIN_METHOD_START, null, bundle);
     }",NotBuggy,"Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5.1 (see https://developers.facebook.com/docs/android/change-log-3.x/)

Test Plan: - Built ZIP

Reviewers: mingfli

Reviewed By: mingfli

CC: subodh, platform-diffs@lists

Differential Revision: https://phabricator.fb.com/D986742
",NotBuggy,"The change in the diff modifies how the `appEventsLogger` object is accessed by replacing a direct field call with a method call `getAppEventsLogger()`. This can suggest several possibilities:

1. **Lazy Initialization**: It could be that `appEventsLogger` needs to be initialized fully before calling `logSdkEvent`, and `getAppEventsLogger()` ensures that the logger is properly initialized. This is a common bug when accessing resources that need initialization before use.

2. **Thread Safety**: The method call might provide synchronization or some level of thread safety which the direct field access does not, preventing a potential concurrency bug.

3. **Encapsulation**: By using a getter method, the code might be ensuring better encapsulation, which could be part of a fix where external access to the logger instance was causing issues.

However, the commit message doesn't explicitly state a bug fix, and the message is more of an administrative update related to a new Facebook SDK version. The given change might be more about internal consistency, preparing for SDK changes, or minor improvements rather than a direct user-facing bug fix.

Considering these points, while there is a potential for a bug fix related to initialization or thread safety, the commit does not explicitly state such. Without more context or evidence of a bug, it's hard to definitively conclude a fix was made. Therefore, the answer is **NotBuggy**."
facebook-android-sdk,2354.json,2921bd7e2a268d0e08d10c13f90f8381d27c24ca,"@@ -1,8 +1,11 @@
     public void add(Activity activity) {
+        if (InternalSettings.isUnityApp()) {
+            return;
+        }
         if (Thread.currentThread() != Looper.getMainLooper().getThread()) {
             throw new FacebookException(""Can't add activity to CodelessMatcher on non-UI thread"");
         }
         this.activitiesSet.add(activity);
         delegateMap.clear();
         startTracking();
     }",NotBuggy,"Delegate event matching to unity sdk

Summary:
Codeless event matching should be handled in Unity SDK due to the special tree hierarchy in Unity apps. This diff does 2 things:
1. Send event mappin to Unity on parsing the app settings.
2. Skip event matching for unity apps.

Differential Revision: D10215680

fbshipit-source-id: 28b2128619aa220cd5cf178d7024464d5e126bd3
",NotBuggy,"The commit message indicates changes related to handling events specifically for Unity apps. The code change makes sense in this context as it skips adding activities to CodelessMatcher if the application is a Unity app. This aligns with the summary that event matching should be delegated to the Unity SDK for these apps. The change involves checking whether the app is a Unity application using `InternalSettings.isUnityApp()` and returning early if true, which stops further processing specific to CodelessMatcher that is presumably not necessary for Unity apps due to inherent differences in their tree hierarchy.

The changes do not explicitly indicate a bug fix. They reflect a feature change or enhancement rather than direct error-handling or corrections. Thus, this alteration seems aligned more with functionality delegation rather than resolving a defect.

Therefore, I conclude with **NotBuggy**."
facebook-android-sdk,1430.json,8d168dd203acf2e068a6b6f6029e9baaea992538,"@@ -1,4 +1,3 @@
                     public void onBackgroundProcessingStarted() {
-                        view.findViewById(R.id.com_facebook_login_activity_progress_bar)
-                                .setVisibility(View.VISIBLE);
+                        progressBar.setVisibility(View.VISIBLE);
                     }",NotBuggy,"Facebook Android SDK 4.28
",NotBuggy,"The commit message ""Facebook Android SDK 4.28"" lacks detail about specific changes or bug fixes. However, the change in the code involves replacing a hard-coded view reference with a presumably pre-defined variable `progressBar`. This modification improves code readability and maintainability, but it doesn't indicate a bug fix relating to application logic, error handling, or exception handling.

Given these observations, the changes do not demonstrate a bug fix. Thus, the conclusion is **NotBuggy**."
facebook-android-sdk,2780.json,9dd41692e4bc962ab7bc56c5a17679be80ab50bf,"@@ -1,4 +1,7 @@
     private static String getUserAgent() {
-        // TODO port: construct user agent string with version
-        return ""FBAndroidSDK"";
+        if (userAgent == null) {
+            userAgent = String.format(""%s.%s"", USER_AGENT_BASE, FacebookSdkVersion.BUILD);
+        }
+
+        return userAgent;
     }",NotBuggy,"Added UserAgent, Migration Bundle

Summary: Adding version string in UserAgent and current sdk Migration Bundle.

Test Plan:
- Unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang, jacl

Reviewed By: mingfli

CC: msdkexp@

Differential Revision: https://phabricator.fb.com/D583727

Task ID: 994912
",NotBuggy,"The commit message primarily discusses the addition of a UserAgent and a Migration Bundle, while the code change involves enhancing the `getUserAgent` method by implementing a logic to construct the user agent string with a version number. The previous code returned a static string ""FBAndroidSDK,"" and the new code conditionally constructs a string using a base (""USER_AGENT_BASE"") and a version number (""FacebookSdkVersion.BUILD"").

There's no direct mention of a bug or error in the code being corrected. The change is more about introducing new functionality (adding version information to the user agent string). The commit does not specify any bug fixes, and the code modifications align with adding features rather than correcting erroneous behavior.

The change aligns with the commit message regarding the addition of features. However, this is not indicative of a bug fix. Based on this assessment, I would conclude that the changes do not directly correspond to a bug fix. Therefore, the conclusion is **NotBuggy**."
facebook-android-sdk,2061.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-    public List<String> getDeclinedPermissions() {
+    public Set<String> getDeclinedPermissions() {
         return this.declinedPermissions;
     }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The change in this diff modifies the return type of the `getDeclinedPermissions` method from a `List<String>` to a `Set<String>`. This could address potential issues related to allowing duplicate permissions when they should be unique, as a `Set` enforces uniqueness by definition.

This change may unintentionally fix a latent bug where duplicates in permissions were not being handled correctly when using a `List`. However, since there isn't explicit information about bug symptoms being addressed or behavior corrections, it's hard to definitively state that a specific bug is being targeted or fixed.

### Evaluation:
- **Change in Data Structure**: The alteration from `List` to `Set` suggests a focus on ensuring uniqueness, which could relate to an issue if duplicates were problematic.
- **Relevance to Bug Fixes**: While the change aligns more with code quality or refactoring for correctness and may prevent future bugs or misunderstandings, it doesn't explicitly fix a current bug detailed in the commit message.

Given the context of a major version update and lack of explicit bug reference, this change seems more aligned with an enhancement or refactoring for better data integrity rather than fixing a specific known bug.

Thus, the conclusion for this diff is: **NotBuggy**."
facebook-android-sdk,775.json,752ea7b77a1779ed8b784dbd55a886cc00aeb8a6,"@@ -1,3 +1,3 @@
-        public int describeContents() {
-            return 0;
-        }

+    public int describeContents() {
+        return 0;
+    }",NotBuggy,"Facebook Android SDK 4.0
",NotBuggy,"The commit message, ""Facebook Android SDK 4.0,"" is a versioning or branding update and does not hint at specific changes or bug fixes. The code diff shows a minor formatter adjustment, altering the indentation but not changing logic or functionality. This modification does not indicate a bug fix but is more likely a cosmetic or stylistic change.

1. **No Logical Change**: The original and the updated code for the `describeContents` method are functionally identical. The method still returns 0, and the changes are limited to formatting.

2. **Commit Message Irrelevance**: The commit message does not mention a specific bug or issue that needs fixing—just version information.

Considering these points, the changes do not suggest a fix for any bug. They are limited to formatting adjustments without impact on code functionality.

Conclusion: **NotBuggy**"
facebook-android-sdk,477.json,0086a52b843d60753942a20573fcdde4c9a3e948,"@@ -1,4 +1,4 @@
-        public Builder setObjectType(final LikeView.ObjectType objectType) {
-            this.objectType = objectType == null ? LikeView.ObjectType.UNKNOWN : objectType;
+        public Builder setObjectType(final String objectType) {
+            this.objectType = objectType;
             return this;
         }",NotBuggy,"Facebook Android SDK 4.1
",NotBuggy,"The change modifies a method to accept a String rather than an enum (LikeView.ObjectType). It removes a null check and enum default, losing type safety and potential protection against invalid values. However, this does not clearly fit within bug-fix logic. The commit message doesn't indicate a bug fix, so the conclusion is **NotBuggy**."
facebook-android-sdk,818.json,845ba874cc5552f4deccf7d6262d351e4bbd72e4,"@@ -1,4 +1,5 @@
-        public void writeToParcel(Parcel parcel, int flags) {
-            parcel.writeString(actionText);
-            parcel.writeLong(publishDate.getTime());
+        public void writeToParcel(Parcel parcel, int i) {
+            parcel.writeString(callId.toString());
+            parcel.writeParcelable(requestIntent, 0);
+            parcel.writeInt(requestCode);
         }",NotBuggy,"Facebook Android SDK 3.5 (see https://developers.facebook.com/android/change-log-3.x/)

Summary: Facebook Android SDK 3.5

Test Plan: - Release testing

Reviewers: mingfli

Reviewed By: mingfli

CC: platform-diffs@lists, subodh, dreiss

Differential Revision: https://phabricator.fb.com/D935263
",NotBuggy,"The commit message relates to a release update, not explicitly mentioning bugs. The code change alters Android's `writeToParcel` method, affecting Parcelable serialization. New data fields (a `callId` as a `String`, `requestIntent` as a `Parcelable`, and `requestCode` as an `int`) replace previous ones. This likely updates data structure serialization but isn't explicitly fixing a bug. Thus, the change reflects structural data changes for new or different data handling.

Given the release context and no clear indication of error correction or logic fix, this change isn't marked explicitly as a bug fix.

The answer is **NotBuggy**."
facebook-android-sdk,2702.json,d4a9d0c92085e732231e5bbb09f8e1f1fe292f6a,"@@ -1,3 +1,11 @@
-    public static Request newMyFriendsRequest(Session session, Callback callback) {
-        return new Request(session, MY_FRIENDS, null, null, callback);
+    public static Request newMyFriendsRequest(Session session, final GraphUserListCallback callback) {
+        Callback wrapper = new Callback() {
+            @Override
+            public void onCompleted(Response response) {
+                if (callback != null) {
+                    callback.onCompleted(typedListFromResponse(response, GraphUser.class), response);
+                }
+            }
+        };
+        return new Request(session, MY_FRIENDS, null, null, wrapper);
     }",NotBuggy,"Minor Request API changes

Summary:
- Added execute variants to Request static helpers
- Made callbacks strongly typed for Request statics
- Renamed execute -> executeAndWait
- Added instance executeAndWait/executeAsync to Request and RequestBatch

Test Plan:
- added unit tests

Revert Plan:
- ok

Tags:

Reviewers: mingfli, clang

Reviewed By: mingfli

Differential Revision: https://phabricator.fb.com/D600294

Task ID: 1801701
",NotBuggy,"The commit revolves around API adjustments, focusing on type safety and method enhancements rather than bug fixes. The diff shows stronger typing for callbacks by wrapping them, consistent with the commit's note on type improvements, and focuses on API usability without fixing bugs. The answer is **NotBuggy**."
facebook-android-sdk,2571.json,1ebea4d8e02aff0bc96cad34c959ab680421f8eb,"@@ -1,7 +1,9 @@
     private static void cancelCurrentTask() {
-        if (currentFuture != null) {
-            currentFuture.cancel(false);
-        }
+        synchronized (currentFutureLock) {
+            if (currentFuture != null) {
+                currentFuture.cancel(false);
+            }
 
-        currentFuture = null;
+            currentFuture = null;
+        }
     }",NotBuggy,"Facebook Android SDK 4.27
",Buggy,"The code change involves adding synchronization around the cancelCurrentTask method. Originally, the method canceled a task without synchronization, which could result in race conditions or unexpected behavior if accessed by multiple threads concurrently. By introducing a synchronized block using the currentFutureLock, the change ensures that access to currentFuture is thread-safe. This helps prevent potential issues linked to concurrency, like attempting to cancel a task that might not be in the expected state due to simultaneous thread manipulation.

This adjustment aims to fix a potential bug related to thread safety and is an improvement to the error-handling in a multithreaded context. Therefore, this change aligns with indications of a bug fix.

**Buggy**"
facebook-android-sdk,2275.json,2edb0d361bce159d052355395d6f3dd443c29a6b,"@@ -1,38 +1,38 @@
     private static synchronized void initialize() {
         try {
             FetchedAppSettings settings = FetchedAppSettingsManager.queryAppSettings(
                     FacebookSdk.getApplicationId(), false);
             if (settings == null) {
                 return;
             }
             String eventFilterResponse = settings.getRestrictiveDataSetting();
             if (!eventFilterResponse.isEmpty()) {
                 JSONObject jsonObject = new JSONObject(eventFilterResponse);
 
-                deprecatedParams.clear();
+                deprecatedParamFilters.clear();
 
                 Iterator<String> keys = jsonObject.keys();
                 while (keys.hasNext()) {
                     String key = keys.next();
                     JSONObject json = jsonObject.getJSONObject(key);
                     if (json != null) {
                         if (json.optBoolean(""is_deprecated_event"")) {
                             deprecatedEvents.add(key);
                         } else {
                             JSONArray deprecatedParamJsonArray = json
                                     .optJSONArray(""deprecated_param"");
-                            DeprecatedParam deprecatedParam = new DeprecatedParam(key,
+                            DeprecatedParamFilter deprecatedParamFilter = new DeprecatedParamFilter(key,
                                     new ArrayList<String>());
                             if (deprecatedParamJsonArray != null) {
-                                deprecatedParam.deprecateParams = Utility
+                                deprecatedParamFilter.deprecateParams = Utility
                                         .convertJSONArrayToList(deprecatedParamJsonArray);
                             }
-                            deprecatedParams.add(deprecatedParam);
+                            deprecatedParamFilters.add(deprecatedParamFilter);
                         }
                     }
                 }
             }
         } catch (Exception e) {
             /* swallow */
         }
     }",NotBuggy,"Refactor: better naming and several small changes

Reviewed By: ZebingZong

Differential Revision: D19523661

fbshipit-source-id: 664f47c474172136f9ef6e2f7f157f428cd8f5f6
",NotBuggy,"The commit message indicates a refactoring aimed at better naming and small code changes. Upon examining the provided diff:

1. **Variable Renaming**: 
   - The variable names `deprecatedParams` and `deprecatedParam` are changed to `deprecatedParamFilters` and `deprecatedParamFilter`, respectively. This suggests a clarification in the purpose or characteristics of these variables, potentially improving code readability.

2. **Class Renaming**:
   - The class `DeprecatedParam` is renamed to `DeprecatedParamFilter`, which aligns with the variable name changes. This indicates a clearer semantic representation of what the class or its instances are meant to do, which is likely related to filtering parameters rather than just marking them as deprecated.

3. **Logic**:
   - The logic of the code remains unchanged. The modifications are restricted to renaming and therefore should not impact the execution behavior.

4. **Error Handling**:
   - No changes have been made to exception handling or logic corrections that would indicate a fix for a defect.

The renaming of variables and class seems to be primarily for better clarity and maintainability of the code without affecting its logic or functionality. There are no indications of bug fixes such as correcting logic errors, improving error handling, or adjusting faulty operation.

Given the context and nature of the changes, these align with what is described in the commit message as refactoring. This isn’t indicative of a direct bug fix but rather a move towards better code clarity and structure.

Therefore, the conclusion is **NotBuggy**."
facebook-android-sdk,2617.json,426267f3dcc9daadcf383ec1896bffa1d7d46852,"@@ -1,9 +1,7 @@
         public String toKey() {
             switch (this) {
-                case ADDRESS_DETECTION:
-                case APP_EVENT_PREDICTION: return ""fc3"";
                 case MTML_ADDRESS_DETECTION: return ""address_detect"";
                 case MTML_APP_EVENT_PREDICTION: return ""app_event_pred"";
             }
             return ""Unknown"";
         }",NotBuggy,"Remove Non-MTML model

Summary: Same as the title

Reviewed By: jiangcici

Differential Revision: D20863645

fbshipit-source-id: e34e7b2a6a3aaddbf82b543c46eda834339da7ae
",NotBuggy,"The commit message states that a ""Non-MTML model"" is being removed, and the code change reflects this by removing the `ADDRESS_DETECTION` and `APP_EVENT_PREDICTION` cases. These cases are separate from the `MTML_ADDRESS_DETECTION` and `MTML_APP_EVENT_PREDICTION` cases, which are explicitly retained. The commit message and code change align, focusing on a removal task rather than addressing any bug fix or error correction.

Given this analysis, the changes in the commit appear to be about removing specific features or paths in the code, rather than fixing bugs. Therefore, the conclusion is **NotBuggy**."
facebook-android-sdk,304.json,b2fa42b8fca13e5f44025fb76b270ef8b9733eb8,"@@ -1,20 +1,26 @@
     private static void validate(ShareContent content, Validator validator)
             throws FacebookException {
         if (content == null) {
             throw new FacebookException(""Must provide non-null content to share"");
         }
 
         if (content instanceof ShareLinkContent) {
             validator.validate((ShareLinkContent) content);
         } else if (content instanceof SharePhotoContent) {
             validator.validate((SharePhotoContent) content);
         } else if (content instanceof ShareVideoContent) {
             validator.validate((ShareVideoContent) content);
         } else if (content instanceof ShareOpenGraphContent) {
             validator.validate((ShareOpenGraphContent) content);
         } else if (content instanceof ShareMediaContent) {
             validator.validate((ShareMediaContent) content);
         } else if (content instanceof ShareCameraEffectContent) {
             validator.validate((ShareCameraEffectContent) content);
+        } else if (content instanceof ShareMessengerOpenGraphMusicTemplateContent) {
+            validator.validate((ShareMessengerOpenGraphMusicTemplateContent) content);
+        } else if (content instanceof ShareMessengerMediaTemplateContent) {
+            validator.validate((ShareMessengerMediaTemplateContent) content);
+        } else if (content instanceof ShareMessengerGenericTemplateContent) {
+            validator.validate((ShareMessengerGenericTemplateContent) content);
         }
     }",NotBuggy,"Facebook Android SDK 4.29
",NotBuggy,"The commit message doesn't indicate a bug fix, and the changes extend the functionality by adding validation for new content types: ShareMessengerOpenGraphMusicTemplateContent, ShareMessengerMediaTemplateContent, and ShareMessengerGenericTemplateContent. This suggests feature expansion rather than a correction of errors. Therefore, the answer is **NotBuggy**."
titan,4308.json,53441fa4ba940ba38ebbe2710ce9041819722020,"@@ -1,3 +1,3 @@
-    public boolean isValidDataType(Class<?> clazz) {
-        return true;
+    public boolean isQNF() {
+        return false;
     }",NotBuggy,"Added the notion of Query-Normal-Form (QNF) to property constraints and queries in Titan. Now, all queries inside titan have to conform to QNF which makes it easier to deal with conditions because one can precondition on QNF and then process accordingly. NOT YET TESTED.
",NotBuggy,"The commit message describes adding a feature related to Query-Normal-Form (QNF) but does not mention changes related to fixing any bugs. The change in the diff alters the method from `isValidDataType` to `isQNF` and also changes the return type from `true` to `false`. 

This change suggests a refactor or a conceptual shift towards the introduction of the QNF concept, not a correction of faulty behavior. The modification isn't addressing an inherent bug in the logic, rather aligning the logic with a new feature specification concerning QNF. 

Given that there's no mention of error handling or fixing incorrect functionality, the answer is **NotBuggy**."
titan,1229.json,ba36d65be185a738d418b86746aad625361bb71b,"@@ -1,3 +1,3 @@
     private static int getLimit(long limitAndValuePos) {
-        return (int)(limitAndValuePos>>>32);
+        return (int)(limitAndValuePos>>>32l);
     }",NotBuggy,"Bugifixing of backend change
",NotBuggy,"The code change alters the right-shift operation from 32 to 32l in the method `getLimit`. However, `32l` is the same as `32` because bit shifts are determined by the lower 5 bits for ints, making the change unnecessary. The commit message mentions a bug fix, but the change doesn't reflect a logical correction or fix any error in functionality. The modification appears to be a misunderstanding or unnecessary attempt at changing the code, as typical integer-promoted shifts would not require differentiating between an int and long in this context.

Given that nothing about the change indicates a resolved error or improved functionality:

The answer is **NotBuggy**."
titan,4809.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
-	public int getNumCompactions() {
-		return numberOfCompactions;
-	}

+    public int getNumCompactions() {
+        return numberOfCompactions;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message suggests a significant refactor, but the provided diff only shows changes in indentation, not substantive code changes. There's no indication here of a logic error fix or improved exception handling typically associated with a bug fix. The message also explicitly notes that tests do not currently pass, which implies the refactor is incomplete or in progress. Based on these observations, the changes in the diff do not align with a bug fix.

Therefore, the conclusion is **NotBuggy**."
titan,4181.json,9737d31b23256e071d7e761a53819feff896b472,"@@ -1,6 +1,9 @@
     public QueryProfiler addNested(String groupName) {
+        //Flatten out AND/OR nesting
+        if (groupName.equals(AND_QUERY) || groupName.equals(OR_QUERY)) return this;
+
         int nextId = (subMetricCounter++);
         MutableMetrics nested = new MutableMetrics(metrics.getId()+"".""+groupName+""_""+nextId,groupName);
         metrics.addNested(nested);
         return new TP3ProfileWrapper(nested);
     }",NotBuggy,"Flattening profile metrics
",NotBuggy,"The commit message states ""Flattening profile metrics,"" suggesting a change to how metrics are structured or nested. The code change aligns with this by preventing nested profiles for specific query types (""AND_QUERY"" and ""OR_QUERY""). Instead of creating new nested profiles, it returns the current instance, effectively flattening the hierarchy for these cases.

This modification does not indicate a bug fix. Instead, it alters the logic of metric nesting for specific conditions likely related to optimizing or simplifying the metric structure. There's no evidence of error correction, exception handling, or logical fixes that would suggest a bug resolution.

Therefore, the conclusion is **NotBuggy**."
titan,816.json,eab917b78fa300d8e6d81a543d4f1738f945b5b2,"@@ -1,3 +1,3 @@
-    public ByteBuffer getSliceEnd() {
+    public StaticBuffer getSliceEnd() {
         return sliceEnd;
     }",NotBuggy,"Refactored titan-core to use custom Buffer interfaces for better abstraction and thread safety instead of ByteBuffer.

INTERMEDIATE COMMIT - DOES NOT BUILD
",NotBuggy,"The commit message indicates a refactoring to use a custom Buffer interface, focusing on abstraction and thread safety rather than fixing any specific bug. The change in the diff reflects this shift from `ByteBuffer` to `StaticBuffer`. Since the commit message explicitly mentions it's an intermediate change and does not build, it suggests ongoing development rather than a bug fix. Therefore, the purpose is refactoring, improving structure and design.

The analysis confirms the commit aligns with the message focusing on a transition from one buffer interface to another for architectural reasons, without addressing a specific bug.

**NotBuggy**"
titan,3913.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,3 +1,3 @@
         public boolean hasNext() {
-            return nextEdge!=null;
+            return nextEdge != null;
         }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The commit message suggests a major refactor, indicating that the focus is on changing the structure or design of the codebase rather than fixing a specific bug. The change in the code is a minor formatting adjustment, adding spaces for readability around the `!=` operator. This change does not fix a bug but rather improves code readability. Additionally, the commit message explicitly states that tests do not pass, further indicating that this is a work-in-progress refactor rather than a bug fix.

Therefore, the modifications do not align with a bug fix, concluding with **NotBuggy**."
titan,2659.json,d5662482340f61d370cde27fece75d7ecb6b16e4,"@@ -1,3 +1,3 @@
-                public boolean apply(@Nullable InternalRelation internalRelation) {
-                    return internalRelation.getType().equals(key);
-                }

+                    public boolean apply(@Nullable TitanVertex vertex) {
+                        return query.matches(vertex);
+                    }",NotBuggy,"Intermediate commit on indexing and complete refactor
",NotBuggy,"The commit message describes an ""Intermediate commit on indexing and complete refactor,"" suggesting this change is a part of a work-in-progress refactoring rather than a bug fix. The code changes replace a method's logic and parameters:

1. **Parameter Change:** The method's parameter changes from `InternalRelation internalRelation` to `TitanVertex vertex`.
2. **Logic Change:** The logic inside the method changes from checking if the internal relation's type equals a key to whether a query matches a vertex.

This refactor aligns with adjusting or improving functionality, potentially part of a broader re-architecting effort. There's no direct indication of a bug being fixed, as the modifications seem more about altering the scope or purpose of the method to suit new requirements or behaviors.

Thus, without explicit evidence pointing to a bug resolution, the answer concludes with **NotBuggy**."
titan,526.json,d4056cb5a0ec5764ba5d0bd7454c68062d576ba1,"@@ -1,4 +1,4 @@
-        public Builder supportedMappings(Mapping... maps) {
+        public Builder supportedStringMappings(Mapping... maps) {
             supportedMappings.addAll(Arrays.asList(maps));
             return this;
         }",NotBuggy,"Finised streamlining test cases and query semantics across the different indexing backends. Added additional test coverage for Lucene.
SOLR does not yet work.
",NotBuggy,"The commit message talks about streamlining test cases and query semantics, while the code change itself simply renames a method from `supportedMappings` to `supportedStringMappings`, likely to clarify its specific use or distinguish it from other types of mappings. There is no indication of a bug being fixed here—the change does not correct any logic or behavior, nor does it address any issue that would cause malfunction. It appears to be more about code clarity or organization.

The changes do not align with a bug fix as described in the commit message and there is no indication of error handling, exception improvements, or logic corrections that would typically hint at such. Therefore, this modification does not suggest a bug fix.

The conclusion is **NotBuggy**."
titan,5594.json,31b68d5c462b46d42f4ca7d3b3f63923a6d5a1c7,"@@ -1,12 +1,12 @@
     public static ScanMetrics hbaseRemove(String titanPropertiesPath, String indexName, String relationType)
             throws InterruptedException, IOException, ClassNotFoundException {
         Properties p = new Properties();
         FileInputStream fis = null;
         try {
             fis = new FileInputStream(titanPropertiesPath);
             p.load(fis);
             return hbaseRemove(p, indexName, relationType);
         } finally {
-            IOUtils.close(fis);
+            IOUtils.closeQuietly(fis);
         }
     }",NotBuggy,"Remove elasticsearch-hadoop-mr dependency

titan-hadoop has depended on this artifact since
042d04148489f4650e232682388f24ace709a95b, but it was only actually
used for its IOUtils.close static helper method.  Titan already has a
bunch of alternative providers of static helper methods with
equivalent functionality.
",NotBuggy,"The commit message indicates the removal of a dependency, specifically stating that it was only used for a static helper method, `IOUtils.close`. The code change in the diff switches from `IOUtils.close(fis)` to `IOUtils.closeQuietly(fis)`. This addresses exception handling during resource closing, changing from a method that might throw an `IOException` to one that handles it quietly without propagating the exception.

Given this context, the change appears to be a modification for safer resource management, but it doesn't directly address a bug. Instead, it reflects a routine improvement in how resources are managed (making the closing process more robust against exceptions) and alignment with the intent to remove a dependency. 

The change in this commit is more about refactoring for consistency and dependency removal than a direct bug fix. Therefore, the correct classification is **NotBuggy**."
titan,2713.json,51075f047f33c09c17c33f7e61fcf859c815ed52,"@@ -1,5 +1,6 @@
-    protected void verifyAccess() {
-        if (isRemoved()) {
-            throw InvalidElementException.removedException(this);
-        }
+    protected final void verifyAccess() {
+        return;
+//        if (isRemoved()) {
+//            throw InvalidElementException.removedException(this);
+//        }
     }",NotBuggy,"Updated to new TP3
",NotBuggy,"The commit message states ""Updated to new TP3,"" which suggests an update or migration to a new version or framework, but it does not specifically mention a bug fix.

Looking at the code:
- The access verification logic has been changed. The `verifyAccess` method was checking if an element is removed and throwing an exception if so.
- The new implementation makes the method `final` and removes its original logic, replacing it with a simple `return;` and commenting out the original code.

This change:
- Removes the exception handling that would occur if an element was removed. This could potentially be a bug if the method is meant to ensure validity and protect against operations on removed elements.
- Makes the method `final`, preventing it from being overridden in subclasses, which could either be a design choice or necessary for TP3 compliance.

The commit message does not mention a bug fix, but given the content of the commit, it suggests a potentially significant change in behavior by bypassing error handling. However, there's no direct indication that this was to fix a bug; it primarily looks like a structural change related to the update to TP3.

Based on the available information, and since the commit message doesn't support a bug-fix conclusion tied to this specific change, the assessment is **NotBuggy**."
titan,2620.json,aead96abd7152d9843e25578fd640dbbdaa8b765,"@@ -1,3 +1,3 @@
     public Iterable<Edge> getEdges() {
-        return new VertexCentricEdgeIterable(getVertices());
+        return new VertexCentricEdgeIterable(getInternalVertices(),RelationCategory.EDGE);
     }",NotBuggy,"Intermediate big bang commit
",NotBuggy,"The commit message ""Intermediate big bang commit"" suggests a large refactoring or restructuring, typically with a high level of changes not focused on specific bug fixes. 

Analyzing the diff:

- The code change modifies the `getEdges` method to change its return value from using `getVertices()` to `getInternalVertices()` and adds a new parameter `RelationCategory.EDGE`.
  
Key considerations:

1. **Change in Method Call**: Switching from `getVertices()` to `getInternalVertices()` may imply internal functionality changes. If `getInternalVertices()` better fits the intended semantic or operational context of this method, it could be addressing a logical error. However, the commit message doesn't specify a bug fix.

2. **Additional Parameter**: Adding the parameter `RelationCategory.EDGE` introduces a new dimension to the `VertexCentricEdgeIterable`. This indicates a shift in how edges are processed or categorized but does not clearly outline a specific bug resolution.

3. **Commit Message Context**: The term ""big bang"" in the commit message implies a significant change, likely more focused on refactoring or a new feature, rather than a targeted bug fix.

Given these points, the modification seems more structural or feature-oriented rather than a straightforward bug correction. Although it might address underlying issues indirectly, the commit message suggests it is not primarily a bug fix.

**Conclusion**: **NotBuggy**."
titan,4872.json,ae15683f30b52b053cb7c62c6bd9162e374f62a4,"@@ -1,12 +1,12 @@
-	public static final long[] arrayDifference(long[] arr, long[] subset) {
-		long[] res = new long[arr.length-subset.length];
-		int pos = 0;
-		for (int i=0;i<arr.length;i++) {
-			if (!Longs.contains(subset, arr[i])) {
-				res[pos]=arr[i];
-				pos++;
-			}
-		}
-		assert pos==res.length;
-		return res;
-	}

+    public static final long[] arrayDifference(long[] arr, long[] subset) {
+        long[] res = new long[arr.length - subset.length];
+        int pos = 0;
+        for (int i = 0; i < arr.length; i++) {
+            if (!Longs.contains(subset, arr[i])) {
+                res[pos] = arr[i];
+                pos++;
+            }
+        }
+        assert pos == res.length;
+        return res;
+    }",NotBuggy,"big fatty refactor -- tests do not pass.
",NotBuggy,"The diff provided displays formatting changes without logic or functionality modifications. The commit message states a ""big fatty refactor"" and notes that tests do not pass, suggesting the refactor possibly introduced issues. However, based on this diff alone, there is no indication of a bug fix being implemented. 

Thus, the relevant conclusion based on the given changes and message context is **NotBuggy**."
titan,4164.json,7f94323100a007d1f346b163485b14a43a53dd4c,"@@ -1,3 +1,3 @@
     public int getLimit() {
-        return getBranchFactor();
+        return this.limit;
     }",NotBuggy,"Updated optimizers to include PropertyStep. Reworked current logic. Tests to be added.
",NotBuggy,"The commit message indicates updates to optimizers and a rework of existing logic, mentioning that tests will be added. However, the specific change in the diff addresses a single method alteration in which `getLimit()` now returns `this.limit` instead of `getBranchFactor()`. This change suggests a logical update in how limits are determined, potentially reflecting a correction or improvement to the current logic but does not directly indicate a bug fix. The correction may align with the broader intent of reworking logic but not specifically fixing an erroneous condition or behavior that would have been classified clearly as a bug in the existing code.

Without clear indication that this change corrects a bug specifically – such as related to previous erroneous output or behavioral flaws – based on the provided information and commit message, this change seems more like an alignment of logic than a strict bug fix. Thus, the evaluation leads to a conclusion of **NotBuggy**."
titan,2594.json,e1ac9ae34518829d69f60234525b2133b31a88e2,"@@ -1,4 +1,4 @@
     public InternalVertex getOtherPartitionVertex(TitanVertex partitionedVertex, long otherPartition) {
         Preconditions.checkArgument(isPartitionedVertex(partitionedVertex));
-        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getID(), otherPartition));
+        return getExistingVertex(idManager.getPartitionedVertexId(partitionedVertex.getLongId(), otherPartition));
     }",NotBuggy,"Partial Faunus refactoring
",NotBuggy,"The diff changes the method call from `partitionedVertex.getID()` to `partitionedVertex.getLongId()`, which might reflect an update in how the vertex ID is accessed. However, the commit message mentions ""Partial Faunus refactoring,"" which does not indicate a specific bug fix. Instead, it suggests an ongoing refactoring process. The change does not highlight explicit bug-fixing activity, such as correcting a logic error or fixing exception handling.

Since the commit message does not claim a bug fix and the code change aligns with the message indicating refactoring, rather than fixing a malfunction, I will conclude it is **NotBuggy**."
